file_path,api_count,code
setup.py,0,"b""from distutils.core import setup\nimport setuptools\n\nwith open('README.md') as readme_file: \n    readme_file.readline()\n    readme = readme_file.read()\nexec(open('ktrain/version.py').read())\n\n\nsetup(\n  name = 'ktrain',\n  packages = setuptools.find_packages(),\n  package_data={'ktrain': ['text/shallownlp/ner_models/*']},\n  version = __version__,\n  license='Apache License 2.0',\n  description = 'ktrain is a wrapper for TensorFlow Keras that makes deep learning and AI more accessible and easier to apply',\n  #description = 'ktrain is a lightweight wrapper for TensorFlow Keras to help train neural networks',\n  long_description = readme,\n  long_description_content_type = 'text/markdown',\n  author = 'Arun S. Maiya',\n  author_email = 'arun@maiya.net',\n  url = 'https://github.com/amaiya/ktrain',\n  keywords = ['tensorflow', 'keras', 'deep learning', 'machine learning'],\n  install_requires=[\n          'tensorflow==2.1.0',\n          'scikit-learn==0.21.3', # affects format of predictor.explain\n          'matplotlib >= 3.0.0',\n          'pandas >= 1.0.1',\n          'fastprogress >= 0.1.21',\n          'keras_bert>=0.81.0',\n          'requests',\n          'joblib',\n          'langdetect',\n          'jieba',\n          # fix cchardet to 2.1.5 due to this issue: https://github.com/PyYoshi/cChardet/issues/61\n          'cchardet==2.1.5', \n          'networkx>=2.3',\n          'bokeh',\n          'seqeval',\n          'packaging',\n          'tensorflow_datasets',\n          'transformers>=2.11.0', # due to breaking change in v2.11.0\n          'ipython',\n          'syntok',\n          'whoosh'\n          #'stellargraph>=0.8.2',\n          #'eli5 >= 0.10.0',\n          #'allennlp', # required for Elmo embeddings since TF2 TF_HUB does not work\n          #'pillow'\n      ],\n  classifiers=[  # Optional\n    # How mature is this project? Common values are\n    #   3 - Alpha\n    #   4 - Beta\n    #   5 - Production/Stable\n    'Development Status :: 4 - Beta',\n\n    # Indicate who your project is intended for\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n\n    # Pick your license as you wish\n    'License :: OSI Approved :: Apache Software License',\n\n    # Specify the Python versions you support here. In particular, ensure\n    # that you indicate whether you support Python 2, Python 3 or both.\n    'Programming Language :: Python :: 3.6',\n    'Programming Language :: Python :: 3.7',\n  ],\n)\n"""
ktrain/__init__.py,2,"b'from .version import __version__\nfrom .imports import *\nfrom .core import ArrayLearner, GenLearner, get_predictor, load_predictor, release_gpu_memory\nfrom .vision.learner import ImageClassLearner\nfrom .text.learner import BERTTextClassLearner, TransformerTextClassLearner\nfrom .text.ner.learner import NERLearner\nfrom .graph.learner import NodeClassLearner, LinkPredLearner\nfrom .data import Dataset, TFDataset, SequenceDataset\n\nfrom . import utils as U\n\n__all__ = [\'get_learner\', \'get_predictor\', \'load_predictor\', \'release_gpu_memory\',\n           \'Dataset\', \'TFDataset\', \'SequenceDataset\']\n\n\n\n\ndef get_learner(model, train_data=None, val_data=None, \n                batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                workers=1, use_multiprocessing=False, multigpu=False):\n    """"""\n    Returns a Learner instance that can be used to tune and train Keras models.\n\n    model (Model):        A compiled instance of keras.engine.training.Model\n    train_data (tuple or generator): Either a: \n                                   1) tuple of (x_train, y_train), where x_train and \n                                      y_train are numpy.ndarrays or \n                                   2) Iterator\n    val_data (tuple or generator): Either a: \n                                   1) tuple of (x_test, y_test), where x_testand \n                                      y_test are numpy.ndarrays or \n                                   2) Iterator\n                                   Note: Should be same type as train_data.\n    batch_size (int):              Batch size to use in training. default:32\n    eval_batch_size(int):  batch size used by learner.predict\n                           only applies to validaton data during training if\n                           val_data is instance of utils.Sequence.\n                           default:32\n    workers (int): number of cpu processes used to load data.\n                   This is ignored unless train_data/val_data is an instance of \n                   tf.keras.preprocessing.image.DirectoryIterator or tf.keras.preprocessing.image.DataFrameIterator. \n    use_multiprocessing(bool):  whether or not to use multiprocessing for workers\n                               This is ignored unless train_data/val_data is an instance of \n                               tf.keras.preprocessing.image.DirectoryIterator or tf.keras.preprocessing.image.DataFrameIterator. \n    multigpu(bool):             Lets the Learner know that the model has been \n                                replicated on more than 1 GPU.\n                                Only supported for models from vision.image_classifiers\n                                at this time.\n    """"""\n\n    # check arguments\n    if not isinstance(model, Model):\n        raise ValueError(\'model must be of instance Model\')\n    U.data_arg_check(train_data=train_data, val_data=val_data)\n    if type(workers) != type(1) or workers < 1:\n        workers =1\n    # check for NumpyArrayIterator \n    if train_data and not U.ondisk(train_data):\n        if workers > 1 and not use_multiprocessing:\n            use_multiprocessing = True\n            wrn_msg = \'Changed use_multiprocessing to True because NumpyArrayIterator with workers>1\'\n            wrn_msg +=\' is slow when use_multiprocessing=False.\'\n            wrn_msg += \' If you experience issues with this, please set workers=1 and use_multiprocessing=False.\'\n            warnings.warn(wrn_msg)\n\n    # verify BERT\n    is_bert = U.bert_data_tuple(train_data)\n    if is_bert:\n        maxlen = U.shape_from_data(train_data)[1]\n        msg = """"""For a GPU with 12GB of RAM, the following maxima apply:\n        sequence len=64, max_batch_size=64\n        sequence len=128, max_batch_size=32\n        sequence len=256, max_batch_size=16\n        sequence len=320, max_batch_size=14\n        sequence len=384, max_batch_size=12\n        sequence len=512, max_batch_size=6\n        \n        You\'ve exceeded these limits.\n        If using a GPU with <=12GB of memory, you may run out of memory during training.\n        If necessary, adjust sequence length or batch size based on above.""""""\n        wrn = False\n        if maxlen > 64 and batch_size > 64:\n            wrn=True\n        elif maxlen > 128 and batch_size>32:\n            wrn=True\n        elif maxlen>256 and batch_size>16:\n            wrn=True\n        elif maxlen>320 and batch_size>14:\n            wrn=True\n        elif maxlen>384 and batch_size>12:\n            wrn=True\n        elif maxlen > 512 and batch_size>6:\n            wrn=True\n        if wrn: warnings.warn(msg)\n\n\n    # return the appropriate trainer\n    if U.is_iter(train_data):\n        if U.is_ner(model=model, data=train_data):\n            learner = NERLearner\n        elif U.is_imageclass_from_data(train_data):\n            learner = ImageClassLearner\n        elif U.is_nodeclass(data=train_data):\n            learner = NodeClassLearner\n        elif U.is_nodeclass(data=train_data):\n            learner = LinkPredLearner\n        elif U.is_huggingface(data=train_data):\n            learner = TransformerTextClassLearner\n        else:\n            learner = GenLearner\n    else:\n        if is_bert: \n            learner = BERTTextClassLearner\n        else: # vanilla text classifiers use standard ArrayLearners\n            learner = ArrayLearner\n    return learner(model, train_data=train_data, val_data=val_data, \n                   batch_size=batch_size, eval_batch_size=eval_batch_size, \n                   workers=workers, use_multiprocessing=use_multiprocessing, multigpu=multigpu)\n'"
ktrain/core.py,5,"b'from .imports import *\n\nfrom .lroptimize.sgdr import *\nfrom .lroptimize.triangular import *\nfrom .lroptimize.lrfinder import *\nfrom .lroptimize.optimization import AdamWeightDecay\nfrom . import utils as U\n\nfrom .vision.preprocessor import ImagePreprocessor\nfrom .vision.predictor import ImagePredictor\nfrom .text.preprocessor import TextPreprocessor, BERTPreprocessor, TransformersPreprocessor\nfrom .text.predictor import TextPredictor\nfrom .text.ner.predictor import NERPredictor\nfrom .text.ner.preprocessor import NERPreprocessor\nfrom .graph.predictor import NodePredictor, LinkPredictor\nfrom .graph.preprocessor import NodePreprocessor, LinkPreprocessor\n\n\nclass Learner(ABC):\n    """"""\n    Abstract class used to tune and train Keras models. The fit method is\n    an abstract method and must be implemented by subclasses.\n\n    """"""\n    def __init__(self, model, workers=1, use_multiprocessing=False, multigpu=False):\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        self.model = model\n        self.lr_finder = LRFinder(self.model)\n        self.workers = workers\n        self.use_multiprocessing = use_multiprocessing\n        self.multigpu=multigpu\n        self.history = None\n\n        # save original weights of model\n        try:\n            new_file, weightfile = tempfile.mkstemp()\n            self.model.save_weights(weightfile)\n            self._original_weights = weightfile\n        except:\n            warnings.warn(\'Could not save original model weights\')\n            self._original_weights = None\n\n\n    def get_weight_decay(self):\n        """"""\n        Get current weight decay rate\n        """"""\n        if type(self.model.optimizer).__name__ == \'AdamWeightDecay\':\n            return self.model.optimizer.weight_decay_rate\n        else:\n            return None\n\n\n    def set_weight_decay(self, wd=U.DEFAULT_WD):\n        """"""\n        Sets global weight decay via AdamWeightDecay optimizer\n        Args:\n          wd(float): weight decay\n        Returns:\n          None\n              \n        """"""\n        self._recompile(wd=wd)\n        return\n        \n\n\n\n    def validate(self, val_data=None, print_report=True, class_names=[]):\n        """"""\n        Returns confusion matrix and optionally prints\n        a classification report.\n        This is currently only supported for binary and multiclass\n        classification, not multilabel classification.\n        """"""\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n\n        classification, multilabel = U.is_classifier(self.model)\n        if not classification:\n            warnings.warn(\'learner.validate is only for classification problems. \' \n                          \'For regression, etc., use learner.predict and learner.ground_truth \'\n                          \'to manually validate.\')\n            return\n            \n        if U.is_multilabel(val) or multilabel:\n            warnings.warn(\'multilabel confusion matrices not yet supported\')\n            return\n        y_pred = self.predict(val_data=val)\n        y_true = self.ground_truth(val_data=val)\n        y_pred = np.squeeze(y_pred)\n        y_true = np.squeeze(y_true)\n        if len(y_pred.shape) == 1:\n            y_pred = np.where(y_pred > 0.5, 1, 0)\n            y_true = np.where(y_true > 0.5, 1, 0)\n        else:\n            y_pred = np.argmax(y_pred, axis=1)\n            y_true = np.argmax(y_true, axis=1)\n        if print_report:\n            if class_names:\n                try:\n                    class_names = [str(s) for s in class_names]\n                except:\n                    pass\n                report = classification_report(y_true, y_pred, target_names=class_names)\n            else:\n                report = classification_report(y_true, y_pred)\n            print(report)\n            cm_func = confusion_matrix\n        cm =  confusion_matrix(y_true,  y_pred)\n        return cm\n\n\n    def _check_val(self, val_data):\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if val is None: raise Exception(\'val_data must be supplied to get_learner or view_top_losses\')\n        return val\n\n\n    def top_losses(self, n=4, val_data=None, preproc=None):\n        """"""\n        Computes losses on validation set sorted by examples with top losses\n        Args:\n          n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n          val_data:  optional val_data to use instead of self.val_data\n          preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                  For some data like text data, a preprocessor\n                                  is required to undo the pre-processing\n                                   to correctly view raw data.\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n\n\n        # check validation data and arguments\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if val is None: raise Exception(\'val_data must be supplied to get_learner or top_losses\')\n        if type(n) == type(42):\n            n = (0, n)\n\n\n        #multilabel = True if U.is_multilabel(val) else False\n        classification, multilabel = U.is_classifier(self.model)\n\n\n        # get predicictions and ground truth\n        y_pred = self.predict(val_data=val)\n        y_true = self.ground_truth(val_data=val)\n        y_true = y_true.astype(\'float32\')\n\n        # adjust y_true for regression problems\n        if not classification and len(y_true.shape) == 1 and\\\n                (len(y_pred.shape) == 2 and y_pred.shape[1] == 1):\n            y_true = np.expand_dims(y_true, -1)\n\n\n        # compute loss\n        # this doesn\'t work in tf.keras 1.14\n        #losses = self.model.loss_functions[0](tf.convert_to_tensor(y_true), tf.convert_to_tensor(y_pred))\n        #if U.is_tf_keras():\n            #L = self.model.loss_functions[0].fn\n        #else:\n            #L = self.model.loss_functions[0]\n        L = U.loss_fn_from_model(self.model)\n        losses = L(tf.convert_to_tensor(y_true), tf.convert_to_tensor(y_pred))\n        if DISABLE_V2_BEHAVIOR:\n            losses = tf.Session().run(losses)\n        else:\n            losses = losses.numpy()\n\n\n        class_names = [] if preproc is None else preproc.get_classes()\n        if preproc is None: \n            class_fcn = lambda x:""%s"" % (x)\n        else:\n            class_fcn = lambda x:class_names[x]\n\n        # regression output modifications\n        if not classification:\n            if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\n                y_pred = np.squeeze(y_pred)\n                y_pred = np.around(y_pred, 2)\n            if len(y_true.shape) == 2 and y_true.shape[1] == 1:\n                y_true = np.squeeze(y_true)\n                y_true = np.around(y_true, 2)\n\n        # sort by loss and prune correct classifications, if necessary\n        if classification and not multilabel:\n            y_pred = np.squeeze(y_pred)\n            y_true = np.squeeze(y_true)\n            if len(y_pred.shape) == 1:\n                y_p = np.where(y_pred > 0.5, 1, 0)\n                y_t = np.where(y_true>0.5, 1, 0)\n            else:\n                y_p = np.argmax(y_pred, axis=1)\n                y_t = np.argmax(y_true, axis=1)\n            tups = [(i,x, class_fcn(y_t[i]), class_fcn(y_p[i])) for i, x in enumerate(losses) \n                     if y_p[i] != y_t[i]]\n        else:\n            tups = [(i,x, y_true[i], np.around(y_pred[i],2)) for i, x in enumerate(losses)]\n        tups.sort(key=operator.itemgetter(1), reverse=True)\n\n        # prune by given range\n        tups = tups[n[0]:n[1]] if n is not None else tups\n        return tups\n\n\n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Musta be overridden by Learner subclasses.\n        """"""\n        raise NotImplementedError(\'view_top_losses must be overriden by Learner subclass\')\n\n\n    def _make_model_folder(self, fpath):\n        if os.path.isfile(fpath):\n            raise ValueError(f\'There is an existing file named {fpath}. \' +\\\n                              \'Please use dfferent value for fpath.\')\n        elif os.path.exists(fpath):\n            #warnings.warn(\'model is being saved to folder that already exists: %s\' % (fpath))\n            pass\n        elif not os.path.exists(fpath):\n            os.makedirs(fpath)\n\n\n    def save_model(self, fpath):\n        """"""\n        a wrapper to model.save\n        """"""\n        self._make_model_folder(fpath)\n        self.model.save(os.path.join(fpath, U.MODEL_NAME), save_format=\'h5\')\n        return\n\n\n    def load_model(self, fpath, custom_objects=None):\n        """"""\n        a wrapper to load_model\n        """"""\n        self.model = _load_model(fpath, train_data=self.train_data, custom_objects=custom_objects)\n        return\n\n    def _is_adamlike(self):\n        """"""\n        checks whether optimizer attached to model is an \n        ""Adam-like"" optimizer with beta_1 parameter.\n        """"""\n        return self.model is not None and hasattr(self.model.optimizer, \'beta_1\')\n\n\n    def _recompile(self, wd=None):\n        # ISSUE: recompile does not work correctly with multigpu models\n        if self.multigpu:\n            err_msg = """"""\n                   IMPORTANT: freeze and unfreeze methods do not currently work with \n                   multi-GPU models.  If you are using the load_imagemodel method to\n                   define your model, please reload your model and use the freeze_layers \n                   argument of load_imagemodel to selectively freeze layers.\n                   """"""\n            raise Exception(err_msg)\n        \n        #if self.multigpu:\n            #with tf.device(""/cpu:0""):\n                #metrics = [m.name for m in self.model.metrics] if U.is_tf_keras() else self.model.metrics\n                #self.model.compile(optimizer=self.model.optimizer,\n                                   #loss=self.model.loss,\n                                   #metrics=metrics)\n        metrics = [m.name for m in self.model.metrics] if U.is_tf_keras() else self.model.metrics\n        if wd is not None and type(self.model.optimizer).__name__ != \'AdamWeightDecay\':\n            warnings.warn(\'recompiling model to use AdamWeightDecay as opimizer with weight decay of %s\' % (wd) )\n            optimizer = U.get_default_optimizer(wd=wd)\n        elif wd is not None:\n            optimizer = U.get_default_optimizer(wd=wd)\n        else:\n            optimizer = self.model.optimizer\n        self.model.compile(optimizer=optimizer,\n                           loss=self.model.loss,\n                           metrics=metrics)\n\n        return\n\n\n    def set_model(self, model):\n        """"""\n        replace model in this Learner instance\n        """"""\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        self.model = model\n        self.history = None\n        return\n\n\n    def freeze(self, freeze_range=None):\n        """"""\n        If freeze_range is None, makes all layers trainable=False except last Dense layer.\n        If freeze_range is given, freezes the first <freeze_range> layers and\n        unfrezes all remaining layers.\n        NOTE:      Freeze method does not currently work with \n                   multi-GPU models.  If you are using the load_imagemodel method,\n                   please use the freeze_layers argument of load_imagemodel\n                   to freeze layers.\n        Args:\n            freeze_range(int): number of layers to freeze\n        Returns:\n            None\n        """"""\n\n        if freeze_range is None:\n            # freeze everything except last Dense layer\n            # first find last dense layer\n            dense_id = None\n            for i, layer in reversed(list(enumerate(self.model.layers))):\n                if isinstance(layer, Dense):\n                    dense_id = i\n                    break\n            if dense_id is None: raise Exception(\'cannot find Dense layer in this model\')\n            for i, layer in enumerate(self.model.layers):\n                if i < dense_id: \n                    layer.trainable=False\n                else:\n                    layer.trainable=True\n        else:\n            # freeze all layers up to and including layer_id\n            if type(freeze_range) != type(1) or freeze_range <1: \n                raise ValueError(\'freeze_range must be integer > 0\')\n            for i, layer in enumerate(self.model.layers):\n                if i < freeze_range: \n                    layer.trainable=False\n                else:\n                    layer.trainable=True\n        self._recompile()\n        return\n\n\n    def unfreeze(self, exclude_range=None):\n        """"""\n        Make every layer trainable except those in exclude_range.\n        unfreeze is simply a proxy method to freeze.\n        NOTE:      Unfreeze method does not currently work with \n                   multi-GPU models.  If you are using the load_imagemodel method,\n                   please use the freeze_layers argument of load_imagemodel\n                   to freeze layers.\n        """"""\n        # make all layers trainable\n        for i, layer in enumerate(self.model.layers):\n            layer.trainable = True\n        if exclude_range:\n            for i, layer in enumerate(self.model.layers[:exclude_range]):\n                layer.trainable = False\n        self._recompile()\n        return\n\n\n    def reset_weights(self, nosave=False, verbose=1):\n        """"""\n        Re-initializes network - use with caution, as this may not be robust\n        """"""\n        #initial_weights = self.model.get_weights()\n        #backend_name = K.backend()\n        #if backend_name == \'tensorflow\': \n            #k_eval = lambda placeholder: placeholder.eval(session=K.get_session())\n        #elif backend_name == \'theano\': \n            #k_eval = lambda placeholder: placeholder.eval()\n        #else: \n            #raise ValueError(""Unsupported backend"")\n        #new_weights = [k_eval(glorot_uniform()(w.shape)) for w in initial_weights]\n        #if nosave: return new_weights\n        #self.model.set_weights(new_weights)\n        #self.history = None\n        #print(\'Weights of moedl have been reset.\')\n\n        if os.path.isfile(self._original_weights):\n            self.model.load_weights(self._original_weights)\n            self.history = None\n            U.vprint(\'Model weights have been reset.\', verbose=verbose)\n        else:\n            warnings.warn(\'Weights have not been reset because the original weights file \'+\\\n                          \'(%s) no longer exists.\' % (self._original_weights))\n        return\n\n\n\n    def lr_find(self, start_lr=1e-7, lr_mult=1.01, max_epochs=None, \n                stop_factor=4, show_plot=False, verbose=1):\n        """"""\n        Plots loss as learning rate is increased.  Highest learning rate \n        corresponding to a still falling loss should be chosen.\n\n        If you find the LR finder is running for more epochs than you\'d prefer,\n        you can set max_epochs (e.g., max_epochs=5) to estimate LR with a \n        smaller sample size.\n\n        If lr_mult is supplied and max_epochs is None, LR will increase until loss diverges.\n        Reasonable values of lr_mult are between 1.01 and 1.05.\n\n        If max_epochs is supplied, lr_mult argument is ignored and computed automatically.\n\n        Reference: https://arxiv.org/abs/1506.01186\n\n        Args:\n            start_lr (float): smallest lr to start simulation\n            lr_mult (float): multiplication factor to increase LR.\n                             Ignored if max_epochs is supplied.\n            max_epochs (int):  maximum number of epochs to simulate.\n                               lr_mult is ignored if max_epoch is supplied.\n                               Default is None. Set max_epochs to an integer\n                               (e.g., 5) if lr_find is taking too long\n                               and running for more epochs than desired.\n            stop_factor(int): factor used to determine threhsold that loss \n                              must exceed to stop training simulation.\n                              Increase this if loss is erratic and lr_find\n                              exits too early.\n            show_plot (bool):  If True, automatically invoke lr_plot\n            verbose (bool): specifies how much output to print\n        Returns:\n            float:  Numerical estimate of best lr.  \n                    The lr_plot method should be invoked to\n                    identify the maximal loss associated with falling loss.\n        """"""\n\n        U.vprint(\'simulating training for different learning rates... this may take a few moments...\',\n                verbose=verbose)\n\n        # save current weights and temporarily restore original weights\n        new_file, weightfile = tempfile.mkstemp()\n        self.model.save_weights(weightfile)\n        #self.model.load_weights(self._original_weights)\n\n\n         # compute steps_per_epoch\n        num_samples = U.nsamples_from_data(self.train_data)\n        bs = self.train_data.batch_size if hasattr(self.train_data, \'batch_size\') else self.batch_size\n        if U.is_iter(self.train_data):\n            use_gen = True\n            steps_per_epoch = num_samples // bs\n        else:\n            use_gen = False\n            steps_per_epoch = np.ceil(num_samples/bs)\n\n        # check steps_per_epoch\n        if steps_per_epoch <=64 and max_epochs is None:\n            warnings.warn(\'max_epochs is being set to 5 since steps per epoch is small. \' +\\\n                          \'If you wish to estimate LR using more epochs, set max_epochs manually.\')\n            max_epochs = 5\n\n\n        try:\n            # track and plot learning rates\n            self.lr_finder = LRFinder(self.model, stop_factor=stop_factor)\n            self.lr_finder.find(self._prepare(self.train_data), \n                                steps_per_epoch,\n                                use_gen=use_gen,\n                                start_lr=start_lr, lr_mult=lr_mult, \n                                max_epochs=max_epochs,\n                                workers=self.workers, \n                                use_multiprocessing=self.use_multiprocessing, \n                                batch_size=self.batch_size,\n                                verbose=verbose)\n        except KeyboardInterrupt:\n            # re-load current weights\n            self.model.load_weights(weightfile)\n            return\n\n        # re-load current weights\n        self.model.load_weights(weightfile)\n\n        # instructions to invoker\n        U.vprint(\'\\n\', verbose=verbose)\n        U.vprint(\'done.\', verbose=verbose)\n        if show_plot:\n            U.vprint(\'Visually inspect loss plot and select learning rate associated with falling loss\', verbose=verbose)\n            self.lr_plot()\n        else:\n            U.vprint(\'Please invoke the Learner.lr_plot() method to visually inspect \'\n                  \'the loss plot to help identify the maximal learning rate \'\n                  \'associated with falling loss.\', verbose=verbose)\n        return \n\n\n    def lr_plot(self, n_skip_beginning=10, n_skip_end=5, suggest=False):\n        """"""\n        Plots the loss vs. learning rate to help identify\n        The maximal learning rate associated with a falling loss.\n        The nskip_beginning and n_skip_end arguments can be used\n        to ""zoom in"" on the plot.\n        Args:\n            n_skip_beginning(int): number of batches to skip on the left.\n            n_skip_end(int):  number of batches to skip on the right.\n            suggest(bool): will highlight numerical estimate\n                           of best lr if True - methods adapted from fastai\n          \n        """"""\n        self.lr_finder.plot_loss(n_skip_beginning=n_skip_beginning,\n                                 n_skip_end=n_skip_end, suggest=suggest)\n        return\n\n\n    def plot(self, plot_type=\'loss\'):\n        """"""\n        plots training history\n        Args:\n          plot_type (str):  one of {\'loss\', \'lr\', \'momentum\'}\n        Return:\n          None\n        """"""\n        if self.history is None:\n            raise Exception(\'No training history - did you train the model yet?\')\n\n        if plot_type == \'loss\':\n            plt.plot(self.history.history[\'loss\'])\n            if \'val_loss\' in self.history.history:\n                plt.plot(self.history.history[\'val_loss\'])\n                legend_items = [\'train\', \'validation\']\n            else:\n                legend_items = [\'train\']\n            plt.title(\'Model Loss\')\n            plt.ylabel(\'loss\')\n            plt.xlabel(\'epoch\')\n            plt.legend(legend_items, loc=\'upper left\')\n            plt.show()\n        elif plot_type == \'lr\':\n            if \'lr\' not in self.history.history:\n                raise ValueError(\'no lr in history: are you sure you used autofit or fit_onecycle to train?\')\n            plt.plot(self.history.history[\'lr\'])\n            plt.title(\'LR Schedule\')\n            plt.ylabel(\'lr\')\n            plt.xlabel(\'iterations\')\n            plt.show()\n        elif plot_type == \'momentum\':\n            if \'momentum\' not in self.history.history:\n                raise ValueError(\'no momentum history: are you sure you used autofit or fit_onecycle to train?\')\n            plt.plot(self.history.history[\'momentum\'])\n            plt.title(\'Momentum Schedule\')\n            plt.ylabel(\'momentum\')\n            plt.xlabel(\'iterations\')\n            plt.show()\n        else:\n            raise ValueError(\'invalid type: choose loss, lr, or momentum\')\n        return\n\n\n    def print_layers(self, show_wd=False):\n        """"""\n        prints the layers of the model along with indices\n        """"""\n        for i, layer in enumerate(self.model.layers):\n            if show_wd and hasattr(layer, \'kernel_regularizer\'):\n                reg = layer.kernel_regularizer\n                if hasattr(reg, \'l2\'):\n                    wd = reg.l2\n                elif hasattr(reg, \'l1\'):\n                    wd = reg.l1\n                else:\n                    wd = None\n                print(""%s (trainable=%s, wd=%s) : %s"" % (i, layer.trainable, wd, layer))\n            else:\n                print(""%s (trainable=%s) : %s"" % (i, layer.trainable, layer))\n        return\n\n\n    def layer_output(self, layer_id, example_id=0, use_val=False):\n        # should implemented in subclass\n        raise NotImplementedError\n\n\n    def set_lr(self, lr):\n        K.set_value(self.model.optimizer.lr, lr)\n        return\n\n\n    def _check_cycles(self, n_cycles, cycle_len, cycle_mult):\n        if type(n_cycles) != type(1) or n_cycles <1:\n            raise ValueError(\'n_cycles must be >= 1\')\n        if type(cycle_mult) != type(1) or cycle_mult < 1:\n            raise ValueError(\'cycle_mult must by >= 1\')\n        if cycle_len is not None:\n            if type(cycle_len) != type(1) or cycle_len < 1:\n                raise ValueError(\'cycle_len must either be None or >= 1\')\n\n        # calculate number of epochs\n        if cycle_len is None:\n            epochs = n_cycles\n        else:\n            epochs = 0\n            tmp_cycle_len = cycle_len\n            for i in range(n_cycles):\n                epochs += tmp_cycle_len\n                tmp_cycle_len *= cycle_mult\n        return epochs\n\n\n    def _cb_sgdr(self, max_lr, steps_per_epoch, cycle_len, cycle_mult, lr_decay=1.0, callbacks=[]):\n        # configuration\n        min_lr = 1e-9\n        if max_lr <= min_lr: min_lr = max_lr/10\n\n        #  use learning_rate schedule\n        if cycle_len is not None:\n            if not isinstance(callbacks, list): callbacks = []\n            schedule = SGDRScheduler(min_lr=min_lr,\n                                     max_lr=max_lr,\n                                     steps_per_epoch=steps_per_epoch,\n                                     lr_decay=lr_decay,\n                                     cycle_length=cycle_len,\n                                     mult_factor=cycle_mult)\n            callbacks.append(schedule)\n        if not callbacks: callbacks=None\n        return callbacks\n\n\n    def _cb_checkpoint(self, folder, callbacks=[]):\n        if folder is not None:\n            os.makedirs(folder, exist_ok=True)\n            if not isinstance(callbacks, list): callbacks = []\n            #filepath=os.path.join(folder, ""weights-{epoch:02d}-{val_loss:.2f}.hdf5"")\n            filepath=os.path.join(folder, ""weights-{epoch:02d}.hdf5"")\n            callbacks.append(ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True))\n        if not callbacks: callbacks=None\n        return callbacks\n\n\n    def _cb_earlystopping(self, early_stopping, callbacks=[]):\n        if early_stopping:\n            if not isinstance(callbacks, list): callbacks = []\n            #if StrictVersion(keras.__version__) >= StrictVersion(\'2.2.3\'):\n            try:\n                callbacks.append(EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=early_stopping, \n                                               restore_best_weights=True, verbose=0, mode=\'auto\'))\n            except TypeError:\n                warnings.warn(""""""\n                              The early_stopping=True argument relies on EarlyStopping.restore_best_weights,\n                              which is only supported on Keras 2.2.3 or greater. \n                              For now, we are falling back to EarlyStopping.restore_best_weights=False.\n                              Please use checkpoint_folder option in fit() to restore best weights."""""")\n                callbacks.append(EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=early_stopping, \n                                               verbose=0, mode=\'auto\'))\n\n        if not callbacks: callbacks=None\n        return callbacks\n\n\n    def _prepare(self, data, train=True):\n        """"""\n        Subclasses can override this method if data\n        needs to be specially-prepared prior to invoking fit methods\n        Args:\n          data:  dataset\n          train(bool):  If True, prepare for training. Otherwise, prepare for evaluation.\n        """"""\n        if data is None: return None\n\n        if hasattr(data, \'to_tfdataset\'):\n            return data.to_tfdataset(train=train)\n        else:\n            return data\n\n\n    @abstractmethod\n    def fit(self, lr, n_cycles, cycle_len=None, cycle_mult=1, batch_size=U.DEFAULT_BS):\n        pass\n\n\n    def fit_onecycle(self, lr, epochs, checkpoint_folder=None, \n                     cycle_momentum=True, max_momentum=0.95, min_momentum=0.85,\n                     verbose=1, class_weight=None, callbacks=[]):\n        """"""\n        Train model using a version of Leslie Smith\'s 1cycle policy.\n        This method can be used with any optimizer. Thus,\n        cyclical momentum is not currently implemented.\n\n        Args:\n            lr (float): (maximum) learning rate.  \n                       It is recommended that you estimate lr yourself by \n                       running lr_finder (and lr_plot) and visually inspect plot\n                       for dramatic loss drop.\n            epochs (int): Number of epochs.  Number of epochs\n            checkpoint_folder (string): Folder path in which to save the model weights \n                                        for each epoch.\n                                        File name will be of the form: \n                                        weights-{epoch:02d}-{val_loss:.2f}.hdf5\n            cycle_momentum (bool):    If True and optimizer is Adam, Nadam, or Adamax, momentum of \n                                      optimzer will be cycled between 0.95 and 0.85 as described in \n                                      https://arxiv.org/abs/1803.09820.\n                                      Only takes effect if Adam, Nadam, or Adamax optimizer is used.\n            max_momentum(float): Maximum momentum to use if cycle_momentum=True\n            min_momentum(float): minimum momentum to use if cycle_momentum=True\n            class_weight (dict):       Optional dictionary mapping class indices (integers) to a weight (float) \n            callbacks (list): list of Callback instances to employ during training\n            verbose (bool):  verbose mode\n        """"""\n        if not self._is_adamlike() and cycle_momentum:\n            warnings.warn(\'cyclical momentum has been disabled because \'+\\\n                           \'optimizer is not ""Adam-like"" with beta_1 param\')\n            cycle_momentum=False\n\n\n        num_samples = U.nsamples_from_data(self.train_data)\n        steps_per_epoch = math.ceil(num_samples/self.batch_size)\n\n        # setup callbacks for learning rates and early stopping\n        if not callbacks: kcallbacks = []\n        else:\n            kcallbacks = callbacks[:] \n        if cycle_momentum:\n            max_momentum = max_momentum\n            min_momentum = min_momentum\n        else:\n            max_momentum = None\n            min_momentum = None\n        clr = CyclicLR(base_lr=lr/10, max_lr=lr,\n                       step_size=math.ceil((steps_per_epoch*epochs)/2), \n                       reduce_on_plateau=0,\n                       max_momentum=max_momentum,\n                       min_momentum=min_momentum,\n                       verbose=verbose)\n        kcallbacks.append(clr)\n\n        # start training\n        policy=\'onecycle\'\n        U.vprint(\'\\n\', verbose=verbose)\n        U.vprint(\'begin training using %s policy with max lr of %s...\' % (policy, lr), \n                verbose=verbose)\n        hist = self.fit(lr, epochs, early_stopping=None,\n                        checkpoint_folder=checkpoint_folder,\n                        verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n        hist.history[\'lr\'] = clr.history[\'lr\']\n        hist.history[\'iterations\'] = clr.history[\'iterations\']\n        if cycle_momentum:\n            hist.history[\'momentum\'] = clr.history[\'momentum\']\n        self.history = hist\n        return hist\n\n\n\n    def autofit(self, lr, epochs=None, \n                early_stopping=None, reduce_on_plateau=None, reduce_factor=2, \n                cycle_momentum=True, max_momentum=0.95, min_momentum=0.85,\n                monitor=\'val_loss\', checkpoint_folder=None, verbose=1, \n                class_weight=None, callbacks=[]):\n        """"""\n        Automatically train model using a default learning rate schedule shown to work well\n        in practice.  By default, this method currently employs a triangular learning \n        rate policy (https://arxiv.org/abs/1506.01186).  \n        During each epoch, this learning rate policy varies the learning rate from lr/10 to lr\n        and then back to a low learning rate that is near-zero. \n        If epochs is None, then early_stopping and reduce_on_plateau are atomatically\n        set to 6 and 3, respectively.\n\n        Args:\n            lr (float): optional initial learning rate.  If missing,\n                       lr will be estimated automatically.\n                       It is recommended that you estimate lr yourself by \n                       running lr_finder (and lr_plot) and visually inspect plot\n                       for dramatic loss drop.\n            epochs (int): Number of epochs.  If None, training will continue until\n                          validation loss no longer improves after 5 epochs.\n            early_stopping (int):     If not None, training will automatically stop after this many \n                                      epochs of no improvement in validation loss.\n                                      Upon completion, model will be loaded with weights from epoch\n                                      with lowest validation loss.\n                                      NOTE: If reduce_on_plateau is also enabled, then\n                                      early_stopping must be greater than reduce_on_plateau.\n                                      Example: early_stopping=6, reduce_on_plateau=3.\n            recuce_on_plateau (int):  If not None, will lower learning rate when\n                                      when validation loss fails to improve after\n                                      the specified number of epochs.\n                                      NOTE: If early_stopping is enabled, then\n                                      reduce_on_plateu must be less than early_stopping.\n                                      Example: early_stopping=6, reduce_on_plateau=3.\n            reduce_factor (int):      Learning reate is reduced by this factor on plateau.\n                                      Only takes effect if reduce_on_plateau > 0.\n            cycle_momentum (bool):    If True and optimizer is Adam, Nadam, or Adamax, momentum of \n                                      optimzer will be cycled between 0.95 and 0.85 as described in \n                                      https://arxiv.org/abs/1803.09820.\n                                      Only takes effect if Adam, Nadam, or Adamax optimizer is used.\n            max_momentum(float):  maximum momentum to use when cycle_momentum=True\n            min_momentum(float): minimum momentum to use when cycle_momentum=True\n            checkpoint_folder (string): Folder path in which to save the model weights \n                                        for each epoch.\n                                        File name will be of the form: \n                                        weights-{epoch:02d}-{val_loss:.2f}.hdf5\n            monitor (str):              what metric to monitor for early_stopping\n                                        and reduce_on_plateau (either val_loss or val_accuracy).\n                                        Only used if early_stopping or reduce_on_plateau\n                                        is enabled.\n            class_weight (dict):       Optional dictionary mapping class indices (integers) to a weight (float) \n            callbacks (list): list of Callback instances to employ during training\n            verbose (bool):  verbose mode\n        """"""\n        # check optimizer\n        if not self._is_adamlike() and cycle_momentum:\n            warnings.warn(\'cyclical momentum has been disabled because \'+\\\n                           \'optimizer is not ""Adam-like"" with beta_1 param\')\n            cycle_momentum=False\n\n        # check monitor\n        if monitor not in [VAL_ACC_NAME, \'val_loss\']:\n            raise ValueError(""monitor must be one of {%s, val_loss\'}"" % (VAL_ACC_NAME))\n\n        # setup learning rate policy \n        num_samples = U.nsamples_from_data(self.train_data)\n        steps_per_epoch = math.ceil(num_samples/self.batch_size)\n        step_size = math.ceil(steps_per_epoch/2)\n\n        # handle missing epochs\n        if epochs is None:\n            epochs = 1024\n            if not early_stopping:\n                early_stopping = U.DEFAULT_ES\n                U.vprint(\'early_stopping automatically enabled at patience=%s\' % (U.DEFAULT_ES),\n                        verbose=verbose)\n            if not reduce_on_plateau:\n                reduce_on_plateau = U.DEFAULT_ROP\n                U.vprint(\'reduce_on_plateau automatically enabled at patience=%s\' % (U.DEFAULT_ROP),\n                        verbose=verbose)\n        if reduce_on_plateau and early_stopping and (reduce_on_plateau  > early_stopping):\n            warnings.warn(\'reduce_on_plateau=%s and is greater than \' % (reduce_on_plateau) +\\\n                          \'early_stopping=%s.  \' % (early_stopping)  +\\\n                          \'Either reduce reduce_on_plateau or set early_stopping \' +\\\n                          \'to be higher.\')\n\n        if self.val_data is None and monitor in [\'val_loss\', VAL_ACC_NAME] and\\\n           (reduce_on_plateau is not None or early_stopping is not None):\n            raise Exception(\'cannot monitor %s \' % (monitor)  +\\\n                            \'without validation data - please change monitor\')\n\n\n\n        # setup callbacks for learning rates and early stopping\n        if not callbacks: kcallbacks = []\n        else:\n            kcallbacks = callbacks[:] \n        if cycle_momentum:\n            max_momentum = max_momentum\n            min_momentum = min_momentum\n        else:\n            max_momentum = None\n            min_momentum = None\n\n        clr = CyclicLR(base_lr=lr/10, max_lr=lr,\n                       step_size=step_size, verbose=verbose,\n                       monitor=monitor,\n                       reduce_on_plateau=reduce_on_plateau,\n                       reduce_factor=reduce_factor,\n                       max_momentum=max_momentum,\n                       min_momentum=min_momentum)\n        kcallbacks.append(clr)\n        if early_stopping:\n            kcallbacks.append(EarlyStopping(monitor=monitor, min_delta=0, \n                                           patience=early_stopping,\n                                           restore_best_weights=True, \n                                           verbose=1, mode=\'auto\'))\n\n        # start training\n        U.vprint(\'\\n\', verbose=verbose)\n        policy = \'triangular learning rate\'\n        U.vprint(\'begin training using %s policy with max lr of %s...\' % (policy, lr), \n                verbose=verbose)\n        hist = self.fit(lr, epochs, early_stopping=early_stopping,\n                        checkpoint_folder=checkpoint_folder,\n                        verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n        hist.history[\'lr\'] = clr.history[\'lr\']\n        hist.history[\'iterations\'] = clr.history[\'iterations\']\n        if cycle_momentum:\n            hist.history[\'momentum\'] = clr.history[\'momentum\']\n        self.history = hist\n        return hist\n\n\n    def ground_truth(self, val_data=None):\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if not val: raise Exception(\'val_data must be supplied to get_learner or ground_truth\')\n        return U.y_from_data(val)\n\n\n    def predict(self, val_data=None):\n        """"""\n        Makes predictions on validation set\n        """"""\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if val is None: raise Exception(\'val_data must be supplied to get_learner or predict\')\n        if U.is_iter(val):\n            if hasattr(val, \'reset\'): val.reset()\n            steps = np.ceil(U.nsamples_from_data(val)/val.batch_size)\n            # *_generator methods are deprecated from TF 2.1.0\n            #result = self.model.predict_generator(self._prepare(val, train=False), \n                                                #steps=steps)\n            result = self.model.predict(self._prepare(val, train=False), steps=steps)\n            return result\n        else:\n            return self.model.predict(val[0], batch_size=self.eval_batch_size)\n\n    \n\nclass ArrayLearner(Learner):\n    """"""\n    Main class used to tune and train Keras models\n    using Array data.  An objects of this class should be instantiated\n    via the ktrain.get_learner method instead of directly.\n    Main parameters are:\n\n    \n    model (Model):        A compiled instance of keras.engine.training.Model\n    train_data (ndarray): A tuple of (x_train, y_train), where x_train and \n                          y_train are numpy.ndarrays.\n    val_data (ndarray):   A tuple of (x_test, y_test), where x_test and \n                          y_test are numpy.ndarrays.\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS, \n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, workers=workers, use_multiprocessing=use_multiprocessing, multigpu=multigpu)\n        self.train_data = train_data\n        self.val_data = val_data\n        self.batch_size = batch_size\n        self.eval_batch_size = eval_batch_size\n        return\n\n    \n    def fit(self, lr, n_cycles, cycle_len=None, cycle_mult=1, \n            lr_decay=1, checkpoint_folder = None, early_stopping=None,\n            verbose=1, class_weight=None, callbacks=[]):\n        """"""\n        Trains the model. By default, fit is simply a wrapper for model.fit.\n        When cycle_len parameter is supplied, an SGDR learning rate schedule is used.\n        Trains the model.\n\n        lr (float): learning rate \n        n_cycles (int):  n_cycles\n        cycle_len (int): If not None, decay learning rate over <cycle_len>\n                         epochs until restarting/resetting learning rate to <lr>.\n                         If None, lr remains constant\n        cycle_mult (int): Increase cycle_len by factor of cycle_mult.\n                          This will gradually elongate the cycle.\n                          Has no effect if cycle_len is None.\n        lr_decay(float): rate of decay of learning rate each cycle\n        checkpoint_folder (string): Folder path in which to save the model weights \n                                   for each epoch.\n                                   File name will be of the form: \n                                   weights-{epoch:02d}-{val_loss:.2f}.hdf5\n        early_stopping (int):     If not None, training will automatically stop after this many \n                                  epochs of no improvement in validation loss.\n                                  Upon completion, model will be loaded with weights from epoch\n                                  with lowest validation loss.\n        callbacks (list):         list of Callback instances to employ during training\n        class_weight (dict):       Optional dictionary mapping class indices (integers) to a weight (float) \n        verbose (bool):           whether or not to show progress bar\n        """"""\n\n        # check early_stopping\n        if self.val_data is None and early_stopping is not None:\n            raise ValueError(\'early_stopping monitors val_loss but validation data not set\')\n\n\n        # setup data\n        x_train = self.train_data[0]\n        y_train = self.train_data[1]\n        validation = None\n        if self.val_data:\n            validation = (self.val_data[0], self.val_data[1])\n        # setup learning rate schedule\n        epochs = self._check_cycles(n_cycles, cycle_len, cycle_mult)\n        self.set_lr(lr)\n        kcallbacks = self._cb_sgdr(lr, \n                                  np.ceil(len(x_train)/self.batch_size), \n                                  cycle_len, cycle_mult, lr_decay=lr_decay, callbacks=None)\n        sgdr = kcallbacks[0] if kcallbacks is not None else None\n        kcallbacks = self._cb_checkpoint(checkpoint_folder, callbacks=kcallbacks)\n        kcallbacks = self._cb_earlystopping(early_stopping, callbacks=kcallbacks)\n        if callbacks:\n            if kcallbacks is None: kcallbacks = []\n            kcallbacks.extend(callbacks)\n\n        # train model\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\'ignore\', message=\'.*Check your callbacks.*\')\n            hist = self.model.fit(self._prepare(x_train), \n                                  self._prepare(y_train, train=False),\n                                  batch_size=self.batch_size,\n                                  epochs=epochs,\n                                  validation_data=validation, verbose=verbose, \n                                  shuffle=True,\n                                  class_weight=class_weight,\n                                  callbacks=kcallbacks)\n\n        if sgdr is not None: hist.history[\'lr\'] = sgdr.history[\'lr\']\n        self.history = hist\n\n        if early_stopping:\n            U.vprint(\'Weights from best epoch have been loaded into model.\', verbose=verbose)\n            #loss, acc = self.model.evaluate(self.val_data[0], self.val_data[1])\n            #U.vprint(\'\\n\', verbose=verbose)\n            #U.vprint(\'Early stopping due to no further improvement.\', verbose=verbose)\n            #U.vprint(\'final loss:%s, final score:%s\' % (loss, acc), verbose=verbose)\n\n        return hist\n\n\n    def layer_output(self, layer_id, example_id=0, use_val=False):\n        """"""\n        Prints output of layer with index <layer_id> to help debug models.\n        Uses first example (example_id=0) from training set, by default.\n        """"""\n                                                                                \n        inp = self.model.layers[0].input\n        outp = self.model.layers[layer_id].output\n        f_out = K.function([inp], [outp])\n        if not use_val:\n            example = self.train_data[0][example_id]\n        else:\n            example = self.val_data[0][example_id]\n        layer_out = f_out([np.array([example,])])[0]\n        return layer_out\n\n\n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Typically over-ridden by Learner subclasses.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            obs = val[0][idx]\n            join_char = \' \'\n            if preproc is not None: obs = preproc.undo(obs)\n            if preproc is not None and isinstance(preproc, TextPreprocessor):\n                if preproc.is_nospace_lang(): join_char = \'\'\n            if type(obs) == str:\n                obs = join_char.join(obs.split()[:512])\n            print(\'----------\')\n            print(""id:%s | loss:%s | true:%s | pred:%s)\\n"" % (idx, round(loss,2), truth, pred))\n            print(obs)\n        return\n\n\n\nclass GenLearner(Learner):\n    """"""\n    Main class used to tune and train Keras models\n    using a Keras generator (e.g., DirectoryIterator).\n    Objects of this class should be instantiated using the\n    ktrain.get_learner function, rather than directly.\n\n    Main parameters are:\n\n    model (Model): A compiled instance of keras.engine.training.Model\n    train_data (Iterator): a Iterator instance for training set\n    val_data (Iterator):   A Iterator instance for validation set\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, workers=workers, use_multiprocessing=use_multiprocessing, multigpu=multigpu)\n        self.train_data = train_data\n        self.val_data = val_data\n        self.batch_size = batch_size\n        self.eval_batch_size = eval_batch_size\n        if self.train_data:\n            self.train_data.batch_size = batch_size\n        if self.val_data:\n            self.val_data.batch_size = eval_batch_size\n        return\n\n    \n    def fit(self, lr, n_cycles, cycle_len=None, cycle_mult=1,\n            lr_decay=1.0, checkpoint_folder=None, early_stopping=None, \n            class_weight=None, callbacks=[], verbose=1):\n        """"""\n        Trains the model. By default, fit is simply a wrapper for model.fit (for generators/sequences).\n        When cycle_len parameter is supplied, an SGDR learning rate schedule is used.\n\n        lr (float): learning rate \n        n_cycles (int):  n_cycles\n        cycle_len (int): If not None, decay learning rate over <cycle_len>\n                         epochs until restarting/resetting learning rate to <lr>.\n                         If None, lr remains constant\n        cycle_mult (int): Increase cycle_len by factor of cycle_mult.\n                          This will gradually elongate the cycle.\n                          Has no effect if cycle_len is None.\n        lr_decay (float): rate of decay of learning reach each cycle.\n                          Has no effect if cycle_len is None\n        checkpoint_folder (string): Folder path in which to save the model weights \n                                   for each epoch.\n                                   File name will be of the form: \n                                   weights-{epoch:02d}-{val_loss:.2f}.hdf5\n        early_stopping (int):     If not None, training will automatically stop after this many \n                                  epochs of no improvement in validation loss.\n                                  Upon completion, model will be loaded with weights from epoch\n                                  with lowest validation loss.\n        class_weight (dict):       Optional dictionary mapping class indices (integers) to a weight (float) \n        callbacks (list):         list of Callback instances to employ during training\n        verbose (boolean):       whether or not to print progress bar\n        """"""\n        # check early_stopping\n        if self.val_data is None and early_stopping is not None:\n            raise ValueError(\'early_stopping monitors val_loss but validation data not set\')\n\n        \n        # handle callbacks\n        num_samples = U.nsamples_from_data(self.train_data)\n        train_bs = self.train_data.batch_size if hasattr(self.train_data, \'batch_size\') else self.batch_size\n        steps_per_epoch = math.ceil(num_samples/train_bs)\n        validation_steps = None\n        if self.val_data is not None:\n            val_bs = self.val_data.batch_size if hasattr(self.val_data, \'batch_size\') else self.batch_size\n            validation_steps = math.ceil(U.nsamples_from_data(self.val_data)/val_bs)\n\n        epochs = self._check_cycles(n_cycles, cycle_len, cycle_mult)\n        self.set_lr(lr)\n        kcallbacks = self._cb_sgdr(lr, \n                                  steps_per_epoch,\n                                  cycle_len, cycle_mult, lr_decay, callbacks=None)\n        sgdr = kcallbacks[0] if kcallbacks is not None else None\n        kcallbacks = self._cb_checkpoint(checkpoint_folder, callbacks=kcallbacks)\n        kcallbacks = self._cb_earlystopping(early_stopping, callbacks=kcallbacks)\n        if callbacks:\n            if kcallbacks is None: kcallbacks = []\n            kcallbacks.extend(callbacks)\n            \n        # MNIST times per epoch on Titan V\n        # workers=4, usemp=True 9 sec.\n        # workers=1, usemp=True 12 sec.\n        # workers=1, usemp=False 16 sec.\n        # workers=4, usemp=False 30+ sec.\n        #print(self.workers)\n        #print(self.use_multiprocessing)\n\n        # train model\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\'ignore\', message=\'.*Check your callbacks.*\')\n            fit_fn = self.model.fit\n            hist = fit_fn(self._prepare(self.train_data),\n                                        steps_per_epoch = steps_per_epoch,\n                                        validation_steps = validation_steps,\n                                        epochs=epochs,\n                                        validation_data=self._prepare(self.val_data, train=False),\n                                        workers=self.workers,\n                                        use_multiprocessing=self.use_multiprocessing, \n                                        verbose=verbose,\n                                        shuffle=True,\n                                        class_weight=class_weight,\n                                        callbacks=kcallbacks)\n        if sgdr is not None: hist.history[\'lr\'] = sgdr.history[\'lr\']\n        self.history = hist\n\n        if early_stopping:\n            U.vprint(\'Weights from best epoch have been loaded into model.\', verbose=verbose)\n            #loss, acc = self.model.evaluate_generator(self.val_data)\n            #U.vprint(\'\\n\', verbose=verbose)\n            #U.vprint(\'Early stopping due to no further improvement.\', verbose=verbose)\n            #U.vprint(\'final loss:%s, final score:%s\' % (loss, acc), verbose=verbose)\n        return hist\n\n\n    def layer_output(self, layer_id, example_id=0, batch_id=0, use_val=False):\n        """"""\n        Prints output of layer with index <layer_id> to help debug models.\n        Uses first example (example_id=0) from training set, by default.\n        """"""\n                                                                                \n        inp = self.model.layers[0].input\n        outp = self.model.layers[layer_id].output\n        f_out = K.function([inp], [outp])\n        if not use_val:\n            example = self.train_data[0][batch_id][example_id]\n        else:\n            example = self.val_data[0][batch_id][example_id]\n        layer_out = f_out([np.array([example,])])[0]\n        return layer_out\n\n\n    #def view_top_losses(self, n=4, preproc=None, val_data=None):\n    #    """"""\n    #    Views observations with top losses in validation set.\n    #    Musta be overridden by Learner subclasses.\n    #    """"""\n    #    raise NotImplementedError(\'view_top_losses must be overriden by GenLearner subclass\')\n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Typically over-ridden by Learner subclasses.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            print(\'----------\')\n            print(""id:%s | loss:%s | true:%s | pred:%s)\\n"" % (idx, round(loss,2), truth, pred))\n        return\n\n\n\n#------------------------------------------------------------------------------\n# Predictor functions\n#------------------------------------------------------------------------------\n\ndef get_predictor(model, preproc, batch_size=U.DEFAULT_BS):\n    """"""\n    Returns a Predictor instance that can be used to make predictions on\n    unlabeled examples.  Can be saved to disk and reloaded as part of a \n    larger application.\n\n    Args\n        model (Model):        A compiled instance of keras.engine.training.Model\n        preproc(Preprocessor):   An instance of TextPreprocessor,ImagePreprocessor,\n                                 or NERPreprocessor.\n                                 These instances are returned from the data loading\n                                 functions in the ktrain vision and text modules:\n\n                                 ktrain.vision.images_from_folder\n                                 ktrain.vision.images_from_csv\n                                 ktrain.vision.images_from_array\n                                 ktrain.text.texts_from_folder\n                                 ktrain.text.texts_from_csv\n                                 ktrain.text.ner.entities_from_csv\n        batch_size(int):    batch size to use.  default:32\n    """"""\n\n    # check arguments\n    if not isinstance(model, Model):\n        raise ValueError(\'model must be of instance Model\')\n    if not isinstance(preproc, (ImagePreprocessor,TextPreprocessor, NERPreprocessor, NodePreprocessor, LinkPreprocessor)):\n        raise ValueError(\'preproc must be instance of ktrain.preprocessor.Preprocessor\')\n    if isinstance(preproc, ImagePreprocessor):\n        return ImagePredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, TextPreprocessor):\n    #elif type(preproc).__name__ == \'TextPreprocessor\':\n        return TextPredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, NERPreprocessor):\n        return NERPredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, NodePreprocessor):\n        return NodePredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, LinkPreprocessor):\n        return LinkPredictor(model, preproc, batch_size=batch_size)\n    else:\n        raise Exception(\'preproc of type %s not currently supported\' % (type(preproc)))\n\n\ndef load_predictor(fpath, batch_size=U.DEFAULT_BS):\n    """"""\n    Loads a previously saved Predictor instance\n    Args\n      fpath(str): predictor path name (value supplied to predictor.save)\n                  From v0.16.x, this is always the path to a folder.\n                  Pre-v0.16.x, this is the base name used to save model and .preproc instance.\n      batch_size(int): batch size to use for predictions. default:32\n    """"""\n\n    # load the preprocessor\n    preproc = None\n    try:\n        preproc_name = os.path.join(fpath, U.PREPROC_NAME)\n        with open(preproc_name, \'rb\') as f: preproc = pickle.load(f)\n    except:\n        try:\n            preproc_name = fpath +\'.preproc\'\n            #warnings.warn(\'could not load .preproc file as %s - attempting to load as %s\' % (os.path.join(fpath, U.PREPROC_NAME), preproc_name))\n            with open(preproc_name, \'rb\') as f: preproc = pickle.load(f)\n        except:\n            raise Exception(\'Could not find a .preproc file in either the post v0.16.x loction (%s) or pre v0.16.x location (%s)\' % (os.path.join(fpath. U.PREPROC_NAME), fpath+\'.preproc\'))\n\n    # load the model\n    model = _load_model(fpath, preproc=preproc)\n\n\n    # preprocessing functions in ImageDataGenerators are not pickable\n    # so, we must reconstruct\n    if hasattr(preproc, \'datagen\') and hasattr(preproc.datagen, \'ktrain_preproc\'):\n        preproc_name = preproc.datagen.ktrain_preproc\n        if preproc_name == \'resnet50\':\n            preproc.datagen.preprocessing_function = pre_resnet50\n        elif preproc_name == \'mobilenet\':\n            preproc.datagen.preprocessing_function = pre_mobilenet\n        elif preproc_name == \'inception\':\n            preproc.datagen.preprocessing_function = pre_incpeption\n        else:\n            raise Exception(\'Uknown preprocessing_function name: %s\' % (preproc_name))\n    \n    # return the appropriate predictor\n    if not isinstance(model, Model):\n        raise ValueError(\'model must be of instance Model\')\n    if not isinstance(preproc, (ImagePreprocessor, TextPreprocessor, NERPreprocessor, NodePreprocessor, LinkPreprocessor)):\n        raise ValueError(\'preproc must be instance of ktrain.preprocessor.Preprocessor\')\n    if isinstance(preproc, ImagePreprocessor):\n        return ImagePredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, TextPreprocessor):\n        return TextPredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, NERPreprocessor):\n        return NERPredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, NodePreprocessor):\n        return NodePredictor(model, preproc, batch_size=batch_size)\n    elif isinstance(preproc, LinkPreprocessor):\n        return LinkPredictor(model, preproc, batch_size=batch_size)\n    else:\n        raise Exception(\'preprocessor not currently supported\')\n\n\n\n\n\n#----------------------------------------\n# Utility Functions\n#----------------------------------------\n\n\n\n\ndef release_gpu_memory(device=0):\n    """"""\n    Relase GPU memory allocated by Tensorflow\n    Source: \n    https://stackoverflow.com/questions/51005147/keras-release-memory-after-finish-training-process\n    """"""\n    from numba import cuda\n    K.clear_session()\n    cuda.select_device(device)\n    cuda.close()\n    return\n\n\ndef _load_model(fpath, preproc=None, train_data=None, custom_objects=None):\n    if not preproc and not train_data:\n        raise ValueError(\'Either preproc or train_data is required.\')\n    if preproc and isinstance(preproc, TransformersPreprocessor):\n        # note: with transformer models, fname is actually a directory\n        model = preproc.get_model(fpath=fpath)\n        return model\n    elif (preproc and (isinstance(preproc, BERTPreprocessor) or \\\n                    type(preproc).__name__ == \'BERTPreprocessor\')) or\\\n       train_data and U.bert_data_tuple(train_data):\n        # custom BERT model\n        from keras_bert import get_custom_objects\n        custom_objects = get_custom_objects()\n    elif (preproc and (isinstance(preproc, NERPreprocessor) or \\\n                    type(preproc).__name__ == \'NERPreprocessor\')) or \\\n        train_data and U.is_ner(data=train_data):\n        from .text.ner.anago.layers import CRF\n        from .text.ner import crf_loss\n        custom_objects={\'CRF\': CRF, \'crf_loss\':crf_loss}\n    elif (preproc and (isinstance(preproc, NodePreprocessor) or \\\n                    type(preproc).__name__ == \'NodePreprocessor\')) or \\\n        train_data and U.is_nodeclass(data=train_data):\n        from stellargraph.layer import MeanAggregator\n        custom_objects={\'MeanAggregator\': MeanAggregator}\n    elif (preproc and (isinstance(preproc, LinkPreprocessor) or \\\n                    type(preproc).__name__ == \'LinkPreprocessor\')) or \\\n        train_data and U.is_linkpred(data=train_data):\n        from stellargraph.layer import MeanAggregator\n        custom_objects={\'MeanAggregator\': MeanAggregator}\n    custom_objects = {} if custom_objects is None else custom_objects\n    custom_objects[\'AdamWeightDecay\'] = AdamWeightDecay\n    try:\n        try:\n            model = load_model(os.path.join(fpath, U.MODEL_NAME), custom_objects=custom_objects)\n        except:\n            try:\n                # pre-0.16: model fpath was file name of model not folder for non-Transformer models\n                #warnings.warn(\'could not load model as %s - attempting to load model as %s\' % (os.path.join(fpath, U.MODEL_NAME), fpath))\n                model = load_model(fpath, custom_objects=custom_objects)\n            except:\n                # for bilstm models without CRF layer on TF2 where CRF is not supported \n                model = load_model(fpath, custom_objects={\'AdamWeightDecay\':AdamWeightDecay})\n    except Exception as e:\n        print(\'Call to keras.models.load_model failed.  \'\n              \'Try using the learner.model.save_weights and \'\n              \'learner.model.load_weights instead.\')\n        print(\'Error was: %s\' % (e))\n        return\n\n    # see issue https://github.com/amaiya/ktrain/issues/21\n    if hasattr(model, \'_make_predict_function\'):\n        model._make_predict_function()\n\n    return model\n\n\n\n'"
ktrain/data.py,10,"b'from .imports import *\n\n\nclass Dataset:\n    """"""\n    Base class for custom datasets in ktrain.\n\n    If subclass of Dataset implements a method to to_tfdataset\n    that converts the data to a tf.Dataset, then this will be\n    invoked by Learner instances just prior to training so\n    fit() will train using a tf.Dataset representation of your data.\n    Sequence methods such as __get_item__ and __len__\n    must still be implemented.\n\n    The signature of to_tfdataset is as follows:\n\n    def to_tfdataset(self, train=True)\n\n    See ktrain.text.preprocess.TransformerDataset as an example.\n    """"""\n\n    # required: used by ktrain.core.Learner instances\n    def nsamples(self):\n        raise NotImplemented\n\n    # required: used by ktrain.core.Learner instances\n    def get_y(self):\n        raise NotImplemented\n\n    # optional: to modify dataset between epochs (e.g., shuffle)\n    def on_epoch_end(self):\n        pass\n\n    # optional\n    def ondisk(self):\n        """"""\n        Is data being read from disk like with DirectoryIterators?\n        """"""\n        return False\n\n    # optional: used only if invoking *_classifier functions\n    def xshape(self):\n        """"""\n        shape of X\n        Examples:\n            for images: input_shape\n            for text: (n_example, sequence_length)\n        """"""\n        raise NotImplemented\n    \n    # optional: used only if invoking *_classifier functions\n    def nclasses(self):\n        """"""\n        Number of classes\n        For classification problems: this is the number of labels\n        Not used for regression problems\n        """"""\n        raise NotImplemented\n\n\nclass TFDataset(Dataset):\n    """"""\n    Wrapper for tf.data.Datasets\n    """"""\n    def __init__(self, tfdataset, n, y):\n        """"""\n        Args:\n          tfdataset(tf.data.Dataset):  a tf.Dataset instance\n          n(int): number of examples in dataset (cardinality, which can\'t reliably be extracted from tf.data.Datasets)\n          y(np.ndarray): y values for each example - should be in the format expected by your moddel (e.g., 1-hot-encoded)\n        """"""\n        if not isinstance(tfdataset, tf.data.Dataset):\n            raise ValueError(\'tfdataset must be a fully-configured tf.data.Dataset with batch_size, etc. set appropriately\')\n        self.tfdataset = tfdataset\n        self.bs = next(tfdataset.as_numpy_iterator())[-1].shape[0] # extract batch_size from tfdataset\n        self.n = n\n        self.y = y\n\n    @property\n    def batch_size(self):\n        return self.bs\n\n    @batch_size.setter\n    def batch_size(self, value):\n        if value != self.bs:\n            warnings.warn(\'batch_size parameter is ignored, as pre-configured batch_size of tf.data.Dataset is used\')\n\n\n    def nsamples(self):\n        return self.n\n\n    def get_y(self):\n        return self.y\n\n    def to_tfdataset(self, train=True):\n        return self.tfdataset\n\n\nclass SequenceDataset(Dataset, Sequence):\n    """"""\n    Base class for custom datasets in ktrain.\n\n    If subclass of Dataset implements a method to to_tfdataset\n    that converts the data to a tf.Dataset, then this will be\n    invoked by Learner instances just prior to training so\n    fit() will train using a tf.Dataset representation of your data.\n    Sequence methods such as __get_item__ and __len__\n    must still be implemented.\n\n    The signature of to_tfdataset is as follows:\n\n    def to_tfdataset(self, training=True)\n\n    See ktrain.text.preprocess.TransformerDataset as an example.\n    """"""\n    def __init__(self, batch_size=32):\n        self.batch_size = batch_size\n\n    # required by keras.utils.Sequence instances\n    def __len__(self):\n        raise NotImplemented\n\n    # required by keras.utils.Sequence instances\n    def __getitem__(self, idx):\n        raise NotImplemented\n\n        return False\n\n\n\n\nclass MultiArrayDataset(SequenceDataset):\n    def __init__(self, x, y, batch_size=32, shuffle=True):\n        # error checks\n        err = False\n        if type(x) == np.ndarray and len(x.shape) != 2: err = True\n        elif type(x) == list:\n            for d in x:\n                if type(d) != np.ndarray or len(d.shape) != 2:\n                    err = True\n                    break\n        else: err = True\n        if err:\n            raise ValueError(\'x must be a 2d numpy array or a list of 2d numpy arrays\')\n        if type(y) != np.ndarray:\n            raise ValueError(\'y must be a numpy array\')\n        if type(x) == np.ndarray:\n            x = [x]\n\n        # set variables\n        super().__init__(batch_size=batch_size)\n        self.x, self.y = x, y\n        self.indices = np.arange(self.x[0].shape[0])\n        self.n_inputs = len(x)\n        self.shuffle = shuffle\n\n\n    def __len__(self):\n        return math.ceil(self.x[0].shape[0] / self.batch_size)\n\n    def __getitem__(self, idx):\n        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = []\n        for i in range(self.n_inputs):\n            batch_x.append(self.x[i][inds])\n        batch_y = self.y[inds]\n        return tuple(batch_x), batch_y\n\n    def nsamples(self):\n        return self.x[0].shape[0]\n\n    def get_y(self):\n        return self.y\n\n    def on_epoch_end(self):\n        if self.shuffle: np.random.shuffle(self.indices)\n\n    def xshape(self):\n        return self.x[0].shape\n\n    def nclasses(self):\n        return self.y.shape[1]\n\n    def ondisk(self):\n        return False\n\n'"
ktrain/imports.py,6,"b'\n#--------------------------\n# Tensorflow Keras imports\n#--------------------------\n\nimport os\nimport warnings\nimport logging\nfrom distutils.util import strtobool\nfrom packaging import version\n\n\n# suppress warnings\nSUPPRESS_TF_WARNINGS = strtobool(os.environ.get(\'SUPPRESS_TF_WARNINGS\', \'1\'))\nif SUPPRESS_TF_WARNINGS:\n    os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n    logging.getLogger(\'tensorflow\').setLevel(logging.ERROR)\n    warnings.simplefilter(action=\'ignore\', category=FutureWarning)\n    # elevate warnings to errors for debugging dependencies\n    #warnings.simplefilter(\'error\', FutureWarning)\n\n\n\n# TF1\n#import tensorflow as tf\n#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n#from tensorflow import keras\n\nDISABLE_V2_BEHAVIOR = strtobool(os.environ.get(\'DISABLE_V2_BEHAVIOR\', \'0\'))\nif DISABLE_V2_BEHAVIOR:\n    # TF2-transition\n    ACC_NAME = \'acc\'\n    VAL_ACC_NAME = \'val_acc\'\n    import tensorflow.compat.v1 as tf\n    tf.disable_v2_behavior()\n    from tensorflow.compat.v1 import keras\n    print(\'Using DISABLE_V2_BEHAVIOR with TensorFlow\')\nelse:\n    # TF2\n    ACC_NAME = \'accuracy\'\n    VAL_ACC_NAME = \'val_accuracy\'\n    import tensorflow as tf\n    from tensorflow import keras\n\n# suppress autograph warnings\ntf.autograph.set_verbosity(1)\n#if SUPPRESS_TF_WARNINGS:\n    #tf.autograph.set_verbosity(1)\n\nif version.parse(tf.__version__) < version.parse(\'2.0\'):\n    raise Exception(\'As of v0.8.x, ktrain needs TensorFlow 2. Please upgrade TensorFlow.\')\n\nos.environ[\'TF_KERAS\'] = \'1\' # to use keras_bert package below with tf.Keras\n\n\n\n\n\n# output Keras version\n#print(""using Keras version: %s"" % (keras.__version__))\n\nK = keras.backend\nLayer = keras.layers.Layer\nInputSpec = keras.layers.InputSpec\nModel = keras.Model\nmodel_from_json = keras.models.model_from_json\nload_model = keras.models.load_model\nSequential = keras.models.Sequential\nModelCheckpoint = keras.callbacks.ModelCheckpoint\nEarlyStopping = keras.callbacks.EarlyStopping\nLambdaCallback = keras.callbacks.LambdaCallback\nCallback = keras.callbacks.Callback\nDense = keras.layers.Dense\nEmbedding = keras.layers.Embedding\nInput = keras.layers.Input\nFlatten = keras.layers.Flatten\nGRU = keras.layers.GRU\nBidirectional = keras.layers.Bidirectional\nLSTM = keras.layers.LSTM\nLeakyReLU = keras.layers.LeakyReLU # SG\nMultiply = keras.layers.Multiply   # SG\nAverage = keras.layers.Average     # SG\nReshape = keras.layers.Reshape     #SG\nSpatialDropout1D = keras.layers.SpatialDropout1D\nGlobalMaxPool1D = keras.layers.GlobalMaxPool1D\nGlobalAveragePooling1D = keras.layers.GlobalAveragePooling1D\nconcatenate = keras.layers.concatenate\ndot = keras.layers.dot\nDropout = keras.layers.Dropout\nBatchNormalization = keras.layers.BatchNormalization\nAdd = keras.layers.Add\nConvolution2D = keras.layers.Convolution2D\nMaxPooling2D = keras.layers.MaxPooling2D\nAveragePooling2D = keras.layers.AveragePooling2D\nConv2D = keras.layers.Conv2D\nMaxPooling2D = keras.layers.MaxPooling2D\nTimeDistributed = keras.layers.TimeDistributed\nLambda = keras.layers.Lambda\nActivation = keras.layers.Activation\nadd = keras.layers.add\nConcatenate = keras.layers.Concatenate\ninitializers = keras.initializers\nglorot_uniform = keras.initializers.glorot_uniform\nregularizers = keras.regularizers\nl2 = keras.regularizers.l2\nconstraints = keras.constraints\nsequence = keras.preprocessing.sequence\nimage = keras.preprocessing.image\nNumpyArrayIterator = keras.preprocessing.image.NumpyArrayIterator\nIterator = keras.preprocessing.image.Iterator\nImageDataGenerator = keras.preprocessing.image.ImageDataGenerator\nTokenizer = keras.preprocessing.text.Tokenizer\nSequence = keras.utils.Sequence\nget_file = keras.utils.get_file\nplot_model = keras.utils.plot_model\nto_categorical = keras.utils.to_categorical\nmulti_gpu_model = keras.utils.multi_gpu_model\nactivations = keras.activations\nsigmoid = keras.activations.sigmoid\ncategorical_crossentropy = keras.losses.categorical_crossentropy\nsparse_categorical_crossentropy = keras.losses.sparse_categorical_crossentropy\nResNet50 = keras.applications.ResNet50\nMobileNet = keras.applications.mobilenet.MobileNet\nInceptionV3 = keras.applications.inception_v3.InceptionV3\npre_resnet50 = keras.applications.resnet50.preprocess_input\npre_mobilenet = keras.applications.mobilenet.preprocess_input\npre_inception = keras.applications.inception_v3.preprocess_input\n\n\n#----------------------------------------------------------\n# standards\n#----------------------------------------------------------\n\n#import warnings # imported above\nimport sys\nimport os\nimport os.path\nimport re\nimport operator\nfrom collections import Counter\nfrom distutils.version import StrictVersion\nimport tempfile\nimport pickle\nfrom abc import ABC, abstractmethod\nimport math\nimport itertools\nimport csv\nimport copy\nimport glob\nimport codecs\nimport urllib.request\nimport zipfile\nimport gzip\nimport shutil\nimport string\nimport random\nimport json\nimport mimetypes\n\n\n\n\n\n\n\n\n\n#----------------------------------------------------------\n# external dependencies\n#----------------------------------------------------------\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import rgb2hex\nplt.ion() # interactive mode\nimport sklearn\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n#from sklearn.externals import joblib\nimport joblib\nfrom scipy import sparse # utils\nfrom scipy.sparse import csr_matrix\nimport pandas as pd\ntry:\n    # fastprogress >= v0.2.0\n    from fastprogress.fastprogress import master_bar, progress_bar \nexcept:\n    # fastprogress < v0.2.0\n    from fastprogress import master_bar, progress_bar \nimport keras_bert\nfrom keras_bert import Tokenizer as BERT_Tokenizer\nimport requests\n# verify=False added to avoid headaches from some corporate networks\nfrom requests.packages.urllib3.exceptions import InsecureRequestWarning\nrequests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n\n# multilingual\nimport langdetect\nimport jieba\nimport cchardet as chardet\n\n# graphs\nimport networkx as nx\n#from sklearn import preprocessing, feature_extraction, model_selection\n\n# ner\nfrom seqeval.metrics import classification_report as ner_classification_report\nfrom seqeval.metrics import f1_score as ner_f1_score\nfrom seqeval.metrics import accuracy_score as ner_accuracy_score\nfrom seqeval.metrics.sequence_labeling import get_entities\nimport syntok.segmenter as segmenter\n\n\n# transformers\ntry:\n    logging.getLogger(\'transformers\').setLevel(logging.CRITICAL)\nexcept: pass\nimport transformers\n\n\ntry:\n    from PIL import Image\n    PIL_INSTALLED = True\nexcept:\n    PIL_INSTALLED = False\n\nSG_ERRMSG = \'ktrain currently uses a forked version of stellargraph v0.8.2. \'+\\\n            \'Please install with: \'+\\\n            \'pip3 install git+https://github.com/amaiya/stellargraph@no_tf_dep_082\'\n\nALLENNLP_ERRMSG  = \'To use ELMo embedings, please install allenlp:\\n\' +\\\n                   \'pip3 install allennlp\'\n\n\n\n'"
ktrain/predictor.py,0,"b'from .imports import *\nfrom . import utils as U\nclass Predictor(ABC):\n    """"""\n    Abstract class to preprocess data\n    """"""\n    @abstractmethod\n    def predict(self, data):\n        pass\n\n    @abstractmethod\n    def get_classes(self, filename):\n        pass\n\n    def explain(self, x):\n        raise NotImplementedError(\'explain is not currently supported for this model\')\n\n\n    def _make_predictor_folder(self, fpath):\n        if os.path.isfile(fpath):\n            raise ValueError(f\'There is an existing file named {fpath}. \' +\\\n                              \'Please use dfferent value for fpath.\')\n        elif os.path.exists(fpath):\n            #warnings.warn(\'predictor files are being saved to folder that already exists: %s\' % (fpath))\n            pass\n        elif not os.path.exists(fpath):\n            os.makedirs(fpath)\n        return\n\n\n    def _save_preproc(self, fpath):\n        with open(os.path.join(fpath, U.PREPROC_NAME), \'wb\') as f:\n            pickle.dump(self.preproc, f)\n        return\n\n\n    def _save_model(self, fpath):\n        if U.is_crf(self.model): # TODO: fix/refactor this\n            from .text.ner import crf_loss\n            self.model.compile(loss=crf_loss, optimizer=U.DEFAULT_OPT)\n        model_path = os.path.join(fpath, U.MODEL_NAME)\n        self.model.save(model_path, save_format=\'h5\')\n        return\n\n\n\n    def save(self, fpath):\n        """"""\n        saves both model and Preprocessor instance associated with Predictor \n        Args:\n          fpath(str): path to folder to store model and Preprocessor instance (.preproc file)\n        Returns:\n          None\n        """"""\n        self._make_predictor_folder(fpath)\n        self._save_model(fpath)\n        self._save_preproc(fpath)\n        return\n\n'"
ktrain/preprocessor.py,0,"b'from .imports import *\nclass Preprocessor(ABC):\n    """"""\n    Abstract class to preprocess data\n    """"""\n    @abstractmethod\n    def get_preprocessor(self):\n        pass\n    @abstractmethod\n    def get_classes(self):\n        pass\n    @abstractmethod\n    def preprocess(self):\n        pass\n\n    def undo(self, data_instance):\n        return data_instance\n\n\n\n\n'"
ktrain/utils.py,2,"b'from .imports import *\nfrom .data import Dataset\n\n\n#------------------------------------------------------------------------------\n# KTRAIN DEFAULTS\n#------------------------------------------------------------------------------\nDEFAULT_WD = 0.01\ndef get_default_optimizer(lr=0.001, wd=DEFAULT_WD):\n    from .lroptimize.optimization import AdamWeightDecay\n    opt = AdamWeightDecay(learning_rate=lr, \n                         weight_decay_rate=wd, \n                         beta_1=0.9,\n                         beta_2=0.999,\n                         epsilon=1e-6,\n                         exclude_from_weight_decay=[\'layer_norm\', \'bias\'])\n    return opt\n# Use vanilla Adam as default unless weight decay is explicitly set by user\n# in which case AdamWeightDecay is default optimizer.\n# See core.Learner.set_weight_decay for more information\nDEFAULT_OPT = \'adam\' \nDEFAULT_BS = 32\nDEFAULT_ES = 5 \nDEFAULT_ROP = 2 \n#from .lroptimize.optimization import AdamWeightDecay\n#DEFAULT_OPT = AdamWeightDecay(learning_rate=0.001, \n                              #weight_decay_rate=0.01, \n                              #beta_1=0.9,\n                              #beta_2=0.999,\n                              #epsilon=1e-6,\n                              #exclude_from_weight_decay=[\'layer_norm\', \'bias\'])\nDEFAULT_TRANSFORMER_LAYERS = [-2] # second-to-last hidden state\nDEFAULT_TRANSFORMER_MAXLEN = 512\nDEFAULT_TRANSFORMER_NUM_SPECIAL = 2\nMODEL_BASENAME = \'tf_model\'\nMODEL_NAME = MODEL_BASENAME+\'.h5\'\nPREPROC_NAME = MODEL_BASENAME+\'.preproc\'\n\n\n\n#------------------------------------------------------------------------------\n# DATA/MODEL INSPECTORS\n#------------------------------------------------------------------------------\n\ndef loss_fn_from_model(model):\n    if version.parse(tf.__version__) < version.parse(\'2.2\'):\n        return model.loss_functions[0].fn\n    else: # TF 2.2.0\n        return model.compiled_loss._get_loss_object(model.compiled_loss._losses[0].name).fn\n\n\ndef is_classifier(model):\n    """"""\n    checks for classification and mutlilabel from model\n    """"""\n    is_classifier = False\n    is_multilabel = False\n\n    # get loss name\n    loss = model.loss\n    if callable(loss): \n        if hasattr(loss, \'__name__\'):\n            loss = loss.__name__\n        elif hasattr(loss, \'name\'):\n            loss = loss.name\n        else:\n            raise Exception(\'could not get loss name\')\n\n    # check for classification\n    if loss in [\'categorical_crossentropy\',\n                 \'sparse_categorical_crossentropy\',\n                 \'binary_crossentropy\']:\n        is_classifier = True\n\n    # check for multilabel\n    if loss == \'binary_crossentropy\':\n        if is_huggingface(model=model):\n            is_multilabel = True\n        else:\n            last = model.layers[-1]\n            output_shape = last.output_shape\n            mult_output = True if len(output_shape) ==2 and output_shape[1] >  1 else False\n            if ( (hasattr(last, \'activation\') and isinstance(last.activation, type(sigmoid))) or\\\n               isinstance(last, type(sigmoid)) ) and mult_output:\n                is_multilabel = True\n    return (is_classifier, is_multilabel)\n\n\ndef is_huggingface(model=None, data=None):\n    """"""\n    check for hugging face transformer model\n    from  model and/or data\n    """"""\n    huggingface = False\n    if model is not None and is_huggingface_from_model(model):\n        huggingface = True\n    elif data is not None and is_huggingface_from_data(data):\n        huggingface = True\n    return huggingface\n\n\ndef is_huggingface_from_model(model):\n    return \'transformers.modeling_tf\' in str(type(model))\n\n\ndef is_huggingface_from_data(data):\n    return type(data).__name__ in [\'TransformerDataset\']\n\n\n\ndef is_ner(model=None, data=None):\n    ner = False\n    if data is None:\n        warnings.warn(\'is_ner only detects CRF-based NER models when data is None\')\n    if model is not None and is_crf(model):\n        ner = True\n    elif data is not None and is_ner_from_data(data):\n        ner = True\n    return ner \n\n\ndef is_crf(model):\n    """"""\n    checks for CRF sequence tagger.\n    """"""\n    #loss = model.loss\n    #if callable(loss): \n        #if hasattr(loss, \'__name__\'):\n            #loss = loss.__name__\n        #elif hasattr(loss, \'name\'):\n            #loss = loss.name\n        #else:\n            #raise Exception(\'could not get loss name\')\n    #return loss == \'crf_loss\' or \'CRF.loss_function\' in str(model.loss)\n    return type(model.layers[-1]).__name__ == \'CRF\'\n\n\n#def is_ner_from_model(model):\n    #""""""\n    #checks for sequence tagger.\n    #Curently, only checks for a CRF-based sequence tagger\n    #""""""\n    #loss = model.loss\n    #if callable(loss): \n        #if hasattr(loss, \'__name__\'):\n            #loss = loss.__name__\n        #elif hasattr(loss, \'name\'):\n            #loss = loss.name\n        #else:\n            #raise Exception(\'could not get loss name\')\n\n    #return loss == \'crf_loss\' or \'CRF.loss_function\' in str(model.loss)\n\n\ndef is_ner_from_data(data):\n    return type(data).__name__ == \'NERSequence\'\n\n\ndef is_nodeclass(model=None, data=None):\n    result = False\n    if data is not None and type(data).__name__ == \'NodeSequenceWrapper\':\n        result = True\n    return result\n\ndef is_linkpred(model=None, data=None):\n    result = False\n    if data is not None and type(data).__name__ == \'LinkSequenceWrapper\':\n        result = True\n    return result\n\n\ndef is_imageclass_from_data(data):\n    return type(data).__name__ in [\'DirectoryIterator\', \'DataFrameIterator\', \'NumpyArrayIterator\']\n\n\ndef is_multilabel(data):\n    """"""\n    checks for multilabel from data\n    """"""\n    data_arg_check(val_data=data, val_required=True)\n    if is_ner(data=data): return False   # NERSequence\n    elif is_nodeclass(data=data): return False  # NodeSequenceWrapper\n    multilabel = False\n    Y = y_from_data(data)\n    if len(Y.shape) == 1 or (len(Y.shape) > 1 and Y.shape[1] == 1): return False\n    for idx, y in enumerate(Y):\n        if idx >= 1024: break\n        if np.issubdtype(type(y), np.integer) or np.issubdtype(type(y), np.floating):\n            return False\n        total_for_example = sum(y)\n        if total_for_example > 1:\n            multilabel=True\n            break\n    return multilabel\n\n\ndef shape_from_data(data):\n    err_msg = \'could not determine shape from %s\' % (type(data))\n    if is_iter(data):\n        if isinstance(data, Dataset): return data.xshape()\n        elif hasattr(data, \'image_shape\'): return data.image_shape          # DirectoryIterator/DataFrameIterator\n        elif hasattr(data, \'x\'):                                            # NumpyIterator\n            return data.x.shape[1:]\n        else:\n            try:\n                return data[0][0].shape[1:]\n            except:\n                raise Exception(err_msg)\n    else:\n        try:\n            if type(data[0]) == list: # BERT-style tuple\n                return data[0][0].shape\n            else:\n                return data[0].shape  # standard tuple\n        except:\n            raise Exception(err_msg)\n\n\ndef ondisk(data):\n    if hasattr(data, \'ondisk\'): return data.ondisk()\n\n    ondisk = is_iter(data) and \\\n             (type(data).__name__ not in  [\'NumpyArrayIterator\'])\n    return ondisk\n\n\ndef nsamples_from_data(data):\n    err_msg = \'could not determine number of samples from %s\' % (type(data))\n    if is_iter(data):\n        if isinstance(data, Dataset): return data.nsamples()\n        elif hasattr(data, \'samples\'):  # DirectoryIterator/DataFrameIterator\n            return data.samples\n        elif hasattr(data, \'n\'):     # DirectoryIterator/DataFrameIterator/NumpyIterator\n            return data.n\n        else:\n            raise Exception(err_msg)\n    else:\n        try:\n            if type(data[0]) == list: # BERT-style tuple\n                return len(data[0][0])\n            else:\n                return len(data[0])   # standard tuple\n        except:\n            raise Exception(err_msg)\n\n\ndef nclasses_from_data(data):\n    if is_iter(data):\n        if isinstance(data, Dataset): return data.nclasses()\n        elif hasattr(data, \'classes\'):   # DirectoryIterator\n            return len(set(data.classes))\n        else:\n            try:\n                return data[0][1].shape[1]  # DataFrameIterator/NumpyIterator\n            except:\n                raise Exception(\'could not determine number of classes from %s\' % (type(data)))\n    else:\n        try:\n            return data[1].shape[1]\n        except:\n                raise Exception(\'could not determine number of classes from %s\' % (type(data)))\n\n\ndef y_from_data(data):\n    if is_iter(data):\n        if isinstance(data, Dataset): return data.get_y()\n        elif hasattr(data, \'classes\'): # DirectoryIterator\n            return to_categorical(data.classes)\n        elif hasattr(data, \'labels\'):  # DataFrameIterator\n            return data.labels\n        elif hasattr(data, \'y\'): # NumpyArrayIterator\n            #return to_categorical(data.y)\n            return data.y\n        else:\n            raise Exception(\'could not determine number of classes from %s\' % (type(data)))\n    else:\n        try:\n            return data[1]\n        except:\n            raise Exception(\'could not determine number of classes from %s\' % (type(data)))\n\n\ndef is_iter(data, ignore=False):\n    if ignore: return True\n    iter_classes = [""NumpyArrayIterator"", ""DirectoryIterator"", ""DataFrameIterator""]\n    return data.__class__.__name__ in iter_classes or isinstance(data, Dataset)\n\n\n\ndef data_arg_check(train_data=None, val_data=None, train_required=False, val_required=False,\n                   ndarray_only=False):\n    if train_required and train_data is None:\n        raise ValueError(\'train_data is required\')\n    if val_required and val_data is None:\n        raise ValueError(\'val_data is required\')\n    if train_data is not None and not is_iter(train_data, ndarray_only):\n        if bad_data_tuple(train_data):\n            err_msg = \'data must be tuple of numpy.ndarrays\'\n            if not ndarray_only: err_msg += \' or an instance of ktrain.Dataset\'\n            raise ValueError(err_msg)\n    if val_data is not None and not is_iter(val_data, ndarray_only):\n        if bad_data_tuple(val_data):\n            err_msg = \'data must be tuple of numpy.ndarrays or BERT-style tuple\'\n            if not ndarray_only: err_msg += \' or an instance of Iterator\'\n            raise ValueError(err_msg)\n    return\n\n\ndef bert_data_tuple(data):\n    """"""\n    checks if data tuple is BERT-style format\n    """"""\n    if is_iter(data): return False\n    if type(data[0]) == list and len(data[0]) == 2 and \\\n       type(data[0][0]) is np.ndarray and type(data[0][1]) is np.ndarray and \\\n       type(data[1]) is np.ndarray and np.count_nonzero(data[0][1]) == 0:\n           return True\n    else:\n        return False\n\n\ndef bad_data_tuple(data):\n    """"""\n    Checks for standard tuple or BERT-style tuple\n    """"""\n    if not isinstance(data, tuple) or len(data) != 2 or \\\n       type(data[0]) not in [np.ndarray, list] or \\\n       (type(data[0]) in [list] and type(data[0][0]) is not np.ndarray) or \\\n       type(data[1]) is not np.ndarray: \n        return True\n    else:\n        return False\n\n\n\n#------------------------------------------------------------------------------\n# PLOTTING UTILITIES\n#------------------------------------------------------------------------------\n\n\n# plots images with labels within jupyter notebook\ndef plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n    #if type(ims[0]) is np.ndarray:\n        #ims = np.array(ims).astype(np.uint8)\n        #if (ims.shape[-1] != 3):\n            #ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis(\'Off\')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else \'none\')\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title=\'Confusion matrix\',\n                          cmap=plt.cm.Blues):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.tight_layout()\n    plt.ylabel(\'True label\')\n    plt.xlabel(\'Predicted label\')\n\n\n\n#------------------------------------------------------------------------------\n# DOWNLOAD UTILITIES\n#------------------------------------------------------------------------------\n\n\ndef download(url, filename):\n    with open(filename, \'wb\') as f:\n        response = requests.get(url, stream=True,  verify=False)\n        total = response.headers.get(\'content-length\')\n\n        if total is None:\n            f.write(response.content)\n        else:\n            downloaded = 0\n            total = int(total)\n            #print(total)\n            for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n                downloaded += len(data)\n                f.write(data)\n                done = int(50*downloaded/total)\n                sys.stdout.write(\'\\r[{}{}]\'.format(\'\xe2\x96\x88\' * done, \'.\' * (50-done)))\n                sys.stdout.flush()\n\n\ndef get_ktrain_data():\n    home = os.path.expanduser(\'~\')\n    ktrain_data = os.path.join(home, \'ktrain_data\')\n    if not os.path.isdir(ktrain_data):\n        os.mkdir(ktrain_data)\n    return ktrain_data\n\n\n\n#------------------------------------------------------------------------------\n# MISC UTILITIES\n#------------------------------------------------------------------------------\n\ndef is_tf_keras():\n    if keras.__name__ == \'keras\':\n        is_tf_keras = False\n    elif keras.__name__ in [\'tensorflow.keras\', \'tensorflow.python.keras\', \'tensorflow_core.keras\'] or\\\n            keras.__version__[-3:] == \'-tf\':\n        is_tf_keras = True\n    else:\n        raise KeyError(\'Cannot detect if using keras or tf.keras.\')\n    return is_tf_keras\n\n\ndef vprint(s=None, verbose=1):\n    if not s: s = \'\\n\'\n    if verbose:\n        print(s)\n\n\ndef add_headers_to_df(fname_in, header_dict, fname_out=None):\n\n    df = pd.read_csv(fname_in, header=None)\n    df.rename(columns=header_dict, inplace=True)\n    if fname_out is None:\n        name, ext = os.path.splitext(fname_in)\n        name += \'-headers\'\n        fname_out = name + \'.\' + ext\n    df.to_csv(fname_out, index=False) # save to new csv file\n    return\n\n\ndef get_random_colors(n, name=\'hsv\', hex_format=True):\n    \'\'\'Returns a function that maps each index in 0, 1, ..., n-1 to a distinct\n    RGB color; the keyword argument name must be a standard mpl colormap name.\'\'\'\n    cmap = plt.cm.get_cmap(name, n)\n    result = []\n    for i in range(n):\n        color = cmap(i)\n        if hex_format: color = rgb2hex(color)\n        result.append(color)\n    return np.array(result)\n\n\n'"
ktrain/version.py,0,"b""__all__ = ['__version__']\n__version__ = '0.16.3'\n"""
ktrain/graph/__init__.py,0,"b""from .models import *\nfrom .data import *\n#from .predictor import *\n__all__ = [\n           'graph_nodes_from_csv'\n           'graph_links_from_csv'\n           'print_node_classifiers',\n           'print_link_predictors',\n           'node_classifier'\n           'link_predictor'\n           ]\n\n"""
ktrain/graph/data.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom .preprocessor import NodePreprocessor, LinkPreprocessor\n\n\ndef graph_nodes_from_csv(nodes_filepath, \n                         links_filepath,\n                         use_lcc=True,\n                         sample_size=10,\n                         train_pct=0.1, sep=\',\', \n                         holdout_pct=None, \n                         holdout_for_inductive=False,\n                         missing_label_value=None,\n                         random_state=None,\n                         verbose=1):\n    """"""\n    Loads graph data from CSV files. \n    Returns generators for nodes in graph for use with GraphSAGE model.\n    Args:\n        nodes_filepath(str): file path to training CSV containing node attributes\n        links_filepath(str): file path to training CSV describing links among nodes\n        use_lcc(bool):  If True, consider the largest connected component only.\n        sample_size(int): Number of nodes to sample at each neighborhood level\n        train_pct(float): Proportion of nodes to use for training.\n                          Default is 0.1.\n        sep (str):  delimiter for CSVs. Default is comma.\n        holdout_pct(float): Percentage of nodes to remove and return separately\n                        for later transductive/inductive inference.\n                        Example -->  train_pct=0.1 and holdout_pct=0.2:\n\n                        Out of 1000 nodes, 200 (holdout_pct*1000) will be held out.\n                        Of the remaining 800, 80 (train_pct*800) will be used for training\n                        and 720 ((1-train_pct)*800) will be used for validation.\n                        200 nodes will be used for transductive or inductive inference.\n\n                        Note that holdout_pct is ignored if at least one node has\n                        a missing label in nodes_filepath, in which case\n                        these nodes are assumed to be the holdout set.\n        holdout_for_inductive(bool):  If True, the holdout nodes will be removed from \n                                      training graph and their features will not be visible\n                                      during training.  Only features of training and\n                                      validation nodes will be visible.\n                                      If False, holdout nodes will be included in graph\n                                      and their features (but not labels) are accessible\n                                      during training.\n        random_state (int):  random seed for train/test split\n        verbose (boolean): verbosity\n    Return:\n        tuple of NodeSequenceWrapper objects for train and validation sets and NodePreprocessor\n        If holdout_pct is not None or number of nodes with missing labels is non-zero, \n        fourth and fifth return values are pd.DataFrame and nx.Graph\n        comprising the held out nodes.\n    """"""\n\n    #----------------------------------------------------------------\n    # read graph structure\n    #----------------------------------------------------------------\n    nx_sep = None if sep in [\' \', \'\\t\'] else sep\n    g_nx = nx.read_edgelist(path=links_filepath, delimiter=nx_sep)\n\n    # read node attributes\n    #node_attr = pd.read_csv(nodes_filepath, sep=sep, header=None)\n\n    # store class labels within graph nodes\n    #values = { str(row.tolist()[0]): row.tolist()[-1] for _, row in node_attr.iterrows()}\n    #nx.set_node_attributes(g_nx, values, \'target\')\n\n    # select largest connected component\n    if use_lcc:\n        g_nx_ccs = (g_nx.subgraph(c).copy() for c in nx.connected_components(g_nx))\n        g_nx = max(g_nx_ccs, key=len)\n        if verbose:\n            print(""Largest subgraph statistics: {} nodes, {} edges"".format(\n            g_nx.number_of_nodes(), g_nx.number_of_edges()))\n\n\n    #----------------------------------------------------------------\n    # read node attributes and split into train/validation\n    #----------------------------------------------------------------\n    node_attr = pd.read_csv(nodes_filepath, sep=sep, header=None)\n    num_features = len(node_attr.columns.values) - 2 # subract ID and target\n    feature_names = [""w_{}"".format(ii) for ii in range(num_features)]\n    column_names =  feature_names + [""target""]\n    node_data = pd.read_csv(nodes_filepath, header=None, names=column_names, sep=sep)\n    node_data.index = node_data.index.map(str)\n    node_data = node_data[node_data.index.isin(list(g_nx.nodes()))]\n\n\n    #----------------------------------------------------------------\n    # check for holdout nodes\n    #----------------------------------------------------------------\n    num_null = node_data[node_data.target.isnull()].shape[0]\n    num_missing = 0\n    if missing_label_value is not None:\n        num_missing = node_data[node_data.target == missing_label_value].shape[0]\n\n    if num_missing > 0 and num_null >0:\n        raise ValueError(\'Param missing_label_value is not None but there are \' +\\\n                         \'NULLs in last column. Replace these with missing_label_value.\')\n\n    if (num_null > 0 or num_missing > 0) and holdout_pct is not None:\n        warnings.warn(\'Number of nodes in having NULL  or missing_label_value in target \'+\\\n                      \'column is non-zero. Using these as holdout nodes and ignoring holdout_pct.\')\n\n\n\n    #----------------------------------------------------------------\n    # set df and G and optionally holdout nodes\n    #----------------------------------------------------------------\n    if num_null > 0:\n        df_annotated = node_data[~node_data.target.isnull()]\n        df_holdout = node_data[~node_data.target.isnull()]\n        G_holdout = g_nx\n        df_G = df_annotated if holdout_for_inductive else node_data\n        G = g_nx.subgraph(df_annotated.index).copy() if holdout_for_inductive else g_nx\n        U.vprint(\'using %s nodes with target=NULL as holdout set\' % (num_null), verbose=verbose)\n    elif num_missing > 0:\n        df_annotated = node_data[node_data.target != missing_label_value]\n        df_holdout = node_data[node_data.target == missing_label_value]\n        G_holdout = g_nx\n        df_G = df_annotated if holdout_for_inductive else node_data\n        G = g_nx.subgraph(df_annotated.index).copy() if holdout_for_inductive else g_nx\n        U.vprint(\'using %s nodes with missing target as holdout set\' % (num_missing), verbose=verbose)\n    elif holdout_pct is not None:\n        df_annotated = node_data.sample(frac=1-holdout_pct, replace=False, random_state=None)\n        df_holdout = node_data[~node_data.index.isin(df_annotated.index)]\n        G_holdout = g_nx\n        df_G = df_annotated if holdout_for_inductive else node_data\n        G = g_nx.subgraph(df_annotated.index).copy() if holdout_for_inductive else g_nx\n    else:\n        if holdout_for_inductive:\n            warnings.warn(\'holdout_for_inductive is True but no nodes were heldout \'\n                          \'because holdout_pct is None and no missing targets\')\n        df_annotated = node_data\n        df_holdout = None\n        G_holdout = None\n        df_G = node_data\n        G = g_nx\n\n\n    #----------------------------------------------------------------\n    # split into train and validation\n    #----------------------------------------------------------------\n    tr_data, te_data = sklearn.model_selection.train_test_split(df_annotated, \n                                                        train_size=train_pct,\n                                                        test_size=None,\n                                                        stratify=df_annotated[\'target\'], \n                                                        random_state=random_state)\n    #te_data, test_data = sklearn.model_selection.train_test_split(test_data,\n                                                                #train_size=0.2,\n                                                                #test_size=None,\n                                                                 #stratify=test_data[""target""],\n                                                                 #random_state=100)\n\n    #----------------------------------------------------------------\n    # print summary\n    #----------------------------------------------------------------\n    if verbose:\n        print(""Size of training graph: %s nodes"" % (G.number_of_nodes()))\n        print(""Training nodes: %s"" % (tr_data.shape[0]))\n        print(""Validation nodes: %s"" % (te_data.shape[0]))\n        if df_holdout is not None and G_holdout is not None:\n            print(""Nodes treated as unlabeled for testing/inference: %s"" % (df_holdout.shape[0]))\n            if holdout_for_inductive:\n                print(""Size of graph with added holdout nodes: %s"" % (G_holdout.number_of_nodes()))\n                print(""Holdout node features are not visible during training (inductive inference)"")\n            else:\n                print(""Holdout node features are visible during training (transductive inference)"")\n        print()\n\n\n\n    #----------------------------------------------------------------\n    # Preprocess training and validation datasets using NodePreprocessor\n    #----------------------------------------------------------------\n    preproc = NodePreprocessor(G, df_G, sample_size=sample_size, missing_label_value=missing_label_value)\n    trn = preproc.preprocess_train(list(tr_data.index))\n    val = preproc.preprocess_valid(list(te_data.index))\n    if df_holdout is not None and G_holdout is not None: \n        return (trn, val, preproc, df_holdout, G_holdout)\n    else:\n        return (trn, val, preproc)\n\n\n\ndef graph_links_from_csv(nodes_filepath, \n                         links_filepath,\n                         sample_sizes=[10, 20],\n                         train_pct=0.1, val_pct=0.1, sep=\',\', \n                         holdout_pct=None, \n                         holdout_for_inductive=False,\n                         missing_label_value=None,\n                         random_state=None,\n                         verbose=1):\n    """"""\n    Loads graph data from CSV files. \n    Returns generators for links in graph for use with GraphSAGE model.\n    Args:\n        nodes_filepath(str): file path to training CSV containing node attributes\n        links_filepath(str): file path to training CSV describing links among nodes\n        sample_sizes(int): Number of nodes to sample at each neighborhood level.\n        train_pct(float): Proportion of edges to use for training.\n                          Default is 0.1.\n                          Note that train_pct is applied after val_pct is applied.\n        val_pct(float): Proportion of edges to use for validation\n        sep (str):  delimiter for CSVs. Default is comma.\n        random_state (int):  random seed for train/test split\n        verbose (boolean): verbosity\n    Return:\n        tuple of EdgeSequenceWrapper objects for train and validation sets and LinkPreprocessor\n    """"""\n\n    # import stellargraph\n    try:\n        import stellargraph as sg\n        from stellargraph.data import EdgeSplitter\n    except:\n        raise Exception(SG_ERRMSG)\n    if version.parse(sg.__version__) < version.parse(\'0.8\'):\n        raise Exception(SG_ERRMSG)\n\n\n    #----------------------------------------------------------------\n    # read graph structure\n    #----------------------------------------------------------------\n    nx_sep = None if sep in [\' \', \'\\t\'] else sep\n    G = nx.read_edgelist(path=links_filepath, delimiter=nx_sep)\n    print(nx.info(G))\n\n\n\n\n    #----------------------------------------------------------------\n    # read node attributes\n    #----------------------------------------------------------------\n    node_attr = pd.read_csv(nodes_filepath, sep=sep, header=None)\n    num_features = len(node_attr.columns.values) - 1 # subract ID and treat all other columns as features\n    feature_names = [""w_{}"".format(ii) for ii in range(num_features)]\n    node_data = pd.read_csv(nodes_filepath, header=None, names=feature_names, sep=sep)\n    node_data.index = node_data.index.map(str)\n    df = node_data[node_data.index.isin(list(G.nodes()))]\n    for col in feature_names:\n        if not isinstance(node_data[col].values[0], str): continue\n        df = pd.concat([df, df[col].astype(\'str\').str.get_dummies().add_prefix(col+\'_\')], axis=1, sort=False)\n        df = df.drop([col], axis=1)\n    feature_names = df.columns.values\n    node_data = df\n    node_features = node_data[feature_names].values\n    for nid, f in zip(node_data.index, node_features):\n        G.node[nid][sg.globalvar.TYPE_ATTR_NAME] = ""node""  \n        G.node[nid][""feature""] = f\n\n\n    #----------------------------------------------------------------\n    # train/validation sets\n    #----------------------------------------------------------------\n    edge_splitter_test = EdgeSplitter(G)\n    G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(p=val_pct, method=""global"", keep_connected=True)\n    edge_splitter_train = EdgeSplitter(G_test)\n    G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(p=train_pct, method=""global"", keep_connected=True)\n    epp = LinkPreprocessor(G, sample_sizes=sample_sizes)\n    trn = epp.preprocess_train(G_train, edge_ids_train, edge_labels_train)\n    val = epp.preprocess_valid(G_test, edge_ids_test, edge_labels_test)\n    \n    return (trn, val, epp)\n\n\n\n\n'"
ktrain/graph/learner.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom ..core import GenLearner\n\n\n\n\nclass NodeClassLearner(GenLearner):\n    """"""\n    Main class used to tune and train Keras models for node classification\n    Main parameters are:\n\n    model (Model): A compiled instance of keras.engine.training.Model\n    train_data (Iterator): a Iterator instance for training set\n    val_data (Iterator):   A Iterator instance for validation set\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, train_data=train_data, val_data=val_data, \n                         batch_size=batch_size, eval_batch_size=eval_batch_size,\n                         workers=workers, use_multiprocessing=use_multiprocessing,\n                         multigpu=multigpu)\n        return\n\n    \n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Typically over-ridden by Learner subclasses.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            print(\'----------\')\n            print(""id:%s | loss:%s | true:%s | pred:%s)\\n"" % (idx, round(loss,2), truth, pred))\n            #print(obs)\n        return\n\n\n\n    def layer_output(self, layer_id, example_id=0, batch_id=0, use_val=False):\n        """"""\n        Prints output of layer with index <layer_id> to help debug models.\n        Uses first example (example_id=0) from training set, by default.\n        """"""\n        raise Exception(\'currently_unsupported: layer_output method is not yet supported for \' +\n                      \'graph neural networks in ktrain\')\n\n\nclass LinkPredLearner(GenLearner):\n    """"""\n    Main class used to tune and train Keras models for link prediction\n    Main parameters are:\n\n    model (Model): A compiled instance of keras.engine.training.Model\n    train_data (Iterator): a Iterator instance for training set\n    val_data (Iterator):   A Iterator instance for validation set\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, train_data=train_data, val_data=val_data, \n                         batch_size=batch_size, eval_batch_size=eval_batch_size,\n                         workers=workers, use_multiprocessing=use_multiprocessing,\n                         multigpu=multigpu)\n        return\n\n    \n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Typically over-ridden by Learner subclasses.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            print(\'----------\')\n            print(""id:%s | loss:%s | true:%s | pred:%s)\\n"" % (idx, round(loss,2), truth, pred))\n            #print(obs)\n        return\n\n\n\n    def layer_output(self, layer_id, example_id=0, batch_id=0, use_val=False):\n        """"""\n        Prints output of layer with index <layer_id> to help debug models.\n        Uses first example (example_id=0) from training set, by default.\n        """"""\n        raise Exception(\'currently_unsupported: layer_output method is not yet supported for \' +\n                      \'graph neural networks in ktrain\')\n\n\n'"
ktrain/graph/models.py,0,"b'from ..imports import *\nfrom .. import utils as U\n\n\n\n\n\n\n\nGRAPHSAGE = \'graphsage\'\nNODE_CLASSIFIERS = {\n        GRAPHSAGE: \'GraphSAGE:  http://arxiv.org/pdf/1607.01759.pdf\'}\n\nLINK_PREDICTORS = {\n        GRAPHSAGE: \'GraphSAGE:  http://arxiv.org/pdf/1607.01759.pdf\'}\n\n\ndef print_node_classifiers():\n    for k,v in NODE_CLASSIFIERS.items():\n        print(""%s: %s"" % (k,v))\n\ndef print_link_predictors():\n    for k,v in LINK_PREDICTORS.items():\n        print(""%s: %s"" % (k,v))\n\n\ndef graph_node_classifier(name, train_data, layer_sizes=[32,32], verbose=1):\n    """"""\n    Build and return a neural node classification model.\n    Notes: Only mutually-exclusive class labels are supported.\n\n    Args:\n        name (string): one of:\n                      - \'graphsage\' for GraphSAGE model \n                      (only GraphSAGE currently supported)\n\n        train_data (NodeSequenceWrapper): a ktrain.graph.sg_wrappers.NodeSequenceWrapper object\n        verbose (boolean): verbosity of output\n    Return:\n        model (Model): A Keras Model instance\n    """"""\n    from .sg_wrappers import NodeSequenceWrapper\n\n    # check argument\n    if not isinstance(train_data, NodeSequenceWrapper):\n        err =""""""\n            train_data must be a ktrain.graph.sg_wrappers.NodeSequenceWrapper object\n            """"""\n        raise Exception(err)\n    if len(layer_sizes) != 2:\n        raise ValueError(\'layer_sizes must be of length 2\')\n\n    num_classes = U.nclasses_from_data(train_data)\n\n    # determine multilabel\n    multilabel = U.is_multilabel(train_data)\n    if multilabel:\n        raise ValueError(\'Multi-label classification not currently supported for graphs.\')\n    U.vprint(""Is Multi-Label? %s"" % (multilabel), verbose=verbose)\n\n    # set loss and activations\n    loss_func = \'categorical_crossentropy\'\n    activation = \'softmax\'\n\n    # import stellargraph\n    try:\n        import stellargraph as sg\n        from stellargraph.layer import GraphSAGE\n    except:\n        raise Exception(SG_ERRMSG)\n    if version.parse(sg.__version__) < version.parse(\'0.8\'):\n        raise Exception(SG_ERRMSG)\n\n\n\n\n\n    # build a GraphSAGE node classification model\n    graphsage_model = GraphSAGE(\n        layer_sizes=layer_sizes,\n        generator=train_data,\n        bias=True,\n        dropout=0.5,\n\t)\n    #x_inp, x_out = graphsage_model.default_model(flatten_output=True)\n    x_inp, x_out = graphsage_model.build()\n    prediction = Dense(units=num_classes, activation=activation)(x_out)\n    model = Model(inputs=x_inp, outputs=prediction)\n    model.compile(optimizer=\'adam\',\n                  loss=loss_func,\n                  metrics=[""accuracy""])\n    U.vprint(\'done\', verbose=verbose)\n    return model\n\n\n\ndef graph_link_predictor(name, train_data, preproc, layer_sizes=[20,20], verbose=1):\n    """"""\n    Build and return a neural link prediction model.\n\n    Args:\n        name (string): one of:\n                      - \'graphsage\' for GraphSAGE model \n                      (only GraphSAGE currently supported)\n\n        train_data (LinkSequenceWrapper): a ktrain.graph.sg_wrappers.LinkSequenceWrapper object\n        preproc(LinkPreprocessor): a LinkPreprocessor instance\n        verbose (boolean): verbosity of output\n    Return:\n        model (Model): A Keras Model instance\n    """"""\n    from .sg_wrappers import LinkSequenceWrapper\n\n    # check argument\n    if not isinstance(train_data, LinkSequenceWrapper):\n        err =""""""\n            train_data must be a ktrain.graph.sg_wrappers.LinkSequenceWrapper object\n            """"""\n        raise Exception(err)\n    if len(layer_sizes) != len(preproc.sample_sizes):\n        raise ValueError(\'number of layer_sizes must match len(preproc.sample_sizes)\')\n\n    num_classes = U.nclasses_from_data(train_data)\n\n\n    # set loss and activations\n    loss_func = \'categorical_crossentropy\'\n    activation = \'softmax\'\n\n    # import stellargraph\n    try:\n        import stellargraph as sg\n        from stellargraph.layer import GraphSAGE, link_classification\n    except:\n        raise Exception(SG_ERRMSG)\n    if version.parse(sg.__version__) < version.parse(\'0.8\'):\n        raise Exception(SG_ERRMSG)\n\n\n\n    # build a GraphSAGE link prediction model\n    graphsage = GraphSAGE(layer_sizes=layer_sizes, generator=train_data, bias=True, dropout=0.3) \n    x_inp, x_out = graphsage.build()\n    prediction = link_classification( output_dim=1, output_act=""relu"", edge_embedding_method=\'ip\')(x_out)\n    model = Model(inputs=x_inp, outputs=prediction)\n    model.compile( optimizer=U.DEFAULT_OPT, loss=\'binary_crossentropy\', metrics=[""accuracy""])\n    return model\n\n'"
ktrain/graph/predictor.py,0,"b'from ..imports import *\nfrom ..predictor import Predictor\nfrom .preprocessor import NodePreprocessor, LinkPreprocessor\nfrom .. import utils as U\n\nclass NodePredictor(Predictor):\n    """"""\n    predicts graph node\'s classes\n    """"""\n\n    def __init__(self, model, preproc, batch_size=U.DEFAULT_BS):\n\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        if not isinstance(preproc, NodePreprocessor):\n            raise ValueError(\'preproc must be a NodePreprocessor object\')\n        self.model = model\n        self.preproc = preproc\n        self.c = self.preproc.get_classes()\n        self.batch_size = batch_size\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def predict(self, node_ids, return_proba=False):\n        return self.predict_transductive(node_ids, return_proba=return_proba)\n\n\n    def predict_transductive(self, node_ids, return_proba=False):\n        """"""\n        Performs transductive inference.\n        If return_proba is True, returns probabilities of each class.\n        """"""\n        gen = self.preproc.preprocess_valid(node_ids)\n        gen.batch_size = self.batch_size\n        # *_generator methods are deprecated from TF 2.1.0\n        #preds = self.model.predict_generator(gen)\n        preds = self.model.predict(gen)\n        result =  preds if return_proba else [self.c[np.argmax(pred)] for pred in preds]\n        return result\n\n\n    def predict_inductive(self, df, G, return_proba=False):\n        """"""\n        Performs inductive inference.\n        If return_proba is True, returns probabilities of each class.\n        """"""\n\n        gen = self.preproc.preprocess(df, G)\n        gen.batch_size = self.batch_size\n        # *_generator methods are deprecated from TF 2.1.0\n        #preds = self.model.predict_generator(gen)\n        preds = self.model.predict(gen)\n        result =  preds if return_proba else [self.c[np.argmax(pred)] for pred in preds]\n        return result\n\n\nclass LinkPredictor(Predictor):\n    """"""\n    predicts graph node\'s classes\n    """"""\n\n    def __init__(self, model, preproc, batch_size=U.DEFAULT_BS):\n\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        if not isinstance(preproc, LinkPreprocessor):\n            raise ValueError(\'preproc must be a LinkPreprocessor object\')\n        self.model = model\n        self.preproc = preproc\n        self.c = self.preproc.get_classes()\n        self.batch_size = batch_size\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def predict(self, G, edge_ids, return_proba=False):\n        """"""\n        Performs link prediction\n        If return_proba is True, returns probabilities of each class.\n        """"""\n        gen = self.preproc.preprocess(G, edge_ids)\n        gen.batch_size = self.batch_size\n        # *_generator methods are deprecated from TF 2.1.0\n        #preds = self.model.predict_generator(gen)\n        preds = self.model.predict(gen)\n        preds = np.squeeze(preds)\n        if return_proba:\n            return [[1-pred, pred] for pred in preds] \n        result =  np.where(preds > 0.5, self.c[1], self.c[0])\n        return result\n\n'"
ktrain/graph/preprocessor.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom ..preprocessor import Preprocessor\n\n\nclass NodePreprocessor(Preprocessor):\n    """"""\n    Node preprocessing base class\n    """"""\n\n    def __init__(self, G_nx, df, sample_size=10, missing_label_value=None):\n\n        self.sampsize = sample_size       # neighbor sample size\n        self.df = df                      # node attributes and targets\n        # TODO: eliminate storage redundancy\n        self.G = G_nx                     # networkx graph\n        self.G_sg = None                  # StellarGraph \n\n        # clean df\n        df.index = df.index.map(str)\n        df= df[df.index.isin(list(self.G.nodes()))]\n\n        # class names\n        self.c = list(set([c[0] for c in df[[\'target\']].values]))\n        if missing_label_value is not None: self.c.remove(missing_label_value)\n        self.c.sort()\n\n        # feature names + target\n        self.colnames = list(df.columns.values)\n        if self.colnames[-1] != \'target\':\n            raise ValueError(\'last column of df must be ""target""\')\n\n        # set by preprocess_train\n        self.y_encoding = None\n\n\n    def get_preprocessor(self):\n        return (self.G, self.df)\n\n\n    def get_classes(self):\n        return self.c\n\n    @property\n    def feature_names(self):\n        return self.colnames[:-1]\n\n\n    def preprocess(self, df, G):\n        return self.preprocess_test(df, G)\n\n\n    def ids_exist(self, node_ids):\n        """"""\n        check validity of node IDs\n        """"""\n        df = self.df[self.df.index.isin(node_ids)]\n        return df.shape[0] > 0\n\n\n\n    def preprocess_train(self, node_ids):\n        """"""\n        preprocess training set\n        """"""\n        if not self.ids_exist(node_ids): raise ValueError(\'node_ids must exist in self.df\')\n\n        # subset df for training nodes\n        df_tr = self.df[self.df.index.isin(node_ids)]\n\n        # one-hot-encode target\n        self.y_encoding = sklearn.feature_extraction.DictVectorizer(sparse=False)\n        train_targets = self.y_encoding.fit_transform(df_tr[[""target""]].to_dict(\'records\'))\n\n\n\n        # import stellargraph\n        try:\n            import stellargraph as sg\n            from stellargraph.mapper import GraphSAGENodeGenerator\n        except:\n            raise Exception(SG_ERRMSG)\n        if version.parse(sg.__version__) < version.parse(\'0.8\'):\n            raise Exception(SG_ERRMSG)\n\n\n\n        # return generator\n        G_sg = sg.StellarGraph(self.G, node_features=self.df[self.feature_names])\n        self.G_sg = G_sg\n        generator = GraphSAGENodeGenerator(G_sg, U.DEFAULT_BS, [self.sampsize, self.sampsize])\n        train_gen = generator.flow(df_tr.index, train_targets, shuffle=True)\n        from .sg_wrappers import NodeSequenceWrapper\n        return NodeSequenceWrapper(train_gen)\n\n\n    def preprocess_valid(self, node_ids):\n        """"""\n        preprocess validation nodes (transductive inference)\n        node_ids (list):  list of node IDs that generator will yield\n        """"""\n        if not self.ids_exist(node_ids): raise ValueError(\'node_ids must exist in self.df\')\n        if self.y_encoding is None:\n            raise Exception(\'Unset parameters. Are you sure you called preprocess_train first?\')\n\n        # subset df for validation nodes\n        df_val = self.df[self.df.index.isin(node_ids)]\n\n\n        # one-hot-encode target\n        val_targets = self.y_encoding.transform(df_val[[""target""]].to_dict(\'records\'))\n\n\n        # import stellargraph\n        try:\n            import stellargraph as sg\n            from stellargraph.mapper import GraphSAGENodeGenerator\n        except:\n            raise Exception(SG_ERRMSG)\n        if version.parse(sg.__version__) < version.parse(\'0.8\'):\n            raise Exception(SG_ERRMSG)\n\n\n        # return generator\n        if self.G_sg is None:\n            self.G_sg = sg.StellarGraph(self.G, node_features=self.df[self.feature_names])\n        generator = GraphSAGENodeGenerator(self.G_sg, U.DEFAULT_BS, [self.sampsize,self.sampsize])\n        val_gen = generator.flow(df_val.index, val_targets, shuffle=False)\n        from .sg_wrappers import NodeSequenceWrapper\n        return NodeSequenceWrapper(val_gen)\n\n\n\n    def preprocess_test(self, df_te, G_te):\n        """"""\n        preprocess for inductive inference\n        df_te (DataFrame): pandas dataframe containing new node attributes\n        G_te (Graph):  a networkx Graph containing new nodes\n        """"""\n        if self.y_encoding is None:\n            raise Exception(\'Unset parameters. Are you sure you called preprocess_train first?\')\n\n        # get aggregrated df\n        #df_agg = pd.concat([df_te, self.df]).drop_duplicates(keep=\'last\')\n        df_agg = pd.concat([df_te, self.df])\n        #df_te = pd.concat([self.df, df_agg]).drop_duplicates(keep=False)\n\n\n        # get aggregrated graph\n        is_subset = set(self.G.nodes()) <= set(G_te.nodes())\n        if not is_subset:\n            raise ValueError(\'Nodes in self.G must be subset of G_te\')\n        G_agg = nx.compose(self.G, G_te)    \n\n        \n        # one-hot-encode target\n        if \'target\' in df_te.columns:\n            test_targets = self.y_encoding.transform(df_te[[""target""]].to_dict(\'records\'))\n        else:\n            test_targets = [-1] * len(df_te.shape[0])\n\n\n        # import stellargraph\n        try:\n            import stellargraph as sg\n            from stellargraph.mapper import GraphSAGENodeGenerator\n        except:\n            raise Exception(SG_ERRMSG)\n        if version.parse(sg.__version__) < version.parse(\'0.8\'):\n            raise Exception(SG_ERRMSG)\n\n\n        # return generator\n        G_sg = sg.StellarGraph(G_agg, node_features=df_agg[self.feature_names])\n        generator = GraphSAGENodeGenerator(G_sg, U.DEFAULT_BS, [self.sampsize,self.sampsize])\n        test_gen = generator.flow(df_te.index, test_targets, shuffle=False)\n        from .sg_wrappers import NodeSequenceWrapper\n        return NodeSequenceWrapper(test_gen)\n\n\n\n\nclass LinkPreprocessor(Preprocessor):\n    """"""\n    Link preprocessing base class\n    """"""\n\n    def __init__(self, G,  sample_sizes=[10, 20]):\n        self.sample_sizes = sample_sizes\n        self.G = G # original graph under consideration with all original links\n\n\n        # class names\n        self.c = [\'negative\', \'positive\']\n\n\n    def get_preprocessor(self):\n        return self\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def preprocess(self, G, edge_ids):\n        edge_labels = [1] * len(edge_ids)\n        return self.preprocess_valid(G, edge_ids, edge_labels)\n\n\n    def preprocess_train(self, G, edge_ids, edge_labels, mode=\'train\'):\n        """"""\n        preprocess training set\n        Args:\n          G (networkx graph): networkx graph\n          edge_ids(list): list of tuples representing edge ids\n          edge_labels(list): edge labels (1 or 0 to indicated whether it is a true edge in original graph or not)\n        """"""\n        # import stellargraph\n        try:\n            import stellargraph as sg\n            from stellargraph.mapper import GraphSAGELinkGenerator\n        except:\n            raise Exception(SG_ERRMSG)\n        if version.parse(sg.__version__) < version.parse(\'0.8\'):\n            raise Exception(SG_ERRMSG)\n\n        #edge_labels = to_categorical(edge_labels)\n        G_sg = sg.StellarGraph(G, node_features=""feature"")\n        #print(G_sg.info())\n        shuffle = True if mode == \'train\' else False\n        link_seq = GraphSAGELinkGenerator(G_sg, U.DEFAULT_BS, self.sample_sizes).flow(edge_ids, edge_labels, shuffle=shuffle)\n        from .sg_wrappers import LinkSequenceWrapper\n        return LinkSequenceWrapper(link_seq)\n\n\n    def preprocess_valid(self, G, edge_ids, edge_labels):\n        """"""\n        preprocess training set\n        Args:\n          G (networkx graph): networkx graph\n          edge_ids(list): list of tuples representing edge ids\n          edge_labels(list): edge labels (1 or 0 to indicated whether it is a true edge in original graph or not)\n        """"""\n        return self.preprocess_train(G, edge_ids, edge_labels, mode=\'valid\')\n\n\n'"
ktrain/graph/sg_wrappers.py,0,"b""from ..imports import *\nfrom ..data import SequenceDataset\n\n\n# import stellargraph\ntry:\n    import stellargraph as sg\n    from stellargraph.mapper import node_mappers, link_mappers\nexcept:\n    raise Exception(SG_ERRMSG)\nif version.parse(sg.__version__) < version.parse('0.8'):\n    raise Exception(SG_ERRMSG)\n\n\nclass NodeSequenceWrapper(node_mappers.NodeSequence, SequenceDataset):\n    def __init__(self, node_seq):\n        if not isinstance(node_seq, node_mappers.NodeSequence):\n            raise ValueError('node_seq must by a stellargraph NodeSequence object')\n        self.node_seq = node_seq\n        self.targets = node_seq.targets\n        self.generator = node_seq.generator\n        self.ids = node_seq.ids\n        self.__len__ = node_seq.__len__\n        self.__getitem__ = node_seq.__getitem__\n        self.on_epoch_end = node_seq.on_epoch_end\n        self.indices = node_seq.indices\n\n\n\n\n    def __setattr__(self, name, value):\n        if name == 'batch_size':\n            self.generator.batch_size = value\n        elif name == 'data_size':\n            self.node_seq.data_size = value\n        elif name == 'shuffle':\n            self.node_seq.shuffle = value\n        elif name == 'head_node_types':\n            self.node_seq.head_node_types = value\n        elif name == '_sampling_schema':\n            self.node_seq._sample_schema = value\n        else:\n            self.__dict__[name] = value\n        return\n\n\n\n    def __getattr__(self, name):\n        if name == 'batch_size':\n            return self.generator.batch_size\n        elif name == 'data_size':\n            return self.node_seq.data_size\n        elif name == 'shuffle':\n            return self.node_seq.shuffle\n        elif name == 'head_node_types':\n            return self.node_seq.head_node_types\n        elif name == '_sampling_schema':\n            return self.node_seq._sampling_schema\n        elif name == 'reset':\n            # stellargraph did not implement reset for its generators\n            # return a zero-argument lambda that returns None\n            return lambda:None \n        elif name == 'graph':\n            return self.generator.graph\n        else:\n            try:\n                return self.__dict__[name] \n            except:\n                raise AttributeError\n        return\n\n\n    def nsamples(self):\n        return self.targets.shape[0]\n\n\n    def get_y(self):\n        return self.targets\n\n\n    def xshape(self):\n        return self[0][0][0].shape[1:]  # returns 1st neighborhood only\n\n\n    def nclasses(self):\n        return self[0][1].shape[1]                                   \n\n\n\n\nclass LinkSequenceWrapper(link_mappers.LinkSequence, SequenceDataset):\n    def __init__(self, link_seq):\n        if not isinstance(link_seq, link_mappers.LinkSequence):\n            raise ValueError('link_seq must by a stellargraph LinkSequence object')\n        self.link_seq = link_seq\n        self.targets = link_seq.targets\n        self.generator = link_seq.generator\n        self.ids = link_seq.ids\n        self.__len__ = link_seq.__len__\n        self.__getitem__ = link_seq.__getitem__\n        self.on_epoch_end = link_seq.on_epoch_end\n        self.indices = link_seq.indices\n\n\n\n\n    def __setattr__(self, name, value):\n        if name == 'batch_size':\n            self.generator.batch_size = value\n        elif name == 'data_size':\n            self.link_seq.data_size = value\n        elif name == 'shuffle':\n            self.link_seq.shuffle = value\n        elif name == 'head_node_types':\n            self.link_seq.head_node_types = value\n        elif name == '_sampling_schema':\n            self.link_seq._sample_schema = value\n        else:\n            self.__dict__[name] = value\n        return\n\n\n\n    def __getattr__(self, name):\n        if name == 'batch_size':\n            return self.generator.batch_size\n        elif name == 'data_size':\n            return self.link_seq.data_size\n        elif name == 'shuffle':\n            return self.link_seq.shuffle\n        elif name == 'head_node_types':\n            return self.link_seq.head_node_types\n        elif name == '_sampling_schema':\n            return self.link_seq._sampling_schema\n        elif name == 'reset':\n            # stellargraph did not implement reset for its generators\n            # return a zero-argument lambda that returns None\n            return lambda:None \n        elif name == 'graph':\n            return self.generator.graph\n        else:\n            try:\n                return self.__dict__[name] \n            except:\n                raise AttributeError\n        return\n\n\n    def nsamples(self):\n        return self.targets.shape[0]\n\n\n    def get_y(self):\n        return self.targets\n\n\n    def xshape(self):\n        return self[0][0][0].shape[1:]  # returns 1st neighborhood only\n\n    def nclasses(self):\n        return 2\n        return self[0][1].shape[1]                                   \n\n\n\n\n\n\n\n\n\n\n"""
ktrain/lroptimize/__init__.py,0,b''
ktrain/lroptimize/lrfinder.py,0,"b'from ..imports import *\nfrom .. import utils as U\n\n\nclass LRFinder:\n    """"""\n    Tracks (and plots) the change in loss of a Keras model as learning rate is gradually increased.\n    Used to visually identify a good learning rate, given model and data.\n    Reference:\n        Original Paper: https://arxiv.org/abs/1506.01186\n    """"""\n    def __init__(self, model, stop_factor=4):\n        self.model = model\n        self.losses = []\n        self.lrs = []\n        self.best_loss = 1e9\n        self._weightfile = None\n        self.stop_factor = stop_factor\n\n        self.avg_loss = 0\n        self.batch_num = 0\n        self.beta = 0.98\n\n    def on_batch_end(self, batch, logs):\n        # Log the learning rate\n        lr = K.get_value(self.model.optimizer.lr)\n        self.lrs.append(lr)\n\n        # Log the loss\n        loss = logs[\'loss\']\n        self.batch_num +=1\n        self.avg_loss = self.beta * self.avg_loss + (1-self.beta) *loss\n        smoothed_loss = self.avg_loss / (1 - self.beta**self.batch_num)\n        self.losses.append(smoothed_loss)\n\n\n        # Check whether the loss got too large or NaN\n        if self.batch_num > 1 and smoothed_loss > self.stop_factor * self.best_loss:\n            self.model.stop_training = True\n            return\n\n        # record best loss\n        if smoothed_loss < self.best_loss or self.batch_num==1:\n            self.best_loss = smoothed_loss\n\n        # Increase the learning rate for the next batch\n        lr *= self.lr_mult\n        K.set_value(self.model.optimizer.lr, lr)\n\n        # stop if LR grows too large\n        if lr > 10.:\n            self.model.stop_training = True\n            return\n\n\n    def find(self, train_data, steps_per_epoch, use_gen=False,\n             start_lr=1e-7, lr_mult=1.01, max_epochs=None, \n             batch_size=U.DEFAULT_BS, workers=1, use_multiprocessing=False, verbose=1):\n        """"""\n        Track loss as learning rate is increased.\n        NOTE: batch_size is ignored when train_data is instance of Iterator.\n        """"""\n\n        # check arguments and initialize\n        if train_data is None:\n            raise ValueError(\'train_data is required\')\n        #U.data_arg_check(train_data=train_data, train_required=True)\n        self.lrs = []\n        self.losses = []\n\n         # compute steps_per_epoch\n        #num_samples = U.nsamples_from_data(train_data)\n        #if U.is_iter(train_data):\n            #use_gen = True\n            #steps_per_epoch = num_samples // train_data.batch_size\n        #else:\n            #use_gen = False\n            #steps_per_epoch = np.ceil(num_samples/batch_size)\n\n        # max_epochs and lr_mult are None, set max_epochs\n        # using sample size of 1500 batches\n        if max_epochs is None and lr_mult is None:\n            max_epochs = int(np.ceil(1500./steps_per_epoch))\n\n        if max_epochs:\n            epochs = max_epochs\n            num_batches = epochs * steps_per_epoch\n            end_lr = 10 if start_lr < 10 else start_lr * 10\n            self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n        else:\n            epochs = 1024\n            self.lr_mult = lr_mult\n\n        # Save weights into a file\n        new_file, self._weightfile = tempfile.mkstemp()\n        self.model.save_weights(self._weightfile)\n\n        # Remember the original learning rate\n        original_lr = K.get_value(self.model.optimizer.lr)\n\n        # Set the initial learning rate\n        K.set_value(self.model.optimizer.lr, start_lr)\n\n        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n\n\n        if use_gen:\n            # *_generator methods are deprecated from TF 2.1.0\n            fit_fn = self.model.fit\n            fit_fn(train_data, steps_per_epoch=steps_per_epoch, \n                   epochs=epochs, \n                   workers=workers, use_multiprocessing=use_multiprocessing,\n                   verbose=verbose,\n                   callbacks=[callback])\n        else:\n            self.model.fit(train_data[0], train_data[1],\n                            batch_size=batch_size, epochs=epochs, verbose=verbose,\n                            callbacks=[callback])\n\n\n        # Restore the weights to the state before model fitting\n        self.model.load_weights(self._weightfile)\n        self._weightfile=None\n\n        # Restore the original learning rate\n        K.set_value(self.model.optimizer.lr, original_lr)\n\n\n        return \n\n\n    def plot_loss(self, n_skip_beginning=10, n_skip_end=1, suggest=False):\n        """"""\n        Plots the loss.\n        Args:\n            n_skip_beginning(int): number of batches to skip on the left.\n            n_skip_end(int):  number of batches to skip on the right.\n            suggest(bool): will highlight numerical estimate\n                           of best lr if True - methods adapted from fastai\n        """"""\n        fig, ax = plt.subplots()\n        plt.ylabel(""loss"")\n        plt.xlabel(""learning rate (log scale)"")\n        ax.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n        plt.xscale(\'log\')\n\n        if suggest:\n            # this code was adapted from fastai: https://github.com/fastai/fastai\n            try: \n                ml = np.argmin(self.losses)\n                mg = (np.gradient(np.array(self.losses[32:ml]))).argmin()\n            except:\n                print(""Failed to compute the gradients, there might not be enough points.\\n"" +\\\n                       ""Plot displayed without suggestion."")\n                return\n            else:\n                print(\'Two possible suggestions for LR from plot:\')\n                print(f""\\tMin numerical gradient: {self.lrs[mg]:.2E}"")\n                print(f""\\tMin loss divided by 10: {self.lrs[ml]/10:.2E}"")\n                print(mg)\n                ax.plot(self.lrs[mg],self.losses[mg], markersize=10,marker=\'o\',color=\'red\')\n        return\n\n\n\n        \n    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n        """"""\n        Plots rate of change of the loss function.\n        Parameters:\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n            y_lim - limits for the y axis.\n        """"""\n        assert sma >= 1\n        derivatives = [0] * sma\n        for i in range(sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n            derivatives.append(derivative)\n\n        plt.ylabel(""rate of loss change"")\n        plt.xlabel(""learning rate (log scale)"")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n        plt.xscale(\'log\')\n        plt.ylim(y_lim)\n\n'"
ktrain/lroptimize/optimization.py,13,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions and classes related to optimization (weight updates).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\n\nclass WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Applys a warmup schedule on a given learning rate decay schedule.""""""\n\n  def __init__(\n      self,\n      initial_learning_rate,\n      decay_schedule_fn,\n      warmup_steps,\n      power=1.0,\n      name=None):\n    super(WarmUp, self).__init__()\n    self.initial_learning_rate = initial_learning_rate\n    self.warmup_steps = warmup_steps\n    self.power = power\n    self.decay_schedule_fn = decay_schedule_fn\n    self.name = name\n\n  def __call__(self, step):\n    with tf.name_scope(self.name or \'WarmUp\') as name:\n      # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n      # learning rate will be `global_step/num_warmup_steps * init_lr`.\n      global_step_float = tf.cast(step, tf.float32)\n      warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n      warmup_percent_done = global_step_float / warmup_steps_float\n      warmup_learning_rate = (\n          self.initial_learning_rate *\n          tf.math.pow(warmup_percent_done, self.power))\n      return tf.cond(global_step_float < warmup_steps_float,\n                     lambda: warmup_learning_rate,\n                     lambda: self.decay_schedule_fn(step),\n                     name=name)\n\n  def get_config(self):\n    return {\n        \'initial_learning_rate\': self.initial_learning_rate,\n        \'decay_schedule_fn\': self.decay_schedule_fn,\n        \'warmup_steps\': self.warmup_steps,\n        \'power\': self.power,\n        \'name\': self.name\n    }\n\n\ndef create_optimizer(init_lr, num_train_steps, num_warmup_steps):\n  """"""Creates an optimizer with learning rate schedule.""""""\n  # Implements linear decay of the learning rate.\n  learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n      initial_learning_rate=init_lr,\n      decay_steps=num_train_steps,\n      end_learning_rate=0.0)\n  if num_warmup_steps:\n    learning_rate_fn = WarmUp(initial_learning_rate=init_lr,\n                              decay_schedule_fn=learning_rate_fn,\n                              warmup_steps=num_warmup_steps)\n  optimizer = AdamWeightDecay(\n      learning_rate=learning_rate_fn,\n      weight_decay_rate=0.01,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=[\'layer_norm\', \'bias\'])\n  return optimizer\n\n\nclass AdamWeightDecay(tf.keras.optimizers.Adam):\n  """"""Adam enables L2 weight decay and clip_by_global_norm on gradients.\n\n  Just adding the square of the weights to the loss function is *not* the\n  correct way of using L2 regularization/weight decay with Adam, since that will\n  interact with the m and v parameters in strange ways.\n\n  Instead we want ot decay the weights in a manner that doesn\'t interact with\n  the m/v parameters. This is equivalent to adding the square of the weights to\n  the loss with plain (non-momentum) SGD.\n  """"""\n\n  def __init__(self,\n               learning_rate=0.001,\n               beta_1=0.9,\n               beta_2=0.999,\n               epsilon=1e-7,\n               amsgrad=False,\n               weight_decay_rate=0.0,\n               include_in_weight_decay=None,\n               exclude_from_weight_decay=None,\n               name=\'AdamWeightDecay\',\n               **kwargs):\n    super(AdamWeightDecay, self).__init__(\n        learning_rate, beta_1, beta_2, epsilon, amsgrad, name, **kwargs)\n    self.weight_decay_rate = weight_decay_rate\n    self._include_in_weight_decay = include_in_weight_decay\n    self._exclude_from_weight_decay = exclude_from_weight_decay\n\n  @classmethod\n  def from_config(cls, config):\n    """"""Creates an optimizer from its config with WarmUp custom object.""""""\n    custom_objects = {\'WarmUp\': WarmUp}\n    return super(AdamWeightDecay, cls).from_config(\n        config, custom_objects=custom_objects)\n\n  def _prepare_local(self, var_device, var_dtype, apply_state):\n    super(AdamWeightDecay, self)._prepare_local(var_device, var_dtype,\n                                                apply_state)\n    apply_state[\'weight_decay_rate\'] = tf.constant(\n        self.weight_decay_rate, name=\'adam_weight_decay_rate\')\n\n  def _decay_weights_op(self, var, learning_rate, apply_state):\n    do_decay = self._do_use_weight_decay(var.name)\n    if do_decay:\n      return var.assign_sub(\n          learning_rate * var *\n          apply_state[\'weight_decay_rate\'],\n          use_locking=self._use_locking)\n    return tf.no_op()\n\n  def apply_gradients(self, grads_and_vars, name=None):\n    grads, tvars = list(zip(*grads_and_vars))\n    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n    return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars))\n\n  def _get_lr(self, var_device, var_dtype, apply_state):\n    """"""Retrieves the learning rate with the given state.""""""\n    if apply_state is None:\n      return self._decayed_lr_t[var_dtype], {}\n\n    apply_state = apply_state or {}\n    coefficients = apply_state.get((var_device, var_dtype))\n    if coefficients is None:\n      coefficients = self._fallback_apply_state(var_device, var_dtype)\n      apply_state[(var_device, var_dtype)] = coefficients\n\n    return coefficients[\'lr_t\'], dict(apply_state=apply_state)\n\n  def _resource_apply_dense(self, grad, var, apply_state=None):\n    lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n    decay = self._decay_weights_op(var, lr_t, apply_state)\n    with tf.control_dependencies([decay]):\n      return super(AdamWeightDecay, self)._resource_apply_dense(\n          grad, var, **kwargs)\n\n  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n    lr_t, kwargs = self._get_lr(var.device, var.dtype.base_dtype, apply_state)\n    decay = self._decay_weights_op(var, lr_t, apply_state)\n    with tf.control_dependencies([decay]):\n      return super(AdamWeightDecay, self)._resource_apply_sparse(\n          grad, var, indices, **kwargs)\n\n  def get_config(self):\n    config = super(AdamWeightDecay, self).get_config()\n    config.update({\n        \'weight_decay_rate\': self.weight_decay_rate,\n    })\n    return config\n\n  def _do_use_weight_decay(self, param_name):\n    """"""Whether to use L2 weight decay for `param_name`.""""""\n    if self.weight_decay_rate == 0:\n      return False\n\n    if self._include_in_weight_decay:\n      for r in self._include_in_weight_decay:\n        if re.search(r, param_name) is not None:\n          return True\n\n    if self._exclude_from_weight_decay:\n      for r in self._exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n'"
ktrain/lroptimize/sgdr.py,0,"b""from ..imports import *\n\nclass SGDRScheduler(Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-7,\n                                     max_lr=1e-1,\n                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n                                     lr_decay=0.9,\n                                     cycle_length=1,\n                                     mult_factor=2)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n    # References\n        Original paper: http://arxiv.org/abs/1608.03983\n        Blog Post:      http://www.jeremyjordan.me/nn-learning-rate/\n    '''\n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 steps_per_epoch,\n                 lr_decay=0.9,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        super(Callback, self).__init__()\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n\n        self.steps_per_epoch = steps_per_epoch\n\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n        self.history = {}\n\n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.max_lr)\n\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n        #print(K.eval(self.model.optimizer.lr))\n\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        #print(K.eval(self.model.optimizer.lr))\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            # no longer needed as kauto completes cycles/epochs\n            #self.best_weights = self.model.get_weights()\n\n\n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        # no longer needed as kauto completes cycles/epochs\n        #self.model.set_weights(self.best_weights)\n        pass\n\n\n    def on_epoch_begin(self, epoch, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n"""
ktrain/lroptimize/triangular.py,0,"b'from ..imports import * \nfrom .. import utils as U\n\nclass CyclicLR(Callback):\n    """"""This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    ""triangular"":\n        A basic triangular cycle w/ no amplitude scaling.\n    ""triangular2"":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    ""exp_range"":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode=\'triangular\')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode=\'cycle\')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default \'triangular\'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in \'exp_range\' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {\'cycle\', \'iterations\'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is \'cycle\'.\n        reduce_on_plateau (int): LR will be reduced after this many\n                                 epochs with no improvement on validation loss.\n                                 If zero or None, no reduction will take place\n        reduce_factor(int):      LR is reduced by this factor (e.g., 2 = 1/2  = 0.5)\n        monitor (str):           Value to monitor when reducing LR\n        max_momentum(float):     maximum momentum when momentum is cycled \n                                 If both max_momentum and min_momentum is None,\n                                 default momentum for Adam is used.\n                                 (only used if optimizer is Adam)\n        min_momentum(float):     minimum momentum when momentum is cycled\n                                 If both max_momentum and min_momentum is None,\n                                 default momentum for Adam is used.\n                                 (only used if optimizer is Adam)\n        verbose (bool):          If True, will print information on LR reduction\n    References:\n        Original Paper: https://arxiv.org/abs/1803.09820\n        Blog Post: https://sgugger.github.io/the-1cycle-policy.html\n        Code Reference: https://github.com/bckenstler/CLR\n    """"""\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode=\'triangular\',\n                 gamma=1., scale_fn=None, scale_mode=\'cycle\', \n                 reduce_on_plateau=0, monitor=\'val_loss\', reduce_factor=2, \n                 max_momentum=0.95, min_momentum=0.85, verbose=1):\n        super(Callback, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == \'triangular\':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'triangular2\':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = \'cycle\'\n            elif self.mode == \'exp_range\':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = \'iterations\'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        # restoring weights due to CRF bug\n        self.best_weights = None\n\n        # LR reduction\n        self.verbose = verbose\n        self.patience = reduce_on_plateau\n        self.factor = 1./reduce_factor\n        self.monitor = monitor\n        if \'acc\' not in self.monitor:\n            self.monitor_op = lambda a, b: np.less(a, b)\n            self.best = np.Inf\n        else:\n            self.monitor_op = lambda a, b: np.greater(a, b)\n            self.best = -np.Inf\n\n        # annihalting LR\n        self.overhump = False\n\n        # cyclical momentum\n        self.max_momentum = max_momentum\n        self.min_momentum = min_momentum\n        if self.min_momentum is None and self.max_momentum:\n            self.min_momentum = self.max_momentum\n        elif self.min_momentum and self.max_momentum is None:\n            self.max_momentum = self.min_momentum\n        self.cycle_momentum = True if self.max_momentum is not None else False\n\n        self._reset()\n\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        """"""Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        """"""\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == \'cycle\':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n\n        self.orig_base_lr = self.base_lr\n\n    def on_batch_end(self, batch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault(\'lr\', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault(\'iterations\', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        # annihilate learning rate\n        prev_overhump = self.overhump\n        iterations = (self.clr_iterations+1) % (self.step_size*2)\n        if iterations/self.step_size > 1: \n            self.overhump = True\n        else:\n            self.overhump = False\n        if not prev_overhump and self.overhump:\n            self.base_lr = self.max_lr/1000\n        elif prev_overhump and not self.overhump:\n            self.base_lr = self.orig_base_lr\n\n        # set momentum\n        if self.cycle_momentum:\n            if self.overhump:\n                current_percentage = 1. - ((iterations - self.step_size) / float(\n                                            self.step_size))\n                new_momentum = self.max_momentum - current_percentage * (\n                    self.max_momentum - self.min_momentum)\n            else:\n                current_percentage = iterations / float(self.step_size)\n                new_momentum = self.max_momentum - current_percentage * (\n                    self.max_momentum - self.min_momentum)\n            K.set_value(self.model.optimizer.beta_1, new_momentum)\n            self.history.setdefault(\'momentum\', []).append(K.get_value(self.model.optimizer.beta_1))\n\n\n    def on_epoch_end(self, epoch, logs=None):\n        #print(K.eval(self.model.optimizer.lr))\n\n        # Stop training if training loss becomes zero or negative\n        # to address bug in keras_contrib code for CRF.\n        # We restore the weights from previous best epoch\n        # rather than this epoch.\n        crf = U.is_crf(self.model)\n        if crf:\n            current_loss = logs.get(\'loss\')\n            current_val_loss = logs.get(\'val_loss\', None)\n            if (current_loss is not None and current_loss <= 0.0) or\\\n                    (current_val_loss is not None and current_val_loss <= 0.0):\n                self.model.stop_training = True\n                if crf and self.best_weights is not None:\n                    if self.verbose > 0:\n                        print(\'Restoring model weights from the end of \'\n                              \'the best epoch\')\n                    self.model.set_weights(self.best_weights)\n                return\n\n\n        if self.patience:\n            current = logs.get(self.monitor)\n            if current is None:\n                raise Exception(\'cannot monitor %s\' % (self.monitor))\n            if self.monitor_op(current, self.best):\n                self.best = current\n                self.wait = 0\n                if crf:\n                    self.best_weights = self.model.get_weights()\n            else:\n                self.wait += 1\n                if self.wait >= self.patience:\n                    min_lr = 1e-7\n                    current_lr = float(K.get_value(self.model.optimizer.lr))\n                    if self.max_lr > min_lr:\n                        self.base_lr = self.base_lr * self.factor\n                        self.max_lr = self.max_lr * self.factor\n                        new_lr = current_lr * self.factor\n                        new_lr = max(new_lr, min_lr)\n                        K.set_value(self.model.optimizer.lr, new_lr)\n                        if self.verbose:\n                            print(\'\\nEpoch %05d: Reducing Max LR on Plateau: \'\n                                  \'new max lr will be %s (if not early_stopping).\' % (epoch + 1, self.max_lr))\n                        self.wait = 0\n'"
ktrain/tests/test_chinese_text.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\n\nimport ktrain\nfrom ktrain import text as txt\nTEST_DOC = \'\xe8\xbf\x98\xe5\xa5\xbd\xef\xbc\x8c\xe5\xba\x8a\xe5\xbe\x88\xe5\xa4\xa7\xe8\x80\x8c\xe4\xb8\x94\xe5\xbe\x88\xe5\xb9\xb2\xe5\x87\x80\xef\xbc\x8c\xe5\x89\x8d\xe5\x8f\xb0\xe5\xbe\x88\xe5\x8f\x8b\xe5\xa5\xbd\xef\xbc\x8c\xe5\xbe\x88\xe6\xbb\xa1\xe6\x84\x8f\xef\xbc\x8c\xe4\xb8\x8b\xe6\xac\xa1\xe8\xbf\x98\xe6\x9d\xa5\xe3\x80\x82\'\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\n\n\nclass TestTextClassification(TestCase):\n\n\n    def test_fasttext_chinese(self):\n        trn, val, preproc = txt.texts_from_csv(\'./text_data/chinese_hotel_reviews.csv\',\n                      \'content\',\n                      label_columns = [""pos"", ""neg""],\n                      max_features=30000, maxlen=75,\n                      preprocess_mode=\'standard\', sep=\'|\')\n        model = txt.text_classifier(\'fasttext\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32)\n        lr = 5e-3\n        hist = learner.autofit(lr, 10)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.85)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val[0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(),None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate(class_names=preproc.get_classes())\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'pos\')\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        self.assertEqual(p.predict(TEST_DOC), \'pos\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 0)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n        #self.assertEqual(type(p.explain(TEST_DOC)), type(None))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_dataloading.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nfrom unittest import TestCase, main, skip\nimport ktrain\nfrom ktrain import text as txt\nfrom ktrain import vision as vis\nfrom ktrain import utils as U\n\n\ndef texts_from_folder(preprocess_mode=\'standard\'):\n    DATADIR = \'./text_data/text_folder\'\n    trn, val, preproc = txt.texts_from_folder(DATADIR, \n                                                    max_features=100, maxlen=10, \n                                                    ngram_range=3, \n                                                    classes=[\'pos\', \'neg\'], \n                                                    train_test_names = [\'train\', \'test\'],\n                                                    preprocess_mode=preprocess_mode)\n\n    return (trn, val, preproc)\n\n\ndef texts_from_csv(preprocess_mode=\'standard\'):\n    DATA_PATH = \'./text_data/texts.csv\'\n    trn, val, preproc = txt.texts_from_csv(DATA_PATH,\n                          \'text\',\n                          val_filepath = DATA_PATH,\n                          label_columns = [""neg"", ""pos""],\n                          max_features=100, maxlen=10,\n                          ngram_range=3,\n                          preprocess_mode=preprocess_mode)\n    return (trn, val, preproc)\n\n\ndef texts_from_csv_string(preprocess_mode=\'standard\'):\n    DATA_PATH = \'./text_data/texts-strings.csv\'\n    trn, val, preproc = txt.texts_from_csv(DATA_PATH,\n                          \'text\',\n                          val_filepath = DATA_PATH,\n                          label_columns = ""label"",\n                          max_features=100, maxlen=10,\n                          ngram_range=3,\n                          preprocess_mode=preprocess_mode)\n    return (trn, val, preproc)\n\ndef texts_from_csv_int(preprocess_mode=\'standard\'):\n    DATA_PATH = \'./text_data/texts-ints.csv\'\n    trn, val, preproc = txt.texts_from_csv(DATA_PATH,\n                          \'text\',\n                          val_filepath = DATA_PATH,\n                          label_columns = [""label""],\n                          max_features=100, maxlen=10,\n                          ngram_range=3,\n                          preprocess_mode=preprocess_mode)\n    return (trn, val, preproc)\n\n\ndef entities_from_conll2003():\n    TDATA = \'conll2003/train.txt\'\n    VDATA = \'conll2003/valid.txt\'\n    (trn, val, preproc) = txt.entities_from_conll2003(TDATA, val_filepath=VDATA)\n    return (trn, val, preproc)\n\n\ndef images_from_folder():\n    (trn, val, preproc) = vis.images_from_folder(\n                                                  datadir=\'image_data/image_folder\',\n                                                  data_aug=vis.get_data_aug(horizontal_flip=True), \n                                                  classes=[\'cat\', \'dog\'],\n                                                  train_test_names=[\'train\', \'valid\'])\n    return (trn, val, preproc)\n\n\ndef images_from_csv():\n    train_fpath = \'./image_data/train-vision.csv\'\n    val_fpath = \'./image_data/valid-vision.csv\'\n    trn, val, preproc = vis.images_from_csv(\n                          train_fpath,\n                          \'filename\',\n                          directory=\'./image_data/image_folder/all\',\n                          val_filepath = val_fpath,\n                          label_columns = [\'cat\', \'dog\'], \n                          data_aug=vis.get_data_aug(horizontal_flip=True))\n    return (trn, val, preproc)\n\n\ndef images_from_fname():\n    trn, val, preproc = vis.images_from_fname(\n                          \'./image_data/image_folder/all\',\n                          pattern=r\'([^/]+)\\.\\d+.jpg$\',\n                          val_pct=0.25, random_state=42,\n                          data_aug=vis.get_data_aug(horizontal_flip=True))\n    return (trn, val, preproc)\n\n\n\nclass TestTextData(TestCase):\n\n    def test_texts_from_folder_standard(self):\n        (trn, val, preproc)  = texts_from_folder()\n        self.__test_texts_standard(trn, val, preproc)\n\n\n    def test_texts_from_csv_standard(self):\n        (trn, val, preproc)  = texts_from_csv()\n        self.__test_texts_standard(trn, val, preproc)\n\n    def test_texts_from_csv_string_standard(self):\n        (trn, val, preproc)  = texts_from_csv_string()\n        self.__test_texts_standard(trn, val, preproc)\n\n    def test_texts_from_csv_int_standard(self):\n        (trn, val, preproc)  = texts_from_csv_int()\n        self.__test_texts_standard(trn, val, preproc)\n\n\n    def test_texts_from_folder_bert(self):\n        (trn, val, preproc)  = texts_from_folder(preprocess_mode=\'bert\')\n        self.__test_texts_bert(trn, val, preproc)\n\n\n    def test_texts_from_csv_bert(self):\n        (trn, val, preproc)  = texts_from_csv(preprocess_mode=\'bert\')\n        self.__test_texts_bert(trn, val, preproc)\n\n\n    def test_texts_from_csv_string_bert(self):\n        (trn, val, preproc)  = texts_from_csv_string(preprocess_mode=\'bert\')\n        self.__test_texts_bert(trn, val, preproc)\n\n    def test_texts_from_csv_int_bert(self):\n        (trn, val, preproc)  = texts_from_csv_int(preprocess_mode=\'bert\')\n        self.__test_texts_bert(trn, val, preproc)\n\n\n    def __test_texts_standard(self, trn, val, preproc):\n        self.assertFalse(U.is_iter(trn))\n        self.assertEqual(trn[0].shape, (4, 10))\n        self.assertEqual(trn[1].shape, (4, 2))\n        self.assertEqual(val[0].shape, (4, 10))\n        self.assertEqual(val[1].shape, (4, 2))\n        self.assertFalse(U.is_multilabel(trn))\n        self.assertEqual(U.shape_from_data(trn), (4, 10))\n        self.assertFalse(U.ondisk(trn))\n        self.assertEqual(U.nsamples_from_data(trn), 4)\n        self.assertEqual(U.nclasses_from_data(trn), 2)\n        self.assertEqual(U.y_from_data(trn).shape, (4,2))\n        self.assertFalse(U.bert_data_tuple(trn))\n        self.assertEqual(preproc.get_classes(), preproc.get_classes())\n        self.assertEqual(preproc.ngram_count(), 3)\n        self.assertEqual(preproc.preprocess([\'hello book\'])[0][-1], 1)\n        self.assertEqual(preproc.preprocess([\'hello book\']).shape, (1, 10))\n        self.assertEqual(preproc.undo(val[0][0]), \'the book is bad\')\n\n\n    def __test_texts_bert(self, trn, val, preproc):\n        self.assertFalse(U.is_iter(trn))\n        self.assertEqual(trn[0][0].shape, (4, 10))\n        self.assertEqual(trn[1].shape, (4, 2))\n        self.assertEqual(val[0][0].shape, (4, 10))\n        self.assertEqual(val[1].shape, (4, 2))\n        self.assertFalse(U.is_multilabel(trn))\n        self.assertEqual(U.shape_from_data(trn), (4, 10))\n        self.assertFalse(U.ondisk(trn))\n        self.assertEqual(U.nsamples_from_data(trn), 4)\n        self.assertEqual(U.nclasses_from_data(trn), 2)\n        self.assertEqual(U.y_from_data(trn).shape, (4,2))\n        self.assertTrue(U.bert_data_tuple(trn))\n        self.assertEqual(preproc.get_classes(), preproc.get_classes())\n        self.assertEqual(preproc.preprocess([\'hello book\'])[0][0][0], 101)\n        self.assertEqual(preproc.preprocess([\'hello book\'])[0].shape, (1, 10))\n        self.assertEqual(preproc.undo(val[0][0][0]), \'[CLS] the book is bad . [SEP]\')\n\n\n\nclass TestNERData(TestCase):\n\n    def test_entities_from_conll2003(self):\n        (trn, val, preproc)  = entities_from_conll2003()\n        self.__test_ner(trn, val, preproc)\n\n\n\n    def __test_ner(self, trn, val, preproc):\n        self.assertTrue(U.is_iter(trn))\n        self.assertTrue(U.is_ner(data=trn))\n        self.assertFalse(U.is_multilabel(trn))\n        self.assertEqual(U.shape_from_data(trn), (14041, 47))\n        self.assertFalse(U.ondisk(trn))\n        self.assertEqual(U.nsamples_from_data(trn), 14041)\n        self.assertEqual(U.nclasses_from_data(trn), 10)\n        self.assertEqual(len(U.y_from_data(trn)), 14041)\n        self.assertFalse(U.bert_data_tuple(trn))\n        self.assertEqual(preproc.get_classes(), [\'<pad>\', \'O\', \'B-LOC\', \'B-PER\', \'B-ORG\', \'I-PER\', \n                                                 \'I-ORG\', \'B-MISC\', \'I-LOC\', \'I-MISC\'])\n        nerseq = preproc.preprocess([\'hello world\'])\n        self.assertEqual(len(nerseq), 1)\n        self.assertEqual(nerseq[0][0][0][0].tolist(), [21010, 100])\n\n\nclass TestImageData(TestCase):\n\n    def test_images_from_folder(self):\n        (trn, val, preproc)  = images_from_folder()\n        self.__test_images(trn, val, preproc)\n\n\n    def test_images_from_csv(self):\n        (trn, val, preproc)  = images_from_csv()\n        self.__test_images(trn, val, preproc)\n\n    def test_images_from_fname(self):\n        (trn, val, preproc)  = images_from_fname()\n        self.__test_images(trn, val, preproc, nsamples=20)\n\n\n    def __test_images(self, trn, val, preproc, nsamples=16):\n        self.assertTrue(U.is_iter(trn))\n        self.assertEqual(U.shape_from_data(trn), (224, 224, 3))\n        self.assertTrue(U.ondisk(trn))\n        self.assertEqual(U.nsamples_from_data(trn), nsamples)\n        self.assertEqual(U.nclasses_from_data(trn), 2)\n        self.assertEqual(U.y_from_data(trn).shape, (nsamples,2))\n        self.assertFalse(U.bert_data_tuple(trn))\n        self.assertEqual(preproc.get_classes(), [\'cat\', \'dog\'])\n        (gen, steps)  = preproc.preprocess(\'./image_data/image_folder/all\')\n        self.assertEqual(type(gen).__name__, \'DirectoryIterator\')\n        self.assertEqual(steps, 1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_imageclassification.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain image classification flows\n""""""\nimport testenv\nfrom unittest import TestCase, main, skip\nimport numpy as np\n\n\n\nimport ktrain\nfrom ktrain import vision as vis\nimport ktrain.utils as U\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\n\n\n#def classify_from_csv():\n    #train_fpath = \'./image_data/train-vision.csv\'\n    #val_fpath = \'./image_data/valid-vision.csv\'\n    #trn, val, preproc = vis.images_from_csv(\n                          #train_fpath,\n                          #\'filename\',\n                          #directory=\'./image_data/image_folder/all\',\n                          #val_filepath = val_fpath,\n                          #label_columns = [\'cat\', \'dog\'], \n                          #data_aug=vis.get_data_aug(horizontal_flip=True))\n    #print(vars(trn))\n    #model = vis.image_classifier(\'pretrained_resnet50\', trn, val)\n    #learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=1)\n    #learner.freeze()\n    #hist = learner.autofit(1e-3, 10)\n    #return hist\n\n\nclass TestImageClassification(TestCase):\n    #@skip(\'temporarily disabled\')\n    def test_folder(self):\n        (trn, val, preproc) = vis.images_from_folder(\n                                                      datadir=\'image_data/image_folder\',\n                                                      data_aug=vis.get_data_aug(horizontal_flip=True), \n                                                      classes=[\'cat\', \'dog\'],\n                                                      train_test_names=[\'train\', \'valid\'])\n        model = vis.image_classifier(\'pretrained_resnet50\', trn, val)\n        learner = ktrain.get_learner(model=model, train_data=trn, val_data=val, batch_size=1)\n        learner.freeze()\n\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # train\n        hist = learner.autofit(1e-3, monitor=VAL_ACC_NAME)\n\n        # test train\n        self.assertAlmostEqual(max(hist.history[\'lr\']), 1e-3)\n        if max(hist.history[ACC_NAME]) == 0.5:\n            raise Exception(\'unlucky initialization: please run test again\')\n        self.assertGreater(max(hist.history[ACC_NAME]), 0.8)\n\n        # test top_losses\n        obs = learner.top_losses(n=1, val_data=val)\n        print(obs)\n        if obs:\n            self.assertIn(obs[0][0], list(range(U.nsamples_from_data(val))))\n        else:\n            self.assertEqual(max(hist.history[VAL_ACC_NAME]), 1)\n\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate(val_data=val)\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc)\n        r = p.predict_folder(\'image_data/image_folder/train/\')\n        print(r)\n        self.assertEqual(r[0][1], \'cat\')\n        r = p.predict_proba_folder(\'image_data/image_folder/train/\')\n        self.assertEqual(np.argmax(r[0][1]), 0)\n        r = p.predict_filename(\'image_data/image_folder/train/cat/cat.11737.jpg\')\n        self.assertEqual(r, [\'cat\'])\n        r = p.predict_proba_filename(\'image_data/image_folder/train/cat/cat.11737.jpg\')\n        self.assertEqual(np.argmax(r), 0)\n\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        r = p.predict_filename(\'image_data/image_folder/train/cat/cat.11737.jpg\')\n        self.assertEqual(r, [\'cat\'])\n\n\n    @skip(\'temporarily disabled\')\n    def test_csv(self):\n        train_fpath = \'./image_data/train-vision.csv\'\n        val_fpath = \'./image_data/valid-vision.csv\'\n        trn, val, preproc = vis.images_from_csv(\n                              train_fpath,\n                              \'filename\',\n                              directory=\'./image_data/image_folder/all\',\n                              val_filepath = val_fpath,\n                              label_columns = [\'cat\', \'dog\'], \n                              data_aug=vis.get_data_aug(horizontal_flip=True))\n\n        lr = 1e-4\n        model = vis.image_classifier(\'pretrained_resnet50\', trn, val)\n        learner = ktrain.get_learner(model=model, train_data=trn, val_data=val, batch_size=4)\n        learner.freeze()\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n\n        # train\n        hist = learner.fit_onecycle(lr, 3)\n\n        # test train\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        if max(hist.history[ACC_NAME]) == 0.5:\n            raise Exception(\'unlucky initialization: please run test again\')\n        self.assertGreater(max(hist.history[ACC_NAME]), 0.8)\n\n        # test top_losses\n        obs = learner.top_losses(n=1, val_data=val)\n        print(obs)\n        if obs:\n            self.assertIn(obs[0][0], list(range(U.nsamples_from_data(val))))\n        else:\n            self.assertEqual(max(hist.history[VAL_ACC_NAME]), 1)\n\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate(val_data=val)\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc)\n        r = p.predict_folder(\'image_data/image_folder/train/\')\n        print(r)\n        self.assertEqual(r[0][1], \'cat\')\n        r = p.predict_proba_folder(\'image_data/image_folder/train/\')\n        self.assertEqual(np.argmax(r[0][1]), 0)\n        r = p.predict_filename(\'image_data/image_folder/train/cat/cat.11737.jpg\')\n        self.assertEqual(r, [\'cat\'])\n        r = p.predict_proba_filename(\'image_data/image_folder/train/cat/cat.11737.jpg\')\n        self.assertEqual(np.argmax(r), 0)\n\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        r = p.predict_filename(\'image_data/image_folder/train/cat/cat.11737.jpg\')\n        self.assertEqual(r, [\'cat\'])\n\n\n    #@skip(\'temporarily disabled\')\n    def test_array(self):\n\n        from tensorflow.keras.datasets import mnist\n        from tensorflow.keras.utils import to_categorical\n        import numpy as np\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n        x_train = x_train.astype(\'float32\')\n        x_test = x_test.astype(\'float32\')\n        x_train /= 255\n        x_test /= 255\n        x_train = np.expand_dims(x_train, axis=3)\n        x_test = np.expand_dims(x_test, axis=3)\n        y_train = to_categorical(y_train)\n        y_test = to_categorical(y_test)\n\n\n        classes = [\'zero\', \'one\', \'two\', \'three\', \'four\', \'five\', \'six\', \'seven\', \'eight\', \'nine\']\n        data_aug = vis.get_data_aug(  rotation_range=15,\n                                      zoom_range=0.1,\n                                      width_shift_range=0.1,\n                                      height_shift_range=0.1,\n                                      featurewise_center=False, \n                                       featurewise_std_normalization=False,)\n\n        (trn, val, preproc) = vis.images_from_array(x_train, y_train, \n                                                    validation_data=(x_test, y_test),\n                                                    data_aug=data_aug,\n                                                    class_names=classes)\n\n        model = vis.image_classifier(\'default_cnn\', trn, val)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=128)\n        hist = learner.fit_onecycle(1e-3, 1)\n\n\n\n\n\n        # test train\n        self.assertAlmostEqual(max(hist.history[\'lr\']), 1e-3)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.97)\n\n        # test top_losses\n        obs = learner.top_losses(n=1, val_data=val)\n        print(obs)\n        if obs:\n            self.assertIn(obs[0][0], list(range(U.nsamples_from_data(val))))\n        else:\n            self.assertEqual(max(hist.history[VAL_ACC_NAME]), 1)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate(val_data=val)\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        p = ktrain.get_predictor(learner.model, preproc)\n        r = p.predict(x_test[0:1])\n        print(r)\n        self.assertEqual(r[0], \'seven\')\n        r = p.predict(x_test[0:1], return_proba=True)\n        self.assertEqual(np.argmax(r[0]), 7)\n\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        r = p.predict(x_test[0:1])\n        self.assertEqual(r[0], \'seven\')\n\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
ktrain/tests/test_lda.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport ktrain\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\n\nclass TestLDA(TestCase):\n\n\n    def test_qa(self):\n        rawtext = """"""\n            Elon Musk leads Space Exploration Technologies (SpaceX), where he oversees\n            the development and manufacturing of advanced rockets and spacecraft for missions\n            to and beyond Earth orbit.\n            """"""\n        \n        # collect data\n        import numpy as np\n        import pandas as pd\n        from sklearn.datasets import fetch_20newsgroups\n        remove = (\'headers\', \'footers\', \'quotes\')\n        newsgroups_train = fetch_20newsgroups(subset=\'train\', remove=remove)\n        newsgroups_test = fetch_20newsgroups(subset=\'test\', remove=remove)\n        texts = newsgroups_train.data +  newsgroups_test.data\n\n        # buld and test LDA topic model\n        tm = ktrain.text.get_topic_model(texts, n_features=10000)\n        tm.build(texts, threshold=0.25)\n        texts = tm.filter(texts)\n        tags = tm.topics[ np.argmax(tm.predict([rawtext]))]\n        self.assertEqual(tags, \'space nasa earth data launch surface solar moon mission planet\')\n        tm.save(\'/tmp/tm\')\n        tm = ktrain.text.load_topic_model(\'/tmp/tm\')\n        tm.build(texts, threshold=0.25)\n        tags = tm.topics[ np.argmax(tm.predict([rawtext]))]\n        self.assertEqual(tags, \'space nasa earth data launch surface solar moon mission planet\')\n\n        # document similarity\n        tech_topics = [51, 85, 94, 22]\n        tech_probs = tm.get_doctopics(topic_ids=tech_topics)\n        doc_ids = [doc[1] for doc in tm.get_docs(topic_ids=tech_topics)]\n        tm.train_scorer(topic_ids=tech_topics)\n        other_topics = [i for i in range(tm.n_topics) if i not in tech_topics]\n        other_texts = [d[0] for d in tm.get_docs(topic_ids=other_topics)]\n        other_scores = tm.score(other_texts)\n        # display results in Pandas dataframe\n        other_preds = [int(score > 0) for score in other_scores]\n        data = sorted(list(zip(other_preds, other_scores, other_texts)), key=lambda item:item[1], reverse=True)\n        df = pd.DataFrame(data, columns=[\'Prediction\', \'Score\', \'Text\'])\n        self.assertTrue(\'recommendations for a laser printer\' in df[\'Text\'].values[0])\n\n        # recommender\n        tm.train_recommender()\n        results = tm.recommend(text=rawtext, n=1)\n        self.assertTrue(results[0][0].startswith(\'Archive-name\'))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_linkpred.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport ktrain\nfrom ktrain import graph as gr\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\n\nclass TestNodeClassification(TestCase):\n\n\n    def test_cora(self):\n        \n        (trn, val, preproc) = gr.graph_links_from_csv(\n                                                      \'graph_data/cora/cora.content\', \n                                                      \'graph_data/cora/cora.cites\',  \n                                                       sep=\'\\t\')\n\n\n        \n        learner = ktrain.get_learner(model=gr.graph_link_predictor(\'graphsage\', trn, preproc),\n                                     train_data=trn, val_data=val)\n\n\n\n        lr = 0.01\n        hist = learner.fit_onecycle(lr, 5)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.78)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=val)\n        self.assertIn(obs[0][0], list(range(val.targets.shape[0])))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=val)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        learner.validate(val_data=val)\n        cm = learner.validate(val_data=val)\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc)\n        self.assertIn(p.predict(preproc.G, list(preproc.G.edges()))[:5][0], preproc.get_classes())\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        self.assertEqual(p.predict(preproc.G, list(preproc.G.edges()))[:5][0], preproc.get_classes()[1])\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_multilabel.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport numpy as np\nfrom unittest import TestCase, main, skip\nimport ktrain\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\nfrom ktrain import utils as U\n\n\nSequential = ktrain.imports.Sequential\nDense = ktrain.imports.Dense\nEmbedding = ktrain.imports.Embedding\nGlobalAveragePooling1D = ktrain.imports.GlobalAveragePooling1D\n\ndef synthetic_multilabel():\n\n\n\n    # data\n    X = [[1,0,0,0,0,0,0],\n          [1,2,0,0,0,0,0],\n          [3,0,0,0,0,0,0],\n          [3,4,0,0,0,0,0],\n          [2,0,0,0,0,0,0],\n          [3,0,0,0,0,0,0],\n          [4,0,0,0,0,0,0],\n          [2,3,0,0,0,0,0],\n          [1,2,3,0,0,0,0],\n          [1,2,3,4,0,0,0],\n          [0,0,0,0,0,0,0],\n          [1,1,2,3,0,0,0],\n          [2,3,3,4,0,0,0],\n          [4,4,1,1,2,0,0],\n          [1,2,3,3,3,3,3],\n          [2,4,2,4,2,0,0],\n          [1,3,3,3,0,0,0],\n          [4,4,0,0,0,0,0],\n          [3,3,0,0,0,0,0],\n          [1,1,4,0,0,0,0]]\n                                                                    \n    Y = [[1,0,0,0],                                                     \n        [1,1,0,0],                                                      \n        [0,0,1,0],                                                      \n        [0,0,1,1],\n        [0,1,0,0],\n        [0,0,1,0],\n        [0,0,0,1],\n        [0,1,1,0],\n        [1,1,1,0],\n        [1,1,1,1],\n        [0,0,0,0],\n        [1,1,1,0],\n        [0,1,1,1],\n        [1,1,0,1],\n        [1,1,1,0],\n        [0,1,0,0],\n        [1,0,1,0],\n        [0,0,0,1],\n        [0,0,1,0],\n        [1,0,0,1]]\n\n\n    X = np.array(X)\n    Y = np.array(Y)\n    return (X, Y)\n\n\nclass TestMultilabel(TestCase):\n\n    def test_multilabel(self):\n        X, Y = synthetic_multilabel()\n        self.assertTrue(U.is_multilabel( (X,Y)))\n        MAXLEN = 7\n        MAXFEATURES = 4\n        NUM_CLASSES = 4\n        model = Sequential()\n        model.add(Embedding(MAXFEATURES+1,\n                            50,\n                            input_length=MAXLEN))\n        model.add(GlobalAveragePooling1D())\n        model.add(Dense(NUM_CLASSES, activation=\'sigmoid\'))\n        model.compile(loss=\'binary_crossentropy\',\n                      optimizer=\'adam\',\n                      metrics=[\'accuracy\'])\n        learner = ktrain.get_learner(model, \n                                     train_data=(X, Y),\n                                     val_data=(X, Y),\n                                     batch_size=1)\n        learner.lr_find()\n        hist = learner.fit(0.001, 200)\n        learner.view_top_losses(n=5)\n        learner.validate()\n        final_acc = hist.history[VAL_ACC_NAME][-1]\n        print(\'final_accuracy:%s\' % (final_acc))\n        self.assertGreater(final_acc, 0.97)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_ner.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\n\nimport os\nos.environ[\'DISABLE_V2_BEHAVIOR\'] = \'0\'\n\nimport ktrain\nfrom ktrain import text as txt\n\nclass TestNERClassification(TestCase):\n\n    def setUp(self):\n        TDATA = \'conll2003/train.txt\'\n        (trn, val, preproc) = txt.entities_from_txt(TDATA)\n        self.trn = trn\n        self.val = val\n        self.preproc = preproc\n\n\n\n\n    def test_ner(self):\n        model = txt.sequence_tagger(\'bilstm-bert\', self.preproc, bert_model=\'bert-base-cased\')\n        learner = ktrain.get_learner(model, train_data=self.trn, val_data=self.val, batch_size=128)\n        lr = 0.01\n        hist = learner.fit(lr, 1)\n\n        # test training results\n        #self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(learner.validate(), 0.79)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1)\n        self.assertIn(obs[0][0], list(range(len(self.val.x))))\n        learner.view_top_losses(n=1)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n\n        # test predictor\n        SENT = \'There is a man named John Smith.\'\n        p = ktrain.get_predictor(learner.model,self.preproc)\n        self.assertEqual(p.predict(SENT)[-2][1], \'I-PER\' )\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        self.assertEqual(p.predict(SENT)[-2][1], \'I-PER\' )\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_ner_v1.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\n\nimport os\nos.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\n\nimport ktrain\nfrom ktrain import text as txt\n\nclass TestNERClassification(TestCase):\n\n    def setUp(self):\n        TDATA = \'conll2003/train.txt\'\n        (trn, val, preproc) = txt.entities_from_txt(TDATA, use_char=True)\n        self.trn = trn\n        self.val = val\n        self.preproc = preproc\n\n\n\n\n    def test_ner(self):\n        wv_url = \'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\'\n        model = txt.sequence_tagger(\'bilstm-crf\', self.preproc, wv_path_or_url=wv_url)\n        learner = ktrain.get_learner(model, train_data=self.trn, val_data=self.val, batch_size=128)\n        lr = 0.01\n        hist = learner.fit(lr, 1)\n\n        # test training results\n        #self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(learner.validate(), 0.65)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1)\n        self.assertIn(obs[0][0], list(range(len(self.val.x))))\n        learner.view_top_losses(n=1)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n\n        # test predictor\n        SENT = \'There is a man named John Smith.\'\n        p = ktrain.get_predictor(learner.model,self.preproc)\n        self.assertEqual(p.predict(SENT)[-2][1], \'I-PER\' )\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        self.assertEqual(p.predict(SENT)[-2][1], \'I-PER\' )\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_nodeclass.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport ktrain\nfrom ktrain import graph as gr\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\n\nclass TestNodeClassification(TestCase):\n\n\n    def test_cora(self):\n        \n        (trn, val, preproc, \n        df_holdout, G_complete)  = gr.graph_nodes_from_csv(\n                                                            \'graph_data/cora/cora.content\', \n                                                             \'graph_data/cora/cora.cites\',  \n                                                             sample_size=20, \n                                                             holdout_pct=0.1, holdout_for_inductive=True,\n                                                            train_pct=0.1, sep=\'\\t\')\n\n\n        \n        learner = ktrain.get_learner(model=gr.graph_node_classifier(\'graphsage\', trn,), \n                             train_data=trn, \n                             #val_data=val, \n                             batch_size=64)\n\n\n        lr = 0.01\n        hist = learner.autofit(lr, 10)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[ACC_NAME]), 0.9)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=val)\n        self.assertIn(obs[0][0], list(range(val.targets.shape[0])))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=val)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        learner.validate(val_data=val)\n        cm = learner.validate(val_data=val)\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc)\n        self.assertIn(p.predict_transductive(val.ids[0:1])[0], preproc.get_classes())\n        p.predict_transductive(val.ids[0:1])\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        self.assertIn(p.predict_transductive(val.ids[0:1])[0], preproc.get_classes())\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_qa.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport ktrain\nfrom ktrain import text \nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\n\nclass TestQA(TestCase):\n\n\n    def test_qa(self):\n        \n        from sklearn.datasets import fetch_20newsgroups\n        remove = (\'headers\', \'footers\', \'quotes\')\n        newsgroups_train = fetch_20newsgroups(subset=\'train\', remove=remove)\n        newsgroups_test = fetch_20newsgroups(subset=\'test\', remove=remove)\n        docs = newsgroups_train.data +  newsgroups_test.data\n\n        #tmp_folder = \'/tmp/qa_test\'\n        import tempfile\n        import shutil\n        tmp_folder = tempfile.mkdtemp()\n        shutil.rmtree(tmp_folder)\n        text.SimpleQA.initialize_index(tmp_folder)\n        text.SimpleQA.index_from_list(docs, tmp_folder, commit_every=len(docs))\n        qa = text.SimpleQA(tmp_folder)\n\n        answers = qa.ask(\'When did Cassini launch?\')\n        top_answer = answers[0][\'answer\']\n        self.assertEqual(top_answer, \'in october of 1997\')\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_regression.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nfrom unittest import TestCase, main, skip\nimport ktrain\nSequential = ktrain.imports.Sequential\nDense = ktrain.imports.Dense\n\ndef bostonhousing():\n    from keras.datasets import boston_housing\n    (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n\n    model = Sequential()\n    model.add(Dense(1, input_shape=(x_train.shape[1],), activation=\'linear\'))\n    model.compile(optimizer=\'adam\', loss=\'mse\', metrics=[\'mse\', \'mae\'])\n    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n    learner.lr_find()\n    hist = learner.fit(0.05, 8, cycle_len=1, cycle_mult=2)\n    learner.view_top_losses(n=5)\n    learner.validate()\n    return hist\n\n\n\nclass TestRegression(TestCase):\n\n    def test_bostonhousing(self):\n        hist  = bostonhousing()\n        min_loss = min(hist.history[\'val_loss\'])\n        print(min_loss)\n        self.assertLess(min_loss, 55)\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_textclassification.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport ktrain\nfrom ktrain import text as txt\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\nTEST_DOC = \'god christ jesus mother mary church sunday lord heaven amen\'\nEVAL_BS = 64\n\nclass TestTextClassification(TestCase):\n\n    def setUp(self):\n        # fetch the dataset using scikit-learn\n        categories = [\'alt.atheism\', \'soc.religion.christian\',\n                     \'comp.graphics\', \'sci.med\']\n        from sklearn.datasets import fetch_20newsgroups\n        train_b = fetch_20newsgroups(subset=\'train\',\n           categories=categories, shuffle=True, random_state=42)\n        test_b = fetch_20newsgroups(subset=\'test\',\n           categories=categories, shuffle=True, random_state=42)\n        print(\'size of training set: %s\' % (len(train_b[\'data\'])))\n        print(\'size of validation set: %s\' % (len(test_b[\'data\'])))\n        print(\'classes: %s\' % (train_b.target_names))\n        x_train = train_b.data\n        y_train = train_b.target\n        x_test = test_b.data\n        y_test = test_b.target\n        self.trn = (x_train, y_train)\n        self.val = (x_test, y_test)\n        self.classes = train_b.target_names\n\n\n\n    #@skip(\'temporarily disabled\')\n    def test_fasttext(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 class_names=self.classes,\n                                                 preprocess_mode=\'standard\',\n                                                 maxlen=350, \n                                                 max_features=35000)\n        model = txt.text_classifier(\'fasttext\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32, eval_batch_size=EVAL_BS)\n        lr = 0.01\n        hist = learner.fit(lr, 10, cycle_len=1)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.8)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val[0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate()\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\', batch_size=EVAL_BS)\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n    def test_nbsvm(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 class_names=self.classes,\n                                                 preprocess_mode=\'standard\',\n                                                 maxlen=700, \n                                                 max_features=35000,\n                                                 ngram_range=3)\n        model = txt.text_classifier(\'nbsvm\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32, eval_batch_size=EVAL_BS)\n        lr = 0.01\n        hist = learner.fit_onecycle(lr, 10)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.92)\n        self.assertAlmostEqual(max(hist.history[\'momentum\']), 0.95)\n        self.assertAlmostEqual(min(hist.history[\'momentum\']), 0.85)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val[0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate()\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\', batch_size=EVAL_BS)\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_logreg(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 class_names=self.classes,\n                                                 preprocess_mode=\'standard\',\n                                                 maxlen=700, \n                                                 max_features=35000,\n                                                 ngram_range=3)\n        model = txt.text_classifier(\'logreg\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32, eval_batch_size=EVAL_BS)\n        lr = 0.01\n        hist = learner.autofit(lr)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.9)\n        self.assertAlmostEqual(max(hist.history[\'momentum\']), 0.95)\n        self.assertAlmostEqual(min(hist.history[\'momentum\']), 0.85)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val[0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate()\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\', batch_size=EVAL_BS)\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_bigru(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 class_names=self.classes,\n                                                 preprocess_mode=\'standard\',\n                                                 maxlen=350, \n                                                 max_features=35000,\n                                                 ngram_range=1)\n        model = txt.text_classifier(\'bigru\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=32, eval_batch_size=EVAL_BS)\n        lr = 0.01\n        hist = learner.autofit(lr, 1)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.9)\n        self.assertAlmostEqual(max(hist.history[\'momentum\']), 0.95)\n        self.assertAlmostEqual(min(hist.history[\'momentum\']), 0.85)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val[0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate()\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\', batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_bert(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 class_names=self.classes,\n                                                 preprocess_mode=\'bert\',\n                                                 maxlen=350, \n                                                 max_features=35000)\n        model = txt.text_classifier(\'bert\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, batch_size=6, eval_batch_size=EVAL_BS)\n        lr = 2e-5\n        hist = learner.fit_onecycle(lr, 1)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[ACC_NAME]), 0.7)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=val)\n        self.assertIn(obs[0][0], list(range(len(val[0][0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=val)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test validate\n        cm = learner.validate(val_data=val)\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\', batch_size=EVAL_BS)\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_textregression.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text regression \n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport ktrain\nfrom ktrain import text as txt\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\nTEST_DOC = """"""A wine that has created its own universe. It has a unique, special softness \n              that allies with the total purity that comes from a small, enclosed single vineyard. \n              The fruit is almost irrelevant here, because it comes as part of a much deeper complexity. \n              This is a great wine, at the summit of Champagne, a sublime, unforgettable experience.\n              """"""\n\nclass TestTextRegression(TestCase):\n\n    def setUp(self):\n        import pandas as pd\n\n        # wine price dataset should be downloaded \n        # from: https://github.com/floydhub/regression-template\n        # and prepared as described in the wide-deep.ipynb notebook\n        path = \'./text_data/wines.csv\'  \n        data = pd.read_csv(path)\n        data = data.sample(frac=1., random_state=42)\n\n        # Split data into train and test\n        train_size = int(len(data) * .8)\n        print (""Train size: %d"" % train_size)\n        print (""Test size: %d"" % (len(data) - train_size))\n\n        # Train features\n        description_train = data[\'description\'][:train_size]\n\n        # Train labels\n        labels_train = data[\'price\'][:train_size]\n\n        # Test features\n        description_test = data[\'description\'][train_size:]\n\n        # Test labels\n        labels_test = data[\'price\'][train_size:]\n\n        # dataset\n        x_train = description_train.values\n        y_train = labels_train.values\n        x_test = description_test.values\n        y_test = labels_test.values\n        self.trn = (x_train, y_train)\n        self.val = (x_test, y_test)\n\n\n\n    #@skip(\'temporarily disabled\')\n    def test_linreg(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 preprocess_mode=\'standard\',\n                                                 ngram_range=3,\n                                                 maxlen=200, \n                                                 max_features=35000)\n        model = txt.text_regression_model(\'linreg\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=256)\n        lr = 0.01\n        hist = learner.fit_onecycle(lr, 10)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertLess(min(hist.history[\'val_mae\']), 12)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val[0]))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        learner.save_model(\'/tmp/test_model\')\n        learner.load_model(\'/tmp/test_model\')\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc)\n        self.assertGreater(p.predict([TEST_DOC])[0], 100)\n        p.save(\'/tmp/test_predictor\')\n        p = ktrain.load_predictor(\'/tmp/test_predictor\')\n        self.assertGreater(p.predict([TEST_DOC])[0], 100)\n        self.assertIsNone(p.explain(TEST_DOC))\n\n\n\n    #@skip(\'temporarily disabled\')\n    def test_distilbert(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 preprocess_mode=\'distilbert\',\n                                                 maxlen=75)\n        model = txt.text_regression_model(\'distilbert\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=100)\n        lr = 5e-5\n        hist = learner.fit_onecycle(lr, 1)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertLess(min(hist.history[\'val_mae\']), 16)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val.x))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        tmp_folder = ktrain.imports.tempfile.mkdtemp()\n        learner.save_model(tmp_folder)\n        learner.load_model(tmp_folder, preproc=preproc)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=64)\n        self.assertGreater(p.predict([TEST_DOC])[0], 1)\n        tmp_folder = ktrain.imports.tempfile.mkdtemp()\n        p.save(tmp_folder)\n        p = ktrain.load_predictor(tmp_folder, batch_size=64)\n        self.assertGreater(p.predict([TEST_DOC])[0], 1)\n        self.assertIsNone(p.explain(TEST_DOC))\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_transformers.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nimport testenv\nimport IPython\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport ktrain\nfrom ktrain import text as txt\nfrom ktrain.imports import ACC_NAME, VAL_ACC_NAME\nTEST_DOC = \'god christ jesus mother mary church sunday lord heaven amen\'\nEVAL_BS = 64\n\nclass TestTransformers(TestCase):\n\n    def setUp(self):\n        # fetch the dataset using scikit-learn\n        categories = [\'alt.atheism\', \'soc.religion.christian\',\n                     \'comp.graphics\', \'sci.med\']\n        from sklearn.datasets import fetch_20newsgroups\n        train_b = fetch_20newsgroups(subset=\'train\',\n           categories=categories, shuffle=True, random_state=42)\n        test_b = fetch_20newsgroups(subset=\'test\',\n           categories=categories, shuffle=True, random_state=42)\n        print(\'size of training set: %s\' % (len(train_b[\'data\'])))\n        print(\'size of validation set: %s\' % (len(test_b[\'data\'])))\n        print(\'classes: %s\' % (train_b.target_names))\n        x_train = train_b.data\n        y_train = train_b.target\n        x_test = test_b.data\n        y_test = test_b.target\n        self.trn = (x_train, y_train)\n        self.val = (x_test, y_test)\n        self.classes = train_b.target_names\n\n\n\n    #@skip(\'temporarily disabled\')\n    def test_transformers_api_1(self):\n        trn, val, preproc = txt.texts_from_array(x_train=self.trn[0], \n                                                 y_train=self.trn[1],\n                                                 x_test=self.val[0], \n                                                 y_test=self.val[1],\n                                                 class_names=self.classes,\n                                                 preprocess_mode=\'distilbert\',\n                                                 maxlen=500, \n                                                 max_features=35000)\n        model = txt.text_classifier(\'distilbert\', train_data=trn, preproc=preproc)\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6, eval_batch_size=EVAL_BS)\n\n        # test weight decay\n        # NOTE due to transformers and/or AdamW bug, # val_accuracy is missing in training history if setting weight decay prior to training\n        #self.assertEqual(learner.get_weight_decay(), None)\n        #learner.set_weight_decay(1e-2)\n        #self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # train\n        lr = 5e-5\n        hist = learner.fit_onecycle(lr, 1)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.9)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val.x))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n\n        # test load and save model\n        tmp_folder = ktrain.imports.tempfile.mkdtemp()\n        learner.save_model(tmp_folder)\n        learner.load_model(tmp_folder, preproc=preproc)\n\n        # test validate\n        cm = learner.validate()\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        tmp_folder = ktrain.imports.tempfile.mkdtemp()\n        p.save(tmp_folder)\n        p = ktrain.load_predictor(tmp_folder, batch_size=EVAL_BS)\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_transformers_api_2(self):\n        MODEL_NAME = \'distilbert-base-uncased\'\n        preproc = txt.Transformer(MODEL_NAME, maxlen=500, classes=self.classes)\n        trn = preproc.preprocess_train(self.trn[0], self.trn[1])\n        val = preproc.preprocess_test(self.val[0], self.val[1])\n        model = preproc.get_classifier()\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6, eval_batch_size=EVAL_BS)\n        lr = 5e-5\n        hist = learner.fit_onecycle(lr, 1)\n\n        # test training results\n        self.assertAlmostEqual(max(hist.history[\'lr\']), lr)\n        self.assertGreater(max(hist.history[VAL_ACC_NAME]), 0.9)\n\n\n        # test top losses\n        obs = learner.top_losses(n=1, val_data=None)\n        self.assertIn(obs[0][0], list(range(len(val.x))))\n        learner.view_top_losses(preproc=preproc, n=1, val_data=None)\n\n        # test weight decay\n        self.assertEqual(learner.get_weight_decay(), None)\n        learner.set_weight_decay(1e-2)\n        self.assertAlmostEqual(learner.get_weight_decay(), 1e-2)\n\n        # test load and save model\n        tmp_folder = ktrain.imports.tempfile.mkdtemp()\n        learner.save_model(tmp_folder)\n        learner.load_model(tmp_folder, preproc=preproc)\n\n        # test validate\n        cm = learner.validate()\n        print(cm)\n        for i, row in enumerate(cm):\n            self.assertEqual(np.argmax(row), i)\n\n        # test predictor\n        p = ktrain.get_predictor(learner.model, preproc, batch_size=EVAL_BS)\n        self.assertEqual(p.predict([TEST_DOC])[0], \'soc.religion.christian\')\n        tmp_folder = ktrain.imports.tempfile.mkdtemp()\n        p.save(tmp_folder)\n        p = ktrain.load_predictor(tmp_folder, batch_size=EVAL_BS)\n        self.assertEqual(p.predict(TEST_DOC), \'soc.religion.christian\')\n        self.assertEqual(np.argmax(p.predict_proba([TEST_DOC])[0]), 3)\n        self.assertEqual(type(p.explain(TEST_DOC)), IPython.core.display.HTML)\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/test_zzz_shallownlp.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain shallownlp module:\n2020-05-26: renamed test_zzz_shallownlp.py because\n            causes issues for tests following it when run in conjunction with test_regression.py.\n""""""\nimport testenv\nfrom unittest import TestCase, main, skip\nimport numpy as np\n\nimport os\nos.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\nfrom ktrain.text import shallownlp as snlp\n\nclass TestShallowNLP(TestCase):\n\n\n\n    #@skip(\'temporarily disabled\')\n    def test_classifier(self):\n        categories = [\'alt.atheism\', \'soc.religion.christian\',\n                     \'comp.graphics\', \'sci.med\']\n        from sklearn.datasets import fetch_20newsgroups\n        train_b = fetch_20newsgroups(subset=\'train\',\n           categories=categories, shuffle=True, random_state=42)\n        test_b = fetch_20newsgroups(subset=\'test\',\n           categories=categories, shuffle=True, random_state=42)\n        print(\'size of training set: %s\' % (len(train_b[\'data\'])))\n        print(\'size of validation set: %s\' % (len(test_b[\'data\'])))\n        print(\'classes: %s\' % (train_b.target_names))\n        x_train = train_b.data\n        y_train = train_b.target\n        x_test = test_b.data\n        y_test = test_b.target\n        classes = train_b.target_names\n\n        clf = snlp.Classifier()\n        clf.fit(x_train, y_train, ctype=\'nbsvm\')\n        self.assertGreaterEqual(clf.evaluate(x_test, y_test), 0.93)\n        test_doc = \'god christ jesus mother mary church sunday lord heaven amen\'\n        self.assertEqual(clf.predict(test_doc), 3)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_classifier_chinese(self):\n        fpath = \'./text_data/chinese_hotel_reviews.csv\'\n        (x_train,  y_train, label_names) = snlp.Classifier.load_texts_from_csv(fpath, text_column=\'content\', label_column=\'pos\', sep=\'|\')\n        print(\'label names: %s\' % (label_names))\n        clf = snlp.Classifier()\n        clf.fit(x_train, y_train, ctype=\'nbsvm\')\n        self.assertGreaterEqual(clf.evaluate(x_train, y_train), 0.98)\n        neg_text = \'\xe6\x88\x91\xe8\xae\xa8\xe5\x8e\x8c\xe5\x92\x8c\xe9\x84\x99\xe8\xa7\x86\xe8\xbf\x99\xe5\xae\xb6\xe9\x85\x92\xe5\xba\x97\xe3\x80\x82\'\n        pos_text = \'\xe6\x88\x91\xe5\x96\x9c\xe6\xac\xa2\xe8\xbf\x99\xe5\xae\xb6\xe9\x85\x92\xe5\xba\x97\xe3\x80\x82\'\n        self.assertEqual(clf.predict(pos_text), 1)\n        self.assertEqual(clf.predict(neg_text), 0)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_ner(self):\n        ner = snlp.NER(\'en\')\n        text = """"""\n        Xuetao Cao was head of the Chinese Academy of Medical Sciences and is \n        the current president of Nankai University.\n        """"""\n        result = ner.predict(text)\n        self.assertEqual(len(result), 3)\n        self.assertEqual(result[0][1], \'PER\')\n        self.assertEqual(result[1][1], \'ORG\')\n        self.assertEqual(result[2][1], \'ORG\')\n        self.assertEqual(len(snlp.sent_tokenize(\'Paul Newman is a good actor.  Tommy Wisseau is not.\')), 2)\n\n\n        ner = snlp.NER(\'zh\')\n        text = """"""\n        \xe6\x9b\xb9\xe9\x9b\xaa\xe6\xb6\x9b\xe6\x9b\xbe\xe4\xbb\xbb\xe4\xb8\xad\xe5\x9b\xbd\xe5\x8c\xbb\xe5\xad\xa6\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe9\x99\xa2\xe9\x95\xbf\xef\xbc\x8c\xe7\x8e\xb0\xe4\xbb\xbb\xe5\x8d\x97\xe5\xbc\x80\xe5\xa4\xa7\xe5\xad\xa6\xe6\xa0\xa1\xe9\x95\xbf\xe3\x80\x82\n        """"""\n        result = ner.predict(text)\n        self.assertEqual(len(result), 3)\n        self.assertEqual(result[0][1], \'PER\')\n        self.assertEqual(result[1][1], \'ORG\')\n        self.assertEqual(result[2][1], \'ORG\')\n        self.assertEqual(len(snlp.sent_tokenize(\'\xe8\xbf\x99\xe6\x98\xaf\xe5\x85\xb3\xe4\xba\x8e\xe5\x8f\xb2\xe5\xaf\x86\xe6\x96\xaf\xe5\x8d\x9a\xe5\xa3\xab\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\xe3\x80\x82\xe7\xac\xac\xe4\xba\x8c\xe5\x8f\xa5\xe8\xaf\x9d\xe6\x98\xaf\xe5\x85\xb3\xe4\xba\x8e\xe7\x90\xbc\xe6\x96\xaf\xe5\x85\x88\xe7\x94\x9f\xe7\x9a\x84\xe3\x80\x82\')), 2)\n\n\n        ner = snlp.NER(\'ru\')\n        text = """"""\xd0\x92\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xb8\xd0\xbc\xd0\xb8\xd1\x80 \xd0\x92\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xb8\xd0\xbc\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb8\xd1\x87 \xd0\x9f\xd1\x83\xd1\x82\xd0\xb8\xd0\xbd - \xd1\x80\xd0\xbe\xd1\x81\xd1\x81\xd0\xb8\xd0\xb9\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9 \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xb8\xd1\x82\xd0\xb8\xd0\xba, \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb9 \xd1\x8f\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb7\xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x82\xd0\xbe\xd0\xbc \xd0\xa0\xd0\xbe\xd1\x81\xd1\x81\xd0\xb8\xd0\xb8 \xd1\x81 2012 \xd0\xb3\xd0\xbe\xd0\xb4\xd0\xb0.""""""\n        result = ner.predict(text)\n        self.assertEqual(len(result), 2)\n        self.assertEqual(result[0][1], \'PER\')\n        self.assertEqual(result[1][1], \'LOC\')\n\n\n    #@skip(\'temporarily disabled\')\n    def test_search(self):\n        document1 =""""""\n        Hello there,\n\n        Hope this email finds you well.\n\n        Are you available to talk about our meeting?\n\n        If so, let us plan to schedule the meeting\n        at the Hefei National Laboratory for Physical Sciences at the Microscale.\n\n        As I always say: \xd0\xb6\xd0\xb8\xd0\xb2\xd0\xb8 \xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe\xd0\xb4\xd0\xbd\xd1\x8f \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xb2\xd1\x82\xd1\x80\xd0\xb0\n\n        Sincerely,\n        John Doe\n        \xe5\x90\x88\xe8\x82\xa5\xe5\xbe\xae\xe5\xb0\xba\xe5\xba\xa6\xe5\x9b\xbd\xe5\xae\xb6\xe7\x89\xa9\xe7\x90\x86\xe7\xa7\x91\xe5\xad\xa6\xe5\xae\x9e\xe9\xaa\x8c\xe5\xae\xa4\n        """"""\n\n        document2 =""""""\n        This is a random document with Arabic about our meeting.\n\n        \xd8\xb9\xd8\xb4 \xd8\xa7\xd9\x84\xd9\x8a\xd9\x88\xd9\x85 \xd8\xa7\xd9\x84\xd8\xa3\xd9\x85\xd9\x84 \xd9\x84\xd9\x8a\xd9\x88\xd9\x85 \xd8\xba\xd8\xaf\n\n        Bye for now.\n        """"""\n\n        docs = [document1, document2]\n\n        result = snlp.search([\'physical sciences\', \'meeting\', \'Arabic\'], docs, keys=[\'doc1\', \'doc2\'])\n        self.assertEqual(len(result), 4)\n        self.assertEqual(result[0][2], 1)\n        self.assertEqual(result[1][2], 2)\n        self.assertEqual(result[2][1], \'meeting\')\n        self.assertEqual(result[3][1], \'Arabic\')\n\n        result = snlp.search(\'\xe5\x90\x88\xe8\x82\xa5\xe5\xbe\xae\xe5\xb0\xba\xe5\xba\xa6\xe5\x9b\xbd\xe5\xae\xb6\xe7\x89\xa9\xe7\x90\x86\xe7\xa7\x91\xe5\xad\xa6\xe5\xae\x9e\xe9\xaa\x8c\xe5\xae\xa4\', docs, keys=[\'doc1\', \'doc2\'])\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0][2], 7)\n\n        result = snlp.search(\'\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe\xd0\xb4\xd0\xbd\xd1\x8f \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xb2\xd1\x82\xd1\x80\xd0\xb0\', docs, keys=[\'doc1\', \'doc2\'])\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0][2], 1)\n\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
ktrain/tests/testenv.py,0,"b'import os\n#os.environ[\'TF_KERAS\'] = \'1\'\n#os.environ[\'TF_EAGER\'] = \'0\'\nimport sys\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID"";\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\nsys.path.insert(0,\'../..\')\n\n'"
ktrain/text/__init__.py,0,"b'from .models import print_text_classifiers, print_text_regression_models, text_classifier, text_regression_model\nfrom .data import texts_from_folder, texts_from_csv, texts_from_df,  texts_from_array\nfrom .ner.data import entities_from_gmb, entities_from_conll2003, entities_from_txt, entities_from_df, entities_from_array\nfrom .ner.models import sequence_tagger, print_sequence_taggers\nfrom .eda import get_topic_model\nfrom .textutils import extract_filenames, load_text_files, filter_by_id\nfrom .preprocessor import Transformer, TransformerEmbedding\nfrom .summarization import TransformerSummarizer\nfrom .zsl import ZeroShotClassifier\nfrom . import shallownlp\nfrom .qa import SimpleQA\nfrom . import textutils\nimport pickle\n\n__all__ = [\n           \'text_classifier\', \'text_regression_model\',\n           \'print_text_classifiers\', \'print_text_regression_models\',\n           \'texts_from_folder\', \'texts_from_csv\', \'texts_from_df\', \'texts_from_array\',\n           \'entities_from_gmb\',\n           \'entities_from_conll2003\',\n           \'entities_from_txt\',\n           \'entities_from_array\',\n           \'entities_from_df\',\n           \'sequence_tagger\',\n           \'print_sequence_taggers\',\n           \'get_topic_model\',\n           \'Transformer\',\n           \'TranformerEmbedding\',\n           \'shallownlp\',\n           \'TransformerSummarizer\',\n           \'ZeroShotClassifier\',\n           \'SimpleQA\',\n           \'extract_filenames\', \n           \'load_text_files\',\n           ]\n\n\ndef load_topic_model(fname):\n    """"""\n    Load saved TopicModel object\n    Args:\n        fname(str): base filename for all saved files\n    """"""\n    with open(fname+\'.tm_vect\', \'rb\') as f:\n        vectorizer = pickle.load(f)\n    with open(fname+\'.tm_model\', \'rb\') as f:\n        model = pickle.load(f)\n    with open(fname+\'.tm_params\', \'rb\') as f:\n        params = pickle.load(f)\n    tm = get_topic_model(n_topics=params[\'n_topics\'],\n                         n_features = params[\'n_features\'],\n                         verbose = params[\'verbose\'])\n    tm.model = model\n    tm.vectorizer = vectorizer\n    return tm\n\n\n\nseqlen_stats = Transformer.seqlen_stats\n'"
ktrain/text/data.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom  . import preprocessor as tpp\nfrom . import textutils as TU\n\n\n\nMAX_FEATURES = 20000\nMAXLEN = 400\n\n\n\ndef texts_from_folder(datadir, classes=None, \n                      max_features=MAX_FEATURES, maxlen=MAXLEN,\n                      ngram_range=1,\n                      train_test_names=[\'train\', \'test\'],\n                      preprocess_mode=\'standard\',\n                      encoding=None, # detected automatically\n                      lang=None, # detected automatically\n                      val_pct=0.1, random_state=None,\n                      verbose=1):\n    """"""\n    Returns corpus as sequence of word IDs.\n    Assumes corpus is in the following folder structure:\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 datadir\n    \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n    \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class0       # folder containing documents of class 0\n    \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class1       # folder containing documents of class 1\n    \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class2       # folder containing documents of class 2\n    \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 classN       # folder containing documents of class N\n    \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 test \n    \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class0       # folder containing documents of class 0\n    \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class1       # folder containing documents of class 1\n    \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class2       # folder containing documents of class 2\n    \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 classN       # folder containing documents of class N\n\n    Each subfolder should contain documents in plain text format.\n    If train and test contain additional subfolders that do not represent\n    classes, they can be ignored by explicitly listing the subfolders of\n    interest using the classes argument.\n    Args:\n        datadir (str): path to folder\n        classes (list): list of classes (subfolders to consider).\n                        This is simply supplied as the categories argument\n                        to sklearn\'s load_files function.\n        max_features (int):  maximum number of unigrams to consider\n        maxlen (int):  maximum length of tokens in document\n        ngram_range (int):  If > 1, will include 2=bigrams, 3=trigrams and bigrams\n        train_test_names (list):  list of strings represnting the subfolder\n                                 name for train and validation sets\n                                 if test name is missing, <val_pct> of training\n                                 will be used for validation\n        preprocess_mode (str):  Either \'standard\' (normal tokenization) or one of {\'bert\', \'distilbert\'}\n                                tokenization and preprocessing for use with \n                                BERT/DistilBert text classification model.\n        encoding (str):        character encoding to use. Auto-detected if None\n        lang (str):            language.  Auto-detected if None.\n        val_pct(float):        Onlyl used if train_test_names  has 1 and not 2 names\n        random_state(int):      If integer is supplied, train/test split is reproducible.\n                                IF None, train/test split will be random\n        verbose (bool):         verbosity\n        \n    """"""\n\n    # check train_test_names\n    if len(train_test_names) < 1 or len(train_test_names) > 2:\n        raise ValueError(\'train_test_names must have 1 or two elements for train and optionally validation\')\n\n    # read in training and test corpora\n    train_str = train_test_names[0]\n    train_b = load_files(os.path.join(datadir, train_str), shuffle=True, categories=classes)\n    if len(train_test_names) > 1:\n        test_str = train_test_names[1]\n        test_b = load_files(os.path.join(datadir,  test_str), shuffle=False, categories=classes)\n        x_train = train_b.data\n        y_train = train_b.target\n        x_test = test_b.data\n        y_test = test_b.target\n    else:\n        x_train, x_test, y_train, y_test = train_test_split(train_b.data, \n                                                            train_b.target, \n                                                            test_size=val_pct,\n                                                            random_state=random_state)\n\n    # decode based on supplied encoding\n    if encoding is None:\n        encoding = TU.detect_encoding(x_train)\n        U.vprint(\'detected encoding: %s\' % (encoding), verbose=verbose)\n    \n    try:\n        x_train = [x.decode(encoding) for x in x_train]\n        x_test = [x.decode(encoding) for x in x_test]\n    except:\n        U.vprint(\'Decoding with %s failed 1st attempt - using %s with skips\' % (encoding, \n                                                                                encoding),\n                                                                                verbose=verbose)\n        x_train = TU.decode_by_line(x_train, encoding=encoding, verbose=verbose)\n        x_test = TU.decode_by_line(x_test, encoding=encoding, verbose=verbose)\n\n\n    # detect language\n    if lang is None: lang = TU.detect_lang(x_train)\n    check_unsupported_lang(lang, preprocess_mode)\n\n\n\n    # return preprocessed the texts\n    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)\n    if None: raise ValueError(\'unsupported preprocess_mode\')\n    preproc = preproc_type(maxlen,\n                           max_features,\n                           class_names = train_b.target_names,\n                           lang=lang, ngram_range=ngram_range)\n    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)\n    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)\n    return (trn, val, preproc)\n\n\n\n\n\ndef texts_from_csv(train_filepath, \n                   text_column,\n                   label_columns = [],\n                   val_filepath=None,\n                   max_features=MAX_FEATURES, maxlen=MAXLEN, \n                   val_pct=0.1, ngram_range=1, preprocess_mode=\'standard\', \n                   encoding=None,  # auto-detected\n                   lang=None,      # auto-detected\n                   sep=\',\', random_state=None,       \n                   verbose=1):\n    """"""\n    Loads text data from CSV or TSV file. Class labels are assumed to be\n    one of the following formats:\n        1. one-hot-encoded or multi-hot-encoded arrays representing classes:\n              Example with label_columns=[\'positive\', \'negative\'] and text_column=\'text\':\n                text|positive|negative\n                I like this movie.|1|0\n                I hated this movie.|0|1\n            Classification will have a single one in each row: [[1,0,0], [0,1,0]]]\n            Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]\n        2. labels are in a single column of string or integer values representing classs labels\n               Example with label_columns=[\'label\'] and text_column=\'text\':\n                 text|label\n                 I like this movie.|positive\n                 I hated this movie.|negative\n\n    This treats task as classification problem. If this is a text regression task, use texts_from_array.\n\n    Args:\n        train_filepath(str): file path to training CSV\n        text_column(str): name of column containing the text\n        label_column(list): list of columns that are to be treated as labels\n        val_filepath(string): file path to test CSV.  If not supplied,\n                               10% of documents in training CSV will be\n                               used for testing/validation.\n        max_features(int): max num of words to consider in vocabulary\n        maxlen(int): each document can be of most <maxlen> words. 0 is used as padding ID.\n        ngram_range(int): size of multi-word phrases to consider\n                          e.g., 2 will consider both 1-word phrases and 2-word phrases\n                               limited by max_features\n        val_pct(float): Proportion of training to use for validation.\n                        Has no effect if val_filepath is supplied.\n        preprocess_mode (str):  Either \'standard\' (normal tokenization) or one of {\'bert\', \'distilbert\'}\n                                tokenization and preprocessing for use with \n                                BERT/DistilBert text classification model.\n        encoding (str):        character encoding to use. Auto-detected if None\n        lang (str):            language.  Auto-detected if None.\n        sep(str):              delimiter for CSV (comma is default)\n        random_state(int):      If integer is supplied, train/test split is reproducible.\n                                If None, train/test split will be random\n        verbose (boolean): verbosity\n    """"""\n    if encoding is None:\n        with open(train_filepath, \'rb\') as f:\n            #encoding = chardet.detect(f.read())[\'encoding\']\n            #encoding = \'utf-8\' if encoding.lower() in [\'ascii\', \'utf8\', \'utf-8\'] else encoding\n            encoding = TU.detect_encoding(f.read())\n            U.vprint(\'detected encoding: %s (if wrong, set manually)\' % (encoding), verbose=verbose)\n\n    train_df = pd.read_csv(train_filepath, encoding=encoding,sep=sep)\n    val_df = pd.read_csv(val_filepath, encoding=encoding,sep=sep) if val_filepath is not None else None\n    return texts_from_df(train_df,\n                         text_column,\n                         label_columns=label_columns,\n                         val_df = val_df,\n                         max_features=max_features,\n                         maxlen=maxlen,\n                         val_pct=val_pct,\n                         ngram_range=ngram_range, \n                         preprocess_mode=preprocess_mode,\n                         lang=lang, random_state=random_state,\n                         verbose=verbose)\n\n\ndef texts_from_df(train_df, \n                   text_column,\n                   label_columns = [],\n                   val_df=None,\n                   max_features=MAX_FEATURES, maxlen=MAXLEN, \n                   val_pct=0.1, ngram_range=1, preprocess_mode=\'standard\', \n                   lang=None, # auto-detected\n                   random_state=None,\n                   verbose=1):\n    """"""\n    Loads text data from Pandas dataframe file. Class labels are assumed to be\n    one of the following formats:\n        1. one-hot-encoded or multi-hot-encoded arrays representing classes:\n              Example with label_columns=[\'positive\', \'negative\'] and text_column=\'text\':\n                text|positive|negative\n                I like this movie.|1|0\n                I hated this movie.|0|1\n            Classification will have a single one in each row: [[1,0,0], [0,1,0]]]\n            Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]\n        2. labels are in a single column of string or integer values representing classs labels\n               Example with label_columns=[\'label\'] and text_column=\'text\':\n                 text|label\n                 I like this movie.|positive\n                 I hated this movie.|negative\n\n    This treats task as classification problem. If this is a text regression task, use texts_from_array.\n\n    Args:\n        train_df(dataframe): Pandas dataframe\n        text_column(str): name of column containing the text\n        label_column(list): list of columns that are to be treated as labels\n        val_df(dataframe): file path to test dataframe.  If not supplied,\n                               10% of documents in training df will be\n                               used for testing/validation.\n        max_features(int): max num of words to consider in vocabulary\n        maxlen(int): each document can be of most <maxlen> words. 0 is used as padding ID.\n        ngram_range(int): size of multi-word phrases to consider\n                          e.g., 2 will consider both 1-word phrases and 2-word phrases\n                               limited by max_features\n        val_pct(float): Proportion of training to use for validation.\n                        Has no effect if val_filepath is supplied.\n        preprocess_mode (str):  Either \'standard\' (normal tokenization) or one of {\'bert\', \'distilbert\'}\n                                tokenization and preprocessing for use with \n                                BERT/DistilBert text classification model.\n        lang (str):            language.  Auto-detected if None.\n        random_state(int):      If integer is supplied, train/test split is reproducible.\n                                If None, train/test split will be random\n        verbose (boolean): verbosity\n    """"""\n\n    # read in train and test data\n    train = train_df\n\n    x = train[text_column].fillna(\'fillna\').values\n    y = train[label_columns].values\n    if val_df is not None:\n        test = val_df\n        x_test = test[text_column].fillna(\'fillna\').values\n        y_test = test[label_columns].values\n        x_train = x\n        y_train = y\n    else:\n        x_train, x_test, y_train, y_test = train_test_split(x, y, \n                                                            test_size=val_pct,\n                                                            random_state=random_state)\n    y_train = np.squeeze(y_train)\n    y_test = np.squeeze(y_test)\n    if isinstance(label_columns, str) or (isinstance(label_columns, list) and len(label_columns) == 1):\n        class_names = list(set(y_train))\n        need_transform = True if isinstance(class_names[0], str) else False\n        class_names.sort()\n        class_names = [str(c) for c in class_names]\n        if need_transform:\n            encoder = LabelEncoder()\n            encoder.fit(y_train)\n            y_train = encoder.transform(y_train)\n            y_test = encoder.transform(y_test)\n            class_names = encoder.classes_\n    else:\n        class_names = label_columns\n\n    # detect language\n    if lang is None: lang = TU.detect_lang(x_train)\n    check_unsupported_lang(lang, preprocess_mode)\n\n\n    # return preprocessed the texts\n    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)\n    if None: raise ValueError(\'unsupported preprocess_mode\')\n    preproc = preproc_type(maxlen,\n                           max_features,\n                           class_names = class_names,\n                           lang=lang, ngram_range=ngram_range)\n    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)\n    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)\n    return (trn, val, preproc)\n\n\n\ndef texts_from_array(x_train, y_train, x_test=None, y_test=None, \n                   class_names = [],\n                   max_features=MAX_FEATURES, maxlen=MAXLEN, \n                   val_pct=0.1, ngram_range=1, preprocess_mode=\'standard\', \n                   lang=None, # auto-detected\n                   random_state=None,\n                   verbose=1):\n    """"""\n    Loads and preprocesses text data from arrays.\n    texts_from_array can handle data for both text classification\n    and text regression.  If class_names is empty, a regression task is assumed.\n    Args:\n        x_train(list): list of training texts \n        y_train(list): labels in one of the following forms:\n        x_test(list): list of training texts \n        y_test(list): labels in one of the following forms:\n                       1. list of integers representing classes (class_names is required)\n                       2. list of strings representing classes (class_names is not needed and ignored.)\n                       3. a one or multi hot encoded array representing classes (class_names is required)\n                       4. numerical values for text regresssion (class_names should be left empty)\n        class_names (list): list of strings representing class labels\n                            shape should be (num_examples,1) or (num_examples,)\n        max_features(int): max num of words to consider in vocabulary\n        maxlen(int): each document can be of most <maxlen> words. 0 is used as padding ID.\n        ngram_range(int): size of multi-word phrases to consider\n                          e.g., 2 will consider both 1-word phrases and 2-word phrases\n                               limited by max_features\n        val_pct(float): Proportion of training to use for validation.\n                        Has no effect if x_val and  y_val is supplied.\n        preprocess_mode (str):  Either \'standard\' (normal tokenization) or \'bert\'\n                                tokenization and preprocessing for use with \n                                BERT text classification model.\n        lang (str):            language.  Auto-detected if None.\n        random_state(int):      If integer is supplied, train/test split is reproducible.\n                                If None, train/test split will be random.\n        verbose (boolean): verbosity\n    """"""\n\n    if not class_names and verbose:\n        #classes =  list(set(y_train))\n        #classes.sort()\n        #class_names = [""%s"" % (c) for c in classes]\n        print(\'task: text regression (supply class_names argument if this is supposed to be classification task)\')\n    else:\n        print(\'task: text classification\')\n\n    if x_test is None or y_test is None:\n        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, \n                                                            test_size=val_pct,\n                                                            random_state=random_state)\n\n    # removed as TextPreprocessor now handles this.\n    #if isinstance(y_train[0], str):\n        #if not isinstance(y_test[0], str): \n            #raise ValueError(\'y_train contains strings, but y_test does not\')\n        #encoder = LabelEncoder()\n        #encoder.fit(y_train)\n        #y_train = encoder.transform(y_train)\n        #y_test = encoder.transform(y_test)\n\n\n    # detect language\n    if lang is None: lang = TU.detect_lang(x_train)\n    check_unsupported_lang(lang, preprocess_mode)\n\n    # return preprocessed the texts\n    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)\n    if None: raise ValueError(\'unsupported preprocess_mode\')\n    preproc = preproc_type(maxlen,\n                           max_features,\n                           class_names = class_names,\n                           lang=lang, ngram_range=ngram_range)\n    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)\n    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)\n    return (trn, val, preproc)\n\n\n\ndef check_unsupported_lang(lang, preprocess_mode):\n    """"""\n    check for unsupported language (e.g., nospace langs not supported by Jieba)\n    """"""\n    unsupported = preprocess_mode==\'standard\' and TU.is_nospace_lang(lang) and not TU.is_chinese(lang)\n    if unsupported:\n        raise ValueError(\'language %s is currently only supported by the BERT model. \' % (lang) +\n                         \'Please select preprocess_mode=""bert""\')\n\n'"
ktrain/text/eda.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom . import textutils as TU\nfrom . import preprocessor as pp\nimport time\n\nclass TopicModel():\n\n\n    def __init__(self,texts=None, n_topics=None, n_features=10000, \n                 min_df=5, max_df=0.5,  stop_words=\'english\',\n                 model_type=\'lda\',\n                 lda_max_iter=5, lda_mode=\'online\',\n                 token_pattern=None, verbose=1,\n                 hyperparam_kwargs=None\n    ):\n        """"""\n        Fits a topic model to documents in <texts>.\n        Example:\n            tm = ktrain.text.get_topic_model(docs, n_topics=20, \n                                            n_features=1000, min_df=2, max_df=0.95)\n        Args:\n            texts (list of str): list of texts\n            n_topics (int): number of topics.\n                            If None, n_topics = min{400, sqrt[# documents/2]})\n            n_features (int):  maximum words to consider\n            max_df (float): words in more than max_df proportion of docs discarded\n            stop_words (str or list): either \'english\' for built-in stop words or\n                                      a list of stop words to ignore\n            lda_max_iter (int): maximum iterations for \'lda\'.  5 is default if using lda_mode=\'online\'.\n                                If lda_mode=\'batch\', this should be increased (e.g., 1500).\n                                Ignored if model_type != \'lda\'\n            lda_mode (str):  one of {\'online\', \'batch\'}. Ignored of model_type !=\'lda\'\n            token_pattern(str): regex pattern to use to tokenize documents. \n            verbose(bool): verbosity\n\n        """"""\n        self.verbose=verbose\n\n        # estimate n_topics\n        if n_topics is None:\n            if texts is None:\n                raise ValueError(\'If n_topics is None, texts must be supplied\')\n            estimated = max(1, int(math.floor(math.sqrt(len(texts) / 2))))\n            n_topics = min(400, estimated)\n            print(\'n_topics automatically set to %s\' % (n_topics))\n\n        # train model\n        if texts is not None:\n            (model, vectorizer) = self.train(texts, model_type=model_type,\n                                             n_topics=n_topics, n_features=n_features,\n                                             min_df = min_df, max_df = max_df, \n                                             stop_words=stop_words,\n                                             lda_max_iter=lda_max_iter, lda_mode=lda_mode,\n                                             token_pattern=token_pattern,\n                                             hyperparam_kwargs=hyperparam_kwargs)\n        else:\n            vectorizer = None\n            model = None\n\n\n\n        # save model and vectorizer and hyperparameter settings\n        self.vectorizer = vectorizer\n        self.model = model\n        self.n_topics = n_topics\n        self.n_features = n_features\n        if verbose: print(\'done.\')\n\n        # these variables are set by self.build():\n        self.topic_dict = None\n        self.doc_topics = None\n        self.bool_array = None\n\n        self.scorer = None       # set by self.train_scorer()\n        self.recommender = None  # set by self.train_recommender()\n        return\n\n\n    def train(self,texts, model_type=\'lda\', n_topics=None, n_features=10000,\n              min_df=5, max_df=0.5,  stop_words=\'english\',\n              lda_max_iter=5, lda_mode=\'online\',\n              token_pattern=None, hyperparam_kwargs=None):\n        """"""\n        Fits a topic model to documents in <texts>.\n        Example:\n            tm = ktrain.text.get_topic_model(docs, n_topics=20, \n                                            n_features=1000, min_df=2, max_df=0.95)\n        Args:\n            texts (list of str): list of texts\n            n_topics (int): number of topics.\n                            If None, n_topics = min{400, sqrt[# documents/2]})\n            n_features (int):  maximum words to consider\n            max_df (float): words in more than max_df proportion of docs discarded\n            stop_words (str or list): either \'english\' for built-in stop words or\n                                      a list of stop words to ignore\n            lda_max_iter (int): maximum iterations for \'lda\'.  5 is default if using lda_mode=\'online\'.\n                                If lda_mode=\'batch\', this should be increased (e.g., 1500).\n                                Ignored if model_type != \'lda\'\n            lda_mode (str):  one of {\'online\', \'batch\'}. Ignored of model_type !=\'lda\'\n            token_pattern(str): regex pattern to use to tokenize documents. \n                                If None, a default tokenizer will be used\n            hyperparam_kwargs(dict): hyperparameters for LDA/NMF\n                                     Keys in this dict can be any of the following:\n                                         alpha: alpha for LDA  default: 5./n_topics\n                                         beta: beta for LDA.  default:0.01\n                                         nmf_alpha: alpha for NMF.  default:0\n                                         l1_ratio: l1_ratio for NMF. default: 0\n                                         ngram_range:  whether to consider bigrams, trigrams. default: (1,1) \n                                    \n        Returns:\n            tuple: (model, vectorizer)\n        """"""\n        if hyperparam_kwargs is None:\n            hyperparam_kwargs = {}\n        alpha = hyperparam_kwargs.get(\'alpha\', 5.0 / n_topics)\n        beta = hyperparam_kwargs.get(\'beta\', 0.01)\n        nmf_alpha = hyperparam_kwargs.get(\'nmf_alpha\', 0)\n        l1_ratio = hyperparam_kwargs.get(\'l1_ratio\', 0)\n        ngram_range = hyperparam_kwargs.get(\'ngram_range\', (1,1))\n\n        # adjust defaults based on language detected\n        if texts is not None:\n            lang = TU.detect_lang(texts)\n            if lang != \'en\':\n                stopwords = None if stop_words==\'english\' else stop_words\n                token_pattern = r\'(?u)\\b\\w+\\b\' if token_pattern is None else token_pattern\n            if pp.is_nospace_lang(lang):\n                text_list = []\n                for t in texts:\n                    text_list.append(\' \'.join(jieba.cut(t, HMM=False)))\n                texts = text_list\n            if self.verbose: print(\'lang: %s\' % (lang))\n\n\n        # preprocess texts\n        if self.verbose: print(\'preprocessing texts...\')\n        if token_pattern is None: token_pattern = TU.DEFAULT_TOKEN_PATTERN\n        #if token_pattern is None: token_pattern = r\'(?u)\\b\\w\\w+\\b\'\n        vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n                                 max_features=n_features, stop_words=stop_words,\n                                 token_pattern=token_pattern, ngram_range=ngram_range)\n        \n\n        x_train = vectorizer.fit_transform(texts)\n\n        # fit model\n\n        if self.verbose: print(\'fitting model...\')\n        if model_type == \'lda\':\n            model = LatentDirichletAllocation(n_components=n_topics, max_iter=lda_max_iter,\n                                              learning_method=lda_mode, learning_offset=50.,\n                                              doc_topic_prior=alpha,\n                                              topic_word_prior=beta,\n                                              verbose=self.verbose, random_state=0)\n        elif model_type == \'nmf\':\n            model = NMF(\n                n_components=n_topics,\n                max_iter=lda_max_iter,\n                verbose=self.verbose,\n                alpha=nmf_alpha,\n                l1_ratio=l1_ratio,\n                random_state=0)\n        else:\n            raise ValueError(""unknown model type:"", str(model_type))\n        model.fit(x_train)\n\n        # save model and vectorizer and hyperparameter settings\n        return (model, vectorizer)\n\n\n    @property\n    def topics(self):\n        """"""\n        convenience method/property\n        """"""\n        return self.get_topics()\n\n\n    def get_topics(self, n_words=10, as_string=True):\n        """"""\n        Returns a list of discovered topics\n        Args:\n            n_words(int): number of words to use in topic summary\n            as_string(bool): If True, each summary is a space-delimited string instead of list of words\n        """"""\n        self._check_model()\n        feature_names = self.vectorizer.get_feature_names()\n        topic_summaries = []\n        for topic_idx, topic in enumerate(self.model.components_):\n            summary = [feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]\n            if as_string: summary = "" "".join(summary)\n            topic_summaries.append(summary)\n        return topic_summaries\n\n\n    def print_topics(self, n_words=10, show_counts=False):\n        """"""\n        print topics\n        """"""\n        topics = self.get_topics(n_words=n_words, as_string=True)\n        if show_counts:\n            self._check_build()\n            topic_counts = sorted([ (k, topics[k], len(v)) for k,v in self.topic_dict.items()], \n                                    key=lambda kv:kv[-1], reverse=True)\n            for (idx, topic, count) in topic_counts:\n                print(""topic:%s | count:%s | %s"" %(idx, count, topic))\n        else:\n            for i, t in enumerate(topics):\n                print(\'topic %s | %s\' % (i, t))\n        return\n\n\n    def build(self, texts, threshold=None):\n        """"""\n        Builds the document-topic distribution showing the topic probability distirbution\n        for each document in <texts> with respect to the learned topic space.\n        Args:\n            texts (list of str): list of text documents\n            threshold (float): If not None, documents with whose highest topic probability\n                               is less than threshold are filtered out.\n        """"""\n        doc_topics, bool_array = self.predict(texts, threshold=threshold)\n        self.doc_topics = doc_topics\n        self.bool_array = bool_array\n\n        texts = [text for i, text in enumerate(texts) if bool_array[i]]\n        self.topic_dict = self._rank_documents(texts, doc_topics=doc_topics)\n        return\n\n\n    def filter(self, lst):\n        """"""\n        The build method may prune documents based on threshold.\n        This method prunes other lists based on how build pruned documents.\n        This is useful to filter lists containing metadata associated with documents\n        for use with visualize_documents.\n        Args:\n            lst(list): a list of data\n        Returns:\n            list:  a filtered list of data based on how build filtered the documents\n        """"""\n        if len(lst) != self.bool_array.shape[0]:\n            raise ValueError(\'Length of lst is not consistent with the number of documents \' +\n                             \'supplied to get_topic_model\')\n        arr = np.array(lst)\n        return list(arr[self.bool_array])\n                           \n\n    \n    def get_docs(self, topic_ids=[], doc_ids=[], rank=False):\n        """"""\n        Returns document entries for supplied topic_ids\n        Args:\n            topic_ids(list of ints): list of topid IDs where each id is in the range\n                                     of range(self.n_topics).\n            doc_ids (list of ints): list of document IDs where each id is an index\n                                    into self.doctopics\n            rank(bool): If True, the list is sorted first by topic_id (ascending)\n                        and then ty topic probability (descending).\n                        Otherwise, list is sorted by doc_id (i.e., the order\n                        of texts supplied to self.build (which is the order of self.doc_topics).\n\n        Returns:\n            list of tuples:  list of tuples where each tuple is of the form \n                             (text, doc_id, topic_probability, topic_id).\n            \n        """"""\n        self._check_build()\n        if not topic_ids:\n            topic_ids = list(range(self.n_topics))\n        result_texts = []\n        for topic_id in topic_ids:\n            if topic_id not in self.topic_dict: continue\n            texts = [tup + (topic_id,) for tup in self.topic_dict[topic_id] \n                                           if not doc_ids or tup[1] in doc_ids]\n            result_texts.extend(texts)\n        if not rank:\n            result_texts = sorted(result_texts, key=lambda x:x[1])\n        return result_texts\n\n\n    def get_doctopics(self,  topic_ids=[], doc_ids=[]):\n        """"""\n        Returns a topic probability distribution for documents\n        with primary topic that is one of <topic_ids>\n        Args:\n            topic_ids(list of ints): list of topid IDs where each id is in the range\n                                     of range(self.n_topics).\n            doc_ids (list of ints): list of document IDs where each id is an index\n                                    into self.doctopics\n        Returns:\n            np.ndarray: Each row is the topic probability distribution of a document.\n                        Array is sorted in the order returned by self.get_docs.\n                        \n        """"""\n        docs = self.get_docs(topic_ids=topic_ids, doc_ids=doc_ids)\n        return np.array([self.doc_topics[idx] for idx in [x[1] for x in docs]])\n\n\n    def get_texts(self,  topic_ids=[]):\n        """"""\n        Returns texts for documents\n        with primary topic that is one of <topic_ids>\n        Args:\n            topic_ids(list of ints): list of topic IDs\n        Returns:\n            list of str\n        """"""\n        if not topic_ids: topic_ids = list(range(self.n_topics))\n        docs = self.get_docs(topic_ids)\n        return [x[0] for x in docs]\n\n\n\n    def predict(self, texts, threshold=None, harden=False):\n        """"""\n        Args:\n            texts (list of str): list of texts\n            threshold (float): If not None, documents with maximum topic scores\n                                less than <threshold> are filtered out\n            harden(bool): If True, each document is assigned to a single topic for which\n                          it has the highest score\n        Returns:\n            if threshold is None:\n                np.ndarray: topic distribution for each text document\n            else:\n                (np.ndarray, np.ndarray): topic distribution and boolean array\n        """"""\n        self._check_model()\n        transformed_texts = self.vectorizer.transform(texts)\n        X_topics = self.model.transform(transformed_texts)\n        #if self.model_type == \'nmf\':\n            #scores = np.matrix(X_topics)\n            #scores_normalized= scores/scores.sum(axis=1)\n            #X_topics = scores_normalized\n        _idx = np.array([True] * len(texts))\n        if threshold is not None:\n            _idx = np.amax(X_topics, axis=1) > threshold  # idx of doc that above the threshold\n            _idx = np.array(_idx)\n            X_topics = X_topics[_idx]\n        if harden: X_topics = self._harden_topics(X_topics)\n        if threshold is not None:\n            return (X_topics, _idx)\n        else:\n            return X_topics\n\n\n    def visualize_documents(self, texts=None, doc_topics=None, \n                            width=700, height=700, point_size=5, title=\'Document Visualization\',\n                            extra_info={},\n                            colors=None,\n                            filepath=None,):\n        """"""\n        Generates a visualization of a set of documents based on model.\n        If <texts> is supplied, raw documents will be first transformed into document-topic\n        matrix.  If <doc_topics> is supplied, then this will be used for visualization instead.\n        Args:\n            texts(list of str): list of document texts.  Mutually-exclusive with <doc_topics>\n            doc_topics(ndarray): pre-computed topic distribution for each document in texts.\n                                 Mutually-exclusive with <texts>.\n            width(int): width of image\n            height(int): height of image\n            point_size(int): size of circles in plot\n            title(str):  title of visualization\n            extra_info(dict of lists): A user-supplied information for each datapoint (attributes of the datapoint).\n                                       The keys are field names.  The values are lists - each of which must\n                                       be the same number of elements as <texts> or <doc_topics>. These fields are displayed\n                                       when hovering over datapoints in the visualization.\n            colors(list of str):  list of Hex color codes for each datapoint.\n                                  Length of list must match either len(texts) or doc_topics.shape[0]\n            filepath(str):             Optional filepath to save the interactive visualization\n        """"""\n\n        # error-checking\n        if texts is not None: length = len(texts)\n        else: length = doc_topics.shape[0]\n        if colors is not None and len(colors) != length:\n            raise ValueError(\'length of colors is not consistent with length of texts or doctopics\')\n        if texts is not None and doc_topics is not None:\n            raise ValueError(\'texts is mutually-exclusive with doc_topics\')\n        if texts is None and doc_topics is None:\n            raise ValueError(\'One of texts or doc_topics is required.\')\n        if extra_info:\n            invalid_keys = [\'x\', \'y\', \'topic\', \'fill_color\']\n            for k in extra_info.keys():\n                if k in invalid_keys:\n                    raise ValueError(\'cannot use ""%s"" as key in extra_info\' %(k))\n                lst = extra_info[k]\n                if len(lst) != length:\n                    raise ValueError(\'texts and extra_info lists must be same size\')\n\n        # check fo bokeh\n        try:\n            import bokeh.plotting as bp\n            from bokeh.plotting import save\n            from bokeh.models import HoverTool\n            from bokeh.io import output_notebook\n        except:\n            warnings.warn(\'visualize_documents method requires bokeh package: pip3 install bokeh\')\n            return\n\n        # prepare data\n        if doc_topics is not None:\n            X_topics = doc_topics\n        else:\n            if self.verbose:  print(\'transforming texts...\', end=\'\')\n            X_topics = self.predict(texts, harden=False)\n            if self.verbose: print(\'done.\')\n\n        # reduce to 2-D\n        if self.verbose:  print(\'reducing to 2 dimensions...\', end=\'\')\n        tsne_model = TSNE(n_components=2, verbose=self.verbose, random_state=0, angle=.99, init=\'pca\')\n        tsne_lda = tsne_model.fit_transform(X_topics)\n        print(\'done.\')\n\n        # get random colormap\n        colormap = U.get_random_colors(self.n_topics)\n\n        # generate inline visualization in Jupyter notebook\n        lda_keys = self._harden_topics(X_topics)\n        if colors is None: colors = colormap[lda_keys]\n        topic_summaries = self.get_topics(n_words=5)\n        os.environ[""BOKEH_RESOURCES""]=""inline""\n        output_notebook()\n        dct = { \n                \'x\':tsne_lda[:,0],\n                \'y\':tsne_lda[:, 1],\n                \'topic\':[topic_summaries[tid] for tid in lda_keys],\n                \'fill_color\':colors,}\n        tool_tups = [(\'index\', \'$index\'),\n                     (\'(x,y)\',\'($x,$y)\'),\n                     (\'topic\', \'@topic\')]\n        for k in extra_info.keys():\n            dct[k] = extra_info[k]\n            tool_tups.append((k, \'@\'+k))\n\n        source = bp.ColumnDataSource(data=dct)\n        hover = HoverTool( tooltips=tool_tups)\n        p = bp.figure(plot_width=width, plot_height=height, \n                      tools=[hover, \'save\', \'pan\', \'wheel_zoom\', \'box_zoom\', \'reset\'],\n                      #tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",\n                      title=title)\n        #plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n\t\t\t   #title=title,\n\t\t\t   #tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",\n\t\t\t   #x_axis_type=None, y_axis_type=None, min_border=1)\n        p.circle(\'x\', \'y\', size=point_size, source=source, fill_color= \'fill_color\')\n        bp.show(p)\n        if filepath is not None:\n            bp.output_file(filepath)\n            bp.save(p)\n        return\n\n\n    def train_recommender(self, n_neighbors=20, metric=\'minkowski\', p=2):\n        """"""\n        Trains a recommender that, given a single document, will return\n        documents in the corpus that are semantically similar to it.\n\n        Args:\n            n_neighbors (int): \n        Returns:\n            None\n        """"""\n        from sklearn.neighbors import NearestNeighbors\n        rec = NearestNeighbors(n_neighbors=n_neighbors, metric=metric, p=p)\n        probs = self.get_doctopics()\n        rec.fit(probs)\n        self.recommender = rec\n        return\n\n\n\n    def recommend(self, text=None, doc_topic=None, n=5, n_neighbors=100):\n        """"""\n        Given an example document, recommends documents similar to it\n        from the set of documents supplied to build().\n \n        Args:\n            texts(list of str): list of document texts.  Mutually-exclusive with <doc_topics>\n            doc_topics(ndarray): pre-computed topic distribution for each document in texts.\n                                 Mutually-exclusive with <texts>.\n            n (int): number of recommendations to return\n        Returns:\n            list of tuples: each tuple is of the form:\n                            (text, doc_id, topic_probability, topic_id)\n\n        """"""\n        # error-checks\n        if text is not None and doc_topic is not None:\n            raise ValueError(\'text is mutually-exclusive with doc_topic\')\n        if text is None and doc_topic is None:\n            raise ValueError(\'One of text or doc_topic is required.\')\n        if text is not None and type(text) not in [str]:\n            raise ValueError(\'text must be a str \')\n        if  doc_topic is not None and type(doc_topic) not in [np.ndarray]:\n            raise ValueError(\'doc_topic must be a np.ndarray\')\n\n        if n > n_neighbors: n_neighbors = n\n\n        x_test = [doc_topic]\n        if text:\n            x_test = self.predict([text])\n        docs = self.get_docs()\n        indices = self.recommender.kneighbors(x_test, return_distance=False, n_neighbors=n_neighbors)\n        results = [doc for i, doc in enumerate(docs) if i in indices]\n        return results[:n]\n\n\n\n\n    def train_scorer(self, topic_ids=[], doc_ids=[], n_neighbors=20):\n        """"""\n        Trains a scorer that can score documents based on similarity to a\n        seed set of documents represented by topic_ids and doc_ids.\n\n        Args:\n            topic_ids(list of ints): list of topid IDs where each id is in the range\n                                     of range(self.n_topics).  Documents associated\n                                     with these topic_ids will be used as seed set.\n            doc_ids (list of ints): list of document IDs where each id is an index\n                                    into self.doctopics.  Documents associated \n                                    with these doc_ids will be used as seed set.\n        Returns:\n            None\n        """"""\n        from sklearn.neighbors import LocalOutlierFactor\n        clf = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination=0.1)\n        probs = self.get_doctopics(topic_ids=topic_ids, doc_ids=doc_ids)\n        clf.fit(probs)\n        self.scorer = clf\n        return\n\n\n\n    def score(self, texts=None, doc_topics=None):\n        """"""\n        Given a new set of documents (supplied as texts or doc_topics), the score method\n        uses a One-Class classifier to score documents based on similarity to a\n        seed set of documents (where seed set is computed by train_scorer() method).\n\n        Higher scores indicate a higher degree of similarity.\n        Positive values represent a binary decision of similar.\n        Negative values represent a binary decision of dissimlar.\n        In practice, negative scores closer to zer will also be simlar as One-Class\n        classifiers are more strict than traditional binary classifiers.\n        Documents with negative scores closer to zero are good candidates for\n        inclusion in a training set for binary classification (e.g., active labeling).\n \n        Args:\n            texts(list of str): list of document texts.  Mutually-exclusive with <doc_topics>\n            doc_topics(ndarray): pre-computed topic distribution for each document in texts.\n                                 Mutually-exclusive with <texts>.\n        Returns:\n            list of floats:  larger values indicate higher degree of similarity\n                             positive values indicate a binary decision of similar\n                             negative values indicate binary decision of dissimilar\n                             In practice, negative scores closer to zero will also \n                             be similar as One-class classifiers are more strict\n                             than traditional binary classifiers.\n\n        """"""\n        # error-checks\n        if texts is not None and doc_topics is not None:\n            raise ValueError(\'texts is mutually-exclusive with doc_topics\')\n        if texts is None and doc_topics is None:\n            raise ValueError(\'One of texts or doc_topics is required.\')\n        if texts is not None and type(texts) not in [list, np.ndarray]:\n            raise ValueError(\'texts must be either a list or numpy ndarray\')\n        if  doc_topics is not None and type(doc_topics) not in [np.ndarray]:\n            raise ValueError(\'doc_topics must be a np.ndarray\')\n\n        x_test = doc_topics\n        if texts:\n            x_test = self.predict(texts)\n        return self.scorer.decision_function(x_test)\n\n\n    def search(self, query, topic_ids=[], doc_ids=[], case_sensitive=False):\n        """"""\n        search documents for query string.\n        Args:\n            query(str):  the word or phrase to search\n            topic_ids(list of ints): list of topid IDs where each id is in the range\n                                     of range(self.n_topics).\n            doc_ids (list of ints): list of document IDs where each id is an index\n                                    into self.doctopics\n            case_sensitive(bool):  If True, case sensitive search\n        """"""\n\n        # setup pattern\n        if not case_sensitive: query = query.lower()\n        pattern = re.compile(r\'\\b%s\\b\' % query)\n\n        # retrive docs\n        docs = self.get_docs(topic_ids=topic_ids, doc_ids=doc_ids)\n\n        # search\n        mb = master_bar(range(1))\n        results = []\n        for i in mb:\n            for doc in progress_bar(docs, parent=mb):\n                text = doc[0]\n                if not case_sensitive: text = text.lower()\n                matches = pattern.findall(text)\n                if matches: results.append(doc)\n            if self.verbose: mb.write(\'done.\')\n        return results\n\n\n\n    def _rank_documents(self, \n                       texts,\n                       doc_topics=None):\n        """"""\n        Rank documents by topic score.\n        If topic_index is supplied, rank documents based on relevance to supplied topic.\n        Otherwise, rank all texts by their highest topic score (for any topic).\n        Args:\n            texts(list of str): list of document texts.\n            doc_topics(ndarray): pre-computed topic distribution for each document\n                                 If None, re-computed from texts.\n                              \n        Returns:\n            dict of lists: each element in list is a tuple of (doc_index, topic_index, score)\n            ... where doc_index is an index into either texts \n        """"""\n        if doc_topics is not None:\n            X_topics = doc_topics\n        else:\n            if self.verbose: print(\'transforming texts to topic space...\')\n            X_topics = self.predict(texts)\n        topics = np.argmax(X_topics, axis=1)\n        scores = np.amax(X_topics, axis=1)\n        doc_ids = np.array([i for i, x in enumerate(texts)])\n        result = list(zip(texts, doc_ids, topics, scores))\n        if self.verbose: print(\'done.\')\n        result = sorted(result, key=lambda x: x[-1], reverse=True)\n        result_dict = {}\n        for r in result:\n            text = r[0]\n            doc_id = r[1]\n            topic_id = r[2]\n            score = r[3]\n            lst = result_dict.get(topic_id, [])\n            lst.append((text, doc_id, score))\n            result_dict[topic_id] = lst\n        return result_dict\n\n\n    def _harden_topics(self, X_topics):\n        """"""\n        Transforms soft-clustering to hard-clustering\n        """"""\n        max_topics = []\n        for i in range(X_topics.shape[0]):\n            max_topics.append(X_topics[i].argmax())\n        X_topics = np.array(max_topics)\n        return X_topics\n\n\n    def _check_build(self):\n        self._check_model()\n        if self.topic_dict is None: \n            raise Exception(\'Must call build() method.\')\n\n    def _check_scorer(self):\n        if self.scorer is None:\n            raise Exception(\'Must call train_scorer()\')\n\n    def _check_recommender(self):\n        if self.recommender is None:\n            raise Exception(\'Must call train_recommender()\')\n\n\n    def _check_model(self):\n        if self.model is None or self.vectorizer is None:\n            raise Exception(\'Must call train()\')\n\n\n\n    def save(self, fname):\n        """"""\n        save TopicModel object\n        """"""\n\n        \n        with open(fname+\'.tm_vect\', \'wb\') as f:\n            pickle.dump(self.vectorizer, f)\n        with open(fname+\'.tm_model\', \'wb\') as f:\n            pickle.dump(self.model, f)\n        params = {\'n_topics\': self.n_topics,\n                  \'n_features\': self.n_features,\n                  \'verbose\': self.verbose}\n        with open(fname+\'.tm_params\', \'wb\') as f:\n            pickle.dump(params, f)\n\n        return\n\nget_topic_model = TopicModel \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
ktrain/text/learner.py,4,"b'from ..imports import *\nfrom .. import utils as U\nfrom ..core import ArrayLearner, GenLearner, _load_model\nfrom .preprocessor import TransformersPreprocessor\n\n\n\n\nclass BERTTextClassLearner(ArrayLearner):\n    """"""\n    Main class used to tune and train Keras models for text classification using Array data.\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, train_data=train_data, val_data=val_data,\n                         batch_size=batch_size, eval_batch_size=eval_batch_size,\n                         workers=workers, use_multiprocessing=use_multiprocessing,\n                         multigpu=multigpu)\n        return\n\n\n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            # BERT-style tuple\n            join_char = \' \'\n            obs = val[0][0][idx]\n            if preproc is not None: \n                obs = preproc.undo(obs)\n                if preproc.is_nospace_lang(): join_char = \'\'\n            if type(obs) == str:\n                obs = join_char.join(obs.split()[:512])\n            print(\'----------\')\n            print(""id:%s | loss:%s | true:%s | pred:%s)\\n"" % (idx, round(loss,2), truth, pred))\n            print(obs)\n        return\n\n\nclass TransformerTextClassLearner(GenLearner):\n    """"""\n    Main class used to tune and train Keras models for text classification using Array data.\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, train_data=train_data, val_data=val_data,\n                         batch_size=batch_size, eval_batch_size=eval_batch_size,\n                         workers=workers, use_multiprocessing=use_multiprocessing,\n                         multigpu=multigpu)\n        return\n\n\n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            join_char = \' \'\n            #obs = val.x[idx][0]\n            print(\'----------\')\n            print(""id:%s | loss:%s | true:%s | pred:%s)\\n"" % (idx, round(loss,2), truth, pred))\n        return\n\n\n    def _prepare(self, data, train=True):\n        """"""\n        prepare data as tf.Dataset\n        """"""\n        # HF_EXCEPTION\n        # convert arrays to TF dataset (iterator) on-the-fly\n        # to work around issues with transformers and tf.Datasets\n        if data is None: return None\n        return data.to_tfdataset(train=train)\n\n\n    def predict(self, val_data=None):\n        """"""\n        Makes predictions on validation set\n        """"""\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if val is None: raise Exception(\'val_data must be supplied to get_learner or predict\')\n        if hasattr(val, \'reset\'): val.reset()\n        classification, multilabel = U.is_classifier(self.model)\n        preds = self.model.predict(self._prepare(val, train=False))\n        if classification:\n            if multilabel:\n                return activations.sigmoid(tf.convert_to_tensor(preds)).numpy()\n            else:\n                return activations.softmax(tf.convert_to_tensor(preds)).numpy()\n        else:\n            return preds\n\n\n    def save_model(self, fpath):\n        """"""\n        save Transformers model\n        """"""\n        self._make_model_folder(fpath)\n        self.model.save_pretrained(fpath)\n        return\n\n\n    def load_model(self, fpath, preproc=None):\n        """"""\n        load Transformers model\n        Args:\n          fpath(str): path to folder containing model files\n          preproc(TransformerPreprocessor): a TransformerPreprocessor instance.\n        """"""\n        if preproc is None or not isinstance(preproc, TransformersPreprocessor):\n            raise ValueError(\'preproc arg is required to load Transformer models from disk. \' +\\\n                              \'Supply a TransformersPreprocessor instance. This is \' +\\\n                              \'either the third return value from texts_from* function or \'+\\\n                              \'the result of calling ktrain.text.Transformer\')\n\n\n        self.model = _load_model(fpath, preproc=preproc)\n        return\n\n\n\n'"
ktrain/text/models.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom . import preprocessor as tpp\n\n\nNBSVM = \'nbsvm\'\nFASTTEXT = \'fasttext\'\nLOGREG = \'logreg\'\nBIGRU = \'bigru\'\nSTANDARD_GRU = \'standard_gru\'\nBERT = \'bert\'\nDISTILBERT = tpp.DISTILBERT\nHUGGINGFACE_MODELS = [DISTILBERT]\nLINREG = \'linreg\'\nTEXT_CLASSIFIERS = {\n                    FASTTEXT: ""a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]"",\n                    LOGREG:  ""logistic regression using a trainable Embedding layer"",\n                    NBSVM:  ""NBSVM model [http://www.aclweb.org/anthology/P12-2018]"",\n                    BIGRU:  \'Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\',\n                    STANDARD_GRU: \'simple 2-layer GRU with randomly initialized embeddings\',\n                    BERT:  \'Bidirectional Encoder Representations from Transformers (BERT) [https://arxiv.org/abs/1810.04805]\',\n                    DISTILBERT:  \'distilled, smaller, and faster BERT from Hugging Face [https://arxiv.org/abs/1910.01108]\',\n                    } \n\nTEXT_REGRESSION_MODELS = {\n                    FASTTEXT: ""a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]"",\n                    LINREG:  ""linear text regression using a trainable Embedding layer"",\n                    BIGRU:  \'Bidirectional GRU with pretrained English word vectors [https://arxiv.org/abs/1712.09405]\',\n                    STANDARD_GRU: \'simple 2-layer GRU with randomly initialized embeddings\',\n                    BERT:  \'Bidirectional Encoder Representations from Transformers (BERT) [https://arxiv.org/abs/1810.04805]\',\n                    DISTILBERT:  \'distilled, smaller, and faster BERT from Hugging Face [https://arxiv.org/abs/1910.01108]\',\n                    }\n\n\n\ndef print_text_classifiers():\n    for k,v in TEXT_CLASSIFIERS.items():\n        print(""%s: %s"" % (k,v))\n\ndef print_text_regression_models():\n    for k,v in TEXT_REGRESSION_MODELS.items():\n        print(""%s: %s"" % (k,v))\n\n\ndef calc_pr(y_i, x, y, b):\n    idx = np.argwhere((y==y_i)==b)\n    ct = x[idx[:,0]].sum(0)+1\n    tot = ((y==y_i)==b).sum()+1\n    return ct/tot\n\ndef calc_r(y_i, x, y):\n    return np.log(calc_pr(y_i, x, y, True) / calc_pr(y_i, x, y, False))\n\n\ndef _text_model(name, train_data, preproc=None, multilabel=None, classification=True, metrics=[\'accuracy\'], verbose=1):\n    """"""\n    Build and return a text classification or text regression model.\n\n    Args:\n        name (string): one of:\n                      - \'fasttext\' for FastText model\n                      - \'nbsvm\' for NBSVM model  \n                      - \'logreg\' for logistic regression\n                      - \'bigru\' for Bidirectional GRU with pretrained word vectors\n                      - \'bert\' for BERT Text Classification\n                      - \'distilbert\' for Hugging Face DistilBert model\n        train_data (tuple): a tuple of numpy.ndarrays: (x_train, y_train) or ktrain.Dataset instance\n                            returned from one of the texts_from_* functions\n        preproc: a ktrain.text.TextPreprocessor instance.\n                 As of v0.8.0, this is required.\n        multilabel (bool):  If True, multilabel model will be returned.\n                            If false, binary/multiclass model will be returned.\n                            If None, multilabel will be inferred from data.\n        classification(bool): If True, will build a text classificaton model.\n                              Otherwise, a text regression model will be returned.\n        metrics(list): list of metrics to use\n        verbose (boolean): verbosity of output\n    Return:\n        model (Model): A Keras Model instance\n    """"""\n    # check arguments\n    if not isinstance(train_data, tuple) and not U.is_huggingface_from_data(train_data):\n        err =""""""\n            Please pass training data in the form of a tuple of numpy.ndarrays\n            or data returned from a ktrain texts_from* function.\n            """"""\n        raise Exception(err)\n\n    if not isinstance(preproc, tpp.TextPreprocessor):\n        msg = \'The preproc argument is required.\'\n        msg += \' The preproc arg should be an instance of TextPreprocessor, which is \'\n        msg += \' the third return value from texts_from_folder, texts_from_csv, etc.\'\n        #warnings.warn(msg, FutureWarning)\n        raise ValueError(msg)\n    if name == BIGRU and preproc.ngram_count() != 1:\n        raise ValueError(\'Data should be processed with ngram_range=1 for bigru model.\')\n    is_bert = U.bert_data_tuple(train_data)\n    if (is_bert and name != BERT) or (not is_bert and name == BERT):\n        raise ValueError(""if \'%s\' is selected model, then preprocess_mode=\'%s\' should be used and vice versa"" % (BERT, BERT))\n    is_huggingface = U.is_huggingface(data=train_data)\n    if (is_huggingface and name not in HUGGINGFACE_MODELS) or (not is_huggingface and name in HUGGINGFACE_MODELS):\n        raise ValueError(\'you are using a Hugging Face transformer model but did not preprocess as such (or vice versa)\')\n    if is_huggingface and preproc.name != name:\n        raise ValueError(\'you preprocessed for %s but want to build a %s model\' % (preproc.name, name))\n \n    if not classification: # regression\n        if metrics is None or metrics==[\'accuracy\']: metrics=[\'mae\']\n        num_classes = 1\n        multilabel = False\n        loss_func = \'mse\'\n        activation = None\n        max_features = preproc.max_features\n        features = None\n        maxlen = U.shape_from_data(train_data)[1]\n        U.vprint(\'maxlen is %s\' % (maxlen), verbose=verbose)\n    else:                 # classification\n        if metrics is None: metrics = [\'accuracy\']\n        # set number of classes and multilabel flag\n        num_classes = U.nclasses_from_data(train_data)\n\n        # determine multilabel\n        if multilabel is None:\n            multilabel = U.is_multilabel(train_data)\n        if multilabel and name in [NBSVM, LOGREG]:\n            warnings.warn(\'switching to fasttext model, as data suggests \'\n                          \'multilabel classification from data.\')\n            name = FASTTEXT\n        U.vprint(""Is Multi-Label? %s"" % (multilabel), verbose=verbose)\n\n        # set loss and activations\n        loss_func = \'categorical_crossentropy\'\n        activation = \'softmax\'\n        if multilabel:\n            loss_func = \'binary_crossentropy\'\n            activation = \'sigmoid\'\n\n        # determine number of classes, maxlen, and max_features\n        max_features = preproc.max_features if preproc is not None else None\n        features = set()\n        if not is_bert and not is_huggingface:\n            U.vprint(\'compiling word ID features...\', verbose=verbose)\n            x_train = train_data[0]\n            y_train = train_data[1]\n            if isinstance(y_train[0], int): raise ValueError(\'train labels should not be in sparse format\')\n\n            for x in x_train:\n                features.update(x)\n            #max_features = len(features)\n            if max_features is None: \n                max_features = max(features)+1\n                U.vprint(\'max_features is %s\' % (max_features), verbose=verbose)\n        maxlen = U.shape_from_data(train_data)[1]\n        U.vprint(\'maxlen is %s\' % (maxlen), verbose=verbose)\n\n\n    # return appropriate model\n    if name in [LOGREG, LINREG]:\n        model =  _build_logreg(num_classes, \n                            maxlen,\n                            max_features,\n                            features,\n                            loss_func=loss_func,\n                            activation=activation, metrics=metrics, verbose=verbose)\n\n    elif name==FASTTEXT:\n        model = _build_fasttext(num_classes, \n                            maxlen,\n                            max_features,\n                            features,\n                            loss_func=loss_func,\n                            activation=activation, metrics=metrics, verbose=verbose)\n    elif name==STANDARD_GRU:\n        model = _build_standard_gru(num_classes, \n                                    maxlen,\n                                    max_features,\n                                    features,\n                                    loss_func=loss_func,\n                                    activation=activation, metrics=metrics, verbose=verbose)\n    elif name==NBSVM:\n        model = _build_nbsvm(num_classes, \n                            maxlen,\n                            max_features,\n                            features,\n                            loss_func=loss_func,\n                            activation=activation, metrics=metrics, verbose=verbose,\n                            train_data=train_data)\n\n    elif name==BIGRU:\n        (tokenizer, tok_dct) = preproc.get_preprocessor()\n        model = _build_bigru(num_classes, \n                            maxlen,\n                            max_features,\n                            features,\n                            loss_func=loss_func,\n                            activation=activation, metrics=metrics, verbose=verbose,\n                            tokenizer=tokenizer,\n                            preproc=preproc)\n    elif name == BERT:\n        model =  _build_bert(num_classes, \n                            maxlen,\n                            max_features,\n                            features,\n                            loss_func=loss_func,\n                            activation=activation, metrics=metrics, verbose=verbose,\n                            preproc=preproc)\n    elif name in HUGGINGFACE_MODELS:\n        model =  _build_transformer(num_classes, \n                                   maxlen,\n                                   max_features,\n                                   features,\n                                   loss_func=loss_func,\n                                   activation=activation, metrics=metrics, verbose=verbose,\n                                   preproc=preproc)\n\n    else:\n        raise ValueError(\'name for textclassifier is invalid\')\n    U.vprint(\'done.\', verbose=verbose)\n    return model\n\n\n\ndef _build_logreg(num_classes,\n                  maxlen, max_features, features,\n                 loss_func=\'categorical_crossentropy\',\n                 activation = \'softmax\', metrics=[\'accuracy\'], verbose=1):\n\n    embedding_matrix = np.ones((max_features, 1))\n    embedding_matrix[0] = 0\n\n    # set up the model\n    inp = Input(shape=(maxlen,))\n    r = Embedding(max_features, 1, input_length=maxlen, \n                  weights=[embedding_matrix], trainable=False)(inp)\n    x = Embedding(max_features, num_classes, input_length=maxlen, \n                  embeddings_initializer=\'glorot_normal\')(inp)\n    x = dot([x,r], axes=1)\n    x = Flatten()(x)\n    if activation: x = Activation(activation)(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss=loss_func,\n                  optimizer=U.DEFAULT_OPT,\n                  metrics=metrics)\n    return model\n\n\ndef _build_bert(num_classes,\n                maxlen, max_features, features,\n               loss_func=\'categorical_crossentropy\',\n               activation = \'softmax\', metrics=[\'accuracy\'],  verbose=1,\n               preproc=None):\n    if preproc is None: raise ValueError(\'preproc is missing\')\n    lang = preproc.lang\n    if lang is None: raise ValueError(\'lang is missing\')\n    config_path = os.path.join(tpp.get_bert_path(lang=lang), \'bert_config.json\')\n    checkpoint_path = os.path.join(tpp.get_bert_path(lang=lang), \'bert_model.ckpt\')\n\n    model = keras_bert.load_trained_model_from_checkpoint(\n                                    config_path,\n                                    checkpoint_path,\n                                    training=True,\n                                    trainable=True,\n                                    seq_len=maxlen)\n    inputs = model.inputs[:2]\n    dense = model.get_layer(\'NSP-Dense\').output\n    outputs = Dense(units=num_classes, activation=activation)(dense)\n    model = Model(inputs, outputs)\n    model.compile(loss=loss_func,\n                  optimizer=U.DEFAULT_OPT,\n                  metrics=metrics)\n    return model\n\n\ndef _build_transformer(num_classes,\n                      maxlen, max_features, features,\n                      loss_func=\'categorical_crossentropy\',\n                      activation = \'softmax\', metrics=[\'accuracy\'],  verbose=1,\n                      preproc=None):\n    if not isinstance(preproc, tpp.TransformersPreprocessor): \n        raise ValueError(\'preproc must be instance of %s\' % (str(tpp.TransformersPreprocessor)))\n\n    #model = preproc.model_type.from_pretrained(preproc.model_name, num_labels=num_classes)\n    #loss_map =  {\'categorical_crossentropy\': keras.losses.CategoricalCrossentropy(from_logits=True),\n                 #\'binary_crossentropy\': keras.losses.BinaryCrossentropy(from_logits=True), \n                 #\'mse\': \'mse\'}\n    #model.compile(loss=loss_map[loss_func],\n                  #optimizer=keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08),\n                  #metrics=metrics)\n    if loss_func == \'mse\':\n        if preproc.get_classes(): \n            raise Exception(\'This is supposed to be regression problem, but preproc.get_classes() is not empty. \' +\\\n                            \'Something went wrong.  Please open a GitHub issue.\')\n            if len(preproc.get_classes()) != num_classes:\n                raise Exception(\'Number of labels from preproc.get_classes() is not equal to num_classes. \' +\\\n                                \'Something went wrong. Please open GitHub issue.\')\n    else:\n        if not preproc.get_classes():\n            raise Exception(\'This is supposed to be a classification problem, but preproc.get_classes() is empty. \' +\\\n                            \'Something went wrong.  Please open a GitHub issue.\')\n    return preproc.get_model()\n\n\n\ndef _build_nbsvm(num_classes,\n                 maxlen, max_features, features,\n                 loss_func=\'categorical_crossentropy\',\n                 activation = \'softmax\', metrics=[\'accuracy\'], verbose=1, train_data=None):\n    if train_data is None: raise ValueError(\'train_data is required\')\n    x_train = train_data[0]\n    y_train = train_data[1]\n    Y = np.array([np.argmax(row) for row in y_train])\n    num_columns = max(features) + 1\n    num_rows = len(x_train)\n\n    # set up document-term matrix\n    X = csr_matrix((num_rows, num_columns), dtype=np.int8)\n    #X = lil_matrix((num_rows, num_columns), dtype=np.int8)\n    U.vprint(\'building document-term matrix... this may take a few moments...\',\n            verbose=verbose)\n    r_ids = []\n    c_ids = []\n    data = []\n    for row_id, row in enumerate(x_train):\n        trigger = 10000\n        trigger_end =  min(row_id+trigger, num_rows)\n        if row_id % trigger == 0: \n            U.vprint(\'rows: %s-%s\' % (row_id+1, trigger_end), \n                     verbose=verbose)\n        tmp_c_ids = [column_id for column_id in row if column_id >0 ]\n        num = len(tmp_c_ids)\n        c_ids.extend(tmp_c_ids)\n        r_ids.extend([row_id]* num)\n        data.extend([1] * num)\n    X = csr_matrix( (data,(r_ids,c_ids)), shape=(num_rows, num_columns) )\n\n    # compute Naive Bayes log-count ratios\n    U.vprint(\'computing log-count ratios...\', verbose=verbose)\n    nbratios = np.stack([calc_r(i, X, Y).A1 for i in range(num_classes)])\n    nbratios = nbratios.T\n    embedding_matrix = np.zeros((num_columns, num_classes))\n    for i in range(1, num_columns): \n        for j in range(num_classes):\n            embedding_matrix[i,j] = nbratios[i,j]\n\n    # set up the model\n    inp = Input(shape=(maxlen,))\n    r = Embedding(num_columns, num_classes, input_length=maxlen, \n                  weights=[embedding_matrix], trainable=False)(inp)\n    x = Embedding(num_columns, 1, input_length=maxlen, \n                  embeddings_initializer=\'glorot_normal\')(inp)\n    x = dot([r,x], axes=1)\n    x = Flatten()(x)\n    x = Activation(activation)(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss=loss_func,\n                  optimizer=U.DEFAULT_OPT,\n                  metrics=metrics)\n    return model\n\n\ndef _build_fasttext(num_classes,\n                 maxlen, max_features, features,\n                 loss_func=\'categorical_crossentropy\',\n                 activation = \'softmax\', metrics=[\'accuracy\'],  verbose=1):\n\n    model = Sequential()\n    model.add(Embedding(max_features, 64, input_length=maxlen))\n    model.add(SpatialDropout1D(0.25))\n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dense(64, activation=\'relu\', kernel_initializer=\'he_normal\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes, activation=activation))\n    model.compile(loss=loss_func, optimizer=U.DEFAULT_OPT, metrics=metrics)\n\n    return model\n\n\ndef _build_standard_gru(num_classes,\n                 maxlen, max_features, features,\n                 loss_func=\'categorical_crossentropy\',\n                 activation = \'softmax\', metrics=[\'accuracy\'], verbose=1):\n    model = Sequential()\n    model.add(Embedding(max_features, 256, input_length = maxlen))\n    model.add(GRU(256, dropout=0.9, return_sequences=True))\n    model.add(GRU(256, dropout=0.9))\n    model.add(Dense(num_classes, activation=activation))\n    model.compile(loss=loss_func, optimizer=U.DEFAULT_OPT, metrics=metrics)\n    return model\n\n\n\ndef _build_bigru(num_classes,\n                  maxlen, max_features, features,\n                 loss_func=\'categorical_crossentropy\',\n                 activation = \'softmax\', metrics=[\'accuracy\'], verbose=1,\n                 tokenizer=None, preproc=None):\n\n\n    if tokenizer is None: raise ValueError(\'bigru requires valid Tokenizer object\')\n    if preproc is None: raise ValueError(\'bigru requires valid preproc\')\n    if not hasattr(preproc, \'lang\') or preproc.lang is None: \n        lang = \'en\'\n    else:\n        lang = preproc.lang\n    wv_url = ""https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.%s.300.vec.gz"" % (lang.split(\'-\')[0])\n    if verbose: print(\'word vectors will be loaded from: %s\' % (wv_url))\n\n\n\n    # setup pre-trained word embeddings\n    embed_size = 300\n    U.vprint(\'processing pretrained word vectors...\', verbose=verbose)\n    embeddings_index = tpp.load_wv(wv_path_or_url=wv_url, verbose=verbose)\n    word_index = tokenizer.word_index \n    #nb_words = min(max_features, len(word_index))\n    nb_words = max_features\n    embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    # define model\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(80, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPool1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(num_classes, activation=activation)(conc)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss=loss_func,\n                  optimizer=U.DEFAULT_OPT,\n                  metrics=metrics)\n    return model\n\n\n\ndef text_classifier(name, train_data, preproc=None, multilabel=None, metrics=[\'accuracy\'], verbose=1):\n    """"""\n    Build and return a text classification model.\n\n    Args:\n        name (string): one of:\n                      - \'fasttext\' for FastText model\n                      - \'nbsvm\' for NBSVM model  \n                      - \'logreg\' for logistic regression using embedding layers\n                      - \'bigru\' for Bidirectional GRU with pretrained word vectors\n                      - \'bert\' for BERT Text Classification\n                      - \'distilbert\' for Hugging Face DistilBert model\n\n        train_data (tuple): a tuple of numpy.ndarrays: (x_train, y_train) or ktrain.Dataset instance\n                            returned from one of the texts_from_* functions\n        preproc: a ktrain.text.TextPreprocessor instance.\n                 As of v0.8.0, this is required.\n        multilabel (bool):  If True, multilabel model will be returned.\n                            If false, binary/multiclass model will be returned.\n                            If None, multilabel will be inferred from data.\n        metrics(list): metrics to use\n        verbose (boolean): verbosity of output\n    Return:\n        model (Model): A Keras Model instance\n    """"""\n    if name not in TEXT_CLASSIFIERS:\n        raise ValueError(\'invalid name for text classification: %s\' % (name)) \n    if preproc is not None and not preproc.get_classes():\n        raise ValueError(\'preproc.get_classes() is empty, but required for text classification\')\n    return _text_model(name, train_data, preproc=preproc,\n                       multilabel=multilabel, classification=True, metrics=metrics, verbose=verbose)\n\n\ndef text_regression_model(name, train_data, preproc=None, metrics=[\'mae\'],  verbose=1):\n    """"""\n    Build and return a text regression model.\n\n    Args:\n        name (string): one of:\n                      - \'fasttext\' for FastText model\n                      - \'nbsvm\' for NBSVM model  \n                      - \'linreg\' for linear regression using embedding layers\n                      - \'bigru\' for Bidirectional GRU with pretrained word vectors\n                      - \'bert\' for BERT Text Classification\n                      - \'distilbert\' for Hugging Face DistilBert model\n\n        train_data (tuple): a tuple of numpy.ndarrays: (x_train, y_train)\n        preproc: a ktrain.text.TextPreprocessor instance.\n                 As of v0.8.0, this is required.\n        metrics(list): metrics to use\n        verbose (boolean): verbosity of output\n    Return:\n        model (Model): A Keras Model instance\n    """"""\n    if name not in TEXT_REGRESSION_MODELS:\n        raise ValueError(\'invalid name for text classification: %s\' % (name) )\n    if preproc is not None and preproc.get_classes():\n        raise ValueError(\'preproc.get_classes() is supposed to be empty for text regression tasks\')\n    return _text_model(name, train_data, preproc=preproc,\n                      multilabel=False, classification=False, metrics=metrics, verbose=verbose)\n'"
ktrain/text/predictor.py,3,"b'from ..imports import *\nfrom ..predictor import Predictor\nfrom .preprocessor import TextPreprocessor, TransformersPreprocessor, detect_text_format\nfrom .. import utils as U\n\nclass TextPredictor(Predictor):\n    """"""\n    predicts text classes\n    """"""\n\n    def __init__(self, model, preproc, batch_size=U.DEFAULT_BS):\n\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        if not isinstance(preproc, TextPreprocessor):\n        #if type(preproc).__name__ != \'TextPreprocessor\':\n            raise ValueError(\'preproc must be a TextPreprocessor object\')\n        self.model = model\n        self.preproc = preproc\n        self.c = self.preproc.get_classes()\n        self.batch_size = batch_size\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def predict(self, texts, return_proba=False):\n        """"""\n        Makes predictions for a list of strings where each string is a document\n        or text snippet.\n        If return_proba is True, returns probabilities of each class.\n        Args:\n          texts(str|list): For text classification, texts should be either a str or\n                           a list of str.\n                           For sentence pair classification, texts should be either\n                           a tuple of form (str, str) or list of tuples.\n                           A single tuple of the form (str, str) is automatically treated as sentence pair classification, so\n                           please refrain from using tuples for text classification tasks.\n          return_proba(bool): If True, return probabilities instead of predicted class labels\n        """"""\n\n        is_array, is_pair = detect_text_format(texts)\n        if not is_array: texts = [texts]\n\n        classification, multilabel = U.is_classifier(self.model)\n\n        # get predictions\n        if U.is_huggingface(model=self.model):\n            tseq = self.preproc.preprocess_test(texts, verbose=0)\n            tseq.batch_size = self.batch_size\n            texts = tseq.to_tfdataset(train=False)\n            preds = self.model.predict(texts)\n        else:\n            texts = self.preproc.preprocess(texts)\n            preds = self.model.predict(texts, batch_size=self.batch_size)\n\n        # process predictions\n        if U.is_huggingface(model=self.model):\n            # convert logits to probabilities for Hugging Face models\n            if multilabel and self.c:\n                preds = activations.sigmoid(tf.convert_to_tensor(preds)).numpy()\n            elif self.c:\n                preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n            else:\n                preds = np.squeeze(preds)\n                if len(preds.shape) == 0: preds = np.expand_dims(preds, -1)\n        result =  preds if return_proba or multilabel or not self.c else [self.c[np.argmax(pred)] for pred in preds] \n        if multilabel and not return_proba:\n            result =  [list(zip(self.c, r)) for r in result]\n        if not is_array: return result[0]\n        else:      return result\n\n\n\n    def predict_proba(self, texts):\n        """"""\n        Makes predictions for a list of strings where each string is a document\n        or text snippet.\n        Returns probabilities of each class.\n        """"""\n        return self.predict(texts, return_proba=True)\n\n\n    def explain(self, doc, truncate_len=512, all_targets=False, n_samples=2500):\n        """"""\n        Highlights text to explain prediction\n        Args:\n            doc (str): text of documnet\n            truncate_len(int): truncate document to this many words\n            all_targets(bool):  If True, show visualization for\n                                each target.\n            n_samples(int): number of samples to generate and train on.\n                            Larger values give better results, but will take more time.\n                            Lower this value if explain is taking too long.\n        """"""\n        is_array, is_pair = detect_text_format(doc)\n        if is_pair: \n            warnings.warn(\'currently_unsupported: explain does not currently support sentence pair classification\')\n            return\n        if not self.c:\n            warnings.warn(\'currently_unsupported: explain does not support text regression\')\n            return\n        try:\n            import eli5\n            from eli5.lime import TextExplainer\n        except:\n            msg = \'ktrain requires a forked version of eli5 to support tf.keras. \'+\\\n                  \'Install with: pip3 install git+https://github.com/amaiya/eli5@tfkeras_0_10_1\'\n            warnings.warn(msg)\n            return\n\n        prediction = [self.predict(doc)] if not all_targets else None\n\n        if not isinstance(doc, str): raise Exception(\'text must of type str\')\n        if self.preproc.is_nospace_lang():\n            doc = self.preproc.process_chinese([doc])\n            doc = doc[0]\n        doc = \' \'.join(doc.split()[:truncate_len])\n        te = TextExplainer(random_state=42, n_samples=n_samples)\n        _ = te.fit(doc, self.predict_proba)\n        return te.show_prediction(target_names=self.preproc.get_classes(), targets=prediction)\n\n\n    def analyze_valid(self, val_tup, print_report=True, multilabel=None):\n        """"""\n        Makes predictions on validation set and returns the confusion matrix.\n        Accepts as input the validation set in the standard form of a tuple of\n        two arrays: (X_test, y_test), wehre X_test is a Numpy array of strings\n        where each string is a document or text snippet in the validation set.\n\n        Optionally prints a classification report.\n        Currently, this method is only supported for binary and multiclass \n        problems, not multilabel classification problems.\n        """"""\n        U.data_arg_check(val_data=val_tup, val_required=True, ndarray_only=True)\n        if multilabel is None:\n            multilabel = U.is_multilabel(val_tup)\n        if multilabel:\n            warnings.warn(\'multilabel_confusion_matrix not yet supported\')\n            return\n\n        y_true = val_tup[1]\n        y_true = np.argmax(y_true, axis=1)\n        y_pred = self.model.predict(val_tup[0])\n        y_pred = np.argmax(y_pred, axis=1)\n        \n        if print_report:\n            print(classification_report(y_true, y_pred, target_names=self.c))\n            cm_func = confusion_matrix\n        cm =  confusion_matrix(y_true,  y_pred)\n        return cm\n\n\n    def _save_model(self, fpath):\n        if isinstance(self.preproc, TransformersPreprocessor):\n            self.model.save_pretrained(fpath)\n        else:\n            super()._save_model(fpath)\n        return\n\n'"
ktrain/text/preprocessor.py,33,"b'from ..imports import *\nfrom .. import utils as U\nfrom ..preprocessor import Preprocessor\nfrom ..data import SequenceDataset\nfrom . import textutils as TU\n\nDistilBertTokenizer = transformers.DistilBertTokenizer\nDISTILBERT= \'distilbert\'\n\nfrom transformers import BertConfig, TFBertForSequenceClassification, BertTokenizer, TFBertModel\nfrom transformers import XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer, TFXLNetModel \nfrom transformers import XLMConfig, TFXLMForSequenceClassification, XLMTokenizer, TFXLMModel\nfrom transformers import RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer, TFRobertaModel\nfrom transformers import DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer, TFDistilBertModel\nfrom transformers import AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer, TFAlbertModel\nfrom transformers import CamembertConfig, TFCamembertForSequenceClassification, CamembertTokenizer, TFCamembertModel\nfrom transformers import XLMRobertaConfig, TFXLMRobertaForSequenceClassification, XLMRobertaTokenizer, TFXLMRobertaModel\nfrom transformers import AutoConfig, TFAutoModelForSequenceClassification, AutoTokenizer, TFAutoModel\n\nTRANSFORMER_MODELS = {\n    \'bert\':       (BertConfig, TFBertForSequenceClassification, BertTokenizer, TFBertModel),\n    \'xlnet\':      (XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer, TFXLNetModel),\n    \'xlm\':        (XLMConfig, TFXLMForSequenceClassification, XLMTokenizer, TFXLMModel),\n    \'roberta\':    (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer, TFRobertaModel),\n    \'distilbert\': (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer, TFDistilBertModel),\n    \'albert\':     (AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer, TFAlbertModel),\n    \'camembert\':  (CamembertConfig, TFCamembertForSequenceClassification, CamembertTokenizer, TFCamembertModel),\n    \'xlm_roberta\':  (XLMRobertaConfig, TFXLMRobertaForSequenceClassification, XLMRobertaTokenizer, TFXLMRobertaModel)\n}\n\n\nNOSPACE_LANGS = [\'zh-cn\', \'zh-tw\', \'ja\']\n\n\ndef is_nospace_lang(lang):\n    return lang in NOSPACE_LANGS\n\n\ndef fname_from_url(url):\n    return os.path.split(url)[-1]\n\n\n#------------------------------------------------------------------------------\n# Word Vectors\n#------------------------------------------------------------------------------\nWV_URL = \'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\'\n#WV_URL = \'http://nlp.stanford.edu/data/glove.6B.zip\n\n\ndef get_wv_path(wv_path_or_url=WV_URL):\n    # process if file path given\n    if os.path.isfile(wv_path_or_url) and wv_path_or_url.endswith(\'vec\'): return wv_path_or_url\n    elif os.path.isfile(wv_path_or_url):\n        raise ValueError(""wv_path_or_url must either be URL .vec.zip or .vec.gz file or file path to .vec file"")\n\n    # process if URL is given\n    fasttext_url = \'https://dl.fbaipublicfiles.com/fasttext\'\n    if not wv_path_or_url.startswith(fasttext_url):\n        raise ValueError(\'selected word vector file must be from %s\'% (fasttext_url))\n    if not wv_path_or_url.endswith(\'.vec.zip\') and not wv_path_or_url.endswith(\'vec.gz\'):\n        raise ValueError(\'If wv_path_or_url is URL, must be .vec.zip filea from Facebook fasttext site.\')\n\n    ktrain_data = U.get_ktrain_data()\n    zip_fpath = os.path.join(ktrain_data, fname_from_url(wv_path_or_url))\n    wv_path =  os.path.join(ktrain_data, os.path.splitext(fname_from_url(wv_path_or_url))[0])\n    if not os.path.isfile(wv_path):\n        # download zip\n        print(\'downloading pretrained word vectors to %s ...\' % (ktrain_data))\n        U.download(wv_path_or_url, zip_fpath)\n\n        # unzip\n        print(\'\\nextracting pretrained word vectors...\')\n        if wv_path_or_url.endswith(\'.vec.zip\'):\n            with zipfile.ZipFile(zip_fpath, \'r\') as zip_ref:\n                zip_ref.extractall(ktrain_data)\n        else: # .vec.gz\n            with gzip.open(zip_fpath, \'rb\') as f_in:\n                with open(wv_path, \'wb\') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n        print(\'done.\\n\')\n\n        # cleanup\n        print(\'cleanup downloaded zip...\')\n        try:\n            os.remove(zip_fpath)\n            print(\'done.\\n\')\n        except OSError:\n            print(\'failed to cleanup/remove %s\' % (zip_fpath))\n    return wv_path\n\n\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype=\'float32\')\n\n\n\n#def load_wv(wv_path=None, verbose=1):\n    #if verbose: print(\'Loading pretrained word vectors...this may take a few moments...\')\n    #if wv_path is None: wv_path = get_wv_path()\n    #embeddings_index = dict(get_coefs(*o.rstrip().rsplit(\' \')) for o in open(wv_path, encoding=\'utf-8\'))\n    #if verbose: print(\'Done.\')\n    #return embeddings_index\n\n\ndef file_len(fname):\n    with open(fname, encoding=\'utf-8\') as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1\n\n\ndef load_wv(wv_path_or_url=WV_URL, verbose=1):\n    wv_path = get_wv_path(wv_path_or_url)\n    if verbose: print(\'loading pretrained word vectors...this may take a few moments...\')\n    length = file_len(wv_path)\n    tups = []\n    mb = master_bar(range(1))\n    for i in mb:\n        f = open(wv_path, encoding=\'utf-8\')\n        for o in progress_bar(range(length), parent=mb):\n            o = f.readline()\n            tups.append(get_coefs(*o.rstrip().rsplit(\' \')))\n        f.close()\n        #if verbose: mb.write(\'done.\')\n    return dict(tups)\n\n\n\n#------------------------------------------------------------------------------\n# BERT\n#------------------------------------------------------------------------------\n\n#BERT_PATH = os.path.join(os.path.dirname(os.path.abspath(localbert.__file__)), \'uncased_L-12_H-768_A-12\')\nBERT_URL = \'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\'\nBERT_URL_MULTI = \'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\'\nBERT_URL_CN = \'https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\'\n\ndef get_bert_path(lang=\'en\'):\n    if lang == \'en\':\n        bert_url = BERT_URL\n    elif lang.startswith(\'zh-\'):\n        bert_url = BERT_URL_CN\n    else:\n        bert_url = BERT_URL_MULTI\n    ktrain_data = U.get_ktrain_data()\n    zip_fpath = os.path.join(ktrain_data, fname_from_url(bert_url))\n    bert_path = os.path.join( ktrain_data, os.path.splitext(fname_from_url(bert_url))[0] )\n    if not os.path.isdir(bert_path) or \\\n       not os.path.isfile(os.path.join(bert_path, \'bert_config.json\')) or\\\n       not os.path.isfile(os.path.join(bert_path, \'bert_model.ckpt.data-00000-of-00001\')) or\\\n       not os.path.isfile(os.path.join(bert_path, \'bert_model.ckpt.index\')) or\\\n       not os.path.isfile(os.path.join(bert_path, \'bert_model.ckpt.meta\')) or\\\n       not os.path.isfile(os.path.join(bert_path, \'vocab.txt\')):\n        # download zip\n        print(\'downloading pretrained BERT model (%s)...\' % (fname_from_url(bert_url)))\n        U.download(bert_url, zip_fpath)\n\n        # unzip\n        print(\'\\nextracting pretrained BERT model...\')\n        with zipfile.ZipFile(zip_fpath, \'r\') as zip_ref:\n            zip_ref.extractall(ktrain_data)\n        print(\'done.\\n\')\n\n        # cleanup\n        print(\'cleanup downloaded zip...\')\n        try:\n            os.remove(zip_fpath)\n            print(\'done.\\n\')\n        except OSError:\n            print(\'failed to cleanup/remove %s\' % (zip_fpath))\n    return bert_path\n\n\n\ndef bert_tokenize(docs, tokenizer, maxlen, verbose=1):\n    indices = []\n    mb = master_bar(range(1))\n    for i in mb:\n        for doc in progress_bar(docs, parent=mb):\n            ids, segments = tokenizer.encode(doc, max_len=maxlen)\n            indices.append(ids)\n        if verbose: mb.write(\'done.\')\n    zeros = np.zeros_like(indices)\n    return [np.array(indices), np.array(zeros)]\n\n#------------------------------------------------------------------------------\n# Transformers UTILITIES\n#------------------------------------------------------------------------------\n\n#def convert_to_tfdataset(csv):\n    #def gen():\n        #for ex in csv:\n            #yield  {\'idx\': ex[0],\n                     #\'sentence\': ex[1],\n                     #\'label\': str(ex[2])}\n    #return tf.data.Dataset.from_generator(gen,\n        #{\'idx\': tf.int64,\n          #\'sentence\': tf.string,\n          #\'label\': tf.int64})\n\n\n#def features_to_tfdataset(features):\n\n#    def gen():\n#        for ex in features:\n#            yield ({\'input_ids\': ex.input_ids,\n#                     \'attention_mask\': ex.attention_mask,\n#                     \'token_type_ids\': ex.token_type_ids},\n#                    ex.label)\n\n#    return tf.data.Dataset.from_generator(gen,\n#        ({\'input_ids\': tf.int32,\n#          \'attention_mask\': tf.int32,\n#          \'token_type_ids\': tf.int32},\n#         tf.int64),\n#        ({\'input_ids\': tf.TensorShape([None]),\n#          \'attention_mask\': tf.TensorShape([None]),\n#          \'token_type_ids\': tf.TensorShape([None])},\n#         tf.TensorShape([None])))\n#         #tf.TensorShape(])))\n\ndef _is_sentence_pair(tup):\n    if isinstance(tup, (tuple)) and len(tup) == 2 and\\\n            isinstance(tup[0], str) and isinstance(tup[1], str):\n        return True\n    else:\n        if isinstance(tup, (list, np.ndarray)) and len(tup) == 2 and\\\n                isinstance(tup[0], str) and isinstance(tup[1], str):\n            warnings.warn(\'List or array of two texts supplied, so task being treated as text classification. \' +\\\n                          \'If this is a sentence pair classification task, please cast to tuple.\')\n        return False\n\n\ndef detect_text_format(texts):\n    is_pair = False\n    is_array = False\n    err_msg = \'invalid text format: texts should be list of strings or list of sentence pairs in form of tuples (str, str)\'\n    if _is_sentence_pair(texts):\n        is_pair=True\n        is_array = False\n    elif isinstance(texts, (tuple, list, np.ndarray)):\n        is_array = True\n        if len(texts) == 0: raise ValueError(\'texts is empty\')\n        peek = texts[0]\n        is_pair = _is_sentence_pair(peek)\n        if not is_pair and not isinstance(peek, str):\n            raise ValueError(err_msg)\n    return is_array, is_pair\n\n\n\ndef hf_features_to_tfdataset(features_list, labels):\n    features_list = np.array(features_list)\n    labels = np.array(labels) if labels is not None else None\n    tfdataset = tf.data.Dataset.from_tensor_slices((features_list, labels))\n    tfdataset = tfdataset.map(lambda x,y: ({\'input_ids\': x[0], \n                                            \'attention_mask\': x[1], \n                                             \'token_type_ids\': x[2]}, y))\n\n    return tfdataset\n\n\n\ndef hf_convert_example(text_a, text_b=None, tokenizer=None,\n                       max_length=512,\n                       pad_on_left=False,\n                       pad_token=0,\n                       pad_token_segment_id=0,\n                       mask_padding_with_zero=True):\n    """"""\n    convert InputExample to InputFeature for Hugging Face transformer\n    """"""\n    if tokenizer is None: raise ValueError(\'tokenizer is required\')\n    inputs = tokenizer.encode_plus(\n        text_a,\n        text_b,\n        add_special_tokens=True,\n        return_token_type_ids=True,\n        max_length=max_length,\n    )\n    input_ids, token_type_ids = inputs[""input_ids""], inputs[""token_type_ids""]\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    padding_length = max_length - len(input_ids)\n    if pad_on_left:\n        input_ids = ([pad_token] * padding_length) + input_ids\n        attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n        token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n    else:\n        input_ids = input_ids + ([pad_token] * padding_length)\n        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n\n    assert len(input_ids) == max_length, ""Error with input length {} vs {}"".format(len(input_ids), max_length)\n    assert len(attention_mask) == max_length, ""Error with input length {} vs {}"".format(len(attention_mask), max_length)\n    assert len(token_type_ids) == max_length, ""Error with input length {} vs {}"".format(len(token_type_ids), max_length)\n\n\n    #if ex_index < 1:\n        #print(""*** Example ***"")\n        #print(""guid: %s"" % (example.guid))\n        #print(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n        #print(""attention_mask: %s"" % "" "".join([str(x) for x in attention_mask]))\n        #print(""token_type_ids: %s"" % "" "".join([str(x) for x in token_type_ids]))\n        #print(""label: %s (id = %d)"" % (example.label, label))\n\n    return [input_ids, attention_mask, token_type_ids]\n\n\n\n\ndef hf_convert_examples(texts, y=None, tokenizer=None,\n                        max_length=512,\n                        pad_on_left=False,\n                        pad_token=0,\n                        pad_token_segment_id=0,\n                        mask_padding_with_zero=True):\n    """"""\n    Loads a data file into a list of ``InputFeatures``\n    Args:\n        texts: texts of documents or sentence pairs\n        y:  labels for documents\n        tokenizer: Instance of a tokenizer that will tokenize the examples\n        max_length: Maximum example length\n        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n        pad_token: Padding token\n        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n            actual values)\n    Returns:\n        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n        a list of task-specific ``InputFeatures`` which can be fed to the model.\n    """"""\n\n    is_array, is_pair = detect_text_format(texts)\n    data = []\n    mb = master_bar(range(1))\n    features_list = []\n    labels = []\n    for i in mb:\n        for (idx, text) in enumerate(progress_bar(texts, parent=mb)):\n            if is_pair:\n                text_a = text[0]\n                text_b = text[1]\n            else:\n                text_a = text\n                text_b = None\n            features = hf_convert_example(text_a, text_b=text_b, tokenizer=tokenizer,\n                                          max_length=max_length,\n                                          pad_on_left=pad_on_left,\n                                          pad_token=pad_token,\n                                          pad_token_segment_id=pad_token_segment_id,\n                                          mask_padding_with_zero=mask_padding_with_zero)\n            features_list.append(features)\n            labels.append(y[idx] if y is not None else None)\n    #tfdataset = hf_features_to_tfdataset(features_list, labels)\n    #return tfdataset\n    #return (features_list, labels)\n    # HF_EXCEPTION\n    # due to issues in transormers library and TF2 tf.Datasets, arrays are converted\n    # to iterators on-the-fly\n    #return  TransformerSequence(np.array(features_list), np.array(labels))\n    return  TransformerDataset(np.array(features_list), np.array(labels))\n\n\n#------------------------------------------------------------------------------\n\n\nclass TextPreprocessor(Preprocessor):\n    """"""\n    Text preprocessing base class\n    """"""\n\n    def __init__(self, maxlen, class_names, lang=\'en\', multilabel=None):\n\n        self.set_classes(class_names) # converts to list of necessary\n        self.maxlen = maxlen\n        self.lang = lang\n        self.multilabel = multilabel # currently, this is always initially set None until set by set_multilabel\n        self.preprocess_train_called = False\n        self.label_encoder = None # only set if y is in string format\n        self.c = self.c.tolist() if isinstance(self.c, np.ndarray) else self.c\n\n\n    def migrate_classes(self, class_names, classes):\n        # NOTE: this method transforms to np.ndarray to list.\n        # If removed and ""if class_names"" is issued prior to set_classes(), an error will occur.\n        class_names = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names\n        classes = classes.tolist() if isinstance(classes, np.ndarray) else classes\n\n        if not class_names and classes:\n            class_names = classes\n            warnings.warn(\'The class_names argument is replacing the classes argument. Please update your code.\')\n        return class_names\n\n\n\n    def check_trained(self):\n        if not self.preprocess_train_called:\n            raise Exception(\'preprocess_train must be called\')\n\n\n    def get_preprocessor(self):\n        raise NotImplementedError\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def set_classes(self, class_names):\n        self.c = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names\n\n\n    def preprocess(self, texts):\n        raise NotImplementedError\n\n\n    def set_multilabel(self, data, mode, verbose=1):\n        if mode == \'train\' and self.get_classes():\n            original_multilabel = self.multilabel\n            discovered_multilabel = U.is_multilabel(data)\n            if original_multilabel is None:\n                self.multilabel = discovered_multilabel\n            elif original_multilabel is True and discovered_multilabel is False:\n                warnings.warn(\'The multilabel=True argument was supplied, but labels do not indicate \'+\\\n                              \'a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.\')\n            elif original_multilabel is False and discovered_multilabel is True:\n                warnings.warn(\'The multilabel=False argument was supplied, but labels inidcate that  \'+\\\n                              \'this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.\')\n            U.vprint(""Is Multi-Label? %s"" % (self.multilabel), verbose=verbose)\n\n\n    def undo(self, doc):\n        """"""\n        undoes preprocessing and returns raw data by:\n        converting a list or array of Word IDs back to words\n        """"""\n        raise NotImplementedError\n\n\n    def is_chinese(self):\n        return TU.is_chinese(self.lang)\n\n\n    def is_nospace_lang(self):\n        return TU.is_nospace_lang(self.lang)\n\n\n    def process_chinese(self, texts, lang=None):\n        #if lang is None: lang = langdetect.detect(texts[0])\n        if lang is None: lang = TU.detect_lang(texts)\n        if not TU.is_nospace_lang(lang): return texts\n        return TU.split_chinese(texts)\n\n\n    @classmethod\n    def seqlen_stats(cls, list_of_texts):\n        """"""\n        compute sequence length stats from\n        list of texts in any spaces-segmented language\n        Args:\n            list_of_texts: list of strings\n        Returns:\n            dict: dictionary with keys: mean, 95percentile, 99percentile\n        """"""\n        counts = []\n        for text in list_of_texts:\n            if isinstance(text, (list, np.ndarray)):\n                lst = text\n            else:\n                lst = text.split()\n            counts.append(len(lst))\n        p95 = np.percentile(counts, 95)\n        p99 = np.percentile(counts, 99)\n        avg = sum(counts)/len(counts)\n        return {\'mean\':avg, \'95percentile\': p95, \'99percentile\':p99}\n\n\n    def print_seqlen_stats(self, texts, mode, verbose=1):\n        """"""\n        prints stats about sequence lengths\n        """"""\n        if verbose and not self.is_nospace_lang():\n            stat_dict = TextPreprocessor.seqlen_stats(texts)\n            print( ""%s sequence lengths:"" % mode)\n            for k in stat_dict:\n                print(""\\t%s : %s"" % (k, int(round(stat_dict[k]))))\n\n\n    def _transform_y(self, y_data):\n        """"""\n        preprocess y\n        If shape of y is 1, then task is considered classification if self.c exists\n        or regression if not.\n        """"""\n        if y_data is None: return y_data\n        y_data = np.array(y_data) if type(y_data) == list else y_data\n\n        # check for errors and warnings\n        if not isinstance(y_data[0], str) and len(y_data.shape) ==1 and not self.get_classes():\n            warnings.warn(\'Task is being treated as TEXT REGRESSION because \' +\\\n                          \'class_names argument was not supplied. \' + \\\n                          \'If this is incorrect, supply class_names argument.\')\n        elif len(y_data.shape) > 1 and not self.get_classes():\n            raise ValueError(\'y-values are 1-hot or multi-hot encoded but self.get_classes() is empty. \' +\\\n                             \'The classes argument should have been supplied.\')\n\n        # convert string labels to integers, if necessary\n        if isinstance(y_data[0], str):\n            if self.label_encoder is None:\n                self.label_encoder = LabelEncoder()\n                self.label_encoder.fit(y_data)\n                if self.get_classes(): warnings.warn(\'class_names argument was ignored, as they were extracted from string labels in dataset\')\n                self.set_classes(self.label_encoder.classes_)\n            y_data = self.label_encoder.transform(y_data)\n\n\n        # if shape is 1, this is either a classification or regression task \n        # depending on class_names existing\n        y_data = to_categorical(y_data) if len(y_data.shape) == 1 and self.get_classes() else y_data\n        return y_data\n\n\n\n\n\n\nclass StandardTextPreprocessor(TextPreprocessor):\n    """"""\n    Standard text preprocessing\n    """"""\n\n    def __init__(self, maxlen, max_features, class_names=[], classes=[], \n                 lang=\'en\', ngram_range=1, multilabel=None):\n        class_names = self.migrate_classes(class_names, classes)\n        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)\n        self.tok = None\n        self.tok_dct = {}\n        self.max_features = max_features\n        self.ngram_range = ngram_range\n\n\n    def get_preprocessor(self):\n        return (self.tok, self.tok_dct)\n\n\n    def preprocess(self, texts):\n        return self.preprocess_test(texts, verbose=0)[0]\n\n\n    def undo(self, doc):\n        """"""\n        undoes preprocessing and returns raw data by:\n        converting a list or array of Word IDs back to words\n        """"""\n        dct = self.tok.index_word\n        return "" "".join([dct[wid] for wid in doc if wid != 0 and wid in dct])\n\n\n    def preprocess_train(self, train_text, y_train, verbose=1):\n        """"""\n        preprocess training set\n        """"""\n        if self.lang is None: self.lang = TU.detect_lang(train_text)\n\n\n        U.vprint(\'language: %s\' % (self.lang), verbose=verbose)\n\n        # special processing if Chinese\n        train_text = self.process_chinese(train_text, lang=self.lang)\n\n        # extract vocabulary\n        self.tok = Tokenizer(num_words=self.max_features)\n        self.tok.fit_on_texts(train_text)\n        U.vprint(\'Word Counts: {}\'.format(len(self.tok.word_counts)), verbose=verbose)\n        U.vprint(\'Nrows: {}\'.format(len(train_text)), verbose=verbose)\n\n        # convert to word IDs\n        x_train = self.tok.texts_to_sequences(train_text)\n        U.vprint(\'{} train sequences\'.format(len(x_train)), verbose=verbose)\n        self.print_seqlen_stats(x_train, \'train\', verbose=verbose)\n\n        # add ngrams\n        x_train = self._fit_ngrams(x_train, verbose=verbose)\n\n        # pad sequences\n        x_train = sequence.pad_sequences(x_train, maxlen=self.maxlen)\n        U.vprint(\'x_train shape: ({},{})\'.format(x_train.shape[0], x_train.shape[1]), verbose=verbose)\n\n        # transform y\n        y_train = self._transform_y(y_train)\n        if y_train is not None and verbose:\n            print(\'y_train shape: %s\' % (y_train.shape,))\n\n        # return\n        result =  (x_train, y_train)\n        self.set_multilabel(result, \'train\')\n        self.preprocess_train_called = True\n        return result\n\n\n    def preprocess_test(self, test_text, y_test=None, verbose=1):\n        """"""\n        preprocess validation or test dataset\n        """"""\n        self.check_trained()\n        if self.tok is None or self.lang is None:\n            raise Exception(\'Unfitted tokenizer or missing language. Did you run preprocess_train first?\')\n\n        # check for and process chinese\n        test_text = self.process_chinese(test_text, self.lang)\n\n        # convert to word IDs\n        x_test = self.tok.texts_to_sequences(test_text)\n        U.vprint(\'{} test sequences\'.format(len(x_test)), verbose=verbose)\n        self.print_seqlen_stats(x_test, \'test\', verbose=verbose)\n\n        # add n-grams\n        x_test = self._add_ngrams(x_test, mode=\'test\', verbose=verbose)\n\n\n        # pad sequences\n        x_test = sequence.pad_sequences(x_test, maxlen=self.maxlen)\n        U.vprint(\'x_test shape: ({},{})\'.format(x_test.shape[0], x_test.shape[1]), verbose=verbose)\n\n        # transform y\n        y_test = self._transform_y(y_test)\n        if y_test is not None and verbose:\n            print(\'y_test shape: %s\' % (y_test.shape,))\n\n\n        # return\n        return (x_test, y_test)\n\n\n\n    def _fit_ngrams(self, x_train, verbose=1):\n        self.tok_dct = {}\n        if self.ngram_range < 2: return x_train\n        U.vprint(\'Adding {}-gram features\'.format(self.ngram_range), verbose=verbose)\n        # Create set of unique n-gram from the training set.\n        ngram_set = set()\n        for input_list in x_train:\n            for i in range(2, self.ngram_range + 1):\n                set_of_ngram = self._create_ngram_set(input_list, ngram_value=i)\n                ngram_set.update(set_of_ngram)\n\n        # Dictionary mapping n-gram token to a unique integer.\n        # Integer values are greater than max_features in order\n        # to avoid collision with existing features.\n        start_index = self.max_features + 1\n        token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n        indice_token = {token_indice[k]: k for k in token_indice}\n        self.tok_dct = token_indice\n\n        # max_features is the highest integer that could be found in the dataset.\n        self.max_features = np.max(list(indice_token.keys())) + 1\n        U.vprint(\'max_features changed to %s with addition of ngrams\' % (self.max_features), verbose=verbose)\n\n        # Augmenting x_train with n-grams features\n        x_train = self._add_ngrams(x_train, verbose=verbose, mode=\'train\')\n        return x_train\n\n\n    def _add_ngrams(self, sequences, verbose=1, mode=\'test\'):\n        """"""\n        Augment the input list of list (sequences) by appending n-grams values.\n        Example: adding bi-gram\n        """"""\n        token_indice = self.tok_dct\n        if self.ngram_range < 2: return sequences\n        new_sequences = []\n        for input_list in sequences:\n            new_list = input_list[:]\n            for ngram_value in range(2, self.ngram_range + 1):\n                for i in range(len(new_list) - ngram_value + 1):\n                    ngram = tuple(new_list[i:i + ngram_value])\n                    if ngram in token_indice:\n                        new_list.append(token_indice[ngram])\n            new_sequences.append(new_list)\n        U.vprint(\'Average {} sequence length with ngrams: {}\'.format(mode,\n            np.mean(list(map(len, new_sequences)), dtype=int)), verbose=verbose)    \n        self.print_seqlen_stats(new_sequences, \'%s (w/ngrams)\' % mode, verbose=verbose)\n        return new_sequences\n\n\n\n    def _create_ngram_set(self, input_list, ngram_value=2):\n        """"""\n        Extract a set of n-grams from a list of integers.\n        >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n        {(4, 9), (4, 1), (1, 4), (9, 4)}\n        >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n        [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n        """"""\n        return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\n\n    def ngram_count(self):\n        if not self.tok_dct: return 1\n        s = set()\n        for k in self.tok_dct.keys():\n            s.add(len(k))\n        return max(list(s))\n\n\nclass BERTPreprocessor(TextPreprocessor):\n    """"""\n    text preprocessing for BERT model\n    """"""\n\n    def __init__(self, maxlen, max_features, class_names=[], classes=[], \n                lang=\'en\', ngram_range=1, multilabel=None):\n        class_names = self.migrate_classes(class_names, classes)\n\n\n        if maxlen > 512: raise ValueError(\'BERT only supports maxlen <= 512\')\n\n        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)\n        vocab_path = os.path.join(get_bert_path(lang=lang), \'vocab.txt\')\n        token_dict = {}\n        with codecs.open(vocab_path, \'r\', \'utf8\') as reader:\n            for line in reader:\n                token = line.strip()\n                token_dict[token] = len(token_dict)\n        tokenizer = BERT_Tokenizer(token_dict)\n        self.tok = tokenizer\n        self.tok_dct = dict((v,k) for k,v in token_dict.items())\n        self.max_features = max_features # ignored\n        self.ngram_range = 1 # ignored\n\n\n    def get_preprocessor(self):\n        return (self.tok, self.tok_dct)\n\n\n\n    def preprocess(self, texts):\n        return self.preprocess_test(texts, verbose=0)[0]\n\n\n    def undo(self, doc):\n        """"""\n        undoes preprocessing and returns raw data by:\n        converting a list or array of Word IDs back to words\n        """"""\n        dct = self.tok_dct\n        return "" "".join([dct[wid] for wid in doc if wid != 0 and wid in dct])\n\n\n    def preprocess_train(self, texts, y=None, mode=\'train\', verbose=1):\n        """"""\n        preprocess training set\n        """"""\n        if mode == \'train\' and y is None:\n            raise ValueError(\'y is required when mode=train\')\n        if self.lang is None and mode==\'train\': self.lang = TU.detect_lang(texts)\n        U.vprint(\'preprocessing %s...\' % (mode), verbose=verbose)\n        U.vprint(\'language: %s\' % (self.lang), verbose=verbose)\n\n        x = bert_tokenize(texts, self.tok, self.maxlen, verbose=verbose)\n\n        # transform y\n        y = self._transform_y(y)\n        result = (x, y)\n        self.set_multilabel(result, mode)\n        if mode == \'train\': self.preprocess_train_called = True\n        return result\n\n\n\n    def preprocess_test(self, texts, y=None, mode=\'test\', verbose=1):\n        self.check_trained()\n        return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)\n\n\nclass TransformersPreprocessor(TextPreprocessor):\n    """"""\n    text preprocessing for Hugging Face Transformer models\n    """"""\n\n    def __init__(self,  model_name,\n                maxlen, max_features, class_names=[], classes=[], \n                lang=\'en\', ngram_range=1, multilabel=None):\n        class_names = self.migrate_classes(class_names, classes)\n\n        if maxlen > 512: raise ValueError(\'Transformer models only supports maxlen <= 512\')\n\n        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)\n\n        self.model_name = model_name\n        self.name = model_name.split(\'-\')[0]\n        if model_name.startswith(\'xlm-roberta\'): \n            self.name = \'xlm_roberta\'\n            self.model_name = \'jplu/tf-\' + self.model_name\n        else:\n            self.name = model_name.split(\'-\')[0]\n        if self.name not in TRANSFORMER_MODELS:\n            #raise ValueError(\'unsupported model name %s\' % (model_name))\n            self.config = AutoConfig.from_pretrained(model_name)\n            self.model_type = TFAutoModelForSequenceClassification\n            self.tokenizer_type = AutoTokenizer\n        else:\n            self.config = None # use default config\n            self.model_type = TRANSFORMER_MODELS[self.name][1]\n            self.tokenizer_type = TRANSFORMER_MODELS[self.name][2]\n\n        if ""bert-base-japanese"" in model_name:\n            self.tokenizer_type = transformers.BertJapaneseTokenizer\n\n        # NOTE: As of v0.16.1, do not unnecessarily instantiate tokenizer\n        # as it will be saved/pickled along with Preprocessor, which causes\n        # problems for some community-uploaded models like bert-base-japanse-whole-word-masking.\n        #tokenizer = self.tokenizer_type.from_pretrained(model_name)\n        #self.tok = tokenizer\n        self.tok = None # not pickled,  see __getstate__ \n\n        self.tok_dct = None\n        self.max_features = max_features # ignored\n        self.ngram_range = 1 # ignored\n\n\n    def __getstate__(self):\n        return {k: v for k, v in self.__dict__.items() if k not in [\'tok\']}\n\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        if not hasattr(self, \'tok\'): self.tok = None\n\n\n    def get_preprocessor(self):\n        if self.tok is None:\n            self.tok = self.tokenizer_type.from_pretrained(self.model_name)\n        return (self.tok, self.tok_dct)\n\n\n\n    def preprocess(self, texts):\n        tseq = self.preprocess_test(texts, verbose=0)\n        return tseq.to_tfdataset(train=False)\n\n\n    def undo(self, doc):\n        """"""\n        undoes preprocessing and returns raw data by:\n        converting a list or array of Word IDs back to words\n        """"""\n        tok, _ = self.get_preprocessor()\n        return self.tok.convert_ids_to_tokens(doc)\n        #raise Exception(\'currently_unsupported: Transformers.Preprocessor.undo is not yet supported\')\n\n\n    def preprocess_train(self, texts, y=None, mode=\'train\', verbose=1):\n        """"""\n        preprocess training set\n        """"""\n\n        U.vprint(\'preprocessing %s...\' % (mode), verbose=verbose)\n\n        # detect sentence pairs\n        is_array, is_pair = detect_text_format(texts)\n        if not is_array: raise ValueError(\'texts must be a list of strings or a list of sentence pairs\')\n\n        # detect language\n        if self.lang is None and mode==\'train\': self.lang = TU.detect_lang(texts)\n        U.vprint(\'language: %s\' % (self.lang), verbose=verbose)\n\n        # print stats\n        if not is_pair: self.print_seqlen_stats(texts, mode, verbose=verbose)\n        if is_pair: U.vprint(\'sentence pairs detected\', verbose=verbose)\n\n        # transform y\n        if y is None and mode == \'train\':\n            raise ValueError(\'y is required for training sets\')\n        elif y is None:\n            y = np.array([1] * len(texts))\n        y = self._transform_y(y)\n\n        # convert examples\n        tok, _ = self.get_preprocessor()\n        dataset = hf_convert_examples(texts, y=y, tokenizer=tok, max_length=self.maxlen,\n                                      pad_on_left=bool(self.name in [\'xlnet\']),\n                                      pad_token=tok.convert_tokens_to_ids([tok.pad_token][0]),\n                                      pad_token_segment_id=4 if self.name in [\'xlnet\'] else 0)\n        self.set_multilabel(dataset, mode, verbose=verbose)\n        if mode == \'train\':  self.preprocess_train_called = True\n        return dataset\n\n\n\n    def preprocess_test(self, texts, y=None, mode=\'test\', verbose=1):\n        self.check_trained()\n        return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)\n\n\n    def _load_pretrained(self, mname, num_labels):\n        """"""\n        load pretrained model\n        """"""\n        if self.config is not None:\n            self.config.num_labels = num_labels\n            try:\n                model = self.model_type.from_pretrained(mname, config=self.config)\n            except:\n                try:\n                    model = self.model_type.from_pretrained(mname, config=self.config, from_pt=True)\n                except:\n                    raise ValueError(\'could not load pretrained model %s using both from_pt=False and from_pt=True\' % (mname))\n        else:\n            model = self.model_type.from_pretrained(mname, num_labels=num_labels)\n        return model\n\n\n\n    def get_classifier(self, fpath=None, multilabel=None, metrics=[\'accuracy\']):\n        """"""\n        creates a model for text classification\n        Args:\n          fpath(str): optional path to saved pretrained model. Typically left as None.\n          multilabel(bool): If None, multilabel status is discovered from data [recommended].\n                            If True, model will be forcibly configured for multilabel task.\n                            If False, model will be forcibly configured for non-multilabel task.\n                            It is recommended to leave this as None.\n          metrics(list): metrics to use\n        """"""\n        self.check_trained()\n        if not self.get_classes():\n            warnings.warn(\'no class labels were provided - treating as regression\')\n            return self.get_regression_model()\n\n        # process multilabel task\n        multilabel = self.multilabel if multilabel is None else multilabel\n        if multilabel is True and self.multilabel is False:\n            warnings.warn(\'The multilabel=True argument was supplied, but labels do not indicate \'+\\\n                          \'a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.\')\n        elif multilabel is False and self.multilabel is True:\n                warnings.warn(\'The multilabel=False argument was supplied, but labels inidcate that  \'+\\\n                              \'this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.\')\n\n        # setup model\n        num_labels = len(self.get_classes())\n        mname = fpath if fpath is not None else self.model_name\n        model = self._load_pretrained(mname, num_labels)\n        if multilabel:\n            loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)\n        else:\n            loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n        model.compile(loss=loss_fn,\n                      optimizer=U.DEFAULT_OPT,\n                      metrics=metrics)\n        return model\n\n\n    def get_regression_model(self, fpath=None, metrics=[\'mae\']):\n        """"""\n        creates a model for text regression\n        Args:\n          fpath(str): optional path to saved pretrained model. Typically left as None.\n          metrics(list): metrics to use\n        """"""\n        self.check_trained()\n        if self.get_classes():\n            warnings.warn(\'class labels were provided - treating as classification problem\')\n            return self.get_classifier()\n        num_labels = 1\n        mname = fpath if fpath is not None else self.model_name\n        model = self._load_pretrained(mname, num_labels)\n        loss_fn = \'mse\'\n        model.compile(loss=loss_fn,\n                      optimizer=U.DEFAULT_OPT,\n                      metrics=metrics)\n        return model\n\n\n    def get_model(self, fpath=None):\n        self.check_trained()\n        if not self.get_classes():\n            return self.get_regression_model(fpath=fpath)\n        else:\n            return self.get_classifier(fpath=fpath)\n\n\n\nclass DistilBertPreprocessor(TransformersPreprocessor):\n    """"""\n    text preprocessing for Hugging Face DistlBert model\n    """"""\n\n    def __init__(self, maxlen, max_features, class_names=[], classes=[], \n                lang=\'en\', ngram_range=1):\n        class_names = self.migrate_classes(class_names, classes)\n        name = DISTILBERT\n        if lang == \'en\':\n            model_name = \'distilbert-base-uncased\'\n        else:\n            model_name = \'distilbert-base-multilingual-cased\'\n\n        super().__init__(model_name,\n                         maxlen, max_features, class_names=class_names, \n                         lang=lang, ngram_range=ngram_range)\n\n\nclass Transformer(TransformersPreprocessor):\n    """"""\n    convenience class for text classification Hugging Face transformers \n    Usage:\n       t = Transformer(\'distilbert-base-uncased\', maxlen=128, classes=[\'neg\', \'pos\'], batch_size=16)\n       train_dataset = t.preprocess_train(train_texts, train_labels)\n       model = t.get_classifier()\n       model.fit(train_dataset)\n    """"""\n\n    def __init__(self, model_name, maxlen=128, class_names=[], classes=[],\n                 batch_size=None, use_with_learner=True):\n        """"""\n        Args:\n            model_name (str):  name of Hugging Face pretrained model\n            maxlen (int):  sequence length\n            class_names(list):  list of strings of class names (e.g., \'positive\', \'negative\').\n                                The index position of string is the class ID.\n                                Not required for:\n                                  - regression problems\n                                  - binary/multi classification problems where\n                                    labels in y_train/y_test are in string format.\n                                    In this case, classes will be populated automatically.\n                                    get_classes() can be called to view discovered class labels.\n                                The class_names argument replaces the old classes argument.\n            classes(list):  alias for class_names.  Included for backwards-compatiblity.\n\n            use_with_learner(bool):  If False, preprocess_train and preprocess_test\n                                     will return tf.Datasets for direct use with model.fit\n                                     in tf.Keras.\n                                     If True, preprocess_train and preprocess_test will\n                                     return a ktrain TransformerDataset object for use with\n                                     ktrain.get_learner.\n            batch_size (int): batch_size - only required if use_with_learner=False\n\n\n\n\n        """"""\n        multilabel = None # force discovery of multilabel task from data in preprocess_train->set_multilabel\n        class_names = self.migrate_classes(class_names, classes)\n        if not use_with_learner and batch_size is None:\n            raise ValueError(\'batch_size is required when use_with_learner=False\')\n        if multilabel and (class_names is None or not class_names):\n            raise ValueError(\'classes argument is required when multilabel=True\')\n        super().__init__(model_name,\n                         maxlen, max_features=10000, class_names=class_names, multilabel=multilabel)\n        self.batch_size = batch_size\n        self.use_with_learner = use_with_learner\n        self.lang = None\n\n\n    def preprocess_train(self, texts, y=None, mode=\'train\', verbose=1):\n        """"""\n        Preprocess training set for A Transformer model\n\n        Y values can be in one of the following forms:\n        1) integers representing the class (index into array returned by get_classes)\n           for binary and multiclass text classification.\n           If labels are integers, class_names argument to Transformer constructor is required.\n        2) strings representing the class (e.g., \'negative\', \'positive\').\n           If labels are strings, class_names argument to Transformer constructor is ignored,\n           as class labels will be extracted from y.\n        3) multi-hot-encoded vector for multilabel text classification problems\n           If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.\n        4) Numerical values for regression problems.\n           <class_names> argument to Transformer constructor should NOT be supplied\n\n        Args:\n            texts (list of strings): text of documents\n            y: labels\n            mode (str):  If \'train\' and prepare_for_learner=False,\n                         a tf.Dataset will be returned with repeat enabled\n                         for training with fit_generator\n            verbose(bool): verbosity\n        Returns:\n          TransformerDataset if self.use_with_learner = True else tf.Dataset\n        """"""\n        tseq = super().preprocess_train(texts, y=y, mode=mode, verbose=verbose)\n        if self.use_with_learner: return tseq\n        tseq.batch_size = self.batch_size\n        train = (mode == \'train\')\n        return tseq.to_tfdataset(train=train)\n\n\n    def preprocess_test(self, texts, y=None,  verbose=1):\n        """"""\n        Preprocess the validation or test set for a Transformer model\n        Y values can be in one of the following forms:\n        1) integers representing the class (index into array returned by get_classes)\n           for binary and multiclass text classification.\n           If labels are integers, class_names argument to Transformer constructor is required.\n        2) strings representing the class (e.g., \'negative\', \'positive\').\n           If labels are strings, class_names argument to Transformer constructor is ignored,\n           as class labels will be extracted from y.\n        3) multi-hot-encoded vector for multilabel text classification problems\n           If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.\n        4) Numerical values for regression problems.\n           <class_names> argument to Transformer constructor should NOT be supplied\n\n        Args:\n            texts (list of strings): text of documents\n            y: labels\n            verbose(bool): verbosity\n        Returns:\n            TransformerDataset if self.use_with_learner = True else tf.Dataset\n        """"""\n        self.check_trained()\n        return self.preprocess_train(texts, y=y, mode=\'test\', verbose=verbose)\n\n\nclass TransformerEmbedding():\n    def __init__(self, model_name, layers=U.DEFAULT_TRANSFORMER_LAYERS):\n        """"""\n        Args:\n            model_name (str):  name of Hugging Face pretrained model.\n                               Choose from here: https://huggingface.co/transformers/pretrained_models.html\n            layers(list): list of indexes indicating which hidden layers to use when\n                          constructing the embedding (e.g., last=[-1])\n                               \n        """"""\n        self.layers = layers\n        self.model_name = model_name\n        if model_name.startswith(\'xlm-roberta\'):\n            self.name = \'xlm_roberta\'\n        else:\n            self.name = model_name.split(\'-\')[0]\n\n        if self.name not in TRANSFORMER_MODELS:\n            self.config = AutoConfig.from_pretrained(model_name)\n            self.model_type = TFAutoModel\n            self.tokenizer_type = AutoTokenizer\n        else:\n            self.config = None # use default config\n            self.model_type = TRANSFORMER_MODELS[self.name][3]\n            self.tokenizer_type = TRANSFORMER_MODELS[self.name][2]\n        if ""bert-base-japanese"" in model_name:\n            self.tokenizer_type = transformers.BertJapaneseTokenizer\n\n        self.tokenizer = self.tokenizer_type.from_pretrained(model_name)\n        self.model = self._load_pretrained(model_name)\n        try:\n            self.embsize = self.embed(\'ktrain\', word_level=False).shape[1] # (batch_size, embsize)\n        except:\n            warnings.warn(\'could not determine Embedding size\')\n        if type(self.model).__name__ not in [\'TFBertModel\', \'TFDistilBertModel\', \'TFAlbertModel\']:\n            raise ValueError(\'TransformerEmbedding class currently only supports BERT-style models: \' +\\\n                             \'Bert, DistilBert, and Albert and variants like BioBERT and SciBERT\\n\\n\' +\\\n                             \'model received: %s (%s))\' % (type(self.model).__name__, model_name))\n\n\n    def _load_pretrained(self, model_name):\n        """"""\n        load pretrained model\n        """"""\n        if self.config is not None:\n            self.config.output_hidden_states = True\n            try:\n                model = self.model_type.from_pretrained(model_name, config=self.config)\n            except:\n                try:\n                    model = self.model_type.from_pretrained(model_name, config=self.config, from_pt=True)\n                except:\n                    raise ValueError(\'could not load pretrained model %s using both from_pt=False and from_pt=True\' % (model_name))\n        else:\n            model = self.model_type.from_pretrained(model_name, output_hidden_states=True)\n        return model\n\n\n\n    def embed(self, texts, word_level=True):\n        """"""\n        get embedding for word, phrase, or sentence\n        Args:\n          text(str|list): word, phrase, or sentence or list of them representing a batch\n          word_level(bool): If True, returns embedding for each token in supplied texts.\n                            If False, returns embedding for each text in texts\n        Returns:\n            np.ndarray : embeddings\n        """"""\n        if isinstance(texts, str): texts = [texts]\n        if not isinstance(texts[0], str): texts = ["" "".join(text) for text in texts]\n\n        sentences = []\n        for text in texts:\n            sentences.append(self.tokenizer.tokenize(text))\n        maxlen = len(max([tokens for tokens in sentences], key=len,)) + 2\n        all_input_ids = []\n        all_input_masks = []\n        for text in texts:\n            tokens = self.tokenizer.tokenize(text)\n            if len(tokens) > maxlen - 2:\n                tokens = tokens[0 : (maxlen - 2)]\n            tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]\n            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            input_mask = [1] * len(input_ids)\n            while len(input_ids) < maxlen:\n                input_ids.append(0)\n                input_mask.append(0)\n            all_input_ids.append(input_ids)\n            all_input_masks.append(input_mask)\n\n        all_input_ids = np.array(all_input_ids)\n        all_input_masks = np.array(all_input_masks)\n        outputs = self.model(all_input_ids, attention_mask=all_input_masks)\n        hidden_states = outputs[-1] # output_hidden_states=True\n\n        # compile raw embeddings\n        if len(self.layers) == 1:\n            #raw_embeddings = hidden_states[-1].numpy()\n            raw_embeddings = hidden_states[self.layers[0]].numpy()\n        else:\n            raw_embeddings = []\n            for batch_id in range(hidden_states[0].shape[0]):\n                token_embeddings = []\n                for token_id in range(hidden_states[0].shape[1]):\n                    all_layers = []\n                    for layer_id in self.layers:\n                        all_layers.append(hidden_states[layer_id][batch_id][token_id].numpy())\n                    token_embeddings.append(np.concatenate(all_layers) )  \n                raw_embeddings.append(token_embeddings)\n            raw_embeddings = np.array(raw_embeddings)\n\n        if not word_level: # sentence-level embedding\n            return np.mean(raw_embeddings, axis=1)\n            #return np.squeeze(raw_embeddings[:,0:1,:], axis=1)\n\n        # filter-out extra subword tokens and special tokens \n        # (using first subword of each token as embedding representations)\n        filtered_embeddings = []\n        for batch_idx, tokens in enumerate(sentences):\n            embedding = []\n            for token_idx, token in enumerate(tokens):\n                if token in [self.tokenizer.cls_token, self.tokenizer.sep_token] or token.startswith(\'##\'): continue\n                embedding.append(raw_embeddings[batch_idx][token_idx])\n            filtered_embeddings.append(embedding)\n\n        # pad embeddings with zeros\n        max_length = max([len(e) for e in filtered_embeddings])\n        embeddings = []\n        for e in filtered_embeddings:\n            for i in range(max_length-len(e)):\n                e.append(np.zeros((self.embsize,)))\n            embeddings.append(np.array(e))\n        return np.array(embeddings)\n\n\nclass TransformerDataset(SequenceDataset):\n    """"""\n    Wrapper for Transformer datasets.\n    """"""\n\n    def __init__(self, x, y, batch_size=1):\n        if type(x) not in [list, np.ndarray]: raise ValueError(\'x must be list or np.ndarray\')\n        if type(y) not in [list, np.ndarray]: raise ValueError(\'y must be list or np.ndarray\')\n        if type(x) == list: x = np.array(x)\n        if type(y) == list: y = np.array(y)\n        self.x = x\n        self.y = y\n        self.batch_size = batch_size\n\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n        return (batch_x, batch_y)\n\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n\n    def to_tfdataset(self, train=True):\n        """"""\n        convert transformer features to tf.Dataset\n        """"""\n        if train:\n            shuffle=True\n            repeat = True\n        else:\n            shuffle=False\n            repeat=False\n\n        if len(self.y.shape) == 1:\n            yshape = []\n            ytype = tf.float32\n        else:\n            yshape = [None]\n            ytype = tf.int64\n\n        def gen():\n            for idx, data in enumerate(self.x):\n                yield ({\'input_ids\': data[0],\n                         \'attention_mask\': data[1],\n                         \'token_type_ids\': data[2]},\n                        self.y[idx])\n\n        tfdataset= tf.data.Dataset.from_generator(gen,\n            ({\'input_ids\': tf.int32,\n              \'attention_mask\': tf.int32,\n              \'token_type_ids\': tf.int32},\n             ytype),\n            ({\'input_ids\': tf.TensorShape([None]),\n              \'attention_mask\': tf.TensorShape([None]),\n              \'token_type_ids\': tf.TensorShape([None])},\n             tf.TensorShape(yshape)))\n\n        if shuffle:\n            tfdataset = tfdataset.shuffle(self.x.shape[0])\n        tfdataset = tfdataset.batch(self.batch_size)\n        if repeat:\n            tfdataset = tfdataset.repeat(-1)\n        return tfdataset\n\n\n    def get_y(self):\n        return self.y\n\n    def nsamples(self):\n        return len(self.x)\n\n    def nclasses(self):\n        return self.y.shape[1]\n\n    def xshape(self):\n        return (len(self.x), self.x[0].shape[1])\n\n\n# preprocessors\nTEXT_PREPROCESSORS = {\'standard\': StandardTextPreprocessor,\n                      \'bert\': BERTPreprocessor,\n                      \'distilbert\': DistilBertPreprocessor}\n'"
ktrain/text/textutils.py,0,"b'from ..imports import *\nfrom subprocess import Popen, PIPE, DEVNULL\n\n\nDEFAULT_TOKEN_PATTERN = (r""\\b[a-zA-Z][a-zA-Z0-9]*(?:[_/&-][a-zA-Z0-9]+)+\\b|""\n                         r""\\b\\d*[a-zA-Z][a-zA-Z0-9][a-zA-Z0-9]+\\b"")\n\n\n\ndef extract_copy(corpus_path, output_path):\n    """"""\n    Crawl <corpus_path>, extract or read plain text from application/pdf\n    and text/plain files and then copy them to output_path.\n    Args:\n        corpus_path(str):  root folder containing documents\n        output_path(str):  root folder of output directory\n    Returns:\n        list: list of skipped filenames\n    """"""\n    skipped = set()\n    num_skipped = 0\n    corpus_path = os.path.normpath(corpus_path)\n    output_path = os.path.normpath(output_path)\n    for idx, filename in enumerate(extract_filenames(corpus_path)):\n        if idx %1000 == 0: print(\'processed %s doc(s)\' % (idx+1))\n        mtype = get_mimetype(filename)\n        if mtype == \'application/pdf\':\n            text = pdftotext(filename)\n            text = text.strip()\n        elif mtype and mtype.split(\'/\')[0] == \'text\':\n            with open(filename, \'r\') as f:\n                text = f.read()\n                text = str.encode(text)\n        else:\n            num_skipped += 1\n            if not mtype:\n                mtype =  os.path.splitext(filename)[1]\n                if not mtype: mtype == \'unknown\'\n            skipped.add(mtype)\n            continue\n        if not text: \n            num_skipped += 1\n            continue\n        fpath, fname = os.path.split(filename)\n        if mtype == \'application/pdf\': fname = fname+\'.txt\'\n        relfpath = fpath.replace(corpus_path, \'\')\n        relfpath = relfpath[1:] if relfpath and relfpath[0] == os.sep else relfpath\n        opath = os.path.join(output_path, relfpath)\n        if not os.path.exists(opath):\n            os.makedirs(opath)\n        ofilename = os.path.join(opath, fname)\n        with open(ofilename, \'wb\') as f:\n            f.write(text)\n    print(\'processed %s docs\' % (idx+1))\n    print(\'done.\')\n    print(\'skipped %s docs\' % (num_skipped))\n    if skipped: print(\'%s\' %(skipped))\n\n\ndef get_mimetype(filepath):\n    return mimetypes.guess_type(filepath)[0]\n\ndef is_txt(filepath):\n    return mimetypes.guess_type(filepath)[0] == \'text/plain\'\n\ndef is_pdf(filepath):\n    return mimetypes.guess_type(filepath)[0] == \'application/pdf\'\n\n\n\ndef pdftotext(filename):\n    """"""\n    Use pdftotext program to convert PDF to text string.\n    :param filename: of PDF file\n    :return: text from file, or empty string if failure\n    """"""\n    output = Popen([\'pdftotext\', \'-q\', filename, \'-\'],\n                   stdout=PIPE).communicate()[0]\n    # None may indicate damage, but convert for consistency\n    return \'\' if output is None else output\n\n\n\ndef requires_ocr(filename):\n    """"""\n    Uses pdffonts program to determine if the PDF requires OCR, i.e., it\n    doesn\'t contain any fonts.\n    :param filename: of PDF file\n    :return: True if requires OCR, False if not\n    """"""\n    output = Popen([\'pdffonts\', filename], stdout=PIPE,\n                   stderr=DEVNULL).communicate()[0]\n    return len(output.split(\'\\n\')) < 4\n\n\ndef extract_filenames(corpus_path, follow_links=False):\n    if os.listdir(corpus_path) == []:\n        raise ValueError(""%s: path is empty"" % corpus_path)\n    walk = os.walk\n    for root, dirs, filenames in walk(corpus_path, followlinks=follow_links):\n        for filename in filenames:\n            try:\n                yield os.path.join(root, filename)\n            except:\n                continue\n\n\ndef strip_control_characters(data):\n    if data:\n        # unicode invalid characters\n        re_xml_illegal = (\n            \'([\\u0000-\\u0008\\u000b-\\u000c\\u000e-\\u001f\\ufffe-\\uffff])|\'\n            \'([%s-%s][^%s-%s])|([^%s-%s][%s-%s])|([%s-%s]$)|(^[%s-%s])\'\n            % (chr(0xd800), chr(0xdbff), chr(0xdc00), chr(0xdfff), chr(0xd800),\n               chr(0xdbff), chr(0xdc00), chr(0xdfff), chr(0xd800), chr(0xdbff),\n               chr(0xdc00), chr(0xdfff))\n        )\n        data = re.sub(re_xml_illegal, """", data)\n        # ascii control characters\n        #data = re.sub(r""[\\x01-\\x1F\\x7F]"", """", data)\n        # See:  http://w3.org/International/questions/qa-forms-utf-8.html\n        # Printable utf-8 does not include any of these chars below x7F\n        data = re.sub(r""[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]"", """", data)\n    return data\n\n\n\ndef to_ascii(data):\n    """"""Transform accentuated unicode symbols into ascii or nothing\n\n    Warning: this solution is only suited for languages that have a direct\n    transliteration to ASCII symbols.\n\n    A better solution would be to use transliteration based on a precomputed\n    unidecode map to be used by translate as explained here:\n\n        http://stackoverflow.com/questions/2854230/\n\n    """"""\n    import unicodedata\n    if isinstance(data, bytes):\n        data = data.decode()\n    nkfd_form = unicodedata.normalize(\'NFKD\', data)\n    only_ascii = nkfd_form.encode(\'ASCII\', \'ignore\')\n\n    # Return a string\n    return only_ascii.decode(\'ascii\')\n\n\n\ndef load_text_files(corpus_path, truncate_len=None, \n                    clean=True, return_fnames=False):\n    """"""\n    load text files\n    """"""\n    \n    texts = []\n    filenames = []\n    mb = master_bar(range(1))\n    for i in mb:\n        for filename in progress_bar(list(extract_filenames(corpus_path)), parent=mb):\n            with open(filename, \'r\') as f:\n                text = f.read()\n            if clean:\n                text = strip_control_characters(text)\n                text = to_ascii(text)\n            if truncate_len is not None:\n                text = "" "".join(text.split()[:truncate_len])\n            texts.append(text)\n            filenames.append(filename)\n        mb.write(\'done.\')\n    if return_fnames:\n        return (texts, filenames)\n    else:\n        return texts\n\n\ndef filter_by_id(lst, ids=[]):\n    """"""\n    filter list by supplied IDs\n    """"""\n    return [x for i,x in enumerate(lst) if i in ids]\n\n\n#------------------------------------------------------------------------------\n# Language-Handling\n#------------------------------------------------------------------------------\n\n\ndef detect_lang(texts, sample_size=32):\n    """"""\n    detect language\n    """"""\n\n    # convert sentence pairs\n    if isinstance(texts, (tuple, list, np.ndarray)) and len(texts) == 2:\n        texts = [texts[0], texts[1]]\n    elif isinstance(texts, (tuple, list, np.ndarray)) and isinstance(texts[0], (tuple, list, np.ndarray)) and len(texts[0]) == 2:\n        texts = [t[0] for t in texts]\n\n    if isinstance(texts, (pd.Series, pd.DataFrame)):\n        texts = texts.values\n    if isinstance(texts, str): texts = [texts]\n    if not isinstance(texts, (list, np.ndarray)):\n        raise ValueError(\'texts must be a list or NumPy array of strings\')\n    lst = []\n    for doc in texts[:sample_size]:\n        try:\n            lst.append(langdetect.detect(doc))\n        except:\n            continue\n    if len(lst) == 0: \n        warnings.warn(\'Defaulting to English: could not detect language in random sample of %s docs. Are you sure you provided a list of strings?\'  % (sample_size))\n        lang = \'en\'\n    else:\n        lang = max(set(lst), key=lst.count)\n    #return max(set(lst), key=lst.count)\n    return lang\n\n\n\ndef is_chinese(lang, strict=True):\n    """"""\n    Args:\n      lang(str): language code (e.g., en)\n      strict(bool):  If False, include additional languages due to mistakes on short texts by langdetect\n    """"""\n    if strict:\n        extra_clause = False\n    else:\n        extra_clause = lang in [\'ja\', \'ko\']\n    return lang is not None and lang.startswith(\'zh-\') or extra_clause\n\n\ndef split_chinese(texts):\n    if isinstance(texts, str): texts=[texts]\n\n    split_texts = []\n    for doc in texts:\n        seg_list = jieba.cut(doc, cut_all=False)\n        seg_list = list(seg_list)\n        split_texts.append(seg_list)\n    return ["" "".join(tokens) for tokens in split_texts]\n\n\nNOSPACE_LANGS = [\'zh-cn\', \'zh-tw\', \'ja\']\n\n\ndef is_nospace_lang(lang):\n    return lang in NOSPACE_LANGS\n\n\ndef decode_by_line(texts, encoding=\'utf-8\', verbose=1):\n    """"""\n    Decode text line by line and skip over errors.\n    """"""\n\n    if isinstance(texts, str): texts = [texts]\n    new_texts = []\n    skips=0\n    num_lines = 0\n    for doc in texts:\n        text = """"\n        for line in doc.splitlines():\n            num_lines +=1\n            try:\n                line = line.decode(encoding)\n            except:\n                skips +=1\n                continue\n            text += line\n        new_texts.append(text)\n    pct = round((skips*1./num_lines) * 100, 1)\n    if verbose:\n        print(\'skipped %s lines (%s%%) due to character decoding errors\' % (skips, pct))\n        if pct > 10:\n            print(\'If this is too many, try a different encoding\')\n    return new_texts\n\n\ndef detect_encoding(texts, sample_size=32):\n    if not isinstance(texts, list): texts = [texts] # check for instance of list as bytes are supplied as input\n    lst = [chardet.detect(doc)[\'encoding\'] for doc in texts[:sample_size]]\n    encoding = max(set(lst), key=lst.count)\n    # standardize to utf-8 to prevent BERT problems\n    encoding = \'utf-8\' if encoding.lower() in [\'ascii\', \'utf8\', \'utf-8\'] else encoding\n    return encoding\n\n\ndef read_text(filename):\n    with open(filename, \'rb\') as f:\n        text = f.read()\n    encoding = detect_encoding([text])\n    try:\n        decoded_text = text.decode(encoding)\n    except:\n        U.vprint(\'Decoding with %s failed 1st attempt - using %s with skips\' % (encoding,\n                                                                                encoding),\n                                                                                verbose=verbose)\n        decoded_text = decode_by_line(text, encoding=encoding)\n    return decoded_text.strip()\n\n\n\ndef sent_tokenize(text):\n    """"""\n    segment text into sentences\n    """"""\n    lang = detect_lang(text)\n    sents = []\n    if is_chinese(lang):\n        for sent in re.findall(u\'[^!?\xe3\x80\x82\\.\\!\\?]+[!?\xe3\x80\x82\\.\\!\\?]?\', text, flags=re.U):\n            sents.append(sent)\n    else:\n        for paragraph in segmenter.process(text):\n            for sentence in paragraph:\n                sents.append("" "".join([t.value for t in sentence]))\n    return sents\n\n\n\ndef paragraph_tokenize(text, join_sentences=False):\n    """"""\n    segment text into sentences\n    """"""\n    lang = detect_lang(text)\n    if is_chinese(lang):\n        raise ValueError(\'paragraph_tokenize does not currently support Chinese.\')\n    paragraphs = []\n    sents = []\n    for paragraph in segmenter.process(text):\n        sents = []\n        for sentence in paragraph:\n            sents.append("" "".join([t.value for t in sentence]))\n        if join_sentences: sents = \' \'.join(sents)\n        paragraphs.append(sents)\n    return paragraphs\n'"
ktrain/vision/__init__.py,0,"b""from .models import print_image_classifiers, image_classifier\nfrom .models import print_image_regression_models, image_regression_model\nfrom .data import show_image, show_random_images, preview_data_aug, get_data_aug\nfrom .data import images_from_folder, images_from_csv, images_from_array, images_from_fname, preprocess_csv\nfrom .predictor import ImagePredictor\n__all__ = [\n           'image_classifier', 'image_regression_model',\n           'print_image_classifiers', 'print_image_regression_models',\n           'images_from_folder', 'images_from_csv', 'images_from_array', 'images_from_fname',\n           'get_data_aug',\n           'preprocess_csv',\n           'ImagePredictor',\n           'show_image',\n           'show_random_images',\n           'preview_data_aug'\n           ]\n\n"""
ktrain/vision/data.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom .preprocessor import ImagePreprocessor\n\n\ndef show_image(img_path):\n    """"""\n    Given file path to image, show it in Jupyter notebook\n    """"""\n    if not os.path.isfile(img_path):\n        raise ValueError(\'%s is not valid file\' % (img_path))\n    img = plt.imread(img_path)\n    out = plt.imshow(img)\n    return out\n\n\ndef show_random_images(img_folder, n=4, rows=1):\n    """"""\n    display random images from a img_folder\n    """"""\n    fnames = []\n    for ext in (\'*.gif\', \'*.png\', \'*.jpg\'):\n        fnames.extend(glob.glob(os.path.join(img_folder, ext)))\n    ims = []\n    for i in range(n):\n        img_path = random.choice(fnames)\n        img = image.load_img(img_path)\n        x = image.img_to_array(img)\n        x = x/255.\n        ims.append(x)\n    U.plots(ims, rows=rows)\n    return\n\n\ndef preview_data_aug(img_path, data_aug, rows=1, n=4):\n    """"""\n    Preview data augmentation (ImageDatagenerator)\n    on a supplied image.\n    """"""\n    if type(img_path) != type(\'\') or not os.path.isfile(img_path):\n        raise ValueError(\'img_path must be valid file path to image\')\n    idg = copy.copy(data_aug)\n    idg.featurewise_center = False\n    idg.featurewise_std_normalization = False\n    idg.samplewise_center = False\n    idg.samplewise_std_normalization = False\n    idg.rescale = None\n    idg.zca_whitening = False\n    idg.preprocessing_function = None\n\n    img = image.load_img(img_path)\n    x = image.img_to_array(img)\n    x = x/255.\n    x = x.reshape((1,) + x.shape)\n    i = 0\n    ims = []\n    for batch in idg.flow(x, batch_size=1):\n        ims.append(np.squeeze(batch))\n        i += 1\n        if i >= n: break\n    U.plots(ims, rows=rows)\n    return\n\n\ndef preview_data_aug_OLD(img_path, data_aug, n=4):\n    """"""\n    Preview data augmentation (ImageDatagenerator)\n    on a supplied image.\n    """"""\n    if type(img_path) != type(\'\') or not os.path.isfile(img_path):\n        raise ValueError(\'img_path must be valid file path to image\')\n    idg = copy.copy(data_aug)\n    idg.featurewise_center = False\n    idg.featurewise_std_normalization = False\n    idg.samplewise_center = False\n    idg.samplewise_std_normalization = False\n    idg.rescale = None\n    idg.zca_whitening = False\n    idg.preprocessing_function = None\n\n    img = image.load_img(img_path)\n    x = image.img_to_array(img)\n    x = x/255.\n    x = x.reshape((1,) + x.shape)\n    i = 0\n    for batch in idg.flow(x, batch_size=1):\n        plt.figure()\n        plt.imshow(np.squeeze(batch))\n        i += 1\n        if i >= n: break\n    return\n\n\n\ndef get_data_aug(\n                 rotation_range=40,\n                 zoom_range=0.2,\n                 width_shift_range=0.2,\n                 height_shift_range=0.2,\n                 horizontal_flip=False,\n                 vertical_flip=False,\n                 featurewise_center=True,\n                 featurewise_std_normalization=True,\n                 samplewise_center=False,\n                 samplewise_std_normalization=False,\n                 rescale=None,\n                 **kwargs):\n    """"""\n    This function is simply a wrapper around ImageDataGenerator\n    with some reasonable defaults for data augmentation.\n    Returns the default image_data_generator to support\n    data augmentation and data normalization.\n    Parameters can be adjusted by caller.\n    Note that the ktrain.vision.model.image_classifier\n    function may adjust these as needed.\n    """"""\n\n    data_aug = image.ImageDataGenerator(\n                                rotation_range=rotation_range,\n                                zoom_range=zoom_range,\n                                width_shift_range=width_shift_range,\n                                height_shift_range=height_shift_range,\n                                horizontal_flip=horizontal_flip,\n                                vertical_flip=vertical_flip,\n                                featurewise_center=featurewise_center,\n                                featurewise_std_normalization=featurewise_std_normalization,\n                                samplewise_center=samplewise_center,\n                                samplewise_std_normalization=samplewise_std_normalization,\n                                rescale=rescale,\n                                **kwargs)\n    return data_aug\n\n\ndef get_test_datagen(data_aug=None):\n    if data_aug:\n        featurewise_center = data_aug.featurewise_center\n        featurewise_std_normalization = data_aug.featurewise_std_normalization\n        samplewise_center = data_aug.samplewise_center\n        samplewise_std_normalization = data_aug.samplewise_std_normalization\n        rescale = data_aug.rescale\n        zca_whitening = data_aug.zca_whitening\n        test_datagen = image.ImageDataGenerator(\n                                rescale=rescale,\n                                featurewise_center=featurewise_center,\n                                samplewise_center=samplewise_center,\n                                featurewise_std_normalization=featurewise_std_normalization,\n                                samplewise_std_normalization=samplewise_std_normalization,\n                                zca_whitening=zca_whitening)\n    else:\n        test_datagen = image.ImageDataGenerator()\n    return test_datagen\n\n\n\ndef process_datagen(data_aug, train_array=None, train_directory=None,\n                    target_size=None,\n                    color_mode=\'rgb\',\n                    flat_dir=False):\n    # set generators for train and test\n    if data_aug is not None:\n        train_datagen = data_aug\n        test_datagen = get_test_datagen(data_aug=data_aug)\n    else:\n        train_datagen = get_test_datagen()\n        test_datagen = get_test_datagen()\n\n    # compute statistics for normalization\n    fit_datagens(train_datagen, test_datagen,\n                 train_array=train_array,\n                 train_directory=train_directory,\n                 target_size=target_size,\n                 color_mode=color_mode, flat_dir=flat_dir)\n\n    return (train_datagen, test_datagen)\n\n\n\ndef fit_datagens(train_datagen, test_datagen,\n                 train_array=None, train_directory=None,\n                 target_size=None,\n                 color_mode=\'rgb\', flat_dir=False):\n    """"""\n    computes stats of images for normalization\n    """"""\n    if not datagen_needs_fit(train_datagen): return\n    if bool(train_array is not None) == bool(train_directory):\n        raise ValueError(\'only one of train_array or train_directory is required.\')\n    if train_array is not None:\n        train_datagen.fit(train_array)\n        test_datagen.fit(train_array)\n    else:\n        if target_size is None:\n            raise ValueError(\'target_size is required when train_directory is supplied\')\n        fit_samples = sample_image_folder(train_directory, target_size,\n                                          color_mode=color_mode, flat_dir=flat_dir)\n        train_datagen.fit(fit_samples)\n        test_datagen.fit(fit_samples)\n    return\n\n\ndef datagen_needs_fit(datagen):\n    if datagen.featurewise_center or datagen.featurewise_std_normalization or \\\n       datagen.zca_whitening:\n           return True\n    else:\n        return False\n\ndef sample_image_folder(train_directory,\n                         target_size,\n                         color_mode=\'rgb\', flat_dir=False):\n\n    # adjust train_directory\n    classes = None\n    if flat_dir and train_directory is not None:\n        folder = train_directory\n        if folder[-1] != os.sep: folder += os.sep\n        parent = os.path.dirname(os.path.dirname(folder))\n        folder_name = os.path.basename(os.path.dirname(folder))\n        train_directory = parent\n        classes = [folder_name]\n\n    # sample images\n    batch_size = 100\n    img_gen = image.ImageDataGenerator()\n    batches = img_gen.flow_from_directory(\n                directory=train_directory,\n                classes=classes,\n                target_size=target_size,\n                batch_size=batch_size,\n                color_mode=color_mode,\n                shuffle=True)\n    the_shape = batches[0][0].shape\n    sample_size = the_shape[0]\n    if K.image_data_format() == \'channels_first\':\n        num_channels = the_shape[1]\n    else:\n        num_channels = the_shape[-1]\n    imgs, labels = next(batches)\n    return imgs\n\n\ndef detect_color_mode(train_directory,\n                     target_size=(32,32)):\n    try:\n        fname = glob.glob(os.path.join(train_directory, \'**/*\'))[0]\n        img = Image.open(fname).resize(target_size)\n        num_channels = len(img.getbands())\n        if num_channels == 3: return \'rgb\'\n        elif num_channels == 1: return \'grayscale\'\n        else: return \'rgby\'\n    except:\n        warnings.warn(\'could not detect color_mode from %s\' % (train_directory))\n        return\n\n\n\ndef preprocess_csv(csv_in, csv_out, x_col=\'filename\', y_col=None,\n                   sep=\',\', label_sep=\' \', suffix=\'\', split_by=None):\n    """"""\n    Takes a CSV where the one column contains a file name and a column\n    containing a string representations of the class(es) like here:\n    image_name,tags\n    01, sunny|hot\n    02, cloudy|cold\n    03, cloudy|hot\n\n    .... and one-hot encodes the classes to produce a CSV as follows:\n    image_name, cloudy, cold, hot, sunny\n    01.jpg,0,0,1,1\n    02.jpg,1,1,0,0\n    03.jpg,1,0,1,0\n    Args:\n        csv_in (str):  filepath to input CSV file\n        csv_out (str): filepath to output CSV file\n        x_col (str):  name of column containing file names\n        y_col (str): name of column containing the classes\n        sep (str): field delimiter of entire file (e.g., comma fore CSV)\n        label_sep (str): delimiter for column containing classes\n        suffix (str): adds suffix to x_col values\n        split_by(str): name of column. A separate CSV will be\n                       created for each value in column. Useful\n                       for splitting a CSV based on whether a column\n                       contains \'train\' or \'valid\'.\n    Return:\n        list :  the list of clases (and csv_out will be new CSV file)\n    """"""\n    if not y_col and not suffix:\n        raise ValueError(\'one or both of y_col and suffix should be supplied\')\n    df = pd.read_csv(csv_in, sep=sep)\n    f_csv_out = open(csv_out, \'w\')\n    writer = csv.writer(f_csv_out, delimiter=sep)\n    if y_col: df[y_col] = df[y_col].apply(str)\n\n    # write header\n    if y_col:\n        classes = set()\n        for row in df.iterrows():\n            data = row[1]\n            tags = data[y_col].split(label_sep)\n            classes.update(tags)\n        classes = list(classes)\n        classes.sort()\n        writer.writerow([x_col] + classes)\n    else:\n        classes = df.columns[:-1]\n        write.writerow(df.columns)\n\n    # write rows\n    for row in df.iterrows():\n        data = row[1]\n        data[x_col] = data[x_col] + suffix\n        if y_col:\n            out = list(data[[x_col]].values)\n            tags = set(data[y_col].strip().split(label_sep))\n            for c in classes:\n                if c in tags: out.append(1)\n                else: out.append(0)\n        else:\n            out = data\n        writer.writerow(out)\n    f_csv_out.close()\n    return classes\n\n\ndef images_from_folder(datadir, target_size=(224,224),\n                       classes=None,\n                       color_mode=\'rgb\',\n                       train_test_names=[\'train\', \'test\'],\n                       data_aug=None, verbose=1):\n\n    """"""\n    Returns image generator (Iterator instance).\n    Assumes output will be 2D one-hot-encoded labels for categorization.\n    Note: This function preprocesses the input in preparation\n          for a ResNet50 model.\n\n    Args:\n    datadir (string): path to training (or validation/test) dataset\n        Assumes folder follows this structure:\n        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 datadir\n        \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n        \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class0       # folder containing documents of class 0\n        \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class1       # folder containing documents of class 1\n        \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class2       # folder containing documents of class 2\n        \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 classN       # folder containing documents of class N\n        \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 test\n        \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class0       # folder containing documents of class 0\n        \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class1       # folder containing documents of class 1\n        \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 class2       # folder containing documents of class 2\n        \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 classN       # folder containing documents of class N\n\n    target_size (tuple):  image dimensions\n    classes (list):  optional list of class subdirectories (e.g., [\'cats\',\'dogs\'])\n    color_mode (string):  color mode\n    train_test_names(list): names for train and test subfolders\n    data_aug(ImageDataGenerator):  a keras.preprocessing.image.ImageDataGenerator\n                                  for data augmentation\n    verbose (bool):               verbosity\n\n    Returns:\n    batches: a tuple of two Iterators - one for train and one for test\n\n    """"""\n\n    # train/test names\n    train_str = train_test_names[0]\n    test_str = train_test_names[1]\n    train_dir = os.path.join(datadir, train_str)\n    test_dir = os.path.join(datadir, test_str)\n\n    # color mode warning\n    if PIL_INSTALLED:\n        inferred_color_mode = detect_color_mode(train_dir)\n        if inferred_color_mode is not None and (inferred_color_mode != color_mode):\n            U.vprint(\'color_mode detected (%s) different than color_mode selected (%s)\' % (inferred_color_mode, color_mode),\n                     verbose=verbose)\n\n    # get train and test data generators\n    (train_datagen, test_datagen) = process_datagen(data_aug,\n                                        train_directory=train_dir,\n                                        target_size=target_size,\n                                        color_mode=color_mode)\n    batches_tr = train_datagen.flow_from_directory(train_dir,\n                                         target_size=target_size,\n                                         classes=classes,\n                                         class_mode=\'categorical\',\n                                         shuffle=True,\n                                         interpolation=\'bicubic\',\n                                         color_mode = color_mode)\n\n    batches_te = test_datagen.flow_from_directory(test_dir,\n                                              target_size=target_size,\n                                              classes=classes,\n                                              class_mode=\'categorical\',\n                                              shuffle=False,\n                                              interpolation=\'bicubic\',\n                                              color_mode = color_mode)\n\n    # setup preprocessor\n    class_tup = sorted(batches_tr.class_indices.items(), key=operator.itemgetter(1))\n    preproc = ImagePreprocessor(test_datagen,\n                                [x[0] for x in class_tup],\n                                target_size=target_size,\n                                color_mode=color_mode)\n    return (batches_tr, batches_te, preproc)\n\n\ndef images_from_df(train_df,\n                   image_column,\n                   label_columns=[],\n                   directory=None,\n                   val_directory=None,\n                   suffix=\'\',\n                   val_df=None,\n                   is_regression=False,\n                   target_size=(224,224),\n                    color_mode=\'rgb\',\n                    data_aug=None,\n                    val_pct=0.1, random_state=None):\n\n    """"""\n    Returns image generator (Iterator instance).\n    Assumes output will be 2D one-hot-encoded labels for categorization.\n    Note: This function preprocesses the input in preparation\n          for a ResNet50 model.\n\n    Args:\n    train_df (DataFrame):  pandas dataframe for training dataset \n    image_column (string): name of column containing the filenames of images\n                           If values in image_column do not have a file extension,\n                           the extension should be supplied with suffix argument.\n                           If values in image_column are not full file paths,\n                           then the path to directory containing images should be supplied\n                           as directory argument.\n\n    label_columns(list or str): list or str representing the columns that store labels\n                                Labels can be in any one of the following formats:\n                                1. a single column string string (or integer) labels\n\n                                   image_fname,label\n                                   -----------------\n                                   image01,cat\n                                   image02,dog\n\n                                2. multiple columns for one-hot-encoded labels\n                                   image_fname,cat,dog\n                                   image01,1,0\n                                   image02,0,1\n\n                                3. a single column of numeric values for image regression\n                                   image_fname,age\n                                   -----------------\n                                   image01,68\n                                   image02,18\n\n    directory (string): path to directory containing images\n                        not required if image_column contains full filepaths\n    val_directory(strin): path to directory containing validation images.\n                          only required if validation images are in different folder than train images\n    suffix(str): will be appended to each entry in image_column\n                 Used when the filenames in image_column do not contain file extensions.\n                 The extension in suffx should include ""."".\n    val_df (DataFrame): pandas dataframe for validation set\n\n    is_regression(bool): If True, task is treated as regression. \n                         Used when there is single column of numeric values and\n                         numeric values should be treated as numeric targets as opposed to class labels\n    target_size (tuple):  image dimensions\n    color_mode (string):  color mode\n    data_aug(ImageDataGenerator):  a keras.preprocessing.image.ImageDataGenerator\n                                  for data augmentation\n    val_pct(float):  proportion of training data to be used for validation\n                     only used if val_filepath is None\n    random_state(int): random seed for train/test split\n\n    Returns:\n    batches: a tuple of two Iterators - one for train and one for test\n\n    """"""\n\n    if isinstance(label_columns, (list, np.ndarray)) and len(label_columns) == 1:\n        label_columns = label_columns[0]\n\n    peek = train_df[label_columns].iloc[0]\n    if isinstance(label_columns, str) and peek.isdigit() and not is_regression:\n        warnings.warn(\'Targets are integers, but is_regression=False. Task treated as classification instead of regression.\')\n    if isinstance(peek, str) and is_regression:\n        train_df[label_columns] = train_df[label_columns].astype(\'float32\')\n        if val_df is not None:\n            val_df[label_columns] = val_df[label_columns].astype(\'float32\')\n    peek = train_df[label_columns].iloc[0]\n\n\n    # get train and test data generators\n    if directory:\n        img_folder = directory\n    else:\n        img_folder =  os.path.dirname(train_df[image_column].iloc[0])\n    (train_datagen, test_datagen) = process_datagen(data_aug,\n                                                    train_directory=img_folder,\n                                                    target_size=target_size,\n                                                    color_mode=color_mode,\n                                                    flat_dir=True)\n\n    # convert to dataframes\n    if val_df is None:\n        if val_pct:\n            df = train_df.copy()\n            prop = 1-val_pct\n            if random_state is not None: np.random.seed(42)\n            msk = np.random.rand(len(df)) < prop\n            train_df = df[msk]\n            val_df = df[~msk]\n\n    # class names\n    if isinstance(label_columns, (list, np.ndarray)): label_columns.sort()\n\n    # fix file extensions, if necessary\n    if suffix:\n        train_df = train_df.copy()\n        val_df = val_df.copy()\n        train_df[image_column] = train_df.copy()[image_column].apply(lambda x : x+suffix)\n        val_df[image_column] = val_df.copy()[image_column].apply(lambda x : x+suffix)\n\n    # 1-hot-encode string or integer labels\n    if isinstance(label_columns, str) or \\\n       (isinstance(label_columns, (list, np.ndarray)) and len(label_columns) == 1):\n        label_col_name = label_columns if isinstance(label_columns, str) else label_columns[0]\n        #if not isinstance(df[label_col_name].values[0], str):\n            #raise ValueError(\'If a single label column is provided, labels must be in the form of a string.\')\n        if not is_regression:\n            le = LabelEncoder()\n            train_labels = train_df[label_col_name].values\n            le.fit(train_labels)\n            y_train = to_categorical(le.transform(train_labels))\n            y_val = to_categorical(le.transform(val_df[label_col_name].values))\n            y_train_pd = [y_train[:,i] for i in range(y_train.shape[1])]\n            y_val_pd = [y_val[:,i] for i in range(y_val.shape[1])]\n            label_columns = list(le.classes_)\n            train_df = pd.DataFrame(zip(train_df[image_column].values, *y_train_pd), columns=[image_column]+label_columns)\n            val_df = pd.DataFrame(zip(val_df[image_column].values, *y_val_pd), columns=[image_column]+label_columns)\n\n\n    batches_tr = train_datagen.flow_from_dataframe(\n                                      train_df,\n                                      directory=directory,\n                                      x_col = image_column,\n                                      y_col=label_columns,\n                                      target_size=target_size,\n                                      class_mode=\'other\',\n                                      shuffle=True,\n                                      interpolation=\'bicubic\',\n                                      color_mode = color_mode)\n    batches_te = None\n    if val_df is not None:\n        d =  val_directory if val_directory is not None else directory\n        batches_te = test_datagen.flow_from_dataframe(\n                                          val_df,\n                                          directory=d,\n                                          x_col = image_column,\n                                          y_col=label_columns,\n                                          target_size=target_size,\n                                          class_mode=\'other\',\n                                          shuffle=False,\n                                          interpolation=\'bicubic\',\n                                          color_mode = color_mode)\n\n    # setup preprocessor\n    preproc = ImagePreprocessor(test_datagen,\n                                label_columns,\n                                target_size=target_size,\n                                color_mode=color_mode)\n    return (batches_tr, batches_te, preproc)\n\n\n\ndef images_from_csv(train_filepath,\n                   image_column,\n                   label_columns=[],\n                   directory=None,\n                   suffix=\'\',\n                   val_filepath=None,\n                   is_regression=False,\n                   target_size=(224,224),\n                    color_mode=\'rgb\',\n                    data_aug=None,\n                    val_pct=0.1, random_state=None):\n\n    """"""\n    Returns image generator (Iterator instance).\n    Assumes output will be 2D one-hot-encoded labels for categorization.\n    Note: This function preprocesses the input in preparation\n          for a ResNet50 model.\n\n    Args:\n    train_filepath (string): path to training dataset in CSV format with header row\n    image_column (string): name of column containing the filenames of images\n                           If values in image_column do not have a file extension,\n                           the extension should be supplied with suffix argument.\n                           If values in image_column are not full file paths,\n                           then the path to directory containing images should be supplied\n                           as directory argument.\n\n    label_columns(list or str): list or str representing the columns that store labels\n                                Labels can be in any one of the following formats:\n                                1. a single column string string (or integer) labels\n\n                                   image_fname,label\n                                   -----------------\n                                   image01,cat\n                                   image02,dog\n\n                                2. multiple columns for one-hot-encoded labels\n                                   image_fname,cat,dog\n                                   image01,1,0\n                                   image02,0,1\n\n                                3. a single column of numeric values for image regression\n                                   image_fname,age\n                                   -----------------\n                                   image01,68\n                                   image02,18\n\n    directory (string): path to directory containing images\n                        not required if image_column contains full filepaths\n    suffix(str): will be appended to each entry in image_column\n                 Used when the filenames in image_column do not contain file extensions.\n                 The extension in suffx should include ""."".\n    val_filepath (string): path to validation dataset in CSV format\n    suffix(string): suffix to add to file names in image_column\n    is_regression(bool): If True, task is treated as regression. \n                         Used when there is single column of numeric values and\n                         numeric values should be treated as numeric targets as opposed to class labels\n    target_size (tuple):  image dimensions\n    color_mode (string):  color mode\n    data_aug(ImageDataGenerator):  a keras.preprocessing.image.ImageDataGenerator\n                                  for data augmentation\n    val_pct(float):  proportion of training data to be used for validation\n                     only used if val_filepath is None\n    random_state(int): random seed for train/test split\n\n    Returns:\n    batches: a tuple of two Iterators - one for train and one for test\n\n    """"""\n\n    # convert to dataframes\n    train_df = pd.read_csv(train_filepath)\n    val_df = None\n    if val_filepath is not None:\n        val_df = pd.read_csv(val_filepath)\n\n\n    return images_from_df(train_df,\n                          image_column,\n                          label_columns=label_columns,\n                          directory=directory,\n                          suffix=suffix,\n                          val_df=val_df,\n                          is_regression=is_regression,\n                          target_size=target_size,\n                          color_mode=color_mode,\n                          data_aug=data_aug,\n                          val_pct=val_pct, random_state=random_state)\n\n\n\n\ndef images_from_fname( train_folder,\n                      pattern=r\'([^/]+)_\\d+.jpg$\',\n                      val_folder=None,\n                      is_regression=False,\n                     target_size=(224,224),\n                     color_mode=\'rgb\',\n                     data_aug=None,\n                     val_pct=0.1, random_state=None,\n                     verbose=1):\n\n    """"""\n    Returns image generator (Iterator instance).\n\n    Args:\n    train_folder (str): directory containing images\n    pat (str):  regular expression to extract class from file name of each image\n                Example: r\'([^/]+)_\\d+.jpg$\' to match \'english_setter\' in \'english_setter_140.jpg\'\n                By default, it will extract classes from file names of the form:\n                   <class_name>_<numbers>.jpg\n    val_folder (str): directory containing validation images. default:None\n    is_regression(bool): If True, task is treated as regression. \n                         Used when there is single column of numeric values and\n                         numeric values should be treated as numeric targets as opposed to class labels\n    target_size (tuple):  image dimensions\n    color_mode (string):  color mode\n    data_aug(ImageDataGenerator):  a keras.preprocessing.image.ImageDataGenerator\n                                  for data augmentation\n    val_pct(float):  proportion of training data to be used for validation\n                     only used if val_folder is None\n    random_state(int): random seed for train/test split\n    verbose(bool):   verbosity\n\n    Returns:\n    batches: a tuple of two Iterators - one for train and one for test\n\n    """"""\n\n    image_column = \'image_name\'\n    label_column = \'label\'\n    train_df = _img_fnames_to_df(train_folder, pattern, \n                                 image_column=image_column, label_column=label_column, verbose=verbose)\n    val_df = None\n    if val_folder is not None:\n        val_df = _img_fnames_to_df(val_folder, pattern, \n                                   image_column=image_column, label_column=label_column, verbose=verbose)\n    return images_from_df(train_df,\n                          image_column,\n                          label_columns=label_column,\n                          directory=train_folder,\n                          val_directory=val_folder,\n                          val_df=val_df,\n                          is_regression=is_regression,\n                          target_size=target_size,\n                          color_mode=color_mode,\n                          data_aug=data_aug,\n                          val_pct=val_pct, random_state=random_state)\n\n\n\ndef _img_fnames_to_df(img_folder,pattern, image_column=\'image_name\', label_column=\'label\',  verbose=1):\n    # get fnames\n    fnames = []\n    for ext in (\'*.gif\', \'*.png\', \'*.jpg\'):\n        fnames.extend(glob.glob(os.path.join(img_folder, ext)))\n\n    # process filenames and labels\n    image_names = []\n    labels = []\n    p = re.compile(pattern)\n    for fname in fnames:\n        r = p.search(fname)\n        if r:\n            image_names.append(os.path.basename(fname))\n            labels.append(r.group(1))\n        else:\n            warnings.warn(\'Could not extract target for %s -  skipping this file\'% (fname))\n    dct = {\'image_name\': image_names, \'label\':labels}\n    return pd.DataFrame(dct)\n    \n#    class_names = list(set(labels))\n#    class_names.sort()\n#    c2i = {k:v for v,k in enumerate(class_names)}\n#    labels = [c2i[label] for label in labels]\n#    labels = to_categorical(labels)\n#    #class_names = [str(c) in class_names]\n#    U.vprint(\'Found %s classes: %s\' % (len(class_names), class_names), verbose=verbose)\n#    U.vprint(\'y shape: (%s,%s)\' % (labels.shape[0], labels.shape[1]), verbose=verbose)\n#    dct = {\'image_name\': image_names}\n#    for i in range(labels.shape[1]):\n#        dct[class_names[i]] = labels[:,i]\n\n#    # convert to dataframes\n#    df = pd.DataFrame(dct)\n#    return (df, class_names)\n\n\n\ndef images_from_array(x_train, y_train,\n                      validation_data=None,\n                      val_pct=0.1,\n                      random_state=None,\n                      data_aug=None,\n                      classes=None,\n                      class_names=None,\n                      is_regression=False):\n\n    """"""\n    Returns image generator (Iterator instance) from training\n    and validation data in the form of NumPy arrays.\n    This function only supports image classification.\n    For image regression, please use images_from_df.\n\n    Args:\n      x_train(numpy.ndarray):  training gdata\n      y_train(numpy.ndarray):  labels must either be:\n                               1. one-hot (or multi-hot) encoded arrays\n                               2. integer values representing the label\n      validation_data (tuple): tuple of numpy.ndarrays for validation data.\n                               labels should be in one of the formats listed above.\n      val_pct(float): percentage of training data to use for validaton if validation_data is None\n      random_state(int): random state to use for splitting data\n      data_aug(ImageDataGenerator):  a keras.preprocessing.image.ImageDataGenerator\n      classes(str): old name for class_names - should no longer be used\n      class_names(str): list of strings to use as class names\n      is_regression(bool): If True, task is treated as regression. \n                           Used when there is single column of numeric values and\n                           numeric values should be treated as numeric targets as opposed to class labels\n    Returns:\n      batches: a tuple of two image.Iterator - one for train and one for test and ImagePreprocessor instance\n    """"""\n    if classes is not None: raise ValueError(\'Please use class_names argument instead of ""classes"".\')\n    if class_names and is_regression:\n        warnings.warn(\'is_regression=True, but class_names is not empty.  Task treated as regression.\')\n\n    # one-hot-encode if necessary\n    do_y_transform = False\n    if np.issubdtype(type(y_train[0]), np.integer) or np.issubdtype(type(y_train[0]), np.floating) or\\\n        (isinstance(y_train[0], (list, np.ndarray)) and len(y_train[0]) == 1):\n        if not is_regression:\n            if np.issubdtype(type(y_train[0]), np.integer) and not class_names:\n                warnings.warn(\'Targets are integers, but is_regression=False. Task treated as classification instead of regression.\')\n            y_train = to_categorical(y_train)\n            do_y_transform = True\n    if validation_data:\n        x_test = validation_data[0]\n        y_test = validation_data[1]\n        if do_y_transform:\n            y_test = to_categorical(y_test)\n    elif val_pct is not None and val_pct >0:\n        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n                                                            test_size=val_pct,\n                                                            random_state=random_state)\n    else:\n        x_test = None\n        y_test = None\n\n    # set class labels\n    if not class_names and not is_regression:\n        class_names = list(map(str, range(len(y_train[0]))))\n    elif class_names and not is_regression:\n        assert len(class_names) == len(y_train[0]), \\\n            ""Number of classes has to match length of the one-hot encoding""\n\n    # train and test data generators\n    (train_datagen, test_datagen) = process_datagen(data_aug, train_array=x_train)\n\n    # Image preprocessor\n    preproc = ImagePreprocessor(test_datagen, class_names, target_size=None, color_mode=None)\n\n    # training data\n    batches_tr = train_datagen.flow(x_train, y_train, shuffle=True)\n\n    # validation data\n    batches_te = None\n    if x_test is not None and y_test is not None:\n        batches_te = test_datagen.flow(x_test, y_test,\n                                       shuffle=False)\n    return (batches_tr, batches_te, preproc)\n\n'"
ktrain/vision/learner.py,0,"b'from ..imports import *\nfrom .. import utils as U\nfrom ..core import GenLearner\nfrom .data import show_image\n\n\n\n\nclass ImageClassLearner(GenLearner):\n    """"""\n    Main class used to tune and train Keras models for image classification.\n    Main parameters are:\n\n    model (Model): A compiled instance of keras.engine.training.Model\n    train_data (Iterator): a Iterator instance for training set\n    val_data (Iterator):   A Iterator instance for validation set\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False, multigpu=False):\n        super().__init__(model, train_data=train_data, val_data=val_data,\n                         batch_size=batch_size, eval_batch_size=eval_batch_size,\n                         workers=workers, use_multiprocessing=use_multiprocessing,\n                         multigpu=multigpu)\n        return\n\n    \n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n\n        # check validation data and arguments\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if val is None: raise Exception(\'val_data must be supplied to get_learner or view_top_losses\')\n\n        # get top losses and associated data\n        tups = self.top_losses(n=n, val_data=val, preproc=preproc)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            # Image Classification\n            if type(val).__name__ in [\'DirectoryIterator\', \'DataFrameIterator\']:\n                fpath = val.filepaths[tup[0]]\n                fp = os.path.join(os.path.basename(os.path.dirname(fpath)), os.path.basename(fpath))\n                plt.figure()\n                plt.title(""%s | loss:%s | true:%s | pred:%s)"" % (fp, round(loss,2), truth, pred))\n                show_image(fpath)\n            elif type(val).__name__ in [\'NumpyArrayIterator\']:\n                obs = val.x[idx]\n                #if preproc is not None: obs = preproc.undo(obs)\n                plt.figure()\n                plt.title(""id:%s | loss:%s | true:%s | pred:%s)"" % (idx, round(loss,2), truth, pred))\n                plt.imshow(np.squeeze(obs))\n                # everything else including text classification\n            else:\n                raise Exception(\'ImageClassLearner.view_top_losses only supports \' +\n                                \'DirectoryIterators, DataFrameIterators, and NumpyArrayIterators\')\n        return\n\n\n'"
ktrain/vision/models.py,1,"b'from ..imports import *\nfrom .. import utils as U\nfrom .wrn import create_wide_residual_network\n\n\n\n\nPRETRAINED_RESNET50 = \'pretrained_resnet50\'\nPRETRAINED_MOBILENET = \'pretrained_mobilenet\'\nPRETRAINED_INCEPTION = \'pretrained_inception\'\nRESNET50 = \'resnet50\'\nMOBILENET = \'mobilenet\'\nINCEPTION = \'inception\'\nCNN = \'default_cnn\'\nWRN22 = \'wrn22\'\nPRETRAINED_MODELS = [PRETRAINED_RESNET50, PRETRAINED_MOBILENET, PRETRAINED_INCEPTION]\nPREDEFINED_MODELS = PRETRAINED_MODELS + [RESNET50, MOBILENET, INCEPTION]\nIMAGE_CLASSIFIERS = {\n                     PRETRAINED_RESNET50: \'50-layer Residual Network (pretrained on ImageNet)\',\n                     RESNET50:  \'50-layer Resididual Network (randomly initialized)\',\n                     PRETRAINED_MOBILENET: \'MobileNet Neural Network (pretrained on ImageNet)\',\n                     MOBILENET:  \'MobileNet Neural Network (randomly initialized)\',\n                     PRETRAINED_INCEPTION: \'Inception Version 3  (pretrained on ImageNet)\',\n                     INCEPTION:  \'Inception Version 3 (randomly initialized)\',\n                     WRN22: \'22-layer Wide Residual Network (randomly initialized)\',\n                     CNN : \'a default LeNet-like Convolutional Neural Network\'}\n\ndef print_image_classifiers():\n    for k,v in IMAGE_CLASSIFIERS.items():\n        print(""%s: %s"" % (k,v))\n\n\ndef print_image_regression_models():\n    for k,v in IMAGE_CLASSIFIERS.items():\n        print(""%s: %s"" % (k,v))\n\n\ndef pretrained_datagen(data, name):\n    if not data or not U.is_iter(data): return\n    idg = data.image_data_generator\n    if name == PRETRAINED_RESNET50:\n        idg.preprocessing_function = pre_resnet50\n        idg.ktrain_preproc = \'resnet50\'\n        idg.rescale=None\n        idg.featurewise_center=False\n        idg.samplewise_center=False\n        idg.featurewise_std_normalization=False\n        idg.samplewise_std_normalization=False\n        idg.zca_whitening = False\n    elif name == PRETRAINED_MOBILENET:\n        idg.preprocessing_function = pre_mobilenet\n        idg.ktrain_preproc = \'mobilenet\'\n        idg.rescale=None\n        idg.featurewise_center=False\n        idg.samplewise_center=False\n        idg.featurewise_std_normalization=False\n        idg.samplewise_std_normalization=False\n        idg.zca_whitening = False\n    elif name == PRETRAINED_INCEPTION:\n        idg.preprocessing_function = pre_inception\n        idg.ktrain_preproc = \'inception\'\n        idg.rescale=None\n        idg.featurewise_center=False\n        idg.samplewise_center=False\n        idg.featurewise_std_normalization=False\n        idg.samplewise_std_normalization=False\n        idg.zca_whitening = False\n\n    return\n\n\n\ndef image_classifier(name,\n                     train_data,\n                     val_data=None,\n                     freeze_layers=None, \n                     metrics=[\'accuracy\'],\n                     optimizer_name = U.DEFAULT_OPT,\n                     multilabel=None,\n                     multigpu_number=None, \n                     pt_fc = [],\n                     pt_ps = [],\n                     verbose=1):\n\n    """"""\n    Returns a pre-trained ResNet50 model ready to be fine-tuned\n    for multi-class classification. By default, all layers are\n    trainable/unfrozen.\n\n\n    Args:\n        name (string): one of {\'pretrained_resnet50\', \'resnet50\', \'default_cnn\'}\n        train_data (image.Iterator): train data. Note: Will be manipulated here!\n        val_data (image.Iterator): validation data.  Note: Will be manipulated here!\n        freeze_layers (int):  number of beginning layers to make untrainable\n                            If None, then all layers except new Dense layers\n                            will be frozen/untrainable.\n        metrics (list):  metrics to use\n        optimizer_name(str): name of Keras optimizer (e.g., \'adam\', \'sgd\')\n        multilabel(bool):  If True, model will be build to support\n                           multilabel classificaiton (labels are not mutually exclusive).\n                           If False, binary/multiclassification model will be returned.\n                           If None, multilabel status will be inferred from data.\n        multigpu_number (int): Repicate model on this many GPUS.\n                               Must either be None or greater than 1.\n                               If greater than 1, must meet system specifications.\n                               If None, model is not replicated on multiple GPUS.\n        pt_fc (list of ints): number of hidden units in extra Dense layers\n                                before final Dense layer of pretrained model.\n                                Only takes effect if name in PRETRAINED_MODELS\n        pt_ps (list of floats): dropout probabilities to use before\n                                each extra Dense layer in pretrained model.\n                                Only takes effect if name in PRETRAINED_MODELS\n        verbose (int):         verbosity\n    Return:\n        model(Model):  the compiled model ready to be fine-tuned/trained\n\n        \n    """"""\n    return image_model(name, train_data, val_data=val_data, freeze_layers=freeze_layers,\n                       metrics=metrics, optimizer_name=optimizer_name, multilabel=multilabel,\n                       multigpu_number=multigpu_number,\n                       pt_fc=pt_fc, pt_ps=pt_ps, verbose=verbose)\n\n\n\n\ndef image_regression_model(name,\n                          train_data,\n                          val_data=None,\n                          freeze_layers=None, \n                          metrics=[\'mae\'],\n                          optimizer_name = U.DEFAULT_OPT,\n                          multigpu_number=None, \n                          pt_fc = [],\n                          pt_ps = [],\n                          verbose=1):\n\n    """"""\n    Returns a pre-trained ResNet50 model ready to be fine-tuned\n    for multi-class classification. By default, all layers are\n    trainable/unfrozen.\n\n\n    Args:\n        name (string): one of {\'pretrained_resnet50\', \'resnet50\', \'default_cnn\'}\n        train_data (image.Iterator): train data. Note: Will be manipulated here!\n        val_data (image.Iterator): validation data.  Note: Will be manipulated here!\n        freeze_layers (int):  number of beginning layers to make untrainable\n                            If None, then all layers except new Dense layers\n                            will be frozen/untrainable.\n        metrics (list):  metrics to use\n        optimizer_name(str): name of Keras optimizer (e.g., \'adam\', \'sgd\')\n        multilabel(bool):  If True, model will be build to support\n                           multilabel classificaiton (labels are not mutually exclusive).\n                           If False, binary/multiclassification model will be returned.\n                           If None, multilabel status will be inferred from data.\n        multigpu_number (int): Repicate model on this many GPUS.\n                               Must either be None or greater than 1.\n                               If greater than 1, must meet system specifications.\n                               If None, model is not replicated on multiple GPUS.\n        pt_fc (list of ints): number of hidden units in extra Dense layers\n                                before final Dense layer of pretrained model.\n                                Only takes effect if name in PRETRAINED_MODELS\n        pt_ps (list of floats): dropout probabilities to use before\n                                each extra Dense layer in pretrained model.\n                                Only takes effect if name in PRETRAINED_MODELS\n        verbose (int):         verbosity\n    Return:\n        model(Model):  the compiled model ready to be fine-tuned/trained\n\n        \n    """"""\n\n\n    return image_model(name, train_data, val_data=val_data, freeze_layers=freeze_layers,\n                       metrics=metrics, optimizer_name=optimizer_name, multilabel=False,\n                       multigpu_number=multigpu_number,\n                       pt_fc=pt_fc, pt_ps=pt_ps, verbose=verbose)\n\n\n\ndef image_model( name,\n                 train_data,\n                 val_data=None,\n                 freeze_layers=None, \n                 metrics=[\'accuracy\'],\n                 optimizer_name = U.DEFAULT_OPT,\n                 multilabel=None,\n                 multigpu_number=None, \n                 pt_fc = [],\n                 pt_ps = [],\n                 verbose=1):\n\n    """"""\n    Returns a pre-trained ResNet50 model ready to be fine-tuned\n    for multi-class classification or regression. By default, all layers are\n    trainable/unfrozen.\n\n\n    Args:\n        name (string): one of {\'pretrained_resnet50\', \'resnet50\', \'default_cnn\'}\n        train_data (image.Iterator): train data. Note: Will be manipulated here!\n        val_data (image.Iterator): validation data.  Note: Will be manipulated here!\n        freeze_layers (int):  number of beginning layers to make untrainable\n                            If None, then all layers except new Dense layers\n                            will be frozen/untrainable.\n        metrics (list):  metrics to use\n        optimizer_name(str): name of Keras optimizer (e.g., \'adam\', \'sgd\')\n        multilabel(bool):  If True, model will be build to support\n                           multilabel classificaiton (labels are not mutually exclusive).\n                           If False, binary/multiclassification model will be returned.\n                           If None, multilabel status will be inferred from data.\n        multigpu_number (int): Repicate model on this many GPUS.\n                               Must either be None or greater than 1.\n                               If greater than 1, must meet system specifications.\n                               If None, model is not replicated on multiple GPUS.\n        pt_fc (list of ints): number of hidden units in extra Dense layers\n                                before final Dense layer of pretrained model.\n                                Only takes effect if name in PRETRAINED_MODELS\n        pt_ps (list of floats): dropout probabilities to use before\n                                each extra Dense layer in pretrained model.\n                                Only takes effect if name in PRETRAINED_MODELS\n        verbose (int):         verbosity\n    Return:\n        model(Model):  the compiled model ready to be fine-tuned/trained\n\n        \n    """"""\n    # arg check\n    U.data_arg_check(train_data=train_data, train_required=True)\n    if name not in list(IMAGE_CLASSIFIERS.keys()):\n        raise ValueError(\'Unknown or unsupported model: %s\' % (name))\n\n\n    if not U.is_iter(train_data):\n        raise ValueError(\'train_data must be an Keras iterator \' +\\\n                         \'(e.g., DirectoryIterator, DataframIterator, \'+ \\\n                         \'NumpyArrayIterator) - please use the ktrain.data.images_from* \' +\\\n                         \'functions\')\n\n\n\n    # set pretrained flag\n    pretrained = True if name in PRETRAINED_MODELS else False\n\n    # adjust freeze_layers with warning\n    if not pretrained and freeze_layers is not None and freeze_layers > 0:\n        warnings.warn(\'Only pretrained models (e.g., pretrained_resnet50) support freeze_layers. \' +\\\n                      \'Setting freeze_layers to 0. Use one of the following models if\' +\\\n                      \'desiring a model pretrained on ImageNet: %s\' % (PRETRAINED_MODELS))\n        freeze_layers = 0\n\n    if pretrained and val_data is None:\n        raise ValueError(\'val_data is required if selecting a pretrained model, \'+\\\n                         \'as normalization scheme will be altered.\')\n\n    # adjust the data augmentation based on model selected\n    if pretrained:\n        pretrained_datagen(train_data, name)\n        pretrained_datagen(val_data, name)\n        U.vprint(\'The normalization scheme has been changed for use with a %s\' % (name) +\\\n                \' model. If you decide to use a different model, please reload your\' +\\\n                \' dataset with a ktrain.vision.data.images_from* function.\\n\', verbose=verbose)\n\n    # determine if multilabel\n    if multilabel is None:\n        multilabel = U.is_multilabel(train_data)\n    is_regression=False\n    if not multilabel and len(train_data[0][-1].shape) == 1: is_regression=True\n\n    # set loss and acivations\n    loss_func = \'categorical_crossentropy\'\n    activation = \'softmax\'\n    if multilabel:\n        loss_func = \'binary_crossentropy\'\n        activation = \'sigmoid\'\n    elif is_regression:\n        loss_func = \'mse\'\n        activation = None\n        if metrics == [\'accuracy\']: metrics = [\'mae\']\n\n    U.vprint(""Is Multi-Label? %s"" % (multilabel), verbose=verbose)\n    U.vprint(""Is Regression? %s"" % (is_regression), verbose=verbose)\n\n\n    # determine number of classes and shape\n    num_classes = 1 if is_regression else U.nclasses_from_data(train_data)\n    input_shape = U.shape_from_data(train_data)\n\n\n\n    #------------\n    # build model\n    #------------\n    if type(multigpu_number) == type(1) and multigpu_number < 2:\n        raise ValuError(\'multigpu_number must either be None or > 1\')\n    if multigpu_number is not None and multigpu_number >1:\n        with tf.device(""/cpu:0""):\n            model = build_visionmodel(name,\n                                      num_classes,\n                                      input_shape=input_shape,\n                                      freeze_layers=freeze_layers,\n                                      activation=activation,\n                                      pt_fc = pt_fc,\n                                      pt_ps = pt_ps)\n        parallel_model = multi_gpu_model(model, gpus=multigpu_number)\n        parallel_model.compile(optimizer=optimizer_name, \n                               loss=\'categorical_crossentropy\', metrics=metrics)\n        return parallel_model\n    else:\n        model = build_visionmodel(name,\n                                  num_classes,\n                                  input_shape=input_shape,\n                                  freeze_layers=freeze_layers,\n                                  activation=activation,\n                                  pt_fc = pt_fc,\n                                  pt_ps = pt_ps)\n        model.compile(optimizer=optimizer_name, loss=loss_func, metrics=metrics)\n        return model\n\n\ndef build_visionmodel(name,\n                      num_classes, \n                      input_shape=(224,224,3),\n                      freeze_layers=2, \n                      activation=\'softmax\',\n                      pt_fc=[],\n                      pt_ps = []):\n\n    if name in PREDEFINED_MODELS:\n        model = build_predefined(name, num_classes,\n                                  input_shape=input_shape,\n                                  freeze_layers=freeze_layers,\n                                  activation=activation,\n                                  pt_fc = pt_fc,\n                                  pt_ps = pt_ps)\n    elif name == CNN:\n        model = build_cnn(num_classes,\n                           input_shape=input_shape,\n                           activation=activation)\n    elif name == WRN22:\n        model = create_wide_residual_network(input_shape, nb_classes=num_classes, \n                                             N=3, k=6, dropout=0.00,\n                                             activation=activation, verbose=0)\n    else:\n        raise ValueError(\'Unknown model: %s\' % (name))\n    U.vprint(\'%s model created.\' % (name))\n\n    return model\n\n\n\ndef build_cnn(num_classes, \n              input_shape=(28,28,1),\n              activation=\'softmax\'):\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3),activation=\'relu\',\n                     kernel_initializer=\'he_normal\',input_shape=input_shape))\n    model.add(Conv2D(32, kernel_size=(3, 3),activation=\'relu\',\n                     kernel_initializer=\'he_normal\'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.20))\n    model.add(Conv2D(64, (3, 3), activation=\'relu\',padding=\'same\',\n                      kernel_initializer=\'he_normal\'))\n    model.add(Conv2D(64, (3, 3), activation=\'relu\',padding=\'same\',\n                     kernel_initializer=\'he_normal\'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, (3, 3), activation=\'relu\',padding=\'same\',\n                     kernel_initializer=\'he_normal\'))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation=\'relu\'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(num_classes, activation=activation))\n    return model\n\n\ndef build_predefined(\n                   name,\n                   num_classes, \n                   input_shape=(224,224,3),\n                   freeze_layers=None, \n                   activation=\'softmax\',\n                   pt_fc=[],\n                   pt_ps=[]):\n    """"""\n    Builds a pre-defined architecture supported in Keras.\n\n    Args:\n        name (str): one of ktrain.vision.model.PREDEFINED_MODELS\n        num_classes (int): # of classes\n        input_shape (tuple): the input shape including channels\n        freeze_layers (int): number of early layers to freeze.\n                             Only takes effect if name in PRETRAINED_MODELS.\n                             If None and name in PRETRAINED_MODELS,\n                             all layers except the ""custom head"" \n                             fully-connected (Dense) layers are frozen.\n        activation (str):    name of the Keras activation to use in final layer\n        pt_fc (list of ints): number of hidden units in extra Dense layers\n                                before final Dense layer of pretrained model\n        pt_ps (list of floats): dropout probabilities to use before\n                                      each extra Dense layer in pretrained model\n\n    """"""\n\n    # default parameters\n    include_top = False\n    input_tensor = None\n    dropout = 0.5 # final dropout\n\n    # setup pretrained\n    weights = \'imagenet\' if name in PRETRAINED_MODELS else None\n\n    # setup the pretrained network\n    if name in [RESNET50, PRETRAINED_RESNET50]:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\'ignore\')\n            net = ResNet50(include_top=include_top, \n                           weights=weights,\n                           input_tensor=input_tensor,\n                           input_shape = input_shape)\n    elif name in [MOBILENET, PRETRAINED_MOBILENET]:\n        net = MobileNet(include_top=include_top, \n                        weights=weights,\n                        input_tensor=input_tensor,\n                        input_shape = input_shape)\n    elif name in [INCEPTION, PRETRAINED_INCEPTION]:\n        net = InceptionV3(include_top=include_top, \n                          weights=weights,\n                          input_tensor=input_tensor,\n                           input_shape = input_shape)\n    else:\n        raise ValueError(\'Unsupported model: %s\' % (name))\n\n\n    if freeze_layers is None:\n        for layer in net.layers:\n            layer.trainable = False\n\n    x = net.output\n    x = Flatten()(x)\n\n    # xtra FCs in pretrained model\n    if name in PRETRAINED_MODELS:\n        if len(pt_fc) != len(pt_ps):\n            raise ValueError(\'size off xtra_fc must match size of fc_dropouts\')\n        for i, fc in enumerate(pt_fc):\n            p = pt_ps[i]\n            fc_name = ""fc%s"" % (i)\n            if p is not None:\n                x = Dropout(p)(x)\n            x = Dense(fc, activation=\'relu\', \n                      kernel_initializer=\'he_normal\', name=fc_name)(x)\n\n\n    # final FC\n    x = Dropout(dropout)(x)\n    output_layer = Dense(num_classes, activation=activation, name=activation)(x)\n    model = Model(inputs=net.input, outputs=output_layer)\n\n    if freeze_layers is not None:\n        # set certain earlier layers as non-trainable\n        for layer in model.layers[:freeze_layers]:\n            layer.trainable = False\n        for layer in model.layers[freeze_layers:]:\n            layer.trainable = True\n\n    # set optimizer, loss, and metrics and return model\n    return model\n'"
ktrain/vision/predictor.py,3,"b'from ..imports import *\nfrom ..predictor import Predictor\nfrom .preprocessor import ImagePreprocessor\nfrom .. import utils as U\n\n\n\nclass ImagePredictor(Predictor):\n    """"""\n    predicts image classes\n    """"""\n\n    def __init__(self, model, preproc, batch_size=U.DEFAULT_BS):\n\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        if not isinstance(preproc, ImagePreprocessor):\n            raise ValueError(\'preproc must be instance of ImagePreprocessor\')\n        self.model = model\n        self.preproc = preproc\n        self.datagen = self.preproc.get_preprocessor()\n        self.c = self.preproc.get_classes()\n        self.batch_size = batch_size\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def explain(self, img_fpath):\n        """"""\n        Highlights image to explain prediction\n        """"""\n        #if U.is_tf_keras():\n            #warnings.warn(""currently_unsupported: explain() method is not available because tf.keras is ""+\\\n                          #""not yet adequately supported by the eli5 library. You can switch to "" +\\\n                          #""stand-alone Keras by setting os.environ[\'TF_KERAS\']=\'0\'"" )\n            #return\n\n        try:\n            import eli5\n        except:\n            msg = \'ktrain requires a forked version of eli5 to support tf.keras. \'+\\\n                  \'Install with: pip3 install git+https://github.com/amaiya/eli5@tfkeras_0_10_1\'\n            warnings.warn(msg)\n            return\n\n        if not hasattr(eli5, \'KTRAIN\'):\n            warnings.warn(""Since eli5 does not yet support tf.keras, ktrain uses a forked version of eli5.  "" +\\\n                           ""We do not detect this forked version, so predictor.explain will not work.  "" +\\\n                           ""It will work if you uninstall the current version of eli5 and install ""+\\\n                           ""the forked version:  "" +\\\n                           ""pip3 install git+https://github.com/amaiya/eli5@tfkeras_0_10_1"")\n            return\n\n        if not DISABLE_V2_BEHAVIOR:\n            warnings.warn(""Please add os.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\' at top of your script or notebook."")\n            msg = ""\\nFor image classification, the explain method currently requires disabling V2 behavior in TensorFlow 2.\\n"" +\\\n                    ""Please add the following to the top of your script or notebook BEFORE you import ktrain and restart Colab runtime or Jupyter kernel:\\n\\n"" +\\\n                  ""import os\\n"" +\\\n                  ""os.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\\n""\n            print(msg)\n            return\n\n\n        img = image.load_img(img_fpath,\n                             target_size=self.preproc.target_size,\n                             color_mode=self.preproc.color_mode)\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        return eli5.show_prediction(self.model, x)\n\n\n\n\n    def predict(self, data, return_proba=False):\n        """"""\n        Predicts class from image in array format.\n        If return_proba is True, returns probabilities of each class.\n        """"""\n        if not isinstance(data, np.ndarray):\n            raise ValueError(\'data must be numpy.ndarray\')\n        (generator, steps) = self.preproc.preprocess(data, batch_size=self.batch_size)\n        return self.predict_generator(generator, steps=steps, return_proba=return_proba)\n\n\n    def predict_filename(self, img_path, return_proba=False):\n        """"""\n        Predicts class from filepath to single image file.\n        If return_proba is True, returns probabilities of each class.\n        """"""\n        if not os.path.isfile(img_path): raise ValueError(\'img_path must be valid file\')\n        (generator, steps) = self.preproc.preprocess(img_path, batch_size=self.batch_size)\n        return self.predict_generator(generator, steps=steps, return_proba=return_proba)\n\n\n    def predict_folder(self, folder, return_proba=False):\n        """"""\n        Predicts the classes of all images in a folder.\n        If return_proba is True, returns probabilities of each class.\n\n        """"""\n        if not os.path.isdir(folder): raise ValueError(\'folder must be valid directory\')\n        (generator, steps) = self.preproc.preprocess(folder, batch_size=self.batch_size)\n        result = self.predict_generator(generator, steps=steps, return_proba=return_proba)\n        if len(result) != len(generator.filenames):\n            raise Exception(\'number of results does not equal number of filenames\')\n        return list(zip(generator.filenames, result))\n\n\n    def predict_generator(self, generator, steps=None, return_proba=False):\n        #loss = self.model.loss\n        #if callable(loss): loss = loss.__name__\n        #treat_multilabel = False\n        #if loss != \'categorical_crossentropy\' and not return_proba:\n        #    return_proba=True\n        #    treat_multilabel = True\n        classification, multilabel = U.is_classifier(self.model)\n        if not classification: return_proba=True\n        # *_generator methods are deprecated from TF 2.1.0\n        #preds =  self.model.predict_generator(generator, steps=steps)\n        preds =  self.model.predict(generator, steps=steps)\n        result =  preds if return_proba or multilabel else [self.c[np.argmax(pred)] for pred in preds]\n        if multilabel and not return_proba:\n            return [list(zip(self.c, r)) for r in result]\n        if not classification:\n            return np.squeeze(result, axis=1)\n        else:\n            return result\n\n\n    def predict_proba(self, data):\n        return self.predict(data, return_proba=True)\n\n\n    def predict_proba_folder(self, folder):\n        return self.predict_folder(folder, return_proba=True)\n\n\n    def predict_proba_filename(self, img_path):\n        return self.predict_filename(img_path, return_proba=True)\n\n\n    def predict_proba_generator(self, generator, steps=None):\n        return self.predict_proba_generator(generator, steps=steps, return_proba=True)\n\n\n\n    def analyze_valid(self, generator, print_report=True, multilabel=None):\n        """"""\n        Makes predictions on validation set and returns the confusion matrix.\n        Accepts as input a genrator (e.g., DirectoryIterator, DataframeIterator)\n        representing the validation set.\n\n\n        Optionally prints a classification report.\n        Currently, this method is only supported for binary and multiclass\n        problems, not multilabel classification problems.\n\n        """"""\n        if multilabel is None:\n            multilabel = U.is_multilabel(generator)\n        if multilabel:\n            warnings.warn(\'multilabel_confusion_matrix not yet supported - skipping\')\n            return\n\n        y_true = generator.classes\n        # *_generator methods are deprecated from TF 2.1.0\n        #y_pred = self.model.predict_generator(generator)\n        y_pred = self.model.predict(generator)\n        y_pred = np.argmax(y_pred, axis=1)\n        if print_report:\n            print(classification_report(y_true, y_pred, target_names=self.c))\n        if not multilabel:\n            cm_func = confusion_matrix\n            cm =  cm_func(y_true,  y_pred)\n        else:\n            cm = None\n        return cm\n\n\n    def _save_preproc(self, fpath):\n        preproc_name = \'tf_model.preproc\'\n        with open(os.path.join(fpath, preproc_name), \'wb\') as f:\n            datagen = self.preproc.get_preprocessor()\n            pfunc = datagen.preprocessing_function\n            datagen.preprocessing_function = None\n            pickle.dump(self.preproc, f)\n            datagen.preprocessing_function = pfunc\n        return\n\n'"
ktrain/vision/preprocessor.py,0,"b'from ..imports import *\nfrom ..preprocessor import Preprocessor\nfrom .. import utils as U\nclass ImagePreprocessor(Preprocessor):\n    """"""\n    Image preprocessing\n    """"""\n\n    def __init__(self, datagen, classes, target_size=(224,224), color_mode=\'rgb\'):\n\n        if not isinstance(datagen, image.ImageDataGenerator):\n            raise ValueError(\'datagen must be instance of ImageDataGenerator\')\n        self.datagen = datagen\n        self.c = classes\n        self.target_size = target_size\n        self.color_mode = color_mode\n\n\n    def get_preprocessor(self):\n        return self.datagen\n\n    def get_classes(self):\n        return self.c\n\n    def preprocess(self, data, batch_size=U.DEFAULT_BS):\n        """"""\n        Receives raw data and returns \n        tuple containing the generator and steps\n        argument for model.predict.\n        """"""\n        # input is an array of pixel values\n        if isinstance(data, np.ndarray):\n            generator = self.datagen.flow(data, shuffle=False)\n            generator.batch_size = batch_size\n            nsamples = len(data)\n            steps = math.ceil(nsamples/batch_size)\n            return (generator, steps)\n\n        # input is a folder of images\n        elif os.path.isdir(data):\n            folder = data\n            if folder[-1] != os.sep: folder += os.sep\n            parent = os.path.dirname(os.path.dirname(folder))\n            folder_name = os.path.basename(os.path.dirname(folder))\n            if self.target_size is None or self.color_mode is None:\n                raise Exception(\'To use predict_folder, you must load the data using either \'+\\\n                                \'the images_from_folder function or the images_from_csv function.\')\n            generator= self.datagen.flow_from_directory(parent,\n                                                       classes=[folder_name],\n                                                       target_size=self.target_size,\n                                                       class_mode=\'categorical\',\n                                                       shuffle=False,\n                                                       interpolation=\'bicubic\',\n                                                       color_mode = self.color_mode)\n            generator.batch_size = batch_size\n            nsamples = generator.samples\n            steps = math.ceil(nsamples/batch_size)\n            return (generator, steps)\n        # input is the path to an image file\n        elif os.path.isfile(data):\n            if self.target_size is None or self.color_mode is None:\n                raise Exception(\'To use predict_filename, you must load the data using either \'+\\\n                                \'the ktrain.vision.images_from_folder function or the \' +\\\n                                \'ktrain.vision.images_from_csv function.\')\n            img = image.load_img(data, target_size=self.target_size, color_mode=self.color_mode)\n            x = image.img_to_array(img)\n            x = np.expand_dims(x, axis=0)\n            generator =  self.datagen.flow(np.array(x), shuffle=False)\n            generator.batch_size = batch_size\n            nsamples = 1\n            steps = math.ceil(nsamples/batch_size)\n            return (generator, steps)\n        else:\n            raise ValueError(\'data argument is not valid file, folder, or numpy.ndarray\')\n\n\n\n\n\n'"
ktrain/vision/wrn.py,0,"b'from ..imports import *\n\nweight_decay = 0.0005\n\ndef initial_conv(input):\n    x = Convolution2D(16, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(input)\n\n    channel_axis = 1 if K.image_data_format() == ""channels_first"" else -1\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n    return x\n\n\ndef expand_conv(init, base, k, strides=(1, 1)):\n    x = Convolution2D(base * k, (3, 3), padding=\'same\', strides=strides, kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(init)\n\n    channel_axis = 1 if K.image_data_format() == ""channels_first"" else -1\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Convolution2D(base * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    skip = Convolution2D(base * k, (1, 1), padding=\'same\', strides=strides, kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(init)\n\n    m = Add()([x, skip])\n\n    return m\n\n\ndef conv1_block(input, k=1, dropout=0.0):\n    init = input\n\n    channel_axis = 1 if K.image_data_format() == ""channels_first"" else -1\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(input)\n    x = Activation(\'relu\')(x)\n    x = Convolution2D(16 * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    if dropout > 0.0: x = Dropout(dropout)(x)\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n    x = Convolution2D(16 * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    m = Add()([init, x])\n    return m\n\ndef conv2_block(input, k=1, dropout=0.0):\n    init = input\n\n    #channel_axis = 1 if K.image_dim_ordering() == ""th"" else -1\n    channel_axis = -1 \n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(input)\n    x = Activation(\'relu\')(x)\n    x = Convolution2D(32 * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    if dropout > 0.0: x = Dropout(dropout)(x)\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n    x = Convolution2D(32 * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    m = Add()([init, x])\n    return m\n\ndef conv3_block(input, k=1, dropout=0.0):\n    init = input\n\n    #channel_axis = 1 if K.image_dim_ordering() == ""th"" else -1\n    channel_axis = -1\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(input)\n    x = Activation(\'relu\')(x)\n    x = Convolution2D(64 * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    if dropout > 0.0: x = Dropout(dropout)(x)\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n    x = Convolution2D(64 * k, (3, 3), padding=\'same\', kernel_initializer=\'he_normal\',\n                      kernel_regularizer=l2(weight_decay),\n                      use_bias=False)(x)\n\n    m = Add()([init, x])\n    return m\n\ndef create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, \n                                 activation=\'softmax\', dropout=0.0, verbose=1):\n    """"""\n    Creates a Wide Residual Network with specified parameters\n\n    :param input: Input Keras object\n    :param nb_classes: Number of output classes\n    :param N: Depth of the network. Compute N = (n - 4) / 6.\n              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n    :param k: Width of the network.\n    :param dropout: Adds dropout if value is greater than 0.0\n    :param verbose: Debug info to describe created WRN\n    :return:\n    """"""\n    channel_axis = 1 if K.image_data_format() == ""channels_first"" else -1\n\n    ip = Input(shape=input_dim)\n\n    x = initial_conv(ip)\n    nb_conv = 4\n\n    x = expand_conv(x, 16, k)\n    nb_conv += 2\n\n    for i in range(N - 1):\n        x = conv1_block(x, k, dropout)\n        nb_conv += 2\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = expand_conv(x, 32, k, strides=(2, 2))\n    nb_conv += 2\n\n    for i in range(N - 1):\n        x = conv2_block(x, k, dropout)\n        nb_conv += 2\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = expand_conv(x, 64, k, strides=(2, 2))\n    nb_conv += 2\n\n    for i in range(N - 1):\n        x = conv3_block(x, k, dropout)\n        nb_conv += 2\n\n    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer=\'uniform\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = AveragePooling2D((8, 8))(x)\n    x = Flatten()(x)\n\n    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay), activation=activation)(x)\n\n    model = Model(ip, x)\n\n    if verbose: print(""Wide Residual Network-%d-%d created."" % (nb_conv, k))\n    return model\n\nif __name__ == ""__main__"":\n\n    init = (32, 32, 3)\n\n    wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=2, dropout=0.0)\n\n    wrn_28_10.summary()\n\n    plot_model(wrn_28_10, ""WRN-16-2.png"", show_shapes=True, show_layer_names=True)\n'"
ktrain/tests/extra_tests/testrun_bart.py,0,"b'import os\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID"";\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0"" \n\nfrom ktrain import text\nts = text.TransformerSummarizer()\n\n\nsample_doc = """"""Archive-name: space/new_probes\nLast-modified: $Date: 93/04/01 14:39:17 $\n\nUPCOMING PLANETARY PROBES - MISSIONS AND SCHEDULES\n\n    Information on upcoming or currently active missions not mentioned below\n    would be welcome. Sources: NASA fact sheets, Cassini Mission Design\n    team, ISAS/NASDA launch schedules, press kits.\n\n\n    ASUKA (ASTRO-D) - ISAS (Japan) X-ray astronomy satellite, launched into\n    Earth orbit on 2/20/93. Equipped with large-area wide-wavelength (1-20\n    Angstrom) X-ray telescope, X-ray CCD cameras, and imaging gas\n    scintillation proportional counters.\n\n\n    CASSINI - Saturn orbiter and Titan atmosphere probe. Cassini is a joint\n    NASA/ESA project designed to accomplish an exploration of the Saturnian\n    system with its Cassini Saturn Orbiter and Huygens Titan Probe. Cassini\n    is scheduled for launch aboard a Titan IV/Centaur in October of 1997.\n    After gravity assists of Venus, Earth and Jupiter in a VVEJGA\n    trajectory, the spacecraft will arrive at Saturn in June of 2004. Upon\n    arrival, the Cassini spacecraft performs several maneuvers to achieve an\n    orbit around Saturn. Near the end of this initial orbit, the Huygens\n    Probe separates from the Orbiter and descends through the atmosphere of\n    Titan. The Orbiter relays the Probe data to Earth for about 3 hours\n    while the Probe enters and traverses the cloudy atmosphere to the\n    surface. After the completion of the Probe mission, the Orbiter\n    continues touring the Saturnian system for three and a half years. Titan\n    synchronous orbit trajectories will allow about 35 flybys of Titan and\n    targeted flybys of Iapetus, Dione and Enceladus. The objectives of the\n    mission are threefold: conduct detailed studies of Saturn\'s atmosphere,\n    rings and magnetosphere; conduct close-up studies of Saturn\'s\n    satellites, and characterize Titan\'s atmosphere and surface.\n\n    One of the most intriguing aspects of Titan is the possibility that its\n    surface may be covered in part with lakes of liquid hydrocarbons that\n    result from photochemical processes in its upper atmosphere. These\n    hydrocarbons condense to form a global smog layer and eventually rain\n    down onto the surface. The Cassini orbiter will use onboard radar to\n    peer through Titan\'s clouds and determine if there is liquid on the\n    surface. Experiments aboard both the orbiter and the entry probe will\n    investigate the chemical processes that produce this unique atmosphere.\n\n    The Cassini mission is named for Jean Dominique Cassini (1625-1712), the\n    first director of the Paris Observatory, who discovered several of\n    Saturn\'s satellites and the major division in its rings. The Titan\n    atmospheric entry probe is named for the Dutch physicist Christiaan\n    Huygens (1629-1695), who discovered Titan and first described the true\n    nature of Saturn\'s rings.\n\n\t Key Scheduled Dates for the Cassini Mission (VVEJGA Trajectory)\n\t -------------------------------------------------------------\n\t   10/06/97 - Titan IV/Centaur Launch\n\t   04/21/98 - Venus 1 Gravity Assist\n\t   06/20/99 - Venus 2 Gravity Assist\n\t   08/16/99 - Earth Gravity Assist\n\t   12/30/00 - Jupiter Gravity Assist\n\t   06/25/04 - Saturn Arrival\n\t   01/09/05 - Titan Probe Release\n\t   01/30/05 - Titan Probe Entry\n\t   06/25/08 - End of Primary Mission\n\t    (Schedule last updated 7/22/92)\n\n\n    GALILEO - Jupiter orbiter and atmosphere probe, in transit. Has returned\n    the first resolved images of an asteroid, Gaspra, while in transit to\n    Jupiter. Efforts to unfurl the stuck High-Gain Antenna (HGA) have\n    essentially been abandoned. JPL has developed a backup plan using data\n    compression (JPEG-like for images, lossless compression for data from\n    the other instruments) which should allow the mission to achieve\n    approximately 70% of its original objectives.\n\n\t   Galileo Schedule\n\t   ----------------\n\t   10/18/89 - Launch from Space Shuttle\n\t   02/09/90 - Venus Flyby\n\t   10/**/90 - Venus Data Playback\n\t   12/08/90 - 1st Earth Flyby\n\t   05/01/91 - High Gain Antenna Unfurled\n\t   07/91 - 06/92 - 1st Asteroid Belt Passage\n\t   10/29/91 - Asteroid Gaspra Flyby\n\t   12/08/92 - 2nd Earth Flyby\n\t   05/93 - 11/93 - 2nd Asteroid Belt Passage\n\t   08/28/93 - Asteroid Ida Flyby\n\t   07/02/95 - Probe Separation\n\t   07/09/95 - Orbiter Deflection Maneuver\n\t   12/95 - 10/97 - Orbital Tour of Jovian Moons\n\t   12/07/95 - Jupiter/Io Encounter\n\t   07/18/96 - Ganymede\n\t   09/28/96 - Ganymede\n\t   12/12/96 - Callisto\n\t   01/23/97 - Europa\n\t   02/28/97 - Ganymede\n\t   04/22/97 - Europa\n\t   05/31/97 - Europa\n\t   10/05/97 - Jupiter Magnetotail Exploration\n\n\n    HITEN - Japanese (ISAS) lunar probe launched 1/24/90. Has made\n    multiple lunar flybys. Released Hagoromo, a smaller satellite,\n    into lunar orbit. This mission made Japan the third nation to\n    orbit a satellite around the Moon.\n\n\n    MAGELLAN - Venus radar mapping mission. Has mapped almost the entire\n    surface at high resolution. Currently (4/93) collecting a global gravity\n    map.\n\n\n    MARS OBSERVER - Mars orbiter including 1.5 m/pixel resolution camera.\n    Launched 9/25/92 on a Titan III/TOS booster. MO is currently (4/93) in\n    transit to Mars, arriving on 8/24/93. Operations will start 11/93 for\n    one martian year (687 days).\n\n\n    TOPEX/Poseidon - Joint US/French Earth observing satellite, launched\n    8/10/92 on an Ariane 4 booster. The primary objective of the\n    TOPEX/POSEIDON project is to make precise and accurate global\n    observations of the sea level for several years, substantially\n    increasing understanding of global ocean dynamics. The satellite also\n    will increase understanding of how heat is transported in the ocean.\n\n\n    ULYSSES- European Space Agency probe to study the Sun from an orbit over\n    its poles. Launched in late 1990, it carries particles-and-fields\n    experiments (such as magnetometer, ion and electron collectors for\n    various energy ranges, plasma wave radio receivers, etc.) but no camera.\n\n    Since no human-built rocket is hefty enough to send Ulysses far out of\n    the ecliptic plane, it went to Jupiter instead, and stole energy from\n    that planet by sliding over Jupiter\'s north pole in a gravity-assist\n    manuver in February 1992. This bent its path into a solar orbit tilted\n    about 85 degrees to the ecliptic. It will pass over the Sun\'s south pole\n    in the summer of 1993. Its aphelion is 5.2 AU, and, surprisingly, its\n    perihelion is about 1.5 AU-- that\'s right, a solar-studies spacecraft\n    that\'s always further from the Sun than the Earth is!\n\n    While in Jupiter\'s neigborhood, Ulysses studied the magnetic and\n    radiation environment. For a short summary of these results, see\n    *Science*, V. 257, p. 1487-1489 (11 September 1992). For gory technical\n    detail, see the many articles in the same issue.\n\n\n    OTHER SPACE SCIENCE MISSIONS (note: this is based on a posting by Ron\n    Baalke in 11/89, with ISAS/NASDA information contributed by Yoshiro\n    Yamada (yamada@yscvax.ysc.go.jp). I\'m attempting to track changes based\n    on updated shuttle manifests; corrections and updates are welcome.\n\n    1993 Missions\n\to ALEXIS [spring, Pegasus]\n\t    ALEXIS (Array of Low-Energy X-ray Imaging Sensors) is to perform\n\t    a wide-field sky survey in the ""soft"" (low-energy) X-ray\n\t    spectrum. It will scan the entire sky every six months to search\n\t    for variations in soft-X-ray emission from sources such as white\n\t    dwarfs, cataclysmic variable stars and flare stars. It will also\n\t    search nearby space for such exotic objects as isolated neutron\n\t    stars and gamma-ray bursters. ALEXIS is a project of Los Alamos\n\t    National Laboratory and is primarily a technology development\n\t    mission that uses astrophysical sources to demonstrate the\n\t    technology. Contact project investigator Jeffrey J Bloch\n\t    (jjb@beta.lanl.gov) for more information.\n\n\to Wind [Aug, Delta II rocket]\n\t    Satellite to measure solar wind input to magnetosphere.\n\n\to Space Radar Lab [Sep, STS-60 SRL-01]\n\t    Gather radar images of Earth\'s surface.\n\n\to Total Ozone Mapping Spectrometer [Dec, Pegasus rocket]\n\t    Study of Stratospheric ozone.\n\n\to SFU (Space Flyer Unit) [ISAS]\n\t    Conducting space experiments and observations and this can be\n\t    recovered after it conducts the various scientific and\n\t    engineering experiments. SFU is to be launched by ISAS and\n\t    retrieved by the U.S. Space Shuttle on STS-68 in 1994.\n\n    1994\n\to Polar Auroral Plasma Physics [May, Delta II rocket]\n\t    June, measure solar wind and ions and gases surrounding the\n\t    Earth.\n\n\to IML-2 (STS) [NASDA, Jul 1994 IML-02]\n\t    International Microgravity Laboratory.\n\n\to ADEOS [NASDA]\n\t    Advanced Earth Observing Satellite.\n\n\to MUSES-B (Mu Space Engineering Satellite-B) [ISAS]\n\t    Conducting research on the precise mechanism of space structure\n\t    and in-space astronomical observations of electromagnetic waves.\n\n    1995\n\tLUNAR-A [ISAS]\n\t    Elucidating the crust structure and thermal construction of the\n\t    moon\'s interior.\n\n\n    Proposed Missions:\n\to Advanced X-ray Astronomy Facility (AXAF)\n\t    Possible launch from shuttle in 1995, AXAF is a space\n\t    observatory with a high resolution telescope. It would orbit for\n\t    15 years and study the mysteries and fate of the universe.\n\n\to Earth Observing System (EOS)\n\t    Possible launch in 1997, 1 of 6 US orbiting space platforms to\n\t    provide long-term data (15 years) of Earth systems science\n\t    including planetary evolution.\n\n\to Mercury Observer\n\t    Possible 1997 launch.\n\n\to Lunar Observer\n\t    Possible 1997 launch, would be sent into a long-term lunar\n\t    orbit. The Observer, from 60 miles above the moon\'s poles, would\n\t    survey characteristics to provide a global context for the\n\t    results from the Apollo program.\n\n\to Space Infrared Telescope Facility\n\t    Possible launch by shuttle in 1999, this is the 4th element of\n\t    the Great Observatories program. A free-flying observatory with\n\t    a lifetime of 5 to 10 years, it would observe new comets and\n\t    other primitive bodies in the outer solar system, study cosmic\n\t    birth formation of galaxies, stars and planets and distant\n\t    infrared-emitting galaxies\n\n\to Mars Rover Sample Return (MRSR)\n\t    Robotics rover would return samples of Mars\' atmosphere and\n\t    surface to Earch for analysis. Possible launch dates: 1996 for\n\t    imaging orbiter, 2001 for rover.\n\n\to Fire and Ice\n\t    Possible launch in 2001, will use a gravity assist flyby of\n\t    Earth in 2003, and use a final gravity assist from Jupiter in\n\t    2005, where the probe will split into its Fire and Ice\n\t    components: The Fire probe will journey into the Sun, taking\n\t    measurements of our star\'s upper atmosphere until it is\n\t    vaporized by the intense heat. The Ice probe will head out\n\t    towards Pluto, reaching the tiny world for study by 2016.""""""\n\n\nprint(ts.summarize(sample_doc))\n\n'"
ktrain/text/ner/__init__.py,0,"b'from .data import entities_from_gmb, entities_from_conll2003\nfrom .models import sequence_tagger, print_sequence_taggers\nfrom .anago.layers import crf_loss\n'"
ktrain/text/ner/data.py,0,"b'\nfrom ...imports import *\nfrom ... import utils as U\nfrom .. import textutils as TU\nfrom . import preprocessor as pp\nfrom .preprocessor import NERPreprocessor\n\nfrom .anago.preprocessing import IndexTransformer\n\n\nMAXLEN = 128\nWORD_COL = pp.WORD_COL\nTAG_COL = pp.TAG_COL\nSENT_COL = pp.SENT_COL\n\n\n\ndef entities_from_gmb(train_filepath, \n                      val_filepath=None,\n                      use_char=False,\n                      word_column=WORD_COL,\n                      tag_column=TAG_COL,\n                      sentence_column=SENT_COL,\n                       encoding=None,\n                       val_pct=0.1, verbose=1):\n    """"""\n    Loads sequence-labeled data from text file in the  Groningen\n    Meaning Bank  (GMB) format.\n    """"""\n\n\n    return entities_from_txt(train_filepath=train_filepath,\n                             val_filepath=val_filepath,\n                             use_char=use_char,\n                             word_column=word_column,\n                             tag_column=tag_column,\n                             sentence_column=sentence_column,\n                             data_format=\'gmb\',\n                             encoding=encoding,\n                             val_pct=val_pct, verbose=verbose)\n\n\n        \ndef entities_from_conll2003(train_filepath, \n                            val_filepath=None,\n                            use_char=False,\n                            encoding=None,\n                            val_pct=0.1, verbose=1):\n    """"""\n    Loads sequence-labeled data from a file in CoNLL2003 format.\n    """"""\n    return entities_from_txt(train_filepath=train_filepath,\n                             val_filepath=val_filepath,\n                             use_char=use_char,\n                             data_format=\'conll2003\',\n                             encoding=encoding,\n                             val_pct=val_pct, verbose=verbose)\n\n\n\n\ndef entities_from_txt(train_filepath, \n                      val_filepath=None,\n                      use_char=False,\n                      word_column=WORD_COL,\n                      tag_column=TAG_COL,\n                      sentence_column=SENT_COL,\n                      data_format=\'conll2003\',\n                      encoding=None,\n                      val_pct=0.1, verbose=1):\n    """"""\n    Loads sequence-labeled data from comma or tab-delmited text file.\n    Format of file is either the CoNLL2003 format or Groningen Meaning\n    Bank (GMB) format - specified with data_format parameter.\n\n    In both formats, each word appars on a separate line along with\n    its associated tag (or label).  \n    The last item on each line should be the tag or label assigned to word.\n    \n    In the CoNLL2003 format, there is an empty line after\n    each sentence.  In the GMB format, sentences are deliniated\n    with a third column denoting the Sentence ID.\n\n\n    \n    More information on CoNLL2003 format: \n       https://www.aclweb.org/anthology/W03-0419\n\n    CoNLL Example (each column is typically separated by space or tab)\n    and  no column headings:\n\n       Paul\tB-PER\n       Newman\tI-PER\n       is\tO\n       a\tO\n       great\tO\n       actor\tO\n       !\tO\n\n    More information on GMB format:\n    Refer to ner_dataset.csv on Kaggle here:\n       https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/version/2\n\n    GMB example (each column separated by comma or tab)\n    with column headings:\n\n      SentenceID   Word     Tag    \n      1            Paul     B-PER\n      1            Newman   I-PER\n      1            is       O\n      1            a        O\n      1            great    O\n      1            actor    O\n      1            !        O\n    \n\n    Args:\n        train_filepath(str): file path to training CSV\n        val_filepath (str): file path to validation dataset\n        use_char(bool):    If True, data will be preprocessed to use character embeddings in addition to word embeddings\n        word_column(str): name of column containing the text\n        tag_column(str): name of column containing lael\n        sentence_column(str): name of column containing Sentence IDs\n        data_format(str): one of colnll2003 or gmb\n                          word_column, tag_column, and sentence_column\n                          ignored if \'conll2003\'\n        encoding(str): the encoding to use.  If None, encoding is discovered automatically\n        val_pct(float): Proportion of training to use for validation.\n        verbose (boolean): verbosity\n    """"""\n\n\n\n    # set dataframe converter\n    if data_format == \'gmb\':\n        data_to_df = pp.gmb_to_df\n    else:\n        data_to_df = pp.conll2003_to_df\n        word_column, tag_column, sentence_column = WORD_COL, TAG_COL, SENT_COL\n\n    # detect encoding\n    if encoding is None:\n        with open(train_filepath, \'rb\') as f:\n            encoding = TU.detect_encoding(f.read())\n            U.vprint(\'detected encoding: %s (if wrong, set manually)\' % (encoding), verbose=verbose)\n\n    # create dataframe\n    train_df = data_to_df(train_filepath, encoding=encoding)\n\n\n    val_df = None if val_filepath is None else data_to_df(val_filepath, encoding=encoding)\n    return entities_from_df(train_df,\n                            val_df=val_df,\n                            word_column=word_column,\n                            tag_column=tag_column,\n                            sentence_column=sentence_column,\n                            use_char=use_char,\n                            val_pct=val_pct, verbose=verbose)\n\n\n\ndef entities_from_df(train_df,\n                     val_df=None,\n                     word_column=WORD_COL,\n                     tag_column=TAG_COL,\n                     sentence_column=SENT_COL,\n                     use_char=False,\n                     val_pct=0.1, verbose=1):\n    """"""\n    Load entities from pandas DataFrame\n    Args:\n      train_df(pd.DataFrame): training data\n      val_df(pdf.DataFrame): validation data\n      word_column(str): name of column containing the text\n      tag_column(str): name of column containing lael\n      sentence_column(str): name of column containing Sentence IDs\n      use_char(bool):    If True, data will be preprocessed to use character embeddings  in addition to word embeddings\n      verbose (boolean): verbosity\n\n    """"""\n    # process dataframe and instantiate NERPreprocessor\n    x, y  = pp.process_df(train_df, \n                          word_column=word_column,\n                          tag_column=tag_column,\n                          sentence_column=sentence_column,\n                          verbose=verbose)\n\n    # get validation set\n    if val_df is None:\n        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=val_pct)\n    else:\n        x_train, y_train = x, y\n        (x_valid, y_valid)  = pp.process_df(val_df,\n                                            word_column=word_column,\n                                            tag_column=tag_column,\n                                            sentence_column=sentence_column,\n                                            verbose=0)\n\n    # preprocess and convert to generator\n    p = IndexTransformer(use_char=use_char)\n    preproc = NERPreprocessor(p)\n    preproc.fit(x_train, y_train)\n    trn = pp.NERSequence(x_train, y_train, batch_size=U.DEFAULT_BS, p=p)\n    val = pp.NERSequence(x_valid, y_valid, batch_size=U.DEFAULT_BS, p=p)\n\n    return ( trn, val, preproc)\n\n\n\ndef entities_from_array(x_train, y_train,\n                        x_test=None, y_test=None,\n                        use_char=False,\n                        val_pct=0.1,\n                        verbose=1):\n    """"""\n    Load entities from arrays\n    Args:\n      x_train(list): list of list of entity tokens for training\n                     Example: x_train = [[\'Hello\', \'world\'], [\'Hello\', \'Cher\'], [\'I\', \'love\', \'Chicago\']]\n      y_train(list): list of list of tokens representing entity labels\n                     Example:  y_train = [[\'O\', \'O\'], [\'O\', \'B-PER\'], [\'O\', \'O\', \'B-LOC\']]\n      x_test(list): list of list of entity tokens for validation \n                     Example: x_train = [[\'Hello\', \'world\'], [\'Hello\', \'Cher\'], [\'I\', \'love\', \'Chicago\']]\n      y_test(list): list of list of tokens representing entity labels\n                     Example:  y_train = [[\'O\', \'O\'], [\'O\', \'B-PER\'], [\'O\', \'O\', \'B-LOC\']]\n     use_char(bool):    If True, data will be preprocessed to use character embeddings  in addition to word embeddings\n     val_pct(float):  percentage of training to use for validation if no validation data is supplied\n     verbose (boolean): verbosity\n\n    """"""\n    # TODO: converting to df to use entities_from_df - needs to be refactored\n    train_df = pp.array_to_df(x_train, y_train) \n    val_df = None\n    if x_test is not None and y_test is not None:\n        val_df = pp.array_to_df(x_test, y_test)\n    if verbose:\n        print(\'training data sample:\')\n        print(train_df.head())\n        if val_df is not None:\n            print(\'validation data sample:\')\n            print(val_df.head())\n    return entities_from_df(train_df, val_df=val_df, val_pct=val_pct, \n                            use_char=use_char, verbose=verbose)\n\n\n\n'"
ktrain/text/ner/learner.py,0,"b'from ...imports import *\nfrom ... import utils as U\nfrom ...core import GenLearner\n\n\nclass NERLearner(GenLearner):\n    """"""\n    Learner for Sequence Taggers.\n    """"""\n\n\n    def __init__(self, model, train_data=None, val_data=None, \n                 batch_size=U.DEFAULT_BS, eval_batch_size=U.DEFAULT_BS,\n                 workers=1, use_multiprocessing=False,\n                 multigpu=False):\n        super().__init__(model, train_data=train_data, val_data=val_data, \n                         batch_size=batch_size, eval_batch_size=eval_batch_size,\n                         workers=workers, use_multiprocessing=use_multiprocessing, \n                         multigpu=multigpu)\n        return\n\n\n\n    def validate(self, val_data=None, print_report=True, class_names=[]):\n        """"""\n        Validate text sequence taggers\n        """"""\n        val = self._check_val(val_data)\n\n        if not val.prepare_called:\n            val.prepare()\n\n        if not U.is_ner(model=self.model, data=val):\n            warnings.warn(\'learner.validate_ner is only for sequence taggers.\')\n            return\n\n        label_true = []\n        label_pred = []\n        for i in range(len(val)):\n            x_true, y_true = val[i]\n            #lengths = self.ner_lengths(y_true)\n            lengths = val.get_lengths(i)\n            y_pred = self.model.predict_on_batch(x_true)\n\n            y_true = val.p.inverse_transform(y_true, lengths)\n            y_pred = val.p.inverse_transform(y_pred, lengths)\n\n            label_true.extend(y_true)\n            label_pred.extend(y_pred)\n\n        score = ner_f1_score(label_true, label_pred)\n        #acc = ner_accuracy_score(label_true, label_pred)\n        if print_report:\n            print(\'   F1:  {:04.2f}\'.format(score * 100))\n            #print(\'   ACC: {:04.2f}\'.format(acc * 100))\n            print(ner_classification_report(label_true, label_pred))\n\n        return score\n\n    def top_losses(self, n=4, val_data=None, preproc=None):\n        """"""\n        Computes losses on validation set sorted by examples with top losses\n        Args:\n          n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n        val = self._check_val(val_data)\n        if type(n) == type(42):\n            n = (0, n)\n\n        # get predicictions and ground truth\n        y_pred = self.predict(val_data=val)\n        y_true = self.ground_truth(val_data=val)\n\n        # compute losses and sort\n        losses = []\n        for idx, y_t in enumerate(y_true):\n            y_p = y_pred[idx]\n            #err = 1- sum(1 for x,y in zip(y_t,y_p) if x == y) / len(y_t)\n            err = sum(1 for x,y in zip(y_t,y_p) if x != y) \n            losses.append(err)\n        tups = [(i,x, y_true[i], y_pred[i]) for i, x in enumerate(losses) if x > 0]\n        tups.sort(key=operator.itemgetter(1), reverse=True)\n\n        # prune by given range\n        tups = tups[n[0]:n[1]] if n is not None else tups\n        return tups\n\n\n    def view_top_losses(self, n=4, preproc=None, val_data=None):\n        """"""\n        Views observations with top losses in validation set.\n        Args:\n         n(int or tuple): a range to select in form of int or tuple\n                          e.g., n=8 is treated as n=(0,8)\n         preproc (Preprocessor): A TextPreprocessor or ImagePreprocessor.\n                                 For some data like text data, a preprocessor\n                                 is required to undo the pre-processing\n                                 to correctly view raw data.\n          val_data:  optional val_data to use instead of self.val_data\n        Returns:\n            list of n tuples where first element is either \n            filepath or id of validation example and second element\n            is loss.\n\n        """"""\n\n        # check validation data and arguments\n        val = self._check_val(val_data)\n\n        tups = self.top_losses(n=n, val_data=val)\n\n        # get multilabel status and class names\n        classes = preproc.get_classes() if preproc is not None else None\n\n        # iterate through losses\n        for tup in tups:\n\n            # get data\n            idx = tup[0]\n            loss = tup[1]\n            truth = tup[2]\n            pred = tup[3]\n\n            seq = val.x[idx]\n            print(\'total incorrect: %s\' % (loss))\n            print(""{:15} {:5}: ({})"".format(""Word"", ""True"", ""Pred""))\n            print(""=""*30)\n            for w, true_tag, pred_tag in zip(seq, truth, pred):\n                print(""{:15}:{:5} ({})"".format(w, true_tag, pred_tag))\n            print(\'\\n\')\n        return\n\n\n    def save_model(self, fpath):\n        """"""\n        a wrapper to model.save\n        """"""\n        self._make_model_folder(fpath)\n        if U.is_crf(self.model):\n            from .anago.layers import crf_loss\n            self.model.compile(loss=crf_loss, optimizer=U.DEFAULT_OPT)\n        self.model.save(os.path.join(fpath, U.MODEL_NAME), save_format=\'h5\')\n        return\n\n\n    def predict(self, val_data=None):\n        """"""\n        Makes predictions on validation set\n        """"""\n        if val_data is not None:\n            val = val_data\n        else:\n            val = self.val_data\n        if val is None: raise Exception(\'val_data must be supplied to get_learner or predict\')\n        steps = np.ceil(U.nsamples_from_data(val)/val.batch_size)\n        results = []\n        for idx, (X, y) in enumerate(val):\n            y_pred = self.model.predict_on_batch(X)\n            lengths = val.get_lengths(idx)\n            y_pred = val.p.inverse_transform(y_pred, lengths)\n            results.extend(y_pred)\n        return results\n\n\n    def _prepare(self, data, train=True):\n        """"""\n        prepare NERSequence for training\n        """"""\n        if data is None: return None\n        mode = \'training\' if train else \'validation\'\n        if not data.prepare_called:\n            print(\'preparing %s data ...\' % (mode), end=\'\')\n            data.prepare()\n            print(\'done.\')\n        return data\n\n'"
ktrain/text/ner/models.py,0,"b'\nfrom ...imports import *\nfrom ... import utils as U\nfrom . import preprocessor as pp\nfrom .anago.models import BiLSTMCRF\n\nBILSTM_CRF = \'bilstm-crf\'\nBILSTM = \'bilstm\'\nBILSTM_ELMO = \'bilstm-elmo\'\nBILSTM_CRF_ELMO = \'bilstm-crf-elmo\'\nBILSTM_TRANSFORMER = \'bilstm-bert\'\nSEQUENCE_TAGGERS = {\n                     BILSTM: \'Bidirectional LSTM (https://arxiv.org/abs/1603.01360)\',\n                     BILSTM_TRANSFORMER: \'Bidirectional LSTM w/ BERT embeddings\',\n                     BILSTM_CRF: \'Bidirectional LSTM-CRF  (https://arxiv.org/abs/1603.01360)\',\n                     BILSTM_ELMO: \'Bidirectional LSTM w/ Elmo embeddings [English only]\',\n                     BILSTM_CRF_ELMO: \'Bidirectional LSTM-CRF w/ Elmo embeddings [English only]\',\n                     }\nV1_ONLY_MODELS = [BILSTM_CRF, BILSTM_CRF_ELMO]\nTRANSFORMER_MODELS = [BILSTM_TRANSFORMER]\nELMO_MODELS = [BILSTM_ELMO, BILSTM_CRF_ELMO]\n\ndef print_sequence_taggers():\n    for k,v in SEQUENCE_TAGGERS.items():\n        print(""%s: %s"" % (k,v))\n\n\ndef sequence_tagger(name, preproc, \n                    wv_path_or_url=None,\n                    bert_model = \'bert-base-multilingual-cased\',\n                    bert_layers_to_use = U.DEFAULT_TRANSFORMER_LAYERS, \n                    word_embedding_dim=100,\n                    char_embedding_dim=25,\n                    word_lstm_size=100,\n                    char_lstm_size=25,\n                    fc_dim=100,\n                    dropout=0.5,\n                    verbose=1):\n    """"""\n    Build and return a sequence tagger (i.e., named entity recognizer).\n\n    Args:\n        name (string): one of:\n                      - \'bilstm-crf\' for Bidirectional LSTM-CRF model\n                      - \'bilstm\' for Bidirectional LSTM (no CRF layer)\n        preproc(NERPreprocessor):  an instance of NERPreprocessor\n        wv_path_or_url(str): either a URL or file path toa fasttext word vector file (.vec or .vec.zip or .vec.gz)\n                             Example valid values for wv_path_or_url:\n\n                               Randomly-initialized word embeeddings:\n                                 set wv_path_or_url=None\n                               English pretrained word vectors:\n                                 https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n                               Chinese pretrained word vectors:\n                                 https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz\n                               Russian pretrained word vectors:\n                                 https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n                               Dutch pretrained word vectors:\n                                 https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.nl.300.vec.gz\n\n\n                             See these two Web pages for a full list of URLs to word vector files for \n                             different languages:\n                                1.  https://fasttext.cc/docs/en/english-vectors.html (for English)\n                                2.  https://fasttext.cc/docs/en/crawl-vectors.html (for non-English langages)\n\n                            Default:None (randomly-initialized word embeddings are used)\n\n        bert_model_name(str):  the name of the BERT model.  default: \'bert-base-multilingual-cased\'\n                               This parameter is only used if bilstm-bert is selected for name parameter.\n                               The value of this parameter is a name of BERT model from here:\n                                        https://huggingface.co/transformers/pretrained_models.html\n                               or a community-uploaded BERT model from here:\n                                        https://huggingface.co/models\n                               Example values:\n                                 bert-base-multilingual-cased:  Multilingual BERT (157 languages) - this is the default\n                                 bert-base-cased:  English BERT\n                                 bert-base-chinese: Chinese BERT\n                                 distilbert-base-german-cased: German DistilBert\n                                 albert-base-v2: English ALBERT model\n                                 monologg/biobert_v1.1_pubmed: community uploaded BioBERT (pretrained on PubMed)\n\n        bert_layers_to_use(list): indices of hidden layers to use.  default:[-2] # second-to-last layer\n                                  To use the concatenation of last 4 layers: use [-1, -2, -3, -4]\n        word_embedding_dim (int): word embedding dimensions.\n        char_embedding_dim (int): character embedding dimensions.\n        word_lstm_size (int): character LSTM feature extractor output dimensions.\n        char_lstm_size (int): word tagger LSTM output dimensions.\n        fc_dim (int): output fully-connected layer size.\n        dropout (float): dropout rate.\n\n        verbose (boolean): verbosity of output\n    Return:\n        model (Model): A Keras Model instance\n    """"""\n    \n    if name not in SEQUENCE_TAGGERS:\n        raise ValueError(\'invalid name: %s\' % (name))\n\n    # check BERT\n    if name in TRANSFORMER_MODELS and not bert_model:\n        raise ValueError(\'bert_model is required for bilstm-bert models\')\n    if name in TRANSFORMER_MODELS and DISABLE_V2_BEHAVIOR:\n        raise ValueError(\'BERT and other transformer models cannot be used with DISABLE_v2_BEHAVIOR\')\n\n    # check CRF\n    if not DISABLE_V2_BEHAVIOR and name in V1_ONLY_MODELS:\n        warnings.warn(\'Falling back to BiLSTM (no CRF) because DISABLE_V2_BEHAVIOR=False\')\n        msg = ""\\nIMPORTANT NOTE: ktrain uses the CRF module from keras_contrib, which is not yet\\n"" +\\\n              ""fully compatible with TensorFlow 2. You can still use the BiLSTM-CRF model\\n"" +\\\n              ""in ktrain for sequence tagging with TensorFlow 2, but you must add the\\n"" +\\\n              ""following to the top of your script or notebook BEFORE you import ktrain:\\n\\n"" +\\\n              ""import os\\n"" +\\\n              ""os.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\\n\\n"" +\\\n              ""For this run, a vanilla BiLSTM model (with no CRF layer) will be used.\\n""\n        print(msg)\n        name = BILSTM if name == BILSTM_CRF else BILSTM_ELMO\n\n    # check for use_char=True\n    if not DISABLE_V2_BEHAVIOR and preproc.p._use_char:\n        # turn off masking due to open TF2 issue ##33148: https://github.com/tensorflow/tensorflow/issues/33148\n        warnings.warn(\'Setting use_char=False:  character embeddings cannot be used in TF2 due to open TensorFlow 2 bug (#33148).\\n\' +\\\n                       \'Add os.environ[""DISABLE_V2_BEHAVIOR""] = ""1"" to the top of script if you really want to use it.\')\n        preproc.p._use_char=False\n\n    if verbose:\n        emb_names = []\n        if wv_path_or_url is not None: \n            emb_names.append(\'word embeddings initialized with fasttext word vectors (%s)\' % (os.path.basename(wv_path_or_url)))\n        else:\n            emb_names.append(\'word embeddings initialized randomly\')\n        if name in TRANSFORMER_MODELS: emb_names.append(\'BERT embeddings with \' + bert_model)\n        if name in ELMO_MODELS: emb_names.append(\'Elmo embeddings for English\')\n        if preproc.p._use_char:  emb_names.append(\'character embeddings\')\n        if len(emb_names) > 1:\n            print(\'Embedding schemes employed (combined with concatenation):\')\n        else:\n            print(\'embedding schemes employed:\')\n        for emb_name in emb_names:\n            print(\'\\t%s\' % (emb_name))\n        print()\n\n    # setup embedding\n    if wv_path_or_url is not None:\n        wv_model, word_embedding_dim = preproc.get_wv_model(wv_path_or_url, verbose=verbose)\n    else:\n        wv_model = None\n    if name == BILSTM_CRF:\n        use_crf = False if not DISABLE_V2_BEHAVIOR else True # fallback to bilstm \n    elif name == BILSTM_CRF_ELMO:\n        use_crf = False if not DISABLE_V2_BEHAVIOR else True # fallback to bilstm\n        preproc.p.activate_elmo()\n    elif name == BILSTM:\n        use_crf = False\n    elif name == BILSTM_ELMO:\n        use_crf = False\n        preproc.p.activate_elmo()\n    elif name == BILSTM_TRANSFORMER:\n        use_crf = False\n        preproc.p.activate_transformer(bert_model, layers=bert_layers_to_use, force=True)\n    else:\n        raise ValueError(\'Unsupported model name\')\n    model = BiLSTMCRF(char_embedding_dim=char_embedding_dim,\n                      word_embedding_dim=word_embedding_dim,\n                      char_lstm_size=char_lstm_size,\n                      word_lstm_size=word_lstm_size,\n                      fc_dim=fc_dim,\n                      char_vocab_size=preproc.p.char_vocab_size,\n                      word_vocab_size=preproc.p.word_vocab_size,\n                      num_labels=preproc.p.label_size,\n                      dropout=dropout,\n                      use_crf=use_crf,\n                      use_char=preproc.p._use_char,\n                      embeddings=wv_model,\n                      use_elmo=preproc.p.elmo_is_activated(),\n                      use_transformer_with_dim=preproc.p.get_transformer_dim())\n    model, loss = model.build()\n    model.compile(loss=loss, optimizer=U.DEFAULT_OPT)\n    return model\n\n'"
ktrain/text/ner/predictor.py,0,"b'from ...imports import *\nfrom ...predictor import Predictor\nfrom .preprocessor import NERPreprocessor, tokenize\nfrom ... import utils as U\nfrom .. import textutils as TU\n\nclass NERPredictor(Predictor):\n    """"""\n    predicts  classes for string-representation of sentence\n    """"""\n\n    def __init__(self, model, preproc, batch_size=U.DEFAULT_BS):\n\n        if not isinstance(model, Model):\n            raise ValueError(\'model must be of instance Model\')\n        if not isinstance(preproc, NERPreprocessor):\n        #if type(preproc).__name__ != \'NERPreprocessor\':\n            raise ValueError(\'preproc must be a NERPreprocessor object\')\n        self.model = model\n        self.preproc = preproc\n        self.c = self.preproc.get_classes()\n        self.batch_size = batch_size \n\n\n    def get_classes(self):\n        return self.c\n\n\n    def predict(self, sentence, return_proba=False, merge_tokens=False):\n        """"""\n        Makes predictions for a string-representation of a sentence\n        Args:\n          sentence(str): sentence of text\n          return_proba(bool): If return_proba is True, returns probability distribution for each token\n          merge_tokens(bool):  If True, tokens will be merged together by the entity\n                               to which they are associated:\n                               (\'Paul\', \'B-PER\'), (\'Newman\', \'I-PER\') becomes (\'Paul Newman\', \'PER\')\n\n        Returns:\n          list: list of tuples representing each token.\n        """"""\n        if not isinstance(sentence, str):\n            raise ValueError(\'Param sentence must be a string-representation of a sentence\')\n        if return_proba and merge_tokens:\n            raise ValueError(\'return_proba and merge_tokens are mutually exclusive with one another.\')\n        lang = TU.detect_lang([sentence])\n        nerseq = self.preproc.preprocess([sentence], lang=lang)\n        if not nerseq.prepare_called:\n            nerseq.prepare()\n        nerseq.batch_size = self.batch_size\n        x_true, _ = nerseq[0]\n        lengths = nerseq.get_lengths(0)\n        y_pred = self.model.predict_on_batch(x_true)\n        y_labels = self.preproc.p.inverse_transform(y_pred, lengths)\n        y_labels = y_labels[0]\n        if return_proba:\n            #probs = np.max(y_pred, axis=2)[0]\n            y_pred = y_pred[0].numpy().tolist()\n            return list(zip(nerseq.x[0], y_labels, y_pred))\n        else:\n            result =  list(zip(nerseq.x[0], y_labels))\n            if merge_tokens:\n                result = self.merge_tokens(result, lang)\n            return result\n\n\n    def merge_tokens(self, annotated_sentence, lang):\n\n        if TU.is_chinese(lang, strict=False): # strict=False: workaround for langdetect bug on short chinese texts\n            sep = \'\'\n        else:\n            sep = \' \'\n\n        current_token = """"\n        current_tag = """"\n        entities = []\n\n        for tup in annotated_sentence:\n            token = tup[0]\n            entity = tup[1]\n            tag = entity.split(\'-\')[1] if \'-\' in entity else None\n            prefix = entity.split(\'-\')[0] if \'-\' in entity else None\n            # not within entity\n            if tag is None and not current_token:\n                continue\n            # beginning of entity\n            #elif tag and prefix==\'B\':\n            elif tag and (prefix==\'B\' or prefix==\'I\' and not current_token):\n                if current_token: # consecutive entities\n                    entities.append((current_token, current_tag))\n                    current_token = """"\n                    current_tag = None\n                current_token = token\n                current_tag = tag\n            # end of entity\n            elif tag is None and current_token:\n                entities.append((current_token, current_tag))\n                current_token = """"\n                current_tag = None\n                continue\n            # within entity\n            elif tag and current_token:  #  prefix I\n                current_token = current_token + sep + token\n                current_tag = tag\n        return entities\n\n'"
ktrain/text/ner/preprocessor.py,0,"b'from ...imports import *\nfrom ... import utils as U\nfrom ...preprocessor import Preprocessor\nfrom ...data import SequenceDataset\nfrom .. import textutils as TU\nfrom .. import preprocessor as tpp\nfrom .anago.utils import filter_embeddings\n\nOTHER = \'O\'\nW2V = \'word2vec\'\nSUPPORTED_EMBEDDINGS = [W2V]\n\nWORD_COL = \'Word\'\nTAG_COL = \'Tag\'\nSENT_COL = \'SentenceID\'\n\n\n#tokenizer_filter = rs=\'!""#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\'\nre_tok = re.compile(f\'([{string.punctuation}\xe2\x80\x9c\xe2\x80\x9d\xc2\xa8\xc2\xab\xc2\xbb\xc2\xae\xc2\xb4\xc2\xb7\xc2\xba\xc2\xbd\xc2\xbe\xc2\xbf\xc2\xa1\xc2\xa7\xc2\xa3\xe2\x82\xa4\xe2\x80\x98\xe2\x80\x99])\')\ndef tokenize(s): return re_tok.sub(r\' \\1 \', s).split()\n\n\nclass NERPreprocessor(Preprocessor):\n    """"""\n    NER preprocessing base class\n    """"""\n\n    def __init__(self, p):\n        self.p = p\n        self.c = p._label_vocab._id2token\n\n    def get_preprocessor(self):\n        return self.p\n\n\n    def get_classes(self):\n        return self.c\n\n\n    def filter_embeddings(self, embeddings, vocab, dim):\n        """"""Loads word vectors in numpy array.\n\n        Args:\n            embeddings (dict or TransformerEmbedding): a dictionary of numpy array or Transformer Embedding instance\n            vocab (dict): word_index lookup table.\n\n        Returns:\n            numpy array: an array of word embeddings.\n        """"""\n        if not isinstance(embeddings, dict):\n            return\n        _embeddings = np.zeros([len(vocab), dim])\n        for word in vocab:\n            if word in embeddings:\n                word_idx = vocab[word]\n                _embeddings[word_idx] = embeddings[word]\n        return _embeddings\n\n\n\n    def get_wv_model(self, wv_path_or_url, verbose=1):\n        if wv_path_or_url is None: \n            raise ValueError(\'wordvector_path_or_url is empty: supply a file path or \'+\\\n                             \'URL to fasttext word vector file\')\n        if verbose: print(\'pretrained word embeddings will be loaded from:\\n\\t%s\' % (wv_path_or_url))\n        word_embedding_dim = 300 # all fasttext word vectors are of dim=300\n        embs = tpp.load_wv(wv_path_or_url, verbose=verbose)\n        wv_model = self.filter_embeddings(embs, self.p._word_vocab.vocab, word_embedding_dim)\n        return (wv_model, word_embedding_dim)\n\n\n\n    def preprocess(self, sentences, lang=None):\n        if type(sentences) != list:\n            raise ValueError(\'Param sentences must be a list of strings\')\n\n        # language detection\n        if lang is None: lang = TU.detect_lang(sentences)\n        X = []\n        y = []\n        for s in sentences:\n            if TU.is_chinese(lang, strict=False): # strict=False: workaround for langdetect bug on short chinese texts\n                tokenize_chinese = lambda text:[c for c in text]\n                tokens = tokenize_chinese(s)\n            else:\n                tokens = tokenize(s)\n            X.append(tokens)\n            y.append([OTHER] * len(tokens))\n        nerseq = NERSequence(X, y, p=self.p)\n        return nerseq\n\n    def preprocess_test(self, x_test, y_test, verbose=1):\n        """"""\n        Args:\n          x_test(list of lists of str): lists of token lists\n          x_test (list of lists of str):  lists of tag lists\n          verbose(bool): verbosity\n        Returns:\n          NERSequence:  can be used as argument to NERLearner.validate() to evaluate test sets\n        """"""\n        # array > df > array in order to print statistics more easily\n        from .data import array_to_df\n        test_df = array_to_df(x_test, y_test) \n        (x_list, y_list)  = process_df(test_df, verbose=verbose) \n        return NERSequence(x_list, y_list, batch_size=U.DEFAULT_BS, p=self.p)\n\n\n    def preprocess_test_from_conll2003(self, filepath, verbose=1):\n        df = conll2003_to_df(filepath)\n        (x, y)  = process_df(df)\n        return self.preprocess_test(x, y, verbose=verbose)\n\n\n    def undo(self, nerseq):\n        """"""\n        undoes preprocessing and returns raw data by:\n        converting a list or array of Word IDs back to words\n        """"""\n        return ["" "".join(e) for e in nerseq.x]\n\n\n\n\n    def fit(self, X, y):\n        """"""\n        Learn vocabulary from training set\n        """"""\n        self.p.fit(X, y)\n        return\n\n\n    def transform(self, X, y=None):\n        """"""\n        Transform documents to sequences of word IDs\n        """"""\n        return self.p.transform(X, y=y)\n\n\n\ndef array_to_df(x_list, y_list):\n    ids = []\n    words = []\n    tags = []\n    for idx, lst in enumerate(x_list):\n        length = len(lst)\n        words.extend(lst)\n        tags.extend(y_list[idx])\n        ids.extend([idx] * length)\n    return pd.DataFrame(zip(ids, words, tags), columns=[SENT_COL, WORD_COL, TAG_COL])\n\n\n\n\ndef conll2003_to_df(filepath, encoding=\'latin1\'):\n    # read data and convert to dataframe\n    sents, words, tags = [],  [], []\n    sent_id = 0\n    docstart = False\n    with open(filepath, encoding=encoding) as f:\n        for line in f:\n            line = line.rstrip()\n            if line:\n                if line.startswith(\'-DOCSTART-\'): \n                    docstart=True\n                    continue\n                else:\n                    docstart=False\n                    parts = line.split()\n                    words.append(parts[0])\n                    tags.append(parts[-1])\n                    sents.append(sent_id)\n            else:\n                if not docstart:\n                    sent_id +=1\n    df = pd.DataFrame({SENT_COL: sents, WORD_COL : words, TAG_COL:tags})\n    df = df.fillna(method=""ffill"")\n    return df\n\n\ndef gmb_to_df(filepath, encoding=\'latin1\'):\n    df = pd.read_csv(filepath, encoding=encoding)\n    df = df.fillna(method=""ffill"")\n    return df\n\n\n\ndef process_df(df, \n               sentence_column=\'SentenceID\', \n               word_column=\'Word\', \n               tag_column=\'Tag\',\n               verbose=1):\n    """"""\n    Extract words, tags, and sentences from dataframe\n    """"""\n\n\n    # get words and tags\n    words = list(set(df[word_column].values))\n    n_words = len(words)\n    tags = list(set(df[tag_column].values))\n    n_tags = len(tags)\n    if verbose:\n        print(""Number of sentences: "", len(df.groupby([sentence_column])))\n        print(""Number of words in the dataset: "", n_words)\n        print(""Tags:"", tags)\n        print(""Number of Labels: "", n_tags)\n\n\n    # retrieve all sentences\n    getter = SentenceGetter(df, word_column, tag_column, sentence_column)\n    sentences = getter.sentences\n    largest_sen = max(len(sen) for sen in sentences)\n    if verbose: print(\'Longest sentence: {} words\'.format(largest_sen))\n    data = [list(zip(*s)) for s in sentences]\n    X = [list(e[0]) for e in data]\n    y = [list(e[1]) for e in data]\n    return (X, y)\n\n\n\n\nclass SentenceGetter(object):\n    """"""Class to Get the sentence in this format:\n    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]""""""\n    def __init__(self, data, word_column, tag_column, sentence_column):\n        """"""Args:\n            data is the pandas.DataFrame which contains the above dataset""""""\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[word_column].values.tolist(),\n                                                           s[tag_column].values.tolist())]\n        self.grouped = self.data.groupby(sentence_column).apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n\n    def get_next(self):\n        """"""Return one sentence""""""\n        try:\n            s = self.grouped[""Sentence: {}"".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n\n\n\n\nclass NERSequence(SequenceDataset):\n\n    def __init__(self, x, y, batch_size=1, p=None):\n        self.x = x\n        self.y = y\n        self.batch_size = batch_size\n        self.p = p\n        self.prepare_called = False\n\n    def prepare(self):\n        if self.p is not None and not self.prepare_called:\n            self.x, self.y = self.p.fix_tokenization(self.x, self.y)\n        self.prepare_called = True\n        return\n\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n\n        return self.p.transform(batch_x, batch_y)\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n\n    def get_lengths(self, idx):\n        x_true, y_true = self[idx]\n        lengths = []\n        for y in np.argmax(y_true, -1):\n            try:\n                i = list(y).index(0)\n            except ValueError:\n                i = len(y)\n            lengths.append(i)\n\n        return lengths\n\n    def nsamples(self):\n        return len(self.x)   \n\n\n    def get_y(self):\n        return self.y\n\n\n    def xshape(self):\n        return (len(self.x), self[0][0][0].shape[1]) \n\n\n    def nclasses(self):\n        return len(self.p._label_vocab._id2token) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
ktrain/text/qa/__init__.py,0,b'from .core import SimpleQA\n'
ktrain/text/qa/core.py,0,"b'from ...imports import *\nfrom ... import utils as U\nfrom .. import textutils as TU\nfrom .. import preprocessor as tpp\n\n\nfrom whoosh import index\nfrom whoosh.fields import *\nfrom whoosh import qparser\nfrom whoosh.qparser import QueryParser\n\n\nfrom transformers import TFBertForQuestionAnswering\nfrom transformers import BertTokenizer\nLOWCONF = -10000\n\n\nclass QA(ABC):\n    """"""\n    Base class for QA\n    """"""\n\n    def __init__(self, bert_squad_model=\'bert-large-uncased-whole-word-masking-finetuned-squad\',\n                 bert_emb_model=\'bert-base-uncased\'):\n        self.model_name = bert_squad_model\n        self.model = TFBertForQuestionAnswering.from_pretrained(self.model_name)\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n        self.maxlen = 512\n        self.te = tpp.TransformerEmbedding(bert_emb_model, layers=[-2])\n\n\n    @abstractmethod\n    def search(self, query):\n        pass\n\n    def predict_squad(self, document, question):\n        input_ids = self.tokenizer.encode(question, document)\n        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n        sep_index = input_ids.index(self.tokenizer.sep_token_id)\n        num_seg_a = sep_index + 1\n        num_seg_b = len(input_ids) - num_seg_a\n        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n        assert len(segment_ids) == len(input_ids)\n        n_ids = len(segment_ids)\n        if n_ids < self.maxlen:\n            start_scores, end_scores = self.model(np.array([input_ids]), \n                                             token_type_ids=np.array([segment_ids]))\n        else:\n            #TODO: use different truncation strategies or run multiple inferences\n            start_scores, end_scores = self.model(np.array([input_ids[:self.maxlen]]), \n                                             token_type_ids=np.array([segment_ids[:self.maxlen]]))\n        start_scores = start_scores[:,1:-1]\n        end_scores = end_scores[:,1:-1]\n        answer_start = np.argmax(start_scores)\n        answer_end = np.argmax(end_scores)\n        answer = self._reconstruct_text(tokens, answer_start, answer_end+2)\n        if answer.startswith(\'. \') or answer.startswith(\', \'):\n            answer = answer[2:]  \n        sep_index = tokens.index(\'[SEP]\')\n        full_txt_tokens = tokens[sep_index+1:]\n        paragraph_bert = self._reconstruct_text(full_txt_tokens)\n\n        ans={}\n        ans[\'answer\'] = answer\n        if answer.startswith(\'[CLS]\') or answer_end < sep_index or answer.endswith(\'[SEP]\'):\n            ans[\'confidence\'] = LOWCONF\n        else:\n            #confidence = torch.max(start_scores) + torch.max(end_scores)\n            #confidence = np.log(confidence.item())\n            ans[\'confidence\'] = start_scores[0,answer_start]+end_scores[0,answer_end]\n        ans[\'start\'] = answer_start\n        ans[\'end\'] = answer_end\n        ans[\'context\'] = paragraph_bert\n        return ans\n\n\n    def _reconstruct_text(self, tokens, start=0, stop=-1):\n        tokens = tokens[start: stop]\n        if \'[SEP]\' in tokens:\n            sepind = tokens.index(\'[SEP]\')\n            tokens = tokens[sepind+1:]\n        txt = \' \'.join(tokens)\n        txt = txt.replace(\' ##\', \'\')\n        txt = txt.replace(\'##\', \'\')\n        txt = txt.strip()\n        txt = "" "".join(txt.split())\n        txt = txt.replace(\' .\', \'.\')\n        txt = txt.replace(\'( \', \'(\')\n        txt = txt.replace(\' )\', \')\')\n        txt = txt.replace(\' - \', \'-\')\n        txt_list = txt.split(\' , \')\n        txt = \'\'\n        length = len(txt_list)\n        if length == 1:\n            return txt_list[0]\n        new_list =[]\n        for i,t in enumerate(txt_list):\n            if i < length -1:\n                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n                    new_list += [t,\',\']\n                else:\n                    new_list += [t, \', \']\n            else:\n                new_list += [t]\n        return \'\'.join(new_list)\n\n\n\nclass SimpleQA(QA):\n    """"""\n    SimpleQA: Question-Answering on a list of texts\n    """"""\n    def __init__(self, index_dir, \n                 bert_squad_model=\'bert-large-uncased-whole-word-masking-finetuned-squad\',\n                 bert_emb_model=\'bert-base-uncased\'):\n        """"""\n        SimpleQA constructor\n        Args:\n          index_dir(str):  path to index directory created by SimpleQA.initialze_index\n          bert_squad_model(str): name of BERT SQUAD model to use\n          bert_emb_model(str): BERT model to use to generate embeddings for semantic similarity\n\n        """"""\n\n        self.index_dir = index_dir\n        try:\n            ix = index.open_dir(self.index_dir)\n        except:\n            raise ValueError(\'index_dir has not yet been created - please call SimpleQA.initialize_index(""%s"")\' % (self.index_dir))\n        super().__init__(bert_squad_model=bert_squad_model, bert_emb_model=bert_emb_model)\n\n\n    def _open_ix(self):\n        return index.open_dir(self.index_dir)\n\n\n    @classmethod\n    def initialize_index(cls, index_dir):\n        schema = Schema(reference=ID(stored=True), content=TEXT, rawtext=TEXT(stored=True))\n        if not os.path.exists(index_dir):\n            os.makedirs(index_dir)\n        else:\n            raise ValueError(\'There is already an existing directory or file with path %s\' % (index_dir))\n        ix = index.create_in(index_dir, schema)\n        return ix\n\n    @classmethod\n    def index_from_list(cls, docs, index_dir, commit_every=1024,\n                        procs=1, limitmb=256, multisegment=False):\n        """"""\n        index documents from list.\n        The procs, limitmb, and especially multisegment arguments can be used to \n        speed up indexing, if it is too slow.  Please see the whoosh documentation\n        for more information on these parameters:  https://whoosh.readthedocs.io/en/latest/batch.html\n        Args:\n          docs(list): list of strings representing documents\n          commit_every(int): commet after adding this many documents\n          procs(int): number of processors\n          limitmb(int): memory limit in MB for each process\n          multisegment(bool): new segments written instead of merging\n        """"""\n        if not isinstance(docs, (np.ndarray, list)): raise ValueError(\'docs must be a list of strings\')\n        ix = index.open_dir(index_dir)\n        writer = ix.writer(procs=procs, limitmb=limitmb, multisegment=multisegment)\n        mb = master_bar(range(1))\n        for i in mb:\n            for idx, doc in enumerate(progress_bar(docs, parent=mb)):\n                reference = ""%s"" % (idx)\n                content = doc \n                writer.add_document(reference=reference, content=content, rawtext=content)\n                idx +=1\n                if idx % commit_every == 0:\n                    writer.commit()\n                    #writer = ix.writer()\n                    writer = ix.writer(procs=procs, limitmb=limitmb, multisegment=multisegment)\n            writer.commit()\n        return\n\n\n    @classmethod\n    def index_from_folder(cls, folder_path, index_dir,  commit_every=1024, verbose=1, encoding=\'utf-8\',\n                          procs=1, limitmb=256, multisegment=False):\n        """"""\n        index all plain text documents within a folder.\n        The procs, limitmb, and especially multisegment arguments can be used to \n        speed up indexing, if it is too slow.  Please see the whoosh documentation\n        for more information on these parameters:  https://whoosh.readthedocs.io/en/latest/batch.html\n\n        Args:\n          folder_path(str): path to folder containing plain text documents\n          commit_every(int): commet after adding this many documents\n          procs(int): number of processors\n          limitmb(int): memory limit in MB for each process\n          multisegment(bool): new segments written instead of merging\n\n        """"""\n        if not os.path.isdir(folder_path): raise ValueError(\'folder_path is not a valid folder\')\n        if folder_path[-1] != os.sep: folder_path += os.sep\n        ix = index.open_dir(index_dir)\n        writer = ix.writer(procs=procs, limitmb=limitmb, multisegment=multisegment)\n        for idx, fpath in enumerate(TU.extract_filenames(folder_path)):\n            if not TU.is_txt(fpath): continue\n            reference = ""%s"" % (fpath.join(fpath.split(folder_path)[1:]))\n            with open(fpath, \'r\', encoding=encoding) as f:\n                doc = f.read()\n            content = doc\n            writer.add_document(reference=reference, content=content, rawtext=content)\n            idx +=1\n            if idx % commit_every == 0:\n                writer.commit()\n                #writer = ix.writer()\n                writer = ix.writer(procs=procs, limitmb=limitmb, multisegment=multisegment)\n                if verbose: print(""%s docs indexed"" % (idx))\n        writer.commit()\n        return\n\n\n    def search(self, query, limit=10):\n        """"""\n        search index for query\n        Args:\n          query(str): search query\n          limit(int):  number of top search results to return\n        Returns:\n          list of dicts with keys: reference, rawtext\n        """"""\n        ix = self._open_ix()\n        with ix.searcher() as searcher:\n            query_obj = QueryParser(""content"", ix.schema, group=qparser.OrGroup).parse(query)\n            results = searcher.search(query_obj, limit=limit)\n            docs = []\n            output = [dict(r) for r in results]\n            return output\n\n\n    def _expand_answer(self, answer):\n        """"""\n        expand answer to include more of the context\n        """"""\n        full_abs = answer[\'context\']\n        bert_ans = answer[\'answer\']\n        split_abs = full_abs.split(bert_ans)\n        sent_beginning = split_abs[0][split_abs[0].rfind(\'.\')+1:]\n        if len(split_abs) == 1:\n            sent_end_pos = len(full_abs)\n            sent_end =\'\'\n        else:\n            sent_end_pos = split_abs[1].find(\'. \')+1\n            if sent_end_pos == 0:\n                sent_end = split_abs[1]\n            else:\n                sent_end = split_abs[1][:sent_end_pos]\n            \n        answer[\'full_answer\'] = sent_beginning+bert_ans+sent_end\n        answer[\'full_answer\'] = answer[\'full_answer\'].strip()\n        answer[\'sentence_beginning\'] = sent_beginning\n        answer[\'sentence_end\'] = sent_end\n        return answer\n\n\n\n    def ask(self, question, n_docs_considered=10, n_answers=50, rerank_threshold=0.015):\n        """"""\n        submit question to obtain candidate answers\n\n        Args:\n          question(str): question in the form of a string\n          n_docs_considered(int): number of top search results that will\n                                  be searched for answer\n                                  default:10\n          n_answers(int): maximum number of candidate answers to return\n                          default:50\n          rerank_threshold(int): rerank top answers with confidence >= rerank_threshold\n                                 based on semantic similarity between question and answer.\n                                 This can help bump the correct answer closer to the top.\n                                 default:0.015.\n                                 If None, no re-ranking is performed.\n        Returns:\n          list\n        """"""\n        # locate candidate document contexts\n        doc_results = self.search(question, limit=n_docs_considered)\n        paragraphs = []\n        refs = []\n        for doc_result in doc_results:\n            rawtext = doc_result.get(\'rawtext\', \'\')\n            reference = doc_result.get(\'reference\', \'\')\n            if len(self.tokenizer.tokenize(rawtext)) < self.maxlen:\n                paragraphs.append(rawtext)\n                refs.append(reference)\n                continue\n            plist = TU.paragraph_tokenize(rawtext, join_sentences=True)\n            paragraphs.extend(plist)\n            refs.extend([reference]*len(plist))\n\n        # locate candidate answers\n        answers = []\n        mb = master_bar(range(1))\n        for i in mb:\n            for idx, paragraph in enumerate(progress_bar(paragraphs, parent=mb)):\n                answer = self.predict_squad(paragraph, question)\n                if not answer[\'answer\'] or answer[\'confidence\'] <0: continue\n                answer[\'confidence\'] = answer[\'confidence\'].numpy()\n                answer[\'reference\'] = refs[idx]\n                answer = self._expand_answer(answer)\n                answers.append(answer)\n        answers = sorted(answers, key = lambda k:k[\'confidence\'], reverse=True)\n        if n_answers is not None:\n            answers = answers[:n_answers]\n\n        # transform confidence scores\n        confidences = [a[\'confidence\'] for a in answers]\n        max_conf = max(confidences)\n        total = 0.0\n        exp_scores = []\n        for c in confidences:\n            s = np.exp(c-max_conf)\n            exp_scores.append(s)\n        total = sum(exp_scores)\n        for idx,c in enumerate(confidences):\n            answers[idx][\'confidence\'] = exp_scores[idx]/total\n\n        if rerank_threshold is None:\n            return answers\n\n        # re-rank\n        top_confidences = [a[\'confidence\'] for idx, a in enumerate(answers) if a[\'confidence\']> rerank_threshold]\n        v1 = self.te.embed(question, word_level=False)\n        for idx, answer in enumerate(answers):\n            #if idx >= rerank_top_n: \n            if answer[\'confidence\'] <= rerank_threshold:\n                answer[\'similarity_score\'] = 0.0\n                continue\n            v2 = self.te.embed(answer[\'full_answer\'], word_level=False)\n            score = v1 @ v2.T / (np.linalg.norm(v1)*np.linalg.norm(v2))\n            answer[\'similarity_score\'] = float(np.squeeze(score))\n            answer[\'confidence\'] = top_confidences[idx]\n        answers = sorted(answers, key = lambda k:(k[\'similarity_score\'], k[\'confidence\']), reverse=True)\n        for idx, confidence in enumerate(top_confidences):\n            answers[idx][\'confidence\'] = confidence\n\n\n        return answers\n\n\n    def answers2df(self, answers):\n        dfdata = []\n        for a in answers:\n            answer_text = a[\'answer\']\n            snippet_html = \'<div>\' +a[\'sentence_beginning\'] + "" <font color=\'red\'>""+a[\'answer\']+""</font> ""+a[\'sentence_end\']+\'</div>\'\n            confidence = a[\'confidence\']\n            doc_key = a[\'reference\']\n            dfdata.append([answer_text, snippet_html, confidence, doc_key])\n        df = pd.DataFrame(dfdata, columns = [\'Candidate Answer\', \'Context\',  \'Confidence\', \'Document Reference\'])\n        return df\n\n\n    def display_answers(self, answers):\n        df = self.answers2df(answers)\n        from IPython.core.display import display, HTML\n        display(HTML(df.to_html(render_links=True, escape=False)))\n\n\n'"
ktrain/text/shallownlp/__init__.py,0,"b""from .classifier import Classifier\nfrom .searcher import *\nfrom .ner import NER\nfrom .utils import sent_tokenize, extract_filenames, read_text\n\n\n__all__ = ['Classifier', \n           'Searcher', 'search', 'find_chinese', 'find_arabic', 'find_russian', 'read_text',\n           'NER',\n           'sent_tokenize', 'extract_filenames', 'read_text']\n"""
ktrain/text/shallownlp/classifier.py,0,"b'from .imports import *\nfrom . import utils as U\n\n\n\n__all__ = [\'NBSVM\']\n\n\nclass Classifier:\n    def __init__(self, model=None):\n        """"""\n        instantiate a classifier with an optional previously-saved model\n        """"""\n        self.model = None\n\n\n    def create_model(self, ctype, texts, hp_dict={}, ngram_range=(1,3), binary=True):\n        """"""\n        create a model\n        Args:\n          ctype(str): one of {\'nbsvm\', \'logreg\', \'sgdclassifier\'}\n          texts(list): list of texts\n          hp_dict(dict): dictionary of hyperparameters to use for the ctype selected.\n                         hp_dict can also be used to supply arguments to CountVectorizer\n          ngram_range(tuple): default ngram_range.\n                              overridden if \'ngram_range\' in hp_dict\n          binary(bool): default value for binary argument to CountVectorizer.\n                        overridden if \'binary\' key in hp_dict\n\n        """"""\n        lang = U.detect_lang(texts)\n        if U.is_chinese(lang):\n            token_pattern = r\'(?u)\\b\\w+\\b\'\n        else:\n            token_pattern = r\'\\w+|[%s]\' % string.punctuation\n        if ctype == \'nbsvm\':\n            clf = NBSVM(C=hp_dict.get(\'C\', 0.01), \n                        alpha=hp_dict.get(\'alpha\', 0.75), \n                        beta=hp_dict.get(\'beta\', 0.25), \n                        fit_intercept=hp_dict.get(\'fit_intercept\', False))\n        elif ctype==\'logreg\':\n            clf = LogisticRegression(C=hp_dict.get(\'C\', 0.1), \n                                     dual=hp_dict.get(\'dual\', True),\n                                     penalty=hp_dict.get(\'penalty\', \'l2\'),\n                                     tol=hp_dict.get(\'tol\', 1e-4),\n                                     intercept_scaling=hp_dict.get(\'intercept_scaling\', 1),\n                                     solver=hp_dict.get(\'solver\', \'liblinear\'),\n                                     max_iter=hp_dict.get(\'max_iter\', 100),\n                                     multi_class=hp_dict.get(\'multi_class\', \'auto\'),\n                                     warm_start=hp_dict.get(\'warm_start\', False),\n                                     n_jobs=hp_dict.get(\'n_jobs\', None),\n                                     l1_ratio=hp_dict.get(\'l1_ratio\', None),\n                                     random_state=hp_dict.get(\'random_state\', 42),\n                                     class_weight=hp_dict.get(\'class_weight\', None)\n                                     )\n        elif ctype == \'sgdclassifier\':\n            clf = SGDClassifier(loss=hp_dict.get(\'loss\', \'hinge\'), \n                                penalty=hp_dict.get(\'penalty\', \'l2\'), \n                                alpha=hp_dict.get(\'alpha\', 1e-3), \n                                random_state=hp_dict.get(\'random_state\', 42), \n                                max_iter=hp_dict.get(\'max_iter\', 5),  # scikit-learn default is 1000\n                                tol=hp_dict.get(\'tol\', None),\n                                l1_ratio=hp_dict.get(\'l1_ratio\', 0.15),\n                                fit_intercept=hp_dict.get(\'fit_intercept\', True),\n                                episilon=hp_dict.get(\'epsilon\', 0.1),\n                                n_jobs=hp_dict.get(\'n_jobs\', None),\n                                learning_rate=hp_dict.get(\'learning_rate\', \'optimal\'),\n                                eta0=hp_dict.get(\'eta0\', 0.0),\n                                power_t=hp_dict.get(\'power_t\', 0.5),\n                                early_stopping=hp_dict.get(\'early_stopping\', False),\n                                validation_fraction=hp_dict.get(\'validation_fraction\', 0.1),\n                                n_iter_no_change=hp_dict.get(\'n_iter_no_change\', 5),\n                                warm_start=hp_dict.get(\'warm_start\', False),\n                                average=hp_dict.get(\'average\', False),\n                                class_weight=hp_dict.get(\'class_weight\', None))\n        else:\n            raise ValueError(\'Unknown ctype: %s\' % (ctype))\n\n        self.model = Pipeline([ (\'vect\', CountVectorizer(ngram_range=hp_dict.get(\'ngram_range\', ngram_range), \n                                                         binary=hp_dict.get(\'binary\', binary), \n                                                         token_pattern=token_pattern,\n                                                         max_features=hp_dict.get(\'max_features\', None),\n                                                         max_df=hp_dict.get(\'max_df\', 1.0),\n                                                         min_df=hp_dict.get(\'min_df\', 1),\n                                                         stop_words=hp_dict.get(\'stop_words\', None),\n                                                         lowercase=hp_dict.get(\'lowercase\', True),\n                                                         strip_accents=hp_dict.get(\'strip_accents\', None),\n                                                         encoding=hp_dict.get(\'encoding\', \'utf-8\')\n                                                         )),\n                              (\'clf\', clf) ])\n        return\n\n\n    @classmethod\n    def load_texts_from_folder(cls, folder_path, \n                              subfolders=None, \n                              shuffle=True,\n                              encoding=None):\n        """"""\n        load text files from folder\n\n        Args:\n          folder_path(str): path to folder containing documents\n                            The supplied folder should contain a subfolder\n                            for each category, which will be used as the class label\n          subfolders(list): list of subfolders under folder_path to consider\n                            Example: If folder_path contains subfolders pos, neg, and \n                            unlabeled, then unlabeled folder can be ignored by\n                            setting subfolders=[\'pos\', \'neg\']\n          shuffle(bool):  If True, list of texts will be shuffled\n          encoding(str): encoding to use.  default:None (auto-detected)\n        Returns:\n          tuple: (texts, labels, label_names)\n        """"""\n        bunch = load_files(folder_path, categories=subfolders, shuffle=shuffle)\n        texts = bunch.data\n        labels = bunch.target\n        label_names = bunch.target_names\n        #print(\'target names:\')\n        #for idx, label_name in enumerate(bunch.target_names):\n            #print(\'\\t%s:%s\' % (idx, label_name))\n\n        # decode based on supplied encoding\n        if encoding is None:\n            encoding = U.detect_encoding(texts)\n            if encoding != \'utf-8\':\n                print(\'detected encoding: %s\' % (encoding))\n\n        try:\n            texts = [text.decode(encoding) for text in texts]\n        except:\n            print(\'Decoding with %s failed 1st attempt - using %s with skips\' % (encoding,\n                                                                                 encoding))\n            texts = U.decode_by_line(texts, encoding=encoding)\n        return (texts, labels, label_names)\n\n\n\n    @classmethod\n    def load_texts_from_csv(cls, csv_filepath, text_column=\'text\', label_column=\'label\',\n                            sep=\',\', encoding=None):\n        """"""\n        load text files from csv file\n        CSV should have at least two columns.\n        Example:\n        Text               | Label\n        I love this movie. | positive\n        I hated this movie.| negative\n\n\n        Args:\n          csv_filepath(str): path to CSV file\n          text_column(str): name of column containing the texts. default:\'text\'\n          label_column(str): name of column containing the labels in string format\n                             default:\'label\'\n          sep(str): character that separates columns in CSV. default:\',\'\n          encoding(str): encoding to use. default:None (auto-detected)\n        Returns:\n          tuple: (texts, labels, label_names)\n        """"""\n        if encoding is None:\n            with open(csv_filepath, \'rb\') as f:\n                encoding = U.detect_encoding([f.read()])\n                if encoding != \'utf-8\':\n                    print(\'detected encoding: %s (if wrong, set manually)\' % (encoding))\n        import pandas as pd\n        df = pd.read_csv(csv_filepath, encoding=encoding, sep=sep)\n        texts = df[text_column].fillna(\'fillna\').values\n        labels = df[label_column].values\n        le = LabelEncoder()\n        le.fit(labels)\n        labels = le.transform(labels)\n        return (texts, labels, le.classes_)\n\n\n    def fit(self, x_train, y_train, ctype=\'logreg\'):\n        """"""\n        train a classifier\n        Args:\n          x_train(list or np.ndarray):  training texts\n          y_train(np.ndarray):  training labels\n          ctype(str):  One of {\'logreg\', \'nbsvm\', \'sgdclassifier\'}.  default:nbsvm\n        """"""\n        lang = U.detect_lang(x_train)\n        if U.is_chinese(lang):\n            x_train = U.split_chinese(x_train)\n        if self.model is None:\n            self.create_model(ctype, x_train)\n        self.model.fit(x_train, y_train)\n        return self\n\n\n\n    def predict(self, x_test, return_proba=False):\n        """"""\n        make predictions on text data\n        Args:\n          x_test(list or np.ndarray or str): array of texts on which to make predictions or a string representing text\n        """"""\n        if return_proba and not hasattr(self.model[\'clf\'], \'predict_proba\'): \n            raise ValueError(\'%s does not support predict_proba\' % (type(self.model[\'clf\']).__name__))\n        if isinstance(x_test, str): x_test = [x_test]\n        lang = U.detect_lang(x_test)\n        if U.is_chinese(lang): x_test = U.split_chinese(x_test)\n        if self.model is None: raise ValueError(\'model is None - call fit or load to set the model\')\n        if return_proba:\n            predicted = self.model.predict_proba(x_test)\n        else:\n            predicted = self.model.predict(x_test)\n        if len(predicted) == 1: predicted = predicted[0]\n        return predicted\n\n\n    def predict_proba(self, x_test):\n        """"""\n        predict_proba\n        """"""\n        return self.predict(x_test, return_proba=True)\n\n\n    def evaluate(self, x_test, y_test):\n        """"""\n        evaluate\n        Args:\n          x_test(list or np.ndarray):  training texts\n          y_test(np.ndarray):  training labels\n        """"""\n        predicted = self.predict(x_test)\n        return np.mean(predicted == y_test)\n\n\n    def save(self, filename):\n        """"""\n        save model\n        """"""\n        dump(self.model, filename)\n\n\n    def load(self, filename):\n        """"""\n        load model\n        """"""\n        self.model = load(filename)\n\n    def grid_search(self, params, x_train, y_train, n_jobs=-1):\n        """"""\n        Performs grid search to find optimal set of hyperparameters\n        Args:\n          params (dict):  A dictionary defining the space of the search.\n                          Example for finding optimal value of alpha in NBSVM:\n                        parameters = {\n                                      #\'clf__C\': (1e0, 1e-1, 1e-2),\n                                      \'clf__alpha\': (0.1, 0.2, 0.4, 0.5, 0.75, 0.9, 1.0),\n                                      #\'clf__fit_intercept\': (True, False),\n                                      #\'clf__beta\' : (0.1, 0.25, 0.5, 0.9) \n                                      }\n          n_jobs(int): number of jobs to run in parallel.  default:-1 (use all processors)\n        """"""\n        gs_clf = GridSearchCV(self.model, params, n_jobs=n_jobs)\n        gs_clf = gs_clf.fit(x_train, y_train)\n\t#gs_clf.best_score_                                  \n        for param_name in sorted(params.keys()):\n            print(""%s: %r"" % (param_name, gs_clf.best_params_[param_name]))\n        return\n\n\n\n\n\nclass NBSVM(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n\n    def __init__(self, alpha=1, C=1, beta=0.25, fit_intercept=False):\n        self.alpha = alpha\n        self.C = C\n        self.beta = beta\n        self.fit_intercept = fit_intercept\n\n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        if len(self.classes_) == 2:\n            coef_, intercept_ = self._fit_binary(X, y)\n            self.coef_ = coef_\n            self.intercept_ = intercept_\n        else:\n            coef_, intercept_ = zip(*[\n                self._fit_binary(X, y == class_)\n                for class_ in self.classes_\n            ])\n            self.coef_ = np.concatenate(coef_)\n            self.intercept_ = np.array(intercept_).flatten()\n        return self\n\n    def _fit_binary(self, X, y):\n        p = np.asarray(self.alpha + X[y == 1].sum(axis=0)).flatten()\n        q = np.asarray(self.alpha + X[y == 0].sum(axis=0)).flatten()\n        r = np.log(p/np.abs(p).sum()) - np.log(q/np.abs(q).sum())\n        b = np.log((y == 1).sum()) - np.log((y == 0).sum())\n\n        if isinstance(X, spmatrix):\n            indices = np.arange(len(r))\n            r_sparse = coo_matrix(\n                (r, (indices, indices)),\n                shape=(len(r), len(r))\n            )\n            X_scaled = X * r_sparse\n        else:\n            X_scaled = X * r\n\n        lsvc = LinearSVC(\n            C=self.C,\n            fit_intercept=self.fit_intercept,\n            max_iter=10000\n        ).fit(X_scaled, y)\n\n        mean_mag =  np.abs(lsvc.coef_).mean()\n\n        coef_ = (1 - self.beta) * mean_mag * r + \\\n                self.beta * (r * lsvc.coef_)\n\n        intercept_ = (1 - self.beta) * mean_mag * b + \\\n                     self.beta * lsvc.intercept_\n\n        return coef_, intercept_\n\n'"
ktrain/text/shallownlp/imports.py,1,"b'import os, logging, warnings\n#os.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\n\nfrom ...imports import SUPPRESS_TF_WARNINGS\nif SUPPRESS_TF_WARNINGS:\n    os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n    logging.getLogger(""tensorflow"").setLevel(logging.CRITICAL)\n    logging.getLogger(""tensorflow_hub"").setLevel(logging.CRITICAL)\n    warnings.simplefilter(action=\'ignore\', category=FutureWarning)\n\ntry:\n    import tensorflow as tf\n    TF_INSTALLED = True\nexcept ImportError:\n    TF_INSTALLED = False\nif TF_INSTALLED:\n    tf.autograph.set_verbosity(1)\n\n\n\nimport re\nimport string\nimport os.path\nimport numpy as np\nfrom scipy.sparse import spmatrix, coo_matrix\nfrom sklearn.base import BaseEstimator\nfrom sklearn.linear_model.base import LinearClassifierMixin, SparseCoefMixin\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import load_files\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom joblib import dump, load\nimport syntok.segmenter as segmenter\n\n# ktrain imported locally in ner.py\n#import ktrain \n\n# pandas imported locally in classifier.py\n#import pandas as pd\n\ntry:\n    import langdetect\n    LANGDETECT=True\nexcept:\n    LANGDETECT=False\ntry:\n    import cchardet as chardet\n    CHARDET=True\nexcept:\n    CHARDET=False\ntry:\n    import jieba\n    JIEBA=True\nexcept:\n    JIEBA=False\n'"
ktrain/text/shallownlp/ner.py,1,"b'from .imports import *\n\n\nclass NER:\n    def __init__(self, lang=\'en\', predictor_path=None):\n        """"""\n        pretrained NER.\n        Only English and Chinese are currenty supported.\n\n        Args:\n          lang(str): Currently, one of {\'en\', \'zh\', \'ru\'}: en=English , zh=Chinese, or ru=Russian\n        """"""\n        if lang is None:\n            raise ValueError(\'lang is required (e.g., ""en"" for English, ""zh"" for Chinese, ""ru"" for Russian, etc.\')\n        if predictor_path is None and lang not in [\'en\', \'zh\', \'ru\']:\n            raise ValueError(""Unsupported language: if predictor_path is None,  then lang must be "" +\\\n                             ""\'en\' for English, \'zh\' for Chinese, or \'ru\' for Chinese"")\n        self.lang = lang\n        if os.environ.get(\'DISABLE_V2_BEHAVIOR\', None) != \'1\':\n            warnings.warn(""Please add os.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\' at top of your script or notebook"")\n            msg = ""\\nNER in ktrain uses the CRF module from keras_contrib, which is not yet\\n"" +\\\n                    ""fully compatible with TensorFlow 2. To use NER, you must add the following to the top of your\\n"" +\\\n                    ""script or notebook BEFORE you import ktrain (after restarting runtime):\\n\\n"" +\\\n                  ""import os\\n"" +\\\n                  ""os.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\\n""\n            print(msg)\n            return\n        else:\n            import tensorflow.compat.v1 as tf\n            tf.disable_v2_behavior()\n\n        if predictor_path is None and self.lang == \'zh\':\n            dirpath = os.path.dirname(os.path.abspath(__file__))\n            fpath = os.path.join(dirpath, \'ner_models/ner_chinese\')\n        elif predictor_path is None and self.lang == \'ru\':\n            dirpath = os.path.dirname(os.path.abspath(__file__))\n            fpath = os.path.join(dirpath, \'ner_models/ner_russian\')\n        elif predictor_path is None and self.lang==\'en\':\n            dirpath = os.path.dirname(os.path.abspath(__file__))\n            fpath = os.path.join(dirpath, \'ner_models/ner_english\')\n        elif predictor_path is None:\n            raise ValueError(""Unsupported language: if predictor_path is None,  then lang must be "" +\\\n                             ""\'en\' for English, \'zh\' for Chinese, or \'ru\' for Chinese"")\n        else:\n            if not os.path.isfile(predictor_path) or not os.path.isfile(predictor_path +\'.preproc\'):\n                raise ValueError(\'could not find a valid predictor model \'+\\\n                                 \'%s or valid Preprocessor %s at specified path\' % (predictor_path, predictor_path+\'.preproc\'))\n            fpath = predictor_path\n        try:\n           import io\n           from contextlib import redirect_stdout\n           f = io.StringIO()\n           with redirect_stdout(f):\n               import ktrain\n        except:\n           raise ValueError(\'ktrain could not be imported. Install with: pip3 install ktrain\')\n        self.predictor = ktrain.load_predictor(fpath)\n\n\n    def predict(self, texts, merge_tokens=True):\n        """"""\n        Extract named entities from supplied text\n\n        Args:\n          texts (list of str or str): list of texts to annotate\n          merge_tokens(bool):  If True, tokens will be merged together by the entity\n                               to which they are associated:\n                               (\'Paul\', \'B-PER\'), (\'Newman\', \'I-PER\') becomes (\'Paul Newman\', \'PER\')\n        """"""\n        if isinstance(texts, str): texts = [texts]\n        results = []\n        for text in texts:\n            text = text.strip()\n            result = self.predictor.predict(text, merge_tokens=merge_tokens)\n            #if merge_tokens:\n                #result = self.merge_tokens(result)\n            results.append(result)\n        if len(result) == 1: result = result[0]\n        return result\n\n\n    # 2020-04-30: moved to text.ner.predictor\n    #def merge_tokens(self, annotated_sentence):\n    #    if self.lang.startswith(\'zh\'):\n    #        sep = \'\'\n    #    else:\n    #        sep = \' \'\n    #    current_token = """"\n    #    current_tag = """"\n    #    entities = []\n\n    #    for tup in annotated_sentence:\n    #        token = tup[0]\n    #        entity = tup[1]\n    #        tag = entity.split(\'-\')[1] if \'-\' in entity else None\n    #        prefix = entity.split(\'-\')[0] if \'-\' in entity else None\n    #        # not within entity\n    #        if tag is None and not current_token:\n    #            continue\n    #        # beginning of entity\n    #        #elif tag and prefix==\'B\':\n    #        elif tag and (prefix==\'B\' or prefix==\'I\' and not current_token):\n    #            if current_token: # consecutive entities\n    #                entities.append((current_token, current_tag))\n    #                current_token = """"\n    #                current_tag = None\n    #            current_token = token\n    #            current_tag = tag\n    #        # end of entity\n    #        elif tag is None and current_token:\n    #            entities.append((current_token, current_tag))\n    #            current_token = """"\n    #            current_tag = None\n    #            continue\n    #        # within entity\n    #        elif tag and current_token:  #  prefix I\n    #            current_token = current_token + sep + token\n    #            current_tag = tag\n    #    return entities  \n\n'"
ktrain/text/shallownlp/searcher.py,0,"b'from .imports import *\nfrom . import utils as U\n\n\n\ndef search(query, doc, case_sensitive=False, keys=[], progress=False):\n    searcher = Searcher(query)\n    return searcher.search(doc, case_sensitive=case_sensitive, keys=keys, progress=progress)\n\n\nclass Searcher:\n    """"""\n    Search for keywords in text documents\n    """"""\n    def __init__(self, queries, lang=None):\n        """"""\n        Args:\n          queries(list of str): list of chinese text queries\n          lang(str): language of queries.  default:None --> auto-detected\n        """"""\n        self.queries = queries\n        if isinstance(self.queries, str): self.queries = [self.queries]\n        self.lang = lang\n        if self.lang is None:\n            self.lang = U.detect_lang(queries)\n        #print(""lang:%s"" %(self.lang))\n\n\n\n    def search(self, docs, case_sensitive=False, keys=[], min_matches=1, progress=True):\n        """"""\n        executes self.queries on supplied list of documents\n        Args:\n          docs(list of str): list of chinese texts\n          case_sensitive(bool):  If True, case sensitive search\n          keys(list): list keys for supplied docs (e.g., file paths).\n                      default: key is index in range(len(docs))\n          min_matches(int): results must have at least these many word matches\n          progress(bool): whether or not to show progress bar\n        Returns:\n          list of tuples of results of the form:\n            (key, query, no. of matches)\n          For Chinese, no. of matches will be number of unique Jieba-extracted character sequences that match\n\n        """"""\n        if isinstance(docs, str): \n            docs = [docs]\n        if keys and len(keys) != len(docs):\n            raise ValueError(\'lengths of keys and docs must be the same\')\n        results = []\n        l = len(docs)\n        for idx, text in enumerate(docs):\n            for q in self.queries:\n                if U.is_chinese(self.lang):\n                    r = self._search_chinese(q, [text], min_matches=min_matches, parse=1, progress=False)\n                elif self.lang == \'ar\':\n                    r = self._search(q, [text], case_sensitive=case_sensitive, min_matches=min_matches, \n                                     progress=False, substrings_on=True)\n                else:\n                    r = self._search(q, [text], case_sensitive=case_sensitive, min_matches=min_matches, \n                                     progress=False, substrings_on=False)\n                if not r: continue\n                r = r[0]\n                k = idx\n                if keys: k = keys[idx]\n                num_matches = len(set(r[2])) if U.is_chinese(self.lang) else len(r[2])\n                results.append((k, q, num_matches))\n            if progress: printProgressBar(idx+1, l, prefix=\'progress: \', suffix =\'complete\', length=50)\n        return results\n\n\n    def _search(self, query, docs,\n                case_sensitive=False, substrings_on=False, \n                min_matches=1, progress=True):\n        """"""\n        search documents for query string.\n        Args:\n            query(str or list):  the word or phrase to search (or list of them)\n                                 if list is provided, each element is combined using OR\n            docs (list of str): list of text documents\n            case_sensitive(bool):  If True, case sensitive search\n            substrings_on(bool): whether to use ""\\b"" in regex. default:True\n                                 If True, will find substrings\n        returns:\n            list or tuple:  Returns list of results if len(docs) > 1.  Otherwise, returns tuple of results\n        """"""\n        if not isinstance(query, (list, tuple, str)): raise ValueError(\'query must be str or list of str\')\n        if isinstance(query, str): query = [query]\n        if not isinstance(docs, (list, np.ndarray)): raise ValueError(\'docs must be list of str\')\n\n        flag = 0\n        if not case_sensitive:\n            flag = re.I\n        qlist =[]\n        for q in query:\n            qlist.append(\'\\s+\'.join(q.split()))\n        original_query = query\n        query = \'|\'.join(qlist)\n        bound = r\'\\b\'\n        if substrings_on: bound = \'\'\n        pattern_str = r\'%s(?:%s)%s\' % (bound, query, bound)\n        pattern = re.compile( pattern_str, flag)\n\n        results = []\n        l = len(docs)\n        for idx, text in enumerate(docs):\n            matches = pattern.findall(text)\n            if matches and len(matches)>=min_matches: results.append((idx, text, matches))\n            if progress:\n                printProgressBar(idx+1, l, prefix=\'progress: \', suffix =\'complete\', length=50)\n        return results\n\n\n    def _search_chinese(self, query, docs, \n                        substrings_on=True, parse=1, min_matches=1, progress=False):\n        """"""\n        convenience method to search chinese text\n        """"""\n        original_query = query\n        if not isinstance(query, str): raise ValueError(\'query must be str\')\n        if parse > 0:\n            q = U.split_chinese(query)[0]\n            num_words = len(q.split())\n            query = build_ngrams(q, n=parse)\n            query = ["""".join(q) for q in query]\n        return self._search(query, docs, substrings_on=substrings_on, progress=progress)\n\n\n\n#------------------------------------------------------------------------------\n# Non-English Language-Handling\n#------------------------------------------------------------------------------\ndef find_chinese(s): return re.findall(r\'[\\u4e00-\\u9fff]+\', s)\ndef find_arabic(s): return re.findall(r\'[\\u0600-\\u06FF]+\', s)\ndef find_cyrillic(s): return re.findall(r\'[\\u0400-\\u04FF]+\', s)\ndef find_cyrillic2(s): return re.findall(r\'[\xd0\xb0-\xd1\x8f\xd0\x90-\xd0\xaf]+\', s)\ndef find_russian(s): return find_cyrillic(s)\ndef find_times(s): return re.findall(r\'\\d{2}:\\d{2}(?:am|pm)\', s, re.I)\n\n\ndef build_ngrams(s, n=2):\n    lst = s.split()\n    ngrams = []\n    for i in range(len(lst)-(n-1)):\n        ngram = []\n        for j in range(n):\n            ngram.append(lst[i+j])\n        ngram = tuple(ngram)\n        ngrams.append(ngram)\n    return ngrams\n        \n\n\ndef printProgressBar (iteration, total, prefix = \'\', suffix = \'\', decimals = 1, length = 50, fill = \'\xe2\x96\x88\', printEnd = ""\\r""):\n    """"""\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        length      - Optional  : character length of bar (Int)\n        fill        - Optional  : bar fill character (Str)\n        printEnd    - Optional  : end character (e.g. ""\\r"", ""\\r\\n"") (Str)\n    """"""\n    percent = (""{0:."" + str(decimals) + ""f}"").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength + \'-\' * (length - filledLength)\n    print(\'\\r%s |%s| %s%% %s\' % (prefix, bar, percent, suffix), end = printEnd)\n    # Print New Line on Complete\n    if iteration == total: \n        print()\n\n\n\n'"
ktrain/text/shallownlp/utils.py,0,"b'#\n# The ShallowNLP is kept self-contained for now.\n# Thus, some or all of the functions here are copied from\n# ktrain.text.textutils\n\nfrom .imports import *\n\n\ndef extract_filenames(corpus_path, follow_links=False):\n    if os.listdir(corpus_path) == []:\n        raise ValueError(""%s: path is empty"" % corpus_path)\n    for root, _, fnames in os.walk(corpus_path, followlinks=follow_links):\n        for filename in fnames:\n            try:\n                yield os.path.join(root, filename)\n            except Exception:\n                continue\n\n\ndef detect_lang(texts, sample_size=32):\n    """"""\n    detect language\n    """"""\n    if not LANGDETECT:\n        raise ValueError(\'langdetect is missing - install with pip3 install langdetect\')\n\n    if isinstance(texts, str): texts = [texts]\n    if not isinstance(texts, (list, np.ndarray)):\n        raise ValueError(\'texts must be a list or NumPy array of strings\')\n    lst = []\n    for doc in texts[:sample_size]:\n        try:\n            lst.append(langdetect.detect(doc))\n        except:\n            continue\n    if len(lst) == 0:\n        raise Exception(\'could not detect language in random sample of %s docs.\'  % (sample_size))\n    return max(set(lst), key=lst.count)\n\n\ndef is_chinese(lang):\n    """"""\n    include additional languages due to mistakes on short texts by langdetect\n    """"""\n    return lang is not None and lang.startswith(\'zh-\') or lang in [\'ja\', \'ko\']\n\n\n\ndef split_chinese(texts):\n    if not JIEBA:\n        raise ValueError(\'jieba is missing - install with pip3 install jieba\')\n    if isinstance(texts, str): texts=[texts]\n\n    split_texts = []\n    for doc in texts:\n        seg_list = jieba.cut(doc, cut_all=False)\n        seg_list = list(seg_list)\n        split_texts.append(seg_list)\n    return ["" "".join(tokens) for tokens in split_texts]\n\n\ndef decode_by_line(texts, encoding=\'utf-8\', verbose=1):\n    """"""\n    Decode text line by line and skip over errors.\n    """"""\n    if isinstance(texts, str): texts = [texts]\n    new_texts = []\n    skips=0\n    num_lines = 0\n    for doc in texts:\n        text = """"\n        for line in doc.splitlines():\n            num_lines +=1\n            try:\n                line = line.decode(encoding)\n            except:\n                skips +=1\n                continue\n            text += line\n        new_texts.append(text)\n    pct = round((skips*1./num_lines) * 100, 1)\n    if verbose:\n        print(\'skipped %s lines (%s%%) due to character decoding errors\' % (skips, pct))\n        if pct > 10:\n            print(\'If this is too many, try a different encoding\')\n    return new_texts\n\n\ndef detect_encoding(texts, sample_size=32):\n    if not CHARDET:\n        raise ValueError(\'cchardet is missing - install with pip3 install cchardet\')\n    if isinstance(texts, str): texts = [texts]\n    lst = [chardet.detect(doc)[\'encoding\'] for doc in texts[:sample_size]]\n    encoding = max(set(lst), key=lst.count)\n    encoding = \'utf-8\' if encoding.lower() in [\'ascii\', \'utf8\', \'utf-8\'] else encoding\n    return encoding\n\n\ndef read_text(filename):\n    with open(filename, \'rb\') as f:\n        text = f.read()\n    encoding = detect_encoding([text])\n    try:\n        decoded_text = text.decode(encoding) \n    except:\n        U.vprint(\'Decoding with %s failed 1st attempt - using %s with skips\' % (encoding,\n                                                                                encoding),\n                                                                                verbose=verbose)\n        decoded_text = decode_by_line(text, encoding=encoding)\n    return decoded_text.strip()\n\n\ndef sent_tokenize(text):\n    """"""\n    segment text into sentences\n    """"""\n    lang = detect_lang(text)\n    sents = []\n    if is_chinese(lang):\n        for sent in re.findall(u\'[^!?\xe3\x80\x82\\.\\!\\?]+[!?\xe3\x80\x82\\.\\!\\?]?\', text, flags=re.U):\n            sents.append(sent)\n    else:\n        for paragraph in segmenter.process(text):\n            for sentence in paragraph:\n                sents.append("" "".join([t.value for t in sentence]))\n    return sents\n\n\n'"
ktrain/text/shallownlp/version.py,0,"b""__all__ = ['__version__']\n__version__ = '0.1.0'\n\n"""
ktrain/text/summarization/__init__.py,0,b'from .core import TransformerSummarizer\n'
ktrain/text/summarization/core.py,0,"b'from ...imports import *\nfrom ... import utils as U\n\nclass TransformerSummarizer():\n    """"""\n    interface to Transformer-based text summarization\n    """"""\n\n    def __init__(self, model_name=\'facebook/bart-large-cnn\', device=None):\n        """"""\n        interface to BART-based text summarization using transformers library\n\n        Args:\n          model_name(str): name of BART model for summarization\n          device(str): device to use (e.g., \'cuda\', \'cpu\')\n        """"""\n        if \'bart\' not in model_name:\n            raise ValueError(\'TransformerSummarizer currently only accepts BART models\')\n        try:\n            import torch\n        except ImportError:\n            raise Exception(\'TransformerSummarizer requires PyTorch to be installed.\')\n        self.torch_device = device\n        if self.torch_device is None: self.torch_device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        from transformers import BartTokenizer, BartForConditionalGeneration\n        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.torch_device)\n\n\n    def summarize(self, doc):\n        """"""\n        summarize document text\n        Args:\n          doc(str): text of document\n        Returns:\n          str: summary text\n        """"""\n\n        answers_input_ids = self.tokenizer.batch_encode_plus([doc], \n                                                             return_tensors=\'pt\', \n                                                             max_length=1024)[\'input_ids\'].to(self.torch_device)\n        summary_ids = self.model.generate(answers_input_ids,\n                                          num_beams=4,\n                                          length_penalty=2.0,\n                                          max_length=142,\n                                          min_length=56,\n                                          no_repeat_ngram_size=3)\n\n        exec_sum = self.tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n        return exec_sum\n'"
ktrain/text/zsl/__init__.py,0,b'from .core import ZeroShotClassifier\n'
ktrain/text/zsl/core.py,0,"b'from ...imports import *\nfrom ... import utils as U\n\nclass ZeroShotClassifier():\n    """"""\n    interface to Zero Shot Topic Classifier\n    """"""\n\n    def __init__(self, model_name=\'facebook/bart-large-mnli\', device=None):\n        """"""\n        interface to BART-based text summarization using transformers library\n\n        Args:\n          model_name(str): name of BART model\n          device(str): device to use (e.g., \'cuda\', \'cpu\')\n        """"""\n        if \'mnli\' not in model_name:\n            raise ValueError(\'ZeroShotClasifier requires an MNLI model\')\n        try:\n            import torch\n        except ImportError:\n            raise Exception(\'ZeroShotClassifier requires PyTorch to be installed.\')\n        self.torch_device = device\n        if self.torch_device is None: self.torch_device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        from transformers import BartForSequenceClassification, BartTokenizer\n        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n        self.model = BartForSequenceClassification.from_pretrained(model_name).to(self.torch_device)\n\n\n    def predict(self, doc, topic_strings=[], include_labels=False):\n        """"""\n        zero-shot topic classification\n        Args:\n          doc(str): text of document\n          topic_strings(list): a list of strings representing topics of your choice\n                               Example:\n                               topic_strings=[\'political science\', \'sports\', \'science\']\n        Returns:\n          inferred probabilities\n        """"""\n        if topic_strings is None or len(topic_strings) == 0:\n            raise ValueError(\'topic_strings must be a list of strings\')\n        true_probs = []\n        for topic_string in topic_strings:\n            premise = doc\n            hypothesis = \'This text is about %s.\' % (topic_string)\n            input_ids = self.tokenizer.encode(premise, hypothesis, return_tensors=\'pt\').to(self.torch_device)\n            logits = self.model(input_ids)[0]\n\n            # we throw away ""neutral"" (dim 1) and take the probability of\n            # ""entailment"" (2) as the probability of the label being true \n            # reference: https://joeddav.github.io/blog/2020/05/29/ZSL.html\n            entail_contradiction_logits = logits[:,[0,2]]\n            probs = entail_contradiction_logits.softmax(dim=1)\n            true_prob = probs[:,1].item() \n            true_probs.append(true_prob)\n        if include_labels:\n            true_probs = list(zip(topic_strings, true_probs))\n        return true_probs\n\n'"
ktrain/text/ner/anago/__init__.py,0,b'from .tagger import Tagger\nfrom .trainer import Trainer\nfrom .wrapper import Sequence\n'
ktrain/text/ner/anago/callbacks.py,0,"b'""""""\nCustom callbacks.\n""""""\nfrom ....imports import *\n\nclass F1score(Callback):\n\n    def __init__(self, seq, preprocessor=None):\n        super(F1score, self).__init__()\n        self.seq = seq\n        self.p = preprocessor\n\n    def get_lengths(self, y_true):\n        lengths = []\n        for y in np.argmax(y_true, -1):\n            try:\n                i = list(y).index(0)\n            except ValueError:\n                i = len(y)\n            lengths.append(i)\n\n        return lengths\n\n    def on_epoch_end(self, epoch, logs={}):\n        label_true = []\n        label_pred = []\n        for i in range(len(self.seq)):\n            x_true, y_true = self.seq[i]\n            lengths = self.get_lengths(y_true)\n            y_pred = self.model.predict_on_batch(x_true)\n\n            y_true = self.p.inverse_transform(y_true, lengths)\n            y_pred = self.p.inverse_transform(y_pred, lengths)\n\n            label_true.extend(y_true)\n            label_pred.extend(y_pred)\n\n        score = ner_f1_score(label_true, label_pred)\n        print(\' - f1: {:04.2f}\'.format(score * 100))\n        print(ner_classification_report(label_true, label_pred))\n        logs[\'f1\'] = score\n'"
ktrain/text/ner/anago/layers.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom ....imports import *\nfrom .... import utils as U\n\n#from keras_contrib.losses import crf_loss\n#from keras_contrib.metrics import crf_marginal_accuracy\n#from keras_contrib.metrics import crf_viterbi_accuracy\n#from keras_contrib.utils.test_utils import to_tuple\n\n\nclass CRF(Layer):\n    """"""An implementation of linear chain conditional random field (CRF).\n\n    An linear chain CRF is defined to maximize the following likelihood function:\n\n    $$ L(W, U, b; y_1, ..., y_n) := \\frac{1}{Z}\n    \\sum_{y_1, ..., y_n} \\exp(-a_1\' y_1 - a_n\' y_n\n        - \\sum_{k=1^n}((f(x_k\' W + b) y_k) + y_1\' U y_2)), $$\n\n    where:\n        $Z$: normalization constant\n        $x_k, y_k$:  inputs and outputs\n\n    This implementation has two modes for optimization:\n    1. (`join mode`) optimized by maximizing join likelihood,\n    which is optimal in theory of statistics.\n       Note that in this case, CRF must be the output/last layer.\n    2. (`marginal mode`) return marginal probabilities on each time\n    step and optimized via composition\n       likelihood (product of marginal likelihood), i.e.,\n       using `categorical_crossentropy` loss.\n       Note that in this case, CRF can be either the last layer or an\n       intermediate layer (though not explored).\n\n    For prediction (test phrase), one can choose either Viterbi\n    best path (class indices) or marginal\n    probabilities if probabilities are needed.\n    However, if one chooses *join mode* for training,\n    Viterbi output is typically better than marginal output,\n    but the marginal output will still perform\n    reasonably close, while if *marginal mode* is used for training,\n    marginal output usually performs\n    much better. The default behavior and `metrics.crf_accuracy`\n    is set according to this observation.\n\n    In addition, this implementation supports masking and accepts either\n    onehot or sparse target.\n\n    If you open a issue or a pull request about CRF, please\n    add \'cc @lzfelix\' to notify Luiz Felix.\n\n\n    # Examples\n\n    ```python\n        from keras_contrib.layers import CRF\n        from keras_contrib.losses import crf_loss\n        from keras_contrib.metrics import crf_viterbi_accuracy\n\n        model = Sequential()\n        model.add(Embedding(3001, 300, mask_zero=True)(X)\n\n        # use learn_mode = \'join\', test_mode = \'viterbi\',\n        # sparse_target = True (label indice output)\n        crf = CRF(10, sparse_target=True)\n        model.add(crf)\n\n        # crf_accuracy is default to Viterbi acc if using join-mode (default).\n        # One can add crf.marginal_acc if interested, but may slow down learning\n        model.compile(\'adam\', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n\n        # y must be label indices (with shape 1 at dim 3) here,\n        # since `sparse_target=True`\n        model.fit(x, y)\n\n        # prediction give onehot representation of Viterbi best path\n        y_hat = model.predict(x_test)\n    ```\n\n    The following snippet shows how to load a persisted\n    model that uses the CRF layer:\n\n    ```python\n        from tensorflow.keras.models import load_model\n        from keras_contrib.losses import import crf_loss\n        from keras_contrib.metrics import crf_viterbi_accuracy\n\n        custom_objects={\'CRF\': CRF,\n                        \'crf_loss\': crf_loss,\n                        \'crf_viterbi_accuracy\': crf_viterbi_accuracy}\n\n        loaded_model = load_model(\'<path_to_model>\',\n                                  custom_objects=custom_objects)\n    ```\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        learn_mode: Either \'join\' or \'marginal\'.\n            The former train the model by maximizing join likelihood while the latter\n            maximize the product of marginal likelihood over all time steps.\n            One should use `losses.crf_nll` for \'join\' mode\n            and `losses.categorical_crossentropy` or\n            `losses.sparse_categorical_crossentropy` for\n            `marginal` mode.  For convenience, simply\n            use `losses.crf_loss`, which will decide the proper loss as described.\n        test_mode: Either \'viterbi\' or \'marginal\'.\n            The former is recommended and as default when `learn_mode = \'join\'` and\n            gives one-hot representation of the best path at test (prediction) time,\n            while the latter is recommended and chosen as default\n            when `learn_mode = \'marginal\'`,\n            which produces marginal probabilities for each time step.\n            For evaluating metrics, one should\n            use `metrics.crf_viterbi_accuracy` for \'viterbi\' mode and\n            \'metrics.crf_marginal_accuracy\' for \'marginal\' mode, or\n            simply use `metrics.crf_accuracy` for\n            both which automatically decides it as described.\n            One can also use both for evaluation at training.\n        sparse_target: Boolean (default False) indicating\n            if provided labels are one-hot or\n            indices (with shape 1 at dim 3).\n        use_boundary: Boolean (default True) indicating if trainable\n            start-end chain energies\n            should be added to model.\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        chain_initializer: Initializer for the `chain_kernel` weights matrix,\n            used for the CRF chain energy.\n            (see [initializers](../initializers.md)).\n        boundary_initializer: Initializer for the `left_boundary`,\n            \'right_boundary\' weights vectors,\n            used for the start/left and end/right boundary energy.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        chain_regularizer: Regularizer function applied to\n            the `chain_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        boundary_regularizer: Regularizer function applied to\n            the \'left_boundary\', \'right_boundary\' weight vectors\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        chain_constraint: Constraint function applied to\n            the `chain_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        boundary_constraint: Constraint function applied to\n            the `left_boundary`, `right_boundary` weights vectors\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        unroll: Boolean (default False). If True, the network will be\n            unrolled, else a symbolic loop will be used.\n            Unrolling can speed-up a RNN, although it tends\n            to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n\n    # Output shape\n        3D tensor with shape `(nb_samples, timesteps, units)`.\n\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n\n    """"""\n\n    def __init__(self, units,\n                 learn_mode=\'join\',\n                 test_mode=None,\n                 sparse_target=False,\n                 use_boundary=True,\n                 use_bias=True,\n                 activation=\'linear\',\n                 kernel_initializer=\'glorot_uniform\',\n                 chain_initializer=\'orthogonal\',\n                 bias_initializer=\'zeros\',\n                 boundary_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 chain_regularizer=None,\n                 boundary_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 chain_constraint=None,\n                 boundary_constraint=None,\n                 bias_constraint=None,\n                 input_dim=None,\n                 unroll=False,\n                 **kwargs):\n        super(CRF, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.units = units\n        self.learn_mode = learn_mode\n        assert self.learn_mode in [\'join\', \'marginal\']\n        self.test_mode = test_mode\n        if self.test_mode is None:\n            self.test_mode = \'viterbi\' if self.learn_mode == \'join\' else \'marginal\'\n        else:\n            assert self.test_mode in [\'viterbi\', \'marginal\']\n        self.sparse_target = sparse_target\n        self.use_boundary = use_boundary\n        self.use_bias = use_bias\n\n        self.activation = activations.get(activation)\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.chain_initializer = initializers.get(chain_initializer)\n        self.boundary_initializer = initializers.get(boundary_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.chain_regularizer = regularizers.get(chain_regularizer)\n        self.boundary_regularizer = regularizers.get(boundary_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.chain_constraint = constraints.get(chain_constraint)\n        self.boundary_constraint = constraints.get(boundary_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.unroll = unroll\n\n    def build(self, input_shape):\n        input_shape = to_tuple(input_shape)\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[-1]\n\n        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n                                      name=\'kernel\',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.chain_kernel = self.add_weight(shape=(self.units, self.units),\n                                            name=\'chain_kernel\',\n                                            initializer=self.chain_initializer,\n                                            regularizer=self.chain_regularizer,\n                                            constraint=self.chain_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name=\'bias\',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = 0\n\n        if self.use_boundary:\n            self.left_boundary = self.add_weight(shape=(self.units,),\n                                                 name=\'left_boundary\',\n                                                 initializer=self.boundary_initializer,\n                                                 regularizer=self.boundary_regularizer,\n                                                 constraint=self.boundary_constraint)\n            self.right_boundary = self.add_weight(shape=(self.units,),\n                                                  name=\'right_boundary\',\n                                                  initializer=self.boundary_initializer,\n                                                  regularizer=self.boundary_regularizer,\n                                                  constraint=self.boundary_constraint)\n        self.built = True\n\n    def call(self, X, mask=None):\n        if mask is not None:\n            assert K.ndim(mask) == 2, \'Input mask to CRF must have dim 2 if not None\'\n\n        if self.test_mode == \'viterbi\':\n            test_output = self.viterbi_decoding(X, mask)\n        else:\n            test_output = self.get_marginal_prob(X, mask)\n\n        self.uses_learning_phase = True\n        if self.learn_mode == \'join\':\n            train_output = K.zeros_like(K.dot(X, self.kernel))\n            out = K.in_train_phase(train_output, test_output)\n        else:\n            if self.test_mode == \'viterbi\':\n                train_output = self.get_marginal_prob(X, mask)\n                out = K.in_train_phase(train_output, test_output)\n            else:\n                out = test_output\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:2] + (self.units,)\n\n    def compute_mask(self, input, mask=None):\n        if mask is not None and self.learn_mode == \'join\':\n            return K.any(mask, axis=1)\n        return mask\n\n    def get_config(self):\n        config = {\n            \'units\': self.units,\n            \'learn_mode\': self.learn_mode,\n            \'test_mode\': self.test_mode,\n            \'use_boundary\': self.use_boundary,\n            \'use_bias\': self.use_bias,\n            \'sparse_target\': self.sparse_target,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'chain_initializer\': initializers.serialize(self.chain_initializer),\n            \'boundary_initializer\': initializers.serialize(\n                self.boundary_initializer),\n            \'bias_initializer\': initializers.serialize(self.bias_initializer),\n            \'activation\': activations.serialize(self.activation),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'chain_regularizer\': regularizers.serialize(self.chain_regularizer),\n            \'boundary_regularizer\': regularizers.serialize(\n                self.boundary_regularizer),\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n            \'chain_constraint\': constraints.serialize(self.chain_constraint),\n            \'boundary_constraint\': constraints.serialize(self.boundary_constraint),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint),\n            \'input_dim\': self.input_dim,\n            \'unroll\': self.unroll}\n        base_config = super(CRF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @property\n    def loss_function(self):\n        #warnings.warn(\'CRF.loss_function is deprecated \'\n                      #\'and it might be removed in the future. Please \'\n                      #\'use losses.crf_loss instead.\')\n        return crf_loss\n\n    @property\n    def accuracy(self):\n        #warnings.warn(\'CRF.accuracy is deprecated and it \'\n                      #\'might be removed in the future. Please \'\n                      #\'use metrics.crf_accuracy\')\n        if self.test_mode == \'viterbi\':\n            return crf_viterbi_accuracy\n        else:\n            return crf_marginal_accuracy\n\n    @property\n    def viterbi_acc(self):\n        #warnings.warn(\'CRF.viterbi_acc is deprecated and it might \'\n                      #\'be removed in the future. Please \'\n                      #\'use metrics.viterbi_acc instead.\')\n        return crf_viterbi_accuracy\n\n    @property\n    def marginal_acc(self):\n        #warnings.warn(\'CRF.moarginal_acc is deprecated and it \'\n                      #\'might be removed in the future. Please \'\n                      #\'use metrics.marginal_acc instead.\')\n        return crf_marginal_accuracy\n\n    @staticmethod\n    def softmaxNd(x, axis=-1):\n        m = K.max(x, axis=axis, keepdims=True)\n        exp_x = K.exp(x - m)\n        prob_x = exp_x / K.sum(exp_x, axis=axis, keepdims=True)\n        return prob_x\n\n    @staticmethod\n    def shift_left(x, offset=1):\n        assert offset > 0\n        return K.concatenate([x[:, offset:], K.zeros_like(x[:, :offset])], axis=1)\n\n    @staticmethod\n    def shift_right(x, offset=1):\n        assert offset > 0\n        return K.concatenate([K.zeros_like(x[:, :offset]), x[:, :-offset]], axis=1)\n\n    def add_boundary_energy(self, energy, mask, start, end):\n        start = K.expand_dims(K.expand_dims(start, 0), 0)\n        end = K.expand_dims(K.expand_dims(end, 0), 0)\n        if mask is None:\n            energy = K.concatenate([energy[:, :1, :] + start, energy[:, 1:, :]],\n                                   axis=1)\n            energy = K.concatenate([energy[:, :-1, :], energy[:, -1:, :] + end],\n                                   axis=1)\n        else:\n            mask = K.expand_dims(K.cast(mask, K.floatx()))\n            start_mask = K.cast(K.greater(mask, self.shift_right(mask)), K.floatx())\n            end_mask = K.cast(K.greater(self.shift_left(mask), mask), K.floatx())\n            energy = energy + start_mask * start\n            energy = energy + end_mask * end\n        return energy\n\n    def get_log_normalization_constant(self, input_energy, mask, **kwargs):\n        """"""Compute logarithm of the normalization constant Z, where\n        Z = sum exp(-E) -> logZ = log sum exp(-E) =: -nlogZ\n        """"""\n        # should have logZ[:, i] == logZ[:, j] for any i, j\n        logZ = self.recursion(input_energy, mask, return_sequences=False, **kwargs)\n        return logZ[:, 0]\n\n    def get_energy(self, y_true, input_energy, mask):\n        """"""Energy = a1\' y1 + u1\' y1 + y1\' U y2 + u2\' y2 + y2\' U y3 + u3\' y3 + an\' y3\n        """"""\n        input_energy = K.sum(input_energy * y_true, 2)  # (B, T)\n        # (B, T-1)\n        chain_energy = K.sum(K.dot(y_true[:, :-1, :],\n                                   self.chain_kernel) * y_true[:, 1:, :], 2)\n\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            # (B, T-1), mask[:,:-1]*mask[:,1:] makes it work with any padding\n            chain_mask = mask[:, :-1] * mask[:, 1:]\n            input_energy = input_energy * mask\n            chain_energy = chain_energy * chain_mask\n        total_energy = K.sum(input_energy, -1) + K.sum(chain_energy, -1)  # (B, )\n\n        return total_energy\n\n    def get_negative_log_likelihood(self, y_true, X, mask):\n        """"""Compute the loss, i.e., negative log likelihood (normalize by number of time steps)\n           likelihood = 1/Z * exp(-E) ->  neg_log_like = - log(1/Z * exp(-E)) = logZ + E\n        """"""\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask,\n                                                    self.left_boundary,\n                                                    self.right_boundary)\n        energy = self.get_energy(y_true, input_energy, mask)\n        logZ = self.get_log_normalization_constant(input_energy, mask,\n                                                   input_length=K.int_shape(X)[1])\n        nloglik = logZ + energy\n        if mask is not None:\n            nloglik = nloglik / K.sum(K.cast(mask, K.floatx()), 1)\n        else:\n            nloglik = nloglik / K.cast(K.shape(X)[1], K.floatx())\n        return nloglik\n\n    def step(self, input_energy_t, states, return_logZ=True):\n        # not in the following  `prev_target_val` has shape = (B, F)\n        # where B = batch_size, F = output feature dim\n        # Note: `i` is of float32, due to the behavior of `K.rnn`\n        prev_target_val, i, chain_energy = states[:3]\n        t = K.cast(i[0, 0], dtype=\'int32\')\n        if len(states) > 3:\n            if K.backend() == \'theano\':\n                m = states[3][:, t:(t + 2)]\n            else:\n                m = tf.slice(states[3], [0, t], [-1, 2])\n\n            input_energy_t = input_energy_t * K.expand_dims(m[:, 0])\n            # (1, F, F)*(B, 1, 1) -> (B, F, F)\n            chain_energy = chain_energy * K.expand_dims(\n                K.expand_dims(m[:, 0] * m[:, 1]))\n        if return_logZ:\n            # shapes: (1, B, F) + (B, F, 1) -> (B, F, F)\n            energy = chain_energy + K.expand_dims(input_energy_t - prev_target_val, 2)\n            #new_target_val = K.logsumexp(-energy, 1)  # shapes: (B, F)\n            new_target_val = tf.reduce_logsumexp(-energy, 1)  # shapes: (B, F)\n            return new_target_val, [new_target_val, i + 1]\n        else:\n            energy = chain_energy + K.expand_dims(input_energy_t + prev_target_val, 2)\n            min_energy = K.min(energy, 1)\n            # cast for tf-version `K.rnn\n            argmin_table = K.cast(K.argmin(energy, 1), K.floatx())\n            return argmin_table, [min_energy, i + 1]\n\n    def recursion(self, input_energy, mask=None, go_backwards=False,\n                  return_sequences=True, return_logZ=True, input_length=None):\n        """"""Forward (alpha) or backward (beta) recursion\n\n        If `return_logZ = True`, compute the logZ, the normalization constant:\n\n        \\[ Z = \\sum_{y1, y2, y3} exp(-E) # energy\n          = \\sum_{y1, y2, y3} exp(-(u1\' y1 + y1\' W y2 + u2\' y2 + y2\' W y3 + u3\' y3))\n          = sum_{y2, y3} (exp(-(u2\' y2 + y2\' W y3 + u3\' y3))\n          sum_{y1} exp(-(u1\' y1\' + y1\' W y2))) \\]\n\n        Denote:\n            \\[ S(y2) := sum_{y1} exp(-(u1\' y1 + y1\' W y2)), \\]\n            \\[ Z = sum_{y2, y3} exp(log S(y2) - (u2\' y2 + y2\' W y3 + u3\' y3)) \\]\n            \\[ logS(y2) = log S(y2) = log_sum_exp(-(u1\' y1\' + y1\' W y2)) \\]\n        Note that:\n              yi\'s are one-hot vectors\n              u1, u3: boundary energies have been merged\n\n        If `return_logZ = False`, compute the Viterbi\'s best path lookup table.\n        """"""\n        chain_energy = self.chain_kernel\n        # shape=(1, F, F): F=num of output features. 1st F is for t-1, 2nd F for t\n        chain_energy = K.expand_dims(chain_energy, 0)\n        # shape=(B, F), dtype=float32\n        prev_target_val = K.zeros_like(input_energy[:, 0, :])\n\n        if go_backwards:\n            input_energy = K.reverse(input_energy, 1)\n            if mask is not None:\n                mask = K.reverse(mask, 1)\n\n        initial_states = [prev_target_val, K.zeros_like(prev_target_val[:, :1])]\n        constants = [chain_energy]\n\n        if mask is not None:\n            mask2 = K.cast(K.concatenate([mask, K.zeros_like(mask[:, :1])], axis=1),\n                           K.floatx())\n            constants.append(mask2)\n\n        def _step(input_energy_i, states):\n            return self.step(input_energy_i, states, return_logZ)\n\n        target_val_last, target_val_seq, _ = K.rnn(_step, input_energy,\n                                                   initial_states,\n                                                   constants=constants,\n                                                   input_length=input_length,\n                                                   unroll=self.unroll)\n\n        if return_sequences:\n            if go_backwards:\n                target_val_seq = K.reverse(target_val_seq, 1)\n            return target_val_seq\n        else:\n            return target_val_last\n\n    def forward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, **kwargs)\n\n    def backward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, go_backwards=True, **kwargs)\n\n    def get_marginal_prob(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask,\n                                                    self.left_boundary,\n                                                    self.right_boundary)\n        input_length = K.int_shape(X)[1]\n        alpha = self.forward_recursion(input_energy, mask=mask,\n                                       input_length=input_length)\n        beta = self.backward_recursion(input_energy, mask=mask,\n                                       input_length=input_length)\n        if mask is not None:\n            input_energy = input_energy * K.expand_dims(K.cast(mask, K.floatx()))\n        margin = -(self.shift_right(alpha) + input_energy + self.shift_left(beta))\n        return self.softmaxNd(margin)\n\n    def viterbi_decoding(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(\n                input_energy, mask, self.left_boundary, self.right_boundary)\n\n        argmin_tables = self.recursion(input_energy, mask, return_logZ=False)\n        argmin_tables = K.cast(argmin_tables, \'int32\')\n\n        # backward to find best path, `initial_best_idx` can be any,\n        # as all elements in the last argmin_table are the same\n        argmin_tables = K.reverse(argmin_tables, 1)\n        # matrix instead of vector is required by tf `K.rnn`\n        initial_best_idx = [K.expand_dims(argmin_tables[:, 0, 0])]\n        if K.backend() == \'theano\':\n            from theano import tensor as T\n            initial_best_idx = [T.unbroadcast(initial_best_idx[0], 1)]\n\n        def gather_each_row(params, indices):\n            n = K.shape(indices)[0]\n            if K.backend() == \'theano\':\n                from theano import tensor as T\n                return params[T.arange(n), indices]\n            elif K.backend() == \'tensorflow\':\n                import tensorflow as tf\n                indices = K.transpose(K.stack([tf.range(n), indices]))\n                return tf.gather_nd(params, indices)\n            else:\n                raise NotImplementedError\n\n        def find_path(argmin_table, best_idx):\n            next_best_idx = gather_each_row(argmin_table, best_idx[0][:, 0])\n            next_best_idx = K.expand_dims(next_best_idx)\n            if K.backend() == \'theano\':\n                from theano import tensor as T\n                next_best_idx = T.unbroadcast(next_best_idx, 1)\n            return next_best_idx, [next_best_idx]\n\n        _, best_paths, _ = K.rnn(find_path, argmin_tables, initial_best_idx,\n                                 input_length=K.int_shape(X)[1], unroll=self.unroll)\n        best_paths = K.reverse(best_paths, 1)\n        best_paths = K.squeeze(best_paths, 2)\n\n        return K.one_hot(best_paths, self.units)\ndef crf_nll(y_true, y_pred):\n    """"""The negative log-likelihood for linear chain Conditional Random Field (CRF).\n\n    This loss function is only used when the `layers.CRF` layer\n    is trained in the ""join"" mode.\n\n    # Arguments\n        y_true: tensor with true targets.\n        y_pred: tensor with predicted targets.\n\n    # Returns\n        A scalar representing corresponding to the negative log-likelihood.\n\n    # Raises\n        TypeError: If CRF is not the last layer.\n\n    # About GitHub\n        If you open an issue or a pull request about CRF, please\n        add `cc @lzfelix` to notify Luiz Felix.\n    """"""\n\n    crf, idx = y_pred._keras_history[:2]\n    if crf._outbound_nodes:\n        raise TypeError(\'When learn_model=""join"", CRF must be the last layer.\')\n    if crf.sparse_target:\n        y_true = K.one_hot(K.cast(y_true[:, :, 0], \'int32\'), crf.units)\n    #X = crf._inbound_nodes[idx].input_tensors[0]\n    #mask = crf._inbound_nodes[idx].input_masks[0]\n    X = crf.get_input_at(idx)\n    mask = crf.get_input_mask_at(idx)\n    nloglik = crf.get_negative_log_likelihood(y_true, X, mask)\n    return nloglik\n\n\ndef crf_loss(y_true, y_pred):\n    """"""General CRF loss function depending on the learning mode.\n\n    # Arguments\n        y_true: tensor with true targets.\n        y_pred: tensor with predicted targets.\n\n    # Returns\n        If the CRF layer is being trained in the join mode, returns the negative\n        log-likelihood. Otherwise returns the categorical crossentropy implemented\n        by the underlying Keras backend.\n\n    # About GitHub\n        If you open an issue or a pull request about CRF, please\n        add `cc @lzfelix` to notify Luiz Felix.\n    """"""\n    crf, idx = y_pred._keras_history[:2]\n    if crf.learn_mode == \'join\':\n        return crf_nll(y_true, y_pred)\n    else:\n        if crf.sparse_target:\n            return sparse_categorical_crossentropy(y_true, y_pred)\n        else:\n            return categorical_crossentropy(y_true, y_pred)\n\n\ndef _get_accuracy(y_true, y_pred, mask, sparse_target=False):\n    y_pred = K.argmax(y_pred, -1)\n    if sparse_target:\n        y_true = K.cast(y_true[:, :, 0], K.dtype(y_pred))\n    else:\n        y_true = K.argmax(y_true, -1)\n    judge = K.cast(K.equal(y_pred, y_true), K.floatx())\n    if mask is None:\n        return K.mean(judge)\n    else:\n        mask = K.cast(mask, K.floatx())\n        return K.sum(judge * mask) / K.sum(mask)\n\n\ndef crf_viterbi_accuracy(y_true, y_pred):\n    \'\'\'Use Viterbi algorithm to get best path, and compute its accuracy.\n    `y_pred` must be an output from CRF.\'\'\'\n    crf, idx = y_pred._keras_history[:2]\n    #X = crf._inbound_nodes[idx].input_tensors[0]\n    #mask = crf._inbound_nodes[idx].input_masks[0]\n    X = crf.get_input_at(idx)\n    mask = crf.get_input_mask_at(idx)\n    y_pred = crf.viterbi_decoding(X, mask)\n    return _get_accuracy(y_true, y_pred, mask, crf.sparse_target)\n\n\ndef crf_marginal_accuracy(y_true, y_pred):\n    \'\'\'Use time-wise marginal argmax as prediction.\n    `y_pred` must be an output from CRF with `learn_mode=""marginal""`.\'\'\'\n    crf, idx = y_pred._keras_history[:2]\n    #X = crf._inbound_nodes[idx].input_tensors[0]\n    #mask = crf._inbound_nodes[idx].input_masks[0]\n    X = crf.get_input_at(idx)\n    mask = crf.get_input_mask_at(idx)\n    y_pred = crf.get_marginal_prob(X, mask)\n    return _get_accuracy(y_true, y_pred, mask, crf.sparse_target)\n\n\ndef crf_accuracy(y_true, y_pred):\n    \'\'\'Ge default accuracy based on CRF `test_mode`.\'\'\'\n    crf, idx = y_pred._keras_history[:2]\n    if crf.test_mode == \'viterbi\':\n        return crf_viterbi_accuracy(y_true, y_pred)\n\ndef to_tuple(shape):\n    """"""This functions is here to fix an inconsistency between keras and tf.keras.\n\n    In tf.keras, the input_shape argument is an tuple with `Dimensions` objects.\n    In keras, the input_shape is a simple tuple of ints or `None`.\n\n    We\'ll work with tuples of ints or `None` to be consistent\n    with keras-team/keras. So we must apply this function to\n    all input_shapes of the build methods in custom layers.\n    """"""\n    return tuple(tf.TensorShape(shape).as_list())\n'"
ktrain/text/ner/anago/layers_standalone.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom ....imports import *\n\nclass CRF(Layer):\n    """"""An implementation of linear chain conditional random field (CRF).\n    An linear chain CRF is defined to maximize the following likelihood function:\n    $$ L(W, U, b; y_1, ..., y_n) := \\frac{1}{Z} \\sum_{y_1, ..., y_n} \\exp(-a_1\' y_1 - a_n\' y_n\n        - \\sum_{k=1^n}((f(x_k\' W + b) y_k) + y_1\' U y_2)), $$\n    where:\n        $Z$: normalization constant\n        $x_k, y_k$:  inputs and outputs\n    This implementation has two modes for optimization:\n    1. (`join mode`) optimized by maximizing join likelihood, which is optimal in theory of statistics.\n       Note that in this case, CRF must be the output/last layer.\n    2. (`marginal mode`) return marginal probabilities on each time step and optimized via composition\n       likelihood (product of marginal likelihood), i.e., using `categorical_crossentropy` loss.\n       Note that in this case, CRF can be either the last layer or an intermediate layer (though not explored).\n    For prediction (test phrase), one can choose either Viterbi best path (class indices) or marginal\n    probabilities if probabilities are needed. However, if one chooses *join mode* for training,\n    Viterbi output is typically better than marginal output, but the marginal output will still perform\n    reasonably close, while if *marginal mode* is used for training, marginal output usually performs\n    much better. The default behavior is set according to this observation.\n    In addition, this implementation supports masking and accepts either onehot or sparse target.\n    # Examples\n    ```python\n        model = Sequential()\n        model.add(Embedding(3001, 300, mask_zero=True)(X)\n        # use learn_mode = \'join\', test_mode = \'viterbi\', sparse_target = True (label indice output)\n        crf = CRF(10, sparse_target=True)\n        model.add(crf)\n        # crf.accuracy is default to Viterbi acc if using join-mode (default).\n        # One can add crf.marginal_acc if interested, but may slow down learning\n        model.compile(\'adam\', loss=crf.loss_function, metrics=[crf.accuracy])\n        # y must be label indices (with shape 1 at dim 3) here, since `sparse_target=True`\n        model.fit(x, y)\n        # prediction give onehot representation of Viterbi best path\n        y_hat = model.predict(x_test)\n    ```\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        learn_mode: Either \'join\' or \'marginal\'.\n            The former train the model by maximizing join likelihood while the latter\n            maximize the product of marginal likelihood over all time steps.\n        test_mode: Either \'viterbi\' or \'marginal\'.\n            The former is recommended and as default when `learn_mode = \'join\'` and\n            gives one-hot representation of the best path at test (prediction) time,\n            while the latter is recommended and chosen as default when `learn_mode = \'marginal\'`,\n            which produces marginal probabilities for each time step.\n        sparse_target: Boolean (default False) indicating if provided labels are one-hot or\n            indices (with shape 1 at dim 3).\n        use_boundary: Boolean (default True) indicating if trainable start-end chain energies\n            should be added to model.\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        chain_initializer: Initializer for the `chain_kernel` weights matrix,\n            used for the CRF chain energy.\n            (see [initializers](../initializers.md)).\n        boundary_initializer: Initializer for the `left_boundary`, \'right_boundary\' weights vectors,\n            used for the start/left and end/right boundary energy.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        chain_regularizer: Regularizer function applied to\n            the `chain_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        boundary_regularizer: Regularizer function applied to\n            the \'left_boundary\', \'right_boundary\' weight vectors\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        chain_constraint: Constraint function applied to\n            the `chain_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        boundary_constraint: Constraint function applied to\n            the `left_boundary`, `right_boundary` weights vectors\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        unroll: Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used.\n            Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n    # Output shape\n        3D tensor with shape `(nb_samples, timesteps, units)`.\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n    """"""\n\n    def __init__(self, units,\n                 learn_mode=\'join\',\n                 test_mode=None,\n                 sparse_target=False,\n                 use_boundary=True,\n                 use_bias=True,\n                 activation=\'linear\',\n                 kernel_initializer=\'glorot_uniform\',\n                 chain_initializer=\'orthogonal\',\n                 bias_initializer=\'zeros\',\n                 boundary_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 chain_regularizer=None,\n                 boundary_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 chain_constraint=None,\n                 boundary_constraint=None,\n                 bias_constraint=None,\n                 input_dim=None,\n                 unroll=False,\n                 **kwargs):\n        super(CRF, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.units = units\n        self.learn_mode = learn_mode\n        assert self.learn_mode in [\'join\', \'marginal\']\n        self.test_mode = test_mode\n        if self.test_mode is None:\n            self.test_mode = \'viterbi\' if self.learn_mode == \'join\' else \'marginal\'\n        else:\n            assert self.test_mode in [\'viterbi\', \'marginal\']\n        self.sparse_target = sparse_target\n        self.use_boundary = use_boundary\n        self.use_bias = use_bias\n\n        self.activation = activations.get(activation)\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.chain_initializer = initializers.get(chain_initializer)\n        self.boundary_initializer = initializers.get(boundary_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.chain_regularizer = regularizers.get(chain_regularizer)\n        self.boundary_regularizer = regularizers.get(boundary_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.chain_constraint = constraints.get(chain_constraint)\n        self.boundary_constraint = constraints.get(boundary_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.unroll = unroll\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[-1]\n\n        self.kernel = self.add_weight((self.input_dim, self.units),\n                                      name=\'kernel\',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.chain_kernel = self.add_weight((self.units, self.units),\n                                            name=\'chain_kernel\',\n                                            initializer=self.chain_initializer,\n                                            regularizer=self.chain_regularizer,\n                                            constraint=self.chain_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name=\'bias\',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        if self.use_boundary:\n            self.left_boundary = self.add_weight((self.units,),\n                                                 name=\'left_boundary\',\n                                                 initializer=self.boundary_initializer,\n                                                 regularizer=self.boundary_regularizer,\n                                                 constraint=self.boundary_constraint)\n            self.right_boundary = self.add_weight((self.units,),\n                                                  name=\'right_boundary\',\n                                                  initializer=self.boundary_initializer,\n                                                  regularizer=self.boundary_regularizer,\n                                                  constraint=self.boundary_constraint)\n        self.built = True\n\n    def call(self, X, mask=None):\n        if mask is not None:\n            assert K.ndim(mask) == 2, \'Input mask to CRF must have dim 2 if not None\'\n\n        if self.test_mode == \'viterbi\':\n            test_output = self.viterbi_decoding(X, mask)\n        else:\n            test_output = self.get_marginal_prob(X, mask)\n\n        self.uses_learning_phase = True\n        if self.learn_mode == \'join\':\n            train_output = K.zeros_like(K.dot(X, self.kernel))\n            out = K.in_train_phase(train_output, test_output)\n        else:\n            if self.test_mode == \'viterbi\':\n                train_output = self.get_marginal_prob(X, mask)\n                out = K.in_train_phase(train_output, test_output)\n            else:\n                out = test_output\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:2] + (self.units,)\n\n    def compute_mask(self, input, mask=None):\n        if mask is not None and self.learn_mode == \'join\':\n            return K.any(mask, axis=1)\n        return mask\n\n    def get_config(self):\n        config = {\'units\': self.units,\n                  \'learn_mode\': self.learn_mode,\n                  \'test_mode\': self.test_mode,\n                  \'use_boundary\': self.use_boundary,\n                  \'use_bias\': self.use_bias,\n                  \'sparse_target\': self.sparse_target,\n                  \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n                  \'chain_initializer\': initializers.serialize(self.chain_initializer),\n                  \'boundary_initializer\': initializers.serialize(self.boundary_initializer),\n                  \'bias_initializer\': initializers.serialize(self.bias_initializer),\n                  \'activation\': activations.serialize(self.activation),\n                  \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n                  \'chain_regularizer\': regularizers.serialize(self.chain_regularizer),\n                  \'boundary_regularizer\': regularizers.serialize(self.boundary_regularizer),\n                  \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n                  \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n                  \'chain_constraint\': constraints.serialize(self.chain_constraint),\n                  \'boundary_constraint\': constraints.serialize(self.boundary_constraint),\n                  \'bias_constraint\': constraints.serialize(self.bias_constraint),\n                  \'input_dim\': self.input_dim,\n                  \'unroll\': self.unroll}\n        base_config = super(CRF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @property\n    def loss_function(self):\n        if self.learn_mode == \'join\':\n            def loss(y_true, y_pred):\n                assert self._inbound_nodes, \'CRF has not connected to any layer.\'\n                assert not self._outbound_nodes, \'When learn_model=""join"", CRF must be the last layer.\'\n                if self.sparse_target:\n                    y_true = K.one_hot(K.cast(y_true[:, :, 0], \'int32\'), self.units)\n                X = self._inbound_nodes[0].input_tensors[0]\n                mask = self._inbound_nodes[0].input_masks[0]\n                nloglik = self.get_negative_log_likelihood(y_true, X, mask)\n                return nloglik\n            return loss\n        else:\n            if self.sparse_target:\n                return sparse_categorical_crossentropy\n            else:\n                return categorical_crossentropy\n\n    @property\n    def accuracy(self):\n        if self.test_mode == \'viterbi\':\n            return self.viterbi_acc\n        else:\n            return self.marginal_acc\n\n    @staticmethod\n    def _get_accuracy(y_true, y_pred, mask, sparse_target=False):\n        y_pred = K.argmax(y_pred, -1)\n        if sparse_target:\n            y_true = K.cast(y_true[:, :, 0], K.dtype(y_pred))\n        else:\n            y_true = K.argmax(y_true, -1)\n        judge = K.cast(K.equal(y_pred, y_true), K.floatx())\n        if mask is None:\n            return K.mean(judge)\n        else:\n            mask = K.cast(mask, K.floatx())\n            return K.sum(judge * mask) / K.sum(mask)\n\n    @property\n    def viterbi_acc(self):\n        def acc(y_true, y_pred):\n            X = self._inbound_nodes[0].input_tensors[0]\n            mask = self._inbound_nodes[0].input_masks[0]\n            y_pred = self.viterbi_decoding(X, mask)\n            return self._get_accuracy(y_true, y_pred, mask, self.sparse_target)\n        acc.func_name = \'viterbi_acc\'\n        return acc\n\n    @property\n    def marginal_acc(self):\n        def acc(y_true, y_pred):\n            X = self._inbound_nodes[0].input_tensors[0]\n            mask = self._inbound_nodes[0].input_masks[0]\n            y_pred = self.get_marginal_prob(X, mask)\n            return self._get_accuracy(y_true, y_pred, mask, self.sparse_target)\n        acc.func_name = \'marginal_acc\'\n        return acc\n\n    @staticmethod\n    def softmaxNd(x, axis=-1):\n        m = K.max(x, axis=axis, keepdims=True)\n        exp_x = K.exp(x - m)\n        prob_x = exp_x / K.sum(exp_x, axis=axis, keepdims=True)\n        return prob_x\n\n    @staticmethod\n    def shift_left(x, offset=1):\n        assert offset > 0\n        return K.concatenate([x[:, offset:], K.zeros_like(x[:, :offset])], axis=1)\n\n    @staticmethod\n    def shift_right(x, offset=1):\n        assert offset > 0\n        return K.concatenate([K.zeros_like(x[:, :offset]), x[:, :-offset]], axis=1)\n\n    def add_boundary_energy(self, energy, mask, start, end):\n        start = K.expand_dims(K.expand_dims(start, 0), 0)\n        end = K.expand_dims(K.expand_dims(end, 0), 0)\n        if mask is None:\n            energy = K.concatenate([energy[:, :1, :] + start, energy[:, 1:, :]], axis=1)\n            energy = K.concatenate([energy[:, :-1, :], energy[:, -1:, :] + end], axis=1)\n        else:\n            mask = K.expand_dims(K.cast(mask, K.floatx()))\n            start_mask = K.cast(K.greater(mask, self.shift_right(mask)), K.floatx())\n            end_mask = K.cast(K.greater(self.shift_left(mask), mask), K.floatx())\n            energy = energy + start_mask * start\n            energy = energy + end_mask * end\n        return energy\n\n    def get_log_normalization_constant(self, input_energy, mask, **kwargs):\n        """"""Compute logarithm of the normalization constant Z, where\n        Z = sum exp(-E) -> logZ = log sum exp(-E) =: -nlogZ\n        """"""\n        # should have logZ[:, i] == logZ[:, j] for any i, j\n        logZ = self.recursion(input_energy, mask, return_sequences=False, **kwargs)\n        return logZ[:, 0]\n\n    def get_energy(self, y_true, input_energy, mask):\n        """"""Energy = a1\' y1 + u1\' y1 + y1\' U y2 + u2\' y2 + y2\' U y3 + u3\' y3 + an\' y3\n        """"""\n        input_energy = K.sum(input_energy * y_true, 2)  # (B, T)\n        chain_energy = K.sum(K.dot(y_true[:, :-1, :], self.chain_kernel) * y_true[:, 1:, :], 2)  # (B, T-1)\n\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            chain_mask = mask[:, :-1] * mask[:, 1:]  # (B, T-1), mask[:,:-1]*mask[:,1:] makes it work with any padding\n            input_energy = input_energy * mask\n            chain_energy = chain_energy * chain_mask\n        total_energy = K.sum(input_energy, -1) + K.sum(chain_energy, -1)  # (B, )\n\n        return total_energy\n\n    def get_negative_log_likelihood(self, y_true, X, mask):\n        """"""Compute the loss, i.e., negative log likelihood (normalize by number of time steps)\n           likelihood = 1/Z * exp(-E) ->  neg_log_like = - log(1/Z * exp(-E)) = logZ + E\n        """"""\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask, self.left_boundary, self.right_boundary)\n        energy = self.get_energy(y_true, input_energy, mask)\n        logZ = self.get_log_normalization_constant(input_energy, mask, input_length=K.int_shape(X)[1])\n        nloglik = logZ + energy\n        if mask is not None:\n            nloglik = nloglik / K.sum(K.cast(mask, K.floatx()), 1)\n        else:\n            nloglik = nloglik / K.cast(K.shape(X)[1], K.floatx())\n        return nloglik\n\n    def step(self, input_energy_t, states, return_logZ=True):\n        # not in the following  `prev_target_val` has shape = (B, F)\n        # where B = batch_size, F = output feature dim\n        # Note: `i` is of float32, due to the behavior of `K.rnn`\n        prev_target_val, i, chain_energy = states[:3]\n        t = K.cast(i[0, 0], dtype=\'int32\')\n        if len(states) > 3:\n            if K.backend() == \'theano\':\n                m = states[3][:, t:(t + 2)]\n            else:\n                m = K.tf.slice(states[3], [0, t], [-1, 2])\n            input_energy_t = input_energy_t * K.expand_dims(m[:, 0])\n            chain_energy = chain_energy * K.expand_dims(K.expand_dims(m[:, 0] * m[:, 1]))  # (1, F, F)*(B, 1, 1) -> (B, F, F)\n        if return_logZ:\n            energy = chain_energy + K.expand_dims(input_energy_t - prev_target_val, 2)  # shapes: (1, B, F) + (B, F, 1) -> (B, F, F)\n            new_target_val = K.logsumexp(-energy, 1)  # shapes: (B, F)\n            return new_target_val, [new_target_val, i + 1]\n        else:\n            energy = chain_energy + K.expand_dims(input_energy_t + prev_target_val, 2)\n            min_energy = K.min(energy, 1)\n            argmin_table = K.cast(K.argmin(energy, 1), K.floatx())  # cast for tf-version `K.rnn`\n            return argmin_table, [min_energy, i + 1]\n\n    def recursion(self, input_energy, mask=None, go_backwards=False, return_sequences=True, return_logZ=True, input_length=None):\n        """"""Forward (alpha) or backward (beta) recursion\n        If `return_logZ = True`, compute the logZ, the normalization constant:\n        \\[ Z = \\sum_{y1, y2, y3} exp(-E) # energy\n          = \\sum_{y1, y2, y3} exp(-(u1\' y1 + y1\' W y2 + u2\' y2 + y2\' W y3 + u3\' y3))\n          = sum_{y2, y3} (exp(-(u2\' y2 + y2\' W y3 + u3\' y3)) sum_{y1} exp(-(u1\' y1\' + y1\' W y2))) \\]\n        Denote:\n            \\[ S(y2) := sum_{y1} exp(-(u1\' y1 + y1\' W y2)), \\]\n            \\[ Z = sum_{y2, y3} exp(log S(y2) - (u2\' y2 + y2\' W y3 + u3\' y3)) \\]\n            \\[ logS(y2) = log S(y2) = log_sum_exp(-(u1\' y1\' + y1\' W y2)) \\]\n        Note that:\n              yi\'s are one-hot vectors\n              u1, u3: boundary energies have been merged\n        If `return_logZ = False`, compute the Viterbi\'s best path lookup table.\n        """"""\n        chain_energy = self.chain_kernel\n        chain_energy = K.expand_dims(chain_energy, 0)  # shape=(1, F, F): F=num of output features. 1st F is for t-1, 2nd F for t\n        prev_target_val = K.zeros_like(input_energy[:, 0, :])  # shape=(B, F), dtype=float32\n\n        if go_backwards:\n            input_energy = K.reverse(input_energy, 1)\n            if mask is not None:\n                mask = K.reverse(mask, 1)\n\n        initial_states = [prev_target_val, K.zeros_like(prev_target_val[:, :1])]\n        constants = [chain_energy]\n\n        if mask is not None:\n            mask2 = K.cast(K.concatenate([mask, K.zeros_like(mask[:, :1])], axis=1), K.floatx())\n            constants.append(mask2)\n\n        def _step(input_energy_i, states):\n            return self.step(input_energy_i, states, return_logZ)\n\n        target_val_last, target_val_seq, _ = K.rnn(_step, input_energy, initial_states, constants=constants,\n                                                   input_length=input_length, unroll=self.unroll)\n\n        if return_sequences:\n            if go_backwards:\n                target_val_seq = K.reverse(target_val_seq, 1)\n            return target_val_seq\n        else:\n            return target_val_last\n\n    def forward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, **kwargs)\n\n    def backward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, go_backwards=True, **kwargs)\n\n    def get_marginal_prob(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask, self.left_boundary, self.right_boundary)\n        input_length = K.int_shape(X)[1]\n        alpha = self.forward_recursion(input_energy, mask=mask, input_length=input_length)\n        beta = self.backward_recursion(input_energy, mask=mask, input_length=input_length)\n        if mask is not None:\n            input_energy = input_energy * K.expand_dims(K.cast(mask, K.floatx()))\n        margin = -(self.shift_right(alpha) + input_energy + self.shift_left(beta))\n        return self.softmaxNd(margin)\n\n    def viterbi_decoding(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask, self.left_boundary, self.right_boundary)\n\n        argmin_tables = self.recursion(input_energy, mask, return_logZ=False)\n        argmin_tables = K.cast(argmin_tables, \'int32\')\n\n        # backward to find best path, `initial_best_idx` can be any, as all elements in the last argmin_table are the same\n        argmin_tables = K.reverse(argmin_tables, 1)\n        initial_best_idx = [K.expand_dims(argmin_tables[:, 0, 0])]  # matrix instead of vector is required by tf `K.rnn`\n        if K.backend() == \'theano\':\n            initial_best_idx = [K.T.unbroadcast(initial_best_idx[0], 1)]\n\n        def gather_each_row(params, indices):\n            n = K.shape(indices)[0]\n            if K.backend() == \'theano\':\n                return params[K.T.arange(n), indices]\n            else:\n                indices = K.transpose(K.stack([K.tf.range(n), indices]))\n                return K.tf.gather_nd(params, indices)\n\n        def find_path(argmin_table, best_idx):\n            next_best_idx = gather_each_row(argmin_table, best_idx[0][:, 0])\n            next_best_idx = K.expand_dims(next_best_idx)\n            if K.backend() == \'theano\':\n                next_best_idx = K.T.unbroadcast(next_best_idx, 1)\n            return next_best_idx, [next_best_idx]\n\n        _, best_paths, _ = K.rnn(find_path, argmin_tables, initial_best_idx, input_length=K.int_shape(X)[1], unroll=self.unroll)\n        best_paths = K.reverse(best_paths, 1)\n        best_paths = K.squeeze(best_paths, 2)\n\n        return K.one_hot(best_paths, self.units)\n\n\n\ndef crf_nll(y_true, y_pred):\n    """"""The negative log-likelihood for linear chain Conditional Random Field (CRF).\n    This loss function is only used when the `layers.CRF` layer\n    is trained in the ""join"" mode.\n    # Arguments\n        y_true: tensor with true targets.\n        y_pred: tensor with predicted targets.\n    # Returns\n        A scalar representing corresponding to the negative log-likelihood.\n    # Raises\n        TypeError: If CRF is not the last layer.\n    # About GitHub\n        If you open an issue or a pull request about CRF, please\n        add `cc @lzfelix` to notify Luiz Felix.\n    """"""\n\n    crf, idx = y_pred._keras_history[:2]\n    if crf._outbound_nodes:\n        raise TypeError(\'When learn_model=""join"", CRF must be the last layer.\')\n    if crf.sparse_target:\n        y_true = K.one_hot(K.cast(y_true[:, :, 0], \'int32\'), crf.units)\n    X = crf._inbound_nodes[idx].input_tensors[0]\n    mask = crf._inbound_nodes[idx].input_masks[0]\n    nloglik = crf.get_negative_log_likelihood(y_true, X, mask)\n    return nloglik\n\n\ndef crf_loss(y_true, y_pred):\n    """"""General CRF loss function depending on the learning mode.\n    # Arguments\n        y_true: tensor with true targets.\n        y_pred: tensor with predicted targets.\n    # Returns\n        If the CRF layer is being trained in the join mode, returns the negative\n        log-likelihood. Otherwise returns the categorical crossentropy implemented\n        by the underlying Keras backend.\n    # About GitHub\n        If you open an issue or a pull request about CRF, please\n        add `cc @lzfelix` to notify Luiz Felix.\n    """"""\n    crf, idx = y_pred._keras_history[:2]\n    if crf.learn_mode == \'join\':\n        return crf_nll(y_true, y_pred)\n    else:\n        if crf.sparse_target:\n            return sparse_categorical_crossentropy(y_true, y_pred)\n        else:\n            return categorical_crossentropy(y_true, y_pred)\n'"
ktrain/text/ner/anago/models.py,0,"b'""""""\nModel definition.\n""""""\nfrom ....imports import *\nfrom .... import utils as U\n#if U.is_tf_keras():\n    #from .layers import CRF\n#else:\n    #from .layers_standalone import CRF\n\n\ndef save_model(model, weights_file, params_file):\n    with open(params_file, \'w\') as f:\n        params = model.to_json()\n        json.dump(json.loads(params), f, sort_keys=True, indent=4)\n        model.save_weights(weights_file)\n\n\ndef load_model(weights_file, params_file):\n    with open(params_file) as f:\n        model = model_from_json(f.read(), custom_objects={\'CRF\': CRF})\n        model.load_weights(weights_file)\n\n    return model\n\n\nclass BiLSTMCRF(object):\n    """"""A Keras implementation of BiLSTM-CRF for sequence labeling.\n\n    References\n    --\n    Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer.\n    ""Neural Architectures for Named Entity Recognition"". Proceedings of NAACL 2016.\n    https://arxiv.org/abs/1603.01360\n    """"""\n\n    def __init__(self,\n                 num_labels,\n                 word_vocab_size,\n                 char_vocab_size=None,\n                 word_embedding_dim=100,\n                 char_embedding_dim=25,\n                 word_lstm_size=100,\n                 char_lstm_size=25,\n                 fc_dim=100,\n                 dropout=0.5,\n                 embeddings=None,\n                 use_char=True,\n                 use_crf=True,\n                 char_mask_zero=True,\n                 use_elmo=False,\n                 use_transformer_with_dim=None):\n        """"""Build a Bi-LSTM CRF model.\n\n        Args:\n            word_vocab_size (int): word vocabulary size.\n            char_vocab_size (int): character vocabulary size.\n            num_labels (int): number of entity labels.\n            word_embedding_dim (int): word embedding dimensions.\n            char_embedding_dim (int): character embedding dimensions.\n            word_lstm_size (int): character LSTM feature extractor output dimensions.\n            char_lstm_size (int): word tagger LSTM output dimensions.\n            fc_dim (int): output fully-connected layer size.\n            dropout (float): dropout rate.\n            embeddings (numpy array): word embedding matrix.\n            use_char (boolean): add char feature.\n            use_crf (boolean): use crf as last layer.\n            char_mask_zero(boolean): mask zero for character embedding (see TF2 isse #33148 and #33069)\n            use_elmo(boolean): If True, model will be configured to accept Elmo embeddings\n                               as an additional input to word and character embeddings\n            use_transformer_with_dim(int): If not None, model will be configured to accept\n                                           transformer embeddings of given dimension\n        """"""\n        super(BiLSTMCRF).__init__()\n        self._char_embedding_dim = char_embedding_dim\n        self._word_embedding_dim = word_embedding_dim\n        self._char_lstm_size = char_lstm_size\n        self._word_lstm_size = word_lstm_size\n        self._char_vocab_size = char_vocab_size\n        self._word_vocab_size = word_vocab_size\n        self._fc_dim = fc_dim\n        self._dropout = dropout\n        self._use_char = use_char\n        self._use_crf = use_crf\n        self._embeddings = embeddings\n        self._num_labels = num_labels\n        self._char_mask_zero = char_mask_zero\n        self._use_elmo = use_elmo\n        self._use_transformer_with_dim = use_transformer_with_dim\n\n\n    def build(self):\n\n        # build word embedding\n        word_ids = Input(batch_shape=(None, None), dtype=\'int32\', name=\'word_input\')\n        inputs = [word_ids]\n        embedding_list = []\n        if self._embeddings is None:\n            word_embeddings = Embedding(input_dim=self._word_vocab_size,\n                                        output_dim=self._word_embedding_dim,\n                                        mask_zero=True,\n                                        name=\'word_embedding\')(word_ids)\n        else:\n            word_embeddings = Embedding(input_dim=self._embeddings.shape[0],\n                                        output_dim=self._embeddings.shape[1],\n                                        mask_zero=True,\n                                        weights=[self._embeddings],\n                                        name=\'word_embedding\')(word_ids)\n        embedding_list.append(word_embeddings)\n\n        # build character based word embedding\n        if self._use_char:\n            char_ids = Input(batch_shape=(None, None, None), dtype=\'int32\', name=\'char_input\')\n            inputs.append(char_ids)\n            char_embeddings = Embedding(input_dim=self._char_vocab_size,\n                                        output_dim=self._char_embedding_dim,\n                                        mask_zero=self._char_mask_zero,\n                                        name=\'char_embedding\')(char_ids)\n            char_embeddings = TimeDistributed(Bidirectional(LSTM(self._char_lstm_size)))(char_embeddings)\n            embedding_list.append(char_embeddings)\n\n        # add elmo embedding\n        if self._use_elmo:\n            elmo_embeddings = Input(shape=(None, 1024), dtype=\'float32\')\n            inputs.append(elmo_embeddings)\n            embedding_list.append(elmo_embeddings)\n\n        # add transformer embedding\n        if self._use_transformer_with_dim is not None:\n            transformer_embeddings = Input(shape=(None, self._use_transformer_with_dim), dtype=\'float32\')\n            inputs.append(transformer_embeddings)\n            embedding_list.append(transformer_embeddings)\n\n\n        # concatenate embeddings\n        word_embeddings = Concatenate()(embedding_list) if len(embedding_list) > 1 else embedding_list[0]\n\n\n        # build model\n        word_embeddings = Dropout(self._dropout)(word_embeddings)\n        z = Bidirectional(LSTM(units=self._word_lstm_size, return_sequences=True))(word_embeddings)\n        z = Dense(self._fc_dim, activation=\'tanh\')(z)\n\n        if self._use_crf:\n            from .layers import CRF\n            crf = CRF(self._num_labels, sparse_target=False)\n            loss = crf.loss_function\n            pred = crf(z)\n        else:\n            loss = \'categorical_crossentropy\'\n            pred = Dense(self._num_labels, activation=\'softmax\')(z)\n\n        model = Model(inputs=inputs, outputs=pred)\n\n        return model, loss\n\n'"
ktrain/text/ner/anago/preprocessing.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nPreprocessors.\n""""""\n\nfrom ....imports import *\nfrom .... import utils as U\nfrom .utils import Vocabulary\n\n\ntry:\n    from allennlp.modules.elmo import Elmo, batch_to_ids\n    ALLENNLP_INSTALLED = True\nexcept:\n    ALLENNLP_INSTALLED = False\n\n\noptions_file = \'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\'\nweight_file = \'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\'\n\n\n\ndef normalize_number(text):\n    return re.sub(r\'[0-9\xef\xbc\x90\xef\xbc\x91\xef\xbc\x92\xef\xbc\x93\xef\xbc\x94\xef\xbc\x95\xef\xbc\x96\xef\xbc\x97\xef\xbc\x98\xef\xbc\x99]\', r\'0\', text)\n\n\nclass IndexTransformer(BaseEstimator, TransformerMixin):\n    """"""Convert a collection of raw documents to a document id matrix.\n\n    Attributes:\n        _use_char: boolean. Whether to use char feature.\n        _num_norm: boolean. Whether to normalize text.\n        _word_vocab: dict. A mapping of words to feature indices.\n        _char_vocab: dict. A mapping of chars to feature indices.\n        _label_vocab: dict. A mapping of labels to feature indices.\n    """"""\n\n    def __init__(self, lower=True, num_norm=True,\n                 use_char=True, initial_vocab=None,\n                 use_elmo=False):\n        """"""Create a preprocessor object.\n\n        Args:\n            lower: boolean. Whether to convert the texts to lowercase.\n            use_char: boolean. Whether to use char feature.\n            num_norm: boolean. Whether to normalize text.\n            initial_vocab: Iterable. Initial vocabulary for expanding word_vocab.\n            use_elmo: If True, will generate contextual English Elmo embeddings\n        """"""\n        self._num_norm = num_norm\n        self._use_char = use_char\n        self._word_vocab = Vocabulary(lower=lower)\n        self._char_vocab = Vocabulary(lower=False)\n        self._label_vocab = Vocabulary(lower=False, unk_token=False)\n\n        if initial_vocab:\n            self._word_vocab.add_documents([initial_vocab])\n            self._char_vocab.add_documents(initial_vocab)\n\n        self.elmo = None  # elmo embedding model\n        self.use_elmo = False\n        self.te = None    # transformer embedding model\n        self.te_layers = U.DEFAULT_TRANSFORMER_LAYERS\n        self.te_model = None\n        self._blacklist = [\'te\', \'elmo\']\n\n    def __getstate__(self):\n        return {k: v for k, v in self.__dict__.items() if k not in self._blacklist}\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        if not hasattr(self, \'te_model\'): self.te_model = None\n        if not hasattr(self, \'use_elmo\'): self.use_elmo = False\n        if not hasattr(self, \'te_layers\'): self.te_layers = U.DEFAULT_TRANSFORMER_LAYERS\n\n        if self.te_model is not None: self.activate_transformer(self.te_model, layers=self.te_layers)\n        else:\n            self.te = None\n        if self.use_elmo:  \n            self.activate_elmo()\n        else:\n            self.elmo = None\n\n\n    def activate_elmo(self):\n        if not ALLENNLP_INSTALLED:\n            raise Exception(ALLENNLP_ERRMSG)\n\n        if not hasattr(self, \'elmo\'): self.elmo=None\n        if self.elmo is None:\n            self.elmo = Elmo(options_file, weight_file, 2, dropout=0)\n        self.use_elmo = True\n\n    def activate_transformer(self, model_name, layers=U.DEFAULT_TRANSFORMER_LAYERS, \n                              force=False):\n        from ...preprocessor import TransformerEmbedding\n        if not hasattr(self, \'te\'): self.te = None\n        if self.te is None or self.te_model != model_name or force:\n            self.te_model = model_name\n            self.te = TransformerEmbedding(model_name, layers=layers)\n        self.te_layers = layers\n\n    def get_transformer_dim(self):\n        if not self.transformer_is_activated(): \n            return None\n        else:\n            return self.te.embsize\n\n\n    def elmo_is_activated(self):\n        return self.elmo is not None\n\n\n    def transformer_is_activated(self):\n        return self.te is not None\n\n            \n    def fix_tokenization(self, X, Y, maxlen=U.DEFAULT_TRANSFORMER_MAXLEN, num_special=U.DEFAULT_TRANSFORMER_NUM_SPECIAL):\n        """"""\n        Should be called prior training\n        """"""\n        if not self.transformer_is_activated():\n            return X, Y\n        ids2tok = self.te.tokenizer.convert_ids_to_tokens\n        encode = self.te.tokenizer.encode\n        new_X = []\n        new_Y = []\n        for i, x in enumerate(X):\n            new_x = []\n            new_y =[]\n            seq_len = 0\n            for j,s in enumerate(x):\n                subtokens = ids2tok(encode(s, add_special_tokens=False))\n                token_len = len(subtokens)\n                if seq_len + token_len > (maxlen - num_special):\n                    break\n                seq_len += token_len\n                hf_s = \' \'.join(subtokens).replace(\' ##\', \'\').split()\n                new_x.extend(hf_s)\n                if Y is not None:\n                    tag = Y[i][j]\n                    new_y.extend([tag])\n                    if len(hf_s) > 1:\n                        new_tag = tag\n                        if tag.startswith(\'B-\'): new_tag = \'I-\'+tag[2:]\n                        new_y.extend([new_tag]*(len(hf_s)-1) )\n                    #if tag.startswith(\'B-\'): tag = \'I-\'+tag[2:]\n\n            new_X.append(new_x)\n            new_Y.append(new_y)\n        new_Y = None if Y is None else new_Y\n        return new_X, new_Y\n\n\n    def fit(self, X, y):\n        """"""Learn vocabulary from training set.\n\n        Args:\n            X : iterable. An iterable which yields either str, unicode or file objects.\n\n        Returns:\n            self : IndexTransformer.\n        """"""\n        self._word_vocab.add_documents(X)\n        self._label_vocab.add_documents(y)\n        if self._use_char:\n            for doc in X:\n                self._char_vocab.add_documents(doc)\n\n        self._word_vocab.build()\n        self._char_vocab.build()\n        self._label_vocab.build()\n\n        return self\n\n\n    def transform(self, X, y=None):\n        """"""Transform documents to document ids.\n\n        Uses the vocabulary learned by fit.\n\n        Args:\n            X : iterable\n            an iterable which yields either str, unicode or file objects.\n            y : iterabl, label strings.\n\n        Returns:\n            features: document id matrix.\n            y: label id matrix.\n        """"""\n        # re-instantiate TransformerEmbedding/Elmo if necessary since it is excluded from pickling\n        if self.te_model is not None: self.activate_transformer(self.te_model, layers=self.te_layers)\n        if self.use_elmo: self.activate_elmo()\n\n\n        features = []\n\n        word_ids = [self._word_vocab.doc2id(doc) for doc in X]\n        word_ids = sequence.pad_sequences(word_ids, padding=\'post\')\n        features.append(word_ids)\n\n        if self._use_char:\n            char_ids = [[self._char_vocab.doc2id(w) for w in doc] for doc in X]\n            char_ids = pad_nested_sequences(char_ids)\n            features.append(char_ids)\n\n        if self.elmo is not None:\n            if not ALLENNLP_INSTALLED:        \n                raise Exception(ALLENNLP_ERRMSG)\n\n            character_ids = batch_to_ids(X)\n            elmo_embeddings = self.elmo(character_ids)[\'elmo_representations\'][1]\n            elmo_embeddings = elmo_embeddings.detach().numpy()\n            features.append(elmo_embeddings)\n\n        if self.te is not None:\n            transformer_embeddings = self.te.embed(X, word_level=True)\n            features.append(transformer_embeddings)\n\n\n        if y is not None:\n            y = [self._label_vocab.doc2id(doc) for doc in y]\n            y = sequence.pad_sequences(y, padding=\'post\')\n            y = to_categorical(y, self.label_size).astype(int)\n            # In 2018/06/01, to_categorical is a bit strange.\n            # >>> to_categorical([[1,3]], num_classes=4).shape\n            # (1, 2, 4)\n            # >>> to_categorical([[1]], num_classes=4).shape\n            # (1, 4)\n            # So, I expand dimensions when len(y.shape) == 2.\n            y = y if len(y.shape) == 3 else np.expand_dims(y, axis=0)\n            return features, y\n        else:\n            return features\n\n\n    def fit_transform(self, X, y=None, **params):\n        """"""Learn vocabulary and return document id matrix.\n\n        This is equivalent to fit followed by transform.\n\n        Args:\n            X : iterable\n            an iterable which yields either str, unicode or file objects.\n\n        Returns:\n            list : document id matrix.\n            list: label id matrix.\n        """"""\n        return self.fit(X, y).transform(X, y)\n\n    def inverse_transform(self, y, lengths=None):\n        """"""Return label strings.\n\n        Args:\n            y: label id matrix.\n            lengths: sentences length.\n\n        Returns:\n            list: list of list of strings.\n        """"""\n        y = np.argmax(y, -1)\n        inverse_y = [self._label_vocab.id2doc(ids) for ids in y]\n        if lengths is not None:\n            inverse_y = [iy[:l] for iy, l in zip(inverse_y, lengths)]\n\n        return inverse_y\n\n    @property\n    def word_vocab_size(self):\n        return len(self._word_vocab)\n\n    @property\n    def char_vocab_size(self):\n        return len(self._char_vocab)\n\n    @property\n    def label_size(self):\n        return len(self._label_vocab)\n\n    def save(self, file_path):\n        joblib.dump(self, file_path)\n\n    @classmethod\n    def load(cls, file_path):\n        p = joblib.load(file_path)\n\n        return p\n\n\ndef pad_nested_sequences(sequences, dtype=\'int32\'):\n    """"""Pads nested sequences to the same length.\n\n    This function transforms a list of list sequences\n    into a 3D Numpy array of shape `(num_samples, max_sent_len, max_word_len)`.\n\n    Args:\n        sequences: List of lists of lists.\n        dtype: Type of the output sequences.\n\n    # Returns\n        x: Numpy array.\n    """"""\n    max_sent_len = 0\n    max_word_len = 0\n    for sent in sequences:\n        max_sent_len = max(len(sent), max_sent_len)\n        for word in sent:\n            max_word_len = max(len(word), max_word_len)\n\n    x = np.zeros((len(sequences), max_sent_len, max_word_len)).astype(dtype)\n    for i, sent in enumerate(sequences):\n        for j, word in enumerate(sent):\n            x[i, j, :len(word)] = word\n\n    return x\n\n'"
ktrain/text/ner/anago/tagger.py,0,"b'""""""\nModel API.\n""""""\nfrom ....imports import *\n\nclass Tagger(object):\n    """"""A model API that tags input sentence.\n\n    Attributes:\n        model: Model.\n        preprocessor: Transformer. Preprocessing data for feature extraction.\n        tokenizer: Tokenize input sentence. Default tokenizer is `str.split`.\n    """"""\n\n    def __init__(self, model, preprocessor, tokenizer=str.split):\n        self.model = model\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n\n    def predict_proba(self, text):\n        """"""Probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        Args:\n            text : string, the input text.\n\n        Returns:\n            y : array-like, shape = [num_words, num_classes]\n            Returns the probability of the word for each class in the model,\n        """"""\n        assert isinstance(text, str)\n\n        words = self.tokenizer(text)\n        X = self.preprocessor.transform([words])\n        y = self.model.predict(X)\n        y = y[0]  # reduce batch dimension.\n\n        return y\n\n    def _get_prob(self, pred):\n        prob = np.max(pred, -1)\n\n        return prob\n\n    def _get_tags(self, pred):\n        tags = self.preprocessor.inverse_transform([pred])\n        tags = tags[0]  # reduce batch dimension\n\n        return tags\n\n    def _build_response(self, sent, tags, prob):\n        words = self.tokenizer(sent)\n        res = {\n            \'words\': words,\n            \'entities\': [\n\n            ]\n        }\n        chunks = get_entities(tags)\n\n        for chunk_type, chunk_start, chunk_end in chunks:\n            chunk_end += 1\n            entity = {\n                \'text\': \' \'.join(words[chunk_start: chunk_end]),\n                \'type\': chunk_type,\n                \'score\': float(np.average(prob[chunk_start: chunk_end])),\n                \'beginOffset\': chunk_start,\n                \'endOffset\': chunk_end\n            }\n            res[\'entities\'].append(entity)\n\n        return res\n\n    def analyze(self, text):\n        """"""Analyze text and return pretty format.\n\n        Args:\n            text: string, the input text.\n\n        Returns:\n            res: dict.\n\n        Examples:\n            >>> text = \'President Obama is speaking at the White House.\'\n            >>> model.analyze(text)\n            {\n                ""words"": [\n                    ""President"",\n                    ""Obama"",\n                    ""is"",\n                    ""speaking"",\n                    ""at"",\n                    ""the"",\n                    ""White"",\n                    ""House.""\n                ],\n                ""entities"": [\n                    {\n                        ""beginOffset"": 1,\n                        ""endOffset"": 2,\n                        ""score"": 1,\n                        ""text"": ""Obama"",\n                        ""type"": ""PER""\n                    },\n                    {\n                        ""beginOffset"": 6,\n                        ""endOffset"": 8,\n                        ""score"": 1,\n                        ""text"": ""White House."",\n                        ""type"": ""ORG""\n                    }\n                ]\n            }\n        """"""\n        pred = self.predict_proba(text)\n        tags = self._get_tags(pred)\n        prob = self._get_prob(pred)\n        res = self._build_response(text, tags, prob)\n\n        return res\n\n    def predict(self, text):\n        """"""Predict using the model.\n\n        Args:\n            text: string, the input text.\n\n        Returns:\n            tags: list, shape = (num_words,)\n            Returns predicted values.\n        """"""\n        pred = self.predict_proba(text)\n        tags = self._get_tags(pred)\n\n        return tags\n'"
ktrain/text/ner/anago/trainer.py,0,"b'""""""Training-related module.\n""""""\n\nfrom ....imports import *\nfrom .callbacks import F1score\nfrom .utils import AnagoNERSequence\n\n\nclass Trainer(object):\n    """"""A trainer that train the model.\n\n    Attributes:\n        _model: Model.\n        _preprocessor: Transformer. Preprocessing data for feature extraction.\n    """"""\n\n    def __init__(self, model, preprocessor=None):\n        self._model = model\n        self._preprocessor = preprocessor\n\n    def train(self, x_train, y_train, x_valid=None, y_valid=None,\n              epochs=1, batch_size=32, verbose=1, callbacks=None, shuffle=True):\n        """"""Trains the model for a fixed number of epochs (iterations on a dataset).\n\n        Args:\n            x_train: list of training data.\n            y_train: list of training target (label) data.\n            x_valid: list of validation data.\n            y_valid: list of validation target (label) data.\n            batch_size: Integer.\n                Number of samples per gradient update.\n                If unspecified, `batch_size` will default to 32.\n            epochs: Integer. Number of epochs to train the model.\n            verbose: Integer. 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = one line per epoch.\n            callbacks: List of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n            shuffle: Boolean (whether to shuffle the training data\n                before each epoch). `shuffle` will default to True.\n        """"""\n\n        train_seq = AnagoNERSequence(x_train, y_train, batch_size, self._preprocessor.transform)\n\n        if x_valid and y_valid:\n            valid_seq = AnagoNERSequence(x_valid, y_valid, batch_size, self._preprocessor.transform)\n            f1 = F1score(valid_seq, preprocessor=self._preprocessor)\n            callbacks = [f1] + callbacks if callbacks else [f1]\n\n        self._model.fit_generator(generator=train_seq,\n                                  epochs=epochs,\n                                  callbacks=callbacks,\n                                  verbose=verbose,\n                                  shuffle=shuffle)\n'"
ktrain/text/ner/anago/utils.py,0,"b'""""""\nUtility functions.\n""""""\n\n\nfrom ....imports import *\n\ndef download(url):\n    """"""Download a trained weights, config and preprocessor.\n\n    Args:\n        url (str): target url.\n    """"""\n    filepath = get_file(fname=\'tmp.zip\', origin=url, extract=True)\n    base_dir = os.path.dirname(filepath)\n    weights_file = os.path.join(base_dir, \'weights.h5\')\n    params_file = os.path.join(base_dir, \'params.json\')\n    preprocessor_file = os.path.join(base_dir, \'preprocessor.pickle\')\n\n    return weights_file, params_file, preprocessor_file\n\n\ndef load_data_and_labels(filename, encoding=\'utf-8\'):\n    """"""Loads data and label from a file.\n\n    Args:\n        filename (str): path to the file.\n        encoding (str): file encoding format.\n\n        The file format is tab-separated values.\n        A blank line is required at the end of a sentence.\n\n        For example:\n        ```\n        EU\tB-ORG\n        rejects\tO\n        German\tB-MISC\n        call\tO\n        to\tO\n        boycott\tO\n        British\tB-MISC\n        lamb\tO\n        .\tO\n\n        Peter\tB-PER\n        Blackburn\tI-PER\n        ...\n        ```\n\n    Returns:\n        tuple(numpy array, numpy array): data and labels.\n\n    Example:\n        >>> filename = \'conll2003/en/ner/train.txt\'\n        >>> data, labels = load_data_and_labels(filename)\n    """"""\n    sents, labels = [], []\n    words, tags = [], []\n    with open(filename, encoding=encoding) as f:\n        for line in f:\n            line = line.rstrip()\n            if line:\n                word, tag = line.split(\'\\t\')\n                words.append(word)\n                tags.append(tag)\n            else:\n                sents.append(words)\n                labels.append(tags)\n                words, tags = [], []\n\n    return sents, labels\n\n\nclass AnagoNERSequence(Sequence):\n\n    def __init__(self, x, y, batch_size=1, preprocess=None):\n        self.x = x\n        self.y = y\n        self.batch_size = batch_size\n        self.preprocess = preprocess\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n\n        return self.preprocess(batch_x, batch_y)\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n\nclass Vocabulary(object):\n    """"""A vocabulary that maps tokens to ints (storing a vocabulary).\n\n    Attributes:\n        _token_count: A collections.Counter object holding the frequencies of tokens\n            in the data used to build the Vocabulary.\n        _token2id: A collections.defaultdict instance mapping token strings to\n            numerical identifiers.\n        _id2token: A list of token strings indexed by their numerical identifiers.\n    """"""\n\n    def __init__(self, max_size=None, lower=True, unk_token=True, specials=(\'<pad>\',)):\n        """"""Create a Vocabulary object.\n\n        Args:\n            max_size: The maximum size of the vocabulary, or None for no\n                maximum. Default: None.\n            lower: boolean. Whether to convert the texts to lowercase.\n            unk_token: boolean. Whether to add unknown token.\n            specials: The list of special tokens (e.g., padding or eos) that\n                will be prepended to the vocabulary. Default: (\'<pad>\',)\n        """"""\n        self._max_size = max_size\n        self._lower = lower\n        self._unk = unk_token\n        self._token2id = {token: i for i, token in enumerate(specials)}\n        self._id2token = list(specials)\n        self._token_count = Counter()\n\n    def __len__(self):\n        return len(self._token2id)\n\n    def add_token(self, token):\n        """"""Add token to vocabulary.\n\n        Args:\n            token (str): token to add.\n        """"""\n        token = self.process_token(token)\n        self._token_count.update([token])\n\n    def add_documents(self, docs):\n        """"""Update dictionary from a collection of documents. Each document is a list\n        of tokens.\n\n        Args:\n            docs (list): documents to add.\n        """"""\n        for sent in docs:\n            sent = map(self.process_token, sent)\n            self._token_count.update(sent)\n\n    def doc2id(self, doc):\n        """"""Get the list of token_id given doc.\n\n        Args:\n            doc (list): document.\n\n        Returns:\n            list: int id of doc.\n        """"""\n        doc = map(self.process_token, doc)\n        return [self.token_to_id(token) for token in doc]\n\n    def id2doc(self, ids):\n        """"""Get the token list.\n\n        Args:\n            ids (list): token ids.\n\n        Returns:\n            list: token list.\n        """"""\n        return [self.id_to_token(idx) for idx in ids]\n\n    def build(self):\n        """"""\n        Build vocabulary.\n        """"""\n        token_freq = self._token_count.most_common(self._max_size)\n        idx = len(self.vocab)\n        for token, _ in token_freq:\n            self._token2id[token] = idx\n            self._id2token.append(token)\n            idx += 1\n        if self._unk:\n            unk = \'<unk>\'\n            self._token2id[unk] = idx\n            self._id2token.append(unk)\n\n    def process_token(self, token):\n        """"""Process token before following methods:\n        * add_token\n        * add_documents\n        * doc2id\n        * token_to_id\n\n        Args:\n            token (str): token to process.\n\n        Returns:\n            str: processed token string.\n        """"""\n        if self._lower:\n            token = token.lower()\n\n        return token\n\n    def token_to_id(self, token):\n        """"""Get the token_id of given token.\n\n        Args:\n            token (str): token from vocabulary.\n\n        Returns:\n            int: int id of token.\n        """"""\n        token = self.process_token(token)\n        return self._token2id.get(token, len(self._token2id) - 1)\n\n    def id_to_token(self, idx):\n        """"""token-id to token (string).\n\n        Args:\n            idx (int): token id.\n\n        Returns:\n            str: string of given token id.\n        """"""\n        return self._id2token[idx]\n\n    @property\n    def vocab(self):\n        """"""Return the vocabulary.\n\n        Returns:\n            dict: get the dict object of the vocabulary.\n        """"""\n        return self._token2id\n\n    @property\n    def reverse_vocab(self):\n        """"""Return the vocabulary as a reversed dict object.\n\n        Returns:\n            dict: reversed vocabulary object.\n        """"""\n        return self._id2token\n\n\ndef filter_embeddings(embeddings, vocab, dim):\n    """"""Loads word vectors in numpy array.\n\n    Args:\n        embeddings (dict): a dictionary of numpy array.\n        vocab (dict): word_index lookup table.\n\n    Returns:\n        numpy array: an array of word embeddings.\n    """"""\n    if not isinstance(embeddings, dict):\n        return\n    _embeddings = np.zeros([len(vocab), dim])\n    for word in vocab:\n        if word in embeddings:\n            word_idx = vocab[word]\n            _embeddings[word_idx] = embeddings[word]\n\n    return _embeddings\n\n\ndef load_glove(file):\n    """"""Loads GloVe vectors in numpy array.\n\n    Args:\n        file (str): a path to a glove file.\n\n    Return:\n        dict: a dict of numpy arrays.\n    """"""\n    model = {}\n    with open(file) as f:\n        for line in f:\n            line = line.split(\' \')\n            word = line[0]\n            vector = np.array([float(val) for val in line[1:]])\n            model[word] = vector\n\n    return model\n'"
ktrain/text/ner/anago/wrapper.py,0,"b'""""""\nWrapper class.\n""""""\n\nfrom ....imports import *\nfrom .models import BiLSTMCRF, save_model, load_model\nfrom .preprocessing import IndexTransformer\nfrom .tagger import Tagger\nfrom .trainer import Trainer\nfrom .utils import filter_embeddings\n\n\nclass Sequence(object):\n\n    def __init__(self,\n                 word_embedding_dim=100,\n                 char_embedding_dim=25,\n                 word_lstm_size=100,\n                 char_lstm_size=25,\n                 fc_dim=100,\n                 dropout=0.5,\n                 embeddings=None,\n                 use_char=True,\n                 use_crf=True,\n                 initial_vocab=None,\n                 optimizer=\'adam\'):\n\n        self.model = None\n        self.p = None\n        self.tagger = None\n\n        self.word_embedding_dim = word_embedding_dim\n        self.char_embedding_dim = char_embedding_dim\n        self.word_lstm_size = word_lstm_size\n        self.char_lstm_size = char_lstm_size\n        self.fc_dim = fc_dim\n        self.dropout = dropout\n        self.embeddings = embeddings\n        self.use_char = use_char\n        self.use_crf = use_crf\n        self.initial_vocab = initial_vocab\n        self.optimizer = optimizer\n\n    def fit(self, x_train, y_train, x_valid=None, y_valid=None,\n            epochs=1, batch_size=32, verbose=1, callbacks=None, shuffle=True):\n        """"""Fit the model for a fixed number of epochs.\n\n        Args:\n            x_train: list of training data.\n            y_train: list of training target (label) data.\n            x_valid: list of validation data.\n            y_valid: list of validation target (label) data.\n            batch_size: Integer.\n                Number of samples per gradient update.\n                If unspecified, `batch_size` will default to 32.\n            epochs: Integer. Number of epochs to train the model.\n            verbose: Integer. 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = one line per epoch.\n            callbacks: List of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n            shuffle: Boolean (whether to shuffle the training data\n                before each epoch). `shuffle` will default to True.\n        """"""\n        p = IndexTransformer(initial_vocab=self.initial_vocab, use_char=self.use_char)\n        p.fit(x_train, y_train)\n        embeddings = filter_embeddings(self.embeddings, p._word_vocab.vocab, self.word_embedding_dim)\n\n        model = BiLSTMCRF(char_vocab_size=p.char_vocab_size,\n                          word_vocab_size=p.word_vocab_size,\n                          num_labels=p.label_size,\n                          word_embedding_dim=self.word_embedding_dim,\n                          char_embedding_dim=self.char_embedding_dim,\n                          word_lstm_size=self.word_lstm_size,\n                          char_lstm_size=self.char_lstm_size,\n                          fc_dim=self.fc_dim,\n                          dropout=self.dropout,\n                          embeddings=embeddings,\n                          use_char=self.use_char,\n                          use_crf=self.use_crf)\n        model, loss = model.build()\n        model.compile(loss=loss, optimizer=self.optimizer)\n\n        trainer = Trainer(model, preprocessor=p)\n        trainer.train(x_train, y_train, x_valid, y_valid,\n                      epochs=epochs, batch_size=batch_size,\n                      verbose=verbose, callbacks=callbacks,\n                      shuffle=shuffle)\n\n        self.p = p\n        self.model = model\n\n    def score(self, x_test, y_test):\n        """"""Returns the f1-micro score on the given test data and labels.\n\n        Args:\n            x_test : array-like, shape = (n_samples, sent_length)\n            Test samples.\n\n            y_test : array-like, shape = (n_samples, sent_length)\n            True labels for x.\n\n        Returns:\n            score : float, f1-micro score.\n        """"""\n        if self.model:\n            x_test = self.p.transform(x_test)\n            lengths = map(len, y_test)\n            y_pred = self.model.predict(x_test)\n            y_pred = self.p.inverse_transform(y_pred, lengths)\n            score = ner_f1_score(y_test, y_pred)\n            return score\n        else:\n            raise OSError(\'Could not find a model. Call load(dir_path).\')\n\n    def analyze(self, text, tokenizer=str.split):\n        """"""Analyze text and return pretty format.\n\n        Args:\n            text: string, the input text.\n            tokenizer: Tokenize input sentence. Default tokenizer is `str.split`.\n\n        Returns:\n            res: dict.\n        """"""\n        if not self.tagger:\n            self.tagger = Tagger(self.model,\n                                 preprocessor=self.p,\n                                 tokenizer=tokenizer)\n\n        return self.tagger.analyze(text)\n\n    def save(self, weights_file, params_file, preprocessor_file):\n        self.p.save(preprocessor_file)\n        save_model(self.model, weights_file, params_file)\n\n    @classmethod\n    def load(cls, weights_file, params_file, preprocessor_file):\n        self = cls()\n        self.p = IndexTransformer.load(preprocessor_file)\n        self.model = load_model(weights_file, params_file)\n\n        return self\n'"
ktrain/text/shallownlp/install/setup.py,0,"b""from distutils.core import setup\nimport setuptools\n\nwith open('README.md') as readme_file: \n    readme_file.readline()\n    readme = readme_file.read()\nexec(open('shallownlp/version.py').read())\n\n\nsetup(\n  name = 'shallownlp',\n  packages = setuptools.find_packages(),\n  version = __version__,\n  license='MIT',\n  description = 'shallownlp is an easy-to-apply text analysis library for English and Chinese',\n  long_description = readme,\n  long_description_content_type = 'text/markdown',\n  author = 'Arun S. Maiya',\n  author_email = 'arun@maiya.net',\n  url = 'https://github.com/amaiya/ktrain',\n  keywords = ['nlp', 'text analytics', 'machine learning'],\n  install_requires=[\n          'scikit-learn == 0.21.3',\n          'langdetect',\n          'jieba',\n          'cchardet',\n          'syntok'\n      ],\n  classifiers=[  # Optional\n    # How mature is this project? Common values are\n    #   3 - Alpha\n    #   4 - Beta\n    #   5 - Production/Stable\n    'Development Status :: 4 - Beta',\n\n    # Indicate who your project is intended for\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n\n    # Pick your license as you wish\n    'License :: OSI Approved :: MIT License',\n\n    # Specify the Python versions you support here. In particular, ensure\n    # that you indicate whether you support Python 2, Python 3 or both.\n    'Programming Language :: Python :: 3.6',\n    'Programming Language :: Python :: 3.7',\n  ],\n)\n"""
ktrain/text/shallownlp/tests/test_shallownlp.py,0,"b'#!/usr/bin/env python3\n""""""\nTests of ktrain text classification flows\n""""""\nfrom unittest import TestCase, main, skip\nimport numpy as np\nimport sys\nsys.path.insert(0,\'../..\')\nimport os\nos.environ[\'DISABLE_V2_BEHAVIOR\'] = \'1\'\nimport shallownlp as snlp\n\nclass TestShalloNLP(TestCase):\n\n\n\n    #@skip(\'temporarily disabled\')\n    def test_classifier(self):\n        categories = [\'alt.atheism\', \'soc.religion.christian\',\n                     \'comp.graphics\', \'sci.med\']\n        from sklearn.datasets import fetch_20newsgroups\n        train_b = fetch_20newsgroups(subset=\'train\',\n           categories=categories, shuffle=True, random_state=42)\n        test_b = fetch_20newsgroups(subset=\'test\',\n           categories=categories, shuffle=True, random_state=42)\n        print(\'size of training set: %s\' % (len(train_b[\'data\'])))\n        print(\'size of validation set: %s\' % (len(test_b[\'data\'])))\n        print(\'classes: %s\' % (train_b.target_names))\n        x_train = train_b.data\n        y_train = train_b.target\n        x_test = test_b.data\n        y_test = test_b.target\n        classes = train_b.target_names\n\n        clf = snlp.Classifier()\n        clf.fit(x_train, y_train, ctype=\'nbsvm\')\n        self.assertGreaterEqual(clf.evaluate(x_test, y_test), 0.93)\n        test_doc = \'god christ jesus mother mary church sunday lord heaven amen\'\n        self.assertEqual(clf.predict(test_doc), 3)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_classifier_chinese(self):\n        fpath = \'../../..//tests/text_data/chinese_hotel_reviews.csv\'\n        (x_train,  y_train, label_names) = snlp.Classifier.texts_from_csv(fpath, text_column=\'content\', label_column=\'pos\', sep=\'|\')\n        print(\'label names: %s\' % (label_names))\n        clf = snlp.Classifier()\n        clf.fit(x_train, y_train, ctype=\'nbsvm\')\n        self.assertGreaterEqual(clf.evaluate(x_train, y_train), 0.98)\n        neg_text = \'\xe6\x88\x91\xe8\xae\xa8\xe5\x8e\x8c\xe5\x92\x8c\xe9\x84\x99\xe8\xa7\x86\xe8\xbf\x99\xe5\xae\xb6\xe9\x85\x92\xe5\xba\x97\xe3\x80\x82\'\n        pos_text = \'\xe6\x88\x91\xe5\x96\x9c\xe6\xac\xa2\xe8\xbf\x99\xe5\xae\xb6\xe9\x85\x92\xe5\xba\x97\xe3\x80\x82\'\n        self.assertEqual(clf.predict(pos_text), 1)\n        self.assertEqual(clf.predict(neg_text), 0)\n\n\n    #@skip(\'temporarily disabled\')\n    def test_ner(self):\n        ner = snlp.NER(\'en\')\n        text = """"""\n        Xuetao Cao was head of the Chinese Academy of Medical Sciences and is \n        the current president of Nankai University.\n        """"""\n        result = ner.predict(text)\n        self.assertEqual(len(result), 3)\n        self.assertEqual(result[0][1], \'PER\')\n        self.assertEqual(result[1][1], \'ORG\')\n        self.assertEqual(result[2][1], \'ORG\')\n        self.assertEqual(len(snlp.sent_tokenize(\'Paul Newman is a good actor.  Tommy Wisseau is not.\')), 2)\n\n\n        ner = snlp.NER(\'zh\')\n        text = """"""\n        \xe6\x9b\xb9\xe9\x9b\xaa\xe6\xb6\x9b\xe6\x9b\xbe\xe4\xbb\xbb\xe4\xb8\xad\xe5\x9b\xbd\xe5\x8c\xbb\xe5\xad\xa6\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe9\x99\xa2\xe9\x95\xbf\xef\xbc\x8c\xe7\x8e\xb0\xe4\xbb\xbb\xe5\x8d\x97\xe5\xbc\x80\xe5\xa4\xa7\xe5\xad\xa6\xe6\xa0\xa1\xe9\x95\xbf\xe3\x80\x82\n        """"""\n        result = ner.predict(text)\n        self.assertEqual(len(result), 3)\n        self.assertEqual(result[0][1], \'PER\')\n        self.assertEqual(result[1][1], \'ORG\')\n        self.assertEqual(result[2][1], \'ORG\')\n        self.assertEqual(len(snlp.sent_tokenize(\'\xe8\xbf\x99\xe6\x98\xaf\xe5\x85\xb3\xe4\xba\x8e\xe5\x8f\xb2\xe5\xaf\x86\xe6\x96\xaf\xe5\x8d\x9a\xe5\xa3\xab\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\xe3\x80\x82\xe7\xac\xac\xe4\xba\x8c\xe5\x8f\xa5\xe8\xaf\x9d\xe6\x98\xaf\xe5\x85\xb3\xe4\xba\x8e\xe7\x90\xbc\xe6\x96\xaf\xe5\x85\x88\xe7\x94\x9f\xe7\x9a\x84\xe3\x80\x82\')), 2)\n\n\n\n    def test_search(self):\n        document1 =""""""\n        Hello there,\n\n        Hope this email finds you well.\n\n        Are you available to talk about our meeting?\n\n        If so, let us plan to schedule the meeting\n        at the Hefei National Laboratory for Physical Sciences at the Microscale.\n\n        As I always say: \xd0\xb6\xd0\xb8\xd0\xb2\xd0\xb8 \xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe\xd0\xb4\xd0\xbd\xd1\x8f \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xb2\xd1\x82\xd1\x80\xd0\xb0\n\n        Sincerely,\n        John Doe\n        \xe5\x90\x88\xe8\x82\xa5\xe5\xbe\xae\xe5\xb0\xba\xe5\xba\xa6\xe5\x9b\xbd\xe5\xae\xb6\xe7\x89\xa9\xe7\x90\x86\xe7\xa7\x91\xe5\xad\xa6\xe5\xae\x9e\xe9\xaa\x8c\xe5\xae\xa4\n        """"""\n\n        document2 =""""""\n        This is a random document with Arabic about our meeting.\n\n        \xd8\xb9\xd8\xb4 \xd8\xa7\xd9\x84\xd9\x8a\xd9\x88\xd9\x85 \xd8\xa7\xd9\x84\xd8\xa3\xd9\x85\xd9\x84 \xd9\x84\xd9\x8a\xd9\x88\xd9\x85 \xd8\xba\xd8\xaf\n\n        Bye for now.\n        """"""\n\n        docs = [document1, document2]\n\n        result = snlp.search([\'physical sciences\', \'meeting\', \'Arabic\'], docs, keys=[\'doc1\', \'doc2\'])\n        self.assertEqual(len(result), 4)\n        self.assertEqual(result[0][2], 1)\n        self.assertEqual(result[1][2], 2)\n        self.assertEqual(result[2][1], \'meeting\')\n        self.assertEqual(result[3][1], \'Arabic\')\n\n        result = snlp.search(\'\xe5\x90\x88\xe8\x82\xa5\xe5\xbe\xae\xe5\xb0\xba\xe5\xba\xa6\xe5\x9b\xbd\xe5\xae\xb6\xe7\x89\xa9\xe7\x90\x86\xe7\xa7\x91\xe5\xad\xa6\xe5\xae\x9e\xe9\xaa\x8c\xe5\xae\xa4\', docs, keys=[\'doc1\', \'doc2\'])\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0][2], 7)\n\n        result = snlp.search(\'\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe\xd0\xb4\xd0\xbd\xd1\x8f \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xb2\xd1\x82\xd1\x80\xd0\xb0\', docs, keys=[\'doc1\', \'doc2\'])\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0][2], 1)\n\n\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
