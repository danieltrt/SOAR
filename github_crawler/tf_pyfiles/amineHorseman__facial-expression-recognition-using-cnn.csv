file_path,api_count,code
convert_fer2013_to_images_and_landmarks.py,0,"b'import numpy as np\nimport pandas as pd\nimport os\nimport argparse\nimport errno\nimport scipy.misc\nimport dlib\nimport cv2\n\nfrom skimage.feature import hog\n\n# initialization\nimage_height = 48\nimage_width = 48\nwindow_size = 24\nwindow_step = 6\nONE_HOT_ENCODING = True\nSAVE_IMAGES = False\nGET_LANDMARKS = False\nGET_HOG_FEATURES = False\nGET_HOG_IMAGES = False\nGET_HOG_WINDOWS_FEATURES = False\nSELECTED_LABELS = []\nIMAGES_PER_LABEL = 500\nOUTPUT_FOLDER_NAME = ""fer2013_features""\n\n# parse arguments and initialize variables:\nparser = argparse.ArgumentParser()\nparser.add_argument(""-j"", ""--jpg"", default=""no"", help=""save images as .jpg files"")\nparser.add_argument(""-l"", ""--landmarks"", default=""yes"", help=""extract Dlib Face landmarks"")\nparser.add_argument(""-ho"", ""--hog"", default=""yes"", help=""extract HOG features"")\nparser.add_argument(""-hw"", ""--hog_windows"", default=""yes"", help=""extract HOG features from a sliding window"")\nparser.add_argument(""-hi"", ""--hog_images"", default=""no"", help=""extract HOG images"")\nparser.add_argument(""-o"", ""--onehot"", default=""yes"", help=""one hot encoding"")\nparser.add_argument(""-e"", ""--expressions"", default=""0,1,2,3,4,5,6"", help=""choose the faciale expression you want to use: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral"")\nargs = parser.parse_args()\nif args.jpg == ""yes"":\n    SAVE_IMAGES = True\nif args.landmarks == ""yes"":\n    GET_LANDMARKS = True\nif args.hog == ""yes"":\n    GET_HOG_FEATURES = True\nif args.hog_windows == ""yes"":\n    GET_HOG_WINDOWS_FEATURES = True\nif args.hog_images == ""yes"":\n    GET_HOG_IMAGES = True\nif args.onehot == ""yes"":\n    ONE_HOT_ENCODING = True\nif args.expressions != """":\n    expressions  = args.expressions.split("","")\n    for i in range(0,len(expressions)):\n        label = int(expressions[i])\n        if (label >=0 and label<=6 ):\n            SELECTED_LABELS.append(label)\nif SELECTED_LABELS == []:\n    SELECTED_LABELS = [0,1,2,3,4,5,6]\nprint( str(len(SELECTED_LABELS)) + "" expressions"")\n\n# loading Dlib predictor and preparing arrays:\nprint( ""preparing"")\npredictor = dlib.shape_predictor(\'shape_predictor_68_face_landmarks.dat\')\noriginal_labels = [0, 1, 2, 3, 4, 5, 6]\nnew_labels = list(set(original_labels) & set(SELECTED_LABELS))\nnb_images_per_label = list(np.zeros(len(new_labels), \'uint8\'))\ntry:\n    os.makedirs(OUTPUT_FOLDER_NAME)\nexcept OSError as e:\n    if e.errno == errno.EEXIST and os.path.isdir(OUTPUT_FOLDER_NAME):\n        pass\n    else:\n        raise\n\ndef get_landmarks(image, rects):\n    # this function have been copied from http://bit.ly/2cj7Fpq\n    if len(rects) > 1:\n        raise BaseException(""TooManyFaces"")\n    if len(rects) == 0:\n        raise BaseException(""NoFaces"")\n    return np.matrix([[p.x, p.y] for p in predictor(image, rects[0]).parts()])\n\ndef get_new_label(label, one_hot_encoding=False):\n    if one_hot_encoding:\n        new_label = new_labels.index(label)\n        label = list(np.zeros(len(new_labels), \'uint8\'))\n        label[new_label] = 1\n        return label\n    else:\n        return new_labels.index(label)\n\ndef sliding_hog_windows(image):\n    hog_windows = []\n    for y in range(0, image_height, window_step):\n        for x in range(0, image_width, window_step):\n            window = image[y:y+window_size, x:x+window_size]\n            hog_windows.extend(hog(window, orientations=8, pixels_per_cell=(8, 8),\n                                            cells_per_block=(1, 1), visualise=False))\n    return hog_windows\n\nprint( ""importing csv file"")\ndata = pd.read_csv(\'fer2013.csv\')\n\nfor category in data[\'Usage\'].unique():\n    print( ""converting set: "" + category + ""..."")\n    # create folder\n    if not os.path.exists(category):\n        try:\n            os.makedirs(OUTPUT_FOLDER_NAME + \'/\' + category)\n        except OSError as e:\n            if e.errno == errno.EEXIST and os.path.isdir(OUTPUT_FOLDER_NAME):\n               pass\n            else:\n                raise\n    \n    # get samples and labels of the actual category\n    category_data = data[data[\'Usage\'] == category]\n    samples = category_data[\'pixels\'].values\n    labels = category_data[\'emotion\'].values\n    \n    # get images and extract features\n    images = []\n    labels_list = []\n    landmarks = []\n    hog_features = []\n    hog_images = []\n    for i in range(len(samples)):\n        try:\n            if labels[i] in SELECTED_LABELS and nb_images_per_label[get_new_label(labels[i])] < IMAGES_PER_LABEL:\n                image = np.fromstring(samples[i], dtype=int, sep="" "").reshape((image_height, image_width))\n                images.append(image)\n                if SAVE_IMAGES:\n                    scipy.misc.imsave(category + \'/\' + str(i) + \'.jpg\', image)\n                if GET_HOG_WINDOWS_FEATURES:\n                    features = sliding_hog_windows(image)\n                    f, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n                                            cells_per_block=(1, 1), visualise=True)\n                    hog_features.append(features)\n                    if GET_HOG_IMAGES:\n                        hog_images.append(hog_image)\n                elif GET_HOG_FEATURES:\n                    features, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n                                            cells_per_block=(1, 1), visualise=True)\n                    hog_features.append(features)\n                    if GET_HOG_IMAGES:\n                        hog_images.append(hog_image)\n                if GET_LANDMARKS:\n                    scipy.misc.imsave(\'temp.jpg\', image)\n                    image2 = cv2.imread(\'temp.jpg\')\n                    face_rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]\n                    face_landmarks = get_landmarks(image2, face_rects)\n                    landmarks.append(face_landmarks)            \n                labels_list.append(get_new_label(labels[i], one_hot_encoding=ONE_HOT_ENCODING))\n                nb_images_per_label[get_new_label(labels[i])] += 1\n        except Exception as e:\n            print( ""error in image: "" + str(i) + "" - "" + str(e))\n\n    np.save(OUTPUT_FOLDER_NAME + \'/\' + category + \'/images.npy\', images)\n    if ONE_HOT_ENCODING:\n        np.save(OUTPUT_FOLDER_NAME + \'/\' + category + \'/labels.npy\', labels_list)\n    else:\n        np.save(OUTPUT_FOLDER_NAME + \'/\' + category + \'/labels.npy\', labels_list)\n    if GET_LANDMARKS:\n        np.save(OUTPUT_FOLDER_NAME + \'/\' + category + \'/landmarks.npy\', landmarks)\n    if GET_HOG_FEATURES or GET_HOG_WINDOWS_FEATURES:\n        np.save(OUTPUT_FOLDER_NAME + \'/\' + category + \'/hog_features.npy\', hog_features)\n        if GET_HOG_IMAGES:\n            np.save(OUTPUT_FOLDER_NAME + \'/\' + category + \'/hog_images.npy\', hog_images)\n'"
data_loader.py,0,"b'""""""\n@AmineHorseman\nSep, 1st, 2016\n""""""\nfrom parameters import DATASET, NETWORK\nimport numpy as np\n\ndef load_data(validation=False, test=False):\n    \n    data_dict = dict()\n    validation_dict = dict()\n    test_dict = dict()\n\n    if DATASET.name == ""Fer2013"":\n\n        # load train set\n        data_dict[\'X\'] = np.load(DATASET.train_folder + \'/images.npy\')\n        data_dict[\'X\'] = data_dict[\'X\'].reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n        if NETWORK.use_landmarks:\n            data_dict[\'X2\'] = np.load(DATASET.train_folder + \'/landmarks.npy\')\n        if NETWORK.use_hog_and_landmarks:\n            data_dict[\'X2\'] = np.load(DATASET.train_folder + \'/landmarks.npy\')\n            data_dict[\'X2\'] = np.array([x.flatten() for x in data_dict[\'X2\']])\n            data_dict[\'X2\'] = np.concatenate((data_dict[\'X2\'], np.load(DATASET.train_folder + \'/hog_features.npy\')), axis=1)\n        data_dict[\'Y\'] = np.load(DATASET.train_folder + \'/labels.npy\')\n        if DATASET.trunc_trainset_to > 0:\n            data_dict[\'X\'] = data_dict[\'X\'][0:DATASET.trunc_trainset_to, :, :]\n            if NETWORK.use_landmarks and NETWORK.use_hog_and_landmarks:\n                data_dict[\'X2\'] = data_dict[\'X2\'][0:DATASET.trunc_trainset_to, :]\n            elif NETWORK.use_landmarks:\n                data_dict[\'X2\'] = data_dict[\'X2\'][0:DATASET.trunc_trainset_to, :, :]\n            data_dict[\'Y\'] = data_dict[\'Y\'][0:DATASET.trunc_trainset_to, :]\n\n        if validation:\n            # load validation set\n            validation_dict[\'X\'] = np.load(DATASET.validation_folder + \'/images.npy\')\n            validation_dict[\'X\'] = validation_dict[\'X\'].reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n            if NETWORK.use_landmarks:\n                validation_dict[\'X2\'] = np.load(DATASET.validation_folder + \'/landmarks.npy\')\n            if NETWORK.use_hog_and_landmarks:\n                validation_dict[\'X2\'] = np.load(DATASET.validation_folder + \'/landmarks.npy\')\n                validation_dict[\'X2\'] = np.array([x.flatten() for x in validation_dict[\'X2\']])\n                validation_dict[\'X2\'] = np.concatenate((validation_dict[\'X2\'], np.load(DATASET.validation_folder + \'/hog_features.npy\')), axis=1)\n            validation_dict[\'Y\'] = np.load(DATASET.validation_folder + \'/labels.npy\')\n            if DATASET.trunc_validationset_to > 0:\n                validation_dict[\'X\'] = validation_dict[\'X\'][0:DATASET.trunc_validationset_to, :, :]\n                if NETWORK.use_landmarks and NETWORK.use_hog_and_landmarks:\n                    validation_dict[\'X2\'] = validation_dict[\'X2\'][0:DATASET.trunc_validationset_to, :]\n                elif NETWORK.use_landmarks:\n                    validation_dict[\'X2\'] = validation_dict[\'X2\'][0:DATASET.trunc_validationset_to, :, :]\n                validation_dict[\'Y\'] = validation_dict[\'Y\'][0:DATASET.trunc_validationset_to, :]\n        \n        if test:\n            # load test set\n            test_dict[\'X\'] = np.load(DATASET.test_folder + \'/images.npy\')\n            test_dict[\'X\'] = test_dict[\'X\'].reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n            if NETWORK.use_landmarks:\n                test_dict[\'X2\'] = np.load(DATASET.test_folder + \'/landmarks.npy\')\n            if NETWORK.use_hog_and_landmarks:\n                test_dict[\'X2\'] = np.load(DATASET.test_folder + \'/landmarks.npy\')\n                test_dict[\'X2\'] = np.array([x.flatten() for x in test_dict[\'X2\']])\n                test_dict[\'X2\'] = np.concatenate((test_dict[\'X2\'], np.load(DATASET.test_folder + \'/hog_features.npy\')), axis=1)\n            test_dict[\'Y\'] = np.load(DATASET.test_folder + \'/labels.npy\')\n            if DATASET.trunc_testset_to > 0:\n                test_dict[\'X\'] = test_dict[\'X\'][0:DATASET.trunc_testset_to, :, :]\n                if NETWORK.use_landmarks and NETWORK.use_hog_and_landmarks:\n                    test_dict[\'X2\'] = test_dict[\'X2\'][0:DATASET.trunc_testset_to, :]\n                elif NETWORK.use_landmarks:\n                    test_dict[\'X2\'] = test_dict[\'X2\'][0:DATASET.trunc_testset_to, :, :]\n                test_dict[\'Y\'] = test_dict[\'Y\'][0:DATASET.trunc_testset_to, :]\n\n        if not validation and not test:\n            return data_dict\n        elif not test:\n            return data_dict, validation_dict\n        else: \n            return data_dict, validation_dict, test_dict\n    else:\n        print( ""Unknown dataset"")\n        exit()'"
model.py,0,"b'""""""\n@AmineHorseman\nSep, 1st, 2016\n""""""\nimport tensorflow as tf \nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.merge_ops import merge_outputs, merge\nfrom tflearn.layers.normalization import local_response_normalization, batch_normalization\nfrom tflearn.layers.estimator import regression \nfrom tflearn.optimizers import Momentum, Adam\n\nfrom parameters import NETWORK, HYPERPARAMS\n\ndef build_model(optimizer=HYPERPARAMS.optimizer, optimizer_param=HYPERPARAMS.optimizer_param, \n    learning_rate=HYPERPARAMS.learning_rate, keep_prob=HYPERPARAMS.keep_prob,\n    learning_rate_decay=HYPERPARAMS.learning_rate_decay, decay_step=HYPERPARAMS.decay_step):\n\n    if NETWORK.model == \'A\':\n        return build_modelA(optimizer, optimizer_param, learning_rate, keep_prob, learning_rate_decay, decay_step)\n    elif NETWORK.model == \'B\':\n        return build_modelB(optimizer, optimizer_param, learning_rate, keep_prob, learning_rate_decay, decay_step)\n    else:\n        print( ""ERROR: no model "" + str(NETWORK.model))\n        exit()\n\ndef build_modelB(optimizer=HYPERPARAMS.optimizer, optimizer_param=HYPERPARAMS.optimizer_param, \n    learning_rate=HYPERPARAMS.learning_rate, keep_prob=HYPERPARAMS.keep_prob,\n    learning_rate_decay=HYPERPARAMS.learning_rate_decay, decay_step=HYPERPARAMS.decay_step):\n\n    images_network = input_data(shape=[None, NETWORK.input_size, NETWORK.input_size, 1], name=\'input1\')\n    images_network = conv_2d(images_network, 64, 3, activation=NETWORK.activation)\n    #images_network = local_response_normalization(images_network)\n    if NETWORK.use_batchnorm_after_conv_layers:\n        images_network = batch_normalization(images_network)\n    images_network = max_pool_2d(images_network, 3, strides = 2)\n    images_network = conv_2d(images_network, 128, 3, activation=NETWORK.activation)\n    if NETWORK.use_batchnorm_after_conv_layers:\n        images_network = batch_normalization(images_network)\n    images_network = max_pool_2d(images_network, 3, strides = 2)\n    images_network = conv_2d(images_network, 256, 3, activation=NETWORK.activation)\n    if NETWORK.use_batchnorm_after_conv_layers:\n        images_network = batch_normalization(images_network)\n    images_network = max_pool_2d(images_network, 3, strides = 2)\n    images_network = dropout(images_network, keep_prob=keep_prob)\n    images_network = fully_connected(images_network, 4096, activation=NETWORK.activation)\n    images_network = dropout(images_network, keep_prob=keep_prob)\n    images_network = fully_connected(images_network, 1024, activation=NETWORK.activation)\n    if NETWORK.use_batchnorm_after_fully_connected_layers:\n        images_network = batch_normalization(images_network)\n\n    if NETWORK.use_landmarks or NETWORK.use_hog_and_landmarks:\n        if NETWORK.use_hog_sliding_window_and_landmarks:\n            landmarks_network = input_data(shape=[None, 2728], name=\'input2\')\n        elif NETWORK.use_hog_and_landmarks:\n            landmarks_network = input_data(shape=[None, 208], name=\'input2\')\n        else:\n            landmarks_network = input_data(shape=[None, 68, 2], name=\'input2\')\n        landmarks_network = fully_connected(landmarks_network, 1024, activation=NETWORK.activation)\n        if NETWORK.use_batchnorm_after_fully_connected_layers:\n            landmarks_network = batch_normalization(landmarks_network)\n        landmarks_network = fully_connected(landmarks_network, 128, activation=NETWORK.activation)\n        if NETWORK.use_batchnorm_after_fully_connected_layers:\n            landmarks_network = batch_normalization(landmarks_network)\n        images_network = fully_connected(images_network, 128, activation=NETWORK.activation)\n        network = merge([images_network, landmarks_network], \'concat\', axis=1)\n    else:\n        network = images_network\n    network = fully_connected(network, NETWORK.output_size, activation=\'softmax\')\n\n    if optimizer == \'momentum\':\n        optimizer = Momentum(learning_rate=learning_rate, momentum=optimizer_param, \n                    lr_decay=learning_rate_decay, decay_step=decay_step)\n    elif optimizer == \'adam\':\n        optimizer = Adam(learning_rate=learning_rate, beta1=optimizer_param, beta2=learning_rate_decay)\n    else:\n        print( ""Unknown optimizer: {}"".format(optimizer))\n    network = regression(network, optimizer=optimizer, loss=NETWORK.loss, learning_rate=learning_rate, name=\'output\')\n\n    return network\n\ndef build_modelA(optimizer=HYPERPARAMS.optimizer, optimizer_param=HYPERPARAMS.optimizer_param, \n    learning_rate=HYPERPARAMS.learning_rate, keep_prob=HYPERPARAMS.keep_prob,\n    learning_rate_decay=HYPERPARAMS.learning_rate_decay, decay_step=HYPERPARAMS.decay_step):\n\n    images_network = input_data(shape=[None, NETWORK.input_size, NETWORK.input_size, 1], name=\'input1\')\n    images_network = conv_2d(images_network, 64, 5, activation=NETWORK.activation)\n    #images_network = local_response_normalization(images_network)\n    if NETWORK.use_batchnorm_after_conv_layers:\n        images_network = batch_normalization(images_network)\n    images_network = max_pool_2d(images_network, 3, strides = 2)\n    images_network = conv_2d(images_network, 64, 5, activation=NETWORK.activation)\n    if NETWORK.use_batchnorm_after_conv_layers:\n        images_network = batch_normalization(images_network)\n    images_network = max_pool_2d(images_network, 3, strides = 2)\n    images_network = conv_2d(images_network, 128, 4, activation=NETWORK.activation)\n    if NETWORK.use_batchnorm_after_conv_layers:\n        images_network = batch_normalization(images_network)\n    images_network = dropout(images_network, keep_prob=keep_prob)\n    images_network = fully_connected(images_network, 1024, activation=NETWORK.activation)\n    if NETWORK.use_batchnorm_after_fully_connected_layers:\n        images_network = batch_normalization(images_network)\n\n    if NETWORK.use_landmarks or NETWORK.use_hog_and_landmarks:\n        if NETWORK.use_hog_sliding_window_and_landmarks:\n            landmarks_network = input_data(shape=[None, 2728], name=\'input2\')\n        elif NETWORK.use_hog_and_landmarks:\n            landmarks_network = input_data(shape=[None, 208], name=\'input2\')\n        else:\n            landmarks_network = input_data(shape=[None, 68, 2], name=\'input2\')\n        landmarks_network = fully_connected(landmarks_network, 1024, activation=NETWORK.activation)\n        if NETWORK.use_batchnorm_after_fully_connected_layers:\n            landmarks_network = batch_normalization(landmarks_network)\n        landmarks_network = fully_connected(landmarks_network, 40, activation=NETWORK.activation)\n        if NETWORK.use_batchnorm_after_fully_connected_layers:\n            landmarks_network = batch_normalization(landmarks_network)\n        images_network = fully_connected(images_network, 40, activation=NETWORK.activation)\n        network = merge([images_network, landmarks_network], \'concat\', axis=1)\n    else:\n        network = images_network\n    network = fully_connected(network, NETWORK.output_size, activation=\'softmax\')\n\n    if optimizer == \'momentum\':\n        optimizer = Momentum(learning_rate=learning_rate, momentum=optimizer_param, \n                    lr_decay=learning_rate_decay, decay_step=decay_step)\n    elif optimizer == \'adam\':\n        optimizer = Adam(learning_rate=learning_rate, beta1=optimizer_param, beta2=learning_rate_decay)\n    else:\n        print( ""Unknown optimizer: {}"".format(optimizer))\n    network = regression(network, optimizer=optimizer, loss=NETWORK.loss, learning_rate=learning_rate, name=\'output\')\n\n    return network'"
optimize_hyperparams.py,0,"b'""""""\n@AmineHorseman\nSep, 7th, 2016\n""""""\nimport time\nimport argparse\nimport pprint\nimport numpy as np \nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\nfrom train import train\nfrom parameters import HYPERPARAMS, OPTIMIZER\n\n# define the search space\nfspace = {\n    \'learning_rate\': hp.uniform(\'learning_rate\', OPTIMIZER.learning_rate[\'min\'], OPTIMIZER.learning_rate[\'max\']),\n    \'learning_rate_decay\': hp.uniform(\'learning_rate_decay\', OPTIMIZER.learning_rate_decay[\'min\'], OPTIMIZER.learning_rate_decay[\'max\']),\n    \'optimizer\': hp.choice(\'optimizer\', OPTIMIZER.optimizer),\n    \'optimizer_param\': hp.uniform(\'optimizer_param\', OPTIMIZER.optimizer_param[\'min\'], OPTIMIZER.optimizer_param[\'max\']),\n    \'keep_prob\': hp.uniform(\'keep_prob\', OPTIMIZER.keep_prob[\'min\'], OPTIMIZER.keep_prob[\'max\'])\n}\n\n# parse arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(""-m"", ""--max_evals"", required=True, help=""Maximum number of evaluations during hyperparameters search"")\nargs = parser.parse_args()\nmax_evals = int(args.max_evals)\ncurrent_eval = 1\ntrain_history = []\n\n# defint the fucntion to minimize (will train the model using the specified hyperparameters)\ndef function_to_minimize(hyperparams, optimizer=HYPERPARAMS.optimizer, optimizer_param=HYPERPARAMS.optimizer_param, \n        learning_rate=HYPERPARAMS.learning_rate, keep_prob=HYPERPARAMS.keep_prob, \n        learning_rate_decay=HYPERPARAMS.learning_rate_decay):\n    if \'learning_rate\' in hyperparams: \n        learning_rate = hyperparams[\'learning_rate\']\n    if \'learning_rate_decay\' in hyperparams: \n        learning_rate_decay = hyperparams[\'learning_rate_decay\']\n    if \'keep_prob\' in hyperparams: \n        keep_prob = hyperparams[\'keep_prob\']\n    if \'optimizer\' in hyperparams:\n        optimizer = hyperparams[\'optimizer\']\n    if \'optimizer_param\' in hyperparams:\n        optimizer_param = hyperparams[\'optimizer_param\']\n    global current_eval \n    global max_evals\n    print( ""#################################"")\n    print( ""       Evaluation {} of {}"".format(current_eval, max_evals))\n    print( ""#################################"")\n    start_time = time.time()\n    try:\n        accuracy = train(learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, \n                     optimizer=optimizer, optimizer_param=optimizer_param, keep_prob=keep_prob)\n        training_time = int(round(time.time() - start_time))\n        current_eval += 1\n        train_history.append({\'accuracy\':accuracy, \'learning_rate\':learning_rate, \'learning_rate_decay\':learning_rate_decay, \n                                  \'optimizer\':optimizer, \'optimizer_param\':optimizer_param, \'keep_prob\':keep_prob, \'time\':training_time})\n    except Exception as e:\n        # exception occured during training, saving history and stopping the operation\n        print( ""#################################"")\n        print( ""Exception during training: {}"".format(str(e)))\n        print( ""Saving train history in train_history.npy"")\n        np.save(""train_history.npy"", train_history)\n        exit()\n    return {\'loss\': -accuracy, \'time\': training_time, \'status\': STATUS_OK}\n\n# lunch the hyperparameters search\ntrials = Trials()\nbest_trial = fmin(fn=function_to_minimize, space=fspace, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n\n# get some additional information and print the best parameters\nfor trial in trials.trials:\n    if trial[\'misc\'][\'vals\'][\'keep_prob\'][0] == best_trial[\'keep_prob\'] and \\\n            trial[\'misc\'][\'vals\'][\'learning_rate\'][0] == best_trial[\'learning_rate\'] and \\\n            trial[\'misc\'][\'vals\'][\'learning_rate_decay\'][0] == best_trial[\'learning_rate_decay\']:\n        best_trial[\'accuracy\'] = -trial[\'result\'][\'loss\'] * 100\n        best_trial[\'time\'] = trial[\'result\'][\'time\']\nprint( ""#################################"")\nprint( ""      Best parameters found"")\nprint( ""#################################"")\npprint.pprint(best_trial)\nprint( ""#################################"")\n'"
parameters.py,0,"b'""""""\n@AmineHorseman\nSep, 1st, 2016\n""""""\nimport os\n\nclass Dataset:\n    name = \'Fer2013\'\n    train_folder = \'fer2013_features/Training\'\n    validation_folder = \'fer2013_features/PublicTest\'\n    test_folder = \'fer2013_features/PrivateTest\'\n    shape_predictor_path=\'shape_predictor_68_face_landmarks.dat\'\n    trunc_trainset_to = -1  # put the number of train images to use (-1 = all images of the train set)\n    trunc_validationset_to = -1\n    trunc_testset_to = -1\n\nclass Network:\n    model = \'B\'\n    input_size = 48\n    output_size = 7\n    activation = \'relu\'\n    loss = \'categorical_crossentropy\'\n    use_landmarks = True\n    use_hog_and_landmarks = True\n    use_hog_sliding_window_and_landmarks = True\n    use_batchnorm_after_conv_layers = True\n    use_batchnorm_after_fully_connected_layers = False\n\nclass Hyperparams:\n    keep_prob = 0.956   # dropout = 1 - keep_prob\n    learning_rate = 0.016\n    learning_rate_decay = 0.864\n    decay_step = 50\n    optimizer = \'momentum\'  # {\'momentum\', \'adam\', \'rmsprop\', \'adagrad\', \'adadelta\'}\n    optimizer_param = 0.95   # momentum value for Momentum optimizer, or beta1 value for Adam\n\nclass Training:\n    batch_size = 128\n    epochs = 13\n    snapshot_step = 500\n    vizualize = True\n    logs_dir = ""logs""\n    checkpoint_dir = ""checkpoints/chk""\n    best_checkpoint_path = ""checkpoints/best/""\n    max_checkpoints = 1\n    checkpoint_frequency = 1.0 # in hours\n    save_model = True\n    save_model_path = ""best_model/saved_model.bin""\n\nclass VideoPredictor:\n    emotions = [""Angry"", ""Disgust"", ""Fear"", ""Happy"", ""Sad"", ""Surprise"", ""Neutral""]\n    print_emotions = False\n    camera_source = 0\n    face_detection_classifier = ""lbpcascade_frontalface.xml""\n    show_confidence = False\n    time_to_wait_between_predictions = 0.5\n\nclass OptimizerSearchSpace:\n    learning_rate = {\'min\': 0.00001, \'max\': 0.1}\n    learning_rate_decay = {\'min\': 0.5, \'max\': 0.99}\n    optimizer = [\'momentum\']   # [\'momentum\', \'adam\', \'rmsprop\', \'adagrad\', \'adadelta\']\n    optimizer_param = {\'min\': 0.5, \'max\': 0.99}\n    keep_prob = {\'min\': 0.7, \'max\': 0.99}\n\ndef make_dir(folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\nDATASET = Dataset()\nNETWORK = Network()\nTRAINING = Training()\nHYPERPARAMS = Hyperparams()\nVIDEO_PREDICTOR = VideoPredictor()\nOPTIMIZER = OptimizerSearchSpace()\n\nmake_dir(TRAINING.logs_dir)\nmake_dir(TRAINING.checkpoint_dir)\n'"
predict-from-video.py,0,"b'""""""\n@AmineHorseman \nSep 12th, 2016\n""""""\nimport tensorflow as tf\nfrom tflearn import DNN\nimport imutils\nimport cv2\nimport time\nimport dlib\n\nfrom parameters import NETWORK, DATASET, VIDEO_PREDICTOR\nfrom model import build_model\nfrom predict import load_model, predict\n\nclass EmotionRecognizer:\n    \n    BOX_COLOR = (0, 255, 0)\n    TEXT_COLOR = (0, 255, 0)\n\n    def __init__(self):\n       \n        # initializebevideo stream\n        self.video_stream = cv2.VideoCapture(VIDEO_PREDICTOR.camera_source)\n  \n        self.face_detector = cv2.CascadeClassifier(VIDEO_PREDICTOR.face_detection_classifier)\n\n        self.shape_predictor = None\n        if NETWORK.use_landmarks:\n            self.shape_predictor = dlib.shape_predictor(DATASET.shape_predictor_path)\n        \n        self.model = load_model()\n        self.last_predicted_time = 0\n        self.last_predicted_confidence = 0\n        self.last_predicted_emotion = """"\n\n    def predict_emotion(self, image):\n        image.resize([NETWORK.input_size, NETWORK.input_size], refcheck=False)\n        emotion, confidence = predict(image, self.model, self.shape_predictor)\n        return emotion, confidence\n\n    def recognize_emotions(self):\n        failedFramesCount = 0\n        detected_faces = []\n        time_last_sent = 0\n        while True:\n            grabbed, frame = self.video_stream.read()\n\n            if grabbed:\n                # detection phase\n                frame = imutils.resize(frame, width=600)\n                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n                # detect faces\n                faces = self.face_detector.detectMultiScale(gray, 1.3, 5)\n                for (x,y,w,h) in faces:\n                    if w < 30 and h<30: # skip the small faces (probably false detections)\n                        continue\n\n                    # bounding box\n                    cv2.rectangle(frame, (x, y), (x + w, y + h), self.BOX_COLOR, 2)\n\n                    # try to recognize emotion\n                    face = gray[y:y+h, x:x+w].copy()\n                    if time.time() - self.last_predicted_time < VIDEO_PREDICTOR.time_to_wait_between_predictions:\n                        label = self.last_predicted_emotion\n                        confidence = self.last_predicted_confidence\n                    else:\n                        label, confidence = self.predict_emotion(face)\n                        self.last_predicted_emotion = label\n                        self.last_predicted_confidence = confidence\n                        self.last_predicted_time = time.time()\n                    \n                    # display and send message by socket\n                    if VIDEO_PREDICTOR.show_confidence:\n                        text = ""{0} ({1:.1f}%)"".format(label, confidence*100)\n                    else:\n                        text = label\n                    if label is not None:\n                        cv2.putText(frame, text, (x - 20, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.TEXT_COLOR, 2)\n\n                # display images\n                cv2.imshow(""Facial Expression Recognition"", frame)\n\n                key = cv2.waitKey(1) & 0xFF\n                if key == ord(""q""):\n                    break            \n            else:\n                failedFramesCount += 1\n                if failedFramesCount > 10:\n                    print( ""can\'t grab frames"")\n                    break\n\n        self.video_stream.release()\n        cv2.destroyAllWindows()\n\nr = EmotionRecognizer()\nr.recognize_emotions()'"
predict.py,1,"b'""""""\n@AmineHorseman\nSep, 12th, 2016\n""""""\nimport tensorflow as tf\nfrom tflearn import DNN\nimport time\nimport numpy as np\nimport argparse\nimport dlib\nimport cv2\nimport os\nfrom skimage.feature import hog\n\nfrom parameters import DATASET, TRAINING, NETWORK, VIDEO_PREDICTOR\nfrom model import build_model\n\nwindow_size = 24\nwindow_step = 6\n\ndef load_model():\n    model = None\n    with tf.Graph().as_default():\n        print( ""loading pretrained model..."")\n        network = build_model()\n        model = DNN(network)\n        if os.path.isfile(TRAINING.save_model_path):\n            model.load(TRAINING.save_model_path)\n        else:\n            print( ""Error: file \'{}\' not found"".format(TRAINING.save_model_path))\n    return model\n\ndef get_landmarks(image, rects, predictor):\n    # this function have been copied from http://bit.ly/2cj7Fpq\n    if len(rects) > 1:\n        raise TooManyFaces\n    if len(rects) == 0:\n        raise NoFaces\n    return np.matrix([[p.x, p.y] for p in predictor(image, rects[0]).parts()])\n\ndef sliding_hog_windows(image):\n    hog_windows = []\n    for y in range(0, NETWORK.input_size, window_step):\n        for x in range(0, NETWORK.input_size, window_step):\n            window = image[y:y+window_size, x:x+window_size]\n            hog_windows.extend(hog(window, orientations=8, pixels_per_cell=(8, 8),\n                                            cells_per_block=(1, 1), visualise=False))\n    return hog_windows\n\ndef predict(image, model, shape_predictor=None):\n    # get landmarks\n    if NETWORK.use_landmarks or NETWORK.use_hog_and_landmarks or NETWORK.use_hog_sliding_window_and_landmarks:\n        face_rects = [dlib.rectangle(left=0, top=0, right=NETWORK.input_size, bottom=NETWORK.input_size)]\n        face_landmarks = np.array([get_landmarks(image, face_rects, shape_predictor)])\n        features = face_landmarks\n        if NETWORK.use_hog_sliding_window_and_landmarks: \n            hog_features = sliding_hog_windows(image)\n            hog_features = np.asarray(hog_features)\n            face_landmarks = face_landmarks.flatten()\n            features = np.concatenate((face_landmarks, hog_features))\n        else:\n            hog_features, _ = hog(image, orientations=8, pixels_per_cell=(16, 16),\n                                    cells_per_block=(1, 1), visualise=True)\n            hog_features = np.asarray(hog_features)\n            face_landmarks = face_landmarks.flatten()\n            features = np.concatenate((face_landmarks, hog_features))\n        tensor_image = image.reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n        predicted_label = model.predict([tensor_image, features.reshape((1, -1))])\n        return get_emotion(predicted_label[0])\n    else:\n        tensor_image = image.reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n        predicted_label = model.predict(tensor_image)\n        return get_emotion(predicted_label[0])\n    return None\n\ndef get_emotion(label):\n    if VIDEO_PREDICTOR.print_emotions:\n        print( ""- Angry: {0:.1f}%\\n- Happy: {1:.1f}%\\n- Sad: {2:.1f}%\\n- Surprise: {3:.1f}%\\n- Neutral: {4:.1f}%"".format(\n                label[0]*100, label[1]*100, label[2]*100, label[3]*100, label[4]*100))\n    label = label.tolist()\n    return VIDEO_PREDICTOR.emotions[label.index(max(label))], max(label)\n\n# parse arg to see if we need to launch training now or not yet\nparser = argparse.ArgumentParser()\nparser.add_argument(""-i"", ""--image"", help=""Image file to predict"")\nargs = parser.parse_args()\nif args.image:\n    if os.path.isfile(args.image):\n        model = load_model()\n        image = cv2.imread(args.image, 0)\n        shape_predictor = dlib.shape_predictor(DATASET.shape_predictor_path)\n        start_time = time.time()\n        emotion, confidence = predict(image, model, shape_predictor)\n        total_time = time.time() - start_time\n        print( ""Prediction: {0} (confidence: {1:.1f}%)"".format(emotion, confidence*100))\n        print( ""time: {0:.1f} sec"".format(total_time))\n    else:\n        print( ""Error: file \'{}\' not found"".format(args.image))'"
train.py,1,"b'""""""\n@AmineHorseman\nSep, 1st, 2016\n""""""\nimport tensorflow as tf\nfrom tflearn import DNN\nimport time\nimport argparse\nimport os\n\nfrom data_loader import load_data \nfrom parameters import DATASET, TRAINING, HYPERPARAMS, NETWORK\nfrom model import build_model\n\ndef train(optimizer=HYPERPARAMS.optimizer, optimizer_param=HYPERPARAMS.optimizer_param, \n        learning_rate=HYPERPARAMS.learning_rate, keep_prob=HYPERPARAMS.keep_prob, \n        learning_rate_decay=HYPERPARAMS.learning_rate_decay, decay_step=HYPERPARAMS.decay_step,\n        train_model=True):\n\n        print( ""loading dataset "" + DATASET.name + ""..."")\n        if train_model:\n                data, validation = load_data(validation=True)\n        else:\n                data, validation, test = load_data(validation=True, test=True)\n\n        with tf.Graph().as_default():\n                print( ""building model..."")\n                network = build_model(optimizer, optimizer_param, learning_rate, \n                          keep_prob, learning_rate_decay, decay_step)\n                model = DNN(network, tensorboard_dir=TRAINING.logs_dir, \n                        tensorboard_verbose=0, checkpoint_path=TRAINING.checkpoint_dir,\n                        max_checkpoints=TRAINING.max_checkpoints)\n\n                #tflearn.config.init_graph(seed=None, log_device=False, num_cores=6)\n\n                if train_model:\n                        # Training phase\n                        print( ""start training..."")\n                        print( ""  - emotions = {}"".format(NETWORK.output_size))\n                        print( ""  - model = {}"".format(NETWORK.model))\n                        print( ""  - optimizer = \'{}\'"".format(optimizer))\n                        print( ""  - learning_rate = {}"".format(learning_rate))\n                        print( ""  - learning_rate_decay = {}"".format(learning_rate_decay))\n                        print( ""  - otimizer_param ({}) = {}"".format(\'beta1\' if optimizer == \'adam\' else \'momentum\', optimizer_param))\n                        print( ""  - keep_prob = {}"".format(keep_prob))\n                        print( ""  - epochs = {}"".format(TRAINING.epochs))\n                        print( ""  - use landmarks = {}"".format(NETWORK.use_landmarks))\n                        print( ""  - use hog + landmarks = {}"".format(NETWORK.use_hog_and_landmarks))\n                        print( ""  - use hog sliding window + landmarks = {}"".format(NETWORK.use_hog_sliding_window_and_landmarks))\n                        print( ""  - use batchnorm after conv = {}"".format(NETWORK.use_batchnorm_after_conv_layers))\n                        print( ""  - use batchnorm after fc = {}"".format(NETWORK.use_batchnorm_after_fully_connected_layers))\n\n                        start_time = time.time()\n                        if NETWORK.use_landmarks:\n                                model.fit([data[\'X\'], data[\'X2\']], data[\'Y\'],\n                                        validation_set=([validation[\'X\'], validation[\'X2\']], validation[\'Y\']),\n                                        snapshot_step=TRAINING.snapshot_step,\n                                        show_metric=TRAINING.vizualize,\n                                        batch_size=TRAINING.batch_size,\n                                        n_epoch=TRAINING.epochs)\n                        else:\n                                model.fit(data[\'X\'], data[\'Y\'],\n                                        validation_set=(validation[\'X\'], validation[\'Y\']),\n                                        snapshot_step=TRAINING.snapshot_step,\n                                        show_metric=TRAINING.vizualize,\n                                        batch_size=TRAINING.batch_size,\n                                        n_epoch=TRAINING.epochs)\n                                validation[\'X2\'] = None\n                        training_time = time.time() - start_time\n                        print( ""training time = {0:.1f} sec"".format(training_time))\n\n                        if TRAINING.save_model:\n                                print( ""saving model..."")\n                                model.save(TRAINING.save_model_path)\n                                if not(os.path.isfile(TRAINING.save_model_path)) and \\\n                                        os.path.isfile(TRAINING.save_model_path + "".meta""):\n                                        os.rename(TRAINING.save_model_path + "".meta"", TRAINING.save_model_path)\n\n                        print( ""evaluating..."")\n                        validation_accuracy = evaluate(model, validation[\'X\'], validation[\'X2\'], validation[\'Y\'])\n                        print( ""  - validation accuracy = {0:.1f}"".format(validation_accuracy*100))\n                        return validation_accuracy\n                else:\n                        # Testing phase : load saved model and evaluate on test dataset\n                        print( ""start evaluation..."")\n                        print( ""loading pretrained model..."")\n                        if os.path.isfile(TRAINING.save_model_path):\n                                model.load(TRAINING.save_model_path)\n                        else:\n                                print( ""Error: file \'{}\' not found"".format(TRAINING.save_model_path))\n                                exit()\n                        \n                        if not NETWORK.use_landmarks:\n                                validation[\'X2\'] = None\n                                test[\'X2\'] = None\n\n                        print( ""--"")\n                        print( ""Validation samples: {}"".format(len(validation[\'Y\'])))\n                        print( ""Test samples: {}"".format(len(test[\'Y\'])))\n                        print( ""--"")\n                        print( ""evaluating..."")\n                        start_time = time.time()\n                        validation_accuracy = evaluate(model, validation[\'X\'], validation[\'X2\'], validation[\'Y\'])\n                        print( ""  - validation accuracy = {0:.1f}"".format(validation_accuracy*100))\n                        test_accuracy = evaluate(model, test[\'X\'], test[\'X2\'], test[\'Y\'])\n                        print( ""  - test accuracy = {0:.1f}"".format(test_accuracy*100))\n                        print( ""  - evalution time = {0:.1f} sec"".format(time.time() - start_time))\n                        return test_accuracy\n\ndef evaluate(model, X, X2, Y):\n        if NETWORK.use_landmarks:\n                accuracy = model.evaluate([X, X2], Y)\n        else:\n                accuracy = model.evaluate(X, Y)\n        return accuracy[0]\n\n# parse arg to see if we need to launch training now or not yet\nparser = argparse.ArgumentParser()\nparser.add_argument(""-t"", ""--train"", default=""no"", help=""if \'yes\', launch training from command line"")\nparser.add_argument(""-e"", ""--evaluate"", default=""no"", help=""if \'yes\', launch evaluation on test dataset"")\nparser.add_argument(""-m"", ""--max_evals"", help=""Maximum number of evaluations during hyperparameters search"")\nargs = parser.parse_args()\nif args.train==""yes"" or args.train==""Yes"" or args.train==""YES"":\n        train()\nif args.evaluate==""yes"" or args.evaluate==""Yes"" or args.evaluate==""YES"":\n        train(train_model=False)'"
