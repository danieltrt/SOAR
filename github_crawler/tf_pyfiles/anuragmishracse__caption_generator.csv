file_path,api_count,code
caption_generator/__init__.py,0,b'__version__ = 0.1'
caption_generator/caption_generator.py,0,"b'from vgg16 import VGG16\nfrom keras.applications import inception_v3\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten\nfrom keras.preprocessing import image, sequence\nfrom keras.callbacks import ModelCheckpoint\nimport cPickle as pickle\n\nEMBEDDING_DIM = 128\n\n\nclass CaptionGenerator():\n\n    def __init__(self):\n        self.max_cap_len = None\n        self.vocab_size = None\n        self.index_word = None\n        self.word_index = None\n        self.total_samples = None\n        self.encoded_images = pickle.load( open( ""encoded_images.p"", ""rb"" ) )\n        self.variable_initializer()\n\n    def variable_initializer(self):\n        df = pd.read_csv(\'Flickr8k_text/flickr_8k_train_dataset.txt\', delimiter=\'\\t\')\n        nb_samples = df.shape[0]\n        iter = df.iterrows()\n        caps = []\n        for i in range(nb_samples):\n            x = iter.next()\n            caps.append(x[1][1])\n\n        self.total_samples=0\n        for text in caps:\n            self.total_samples+=len(text.split())-1\n        print ""Total samples : ""+str(self.total_samples)\n        \n        words = [txt.split() for txt in caps]\n        unique = []\n        for word in words:\n            unique.extend(word)\n\n        unique = list(set(unique))\n        self.vocab_size = len(unique)\n        self.word_index = {}\n        self.index_word = {}\n        for i, word in enumerate(unique):\n            self.word_index[word]=i\n            self.index_word[i]=word\n\n        max_len = 0\n        for caption in caps:\n            if(len(caption.split()) > max_len):\n                max_len = len(caption.split())\n        self.max_cap_len = max_len\n        print ""Vocabulary size: ""+str(self.vocab_size)\n        print ""Maximum caption length: ""+str(self.max_cap_len)\n        print ""Variables initialization done!""\n\n\n    def data_generator(self, batch_size = 32):\n        partial_caps = []\n        next_words = []\n        images = []\n        print ""Generating data...""\n        gen_count = 0\n        df = pd.read_csv(\'Flickr8k_text/flickr_8k_train_dataset.txt\', delimiter=\'\\t\')\n        nb_samples = df.shape[0]\n        iter = df.iterrows()\n        caps = []\n        imgs = []\n        for i in range(nb_samples):\n            x = iter.next()\n            caps.append(x[1][1])\n            imgs.append(x[1][0])\n\n\n        total_count = 0\n        while 1:\n            image_counter = -1\n            for text in caps:\n                image_counter+=1\n                current_image = self.encoded_images[imgs[image_counter]]\n                for i in range(len(text.split())-1):\n                    total_count+=1\n                    partial = [self.word_index[txt] for txt in text.split()[:i+1]]\n                    partial_caps.append(partial)\n                    next = np.zeros(self.vocab_size)\n                    next[self.word_index[text.split()[i+1]]] = 1\n                    next_words.append(next)\n                    images.append(current_image)\n\n                    if total_count>=batch_size:\n                        next_words = np.asarray(next_words)\n                        images = np.asarray(images)\n                        partial_caps = sequence.pad_sequences(partial_caps, maxlen=self.max_cap_len, padding=\'post\')\n                        total_count = 0\n                        gen_count+=1\n                        print ""yielding count: ""+str(gen_count)\n                        yield [[images, partial_caps], next_words]\n                        partial_caps = []\n                        next_words = []\n                        images = []\n        \n    def load_image(self, path):\n        img = image.load_img(path, target_size=(224,224))\n        x = image.img_to_array(img)\n        return np.asarray(x)\n\n\n    def create_model(self, ret_model = False):\n        #base_model = VGG16(weights=\'imagenet\', include_top=False, input_shape = (224, 224, 3))\n        #base_model.trainable=False\n        image_model = Sequential()\n        #image_model.add(base_model)\n        #image_model.add(Flatten())\n        image_model.add(Dense(EMBEDDING_DIM, input_dim = 4096, activation=\'relu\'))\n\n        image_model.add(RepeatVector(self.max_cap_len))\n\n        lang_model = Sequential()\n        lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_cap_len))\n        lang_model.add(LSTM(256,return_sequences=True))\n        lang_model.add(TimeDistributed(Dense(EMBEDDING_DIM)))\n\n        model = Sequential()\n        model.add(Merge([image_model, lang_model], mode=\'concat\'))\n        model.add(LSTM(1000,return_sequences=False))\n        model.add(Dense(self.vocab_size))\n        model.add(Activation(\'softmax\'))\n\n        print ""Model created!""\n\n        if(ret_model==True):\n            return model\n\n        model.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\', metrics=[\'accuracy\'])\n        return model\n\n    def get_word(self,index):\n        return self.index_word[index]'"
caption_generator/prepare_dataset.py,0,"b'import cPickle as pickle\nfrom keras.preprocessing import image\nfrom vgg16 import VGG16\nimport numpy as np \nfrom keras.applications.imagenet_utils import preprocess_input\t\n\ncounter = 0\n\ndef load_image(path):\n    img = image.load_img(path, target_size=(224,224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return np.asarray(x)\n\ndef load_encoding_model():\n\tmodel = VGG16(weights=\'imagenet\', include_top=True, input_shape = (224, 224, 3))\n\treturn model\n\ndef get_encoding(model, img):\n\tglobal counter\n\tcounter += 1\n\timage = load_image(\'Flicker8k_Dataset/\'+str(img))\n\tpred = model.predict(image)\n\tpred = np.reshape(pred, pred.shape[1])\n\tprint ""Encoding image: ""+str(counter)\n\tprint pred.shape\n\treturn pred\n\ndef prepare_dataset(no_imgs = -1):\n\tf_train_images = open(\'Flickr8k_text/Flickr_8k.trainImages.txt\',\'rb\')\n\ttrain_imgs = f_train_images.read().strip().split(\'\\n\') if no_imgs == -1 else f_train_images.read().strip().split(\'\\n\')[:no_imgs]\n\tf_train_images.close()\n\n\tf_test_images = open(\'Flickr8k_text/Flickr_8k.testImages.txt\',\'rb\')\n\ttest_imgs = f_test_images.read().strip().split(\'\\n\') if no_imgs == -1 else f_test_images.read().strip().split(\'\\n\')[:no_imgs]\n\tf_test_images.close()\n\n\tf_train_dataset = open(\'Flickr8k_text/flickr_8k_train_dataset.txt\',\'wb\')\n\tf_train_dataset.write(""image_id\\tcaptions\\n"")\n\n\tf_test_dataset = open(\'Flickr8k_text/flickr_8k_test_dataset.txt\',\'wb\')\n\tf_test_dataset.write(""image_id\\tcaptions\\n"")\n\n\tf_captions = open(\'Flickr8k_text/Flickr8k.token.txt\', \'rb\')\n\tcaptions = f_captions.read().strip().split(\'\\n\')\n\tdata = {}\n\tfor row in captions:\n\t\trow = row.split(""\\t"")\n\t\trow[0] = row[0][:len(row[0])-2]\n\t\ttry:\n\t\t\tdata[row[0]].append(row[1])\n\t\texcept:\n\t\t\tdata[row[0]] = [row[1]]\n\tf_captions.close()\n\n\tencoded_images = {}\n\tencoding_model = load_encoding_model()\n\n\tc_train = 0\n\tfor img in train_imgs:\n\t\tencoded_images[img] = get_encoding(encoding_model, img)\n\t\tfor capt in data[img]:\n\t\t\tcaption = ""<start> ""+capt+"" <end>""\n\t\t\tf_train_dataset.write(img+""\\t""+caption+""\\n"")\n\t\t\tf_train_dataset.flush()\n\t\t\tc_train += 1\n\tf_train_dataset.close()\n\n\tc_test = 0\n\tfor img in test_imgs:\n\t\tencoded_images[img] = get_encoding(encoding_model, img)\n\t\tfor capt in data[img]:\n\t\t\tcaption = ""<start> ""+capt+"" <end>""\n\t\t\tf_test_dataset.write(img+""\\t""+caption+""\\n"")\n\t\t\tf_test_dataset.flush()\n\t\t\tc_test += 1\n\tf_test_dataset.close()\n\twith open( ""encoded_images.p"", ""wb"" ) as pickle_f:\n\t\tpickle.dump( encoded_images, pickle_f )  \n\treturn [c_train, c_test]\n\nif __name__ == \'__main__\':\n\tc_train, c_test = prepare_dataset()\n\tprint ""Training samples = ""+str(c_train)\n\tprint ""Test samples = ""+str(c_test)\n'"
caption_generator/test_model.py,0,"b'import cPickle as pickle\nimport caption_generator\nimport numpy as np\nfrom keras.preprocessing import sequence\nimport nltk\n\ncg = caption_generator.CaptionGenerator()\n\ndef process_caption(caption):\n\tcaption_split = caption.split()\n\tprocessed_caption = caption_split[1:]\n\ttry:\n\t\tend_index = processed_caption.index(\'<end>\')\n\t\tprocessed_caption = processed_caption[:end_index]\n\texcept:\n\t\tpass\n\treturn "" "".join([word for word in processed_caption])\n\ndef get_best_caption(captions):\n    captions.sort(key = lambda l:l[1])\n    best_caption = captions[-1][0]\n    return "" "".join([cg.index_word[index] for index in best_caption])\n\ndef get_all_captions(captions):\n    final_captions = []\n    captions.sort(key = lambda l:l[1])\n    for caption in captions:\n        text_caption = "" "".join([cg.index_word[index] for index in caption[0]])\n        final_captions.append([text_caption, caption[1]])\n    return final_captions\n\ndef generate_captions(model, image, beam_size):\n\tstart = [cg.word_index[\'<start>\']]\n\tcaptions = [[start,0.0]]\n\twhile(len(captions[0][0]) < cg.max_cap_len):\n\t\ttemp_captions = []\n\t\tfor caption in captions:\n\t\t\tpartial_caption = sequence.pad_sequences([caption[0]], maxlen=cg.max_cap_len, padding=\'post\')\n\t\t\tnext_words_pred = model.predict([np.asarray([image]), np.asarray(partial_caption)])[0]\n\t\t\tnext_words = np.argsort(next_words_pred)[-beam_size:]\n\t\t\tfor word in next_words:\n\t\t\t\tnew_partial_caption, new_partial_caption_prob = caption[0][:], caption[1]\n\t\t\t\tnew_partial_caption.append(word)\n\t\t\t\tnew_partial_caption_prob+=next_words_pred[word]\n\t\t\t\ttemp_captions.append([new_partial_caption,new_partial_caption_prob])\n\t\tcaptions = temp_captions\n\t\tcaptions.sort(key = lambda l:l[1])\n\t\tcaptions = captions[-beam_size:]\n\n\treturn captions\n\ndef test_model(weight, img_name, beam_size = 3):\n\tencoded_images = pickle.load( open( ""encoded_images.p"", ""rb"" ) )\n\tmodel = cg.create_model(ret_model = True)\n\tmodel.load_weights(weight)\n\n\timage = encoded_images[img_name]\n\tcaptions = generate_captions(model, image, beam_size)\n\treturn process_caption(get_best_caption(captions))\n\t#return [process_caption(caption[0]) for caption in get_all_captions(captions)] \n\ndef bleu_score(hypotheses, references):\n\treturn nltk.translate.bleu_score.corpus_bleu(references, hypotheses)\n\ndef test_model_on_images(weight, img_dir, beam_size = 3):\n\timgs = []\n\tcaptions = {}\n\twith open(img_dir, \'rb\') as f_images:\n\t\timgs = f_images.read().strip().split(\'\\n\')\n\tencoded_images = pickle.load( open( ""encoded_images.p"", ""rb"" ) )\n\tmodel = cg.create_model(ret_model = True)\n\tmodel.load_weights(weight)\n\n\tf_pred_caption = open(\'predicted_captions.txt\', \'wb\')\n\n\tfor count, img_name in enumerate(imgs):\n\t\tprint ""Predicting for image: ""+str(count)\n\t\timage = encoded_images[img_name]\n\t\timage_captions = generate_captions(model, image, beam_size)\n\t\tbest_caption = process_caption(get_best_caption(image_captions))\n\t\tcaptions[img_name] = best_caption\n\t\tprint img_name+"" : ""+str(best_caption)\n\t\tf_pred_caption.write(img_name+""\\t""+str(best_caption))\n\t\tf_pred_caption.flush()\n\tf_pred_caption.close()\n\n\tf_captions = open(\'Flickr8k_text/Flickr8k.token.txt\', \'rb\')\n\tcaptions_text = f_captions.read().strip().split(\'\\n\')\n\timage_captions_pair = {}\n\tfor row in captions_text:\n\t\trow = row.split(""\\t"")\n\t\trow[0] = row[0][:len(row[0])-2]\n\t\ttry:\n\t\t\timage_captions_pair[row[0]].append(row[1])\n\t\texcept:\n\t\t\timage_captions_pair[row[0]] = [row[1]]\n\tf_captions.close()\n\t\n\thypotheses=[]\n\treferences = []\n\tfor img_name in imgs:\n\t\thypothesis = captions[img_name]\n\t\treference = image_captions_pair[img_name]\n\t\thypotheses.append(hypothesis)\n\t\treferences.append(reference)\n\n\treturn bleu_score(hypotheses, references)\n\nif __name__ == \'__main__\':\n\tweight = \'weights-improvement-48.hdf5\'\n\ttest_image = \'3155451946_c0862c70cb.jpg\'\n\ttest_img_dir = \'Flickr8k_text/Flickr_8k.testImages.txt\'\n\t#print test_model(weight, test_image)\n\tprint test_model_on_images(weight, test_img_dir, beam_size=3)\n'"
caption_generator/train_model.py,0,"b'import caption_generator\nfrom keras.callbacks import ModelCheckpoint\n\ndef train_model(weight = None, batch_size=32, epochs = 10):\n\n    cg = caption_generator.CaptionGenerator()\n    model = cg.create_model()\n\n    if weight != None:\n        model.load_weights(weight)\n\n    counter = 0\n    file_name = \'weights-improvement-{epoch:02d}.hdf5\'\n    checkpoint = ModelCheckpoint(file_name, monitor=\'loss\', verbose=1, save_best_only=True, mode=\'min\')\n    callbacks_list = [checkpoint]\n    model.fit_generator(cg.data_generator(batch_size=batch_size), steps_per_epoch=cg.total_samples/batch_size, epochs=epochs, verbose=2, callbacks=callbacks_list)\n    try:\n        model.save(\'Models/WholeModel.h5\', overwrite=True)\n        model.save_weights(\'Models/Weights.h5\',overwrite=True)\n    except:\n        print ""Error in saving model.""\n    print ""Training complete...\\n""\n\nif __name__ == \'__main__\':\n    train_model(epochs=50)'"
caption_generator/vgg16.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'VGG16 model for Keras.\n\n# Reference:\n\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n\'\'\'\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\nfrom keras.applications.imagenet_utils import decode_predictions\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.applications.imagenet_utils import _obtain_input_shape\nfrom keras.engine.topology import get_source_inputs\n\n\nWEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\'\nWEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n\n\ndef VGG16(include_top=True, weights=\'imagenet\',\n          input_tensor=None, input_shape=None,\n          pooling=None,\n          classes=1000):\n    """"""Instantiates the VGG16 architecture.\n\n    Optionally loads weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_data_format=""channels_last""` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The data format\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or ""imagenet"" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` (with `channels_last` data format)\n            or `(3, 224, 244)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 48.\n            E.g. `(200, 200, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n\n    # Returns\n        A Keras model instance.\n\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    """"""\n    if weights not in {\'imagenet\', None}:\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization) or `imagenet` \'\n                         \'(pre-training on ImageNet).\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as imagenet with `include_top`\'\n                         \' as true, `classes` should be 1000\')\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=48,\n                                      data_format=K.image_data_format(),\n                                      include_top=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\', name=\'block1_conv1\')(img_input)\n    x = Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\', name=\'block1_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block1_pool\')(x)\n\n    # Block 2\n    x = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\', name=\'block2_conv1\')(x)\n    x = Conv2D(128, (3, 3), activation=\'relu\', padding=\'same\', name=\'block2_conv2\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block2_pool\')(x)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\', name=\'block3_conv1\')(x)\n    x = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\', name=\'block3_conv2\')(x)\n    x = Conv2D(256, (3, 3), activation=\'relu\', padding=\'same\', name=\'block3_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block3_pool\')(x)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block4_conv1\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block4_conv2\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block4_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block4_pool\')(x)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block5_conv1\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block5_conv2\')(x)\n    x = Conv2D(512, (3, 3), activation=\'relu\', padding=\'same\', name=\'block5_conv3\')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name=\'block5_pool\')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name=\'flatten\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc1\')(x)\n        x = Dense(4096, activation=\'relu\', name=\'fc2\')(x)\n        x = Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n    else:\n        if pooling == \'avg\':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'vgg16\')\n\n    # load weights\n    if weights == \'imagenet\':\n        if include_top:\n            weights_path = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels.h5\',\n                                    WEIGHTS_PATH,\n                                   )\n        else:\n            weights_path = get_file(\'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                                    WEIGHTS_PATH_NO_TOP,\n                                    cache_subdir=\'models\')\n        model.load_weights(weights_path)\n        if K.backend() == \'theano\':\n            layer_utils.convert_all_kernels_in_model(model)\n\n        if K.image_data_format() == \'channels_first\':\n            if include_top:\n                maxpool = model.get_layer(name=\'block5_pool\')\n                shape = maxpool.output_shape[1:]\n                dense = model.get_layer(name=\'fc1\')\n                layer_utils.convert_dense_weights_data_format(dense, shape, \'channels_first\')\n\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image data format convention \'\n                              \'(`image_data_format=""channels_first""`). \'\n                              \'For best performance, set \'\n                              \'`image_data_format=""channels_last""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n    model.layers.pop() # Get rid of the classification layer\n    model.outputs = [model.layers[-1].output]\n    model.layers[-1].outbound_nodes = []\n    return model\n\n\nif __name__ == \'__main__\':\n    model = VGG16(include_top=True, weights=\'imagenet\')\n\n    img_path = \'elephant.jpg\'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print(\'Input image shape:\', x.shape)\n\n    preds = model.predict(x)\n    print(\'Predicted:\', decode_predictions(preds))\n'"
