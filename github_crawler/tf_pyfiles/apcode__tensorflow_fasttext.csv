file_path,api_count,code
classifier.py,58,"b'""""""Train simple fastText-style classifier.\n\nInputs:\n  words - text to classify\n  ngrams - n char ngrams for each word in words\n  labels - output classes to classify\n\nModel:\n  word embedding\n  ngram embedding\n  LogisticRegression classifier of embeddings to labels\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inputs\nimport sys\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import feature_column\nfrom tensorflow.contrib.learn.python.learn.estimators.run_config import RunConfig\n\n\ntf.flags.DEFINE_string(""train_records"", None,\n                       ""Training file pattern for TFRecords, can use wildcards"")\ntf.flags.DEFINE_string(""eval_records"", None,\n                       ""Evaluation file pattern for TFRecords, can use wildcards"")\ntf.flags.DEFINE_string(""predict_records"", None,\n                       ""File pattern for TFRecords to predict, can use wildcards"")\ntf.flags.DEFINE_string(""label_file"", None, ""File containing output labels"")\ntf.flags.DEFINE_integer(""num_labels"", None, ""Number of output labels"")\ntf.flags.DEFINE_string(""vocab_file"", None, ""Vocabulary file, one word per line"")\ntf.flags.DEFINE_integer(""vocab_size"", None, ""Number of words in vocabulary"")\ntf.flags.DEFINE_integer(""num_oov_vocab_buckets"", 20,\n                        ""Number of hash buckets to use for OOV words"")\ntf.flags.DEFINE_string(""model_dir"", ""."",\n                       ""Output directory for checkpoints and summaries"")\ntf.flags.DEFINE_string(""export_dir"", None, ""Directory to store savedmodel"")\n\ntf.flags.DEFINE_integer(""embedding_dimension"", 10, ""Dimension of word embedding"")\ntf.flags.DEFINE_boolean(""use_ngrams"", False, ""Use character ngrams in embedding"")\ntf.flags.DEFINE_integer(""num_ngram_buckets"", 1000000,\n                        ""Number of hash buckets for ngrams"")\ntf.flags.DEFINE_integer(""ngram_embedding_dimension"", 10, ""Dimension of word embedding"")\n\ntf.flags.DEFINE_float(""learning_rate"", 0.001, ""Learning rate for training"")\ntf.flags.DEFINE_float(""clip_gradient"", 5.0, ""Clip gradient norm to this ratio"")\ntf.flags.DEFINE_integer(""batch_size"", 128, ""Training minibatch size"")\ntf.flags.DEFINE_integer(""train_steps"", 1000,\n                        ""Number of train steps, None for continuous"")\ntf.flags.DEFINE_integer(""eval_steps"", 100, ""Number of eval steps"")\ntf.flags.DEFINE_integer(""num_epochs"", None, ""Number of training data epochs"")\ntf.flags.DEFINE_integer(""checkpoint_steps"", 1000,\n                        ""Steps between saving checkpoints"")\ntf.flags.DEFINE_integer(""num_threads"", 1, ""Number of reader threads"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""log where ops are located"")\ntf.flags.DEFINE_boolean(""horovod"", False,\n                        ""Run across multiple GPUs using Horovod MPI. https://github.com/uber/horovod"")\ntf.flags.DEFINE_boolean(""debug"", False, ""Debug"")\nFLAGS = tf.flags.FLAGS\n\nif FLAGS.horovod:\n    try:\n        import horovod.tensorflow as hvd\n    except ImportError, e:\n        print(e)\n        print(""Make sure Horovod is installed: https://github.com/uber/horovod"")\n        sys.exit(1)\n    hvd.init()\n\n\ndef InputFn(mode, input_file):\n    return inputs.InputFn(\n        mode, FLAGS.use_ngrams, input_file, FLAGS.vocab_file, FLAGS.vocab_size,\n        FLAGS.embedding_dimension, FLAGS.num_oov_vocab_buckets,\n        FLAGS.label_file, FLAGS.num_labels,\n        FLAGS.ngram_embedding_dimension, FLAGS.num_ngram_buckets,\n        FLAGS.batch_size, FLAGS.num_epochs, FLAGS.num_threads)\n\n\ndef Exports(probs, embedding):\n    exports = {\n        ""proba"": tf.estimator.export.ClassificationOutput(scores=probs),\n        ""embedding"": tf.estimator.export.RegressionOutput(value=embedding),\n        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: \\\n            tf.estimator.export.ClassificationOutput(scores=probs),\n    }\n    return exports\n\n\ndef FastTextEstimator(model_dir, config=None):\n    params = {\n        ""learning_rate"": FLAGS.learning_rate,\n    }\n    def model_fn(features, labels, mode, params):\n        features[""text""] = tf.sparse_tensor_to_dense(features[""text""],\n                                                     default_value="" "")\n        if FLAGS.use_ngrams:\n            features[""ngrams""] = tf.sparse_tensor_to_dense(features[""ngrams""],\n                                                           default_value="" "")\n        text_lookup_table = tf.contrib.lookup.index_table_from_file(\n            FLAGS.vocab_file, FLAGS.num_oov_vocab_buckets, FLAGS.vocab_size)\n        text_ids = text_lookup_table.lookup(features[""text""])\n        text_embedding_w = tf.Variable(tf.random_uniform(\n            [FLAGS.vocab_size + FLAGS.num_oov_vocab_buckets, FLAGS.embedding_dimension],\n            -0.1, 0.1))\n        text_embedding = tf.reduce_mean(tf.nn.embedding_lookup(\n            text_embedding_w, text_ids), axis=-2)\n        input_layer = text_embedding\n        if FLAGS.use_ngrams:\n            ngram_hash = tf.string_to_hash_bucket(features[""ngrams""],\n                                                  FLAGS.num_ngram_buckets)\n            ngram_embedding_w = tf.Variable(tf.random_uniform(\n                [FLAGS.num_ngram_buckets, FLAGS.ngram_embedding_dimension], -0.1, 0.1))\n            ngram_embedding = tf.reduce_mean(tf.nn.embedding_lookup(\n                ngram_embedding_w, ngram_hash), axis=-2)\n            ngram_embedding = tf.expand_dims(ngram_embedding, -2)\n            input_layer = tf.concat([text_embedding, ngram_embedding], -1)\n        num_classes = FLAGS.num_labels\n        logits = tf.contrib.layers.fully_connected(\n            inputs=input_layer, num_outputs=num_classes,\n            activation_fn=None)\n        predictions = tf.argmax(logits, axis=-1)\n        probs = tf.nn.softmax(logits)\n        loss, train_op = None, None\n        metrics = {}\n        if mode != tf.estimator.ModeKeys.PREDICT:\n            label_lookup_table = tf.contrib.lookup.index_table_from_file(\n                FLAGS.label_file, vocab_size=FLAGS.num_labels)\n            labels = label_lookup_table.lookup(labels)\n            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=labels, logits=logits))\n            opt = tf.train.AdamOptimizer(params[""learning_rate""])\n            if FLAGS.horovod:\n                opt = hvd.DistributedOptimizer(opt)\n            train_op = opt.minimize(loss, global_step=tf.train.get_global_step())\n            metrics = {\n                ""accuracy"": tf.metrics.accuracy(labels, predictions)\n            }\n        exports = {}\n        if FLAGS.export_dir:\n            exports = Exports(probs, text_embedding)\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, loss=loss, train_op=train_op,\n            eval_metric_ops=metrics, export_outputs=exports)\n    session_config = tf.ConfigProto(\n        log_device_placement=FLAGS.log_device_placement)\n    if FLAGS.horovod:\n        session_config.gpu_options.visible_device_list = str(hvd.local_rank())\n    config = tf.contrib.learn.RunConfig(\n        save_checkpoints_secs=None,\n        save_checkpoints_steps=FLAGS.checkpoint_steps,\n        session_config=session_config)\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir,\n                                  params=params, config=config)\n\n\ndef FastTrain():\n    print(""FastTrain"", FLAGS.train_steps)\n    estimator = FastTextEstimator(FLAGS.model_dir)\n    print(""TEST"" + FLAGS.train_records)\n    train_input = InputFn(tf.estimator.ModeKeys.TRAIN, FLAGS.train_records)\n    print(""STARTING TRAIN"")\n    hooks = None\n    if FLAGS.horovod:\n        hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n    estimator.train(input_fn=train_input, steps=FLAGS.train_steps, hooks=hooks)\n    print(""TRAIN COMPLETE"")\n    if not FLAGS.horovod or hvd.rank() == 0:\n        print(""EVALUATE"")\n        eval_input = InputFn(tf.estimator.ModeKeys.EVAL, FLAGS.eval_records)\n        #eval_metrics = { ""accuracy"": tf.metrics.accuracy(labels, predictions) }\n        result = estimator.evaluate(input_fn=eval_input, steps=FLAGS.eval_steps, hooks=None)\n        print(result)\n        print(""DONE"")\n        if FLAGS.export_dir:\n            print(""EXPORTING"")\n            estimator.export_savedmodel(FLAGS.export_dir,\n                                        inputs.ServingInputFn(FLAGS.use_ngrams))\n\n\ndef main(_):\n    if not FLAGS.vocab_size:\n        FLAGS.vocab_size = len(open(FLAGS.vocab_file).readlines())\n    if not FLAGS.num_labels:\n        FLAGS.num_labels = len(open(FLAGS.label_file).readlines())\n    if FLAGS.horovod:\n        nproc = hvd.size()\n        total = FLAGS.train_steps\n        FLAGS.train_steps = total / nproc\n        print(""Running %d steps on each of %d processes for %d total"" % (\n            FLAGS.train_steps, nproc, total))\n    FastTrain()\n\n\nif __name__ == \'__main__\':\n    if FLAGS.debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    tf.app.run()\n'"
inputs.py,11,"b'""""""Input feature columns and input_fn for models.\n\nHandles both training, evaluation and inference.\n""""""\nimport tensorflow as tf\n\n\ndef BuildTextExample(text, ngrams=None, label=None):\n    record = tf.train.Example()\n    text = [tf.compat.as_bytes(x) for x in text]\n    record.features.feature[""text""].bytes_list.value.extend(text)\n    if label is not None:\n        label = tf.compat.as_bytes(label)\n        record.features.feature[""label""].bytes_list.value.append(label)\n    if ngrams is not None:\n        ngrams = [tf.compat.as_bytes(x) for x in ngrams]\n        record.features.feature[""ngrams""].bytes_list.value.extend(ngrams)\n    return record\n\n\ndef ParseSpec(use_ngrams, include_target):\n    parse_spec = {""text"": tf.VarLenFeature(dtype=tf.string)}\n    if use_ngrams:\n        parse_spec[""ngrams""] = tf.VarLenFeature(dtype=tf.string)\n    if include_target:\n        parse_spec[""label""] = tf.FixedLenFeature(shape=(), dtype=tf.string,\n                                                 default_value=None)\n    return parse_spec\n\n\ndef InputFn(mode,\n            use_ngrams,\n            input_file,\n            vocab_file,\n            vocab_size,\n            embedding_dimension,\n            num_oov_vocab_buckets,\n            label_file,\n            label_size,\n            ngram_embedding_dimension,            \n            num_ngram_hash_buckets,\n            batch_size,\n            num_epochs=None,\n            num_threads=1):\n    if num_epochs <= 0:\n        num_epochs=None\n    def input_fn():\n        include_target =  mode != tf.estimator.ModeKeys.PREDICT\n        parse_spec = ParseSpec(use_ngrams, include_target)\n        print(""ParseSpec"", parse_spec)\n        print(""Input file:"", input_file)\n        features = tf.contrib.learn.read_batch_features(\n            input_file, batch_size, parse_spec, tf.TFRecordReader,\n            num_epochs=num_epochs, reader_num_threads=num_threads)\n        label = None\n        if include_target:\n            label = features.pop(""label"")\n        return features, label\n    return input_fn\n\n\ndef ServingInputFn(use_ngrams):\n    parse_spec = ParseSpec(use_ngrams, include_target=False)\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(\n        parse_spec)\n'"
inputs_test.py,20,"b'import sys\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom inputs import FeatureColumns, InputFn\n\nVOCAB_FILE=\'/home/alan/Workspace/other/fastText/data/ag_news.train.vocab\'\nVOCAB_SIZE=95810\nINPUT_FILE=\'/home/alan/Workspace/other/fastText/data/ag_news.train.tfrecords-1-of-1\'\n\ndef test_parse_spec():\n    fc = FeatureColumns(\n        True,\n        False,\n        VOCAB_FILE,\n        VOCAB_SIZE,\n        10,\n        10,\n        1000,\n        10)\n    parse_spec = tf.feature_column.make_parse_example_spec(fc)\n    print parse_spec\n    reader = tf.python_io.tf_record_iterator(INPUT_FILE)\n    sess = tf.Session()\n    for record in reader:\n        example = tf.parse_single_example(\n            record,\n            parse_spec)\n        print sess.run(example)\n        break\n\n\ndef test_reading_inputs():\n    parse_spec = {\n        ""text"": tf.VarLenFeature(tf.string),\n        ""label"": tf.FixedLenFeature(shape=(1,), dtype=tf.int64,\n                                    default_value=None)\n    }\n    sess = tf.Session()\n    reader = tf.python_io.tf_record_iterator(INPUT_FILE)\n    ESZ = 4\n    HSZ = 100\n    NC = 4\n    n = 0\n    text_lookup_table = tf.contrib.lookup.index_table_from_file(\n        VOCAB_FILE, 10, VOCAB_SIZE)\n    text_embedding_w = tf.Variable(tf.random_uniform(\n        [VOCAB_SIZE, ESZ], -1.0, 1.0))\n    sess.run([tf.tables_initializer()])\n    for record in reader:\n        example = tf.parse_single_example(\n            record,\n            parse_spec)\n        text = example[""text""]\n        labels = tf.subtract(example[""label""], 1)\n        text_ids = text_lookup_table.lookup(text)\n        dense = tf.sparse_tensor_to_dense(text_ids)\n        print dense.shape\n        text_embedding = tf.reduce_mean(tf.nn.embedding_lookup(\n            text_embedding_w, dense), axis=-2)\n        print text_embedding.shape\n        text_embedding = tf.expand_dims(text_embedding, -2)\n        print text_embedding.shape\n        text_embedding_2 = tf.contrib.layers.bow_encoder(\n            dense, VOCAB_SIZE, ESZ)\n        print text_embedding_2.shape\n        num_classes = 2\n        logits = tf.contrib.layers.fully_connected(\n            inputs=text_embedding, num_outputs=4,\n            activation_fn=None)\n        sess.run([tf.global_variables_initializer()])\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=labels, logits=logits)\n        x = sess.run([text_embedding, text_embedding_2, logits, labels, loss])\n        print(len(x), list(str(x[i]) for i in range(len(x))))\n        if n > 2:\n            break\n        n += 1\n\n\nif __name__ == \'__main__\':\n    print ""Test Parse Spec:""\n    test_parse_spec()\n    print ""Test Input Fn""\n    test_reading_inputs()\n'"
predictor.py,10,"b'""""""Predict classification on provided text.\n\nUses a SavedModel produced by classifier.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport inputs\nimport text_utils\nfrom tensorflow.contrib.saved_model.python.saved_model import reader\nfrom tensorflow.contrib.saved_model.python.saved_model import signature_def_utils\nfrom tensorflow.python.saved_model import loader\n\ntf.flags.DEFINE_string(""text"", None, ""Text to predict label of"")\ntf.flags.DEFINE_string(""ngrams"", None, ""List of ngram lengths, E.g. --ngrams=2,3,4"")\ntf.flags.DEFINE_string(""signature_def"", ""proba"",\n                       ""Stored signature key of method to call (proba|embedding)"")\ntf.flags.DEFINE_string(""saved_model"", None, ""Directory of SavedModel"")\ntf.flags.DEFINE_string(""tag"", ""serve"", ""SavedModel tag, serve|gpu"")\ntf.flags.DEFINE_boolean(""debug"", False, ""Debug"")\nFLAGS = tf.flags.FLAGS\n\n\ndef RunModel(saved_model_dir, signature_def_key, tag, text, ngrams_list=None):\n    saved_model = reader.read_saved_model(saved_model_dir)\n    meta_graph =  None\n    for meta_graph_def in saved_model.meta_graphs:\n        if tag in meta_graph_def.meta_info_def.tags:\n            meta_graph = meta_graph_def\n            break\n    if meta_graph_def is None:\n        raise ValueError(""Cannot find saved_model with tag"" + tag)\n    signature_def = signature_def_utils.get_signature_def_by_key(\n        meta_graph, signature_def_key)\n    text = text_utils.TokenizeText(text)\n    ngrams = None\n    if ngrams_list is not None:\n        ngrams_list = text_utils.ParseNgramsOpts(ngrams_list)\n        ngrams = text_utils.GenerateNgrams(text, ngrams_list)\n    example = inputs.BuildTextExample(text, ngrams=ngrams)\n    example = example.SerializeToString()\n    inputs_feed_dict = {\n        signature_def.inputs[""inputs""].name: [example],\n    }\n    if signature_def_key == ""proba"":\n        output_key = ""scores""\n    elif signature_def_key == ""embedding"":\n        output_key = ""outputs""\n    else:\n        raise ValueError(""Unrecognised signature_def %s"" % (signature_def_key))\n    output_tensor = signature_def.outputs[output_key].name\n    with tf.Session() as sess:\n        loader.load(sess, [tag], saved_model_dir)\n        outputs = sess.run(output_tensor,\n                           feed_dict=inputs_feed_dict)\n        return outputs\n\n\ndef main(_):\n    if not FLAGS.text:\n        raise ValueError(""No --text provided"")\n    outputs = RunModel(FLAGS.saved_model, FLAGS.signature_def, FLAGS.tag,\n                       FLAGS.text, FLAGS.ngrams)\n    if FLAGS.signature_def == ""proba"":\n        print(""Proba:"", outputs)\n        print(""Class(1-N):"", np.argmax(outputs) + 1)\n    elif FLAGS.signature_def == ""embedding"":\n        print(outputs[0])\n\n\nif __name__ == \'__main__\':\n    if FLAGS.debug:\n        tf.logging.set_verbosity(tf.logging.DEBUG)\n    tf.app.run()\n    \n'"
predictor_client.py,6,"b'""""""Predict classification on provided text.\n\nSend request to a tensorflow_model_server.\n\n   tensorflow_model_server --port=9000 --model_base_path=$export_dir_base\n\nUsage:\n   \n   predictor_client.py --text=\'some text\' --ngrams=1,2,4\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport inputs\nimport text_utils\n\nfrom grpc.beta import implementations\nfrom tensorflow_serving.apis import classification_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\n\ntf.flags.DEFINE_string(\'server\', \'localhost:9000\',\n                       \'TensorflowService host:port\')\ntf.flags.DEFINE_string(""text"", None, ""Text to predict label of"")\ntf.flags.DEFINE_string(""ngrams"", None, ""List of ngram lengths, E.g. --ngrams=2,3,4"")\ntf.flags.DEFINE_string(""signature_def"", ""proba"",\n                       ""Stored signature key of method to call (proba|embedding)"")\nFLAGS = tf.flags.FLAGS\n\n\ndef Request(text, ngrams):\n    text = text_utils.TokenizeText(text)\n    ngrams = None\n    if ngrams is not None:\n        ngrams_list = text_utils.ParseNgramsOpts(ngrams)\n        ngrams = text_utils.GenerateNgrams(text, ngrams_list)\n    example = inputs.BuildTextExample(text, ngrams=ngrams)\n    request = classification_pb2.ClassificationRequest()\n    request.model_spec.name = \'default\'\n    request.model_spec.signature_name = \'proba\'\n    request.input.example_list.examples.extend([example])\n    return request\n\n\ndef main(_):\n    if not FLAGS.text:\n        raise ValueError(""No --text provided"")\n    host, port = FLAGS.server.split(\':\')\n    channel = implementations.insecure_channel(host, int(port))\n    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n    request = Request(FLAGS.text, FLAGS.ngrams)\n    result = stub.Classify(request, 10.0)  # 10 secs timeout\n    print(result)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n    \n'"
process_input.py,9,"b'""""""Process input data into tensorflow examples, to ease training.\n\nInput data is in one of two formats:\n- facebook\'s format used in their fastText library.\n- two text files, one with input text per line, the other a label per line.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport re\nimport sys\nimport tensorflow as tf\nimport inputs\nimport text_utils\nfrom collections import Counter\nfrom six.moves import zip\n\n\ntf.flags.DEFINE_string(""facebook_input"", None,\n                       ""Input file in facebook train|test format"")\ntf.flags.DEFINE_string(""text_input"", None,\n                       """"""Input text file containing one text phrase per line.\n                       Must have --labels defined\n                       Used instead of --facebook_input"""""")\ntf.flags.DEFINE_string(""labels"", None,\n                       """"""Input text file containing one label for\n                       classification  per line.\n                       Must have --text_input defined.\n                       Used instead of --facebook_input"""""")\ntf.flags.DEFINE_string(""ngrams"", None,\n                       ""list of ngram sizes to create, e.g. --ngrams=2,3,4,5"")\ntf.flags.DEFINE_string(""output_dir"", ""."",\n                       ""Directory to store resulting vector models and checkpoints in"")\ntf.flags.DEFINE_integer(""num_shards"", 1,\n                        ""Number of outputfiles to create"")\nFLAGS = tf.flags.FLAGS\n\n\ndef ParseFacebookInput(inputfile, ngrams):\n    """"""Parse input in the format used by facebook FastText.\n    labels are formatted as __label__1\n    where the label values start at 0.\n    """"""\n    examples = []\n    for line in open(inputfile):\n        words = line.split()\n        # label is first field with __label__ removed\n        match = re.match(r\'__label__(.+)\', words[0])\n        label = match.group(1) if match else None\n        # Strip out label and first ,\n        first = 2 if words[1] == "","" else 1\n        words = words[first:]\n        examples.append({\n            ""text"": words,\n            ""label"": label\n        })\n        if ngrams:\n            examples[-1][""ngrams""] = text_utils.GenerateNgrams(words, ngrams)\n    return examples\n\n\ndef ParseTextInput(textfile, labelsfie, ngrams):\n    """"""Parse input from two text files: text and labels.\n    labels are specified 0-offset one per line.\n    """"""\n    examples = []\n    with open(textfile) as f1, open(labelsfile) as f2:\n        for text, label in zip(f1, f2):\n            words = text_utils.TokenizeText(text)\n            examples.append({\n                ""text"": words,\n                ""label"": label,\n            })\n            if ngrams:\n                examples[-1][""ngrams""] = text_utils.GenerateNgrams(words, ngrams)\n    return examples\n\n\ndef WriteExamples(examples, outputfile, num_shards):\n    """"""Write examles in TFRecord format.\n    Args:\n      examples: list of feature dicts.\n                {\'text\': [words], \'label\': [labels]}\n      outputfile: full pathname of output file\n    """"""\n    shard = 0\n    num_per_shard = len(examples) / num_shards + 1\n    for n, example in enumerate(examples):\n        if n % num_per_shard == 0:\n            shard += 1\n            writer = tf.python_io.TFRecordWriter(outputfile + \'-%d-of-%d\' % \\\n                                                 (shard, num_shards))\n        record = inputs.BuildTextExample(\n            example[""text""], example.get(""ngrams"", None), example[""label""])\n        writer.write(record.SerializeToString())\n\n\ndef WriteVocab(examples, vocabfile, labelfile):\n    words = Counter()\n    labels = set()\n    for example in examples:\n        words.update(example[""text""])\n        labels.add(example[""label""])\n    with open(vocabfile, ""w"") as f:\n        # Write out vocab in most common first order\n        # We need this as NCE loss in TF uses Zipf distribution\n        for word in words.most_common():\n            f.write(word[0] + \'\\n\')\n    with open(labelfile, ""w"") as f:\n        labels = sorted(list(labels))\n        for label in labels:\n            f.write(str(label) + \'\\n\')\n\n\ndef main(_):\n    # Check flags\n    if not (FLAGS.facebook_input or (FLAGS.text_input and FLAGS.labels)):\n        print >>sys.stderr, \\\n            ""Error: You must define either facebook_input or both text_input and labels""\n        sys.exit(1)\n    ngrams = None\n    if FLAGS.ngrams:\n        ngrams = text_utils.ParseNgramsOpts(FLAGS.ngrams)\n    if FLAGS.facebook_input:\n        inputfile = FLAGS.facebook_input\n        examples = ParseFacebookInput(FLAGS.facebook_input, ngrams)\n    else:\n        inputfile = FLAGS.text_input\n        examples = ParseTextInput(FLAGS.text_input, FLAGS.labels, ngrams)\n    outputfile = os.path.join(FLAGS.output_dir, inputfile + "".tfrecords"")\n    WriteExamples(examples, outputfile, FLAGS.num_shards)\n    vocabfile = os.path.join(FLAGS.output_dir, inputfile + "".vocab"")\n    labelfile = os.path.join(FLAGS.output_dir, inputfile + "".labels"")\n    WriteVocab(examples, vocabfile, labelfile)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
text_utils.py,0,"b""from nltk.tokenize import word_tokenize\n\n\ndef TokenizeText(text):\n    return word_tokenize(text.lower())\n\n\ndef ParseNgramsOpts(opts):\n    ngrams = [int(g) for g in opts.split(',')]\n    ngrams = [g for g in ngrams if (g > 1 and g < 7)]\n    return ngrams\n\n\ndef GenerateNgrams(words, ngrams):\n    nglist = []\n    for ng in ngrams:\n        for word in words:\n            nglist.extend([word[n:n+ng] for n in range(len(word)-ng+1)])\n    return nglist\n"""
