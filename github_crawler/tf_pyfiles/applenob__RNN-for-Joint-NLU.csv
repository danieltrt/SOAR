file_path,api_count,code
data.py,0,"b'# coding=utf-8\n# @author: cer\n\nimport random\nimport numpy as np\n\n\nflatten = lambda l: [item for sublist in l for item in sublist]  # \xe4\xba\x8c\xe7\xbb\xb4\xe5\xb1\x95\xe6\x88\x90\xe4\xb8\x80\xe7\xbb\xb4\nindex_seq2slot = lambda s, index2slot: [index2slot[i] for i in s]\nindex_seq2word = lambda s, index2word: [index2word[i] for i in s]\n\n\ndef data_pipeline(data, length=50):\n    data = [t[:-1] for t in data]  # \xe5\x8e\xbb\xe6\x8e\x89\'\\n\'\n    # \xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe4\xb8\x80\xe8\xa1\x8c\xe5\x83\x8f\xe8\xbf\x99\xe6\xa0\xb7\xef\xbc\x9a\'BOS i want to fly from baltimore to dallas round trip EOS\n    # \\tO O O O O O B-fromloc.city_name O B-toloc.city_name B-round_trip I-round_trip atis_flight\'\n    # \xe5\x88\x86\xe5\x89\xb2\xe6\x88\x90\xe8\xbf\x99\xe6\xa0\xb7[\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe8\xaf\x8d\xef\xbc\x8c\xe6\xa0\x87\xe6\xb3\xa8\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xef\xbc\x8cintent]\n    data = [[t.split(""\\t"")[0].split("" ""), t.split(""\\t"")[1].split("" "")[:-1], t.split(""\\t"")[1].split("" "")[-1]] for t in\n            data]\n    data = [[t[0][1:-1], t[1][1:], t[2]] for t in data]  # \xe5\xb0\x86BOS\xe5\x92\x8cEOS\xe5\x8e\xbb\xe6\x8e\x89\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8e\xbb\xe6\x8e\x89\xe5\xaf\xb9\xe5\xba\x94\xe6\xa0\x87\xe6\xb3\xa8\xe5\xba\x8f\xe5\x88\x97\xe4\xb8\xad\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe6\xa0\x87\xe6\xb3\xa8\n    seq_in, seq_out, intent = list(zip(*data))\n    sin = []\n    sout = []\n    # padding\xef\xbc\x8c\xe5\x8e\x9f\xe5\xa7\x8b\xe5\xba\x8f\xe5\x88\x97\xe5\x92\x8c\xe6\xa0\x87\xe6\xb3\xa8\xe5\xba\x8f\xe5\x88\x97\xe7\xbb\x93\xe5\xb0\xbe+<EOS>+n\xc3\x97<PAD>\n    for i in range(len(seq_in)):\n        temp = seq_in[i]\n        if len(temp) < length:\n            temp.append(\'<EOS>\')\n            while len(temp) < length:\n                temp.append(\'<PAD>\')\n        else:\n            temp = temp[:length]\n            temp[-1] = \'<EOS>\'\n        sin.append(temp)\n\n        temp = seq_out[i]\n        if len(temp) < length:\n            while len(temp) < length:\n                temp.append(\'<PAD>\')\n        else:\n            temp = temp[:length]\n            temp[-1] = \'<EOS>\'\n        sout.append(temp)\n        data = list(zip(sin, sout, intent))\n    return data\n\n\ndef get_info_from_training_data(data):\n    seq_in, seq_out, intent = list(zip(*data))\n    vocab = set(flatten(seq_in))\n    slot_tag = set(flatten(seq_out))\n    intent_tag = set(intent)\n    # \xe7\x94\x9f\xe6\x88\x90word2index\n    word2index = {\'<PAD>\': 0, \'<UNK>\': 1, \'<SOS>\': 2, \'<EOS>\': 3}\n    for token in vocab:\n        if token not in word2index.keys():\n            word2index[token] = len(word2index)\n\n    # \xe7\x94\x9f\xe6\x88\x90index2word\n    index2word = {v: k for k, v in word2index.items()}\n\n    # \xe7\x94\x9f\xe6\x88\x90tag2index\n    tag2index = {\'<PAD>\': 0, \'<UNK>\': 1, ""O"": 2}\n    for tag in slot_tag:\n        if tag not in tag2index.keys():\n            tag2index[tag] = len(tag2index)\n\n    # \xe7\x94\x9f\xe6\x88\x90index2tag\n    index2tag = {v: k for k, v in tag2index.items()}\n\n    # \xe7\x94\x9f\xe6\x88\x90intent2index\n    intent2index = {\'<UNK>\': 0}\n    for ii in intent_tag:\n        if ii not in intent2index.keys():\n            intent2index[ii] = len(intent2index)\n\n    # \xe7\x94\x9f\xe6\x88\x90index2intent\n    index2intent = {v: k for k, v in intent2index.items()}\n    return word2index, index2word, tag2index, index2tag, intent2index, index2intent\n\n\ndef getBatch(batch_size, train_data):\n    random.shuffle(train_data)\n    sindex = 0\n    eindex = batch_size\n    while eindex < len(train_data):\n        batch = train_data[sindex:eindex]\n        temp = eindex\n        eindex = eindex + batch_size\n        sindex = temp\n        yield batch\n\n\ndef to_index(train, word2index, slot2index, intent2index):\n    new_train = []\n    for sin, sout, intent in train:\n        sin_ix = list(map(lambda i: word2index[i] if i in word2index else word2index[""<UNK>""],\n                          sin))\n        true_length = sin.index(""<EOS>"")\n        sout_ix = list(map(lambda i: slot2index[i] if i in slot2index else slot2index[""<UNK>""],\n                           sout))\n        intent_ix = intent2index[intent] if intent in intent2index else intent2index[""<UNK>""]\n        new_train.append([sin_ix, true_length, sout_ix, intent_ix])\n    return new_train'"
main.py,3,"b'# coding=utf-8\n# @author: cer\nimport tensorflow as tf\nfrom data import *\n# from model import Model\nfrom model import Model\nfrom my_metrics import *\nfrom tensorflow.python import debug as tf_debug\nimport numpy as np\n\ninput_steps = 50\nembedding_size = 64\nhidden_size = 100\nn_layers = 2\nbatch_size = 16\nvocab_size = 871\nslot_size = 122\nintent_size = 22\nepoch_num = 50\n\n\ndef get_model():\n    model = Model(input_steps, embedding_size, hidden_size, vocab_size, slot_size,\n                 intent_size, epoch_num, batch_size, n_layers)\n    model.build()\n    return model\n\n\ndef train(is_debug=False):\n    model = get_model()\n    sess = tf.Session()\n    if is_debug:\n        sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n        sess.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)\n    sess.run(tf.global_variables_initializer())\n    # print(tf.trainable_variables())\n    train_data = open(""dataset/atis-2.train.w-intent.iob"", ""r"").readlines()\n    test_data = open(""dataset/atis-2.dev.w-intent.iob"", ""r"").readlines()\n    train_data_ed = data_pipeline(train_data)\n    test_data_ed = data_pipeline(test_data)\n    word2index, index2word, slot2index, index2slot, intent2index, index2intent = \\\n        get_info_from_training_data(train_data_ed)\n    # print(""slot2index: "", slot2index)\n    # print(""index2slot: "", index2slot)\n    index_train = to_index(train_data_ed, word2index, slot2index, intent2index)\n    index_test = to_index(test_data_ed, word2index, slot2index, intent2index)\n    for epoch in range(epoch_num):\n        mean_loss = 0.0\n        train_loss = 0.0\n        for i, batch in enumerate(getBatch(batch_size, index_train)):\n            # \xe6\x89\xa7\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\n            _, loss, decoder_prediction, intent, mask, slot_W = model.step(sess, ""train"", batch)\n            # if i == 0:\n            #     index = 0\n            #     print(""training debug:"")\n            #     print(""input:"", list(zip(*batch))[0][index])\n            #     print(""length:"", list(zip(*batch))[1][index])\n            #     print(""mask:"", mask[index])\n            #     print(""target:"", list(zip(*batch))[2][index])\n            #     # print(""decoder_targets_one_hot:"")\n            #     # for one in decoder_targets_one_hot[index]:\n            #     #     print("" "".join(map(str, one)))\n            #     print(""decoder_logits: "")\n            #     for one in decoder_logits[index]:\n            #         print("" "".join(map(str, one)))\n            #     print(""slot_W:"", slot_W)\n            #     print(""decoder_prediction:"", decoder_prediction[index])\n            #     print(""intent:"", list(zip(*batch))[3][index])\n            # mean_loss += loss\n            # train_loss += loss\n            # if i % 10 == 0:\n            #     if i > 0:\n            #         mean_loss = mean_loss / 10.0\n            #     print(\'Average train loss at epoch %d, step %d: %f\' % (epoch, i, mean_loss))\n            #     mean_loss = 0\n        train_loss /= (i + 1)\n        print(""[Epoch {}] Average train loss: {}"".format(epoch, train_loss))\n\n        # \xe6\xaf\x8f\xe8\xae\xad\xe4\xb8\x80\xe4\xb8\xaaepoch\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb8\x80\xe6\xac\xa1\n        pred_slots = []\n        slot_accs = []\n        intent_accs = []\n        for j, batch in enumerate(getBatch(batch_size, index_test)):\n            decoder_prediction, intent = model.step(sess, ""test"", batch)\n            decoder_prediction = np.transpose(decoder_prediction, [1, 0])\n            if j == 0:\n                index = random.choice(range(len(batch)))\n                # index = 0\n                sen_len = batch[index][1]\n                print(""Input Sentence        : "", index_seq2word(batch[index][0], index2word)[:sen_len])\n                print(""Slot Truth            : "", index_seq2slot(batch[index][2], index2slot)[:sen_len])\n                print(""Slot Prediction       : "", index_seq2slot(decoder_prediction[index], index2slot)[:sen_len])\n                print(""Intent Truth          : "", index2intent[batch[index][3]])\n                print(""Intent Prediction     : "", index2intent[intent[index]])\n            slot_pred_length = list(np.shape(decoder_prediction))[1]\n            pred_padded = np.lib.pad(decoder_prediction, ((0, 0), (0, input_steps-slot_pred_length)),\n                                     mode=""constant"", constant_values=0)\n            pred_slots.append(pred_padded)\n            # print(""slot_pred_length: "", slot_pred_length)\n            true_slot = np.array((list(zip(*batch))[2]))\n            true_length = np.array((list(zip(*batch))[1]))\n            true_slot = true_slot[:, :slot_pred_length]\n            # print(np.shape(true_slot), np.shape(decoder_prediction))\n            # print(true_slot, decoder_prediction)\n            slot_acc = accuracy_score(true_slot, decoder_prediction, true_length)\n            intent_acc = accuracy_score(list(zip(*batch))[3], intent)\n            # print(""slot accuracy: {}, intent accuracy: {}"".format(slot_acc, intent_acc))\n            slot_accs.append(slot_acc)\n            intent_accs.append(intent_acc)\n        pred_slots_a = np.vstack(pred_slots)\n        # print(""pred_slots_a: "", pred_slots_a.shape)\n        true_slots_a = np.array(list(zip(*index_test))[2])[:pred_slots_a.shape[0]]\n        # print(""true_slots_a: "", true_slots_a.shape)\n        print(""Intent accuracy for epoch {}: {}"".format(epoch, np.average(intent_accs)))\n        print(""Slot accuracy for epoch {}: {}"".format(epoch, np.average(slot_accs)))\n        print(""Slot F1 score for epoch {}: {}"".format(epoch, f1_for_sequence_batch(true_slots_a, pred_slots_a)))\n\n\ndef test_data():\n    train_data = open(""dataset/atis-2.train.w-intent.iob"", ""r"").readlines()\n    test_data = open(""dataset/atis-2.dev.w-intent.iob"", ""r"").readlines()\n    train_data_ed = data_pipeline(train_data)\n    test_data_ed = data_pipeline(test_data)\n    word2index, index2word, slot2index, index2slot, intent2index, index2intent = \\\n        get_info_from_training_data(train_data_ed)\n    # print(""slot2index: "", slot2index)\n    # print(""index2slot: "", index2slot)\n    index_train = to_index(train_data_ed, word2index, slot2index, intent2index)\n    index_test = to_index(test_data_ed, word2index, slot2index, intent2index)\n    batch = next(getBatch(batch_size, index_test))\n    unziped = list(zip(*batch))\n    print(""word num: "", len(word2index.keys()), ""slot num: "", len(slot2index.keys()), ""intent num: "",\n          len(intent2index.keys()))\n    print(np.shape(unziped[0]), np.shape(unziped[1]), np.shape(unziped[2]), np.shape(unziped[3]))\n    print(np.transpose(unziped[0], [1, 0]))\n    print(unziped[1])\n    print(np.shape(list(zip(*index_test))[2]))\n\n\nif __name__ == \'__main__\':\n    # train(is_debug=True)\n    # test_data()\n    train()\n'"
model.py,57,"b'# coding=utf-8\n# @author: cer\nimport tensorflow as tf\nfrom tensorflow.contrib import layers\nimport numpy as np\nfrom tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple,DropoutWrapper\nimport sys\n\n\nclass Model:\n    def __init__(self, input_steps, embedding_size, hidden_size, vocab_size, slot_size,\n                 intent_size, epoch_num, batch_size=16, n_layers=1):\n        self.input_steps = input_steps\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.batch_size = batch_size\n        self.vocab_size = vocab_size\n        self.slot_size = slot_size\n        self.intent_size = intent_size\n        self.epoch_num = epoch_num\n        self.encoder_inputs = tf.placeholder(tf.int32, [input_steps, batch_size],\n                                             name=\'encoder_inputs\')\n        # \xe6\xaf\x8f\xe5\x8f\xa5\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe9\x99\xa4\xe4\xba\x86padding\n        self.encoder_inputs_actual_length = tf.placeholder(tf.int32, [batch_size],\n                                                           name=\'encoder_inputs_actual_length\')\n        self.decoder_targets = tf.placeholder(tf.int32, [batch_size, input_steps],\n                                              name=\'decoder_targets\')\n        self.intent_targets = tf.placeholder(tf.int32, [batch_size],\n                                             name=\'intent_targets\')\n\n    def build(self):\n\n        self.embeddings = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size],\n                                                        -0.1, 0.1), dtype=tf.float32, name=""embedding"")\n\n        self.encoder_inputs_embedded = tf.nn.embedding_lookup(self.embeddings, self.encoder_inputs)\n\n        # Encoder\n\n        # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x8d\x95\xe4\xb8\xaaLSTM cell\n        encoder_f_cell_0 = LSTMCell(self.hidden_size)\n        encoder_b_cell_0 = LSTMCell(self.hidden_size)\n        encoder_f_cell = DropoutWrapper(encoder_f_cell_0,output_keep_prob=0.5)\n        encoder_b_cell = DropoutWrapper(encoder_b_cell_0,output_keep_prob=0.5)\n        # encoder_inputs_time_major = tf.transpose(self.encoder_inputs_embedded, perm=[1, 0, 2])\n        # \xe4\xb8\x8b\xe9\x9d\xa2\xe5\x9b\x9b\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x9aT*B*D\xef\xbc\x8cT*B*D\xef\xbc\x8cB*D\xef\xbc\x8cB*D\n        (encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_final_state, encoder_bw_final_state) = \\\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_f_cell,\n                                            cell_bw=encoder_b_cell,\n                                            inputs=self.encoder_inputs_embedded,\n                                            sequence_length=self.encoder_inputs_actual_length,\n                                            dtype=tf.float32, time_major=True)\n        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n\n        encoder_final_state_c = tf.concat(\n            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n\n        encoder_final_state_h = tf.concat(\n            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n\n        self.encoder_final_state = LSTMStateTuple(\n            c=encoder_final_state_c,\n            h=encoder_final_state_h\n        )\n        print(""encoder_outputs: "", encoder_outputs)\n        print(""encoder_outputs[0]: "", encoder_outputs[0])\n        print(""encoder_final_state_c: "", encoder_final_state_c)\n\n        # Decoder\n        decoder_lengths = self.encoder_inputs_actual_length\n        self.slot_W = tf.Variable(tf.random_uniform([self.hidden_size * 2, self.slot_size], -1, 1),\n                             dtype=tf.float32, name=""slot_W"")\n        self.slot_b = tf.Variable(tf.zeros([self.slot_size]), dtype=tf.float32, name=""slot_b"")\n        intent_W = tf.Variable(tf.random_uniform([self.hidden_size * 2, self.intent_size], -0.1, 0.1),\n                               dtype=tf.float32, name=""intent_W"")\n        intent_b = tf.Variable(tf.zeros([self.intent_size]), dtype=tf.float32, name=""intent_b"")\n\n        # \xe6\xb1\x82intent\n        intent_logits = tf.add(tf.matmul(encoder_final_state_h, intent_W), intent_b)\n        # intent_prob = tf.nn.softmax(intent_logits)\n        self.intent = tf.argmax(intent_logits, axis=1)\n\n        sos_time_slice = tf.ones([self.batch_size], dtype=tf.int32, name=\'SOS\') * 2\n        sos_step_embedded = tf.nn.embedding_lookup(self.embeddings, sos_time_slice)\n        # pad_time_slice = tf.zeros([self.batch_size], dtype=tf.int32, name=\'PAD\')\n        # pad_step_embedded = tf.nn.embedding_lookup(self.embeddings, pad_time_slice)\n        pad_step_embedded = tf.zeros([self.batch_size, self.hidden_size*2+self.embedding_size],\n                                     dtype=tf.float32)\n\n        def initial_fn():\n            initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n            initial_input = tf.concat((sos_step_embedded, encoder_outputs[0]), 1)\n            return initial_elements_finished, initial_input\n\n        def sample_fn(time, outputs, state):\n            # \xe9\x80\x89\xe6\x8b\xa9logit\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xe4\xbd\x9c\xe4\xb8\xbasample\n            print(""outputs"", outputs)\n            # output_logits = tf.add(tf.matmul(outputs, self.slot_W), self.slot_b)\n            # print(""slot output_logits: "", output_logits)\n            # prediction_id = tf.argmax(output_logits, axis=1)\n            prediction_id = tf.to_int32(tf.argmax(outputs, axis=1))\n            return prediction_id\n\n        def next_inputs_fn(time, outputs, state, sample_ids):\n            # \xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96embedding\xe5\x86\x8d\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n            pred_embedding = tf.nn.embedding_lookup(self.embeddings, sample_ids)\n            # \xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafh_i+o_{i-1}+c_i\n            next_input = tf.concat((pred_embedding, encoder_outputs[time]), 1)\n            elements_finished = (time >= decoder_lengths)  # this operation produces boolean tensor of [batch_size]\n            all_finished = tf.reduce_all(elements_finished)  # -> boolean scalar\n            next_inputs = tf.cond(all_finished, lambda: pad_step_embedded, lambda: next_input)\n            next_state = state\n            return elements_finished, next_inputs, next_state\n\n        my_helper = tf.contrib.seq2seq.CustomHelper(initial_fn, sample_fn, next_inputs_fn)\n\n        def decode(helper, scope, reuse=None):\n            with tf.variable_scope(scope, reuse=reuse):\n                memory = tf.transpose(encoder_outputs, [1, 0, 2])\n                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n                    num_units=self.hidden_size, memory=memory,\n                    memory_sequence_length=self.encoder_inputs_actual_length)\n                cell = tf.contrib.rnn.LSTMCell(num_units=self.hidden_size * 2)\n                attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n                    cell, attention_mechanism, attention_layer_size=self.hidden_size)\n                out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n                    attn_cell, self.slot_size, reuse=reuse\n                )\n                decoder = tf.contrib.seq2seq.BasicDecoder(\n                    cell=out_cell, helper=helper,\n                    initial_state=out_cell.zero_state(\n                        dtype=tf.float32, batch_size=self.batch_size))\n                # initial_state=encoder_final_state)\n                final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n                    decoder=decoder, output_time_major=True,\n                    impute_finished=True, maximum_iterations=self.input_steps\n                )\n                return final_outputs\n\n        outputs = decode(my_helper, \'decode\')\n        print(""outputs: "", outputs)\n        print(""outputs.rnn_output: "", outputs.rnn_output)\n        print(""outputs.sample_id: "", outputs.sample_id)\n        # weights = tf.to_float(tf.not_equal(outputs[:, :-1], 0))\n        self.decoder_prediction = outputs.sample_id\n        decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(outputs.rnn_output))\n        self.decoder_targets_time_majored = tf.transpose(self.decoder_targets, [1, 0])\n        self.decoder_targets_true_length = self.decoder_targets_time_majored[:decoder_max_steps]\n        print(""decoder_targets_true_length: "", self.decoder_targets_true_length)\n        # \xe5\xae\x9a\xe4\xb9\x89mask\xef\xbc\x8c\xe4\xbd\xbfpadding\xe4\xb8\x8d\xe8\xae\xa1\xe5\x85\xa5loss\xe8\xae\xa1\xe7\xae\x97\n        self.mask = tf.to_float(tf.not_equal(self.decoder_targets_true_length, 0))\n        # \xe5\xae\x9a\xe4\xb9\x89slot\xe6\xa0\x87\xe6\xb3\xa8\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n        loss_slot = tf.contrib.seq2seq.sequence_loss(\n            outputs.rnn_output, self.decoder_targets_true_length, weights=self.mask)\n        # \xe5\xae\x9a\xe4\xb9\x89intent\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n            labels=tf.one_hot(self.intent_targets, depth=self.intent_size, dtype=tf.float32),\n            logits=intent_logits)\n        loss_intent = tf.reduce_mean(cross_entropy)\n\n        self.loss = loss_slot + loss_intent\n        optimizer = tf.train.AdamOptimizer(name=""a_optimizer"")\n        self.grads, self.vars = zip(*optimizer.compute_gradients(self.loss))\n        print(""vars for loss function: "", self.vars)\n        self.gradients, _ = tf.clip_by_global_norm(self.grads, 5)  # clip gradients\n        self.train_op = optimizer.apply_gradients(zip(self.gradients, self.vars))\n        # self.train_op = optimizer.minimize(self.loss)\n        # train_op = layers.optimize_loss(\n        #     loss, tf.train.get_global_step(),\n        #     optimizer=optimizer,\n        #     learning_rate=0.001,\n        #     summaries=[\'loss\', \'learning_rate\'])\n\n    def step(self, sess, mode, trarin_batch):\n        """""" perform each batch""""""\n        if mode not in [\'train\', \'test\']:\n            print >> sys.stderr, \'mode is not supported\'\n            sys.exit(1)\n        unziped = list(zip(*trarin_batch))\n        # print(np.shape(unziped[0]), np.shape(unziped[1]),\n        #       np.shape(unziped[2]), np.shape(unziped[3]))\n        if mode == \'train\':\n            output_feeds = [self.train_op, self.loss, self.decoder_prediction,\n                            self.intent, self.mask, self.slot_W]\n            feed_dict = {self.encoder_inputs: np.transpose(unziped[0], [1, 0]),\n                         self.encoder_inputs_actual_length: unziped[1],\n                         self.decoder_targets: unziped[2],\n                         self.intent_targets: unziped[3]}\n        if mode in [\'test\']:\n            output_feeds = [self.decoder_prediction, self.intent]\n            feed_dict = {self.encoder_inputs: np.transpose(unziped[0], [1, 0]),\n                         self.encoder_inputs_actual_length: unziped[1]}\n\n        results = sess.run(output_feeds, feed_dict=feed_dict)\n        return results\n'"
my_metrics.py,0,"b'# coding=utf-8\n# @author: cer\nimport numpy as np\nimport numpy.ma as ma\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\n\ndef accuracy_score(true_data, pred_data, true_length=None):\n    true_data = np.array(true_data)\n    pred_data = np.array(pred_data)\n    assert true_data.shape == pred_data.shape\n    if true_length is not None:\n        val_num = np.sum(true_length)\n        assert val_num != 0\n        res = 0\n        for i in range(true_data.shape[0]):\n            res += np.sum(true_data[i, :true_length[i]] == pred_data[i, :true_length[i]])\n    else:\n        val_num = np.prod(true_data.shape)\n        assert val_num != 0\n        res = np.sum(true_data == pred_data)\n    res /= float(val_num)\n    return res\n\n\ndef get_data_from_sequence_batch(true_batch, pred_batch, padding_token):\n    """"""\xe4\xbb\x8e\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84batch\xe4\xb8\xad\xe6\x8f\x90\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\n    [[3,1,2,0,0,0],[5,2,1,4,0,0]] -> [3,1,2,5,2,1,4]""""""\n    true_ma = ma.masked_equal(true_batch, padding_token)\n    pred_ma = ma.masked_array(pred_batch, true_ma.mask)\n    true_ma = true_ma.flatten()\n    pred_ma = pred_ma.flatten()\n    true_ma = true_ma[~true_ma.mask]\n    pred_ma = pred_ma[~pred_ma.mask]\n    return true_ma, pred_ma\n\n\ndef f1_for_sequence_batch(true_batch, pred_batch, average=""micro"", padding_token=0):\n    true, pred = get_data_from_sequence_batch(true_batch, pred_batch, padding_token)\n    labels = list(set(true))\n    return f1_score(true, pred, labels=labels, average=average)\n\n\ndef accuracy_for_sequence_batch(true_batch, pred_batch, padding_token=0):\n    true, pred = get_data_from_sequence_batch(true_batch, pred_batch, padding_token)\n    return accuracy_score(true, pred)'"
