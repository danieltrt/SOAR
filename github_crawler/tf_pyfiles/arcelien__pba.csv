file_path,api_count,code
autoaugment/__init__.py,0,b''
autoaugment/augmentation_transforms.py,0,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Transforms used in the Augmentation Policies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\nimport random\nimport numpy as np\n# pylint:disable=g-multiple-import\nfrom PIL import ImageOps, ImageEnhance, ImageFilter, Image\n# pylint:enable=g-multiple-import\n\n\nIMAGE_SIZE = 32\n# What is the dataset mean and std of the images on the training set\nMEANS = [0.49139968, 0.48215841, 0.44653091]\nSTDS = [0.24703223, 0.24348513, 0.26158784]\nPARAMETER_MAX = 10  # What is the max \'level\' a transform could be predicted\n\n\ndef random_flip(x):\n  """"""Flip the input x horizontally with 50% probability.""""""\n  if np.random.rand(1)[0] > 0.5:\n    return np.fliplr(x)\n  return x\n\n\ndef zero_pad_and_crop(img, amount=4):\n  """"""Zero pad by `amount` zero pixels on each side then take a random crop.\n\n  Args:\n    img: numpy image that will be zero padded and cropped.\n    amount: amount of zeros to pad `img` with horizontally and verically.\n\n  Returns:\n    The cropped zero padded img. The returned numpy array will be of the same\n    shape as `img`.\n  """"""\n  padded_img = np.zeros((img.shape[0] + amount * 2, img.shape[1] + amount * 2,\n                         img.shape[2]))\n  padded_img[amount:img.shape[0] + amount, amount:\n             img.shape[1] + amount, :] = img\n  top = np.random.randint(low=0, high=2 * amount)\n  left = np.random.randint(low=0, high=2 * amount)\n  new_img = padded_img[top:top + img.shape[0], left:left + img.shape[1], :]\n  return new_img\n\n\ndef create_cutout_mask(img_height, img_width, num_channels, size):\n  """"""Creates a zero mask used for cutout of shape `img_height` x `img_width`.\n\n  Args:\n    img_height: Height of image cutout mask will be applied to.\n    img_width: Width of image cutout mask will be applied to.\n    num_channels: Number of channels in the image.\n    size: Size of the zeros mask.\n\n  Returns:\n    A mask of shape `img_height` x `img_width` with all ones except for a\n    square of zeros of shape `size` x `size`. This mask is meant to be\n    elementwise multiplied with the original image. Additionally returns\n    the `upper_coord` and `lower_coord` which specify where the cutout mask\n    will be applied.\n  """"""\n  assert img_height == img_width\n\n  # Sample center where cutout mask will be applied\n  height_loc = np.random.randint(low=0, high=img_height)\n  width_loc = np.random.randint(low=0, high=img_width)\n\n  # Determine upper right and lower left corners of patch\n  upper_coord = (max(0, height_loc - size // 2), max(0, width_loc - size // 2))\n  lower_coord = (min(img_height, height_loc + size // 2),\n                 min(img_width, width_loc + size // 2))\n  mask_height = lower_coord[0] - upper_coord[0]\n  mask_width = lower_coord[1] - upper_coord[1]\n  assert mask_height > 0\n  assert mask_width > 0\n\n  mask = np.ones((img_height, img_width, num_channels))\n  zeros = np.zeros((mask_height, mask_width, num_channels))\n  mask[upper_coord[0]:lower_coord[0], upper_coord[1]:lower_coord[1], :] = (\n      zeros)\n  return mask, upper_coord, lower_coord\n\n\ndef cutout_numpy(img, size=16):\n  """"""Apply cutout with mask of shape `size` x `size` to `img`.\n\n  The cutout operation is from the paper https://arxiv.org/abs/1708.04552.\n  This operation applies a `size`x`size` mask of zeros to a random location\n  within `img`.\n\n  Args:\n    img: Numpy image that cutout will be applied to.\n    size: Height/width of the cutout mask that will be\n\n  Returns:\n    A numpy tensor that is the result of applying the cutout mask to `img`.\n  """"""\n  img_height, img_width, num_channels = (img.shape[0], img.shape[1],\n                                         img.shape[2])\n  assert len(img.shape) == 3\n  mask, _, _ = create_cutout_mask(img_height, img_width, num_channels, size)\n  return img * mask\n\n\ndef float_parameter(level, maxval):\n  """"""Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    A float that results from scaling `maxval` according to `level`.\n  """"""\n  return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n  """"""Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    An int that results from scaling `maxval` according to `level`.\n  """"""\n  return int(level * maxval / PARAMETER_MAX)\n\n\ndef pil_wrap(img):\n  """"""Convert the `img` numpy tensor to a PIL Image.""""""\n  return Image.fromarray(\n      np.uint8((img * STDS + MEANS) * 255.0)).convert(\'RGBA\')\n\n\ndef pil_unwrap(pil_img):\n  """"""Converts the PIL img to a numpy array.""""""\n  pic_array = (np.array(pil_img.getdata()).reshape((32, 32, 4)) / 255.0)\n  i1, i2 = np.where(pic_array[:, :, 3] == 0)\n  pic_array = (pic_array[:, :, :3] - MEANS) / STDS\n  pic_array[i1, i2] = [0, 0, 0]\n  return pic_array\n\n\ndef apply_policy(policy, img):\n  """"""Apply the `policy` to the numpy `img`.\n\n  Args:\n    policy: A list of tuples with the form (name, probability, level) where\n      `name` is the name of the augmentation operation to apply, `probability`\n      is the probability of applying the operation and `level` is what strength\n      the operation to apply.\n    img: Numpy image that will have `policy` applied to it.\n\n  Returns:\n    The result of applying `policy` to `img`.\n  """"""\n  pil_img = pil_wrap(img)\n  for xform in policy:\n    assert len(xform) == 3\n    name, probability, level = xform\n    xform_fn = NAME_TO_TRANSFORM[name].pil_transformer(probability, level)\n    pil_img = xform_fn(pil_img)\n  return pil_unwrap(pil_img)\n\n\nclass TransformFunction(object):\n  """"""Wraps the Transform function for pretty printing options.""""""\n\n  def __init__(self, func, name):\n    self.f = func\n    self.name = name\n\n  def __repr__(self):\n    return \'<\' + self.name + \'>\'\n\n  def __call__(self, pil_img):\n    return self.f(pil_img)\n\n\nclass TransformT(object):\n  """"""Each instance of this class represents a specific transform.""""""\n\n  def __init__(self, name, xform_fn):\n    self.name = name\n    self.xform = xform_fn\n\n  def pil_transformer(self, probability, level):\n\n    def return_function(im):\n      if random.random() < probability:\n        im = self.xform(im, level)\n      return im\n\n    name = self.name + \'({:.1f},{})\'.format(probability, level)\n    return TransformFunction(return_function, name)\n\n  def do_transform(self, image, level):\n    f = self.pil_transformer(PARAMETER_MAX, level)\n    return pil_unwrap(f(pil_wrap(image)))\n\n\n################## Transform Functions ##################\nidentity = TransformT(\'identity\', lambda pil_img, level: pil_img)\nflip_lr = TransformT(\n    \'FlipLR\',\n    lambda pil_img, level: pil_img.transpose(Image.FLIP_LEFT_RIGHT))\nflip_ud = TransformT(\n    \'FlipUD\',\n    lambda pil_img, level: pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n# pylint:disable=g-long-lambda\nauto_contrast = TransformT(\n    \'AutoContrast\',\n    lambda pil_img, level: ImageOps.autocontrast(\n        pil_img.convert(\'RGB\')).convert(\'RGBA\'))\nequalize = TransformT(\n    \'Equalize\',\n    lambda pil_img, level: ImageOps.equalize(\n        pil_img.convert(\'RGB\')).convert(\'RGBA\'))\ninvert = TransformT(\n    \'Invert\',\n    lambda pil_img, level: ImageOps.invert(\n        pil_img.convert(\'RGB\')).convert(\'RGBA\'))\n# pylint:enable=g-long-lambda\nblur = TransformT(\n    \'Blur\', lambda pil_img, level: pil_img.filter(ImageFilter.BLUR))\nsmooth = TransformT(\n    \'Smooth\',\n    lambda pil_img, level: pil_img.filter(ImageFilter.SMOOTH))\n\n\ndef _rotate_impl(pil_img, level):\n  """"""Rotates `pil_img` from -30 to 30 degrees depending on `level`.""""""\n  degrees = int_parameter(level, 30)\n  if random.random() > 0.5:\n    degrees = -degrees\n  return pil_img.rotate(degrees)\n\n\nrotate = TransformT(\'Rotate\', _rotate_impl)\n\n\ndef _posterize_impl(pil_img, level):\n  """"""Applies PIL Posterize to `pil_img`.""""""\n  level = int_parameter(level, 4)\n  return ImageOps.posterize(pil_img.convert(\'RGB\'), 4 - level).convert(\'RGBA\')\n\n\nposterize = TransformT(\'Posterize\', _posterize_impl)\n\n\ndef _shear_x_impl(pil_img, level):\n  """"""Applies PIL ShearX to `pil_img`.\n\n  The ShearX operation shears the image along the horizontal axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  """"""\n  level = float_parameter(level, 0.3)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform((32, 32), Image.AFFINE, (1, level, 0, 0, 1, 0))\n\n\nshear_x = TransformT(\'ShearX\', _shear_x_impl)\n\n\ndef _shear_y_impl(pil_img, level):\n  """"""Applies PIL ShearY to `pil_img`.\n\n  The ShearY operation shears the image along the vertical axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  """"""\n  level = float_parameter(level, 0.3)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform((32, 32), Image.AFFINE, (1, 0, 0, level, 1, 0))\n\n\nshear_y = TransformT(\'ShearY\', _shear_y_impl)\n\n\ndef _translate_x_impl(pil_img, level):\n  """"""Applies PIL TranslateX to `pil_img`.\n\n  Translate the image in the horizontal direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateX applied to it.\n  """"""\n  level = int_parameter(level, 10)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform((32, 32), Image.AFFINE, (1, 0, level, 0, 1, 0))\n\n\ntranslate_x = TransformT(\'TranslateX\', _translate_x_impl)\n\n\ndef _translate_y_impl(pil_img, level):\n  """"""Applies PIL TranslateY to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateY applied to it.\n  """"""\n  level = int_parameter(level, 10)\n  if random.random() > 0.5:\n    level = -level\n  return pil_img.transform((32, 32), Image.AFFINE, (1, 0, 0, 0, 1, level))\n\n\ntranslate_y = TransformT(\'TranslateY\', _translate_y_impl)\n\n\ndef _crop_impl(pil_img, level, interpolation=Image.BILINEAR):\n  """"""Applies a crop to `pil_img` with the size depending on the `level`.""""""\n  cropped = pil_img.crop((level, level, IMAGE_SIZE - level, IMAGE_SIZE - level))\n  resized = cropped.resize((IMAGE_SIZE, IMAGE_SIZE), interpolation)\n  return resized\n\n\ncrop_bilinear = TransformT(\'CropBilinear\', _crop_impl)\n\n\ndef _solarize_impl(pil_img, level):\n  """"""Applies PIL Solarize to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had Solarize applied to it.\n  """"""\n  level = int_parameter(level, 256)\n  return ImageOps.solarize(pil_img.convert(\'RGB\'), 256 - level).convert(\'RGBA\')\n\n\nsolarize = TransformT(\'Solarize\', _solarize_impl)\n\n\ndef _cutout_pil_impl(pil_img, level):\n  """"""Apply cutout to pil_img at the specified level.""""""\n  size = int_parameter(level, 20)\n  if size <= 0:\n    return pil_img\n  img_height, img_width, num_channels = (32, 32, 3)\n  _, upper_coord, lower_coord = (\n      create_cutout_mask(img_height, img_width, num_channels, size))\n  pixels = pil_img.load()  # create the pixel map\n  for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n    for j in range(upper_coord[1], lower_coord[1]):  # For every row\n      pixels[i, j] = (125, 122, 113, 0)  # set the colour accordingly\n  return pil_img\n\ncutout = TransformT(\'Cutout\', _cutout_pil_impl)\n\n\ndef _enhancer_impl(enhancer):\n  """"""Sets level to be between 0.1 and 1.8 for ImageEnhance transforms of PIL.""""""\n  def impl(pil_img, level):\n    v = float_parameter(level, 1.8) + .1  # going to 0 just destroys it\n    return enhancer(pil_img).enhance(v)\n  return impl\n\n\ncolor = TransformT(\'Color\', _enhancer_impl(ImageEnhance.Color))\ncontrast = TransformT(\'Contrast\', _enhancer_impl(ImageEnhance.Contrast))\nbrightness = TransformT(\'Brightness\', _enhancer_impl(\n    ImageEnhance.Brightness))\nsharpness = TransformT(\'Sharpness\', _enhancer_impl(ImageEnhance.Sharpness))\n\nALL_TRANSFORMS = [\n    flip_lr,\n    flip_ud,\n    auto_contrast,\n    equalize,\n    invert,\n    rotate,\n    posterize,\n    crop_bilinear,\n    solarize,\n    color,\n    contrast,\n    brightness,\n    sharpness,\n    shear_x,\n    shear_y,\n    translate_x,\n    translate_y,\n    cutout,\n    blur,\n    smooth\n]\n\nNAME_TO_TRANSFORM = {t.name: t for t in ALL_TRANSFORMS}\nTRANSFORM_NAMES = NAME_TO_TRANSFORM.keys()\n'"
autoaugment/custom_ops.py,27,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains convenience wrappers for typical Neural Network TensorFlow layers.\n\n   Ops that have different behavior during training or eval have an is_training\n   parameter.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport numpy as np\nimport tensorflow as tf\n\n\narg_scope = tf.contrib.framework.arg_scope\n\n\ndef variable(name, shape, dtype, initializer, trainable):\n  """"""Returns a TF variable with the passed in specifications.""""""\n  var = tf.get_variable(\n      name,\n      shape=shape,\n      dtype=dtype,\n      initializer=initializer,\n      trainable=trainable)\n  return var\n\n\ndef global_avg_pool(x, scope=None):\n  """"""Average pools away spatial height and width dimension of 4D tensor.""""""\n  assert x.get_shape().ndims == 4\n  with tf.name_scope(scope, \'global_avg_pool\', [x]):\n    kernel_size = (1, int(x.shape[1]), int(x.shape[2]), 1)\n    squeeze_dims = (1, 2)\n    result = tf.nn.avg_pool(\n        x,\n        ksize=kernel_size,\n        strides=(1, 1, 1, 1),\n        padding=\'VALID\',\n        data_format=\'NHWC\')\n    return tf.squeeze(result, squeeze_dims)\n\n\ndef zero_pad(inputs, in_filter, out_filter):\n  """"""Zero pads `input` tensor to have `out_filter` number of filters.""""""\n  outputs = tf.pad(inputs, [[0, 0], [0, 0], [0, 0],\n                            [(out_filter - in_filter) // 2,\n                             (out_filter - in_filter) // 2]])\n  return outputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef batch_norm(inputs,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               is_training=True,\n               reuse=None,\n               scope=None):\n  """"""Small wrapper around tf.contrib.layers.batch_norm.""""""\n  return tf.contrib.layers.batch_norm(\n      inputs,\n      decay=decay,\n      center=center,\n      scale=scale,\n      epsilon=epsilon,\n      activation_fn=None,\n      param_initializers=None,\n      updates_collections=tf.GraphKeys.UPDATE_OPS,\n      is_training=is_training,\n      reuse=reuse,\n      trainable=True,\n      fused=True,\n      data_format=\'NHWC\',\n      zero_debias_moving_mean=False,\n      scope=scope)\n\n\ndef stride_arr(stride_h, stride_w):\n  return [1, stride_h, stride_w, 1]\n\n\n@tf.contrib.framework.add_arg_scope\ndef conv2d(inputs,\n           num_filters_out,\n           kernel_size,\n           stride=1,\n           scope=None,\n           reuse=None):\n  """"""Adds a 2D convolution.\n\n  conv2d creates a variable called \'weights\', representing the convolutional\n  kernel, that is convolved with the input.\n\n  Args:\n    inputs: a 4D tensor in NHWC format.\n    num_filters_out: the number of output filters.\n    kernel_size: an int specifying the kernel height and width size.\n    stride: an int specifying the height and width stride.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused.\n  Returns:\n    a tensor that is the result of a convolution being applied to `inputs`.\n  """"""\n  with tf.variable_scope(scope, \'Conv\', [inputs], reuse=reuse):\n    num_filters_in = int(inputs.shape[3])\n    weights_shape = [kernel_size, kernel_size, num_filters_in, num_filters_out]\n\n    # Initialization\n    n = int(weights_shape[0] * weights_shape[1] * weights_shape[3])\n    weights_initializer = tf.random_normal_initializer(\n        stddev=np.sqrt(2.0 / n))\n\n    weights = variable(\n        name=\'weights\',\n        shape=weights_shape,\n        dtype=tf.float32,\n        initializer=weights_initializer,\n        trainable=True)\n    strides = stride_arr(stride, stride)\n    outputs = tf.nn.conv2d(\n        inputs, weights, strides, padding=\'SAME\', data_format=\'NHWC\')\n    return outputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef fc(inputs,\n       num_units_out,\n       scope=None,\n       reuse=None):\n  """"""Creates a fully connected layer applied to `inputs`.\n\n  Args:\n    inputs: a tensor that the fully connected layer will be applied to. It\n      will be reshaped if it is not 2D.\n    num_units_out: the number of output units in the layer.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused.\n\n  Returns:\n     a tensor that is the result of applying a linear matrix to `inputs`.\n  """"""\n  if len(inputs.shape) > 2:\n    inputs = tf.reshape(inputs, [int(inputs.shape[0]), -1])\n\n  with tf.variable_scope(scope, \'FC\', [inputs], reuse=reuse):\n    num_units_in = inputs.shape[1]\n    weights_shape = [num_units_in, num_units_out]\n    unif_init_range = 1.0 / (num_units_out)**(0.5)\n    weights_initializer = tf.random_uniform_initializer(\n        -unif_init_range, unif_init_range)\n    weights = variable(\n        name=\'weights\',\n        shape=weights_shape,\n        dtype=tf.float32,\n        initializer=weights_initializer,\n        trainable=True)\n    bias_initializer = tf.constant_initializer(0.0)\n    biases = variable(\n        name=\'biases\',\n        shape=[num_units_out,],\n        dtype=tf.float32,\n        initializer=bias_initializer,\n        trainable=True)\n    outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n    return outputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef avg_pool(inputs, kernel_size, stride=2, padding=\'VALID\', scope=None):\n  """"""Wrapper around tf.nn.avg_pool.""""""\n  with tf.name_scope(scope, \'AvgPool\', [inputs]):\n    kernel = stride_arr(kernel_size, kernel_size)\n    strides = stride_arr(stride, stride)\n    return tf.nn.avg_pool(\n        inputs,\n        ksize=kernel,\n        strides=strides,\n        padding=padding,\n        data_format=\'NHWC\')\n\n'"
autoaugment/data_utils.py,5,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Data utils for CIFAR-10 and CIFAR-100.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport autoaugment.augmentation_transforms as augmentation_transforms\nimport autoaugment.policies as found_policies\n\n# pylint:disable=logging-format-interpolation\n\n\nclass DataSet(object):\n  """"""Dataset object that produces augmented training and eval data.""""""\n\n  def __init__(self, hparams):\n    self.hparams = hparams\n    self.epochs = 0\n    self.curr_train_index = 0\n\n    all_labels = []\n\n    self.good_policies = found_policies.good_policies()\n\n    # Determine how many databatched to load\n    num_data_batches_to_load = 5\n    total_batches_to_load = num_data_batches_to_load\n    train_batches_to_load = total_batches_to_load\n    assert hparams.train_size + hparams.validation_size <= 50000\n    if hparams.eval_test:\n      total_batches_to_load += 1\n    # Determine how many images we have loaded\n    total_dataset_size = 10000 * num_data_batches_to_load\n    train_dataset_size = total_dataset_size\n    if hparams.eval_test:\n      total_dataset_size += 10000\n\n    if hparams.dataset == \'cifar10\':\n      all_data = np.empty((total_batches_to_load, 10000, 3072), dtype=np.uint8)\n    elif hparams.dataset == \'cifar100\':\n      assert num_data_batches_to_load == 5\n      all_data = np.empty((1, 50000, 3072), dtype=np.uint8)\n      if hparams.eval_test:\n        test_data = np.empty((1, 10000, 3072), dtype=np.uint8)\n    if hparams.dataset == \'cifar10\':\n      tf.logging.info(\'Cifar10\')\n      datafiles = [\n          \'data_batch_1\', \'data_batch_2\', \'data_batch_3\', \'data_batch_4\',\n          \'data_batch_5\']\n\n      datafiles = datafiles[:train_batches_to_load]\n      if hparams.eval_test:\n        datafiles.append(\'test_batch\')\n      num_classes = 10\n    elif hparams.dataset == \'cifar100\':\n      datafiles = [\'train\']\n      if hparams.eval_test:\n        datafiles.append(\'test\')\n      num_classes = 100\n    else:\n      raise NotImplementedError(\'Unimplemented dataset: \', hparams.dataset)\n    if hparams.dataset != \'test\':\n      for file_num, f in enumerate(datafiles):\n        d = unpickle(os.path.join(hparams.data_path, f))\n        if f == \'test\':\n          test_data[0] = copy.deepcopy(d[\'data\'])\n          all_data = np.concatenate([all_data, test_data], axis=1)\n        else:\n          all_data[file_num] = copy.deepcopy(d[\'data\'])\n        if hparams.dataset == \'cifar10\':\n          labels = np.array(d[\'labels\'])\n        else:\n          labels = np.array(d[\'fine_labels\'])\n        nsamples = len(labels)\n        for idx in range(nsamples):\n          all_labels.append(labels[idx])\n\n    all_data = all_data.reshape(total_dataset_size, 3072)\n    all_data = all_data.reshape(-1, 3, 32, 32)\n    all_data = all_data.transpose(0, 2, 3, 1).copy()\n    all_data = all_data / 255.0\n    mean = augmentation_transforms.MEANS\n    std = augmentation_transforms.STDS\n    tf.logging.info(\'mean:{}    std: {}\'.format(mean, std))\n\n    all_data = (all_data - mean) / std\n    all_labels = np.eye(num_classes)[np.array(all_labels, dtype=np.int32)]\n    assert len(all_data) == len(all_labels)\n    tf.logging.info(\n        \'In CIFAR10 loader, number of images: {}\'.format(len(all_data)))\n\n    # Break off test data\n    if hparams.eval_test:\n      self.test_images = all_data[train_dataset_size:]\n      self.test_labels = all_labels[train_dataset_size:]\n\n    # Shuffle the rest of the data\n    all_data = all_data[:train_dataset_size]\n    all_labels = all_labels[:train_dataset_size]\n    np.random.seed(0)\n    perm = np.arange(len(all_data))\n    np.random.shuffle(perm)\n    all_data = all_data[perm]\n    all_labels = all_labels[perm]\n\n    # Break into train and val\n    train_size, val_size = hparams.train_size, hparams.validation_size\n    assert 50000 >= train_size + val_size\n    self.train_images = all_data[:train_size]\n    self.train_labels = all_labels[:train_size]\n    self.val_images = all_data[train_size:train_size + val_size]\n    self.val_labels = all_labels[train_size:train_size + val_size]\n    self.num_train = self.train_images.shape[0]\n\n  def next_batch(self):\n    """"""Return the next minibatch of augmented data.""""""\n    next_train_index = self.curr_train_index + self.hparams.batch_size\n    if next_train_index > self.num_train:\n      # Increase epoch number\n      epoch = self.epochs + 1\n      self.reset()\n      self.epochs = epoch\n    batched_data = (\n        self.train_images[self.curr_train_index:\n                          self.curr_train_index + self.hparams.batch_size],\n        self.train_labels[self.curr_train_index:\n                          self.curr_train_index + self.hparams.batch_size])\n    final_imgs = []\n\n    images, labels = batched_data\n    for data in images:\n      epoch_policy = self.good_policies[np.random.choice(\n          len(self.good_policies))]\n      final_img = augmentation_transforms.apply_policy(\n          epoch_policy, data)\n      final_img = augmentation_transforms.random_flip(\n          augmentation_transforms.zero_pad_and_crop(final_img, 4))\n      # Apply cutout\n      final_img = augmentation_transforms.cutout_numpy(final_img)\n      final_imgs.append(final_img)\n    batched_data = (np.array(final_imgs, np.float32), labels)\n    self.curr_train_index += self.hparams.batch_size\n    return batched_data\n\n  def reset(self):\n    """"""Reset training data and index into the training data.""""""\n    self.epochs = 0\n    # Shuffle the training data\n    perm = np.arange(self.num_train)\n    np.random.shuffle(perm)\n    assert self.num_train == self.train_images.shape[\n        0], \'Error incorrect shuffling mask\'\n    self.train_images = self.train_images[perm]\n    self.train_labels = self.train_labels[perm]\n    self.curr_train_index = 0\n\n\ndef unpickle(f):\n  tf.logging.info(\'loading file: {}\'.format(f))\n  fo = tf.gfile.Open(f, \'rb\')\n  try:\n    d = pickle.load(fo, encoding=\'latin1\')\n  except:\n    d = pickle.load(fo)\n  fo.close()\n  return d\n'"
autoaugment/helper_utils.py,10,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper functions used for training AutoAugment models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef setup_loss(logits, labels):\n  """"""Returns the cross entropy for the given `logits` and `labels`.""""""\n  predictions = tf.nn.softmax(logits)\n  cost = tf.losses.softmax_cross_entropy(onehot_labels=labels,\n                                         logits=logits)\n  return predictions, cost\n\n\ndef decay_weights(cost, weight_decay_rate):\n  """"""Calculates the loss for l2 weight decay and adds it to `cost`.""""""\n  costs = []\n  for var in tf.trainable_variables():\n    costs.append(tf.nn.l2_loss(var))\n  cost += tf.multiply(weight_decay_rate, tf.add_n(costs))\n  return cost\n\n\ndef eval_child_model(session, model, data_loader, mode):\n  """"""Evaluates `model` on held out data depending on `mode`.\n\n  Args:\n    session: TensorFlow session the model will be run with.\n    model: TensorFlow model that will be evaluated.\n    data_loader: DataSet object that contains data that `model` will\n      evaluate.\n    mode: Will `model` either evaluate validation or test data.\n\n  Returns:\n    Accuracy of `model` when evaluated on the specified dataset.\n\n  Raises:\n    ValueError: if invalid dataset `mode` is specified.\n  """"""\n  if mode == \'val\':\n    images = data_loader.val_images\n    labels = data_loader.val_labels\n  elif mode == \'test\':\n    images = data_loader.test_images\n    labels = data_loader.test_labels\n  else:\n    raise ValueError(\'Not valid eval mode\')\n  assert len(images) == len(labels)\n  tf.logging.info(\'model.batch_size is {}\'.format(model.batch_size))\n  assert len(images) % model.batch_size == 0\n  eval_batches = int(len(images) / model.batch_size)\n  for i in range(eval_batches):\n    eval_images = images[i * model.batch_size:(i + 1) * model.batch_size]\n    eval_labels = labels[i * model.batch_size:(i + 1) * model.batch_size]\n    _ = session.run(\n        model.eval_op,\n        feed_dict={\n            model.images: eval_images,\n            model.labels: eval_labels,\n        })\n  return session.run(model.accuracy)\n\n\ndef cosine_lr(learning_rate, epoch, iteration, batches_per_epoch, total_epochs):\n  """"""Cosine Learning rate.\n\n  Args:\n    learning_rate: Initial learning rate.\n    epoch: Current epoch we are one. This is one based.\n    iteration: Current batch in this epoch.\n    batches_per_epoch: Batches per epoch.\n    total_epochs: Total epochs you are training for.\n\n  Returns:\n    The learning rate to be used for this current batch.\n  """"""\n  t_total = total_epochs * batches_per_epoch\n  t_cur = float(epoch * batches_per_epoch + iteration)\n  return 0.5 * learning_rate * (1 + np.cos(np.pi * t_cur / t_total))\n\n\ndef get_lr(curr_epoch, hparams, iteration=None):\n  """"""Returns the learning rate during training based on the current epoch.""""""\n  assert iteration is not None\n  batches_per_epoch = int(hparams.train_size / hparams.batch_size)\n  lr = cosine_lr(hparams.lr, curr_epoch, iteration, batches_per_epoch,\n                 hparams.num_epochs)\n  return lr\n\n\ndef run_epoch_training(session, model, data_loader, curr_epoch):\n  """"""Runs one epoch of training for the model passed in.\n\n  Args:\n    session: TensorFlow session the model will be run with.\n    model: TensorFlow model that will be evaluated.\n    data_loader: DataSet object that contains data that `model` will\n      evaluate.\n    curr_epoch: How many of epochs of training have been done so far.\n\n  Returns:\n    The accuracy of \'model\' on the training set\n  """"""\n  steps_per_epoch = int(model.hparams.train_size / model.hparams.batch_size)\n  tf.logging.info(\'steps per epoch: {}\'.format(steps_per_epoch))\n  curr_step = session.run(model.global_step)\n  assert curr_step % steps_per_epoch == 0\n\n  # Get the current learning rate for the model based on the current epoch\n  curr_lr = get_lr(curr_epoch, model.hparams, iteration=0)\n  tf.logging.info(\'lr of {} for epoch {}\'.format(curr_lr, curr_epoch))\n\n  for step in range(steps_per_epoch):\n    curr_lr = get_lr(curr_epoch, model.hparams, iteration=(step + 1))\n    # Update the lr rate variable to the current LR.\n    model.lr_rate_ph.load(curr_lr, session=session)\n    if step % 20 == 0:\n      tf.logging.info(\'Training {}/{}\'.format(step, steps_per_epoch))\n\n    train_images, train_labels = data_loader.next_batch()\n    _, step, _ = session.run(\n        [model.train_op, model.global_step, model.eval_op],\n        feed_dict={\n            model.images: train_images,\n            model.labels: train_labels,\n        })\n\n  train_accuracy = session.run(model.accuracy)\n  tf.logging.info(\'Train accuracy: {}\'.format(train_accuracy))\n  return train_accuracy\n'"
autoaugment/policies.py,0,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef good_policies():\n  """"""AutoAugment policies found on Cifar.""""""\n  exp0_0 = [\n      [(\'Invert\', 0.1, 7), (\'Contrast\', 0.2, 6)],\n      [(\'Rotate\', 0.7, 2), (\'TranslateX\', 0.3, 9)],\n      [(\'Sharpness\', 0.8, 1), (\'Sharpness\', 0.9, 3)],\n      [(\'ShearY\', 0.5, 8), (\'TranslateY\', 0.7, 9)],\n      [(\'AutoContrast\', 0.5, 8), (\'Equalize\', 0.9, 2)]]\n  exp0_1 = [\n      [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.9, 3)],\n      [(\'TranslateY\', 0.9, 9), (\'TranslateY\', 0.7, 9)],\n      [(\'AutoContrast\', 0.9, 2), (\'Solarize\', 0.8, 3)],\n      [(\'Equalize\', 0.8, 8), (\'Invert\', 0.1, 3)],\n      [(\'TranslateY\', 0.7, 9), (\'AutoContrast\', 0.9, 1)]]\n  exp0_2 = [\n      [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.0, 2)],\n      [(\'TranslateY\', 0.7, 9), (\'TranslateY\', 0.7, 9)],\n      [(\'AutoContrast\', 0.9, 0), (\'Solarize\', 0.4, 3)],\n      [(\'Equalize\', 0.7, 5), (\'Invert\', 0.1, 3)],\n      [(\'TranslateY\', 0.7, 9), (\'TranslateY\', 0.7, 9)]]\n  exp0_3 = [\n      [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.9, 1)],\n      [(\'TranslateY\', 0.8, 9), (\'TranslateY\', 0.9, 9)],\n      [(\'AutoContrast\', 0.8, 0), (\'TranslateY\', 0.7, 9)],\n      [(\'TranslateY\', 0.2, 7), (\'Color\', 0.9, 6)],\n      [(\'Equalize\', 0.7, 6), (\'Color\', 0.4, 9)]]\n  exp1_0 = [\n      [(\'ShearY\', 0.2, 7), (\'Posterize\', 0.3, 7)],\n      [(\'Color\', 0.4, 3), (\'Brightness\', 0.6, 7)],\n      [(\'Sharpness\', 0.3, 9), (\'Brightness\', 0.7, 9)],\n      [(\'Equalize\', 0.6, 5), (\'Equalize\', 0.5, 1)],\n      [(\'Contrast\', 0.6, 7), (\'Sharpness\', 0.6, 5)]]\n  exp1_1 = [\n      [(\'Brightness\', 0.3, 7), (\'AutoContrast\', 0.5, 8)],\n      [(\'AutoContrast\', 0.9, 4), (\'AutoContrast\', 0.5, 6)],\n      [(\'Solarize\', 0.3, 5), (\'Equalize\', 0.6, 5)],\n      [(\'TranslateY\', 0.2, 4), (\'Sharpness\', 0.3, 3)],\n      [(\'Brightness\', 0.0, 8), (\'Color\', 0.8, 8)]]\n  exp1_2 = [\n      [(\'Solarize\', 0.2, 6), (\'Color\', 0.8, 6)],\n      [(\'Solarize\', 0.2, 6), (\'AutoContrast\', 0.8, 1)],\n      [(\'Solarize\', 0.4, 1), (\'Equalize\', 0.6, 5)],\n      [(\'Brightness\', 0.0, 0), (\'Solarize\', 0.5, 2)],\n      [(\'AutoContrast\', 0.9, 5), (\'Brightness\', 0.5, 3)]]\n  exp1_3 = [\n      [(\'Contrast\', 0.7, 5), (\'Brightness\', 0.0, 2)],\n      [(\'Solarize\', 0.2, 8), (\'Solarize\', 0.1, 5)],\n      [(\'Contrast\', 0.5, 1), (\'TranslateY\', 0.2, 9)],\n      [(\'AutoContrast\', 0.6, 5), (\'TranslateY\', 0.0, 9)],\n      [(\'AutoContrast\', 0.9, 4), (\'Equalize\', 0.8, 4)]]\n  exp1_4 = [\n      [(\'Brightness\', 0.0, 7), (\'Equalize\', 0.4, 7)],\n      [(\'Solarize\', 0.2, 5), (\'Equalize\', 0.7, 5)],\n      [(\'Equalize\', 0.6, 8), (\'Color\', 0.6, 2)],\n      [(\'Color\', 0.3, 7), (\'Color\', 0.2, 4)],\n      [(\'AutoContrast\', 0.5, 2), (\'Solarize\', 0.7, 2)]]\n  exp1_5 = [\n      [(\'AutoContrast\', 0.2, 0), (\'Equalize\', 0.1, 0)],\n      [(\'ShearY\', 0.6, 5), (\'Equalize\', 0.6, 5)],\n      [(\'Brightness\', 0.9, 3), (\'AutoContrast\', 0.4, 1)],\n      [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.7, 7)],\n      [(\'Equalize\', 0.7, 7), (\'Solarize\', 0.5, 0)]]\n  exp1_6 = [\n      [(\'Equalize\', 0.8, 4), (\'TranslateY\', 0.8, 9)],\n      [(\'TranslateY\', 0.8, 9), (\'TranslateY\', 0.6, 9)],\n      [(\'TranslateY\', 0.9, 0), (\'TranslateY\', 0.5, 9)],\n      [(\'AutoContrast\', 0.5, 3), (\'Solarize\', 0.3, 4)],\n      [(\'Solarize\', 0.5, 3), (\'Equalize\', 0.4, 4)]]\n  exp2_0 = [\n      [(\'Color\', 0.7, 7), (\'TranslateX\', 0.5, 8)],\n      [(\'Equalize\', 0.3, 7), (\'AutoContrast\', 0.4, 8)],\n      [(\'TranslateY\', 0.4, 3), (\'Sharpness\', 0.2, 6)],\n      [(\'Brightness\', 0.9, 6), (\'Color\', 0.2, 8)],\n      [(\'Solarize\', 0.5, 2), (\'Invert\', 0.0, 3)]]\n  exp2_1 = [\n      [(\'AutoContrast\', 0.1, 5), (\'Brightness\', 0.0, 0)],\n      [(\'Cutout\', 0.2, 4), (\'Equalize\', 0.1, 1)],\n      [(\'Equalize\', 0.7, 7), (\'AutoContrast\', 0.6, 4)],\n      [(\'Color\', 0.1, 8), (\'ShearY\', 0.2, 3)],\n      [(\'ShearY\', 0.4, 2), (\'Rotate\', 0.7, 0)]]\n  exp2_2 = [\n      [(\'ShearY\', 0.1, 3), (\'AutoContrast\', 0.9, 5)],\n      [(\'TranslateY\', 0.3, 6), (\'Cutout\', 0.3, 3)],\n      [(\'Equalize\', 0.5, 0), (\'Solarize\', 0.6, 6)],\n      [(\'AutoContrast\', 0.3, 5), (\'Rotate\', 0.2, 7)],\n      [(\'Equalize\', 0.8, 2), (\'Invert\', 0.4, 0)]]\n  exp2_3 = [\n      [(\'Equalize\', 0.9, 5), (\'Color\', 0.7, 0)],\n      [(\'Equalize\', 0.1, 1), (\'ShearY\', 0.1, 3)],\n      [(\'AutoContrast\', 0.7, 3), (\'Equalize\', 0.7, 0)],\n      [(\'Brightness\', 0.5, 1), (\'Contrast\', 0.1, 7)],\n      [(\'Contrast\', 0.1, 4), (\'Solarize\', 0.6, 5)]]\n  exp2_4 = [\n      [(\'Solarize\', 0.2, 3), (\'ShearX\', 0.0, 0)],\n      [(\'TranslateX\', 0.3, 0), (\'TranslateX\', 0.6, 0)],\n      [(\'Equalize\', 0.5, 9), (\'TranslateY\', 0.6, 7)],\n      [(\'ShearX\', 0.1, 0), (\'Sharpness\', 0.5, 1)],\n      [(\'Equalize\', 0.8, 6), (\'Invert\', 0.3, 6)]]\n  exp2_5 = [\n      [(\'AutoContrast\', 0.3, 9), (\'Cutout\', 0.5, 3)],\n      [(\'ShearX\', 0.4, 4), (\'AutoContrast\', 0.9, 2)],\n      [(\'ShearX\', 0.0, 3), (\'Posterize\', 0.0, 3)],\n      [(\'Solarize\', 0.4, 3), (\'Color\', 0.2, 4)],\n      [(\'Equalize\', 0.1, 4), (\'Equalize\', 0.7, 6)]]\n  exp2_6 = [\n      [(\'Equalize\', 0.3, 8), (\'AutoContrast\', 0.4, 3)],\n      [(\'Solarize\', 0.6, 4), (\'AutoContrast\', 0.7, 6)],\n      [(\'AutoContrast\', 0.2, 9), (\'Brightness\', 0.4, 8)],\n      [(\'Equalize\', 0.1, 0), (\'Equalize\', 0.0, 6)],\n      [(\'Equalize\', 0.8, 4), (\'Equalize\', 0.0, 4)]]\n  exp2_7 = [\n      [(\'Equalize\', 0.5, 5), (\'AutoContrast\', 0.1, 2)],\n      [(\'Solarize\', 0.5, 5), (\'AutoContrast\', 0.9, 5)],\n      [(\'AutoContrast\', 0.6, 1), (\'AutoContrast\', 0.7, 8)],\n      [(\'Equalize\', 0.2, 0), (\'AutoContrast\', 0.1, 2)],\n      [(\'Equalize\', 0.6, 9), (\'Equalize\', 0.4, 4)]]\n  exp0s = exp0_0 + exp0_1 + exp0_2 + exp0_3\n  exp1s = exp1_0 + exp1_1 + exp1_2 + exp1_3 + exp1_4 + exp1_5 + exp1_6\n  exp2s = exp2_0 + exp2_1 + exp2_2 + exp2_3 + exp2_4 + exp2_5 + exp2_6 + exp2_7\n  return  exp0s + exp1s + exp2s\n'"
autoaugment/shake_drop.py,13,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builds the Shake-Shake Model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\n\nimport autoaugment.custom_ops as ops\n\n\ndef round_int(x):\n  """"""Rounds `x` and then converts to an int.""""""\n  return int(math.floor(x + 0.5))\n\n\ndef shortcut(x, output_filters, stride):\n  """"""Applies strided avg pool or zero padding to make output_filters match x.""""""\n  num_filters = int(x.shape[3])\n  if stride == 2:\n    x = ops.avg_pool(x, 2, stride=stride, padding=\'SAME\')\n  if num_filters != output_filters:\n    diff = output_filters - num_filters\n    assert diff > 0\n    # Zero padd diff zeros\n    padding = [[0, 0], [0, 0], [0, 0], [0, diff]]\n    x = tf.pad(x, padding)\n  return x\n\n\ndef calc_prob(curr_layer, total_layers, p_l):\n  """"""Calculates drop prob depending on the current layer.""""""\n  return 1 - (float(curr_layer) / total_layers) * p_l\n\n\ndef bottleneck_layer(x, n, stride, prob, is_training, alpha, beta):\n  """"""Bottleneck layer for shake drop model.""""""\n  assert alpha[1] > alpha[0]\n  assert beta[1] > beta[0]\n  with tf.variable_scope(\'bottleneck_{}\'.format(prob)):\n    input_layer = x\n    x = ops.batch_norm(x, scope=\'bn_1_pre\')\n    x = ops.conv2d(x, n, 1, scope=\'1x1_conv_contract\')\n    x = ops.batch_norm(x, scope=\'bn_1_post\')\n    x = tf.nn.relu(x)\n    x = ops.conv2d(x, n, 3, stride=stride, scope=\'3x3\')\n    x = ops.batch_norm(x, scope=\'bn_2\')\n    x = tf.nn.relu(x)\n    x = ops.conv2d(x, n * 4, 1, scope=\'1x1_conv_expand\')\n    x = ops.batch_norm(x, scope=\'bn_3\')\n\n    # Apply regularization here\n    # Sample bernoulli with prob\n    if is_training:\n      batch_size = tf.shape(x)[0]\n      bern_shape = [batch_size, 1, 1, 1]\n      random_tensor = prob\n      random_tensor += tf.random_uniform(bern_shape, dtype=tf.float32)\n      binary_tensor = tf.floor(random_tensor)\n\n      alpha_values = tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=alpha[0], maxval=alpha[1],\n          dtype=tf.float32)\n      beta_values = tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=beta[0], maxval=beta[1],\n          dtype=tf.float32)\n      rand_forward = (\n          binary_tensor + alpha_values - binary_tensor * alpha_values)\n      rand_backward = (\n          binary_tensor + beta_values - binary_tensor * beta_values)\n      x = x * rand_backward + tf.stop_gradient(x * rand_forward -\n                                               x * rand_backward)\n    else:\n      expected_alpha = (alpha[1] + alpha[0])/2\n      # prob is the expectation of the bernoulli variable\n      x = (prob + expected_alpha - prob * expected_alpha) * x\n\n    res = shortcut(input_layer, n * 4, stride)\n    return x + res\n\n\ndef build_shake_drop_model(images, num_classes, is_training):\n  """"""Builds the PyramidNet Shake-Drop model.\n\n  Build the PyramidNet Shake-Drop model from https://arxiv.org/abs/1802.02375.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    is_training: Is the model training or not.\n\n  Returns:\n    The logits of the PyramidNet Shake-Drop model.\n  """"""\n  # ShakeDrop Hparams\n  p_l = 0.5\n  alpha_shake = [-1, 1]\n  beta_shake = [0, 1]\n\n  # PyramidNet Hparams\n  alpha = 200\n  depth = 272\n  # This is for the bottleneck architecture specifically\n  n = int((depth - 2) / 9)\n  start_channel = 16\n  add_channel = alpha / (3 * n)\n\n  # Building the models\n  x = images\n  x = ops.conv2d(x, 16, 3, scope=\'init_conv\')\n  x = ops.batch_norm(x, scope=\'init_bn\')\n\n  layer_num = 1\n  total_layers = n * 3\n  start_channel += add_channel\n  prob = calc_prob(layer_num, total_layers, p_l)\n  x = bottleneck_layer(\n      x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n      beta_shake)\n  layer_num += 1\n  for _ in range(1, n):\n    start_channel += add_channel\n    prob = calc_prob(layer_num, total_layers, p_l)\n    x = bottleneck_layer(\n        x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n        beta_shake)\n    layer_num += 1\n\n  start_channel += add_channel\n  prob = calc_prob(layer_num, total_layers, p_l)\n  x = bottleneck_layer(\n      x, round_int(start_channel), 2, prob, is_training, alpha_shake,\n      beta_shake)\n  layer_num += 1\n  for _ in range(1, n):\n    start_channel += add_channel\n    prob = calc_prob(layer_num, total_layers, p_l)\n    x = bottleneck_layer(\n        x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n        beta_shake)\n    layer_num += 1\n\n  start_channel += add_channel\n  prob = calc_prob(layer_num, total_layers, p_l)\n  x = bottleneck_layer(\n      x, round_int(start_channel), 2, prob, is_training, alpha_shake,\n      beta_shake)\n  layer_num += 1\n  for _ in range(1, n):\n    start_channel += add_channel\n    prob = calc_prob(layer_num, total_layers, p_l)\n    x = bottleneck_layer(\n        x, round_int(start_channel), 1, prob, is_training, alpha_shake,\n        beta_shake)\n    layer_num += 1\n\n  assert layer_num - 1 == total_layers\n  x = ops.batch_norm(x, scope=\'final_bn\')\n  x = tf.nn.relu(x)\n  x = ops.global_avg_pool(x)\n  # Fully connected\n  logits = ops.fc(x, num_classes)\n  return logits\n'"
autoaugment/shake_shake.py,22,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builds the Shake-Shake Model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport autoaugment.custom_ops as ops\n\n\ndef _shake_shake_skip_connection(x, output_filters, stride):\n  """"""Adds a residual connection to the filter x for the shake-shake model.""""""\n  curr_filters = int(x.shape[3])\n  if curr_filters == output_filters:\n    return x\n  stride_spec = ops.stride_arr(stride, stride)\n  # Skip path 1\n  path1 = tf.nn.avg_pool(\n      x, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=\'NHWC\')\n  path1 = ops.conv2d(path1, int(output_filters / 2), 1, scope=\'path1_conv\')\n\n  # Skip path 2\n  # First pad with 0\'s then crop\n  pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]\n  path2 = tf.pad(x, pad_arr)[:, 1:, 1:, :]\n  concat_axis = 3\n\n  path2 = tf.nn.avg_pool(\n      path2, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=\'NHWC\')\n  path2 = ops.conv2d(path2, int(output_filters / 2), 1, scope=\'path2_conv\')\n\n  # Concat and apply BN\n  final_path = tf.concat(values=[path1, path2], axis=concat_axis)\n  final_path = ops.batch_norm(final_path, scope=\'final_path_bn\')\n  return final_path\n\n\ndef _shake_shake_branch(x, output_filters, stride, rand_forward, rand_backward,\n                        is_training):\n  """"""Building a 2 branching convnet.""""""\n  x = tf.nn.relu(x)\n  x = ops.conv2d(x, output_filters, 3, stride=stride, scope=\'conv1\')\n  x = ops.batch_norm(x, scope=\'bn1\')\n  x = tf.nn.relu(x)\n  x = ops.conv2d(x, output_filters, 3, scope=\'conv2\')\n  x = ops.batch_norm(x, scope=\'bn2\')\n  if is_training:\n    x = x * rand_backward + tf.stop_gradient(x * rand_forward -\n                                             x * rand_backward)\n  else:\n    x *= 1.0 / 2\n  return x\n\n\ndef _shake_shake_block(x, output_filters, stride, is_training):\n  """"""Builds a full shake-shake sub layer.""""""\n  batch_size = tf.shape(x)[0]\n\n  # Generate random numbers for scaling the branches\n  rand_forward = [\n      tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.float32)\n      for _ in range(2)\n  ]\n  rand_backward = [\n      tf.random_uniform(\n          [batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.float32)\n      for _ in range(2)\n  ]\n  # Normalize so that all sum to 1\n  total_forward = tf.add_n(rand_forward)\n  total_backward = tf.add_n(rand_backward)\n  rand_forward = [samp / total_forward for samp in rand_forward]\n  rand_backward = [samp / total_backward for samp in rand_backward]\n  zipped_rand = zip(rand_forward, rand_backward)\n\n  branches = []\n  for branch, (r_forward, r_backward) in enumerate(zipped_rand):\n    with tf.variable_scope(\'branch_{}\'.format(branch)):\n      b = _shake_shake_branch(x, output_filters, stride, r_forward, r_backward,\n                              is_training)\n      branches.append(b)\n  res = _shake_shake_skip_connection(x, output_filters, stride)\n  return res + tf.add_n(branches)\n\n\ndef _shake_shake_layer(x, output_filters, num_blocks, stride,\n                       is_training):\n  """"""Builds many sub layers into one full layer.""""""\n  for block_num in range(num_blocks):\n    curr_stride = stride if (block_num == 0) else 1\n    with tf.variable_scope(\'layer_{}\'.format(block_num)):\n      x = _shake_shake_block(x, output_filters, curr_stride,\n                             is_training)\n  return x\n\n\ndef build_shake_shake_model(images, num_classes, hparams, is_training):\n  """"""Builds the Shake-Shake model.\n\n  Build the Shake-Shake model from https://arxiv.org/abs/1705.07485.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    hparams: tf.HParams object that contains additional hparams needed to\n      construct the model. In this case it is the `shake_shake_widen_factor`\n      that is used to determine how many filters the model has.\n    is_training: Is the model training or not.\n\n  Returns:\n    The logits of the Shake-Shake model.\n  """"""\n  depth = 26\n  k = hparams.shake_shake_widen_factor  # The widen factor\n  n = int((depth - 2) / 6)\n  x = images\n\n  x = ops.conv2d(x, 16, 3, scope=\'init_conv\')\n  x = ops.batch_norm(x, scope=\'init_bn\')\n  with tf.variable_scope(\'L1\'):\n    x = _shake_shake_layer(x, 16 * k, n, 1, is_training)\n  with tf.variable_scope(\'L2\'):\n    x = _shake_shake_layer(x, 32 * k, n, 2, is_training)\n  with tf.variable_scope(\'L3\'):\n    x = _shake_shake_layer(x, 64 * k, n, 2, is_training)\n  x = tf.nn.relu(x)\n  x = ops.global_avg_pool(x)\n\n  # Fully connected\n  logits = ops.fc(x, num_classes)\n  return logits\n'"
autoaugment/train_cifar.py,56,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""AutoAugment Train/Eval module.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport autoaugment.custom_ops as ops\nimport autoaugment.data_utils as data_utils\nimport autoaugment.helper_utils as helper_utils\nfrom autoaugment.shake_drop import build_shake_drop_model\nfrom autoaugment.shake_shake import build_shake_shake_model\nfrom autoaugment.wrn import build_wrn_model\n\n\ntf.flags.DEFINE_string(\'model_name\', \'wrn\',\n                       \'wrn, shake_shake_32, shake_shake_96, shake_shake_112, \'\n                       \'pyramid_net\')\ntf.flags.DEFINE_string(\'checkpoint_dir\', \'/tmp/training\', \'Training Directory.\')\ntf.flags.DEFINE_string(\'data_path\', \'/tmp/data\',\n                       \'Directory where dataset is located.\')\ntf.flags.DEFINE_string(\'dataset\', \'cifar10\',\n                       \'Dataset to train with. Either cifar10 or cifar100\')\ntf.flags.DEFINE_integer(\'use_cpu\', 1, \'1 if use CPU, else GPU.\')\n\nFLAGS = tf.flags.FLAGS\n\narg_scope = tf.contrib.framework.arg_scope\n\n\ndef setup_arg_scopes(is_training):\n  """"""Sets up the argscopes that will be used when building an image model.\n\n  Args:\n    is_training: Is the model training or not.\n\n  Returns:\n    Arg scopes to be put around the model being constructed.\n  """"""\n\n  batch_norm_decay = 0.9\n  batch_norm_epsilon = 1e-5\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': True,\n      # collection containing the moving mean and moving variance.\n      \'is_training\': is_training,\n  }\n\n  scopes = []\n\n  scopes.append(arg_scope([ops.batch_norm], **batch_norm_params))\n  return scopes\n\n\ndef build_model(inputs, num_classes, is_training, hparams):\n  """"""Constructs the vision model being trained/evaled.\n\n  Args:\n    inputs: input features/images being fed to the image model build built.\n    num_classes: number of output classes being predicted.\n    is_training: is the model training or not.\n    hparams: additional hyperparameters associated with the image model.\n\n  Returns:\n    The logits of the image model.\n  """"""\n  scopes = setup_arg_scopes(is_training)\n  with contextlib.nested(*scopes):\n    if hparams.model_name == \'pyramid_net\':\n      logits = build_shake_drop_model(\n          inputs, num_classes, is_training)\n    elif hparams.model_name == \'wrn\':\n      logits = build_wrn_model(\n          inputs, num_classes, hparams.wrn_size)\n    elif hparams.model_name == \'shake_shake\':\n      logits = build_shake_shake_model(\n          inputs, num_classes, hparams, is_training)\n  return logits\n\n\nclass CifarModel(object):\n  """"""Builds an image model for Cifar10/Cifar100.""""""\n\n  def __init__(self, hparams):\n    self.hparams = hparams\n\n  def build(self, mode):\n    """"""Construct the cifar model.""""""\n    assert mode in [\'train\', \'eval\']\n    self.mode = mode\n    self._setup_misc(mode)\n    self._setup_images_and_labels()\n    self._build_graph(self.images, self.labels, mode)\n\n    self.init = tf.group(tf.global_variables_initializer(),\n                         tf.local_variables_initializer())\n\n  def _setup_misc(self, mode):\n    """"""Sets up miscellaneous in the cifar model constructor.""""""\n    self.lr_rate_ph = tf.Variable(0.0, name=\'lrn_rate\', trainable=False)\n    self.reuse = None if (mode == \'train\') else True\n    self.batch_size = self.hparams.batch_size\n    if mode == \'eval\':\n      self.batch_size = 25\n\n  def _setup_images_and_labels(self):\n    """"""Sets up image and label placeholders for the cifar model.""""""\n    if FLAGS.dataset == \'cifar10\':\n      self.num_classes = 10\n    else:\n      self.num_classes = 100\n    self.images = tf.placeholder(tf.float32, [self.batch_size, 32, 32, 3])\n    self.labels = tf.placeholder(tf.float32,\n                                 [self.batch_size, self.num_classes])\n\n  def assign_epoch(self, session, epoch_value):\n    session.run(self._epoch_update, feed_dict={self._new_epoch: epoch_value})\n\n  def _build_graph(self, images, labels, mode):\n    """"""Constructs the TF graph for the cifar model.\n\n    Args:\n      images: A 4-D image Tensor\n      labels: A 2-D labels Tensor.\n      mode: string indicating training mode ( e.g., \'train\', \'valid\', \'test\').\n    """"""\n    is_training = \'train\' in mode\n    if is_training:\n      self.global_step = tf.train.get_or_create_global_step()\n\n    logits = build_model(\n        images,\n        self.num_classes,\n        is_training,\n        self.hparams)\n    self.predictions, self.cost = helper_utils.setup_loss(\n        logits, labels)\n    self.accuracy, self.eval_op = tf.metrics.accuracy(\n        tf.argmax(labels, 1), tf.argmax(self.predictions, 1))\n    self._calc_num_trainable_params()\n\n    # Adds L2 weight decay to the cost\n    self.cost = helper_utils.decay_weights(self.cost,\n                                           self.hparams.weight_decay_rate)\n\n    if is_training:\n      self._build_train_op()\n\n    # Setup checkpointing for this child model\n    # Keep 2 or more checkpoints around during training.\n    with tf.device(\'/cpu:0\'):\n      self.saver = tf.train.Saver(max_to_keep=2)\n\n    self.init = tf.group(tf.global_variables_initializer(),\n                         tf.local_variables_initializer())\n\n  def _calc_num_trainable_params(self):\n    self.num_trainable_params = np.sum([\n        np.prod(var.get_shape().as_list()) for var in tf.trainable_variables()\n    ])\n    tf.logging.info(\'number of trainable params: {}\'.format(\n        self.num_trainable_params))\n\n  def _build_train_op(self):\n    """"""Builds the train op for the cifar model.""""""\n    hparams = self.hparams\n    tvars = tf.trainable_variables()\n    grads = tf.gradients(self.cost, tvars)\n    if hparams.gradient_clipping_by_global_norm > 0.0:\n      grads, norm = tf.clip_by_global_norm(\n          grads, hparams.gradient_clipping_by_global_norm)\n      tf.summary.scalar(\'grad_norm\', norm)\n\n    # Setup the initial learning rate\n    initial_lr = self.lr_rate_ph\n    optimizer = tf.train.MomentumOptimizer(\n        initial_lr,\n        0.9,\n        use_nesterov=True)\n\n    self.optimizer = optimizer\n    apply_op = optimizer.apply_gradients(\n        zip(grads, tvars), global_step=self.global_step, name=\'train_step\')\n    train_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies([apply_op]):\n      self.train_op = tf.group(*train_ops)\n\n\nclass CifarModelTrainer(object):\n  """"""Trains an instance of the CifarModel class.""""""\n\n  def __init__(self, hparams):\n    self._session = None\n    self.hparams = hparams\n\n    self.model_dir = os.path.join(FLAGS.checkpoint_dir, \'model\')\n    self.log_dir = os.path.join(FLAGS.checkpoint_dir, \'log\')\n    # Set the random seed to be sure the same validation set\n    # is used for each model\n    np.random.seed(0)\n    self.data_loader = data_utils.DataSet(hparams)\n    np.random.seed()  # Put the random seed back to random\n    self.data_loader.reset()\n\n  def save_model(self, step=None):\n    """"""Dumps model into the backup_dir.\n\n    Args:\n      step: If provided, creates a checkpoint with the given step\n        number, instead of overwriting the existing checkpoints.\n    """"""\n    model_save_name = os.path.join(self.model_dir, \'model.ckpt\')\n    if not tf.gfile.IsDirectory(self.model_dir):\n      tf.gfile.MakeDirs(self.model_dir)\n    self.saver.save(self.session, model_save_name, global_step=step)\n    tf.logging.info(\'Saved child model\')\n\n  def extract_model_spec(self):\n    """"""Loads a checkpoint with the architecture structure stored in the name.""""""\n    checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    if checkpoint_path is not None:\n      self.saver.restore(self.session, checkpoint_path)\n      tf.logging.info(\'Loaded child model checkpoint from %s\',\n                      checkpoint_path)\n    else:\n      self.save_model(step=0)\n\n  def eval_child_model(self, model, data_loader, mode):\n    """"""Evaluate the child model.\n\n    Args:\n      model: image model that will be evaluated.\n      data_loader: dataset object to extract eval data from.\n      mode: will the model be evalled on train, val or test.\n\n    Returns:\n      Accuracy of the model on the specified dataset.\n    """"""\n    tf.logging.info(\'Evaluating child model in mode %s\', mode)\n    while True:\n      try:\n        with self._new_session(model):\n          accuracy = helper_utils.eval_child_model(\n              self.session,\n              model,\n              data_loader,\n              mode)\n          tf.logging.info(\'Eval child model accuracy: {}\'.format(accuracy))\n          # If epoch trained without raising the below errors, break\n          # from loop.\n          break\n      except (tf.errors.AbortedError, tf.errors.UnavailableError) as e:\n        tf.logging.info(\'Retryable error caught: %s.  Retrying.\', e)\n\n    return accuracy\n\n  @contextlib.contextmanager\n  def _new_session(self, m):\n    """"""Creates a new session for model m.""""""\n    # Create a new session for this model, initialize\n    # variables, and save / restore from\n    # checkpoint.\n    self._session = tf.Session(\n        \'\',\n        config=tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False))\n    self.session.run(m.init)\n\n    # Load in a previous checkpoint, or save this one\n    self.extract_model_spec()\n    try:\n      yield\n    finally:\n      tf.Session.reset(\'\')\n      self._session = None\n\n  def _build_models(self):\n    """"""Builds the image models for train and eval.""""""\n    # Determine if we should build the train and eval model. When using\n    # distributed training we only want to build one or the other and not both.\n    with tf.variable_scope(\'model\', use_resource=False):\n      m = CifarModel(self.hparams)\n      m.build(\'train\')\n      self._num_trainable_params = m.num_trainable_params\n      self._saver = m.saver\n    with tf.variable_scope(\'model\', reuse=True, use_resource=False):\n      meval = CifarModel(self.hparams)\n      meval.build(\'eval\')\n    return m, meval\n\n  def _calc_starting_epoch(self, m):\n    """"""Calculates the starting epoch for model m based on global step.""""""\n    hparams = self.hparams\n    batch_size = hparams.batch_size\n    steps_per_epoch = int(hparams.train_size / batch_size)\n    with self._new_session(m):\n      curr_step = self.session.run(m.global_step)\n    total_steps = steps_per_epoch * hparams.num_epochs\n    epochs_left = (total_steps - curr_step) // steps_per_epoch\n    starting_epoch = hparams.num_epochs - epochs_left\n    return starting_epoch\n\n  def _run_training_loop(self, m, curr_epoch):\n    """"""Trains the cifar model `m` for one epoch.""""""\n    start_time = time.time()\n    while True:\n      try:\n        with self._new_session(m):\n          train_accuracy = helper_utils.run_epoch_training(\n              self.session, m, self.data_loader, curr_epoch)\n          tf.logging.info(\'Saving model after epoch\')\n          self.save_model(step=curr_epoch)\n          break\n      except (tf.errors.AbortedError, tf.errors.UnavailableError) as e:\n        tf.logging.info(\'Retryable error caught: %s.  Retrying.\', e)\n    tf.logging.info(\'Finished epoch: {}\'.format(curr_epoch))\n    tf.logging.info(\'Epoch time(min): {}\'.format(\n        (time.time() - start_time) / 60.0))\n    return train_accuracy\n\n  def _compute_final_accuracies(self, meval):\n    """"""Run once training is finished to compute final val/test accuracies.""""""\n    valid_accuracy = self.eval_child_model(meval, self.data_loader, \'val\')\n    if self.hparams.eval_test:\n      test_accuracy = self.eval_child_model(meval, self.data_loader, \'test\')\n    else:\n      test_accuracy = 0\n    tf.logging.info(\'Test Accuracy: {}\'.format(test_accuracy))\n    return valid_accuracy, test_accuracy\n\n  def run_model(self):\n    """"""Trains and evalutes the image model.""""""\n    hparams = self.hparams\n\n    # Build the child graph\n    with tf.Graph().as_default(), tf.device(\n        \'/cpu:0\' if FLAGS.use_cpu else \'/gpu:0\'):\n      m, meval = self._build_models()\n\n      # Figure out what epoch we are on\n      starting_epoch = self._calc_starting_epoch(m)\n\n      # Run the validation error right at the beginning\n      valid_accuracy = self.eval_child_model(\n          meval, self.data_loader, \'val\')\n      tf.logging.info(\'Before Training Epoch: {}     Val Acc: {}\'.format(\n          starting_epoch, valid_accuracy))\n      training_accuracy = None\n\n      for curr_epoch in range(starting_epoch, hparams.num_epochs):\n\n        # Run one training epoch\n        training_accuracy = self._run_training_loop(m, curr_epoch)\n\n        valid_accuracy = self.eval_child_model(\n            meval, self.data_loader, \'val\')\n        tf.logging.info(\'Epoch: {}    Valid Acc: {}\'.format(\n            curr_epoch, valid_accuracy))\n\n      valid_accuracy, test_accuracy = self._compute_final_accuracies(\n          meval)\n\n    tf.logging.info(\n        \'Train Acc: {}    Valid Acc: {}     Test Acc: {}\'.format(\n            training_accuracy, valid_accuracy, test_accuracy))\n\n  @property\n  def saver(self):\n    return self._saver\n\n  @property\n  def session(self):\n    return self._session\n\n  @property\n  def num_trainable_params(self):\n    return self._num_trainable_params\n\n\ndef main(_):\n  if FLAGS.dataset not in [\'cifar10\', \'cifar100\']:\n    raise ValueError(\'Invalid dataset: %s\' % FLAGS.dataset)\n  hparams = tf.contrib.training.HParams(\n      train_size=50000,\n      validation_size=0,\n      eval_test=1,\n      dataset=FLAGS.dataset,\n      data_path=FLAGS.data_path,\n      batch_size=128,\n      gradient_clipping_by_global_norm=5.0)\n  if FLAGS.model_name == \'wrn\':\n    hparams.add_hparam(\'model_name\', \'wrn\')\n    hparams.add_hparam(\'num_epochs\', 200)\n    hparams.add_hparam(\'wrn_size\', 160)\n    hparams.add_hparam(\'lr\', 0.1)\n    hparams.add_hparam(\'weight_decay_rate\', 5e-4)\n  elif FLAGS.model_name == \'shake_shake_32\':\n    hparams.add_hparam(\'model_name\', \'shake_shake\')\n    hparams.add_hparam(\'num_epochs\', 1800)\n    hparams.add_hparam(\'shake_shake_widen_factor\', 2)\n    hparams.add_hparam(\'lr\', 0.01)\n    hparams.add_hparam(\'weight_decay_rate\', 0.001)\n  elif FLAGS.model_name == \'shake_shake_96\':\n    hparams.add_hparam(\'model_name\', \'shake_shake\')\n    hparams.add_hparam(\'num_epochs\', 1800)\n    hparams.add_hparam(\'shake_shake_widen_factor\', 6)\n    hparams.add_hparam(\'lr\', 0.01)\n    hparams.add_hparam(\'weight_decay_rate\', 0.001)\n  elif FLAGS.model_name == \'shake_shake_112\':\n    hparams.add_hparam(\'model_name\', \'shake_shake\')\n    hparams.add_hparam(\'num_epochs\', 1800)\n    hparams.add_hparam(\'shake_shake_widen_factor\', 7)\n    hparams.add_hparam(\'lr\', 0.01)\n    hparams.add_hparam(\'weight_decay_rate\', 0.001)\n  elif FLAGS.model_name == \'pyramid_net\':\n    hparams.add_hparam(\'model_name\', \'pyramid_net\')\n    hparams.add_hparam(\'num_epochs\', 1800)\n    hparams.add_hparam(\'lr\', 0.05)\n    hparams.add_hparam(\'weight_decay_rate\', 5e-5)\n    hparams.batch_size = 64\n  else:\n    raise ValueError(\'Not Valid Model Name: %s\' % FLAGS.model_name)\n  cifar_trainer = CifarModelTrainer(hparams)\n  cifar_trainer.run_model()\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
autoaugment/wrn.py,13,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builds the Wide-ResNet Model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nimport autoaugment.custom_ops as ops\n\n\ndef residual_block(\n    x, in_filter, out_filter, stride, activate_before_residual=False):\n  """"""Adds residual connection to `x` in addition to applying BN->ReLU->3x3 Conv.\n\n  Args:\n    x: Tensor that is the output of the previous layer in the model.\n    in_filter: Number of filters `x` has.\n    out_filter: Number of filters that the output of this layer will have.\n    stride: Integer that specified what stride should be applied to `x`.\n    activate_before_residual: Boolean on whether a BN->ReLU should be applied\n      to x before the convolution is applied.\n\n  Returns:\n    A Tensor that is the result of applying two sequences of BN->ReLU->3x3 Conv\n    and then adding that Tensor to `x`.\n  """"""\n\n  if activate_before_residual:  # Pass up RELU and BN activation for resnet\n    with tf.variable_scope(\'shared_activation\'):\n      x = ops.batch_norm(x, scope=\'init_bn\')\n      x = tf.nn.relu(x)\n      orig_x = x\n  else:\n    orig_x = x\n\n  block_x = x\n  if not activate_before_residual:\n    with tf.variable_scope(\'residual_only_activation\'):\n      block_x = ops.batch_norm(block_x, scope=\'init_bn\')\n      block_x = tf.nn.relu(block_x)\n\n  with tf.variable_scope(\'sub1\'):\n    block_x = ops.conv2d(\n        block_x, out_filter, 3, stride=stride, scope=\'conv1\')\n\n  with tf.variable_scope(\'sub2\'):\n    block_x = ops.batch_norm(block_x, scope=\'bn2\')\n    block_x = tf.nn.relu(block_x)\n    block_x = ops.conv2d(\n        block_x, out_filter, 3, stride=1, scope=\'conv2\')\n\n  with tf.variable_scope(\n      \'sub_add\'):  # If number of filters do not agree then zero pad them\n    if in_filter != out_filter:\n      orig_x = ops.avg_pool(orig_x, stride, stride)\n      orig_x = ops.zero_pad(orig_x, in_filter, out_filter)\n  x = orig_x + block_x\n  return x\n\n\ndef _res_add(in_filter, out_filter, stride, x, orig_x):\n  """"""Adds `x` with `orig_x`, both of which are layers in the model.\n\n  Args:\n    in_filter: Number of filters in `orig_x`.\n    out_filter: Number of filters in `x`.\n    stride: Integer specifying the stide that should be applied `orig_x`.\n    x: Tensor that is the output of the previous layer.\n    orig_x: Tensor that is the output of an earlier layer in the network.\n\n  Returns:\n    A Tensor that is the result of `x` and `orig_x` being added after\n    zero padding and striding are applied to `orig_x` to get the shapes\n    to match.\n  """"""\n  if in_filter != out_filter:\n    orig_x = ops.avg_pool(orig_x, stride, stride)\n    orig_x = ops.zero_pad(orig_x, in_filter, out_filter)\n  x = x + orig_x\n  orig_x = x\n  return x, orig_x\n\n\ndef build_wrn_model(images, num_classes, wrn_size):\n  """"""Builds the WRN model.\n\n  Build the Wide ResNet model from https://arxiv.org/abs/1605.07146.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    wrn_size: Parameter that scales the number of filters in the Wide ResNet\n      model.\n\n  Returns:\n    The logits of the Wide ResNet model.\n  """"""\n  kernel_size = wrn_size\n  filter_size = 3\n  num_blocks_per_resnet = 4\n  filters = [\n      min(kernel_size, 16), kernel_size, kernel_size * 2, kernel_size * 4\n  ]\n  strides = [1, 2, 2]  # stride for each resblock\n\n  # Run the first conv\n  with tf.variable_scope(\'init\'):\n    x = images\n    output_filters = filters[0]\n    x = ops.conv2d(x, output_filters, filter_size, scope=\'init_conv\')\n\n  first_x = x  # Res from the beginning\n  orig_x = x  # Res from previous block\n\n  for block_num in range(1, 4):\n    with tf.variable_scope(\'unit_{}_0\'.format(block_num)):\n      activate_before_residual = True if block_num == 1 else False\n      x = residual_block(\n          x,\n          filters[block_num - 1],\n          filters[block_num],\n          strides[block_num - 1],\n          activate_before_residual=activate_before_residual)\n    for i in range(1, num_blocks_per_resnet):\n      with tf.variable_scope(\'unit_{}_{}\'.format(block_num, i)):\n        x = residual_block(\n            x,\n            filters[block_num],\n            filters[block_num],\n            1,\n            activate_before_residual=False)\n    x, orig_x = _res_add(filters[block_num - 1], filters[block_num],\n                         strides[block_num - 1], x, orig_x)\n  final_stride_val = np.prod(strides)\n  x, _ = _res_add(filters[0], filters[3], final_stride_val, x, first_x)\n  with tf.variable_scope(\'unit_last\'):\n    x = ops.batch_norm(x, scope=\'final_bn\')\n    x = tf.nn.relu(x)\n    x = ops.global_avg_pool(x)\n    logits = ops.fc(x, num_classes)\n  return logits\n'"
pba/__init__.py,0,b''
pba/augmentation_transforms.py,0,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Transforms used in the Augmentation Policies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\nimport random\nimport numpy as np\nfrom PIL import ImageOps, ImageEnhance, ImageFilter, Image\n\nMEANS = {\n    \'cifar10_50000\': [0.49139968, 0.48215841, 0.44653091],\n    \'cifar10_4000\': [0.49056774, 0.48116026, 0.44726052],\n    \'cifar100_50000\': [0.50707516, 0.48654887, 0.44091784],\n    \'svhn_1000\': [0.45163885, 0.4557915, 0.48093327],\n    \'svhn-full_604388\': [0.43090966, 0.4302428, 0.44634357]\n}\nSTDS = {\n    \'cifar10_50000\': [0.24703223, 0.24348513, 0.26158784],\n    \'cifar10_4000\': [0.24710728, 0.24451308, 0.26235099],\n    \'cifar100_50000\': [0.26733429, 0.25643846, 0.27615047],\n    \'svhn_1000\': [0.20385217, 0.20957996, 0.20804394],\n    \'svhn-full_604388\': [0.19652855, 0.19832038, 0.19942076]\n}\nPARAMETER_MAX = 10  # What is the max \'level\' a transform could be predicted\n\n\ndef pil_wrap(img, dataset):\n    """"""Convert the `img` numpy tensor to a PIL Image.""""""\n    return Image.fromarray(\n        np.uint8(\n            (img * STDS[dataset] + MEANS[dataset]) * 255.0)).convert(\'RGBA\')\n\n\ndef pil_unwrap(pil_img, dataset, image_size):\n    """"""Converts the PIL img to a numpy array.""""""\n    pic_array = (np.array(pil_img.getdata()).reshape((image_size, image_size, 4)) / 255.0)\n    i1, i2 = np.where(pic_array[:, :, 3] == 0)\n    pic_array = (pic_array[:, :, :3] - MEANS[dataset]) / STDS[dataset]\n    pic_array[i1, i2] = [0, 0, 0]\n    return pic_array\n\n\ndef apply_policy(policy, img, dset, image_size):\n    """"""Apply the `policy` to the numpy `img`.\n\n  Args:\n    policy: A list of tuples with the form (name, probability, level) where\n      `name` is the name of the augmentation operation to apply, `probability`\n      is the probability of applying the operation and `level` is what strength\n      the operation to apply.\n    img: Numpy image that will have `policy` applied to it.\n    dset: Dataset, one of the keys of MEANS or STDS.\n    image_size: Width and height of image.\n\n  Returns:\n    The result of applying `policy` to `img`.\n  """"""\n    pil_img = pil_wrap(img, dset)\n    for xform in policy:\n        assert len(xform) == 3\n        name, probability, level = xform\n        xform_fn = NAME_TO_TRANSFORM[name].pil_transformer(\n            probability, level, image_size)\n        pil_img = xform_fn(pil_img)\n    return pil_unwrap(pil_img, dset, image_size)\n\n\ndef random_flip(x):\n    """"""Flip the input x horizontally with 50% probability.""""""\n    if np.random.rand(1)[0] > 0.5:\n        return np.fliplr(x)\n    return x\n\n\ndef zero_pad_and_crop(img, amount=4):\n    """"""Zero pad by `amount` zero pixels on each side then take a random crop.\n\n  Args:\n    img: numpy image that will be zero padded and cropped.\n    amount: amount of zeros to pad `img` with horizontally and verically.\n\n  Returns:\n    The cropped zero padded img. The returned numpy array will be of the same\n    shape as `img`.\n  """"""\n    padded_img = np.zeros((img.shape[0] + amount * 2,\n                           img.shape[1] + amount * 2, img.shape[2]))\n    padded_img[amount:img.shape[0] + amount, amount:img.shape[1] +\n               amount, :] = img\n    top = np.random.randint(low=0, high=2 * amount)\n    left = np.random.randint(low=0, high=2 * amount)\n    new_img = padded_img[top:top + img.shape[0], left:left + img.shape[1], :]\n    return new_img\n\n\ndef create_cutout_mask(img_height, img_width, num_channels, size):\n    """"""Creates a zero mask used for cutout of shape `img_height` x `img_width`.\n\n  Args:\n    img_height: Height of image cutout mask will be applied to.\n    img_width: Width of image cutout mask will be applied to.\n    num_channels: Number of channels in the image.\n    size: Size of the zeros mask.\n\n  Returns:\n    A mask of shape `img_height` x `img_width` with all ones except for a\n    square of zeros of shape `size` x `size`. This mask is meant to be\n    elementwise multiplied with the original image. Additionally returns\n    the `upper_coord` and `lower_coord` which specify where the cutout mask\n    will be applied.\n  """"""\n    assert img_height == img_width\n\n    # Sample center where cutout mask will be applied\n    height_loc = np.random.randint(low=0, high=img_height)\n    width_loc = np.random.randint(low=0, high=img_width)\n\n    # Determine upper right and lower left corners of patch\n    upper_coord = (max(0, height_loc - size // 2), max(0,\n                                                       width_loc - size // 2))\n    lower_coord = (min(img_height, height_loc + size // 2),\n                   min(img_width, width_loc + size // 2))\n    mask_height = lower_coord[0] - upper_coord[0]\n    mask_width = lower_coord[1] - upper_coord[1]\n    assert mask_height > 0\n    assert mask_width > 0\n\n    mask = np.ones((img_height, img_width, num_channels))\n    zeros = np.zeros((mask_height, mask_width, num_channels))\n    mask[upper_coord[0]:lower_coord[0], upper_coord[1]:lower_coord[1], :] = (\n        zeros)\n    return mask, upper_coord, lower_coord\n\n\ndef cutout_numpy(img, size=16):\n    """"""Apply cutout with mask of shape `size` x `size` to `img`.\n\n  The cutout operation is from the paper https://arxiv.org/abs/1708.04552.\n  This operation applies a `size`x`size` mask of zeros to a random location\n  within `img`.\n\n  Args:\n    img: Numpy image that cutout will be applied to.\n    size: Height/width of the cutout mask that will be\n\n  Returns:\n    A numpy tensor that is the result of applying the cutout mask to `img`.\n  """"""\n    img_height, img_width, num_channels = (img.shape[0], img.shape[1],\n                                           img.shape[2])\n    assert len(img.shape) == 3\n    mask, _, _ = create_cutout_mask(img_height, img_width, num_channels, size)\n    return img * mask\n\n\ndef float_parameter(level, maxval):\n    """"""Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    A float that results from scaling `maxval` according to `level`.\n  """"""\n    return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n    """"""Helper function to scale `val` between 0 and maxval .\n\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled\n      to level/PARAMETER_MAX.\n\n  Returns:\n    An int that results from scaling `maxval` according to `level`.\n  """"""\n    return int(level * maxval / PARAMETER_MAX)\n\n\nclass TransformFunction(object):\n    """"""Wraps the Transform function for pretty printing options.""""""\n\n    def __init__(self, func, name):\n        self.f = func\n        self.name = name\n\n    def __repr__(self):\n        return \'<\' + self.name + \'>\'\n\n    def __call__(self, pil_img):\n        return self.f(pil_img)\n\n\nclass TransformT(object):\n    """"""Each instance of this class represents a specific transform.""""""\n\n    def __init__(self, name, xform_fn):\n        self.name = name\n        self.xform = xform_fn\n\n    def pil_transformer(self, probability, level, image_size):\n        def return_function(im):\n            if random.random() < probability:\n                if \'image_size\' in inspect.getargspec(self.xform).args:\n                    im = self.xform(im, level, image_size)\n                else:\n                    im = self.xform(im, level)\n            return im\n\n        name = self.name + \'({:.1f},{})\'.format(probability, level)\n        return TransformFunction(return_function, name)\n\n\n################## Transform Functions ##################\nidentity = TransformT(\'identity\', lambda pil_img, level: pil_img)\nflip_lr = TransformT(\n    \'FlipLR\', lambda pil_img, level: pil_img.transpose(Image.FLIP_LEFT_RIGHT))\nflip_ud = TransformT(\n    \'FlipUD\', lambda pil_img, level: pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n# pylint:disable=g-long-lambda\nauto_contrast = TransformT(\n    \'AutoContrast\',\n    lambda pil_img, level: ImageOps.autocontrast(pil_img.convert(\'RGB\')).convert(\'RGBA\')\n)\nequalize = TransformT(\n    \'Equalize\',\n    lambda pil_img, level: ImageOps.equalize(pil_img.convert(\'RGB\')).convert(\'RGBA\')\n)\ninvert = TransformT(\n    \'Invert\',\n    lambda pil_img, level: ImageOps.invert(pil_img.convert(\'RGB\')).convert(\'RGBA\')\n)\n# pylint:enable=g-long-lambda\nblur = TransformT(\'Blur\',\n                  lambda pil_img, level: pil_img.filter(ImageFilter.BLUR))\nsmooth = TransformT(\'Smooth\',\n                    lambda pil_img, level: pil_img.filter(ImageFilter.SMOOTH))\n\n\ndef _rotate_impl(pil_img, level):\n    """"""Rotates `pil_img` from -30 to 30 degrees depending on `level`.""""""\n    degrees = int_parameter(level, 30)\n    if random.random() > 0.5:\n        degrees = -degrees\n    return pil_img.rotate(degrees)\n\n\nrotate = TransformT(\'Rotate\', _rotate_impl)\n\n\ndef _posterize_impl(pil_img, level):\n    """"""Applies PIL Posterize to `pil_img`.""""""\n    level = int_parameter(level, 4)\n    return ImageOps.posterize(pil_img.convert(\'RGB\'),\n                              4 - level).convert(\'RGBA\')\n\n\nposterize = TransformT(\'Posterize\', _posterize_impl)\n\n\ndef _shear_x_impl(pil_img, level, image_size):\n    """"""Applies PIL ShearX to `pil_img`.\n\n  The ShearX operation shears the image along the horizontal axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  """"""\n    level = float_parameter(level, 0.3)\n    if random.random() > 0.5:\n        level = -level\n    return pil_img.transform((image_size, image_size), Image.AFFINE, (1, level, 0, 0, 1, 0))\n\n\nshear_x = TransformT(\'ShearX\', _shear_x_impl)\n\n\ndef _shear_y_impl(pil_img, level, image_size):\n    """"""Applies PIL ShearY to `pil_img`.\n\n  The ShearY operation shears the image along the vertical axis with `level`\n  magnitude.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had ShearX applied to it.\n  """"""\n    level = float_parameter(level, 0.3)\n    if random.random() > 0.5:\n        level = -level\n    return pil_img.transform((image_size, image_size), Image.AFFINE, (1, 0, 0, level, 1, 0))\n\n\nshear_y = TransformT(\'ShearY\', _shear_y_impl)\n\n\ndef _translate_x_impl(pil_img, level, image_size):\n    """"""Applies PIL TranslateX to `pil_img`.\n\n  Translate the image in the horizontal direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateX applied to it.\n  """"""\n    level = int_parameter(level, 10)\n    if random.random() > 0.5:\n        level = -level\n    return pil_img.transform((image_size, image_size), Image.AFFINE, (1, 0, level, 0, 1, 0))\n\n\ntranslate_x = TransformT(\'TranslateX\', _translate_x_impl)\n\n\ndef _translate_y_impl(pil_img, level, image_size):\n    """"""Applies PIL TranslateY to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had TranslateY applied to it.\n  """"""\n    level = int_parameter(level, 10)\n    if random.random() > 0.5:\n        level = -level\n    return pil_img.transform((image_size, image_size), Image.AFFINE, (1, 0, 0, 0, 1, level))\n\n\ntranslate_y = TransformT(\'TranslateY\', _translate_y_impl)\n\n\ndef _crop_impl(pil_img, level, image_size, interpolation=Image.BILINEAR):\n    """"""Applies a crop to `pil_img` with the size depending on the `level`.""""""\n    cropped = pil_img.crop((level, level, image_size - level,\n                            image_size - level))\n    resized = cropped.resize((image_size, image_size), interpolation)\n    return resized\n\n\ncrop_bilinear = TransformT(\'CropBilinear\', _crop_impl)\n\n\ndef _solarize_impl(pil_img, level):\n    """"""Applies PIL Solarize to `pil_img`.\n\n  Translate the image in the vertical direction by `level`\n  number of pixels.\n\n  Args:\n    pil_img: Image in PIL object.\n    level: Strength of the operation specified as an Integer from\n      [0, `PARAMETER_MAX`].\n\n  Returns:\n    A PIL Image that has had Solarize applied to it.\n  """"""\n    level = int_parameter(level, 256)\n    return ImageOps.solarize(pil_img.convert(\'RGB\'),\n                             256 - level).convert(\'RGBA\')\n\n\nsolarize = TransformT(\'Solarize\', _solarize_impl)\n\n\ndef _cutout_pil_impl(pil_img, level, image_size):\n    """"""Apply cutout to pil_img at the specified level.""""""\n    size = int_parameter(level, 20)\n    if size <= 0:\n        return pil_img\n    img_height, img_width, num_channels = (image_size, image_size, 3)\n    _, upper_coord, lower_coord = (create_cutout_mask(img_height, img_width,\n                                                      num_channels, size))\n    pixels = pil_img.load()  # create the pixel map\n    for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n        for j in range(upper_coord[1], lower_coord[1]):  # For every row\n            pixels[i, j] = (125, 122, 113, 0)  # set the colour accordingly\n    return pil_img\n\n\ncutout = TransformT(\'Cutout\', _cutout_pil_impl)\n\n\ndef _enhancer_impl(enhancer):\n    """"""Sets level to be between 0.1 and 1.8 for ImageEnhance transforms of PIL.""""""\n\n    def impl(pil_img, level):\n        v = float_parameter(level, 1.8) + .1  # going to 0 just destroys it\n        return enhancer(pil_img).enhance(v)\n\n    return impl\n\n\ncolor = TransformT(\'Color\', _enhancer_impl(ImageEnhance.Color))\ncontrast = TransformT(\'Contrast\', _enhancer_impl(ImageEnhance.Contrast))\nbrightness = TransformT(\'Brightness\', _enhancer_impl(ImageEnhance.Brightness))\nsharpness = TransformT(\'Sharpness\', _enhancer_impl(ImageEnhance.Sharpness))\n\nALL_TRANSFORMS = [\n    flip_lr, flip_ud, auto_contrast, equalize, invert, rotate, posterize,\n    crop_bilinear, solarize, color, contrast, brightness, sharpness, shear_x,\n    shear_y, translate_x, translate_y, cutout, blur, smooth\n]\n\nNAME_TO_TRANSFORM = {t.name: t for t in ALL_TRANSFORMS}\nTRANSFORM_NAMES = NAME_TO_TRANSFORM.keys()\n'"
pba/augmentation_transforms_hp.py,0,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Transforms used in the PBA Augmentation Policies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport collections\nimport inspect\nimport random\n\nimport numpy as np\nfrom PIL import ImageOps, ImageEnhance, ImageFilter, Image  # pylint:disable=g-multiple-import\n\nfrom pba.augmentation_transforms import random_flip, zero_pad_and_crop, cutout_numpy  # pylint: disable=unused-import\nfrom pba.augmentation_transforms import TransformFunction\nfrom pba.augmentation_transforms import ALL_TRANSFORMS, NAME_TO_TRANSFORM, TRANSFORM_NAMES  # pylint: disable=unused-import\nfrom pba.augmentation_transforms import pil_wrap, pil_unwrap  # pylint: disable=unused-import\nfrom pba.augmentation_transforms import MEANS, STDS, PARAMETER_MAX  # pylint: disable=unused-import\nfrom pba.augmentation_transforms import _rotate_impl, _posterize_impl, _shear_x_impl, _shear_y_impl, _translate_x_impl, _translate_y_impl, _crop_impl, _solarize_impl, _cutout_pil_impl, _enhancer_impl\n\n\ndef apply_policy(policy, img, aug_policy, dset, image_size, verbose=False):\n    """"""Apply the `policy` to the numpy `img`.\n\n  Args:\n    policy: A list of tuples with the form (name, probability, level) where\n      `name` is the name of the augmentation operation to apply, `probability`\n      is the probability of applying the operation and `level` is what strength\n      the operation to apply.\n    img: Numpy image that will have `policy` applied to it.\n    aug_policy: Augmentation policy to use.\n    dset: Dataset, one of the keys of MEANS or STDS.\n    image_size: Width and height of image.\n    verbose: Whether to print applied augmentations.\n\n  Returns:\n    The result of applying `policy` to `img`.\n  """"""\n    if aug_policy == \'cifar10\':\n        count = np.random.choice([0, 1, 2, 3], p=[0.2, 0.3, 0.5, 0.0])\n    else:\n        raise ValueError(\'Unknown aug policy.\')\n    if count != 0:\n        pil_img = pil_wrap(img, dset)\n        policy = copy.copy(policy)\n        random.shuffle(policy)\n        for xform in policy:\n            assert len(xform) == 3\n            name, probability, level = xform\n            assert 0. <= probability <= 1.\n            assert 0 <= level <= PARAMETER_MAX\n            xform_fn = NAME_TO_TRANSFORM[name].pil_transformer(\n                probability, level, image_size)\n            pil_img, res = xform_fn(pil_img)\n            if verbose and res:\n                print(""Op: {}, Magnitude: {}, Prob: {}"".format(name, level, probability))\n            count -= res\n            assert count >= 0\n            if count == 0:\n                break\n        return pil_unwrap(pil_img, dset, image_size)\n    else:\n        return img\n\n\nclass TransformT(object):\n    """"""Each instance of this class represents a specific transform.""""""\n\n    def __init__(self, name, xform_fn):\n        self.name = name\n        self.xform = xform_fn\n\n    def pil_transformer(self, probability, level, image_size):\n        """"""Builds augmentation function which returns resulting image and whether augmentation was applied.""""""\n\n        def return_function(im):\n            res = False\n            if random.random() < probability:\n                if \'image_size\' in inspect.getargspec(self.xform).args:\n                    im = self.xform(im, level, image_size)\n                else:\n                    im = self.xform(im, level)\n                res = True\n            return im, res\n\n        name = self.name + \'({:.1f},{})\'.format(probability, level)\n        return TransformFunction(return_function, name)\n\n    def str(self):\n        return self.name\n\n\n################## Transform Functions ##################\nidentity = TransformT(\'identity\', lambda pil_img, level: pil_img)\nflip_lr = TransformT(\n    \'FlipLR\', lambda pil_img, level: pil_img.transpose(Image.FLIP_LEFT_RIGHT))\nflip_ud = TransformT(\n    \'FlipUD\', lambda pil_img, level: pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n# pylint:disable=g-long-lambda\nauto_contrast = TransformT(\n    \'AutoContrast\',\n    lambda pil_img, level: ImageOps.autocontrast(pil_img.convert(\'RGB\')).convert(\'RGBA\')\n)\nequalize = TransformT(\n    \'Equalize\',\n    lambda pil_img, level: ImageOps.equalize(pil_img.convert(\'RGB\')).convert(\'RGBA\')\n)\ninvert = TransformT(\n    \'Invert\',\n    lambda pil_img, level: ImageOps.invert(pil_img.convert(\'RGB\')).convert(\'RGBA\')\n)\n# pylint:enable=g-long-lambda\nblur = TransformT(\'Blur\',\n                  lambda pil_img, level: pil_img.filter(ImageFilter.BLUR))\nsmooth = TransformT(\'Smooth\',\n                    lambda pil_img, level: pil_img.filter(ImageFilter.SMOOTH))\nrotate = TransformT(\'Rotate\', _rotate_impl)\nposterize = TransformT(\'Posterize\', _posterize_impl)\nshear_x = TransformT(\'ShearX\', _shear_x_impl)\nshear_y = TransformT(\'ShearY\', _shear_y_impl)\ntranslate_x = TransformT(\'TranslateX\', _translate_x_impl)\ntranslate_y = TransformT(\'TranslateY\', _translate_y_impl)\ncrop_bilinear = TransformT(\'CropBilinear\', _crop_impl)\nsolarize = TransformT(\'Solarize\', _solarize_impl)\ncutout = TransformT(\'Cutout\', _cutout_pil_impl)\ncolor = TransformT(\'Color\', _enhancer_impl(ImageEnhance.Color))\ncontrast = TransformT(\'Contrast\', _enhancer_impl(ImageEnhance.Contrast))\nbrightness = TransformT(\'Brightness\', _enhancer_impl(ImageEnhance.Brightness))\nsharpness = TransformT(\'Sharpness\', _enhancer_impl(ImageEnhance.Sharpness))\n\nHP_TRANSFORMS = [\n    rotate,\n    translate_x,\n    translate_y,\n    brightness,\n    color,\n    invert,\n    sharpness,\n    posterize,\n    shear_x,\n    solarize,\n    shear_y,\n    equalize,\n    auto_contrast,\n    cutout,\n    contrast\n]\n\nNAME_TO_TRANSFORM = collections.OrderedDict((t.name, t) for t in HP_TRANSFORMS)\nHP_TRANSFORM_NAMES = NAME_TO_TRANSFORM.keys()\nNUM_HP_TRANSFORM = len(HP_TRANSFORM_NAMES)\n'"
pba/data_utils.py,13,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Data utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\nimport os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport torchvision\n\nfrom autoaugment.data_utils import unpickle\nimport pba.policies as found_policies\nfrom pba.utils import parse_log_schedule\nimport pba.augmentation_transforms_hp as augmentation_transforms_pba\nimport pba.augmentation_transforms as augmentation_transforms_autoaug\n\n\n# pylint:disable=logging-format-interpolation\n\n\ndef parse_policy(policy_emb, augmentation_transforms):\n    policy = []\n    num_xform = augmentation_transforms.NUM_HP_TRANSFORM\n    xform_names = augmentation_transforms.HP_TRANSFORM_NAMES\n    assert len(policy_emb\n               ) == 2 * num_xform, \'policy was: {}, supposed to be: {}\'.format(\n                   len(policy_emb), 2 * num_xform)\n    for i, xform in enumerate(xform_names):\n        policy.append((xform, policy_emb[2 * i] / 10., policy_emb[2 * i + 1]))\n    return policy\n\n\ndef shuffle_data(data, labels):\n    """"""Shuffle data using numpy.""""""\n    np.random.seed(0)\n    perm = np.arange(len(data))\n    np.random.shuffle(perm)\n    data = data[perm]\n    labels = labels[perm]\n    return data, labels\n\n\nclass DataSet(object):\n    """"""Dataset object that produces augmented training and eval data.""""""\n\n    def __init__(self, hparams):\n        self.hparams = hparams\n        self.epochs = 0\n        self.curr_train_index = 0\n\n        self.parse_policy(hparams)\n        self.load_data(hparams)\n\n        # Apply normalization\n        self.train_images = self.train_images.transpose(0, 2, 3, 1) / 255.0\n        self.val_images = self.val_images.transpose(0, 2, 3, 1) / 255.0\n        self.test_images = self.test_images.transpose(0, 2, 3, 1) / 255.0\n        if not hparams.recompute_dset_stats:\n            mean = self.augmentation_transforms.MEANS[hparams.dataset + \'_\' +\n                                                      str(hparams.train_size)]\n            std = self.augmentation_transforms.STDS[hparams.dataset + \'_\' +\n                                                    str(hparams.train_size)]\n        else:\n            mean = self.train_images.mean(axis=(0, 1, 2))\n            std = self.train_images.std(axis=(0, 1, 2))\n            self.augmentation_transforms.MEANS[hparams.dataset + \'_\' + str(hparams.train_size)] = mean\n            self.augmentation_transforms.STDS[hparams.dataset + \'_\' + str(hparams.train_size)] = std\n        tf.logging.info(\'mean:{}    std: {}\'.format(mean, std))\n\n        self.train_images = (self.train_images - mean) / std\n        self.val_images = (self.val_images - mean) / std\n        self.test_images = (self.test_images - mean) / std\n\n        assert len(self.test_images) == len(self.test_labels)\n        assert len(self.train_images) == len(self.train_labels)\n        assert len(self.val_images) == len(self.val_labels)\n        tf.logging.info(\'train dataset size: {}, test: {}, val: {}\'.format(\n            len(self.train_images), len(self.test_images), len(self.val_images)))\n\n    def parse_policy(self, hparams):\n        """"""Parses policy schedule from input, which can be a list, list of lists, text file, or pickled list.\n\n        If list is not nested, then uses the same policy for all epochs.\n\n        Args:\n        hparams: tf.hparams object.\n        """"""\n        # Parse policy\n        if hparams.use_hp_policy:\n            self.augmentation_transforms = augmentation_transforms_pba\n\n            if isinstance(hparams.hp_policy,\n                          str) and hparams.hp_policy.endswith(\'.txt\'):\n                if hparams.num_epochs % hparams.hp_policy_epochs != 0:\n                    tf.logging.warning(\n                        ""Schedule length (%s) doesn\'t divide evenly into epochs (%s), interpolating."",\n                        hparams.num_epochs, hparams.hp_policy_epochs)\n                tf.logging.info(\n                    \'schedule policy trained on {} epochs, parsing from: {}, multiplier: {}\'\n                    .format(\n                        hparams.hp_policy_epochs, hparams.hp_policy,\n                        float(hparams.num_epochs) / hparams.hp_policy_epochs))\n                raw_policy = parse_log_schedule(\n                    hparams.hp_policy,\n                    epochs=hparams.hp_policy_epochs,\n                    multiplier=float(hparams.num_epochs) /\n                    hparams.hp_policy_epochs)\n            elif isinstance(hparams.hp_policy,\n                            str) and hparams.hp_policy.endswith(\'.p\'):\n                assert hparams.num_epochs % hparams.hp_policy_epochs == 0\n                tf.logging.info(\'custom .p file, policy number: {}\'.format(\n                    hparams.schedule_num))\n                with open(hparams.hp_policy, \'rb\') as f:\n                    policy = pickle.load(f)[hparams.schedule_num]\n                raw_policy = []\n                for num_iters, pol in policy:\n                    for _ in range(num_iters * hparams.num_epochs //\n                                   hparams.hp_policy_epochs):\n                        raw_policy.append(pol)\n            else:\n                raw_policy = hparams.hp_policy\n\n            if isinstance(raw_policy[0], list):\n                self.policy = []\n                split = len(raw_policy[0]) // 2\n                for pol in raw_policy:\n                    cur_pol = parse_policy(pol[:split],\n                                           self.augmentation_transforms)\n                    cur_pol.extend(\n                        parse_policy(pol[split:],\n                                     self.augmentation_transforms))\n                    self.policy.append(cur_pol)\n                tf.logging.info(\'using HP policy schedule, last: {}\'.format(\n                    self.policy[-1]))\n            elif isinstance(raw_policy, list):\n                split = len(raw_policy) // 2\n                self.policy = parse_policy(raw_policy[:split],\n                                           self.augmentation_transforms)\n                self.policy.extend(\n                    parse_policy(raw_policy[split:],\n                                 self.augmentation_transforms))\n                tf.logging.info(\'using HP Policy, policy: {}\'.format(\n                    self.policy))\n\n        else:\n            self.augmentation_transforms = augmentation_transforms_autoaug\n            tf.logging.info(\'using ENAS Policy or no augmentaton policy\')\n            if \'svhn\' in hparams.dataset:\n                self.good_policies = found_policies.good_policies_svhn()\n            else:\n                assert \'cifar\' in hparams.dataset\n                self.good_policies = found_policies.good_policies()\n\n    def reset_policy(self, new_hparams):\n        self.hparams = new_hparams\n        self.parse_policy(new_hparams)\n        tf.logging.info(\'reset aug policy\')\n        return\n\n    def load_cifar(self, hparams):\n        train_labels = []\n        test_labels = []\n        num_data_batches_to_load = 5\n        total_batches_to_load = num_data_batches_to_load\n        train_batches_to_load = total_batches_to_load\n        assert hparams.train_size + hparams.validation_size <= 50000\n        # Determine how many images we have loaded\n        train_dataset_size = 10000 * num_data_batches_to_load\n\n        if hparams.dataset == \'cifar10\':\n            train_data = np.empty(\n                (total_batches_to_load, 10000, 3072), dtype=np.uint8)\n            datafiles = [\n                \'data_batch_1\', \'data_batch_2\', \'data_batch_3\', \'data_batch_4\',\n                \'data_batch_5\'\n            ]\n            datafiles = datafiles[:train_batches_to_load]\n            test_data = np.empty((1, 10000, 3072), dtype=np.uint8)\n            datafiles.append(\'test_batch\')\n            num_classes = 10\n        elif hparams.dataset == \'cifar100\':\n            assert num_data_batches_to_load == 5\n            train_data = np.empty((1, 50000, 3072), dtype=np.uint8)\n            datafiles = [\'train\']\n            test_data = np.empty((1, 10000, 3072), dtype=np.uint8)\n            datafiles.append(\'test\')\n            num_classes = 100\n\n        for file_num, f in enumerate(datafiles):\n            d = unpickle(os.path.join(hparams.data_path, f))\n            if f == \'test\' or f == \'test_batch\':\n                test_data[0] = copy.deepcopy(d[\'data\'])\n            else:\n                train_data[file_num] = copy.deepcopy(d[\'data\'])\n            if hparams.dataset == \'cifar10\':\n                labels = np.array(d[\'labels\'])\n            else:\n                labels = np.array(d[\'fine_labels\'])\n            nsamples = len(labels)\n            for idx in range(nsamples):\n                if f == \'test\' or f == \'test_batch\':\n                    test_labels.append(labels[idx])\n                else:\n                    train_labels.append(labels[idx])\n        train_data = train_data.reshape(train_dataset_size, 3072)\n        test_data = test_data.reshape(10000, 3072)\n        train_data = train_data.reshape(-1, 3, 32, 32)\n        test_data = test_data.reshape(-1, 3, 32, 32)\n        train_labels = np.array(train_labels, dtype=np.int32)\n        test_labels = np.array(test_labels, dtype=np.int32)\n\n        self.test_images, self.test_labels = test_data, test_labels\n        train_data, train_labels = shuffle_data(train_data, train_labels)\n        train_size, val_size = hparams.train_size, hparams.validation_size\n        assert 50000 >= train_size + val_size\n        self.train_images = train_data[:train_size]\n        self.train_labels = train_labels[:train_size]\n        self.val_images = train_data[train_size:train_size + val_size]\n        self.val_labels = train_labels[train_size:train_size + val_size]\n        self.num_classes = num_classes\n\n    def load_svhn(self, hparams):\n        train_labels = []\n        test_labels = []\n        if hparams.dataset == \'svhn\':\n            assert hparams.train_size == 1000\n            assert hparams.train_size + hparams.validation_size <= 73257\n            train_loader = torchvision.datasets.SVHN(\n                root=hparams.data_path, split=\'train\', download=True)\n            test_loader = torchvision.datasets.SVHN(\n                root=hparams.data_path, split=\'test\', download=True)\n            num_classes = 10\n            train_data = train_loader.data\n            test_data = test_loader.data\n            train_labels = train_loader.labels\n            test_labels = test_loader.labels\n        elif hparams.dataset == \'svhn-full\':\n            assert hparams.train_size == 73257 + 531131\n            assert hparams.validation_size == 0\n            train_loader = torchvision.datasets.SVHN(\n                root=hparams.data_path, split=\'train\', download=True)\n            test_loader = torchvision.datasets.SVHN(\n                root=hparams.data_path, split=\'test\', download=True)\n            extra_loader = torchvision.datasets.SVHN(\n                root=hparams.data_path, split=\'extra\', download=True)\n            num_classes = 10\n            train_data = np.concatenate(\n                [train_loader.data, extra_loader.data], axis=0)\n            test_data = test_loader.data\n            train_labels = np.concatenate(\n                [train_loader.labels, extra_loader.labels], axis=0)\n            test_labels = test_loader.labels\n        else:\n            raise ValueError(hparams.dataset)\n\n        self.test_images, self.test_labels = test_data, test_labels\n        train_data, train_labels = shuffle_data(train_data, train_labels)\n        train_size, val_size = hparams.train_size, hparams.validation_size\n        if hparams.dataset == \'svhn-full\':\n            assert train_size + val_size <= 604388\n        else:\n            assert train_size + val_size <= 73257\n        self.train_images = train_data[:train_size]\n        self.train_labels = train_labels[:train_size]\n        self.val_images = train_data[-val_size:]\n        self.val_labels = train_labels[-val_size:]\n        self.num_classes = num_classes\n\n    def load_test(self, hparams):\n        """"""Load random data and labels.""""""\n        test_size = 200\n        self.num_classes = 200\n        self.train_images = np.random.random((hparams.train_size, 3, 224, 224)) * 255\n        self.val_images = np.random.random((hparams.validation_size, 3, 224, 224)) * 255\n        self.test_images = np.random.random((test_size, 3, 224, 224)) * 255\n        self.train_labels = np.random.randint(0, self.num_classes, (hparams.train_size))\n        self.val_labels = np.random.randint(0, self.num_classes, (hparams.validation_size))\n        self.test_labels = np.random.randint(0, self.num_classes, (test_size))\n\n    def load_data(self, hparams):\n        """"""Load raw data from specified dataset.\n\n        Assumes data is in NCHW format.\n\n        Populates:\n            self.train_images: Training image data.\n            self.train_labels: Training ground truth labels.\n            self.val_images: Validation/holdout image data.\n            self.val_labels: Validation/holdout ground truth labels.\n            self.test_images: Testing image data.\n            self.test_labels: Testing ground truth labels.\n            self.num_classes: Number of classes.\n            self.num_train: Number of training examples.\n            self.image_size: Width/height of image.\n\n        Args:\n            hparams: tf.hparams object.\n        """"""\n        if hparams.dataset == \'cifar10\' or hparams.dataset == \'cifar100\':\n            self.load_cifar(hparams)\n        elif hparams.dataset == \'svhn\' or hparams.dataset == \'svhn-full\':\n            self.load_svhn(hparams)\n        elif hparams.dataset == \'test\':\n            self.load_test(hparams)\n        else:\n            raise ValueError(\'unimplemented\')\n\n        self.num_train = self.train_images.shape[0]\n        self.image_size = self.train_images.shape[2]\n        self.train_labels = np.eye(self.num_classes)[np.array(\n            self.train_labels, dtype=np.int32)]\n        self.val_labels = np.eye(self.num_classes)[np.array(\n            self.val_labels, dtype=np.int32)]\n        self.test_labels = np.eye(self.num_classes)[np.array(\n            self.test_labels, dtype=np.int32)]\n        assert len(self.train_images) == len(self.train_labels)\n        assert len(self.val_images) == len(self.val_labels)\n        assert len(self.test_images) == len(self.test_labels)\n        assert self.train_images.shape[2] == self.train_images.shape[3]\n\n    def next_batch(self, iteration=None):\n        """"""Return the next minibatch of augmented data.""""""\n        next_train_index = self.curr_train_index + self.hparams.batch_size\n        if next_train_index > self.num_train:\n            # Increase epoch number\n            epoch = self.epochs + 1\n            self.reset()\n            self.epochs = epoch\n        batched_data = (\n            self.train_images[self.curr_train_index:self.curr_train_index +\n                              self.hparams.batch_size],\n            self.train_labels[self.curr_train_index:self.curr_train_index +\n                              self.hparams.batch_size])\n        final_imgs = []\n\n        dset = self.hparams.dataset + \'_\' + str(self.hparams.train_size)\n        images, labels = batched_data\n        for data in images:\n            if not self.hparams.no_aug:\n                if not self.hparams.use_hp_policy:\n                    # apply autoaugment policy\n                    epoch_policy = self.good_policies[np.random.choice(\n                        len(self.good_policies))]\n                    final_img = self.augmentation_transforms.apply_policy(\n                        epoch_policy,\n                        data,\n                        dset=dset,\n                        image_size=self.image_size)\n                else:\n                    # apply PBA policy)\n                    if isinstance(self.policy[0], list):\n                        # single policy\n                        if self.hparams.flatten:\n                            final_img = self.augmentation_transforms.apply_policy(\n                                self.policy[random.randint(\n                                    0,\n                                    len(self.policy) - 1)],\n                                data,\n                                self.hparams.aug_policy,\n                                dset,\n                                image_size=self.image_size)\n                        else:\n                            final_img = self.augmentation_transforms.apply_policy(\n                                self.policy[iteration],\n                                data,\n                                self.hparams.aug_policy,\n                                dset,\n                                image_size=self.image_size)\n                    elif isinstance(self.policy, list):\n                        # policy schedule\n                        final_img = self.augmentation_transforms.apply_policy(\n                            self.policy,\n                            data,\n                            self.hparams.aug_policy,\n                            dset,\n                            image_size=self.image_size)\n                    else:\n                        raise ValueError(\'Unknown policy.\')\n            else:\n                final_img = data\n            if self.hparams.dataset == \'cifar10\' or self.hparams.dataset == \'cifar100\':\n                final_img = self.augmentation_transforms.random_flip(\n                    self.augmentation_transforms.zero_pad_and_crop(\n                        final_img, 4))\n            elif \'svhn\' in self.hparams.dataset:\n                pass\n            else:\n                tf.logging.log_first_n(tf.logging.WARN, \'Using default random flip and crop.\', 1)\n                final_img = self.augmentation_transforms.random_flip(\n                    self.augmentation_transforms.zero_pad_and_crop(\n                        final_img, 4))\n            # Apply cutout\n            if not self.hparams.no_cutout:\n                if \'cifar10\' == self.hparams.dataset:\n                    final_img = self.augmentation_transforms.cutout_numpy(\n                        final_img, size=16)\n                elif \'cifar100\' == self.hparams.dataset:\n                    final_img = self.augmentation_transforms.cutout_numpy(\n                        final_img, size=16)\n                elif \'svhn\' in self.hparams.dataset:\n                    final_img = self.augmentation_transforms.cutout_numpy(\n                        final_img, size=20)\n                else:\n                    tf.logging.log_first_n(tf.logging.WARN, \'Using default cutout size (16x16).\', 1)\n                    final_img = self.augmentation_transforms.cutout_numpy(\n                        final_img)\n            final_imgs.append(final_img)\n        batched_data = (np.array(final_imgs, np.float32), labels)\n        self.curr_train_index += self.hparams.batch_size\n        return batched_data\n\n    def reset(self):\n        """"""Reset training data and index into the training data.""""""\n        self.epochs = 0\n        # Shuffle the training data\n        perm = np.arange(self.num_train)\n        np.random.shuffle(perm)\n        assert self.num_train == self.train_images.shape[\n            0], \'Error incorrect shuffling mask\'\n        self.train_images = self.train_images[perm]\n        self.train_labels = self.train_labels[perm]\n        self.curr_train_index = 0\n'"
pba/helper_utils.py,7,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helper functions used for training PBA models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom autoaugment.helper_utils import setup_loss, decay_weights, cosine_lr  # pylint: disable=unused-import\n\n\ndef eval_child_model(session, model, data_loader, mode):\n    """"""Evaluates `model` on held out data depending on `mode`.\n\n  Args:\n    session: TensorFlow session the model will be run with.\n    model: TensorFlow model that will be evaluated.\n    data_loader: DataSet object that contains data that `model` will evaluate.\n    mode: Will `model` either evaluate validation or test data.\n\n  Returns:\n    Accuracy of `model` when evaluated on the specified dataset.\n\n  Raises:\n    ValueError: if invalid dataset `mode` is specified.\n  """"""\n    if mode == \'val\':\n        images = data_loader.val_images\n        labels = data_loader.val_labels\n    elif mode == \'test\':\n        images = data_loader.test_images\n        labels = data_loader.test_labels\n    else:\n        raise ValueError(\'Not valid eval mode\')\n    assert len(images) == len(labels)\n    tf.logging.info(\'model.batch_size is {}\'.format(model.batch_size))\n    eval_batches = int(len(images) / model.batch_size)\n    if len(images) % model.batch_size != 0:\n        eval_batches += 1\n    correct = 0\n    count = 0\n    for i in range(eval_batches):\n        eval_images = images[i * model.batch_size:(i + 1) * model.batch_size]\n        eval_labels = labels[i * model.batch_size:(i + 1) * model.batch_size]\n        preds = session.run(\n            model.predictions,\n            feed_dict={\n                model.images: eval_images,\n                model.labels: eval_labels,\n            })\n        correct += np.sum(\n            np.equal(np.argmax(eval_labels, 1), np.argmax(preds, 1)))\n        count += len(preds)\n    assert count == len(images)\n    tf.logging.info(\'correct: {}, total: {}\'.format(correct, count))\n    return correct / count\n\n\ndef step_lr(learning_rate, epoch):\n    """"""Step Learning rate.\n\n  Args:\n    learning_rate: Initial learning rate.\n    epoch: Current epoch we are one. This is one based.\n\n  Returns:\n    The learning rate to be used for this current batch.\n  """"""\n    if epoch < 80:\n        return learning_rate\n    elif epoch < 120:\n        return learning_rate * 0.1\n    else:\n        return learning_rate * 0.01\n\n\ndef get_lr(curr_epoch, hparams, iteration=None):\n    """"""Returns the learning rate during training based on the current epoch.""""""\n    assert iteration is not None\n    batches_per_epoch = int(hparams.train_size / hparams.batch_size)\n    if \'svhn\' in hparams.dataset and \'wrn\' in hparams.model_name:\n        lr = step_lr(hparams.lr, curr_epoch)\n    elif \'cifar\' in hparams.dataset or (\'svhn\' in hparams.dataset and\n                                        \'shake_shake\' in hparams.model_name):\n        lr = cosine_lr(hparams.lr, curr_epoch, iteration, batches_per_epoch,\n                       hparams.num_epochs)\n    else:\n        lr = cosine_lr(hparams.lr, curr_epoch, iteration, batches_per_epoch,\n                       hparams.num_epochs)\n        tf.logging.log_first_n(tf.logging.WARN, \'Default to cosine learning rate.\', 1)\n    return lr\n\n\ndef run_epoch_training(session, model, data_loader, curr_epoch):\n    """"""Runs one epoch of training for the model passed in.\n\n  Args:\n    session: TensorFlow session the model will be run with.\n    model: TensorFlow model that will be evaluated.\n    data_loader: DataSet object that contains data that `model` will evaluate.\n    curr_epoch: How many of epochs of training have been done so far.\n\n  Returns:\n    The accuracy of \'model\' on the training set\n  """"""\n    steps_per_epoch = int(model.hparams.train_size / model.hparams.batch_size)\n    tf.logging.info(\'steps per epoch: {}\'.format(steps_per_epoch))\n    curr_step = session.run(model.global_step)\n    assert curr_step % steps_per_epoch == 0\n\n    # Get the current learning rate for the model based on the current epoch\n    curr_lr = get_lr(curr_epoch, model.hparams, iteration=0)\n    tf.logging.info(\'lr of {} for epoch {}\'.format(curr_lr, curr_epoch))\n\n    correct = 0\n    count = 0\n    for step in range(steps_per_epoch):\n        curr_lr = get_lr(curr_epoch, model.hparams, iteration=(step + 1))\n        # Update the lr rate variable to the current LR.\n        model.lr_rate_ph.load(curr_lr, session=session)\n        if step % 20 == 0:\n            tf.logging.info(\'Training {}/{}\'.format(step, steps_per_epoch))\n\n        train_images, train_labels = data_loader.next_batch(curr_epoch)\n        _, step, preds = session.run(\n            [model.train_op, model.global_step, model.predictions],\n            feed_dict={\n                model.images: train_images,\n                model.labels: train_labels,\n            })\n\n        correct += np.sum(\n            np.equal(np.argmax(train_labels, 1), np.argmax(preds, 1)))\n        count += len(preds)\n    train_accuracy = correct / count\n\n    tf.logging.info(\'Train accuracy: {}\'.format(train_accuracy))\n    return train_accuracy\n'"
pba/model.py,39,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""PBA & AutoAugment Train/Eval module.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport autoaugment.custom_ops as ops\nfrom autoaugment.shake_drop import build_shake_drop_model\nfrom autoaugment.shake_shake import build_shake_shake_model\nimport pba.data_utils as data_utils\nimport pba.helper_utils as helper_utils\nfrom pba.wrn import build_wrn_model\nfrom pba.resnet import build_resnet_model\n\narg_scope = tf.contrib.framework.arg_scope\n\n\ndef setup_arg_scopes(is_training):\n    """"""Sets up the argscopes that will be used when building an image model.\n\n    Args:\n      is_training: Is the model training or not.\n\n    Returns:\n      Arg scopes to be put around the model being constructed.\n    """"""\n\n    batch_norm_decay = 0.9\n    batch_norm_epsilon = 1e-5\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': batch_norm_decay,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': batch_norm_epsilon,\n        \'scale\': True,\n        # collection containing the moving mean and moving variance.\n        \'is_training\': is_training,\n    }\n\n    scopes = []\n\n    scopes.append(arg_scope([ops.batch_norm], **batch_norm_params))\n    return scopes\n\n\ndef build_model(inputs, num_classes, is_training, hparams):\n    """"""Constructs the vision model being trained/evaled.\n\n    Args:\n      inputs: input features/images being fed to the image model build built.\n      num_classes: number of output classes being predicted.\n      is_training: is the model training or not.\n      hparams: additional hyperparameters associated with the image model.\n\n    Returns:\n      The logits of the image model.\n    """"""\n    scopes = setup_arg_scopes(is_training)\n    if len(scopes) != 1:\n        raise ValueError(\'Nested scopes depreciated in py3.\')\n    with scopes[0]:\n        if hparams.model_name == \'pyramid_net\':\n            logits = build_shake_drop_model(inputs, num_classes, is_training)\n        elif hparams.model_name == \'wrn\':\n            logits = build_wrn_model(inputs, num_classes, hparams.wrn_size)\n        elif hparams.model_name == \'shake_shake\':\n            logits = build_shake_shake_model(inputs, num_classes, hparams,\n                                             is_training)\n        elif hparams.model_name == \'resnet\':\n            logits = build_resnet_model(inputs, num_classes, hparams,\n                                        is_training)\n        else:\n            raise ValueError(""Unknown model name."")\n    return logits\n\n\nclass Model(object):\n    """"""Builds an model.""""""\n\n    def __init__(self, hparams, num_classes, image_size):\n        self.hparams = hparams\n        self.num_classes = num_classes\n        self.image_size = image_size\n\n    def build(self, mode):\n        """"""Construct the model.""""""\n        assert mode in [\'train\', \'eval\']\n        self.mode = mode\n        self._setup_misc(mode)\n        self._setup_images_and_labels(self.hparams.dataset)\n        self._build_graph(self.images, self.labels, mode)\n\n        self.init = tf.group(tf.global_variables_initializer(),\n                             tf.local_variables_initializer())\n\n    def _setup_misc(self, mode):\n        """"""Sets up miscellaneous in the model constructor.""""""\n        self.lr_rate_ph = tf.Variable(0.0, name=\'lrn_rate\', trainable=False)\n        self.reuse = None if (mode == \'train\') else True\n        self.batch_size = self.hparams.batch_size\n        if mode == \'eval\':\n            self.batch_size = self.hparams.test_batch_size\n\n    def _setup_images_and_labels(self, dataset):\n        """"""Sets up image and label placeholders for the model.""""""\n        if dataset == \'cifar10\' or dataset == \'cifar100\' or self.mode == \'train\':\n            self.images = tf.placeholder(tf.float32,\n                                         [self.batch_size, self.image_size, self.image_size, 3])\n            self.labels = tf.placeholder(tf.float32,\n                                         [self.batch_size, self.num_classes])\n        else:\n            self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3])\n            self.labels = tf.placeholder(tf.float32, [None, self.num_classes])\n\n    def assign_epoch(self, session, epoch_value):\n        session.run(\n            self._epoch_update, feed_dict={self._new_epoch: epoch_value})\n\n    def _build_graph(self, images, labels, mode):\n        """"""Constructs the TF graph for the model.\n\n        Args:\n          images: A 4-D image Tensor\n          labels: A 2-D labels Tensor.\n          mode: string indicating training mode ( e.g., \'train\', \'valid\', \'test\').\n        """"""\n        is_training = \'train\' in mode\n        if is_training:\n            self.global_step = tf.train.get_or_create_global_step()\n\n        logits = build_model(images, self.num_classes, is_training,\n                             self.hparams)\n        self.predictions, self.cost = helper_utils.setup_loss(logits, labels)\n\n        self._calc_num_trainable_params()\n\n        # Adds L2 weight decay to the cost\n        self.cost = helper_utils.decay_weights(self.cost,\n                                               self.hparams.weight_decay_rate)\n\n        if is_training:\n            self._build_train_op()\n\n        # Setup checkpointing for this child model\n        # Keep 2 or more checkpoints around during training.\n        with tf.device(\'/cpu:0\'):\n            self.saver = tf.train.Saver(max_to_keep=10)\n\n        self.init = tf.group(tf.global_variables_initializer(),\n                             tf.local_variables_initializer())\n\n    def _calc_num_trainable_params(self):\n        self.num_trainable_params = np.sum([\n            np.prod(var.get_shape().as_list())\n            for var in tf.trainable_variables()\n        ])\n        tf.logging.info(\'number of trainable params: {}\'.format(\n            self.num_trainable_params))\n\n    def _build_train_op(self):\n        """"""Builds the train op for the model.""""""\n        hparams = self.hparams\n        tvars = tf.trainable_variables()\n        grads = tf.gradients(self.cost, tvars)\n        if hparams.gradient_clipping_by_global_norm > 0.0:\n            grads, norm = tf.clip_by_global_norm(\n                grads, hparams.gradient_clipping_by_global_norm)\n            tf.summary.scalar(\'grad_norm\', norm)\n\n        # Setup the initial learning rate\n        initial_lr = self.lr_rate_ph\n        optimizer = tf.train.MomentumOptimizer(\n            initial_lr, 0.9, use_nesterov=True)\n\n        self.optimizer = optimizer\n        apply_op = optimizer.apply_gradients(\n            zip(grads, tvars), global_step=self.global_step, name=\'train_step\')\n        train_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies([apply_op]):\n            self.train_op = tf.group(*train_ops)\n\n\nclass ModelTrainer(object):\n    """"""Trains an instance of the Model class.""""""\n\n    def __init__(self, hparams):\n        self._session = None\n        self.hparams = hparams\n\n        # Set the random seed to be sure the same validation set\n        # is used for each model\n        np.random.seed(0)\n        self.data_loader = data_utils.DataSet(hparams)\n        np.random.seed()  # Put the random seed back to random\n        self.data_loader.reset()\n\n        # extra stuff for ray\n        self._build_models()\n        self._new_session()\n        self._session.__enter__()\n\n    def save_model(self, checkpoint_dir, step=None):\n        """"""Dumps model into the backup_dir.\n\n        Args:\n          step: If provided, creates a checkpoint with the given step\n            number, instead of overwriting the existing checkpoints.\n        """"""\n        model_save_name = os.path.join(checkpoint_dir,\n                                       \'model.ckpt\') + \'-\' + str(step)\n        save_path = self.saver.save(self.session, model_save_name)\n        tf.logging.info(\'Saved child model\')\n        return model_save_name\n\n    def extract_model_spec(self, checkpoint_path):\n        """"""Loads a checkpoint with the architecture structure stored in the name.""""""\n        self.saver.restore(self.session, checkpoint_path)\n        tf.logging.warning(\n            \'Loaded child model checkpoint from {}\'.format(checkpoint_path))\n\n    def eval_child_model(self, model, data_loader, mode):\n        """"""Evaluate the child model.\n\n        Args:\n          model: image model that will be evaluated.\n          data_loader: dataset object to extract eval data from.\n          mode: will the model be evalled on train, val or test.\n\n        Returns:\n          Accuracy of the model on the specified dataset.\n        """"""\n        tf.logging.info(\'Evaluating child model in mode {}\'.format(mode))\n        while True:\n            try:\n                accuracy = helper_utils.eval_child_model(\n                    self.session, model, data_loader, mode)\n                tf.logging.info(\n                    \'Eval child model accuracy: {}\'.format(accuracy))\n                # If epoch trained without raising the below errors, break\n                # from loop.\n                break\n            except (tf.errors.AbortedError, tf.errors.UnavailableError) as e:\n                tf.logging.info(\n                    \'Retryable error caught: {}.  Retrying.\'.format(e))\n\n        return accuracy\n\n    @contextlib.contextmanager\n    def _new_session(self):\n        """"""Creates a new session for model m.""""""\n        # Create a new session for this model, initialize\n        # variables, and save / restore from checkpoint.\n        sess_cfg = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        sess_cfg.gpu_options.allow_growth = True\n        self._session = tf.Session(\'\', config=sess_cfg)\n        self._session.run([self.m.init, self.meval.init])\n        return self._session\n\n    def _build_models(self):\n        """"""Builds the image models for train and eval.""""""\n        # Determine if we should build the train and eval model. When using\n        # distributed training we only want to build one or the other and not both.\n        with tf.variable_scope(\'model\', use_resource=False):\n            m = Model(self.hparams, self.data_loader.num_classes, self.data_loader.image_size)\n            m.build(\'train\')\n            self._num_trainable_params = m.num_trainable_params\n            self._saver = m.saver\n        with tf.variable_scope(\'model\', reuse=True, use_resource=False):\n            meval = Model(self.hparams, self.data_loader.num_classes, self.data_loader.image_size)\n            meval.build(\'eval\')\n        self.m = m\n        self.meval = meval\n\n    def _run_training_loop(self, curr_epoch):\n        """"""Trains the model `m` for one epoch.""""""\n        start_time = time.time()\n        while True:\n            try:\n                train_accuracy = helper_utils.run_epoch_training(\n                    self.session, self.m, self.data_loader, curr_epoch)\n                break\n            except (tf.errors.AbortedError, tf.errors.UnavailableError) as e:\n                tf.logging.info(\n                    \'Retryable error caught: {}.  Retrying.\'.format(e))\n        tf.logging.info(\'Finished epoch: {}\'.format(curr_epoch))\n        tf.logging.info(\'Epoch time(min): {}\'.format(\n            (time.time() - start_time) / 60.0))\n        return train_accuracy\n\n    def _compute_final_accuracies(self, iteration):\n        """"""Run once training is finished to compute final test accuracy.""""""\n        if (iteration >= self.hparams.num_epochs - 1):\n            test_accuracy = self.eval_child_model(self.meval, self.data_loader,\n                                                  \'test\')\n        else:\n            test_accuracy = 0\n        tf.logging.info(\'Test Accuracy: {}\'.format(test_accuracy))\n        return test_accuracy\n\n    def run_model(self, epoch):\n        """"""Trains and evalutes the image model.""""""\n        valid_accuracy = 0.\n        training_accuracy = self._run_training_loop(epoch)\n        if self.hparams.validation_size > 0:\n            valid_accuracy = self.eval_child_model(self.meval,\n                                                   self.data_loader, \'val\')\n        tf.logging.info(\'Train Acc: {}, Valid Acc: {}\'.format(\n            training_accuracy, valid_accuracy))\n        return training_accuracy, valid_accuracy\n\n    def reset_config(self, new_hparams):\n        self.hparams = new_hparams\n        self.data_loader.reset_policy(new_hparams)\n        return\n\n    @property\n    def saver(self):\n        return self._saver\n\n    @property\n    def session(self):\n        return self._session\n\n    @property\n    def num_trainable_params(self):\n        return self._num_trainable_params\n'"
pba/policies.py,0,"b'# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""AutoAugment augmentation policies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom autoaugment.policies import good_policies  # pylint: disable=unused-import\n\n\ndef good_policies_svhn():\n    return [[(\'ShearX\', 0.9, 4), (\'Invert\', 0.2, 3)], [(\'ShearY\', 0.9, 8),\n                                                       (\'Invert\', 0.7, 5)],\n            [(\'Equalize\', 0.6, 5),\n             (\'Solarize\', 0.6, 6)], [(\'Invert\', 0.9, 3), (\'Equalize\', 0.6, 3)],\n            [(\'Equalize\', 0.6, 1), (\'Rotate\', 0.9, 3)], [\n                (\'ShearX\', 0.9, 4), (\'AutoContrast\', 0.8, 3)\n            ], [(\'ShearY\', 0.9, 8), (\'Invert\', 0.4, 5)], [\n                (\'ShearY\', 0.9, 5), (\'Solarize\', 0.2, 6)\n            ], [(\'Invert\', 0.9, 6), (\'AutoContrast\', 0.8, 1)],\n            [(\'Equalize\', 0.6, 3),\n             (\'Rotate\', 0.9, 3)], [(\'ShearX\', 0.9, 4), (\'Solarize\', 0.3, 3)], [\n                 (\'ShearY\', 0.8, 8), (\'Invert\', 0.7, 4)\n             ], [(\'Equalize\', 0.9, 5), (\'TranslateY\', 0.6, 6)], [\n                 (\'Invert\', 0.9, 4), (\'Equalize\', 0.6, 7)\n             ], [(\'Contrast\', 0.3, 3), (\'Rotate\', 0.8, 4)], [\n                 (\'Invert\', 0.8, 5), (\'TranslateY\', 0.0, 2)\n             ], [(\'ShearY\', 0.7, 6), (\'Solarize\', 0.4, 8)], [\n                 (\'Invert\', 0.6, 4), (\'Rotate\', 0.8, 4)\n             ], [(\'ShearY\', 0.3, 7), (\'TranslateX\', 0.9, 3)], [\n                 (\'ShearX\', 0.1, 6), (\'Invert\', 0.6, 5)\n             ], [(\'Solarize\', 0.7, 2), (\'TranslateY\', 0.6, 7)], [\n                 (\'ShearY\', 0.8, 4), (\'Invert\', 0.8, 8)\n             ], [(\'ShearX\', 0.7, 9), (\'TranslateY\', 0.8, 3)], [\n                 (\'ShearY\', 0.8, 5), (\'AutoContrast\', 0.7, 3)\n             ], [(\'ShearX\', 0.7, 2), (\'Invert\', 0.1, 5)]]\n'"
pba/resnet.py,29,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for Residual Networks.\n\nResidual networks (\'v1\' ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant was introduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer\nrather than after.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_BATCH_NORM_DECAY = 0.997\n_BATCH_NORM_EPSILON = 1e-5\nDEFAULT_VERSION = 2\nDEFAULT_DTYPE = tf.float32\nCASTABLE_TYPES = (tf.float16, )\nALLOWED_TYPES = (DEFAULT_DTYPE, ) + CASTABLE_TYPES\n\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format):\n    """"""Performs a batch normalization using a standard set of parameters.""""""\n    # We set fused=True for a significant performance boost. See\n    # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n    return tf.layers.batch_normalization(\n        inputs=inputs,\n        axis=1 if data_format == \'channels_first\' else 3,\n        momentum=_BATCH_NORM_DECAY,\n        epsilon=_BATCH_NORM_EPSILON,\n        center=True,\n        scale=True,\n        training=training,\n        fused=True)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or [batch,\n      height_in, width_in, channels] depending on data_format.\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n      Should be a positive integer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    A tensor with the same format as the input with the data either intact\n    (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == \'channels_first\':\n        padded_inputs = tf.pad(\n            inputs, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(\n            inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n    """"""Strided 2-D convolution with explicit padding.""""""\n    # The padding is consistent and is based only on `kernel_size`, not on the\n    # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=(\'SAME\' if strides == 1 else \'VALID\'),\n        use_bias=False,\n        kernel_initializer=tf.variance_scaling_initializer(),\n        data_format=data_format)\n\n\n################################################################################\n# ResNet block definitions.\n################################################################################\ndef _building_block_v1(inputs, filters, training, projection_shortcut, strides,\n                       data_format):\n    """"""A single block for ResNet v1, without a bottleneck.\n\n  Convolution then batch normalization then ReLU as described by:\n    Deep Residual Learning for Image Recognition\n    https://arxiv.org/pdf/1512.03385.pdf\n    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or [batch,\n      height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference mode.\n      Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts (typically\n      a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n    shortcut = inputs\n\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n        shortcut = batch_norm(\n            inputs=shortcut, training=training, data_format=data_format)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=3,\n        strides=strides,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=3,\n        strides=1,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs += shortcut\n    inputs = tf.nn.relu(inputs)\n\n    return inputs\n\n\ndef _building_block_v2(inputs, filters, training, projection_shortcut, strides,\n                       data_format):\n    """"""A single block for ResNet v2, without a bottleneck.\n\n  Batch normalization then ReLu then convolution as described by:\n    Identity Mappings in Deep Residual Networks\n    https://arxiv.org/pdf/1603.05027.pdf\n    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or [batch,\n      height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference mode.\n      Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts (typically\n      a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n    shortcut = inputs\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    # The projection shortcut should come after the first batch norm and ReLU\n    # since it performs a 1x1 convolution.\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=3,\n        strides=strides,\n        data_format=data_format)\n\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=3,\n        strides=1,\n        data_format=data_format)\n\n    return inputs + shortcut\n\n\ndef _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n    """"""A single block for ResNet v1, with a bottleneck.\n\n  Similar to _building_block_v1(), except using the ""bottleneck"" blocks\n  described in:\n    Convolution then batch normalization then ReLU as described by:\n      Deep Residual Learning for Image Recognition\n      https://arxiv.org/pdf/1512.03385.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or [batch,\n      height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference mode.\n      Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts (typically\n      a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n    shortcut = inputs\n\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n        shortcut = batch_norm(\n            inputs=shortcut, training=training, data_format=data_format)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=1,\n        strides=1,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=3,\n        strides=strides,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=4 * filters,\n        kernel_size=1,\n        strides=1,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training, data_format)\n    inputs += shortcut\n    inputs = tf.nn.relu(inputs)\n\n    return inputs\n\n\ndef _bottleneck_block_v2(inputs, filters, training, projection_shortcut,\n                         strides, data_format):\n    """"""A single block for ResNet v2, with a bottleneck.\n\n  Similar to _building_block_v2(), except using the ""bottleneck"" blocks\n  described in:\n    Convolution then batch normalization then ReLU as described by:\n      Deep Residual Learning for Image Recognition\n      https://arxiv.org/pdf/1512.03385.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n\n  Adapted to the ordering conventions of:\n    Batch normalization then ReLu then convolution as described by:\n      Identity Mappings in Deep Residual Networks\n      https://arxiv.org/pdf/1603.05027.pdf\n      by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or [batch,\n      height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the convolutions.\n    training: A Boolean for whether the model is in training or inference mode.\n      Needed for batch normalization.\n    projection_shortcut: The function to use for projection shortcuts (typically\n      a 1x1 convolution when downsampling the input).\n    strides: The block\'s stride. If greater than 1, this block will ultimately\n      downsample the input.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block; shape should match inputs.\n  """"""\n    shortcut = inputs\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n\n    # The projection shortcut should come after the first batch norm and ReLU\n    # since it performs a 1x1 convolution.\n    if projection_shortcut is not None:\n        shortcut = projection_shortcut(inputs)\n\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=1,\n        strides=1,\n        data_format=data_format)\n\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=3,\n        strides=strides,\n        data_format=data_format)\n\n    inputs = batch_norm(inputs, training, data_format)\n    inputs = tf.nn.relu(inputs)\n    inputs = conv2d_fixed_padding(\n        inputs=inputs,\n        filters=4 * filters,\n        kernel_size=1,\n        strides=1,\n        data_format=data_format)\n\n    return inputs + shortcut\n\n\ndef block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,\n                training, name, data_format):\n    """"""Creates one layer of blocks for the ResNet model.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or [batch,\n      height_in, width_in, channels] depending on data_format.\n    filters: The number of filters for the first convolution of the layer.\n    bottleneck: Is the block created a bottleneck block.\n    block_fn: The block to use within the model, either `building_block` or\n      `bottleneck_block`.\n    blocks: The number of blocks contained in the layer.\n    strides: The stride to use for the first convolution of the layer. If\n      greater than 1, this layer will ultimately downsample the input.\n    training: Either True or False, whether we are currently training the model.\n      Needed for batch norm.\n    name: A string name for the tensor output of the block layer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    The output tensor of the block layer.\n  """"""\n\n    # Bottleneck blocks end with 4x the number of filters as they start with\n    filters_out = filters * 4 if bottleneck else filters\n\n    def projection_shortcut(inputs):\n        return conv2d_fixed_padding(\n            inputs=inputs,\n            filters=filters_out,\n            kernel_size=1,\n            strides=strides,\n            data_format=data_format)\n\n    # Only the first block per block_layer uses projection_shortcut and strides\n    inputs = block_fn(inputs, filters, training, projection_shortcut, strides,\n                      data_format)\n\n    for _ in range(1, blocks):\n        inputs = block_fn(inputs, filters, training, None, 1, data_format)\n\n    return tf.identity(inputs, name)\n\n\ndef build_resnet_model(inputs, num_classes, hparams, training):\n    """"""Creates a model for classifying an image.\n\n  Args:\n    inputs: Input tensor.\n    num_classes: The number of classes used as labels.\n    hparams: tf.hparam object.\n    training: Training or evaluation.\n\n  Raises:\n    ValueError: if invalid version is selected.\n\n  Returns:\n    Output tensor.\n  """"""\n    resnet_size = hparams.resnet_size\n    if resnet_size % 6 != 2:\n        raise ValueError(\'resnet_size must be 6n + 2:\', resnet_size)\n\n    # Kernel size is 3 and conv stride is 1.\n    bottleneck = False\n    num_filters = hparams.num_filters\n    first_pool_size = None\n    first_pool_stride = None\n    num_blocks = (resnet_size - 2) // 6\n    block_sizes = [num_blocks * 3]\n    block_strides = [1, 2, 2]\n    data_format = \'channels_last\'\n    resnet_version = DEFAULT_VERSION\n    dtype = DEFAULT_DTYPE\n    if resnet_version not in (1, 2):\n        raise ValueError(\n            \'Resnet version should be 1 or 2. See README for citations.\')\n    if bottleneck:\n        if resnet_version == 1:\n            block_fn = _bottleneck_block_v1\n        else:\n            block_fn = _bottleneck_block_v2\n    else:\n        if resnet_version == 1:\n            block_fn = _building_block_v1\n        else:\n            block_fn = _building_block_v2\n\n    if dtype not in ALLOWED_TYPES:\n        raise ValueError(\'dtype must be one of: {}\'.format(ALLOWED_TYPES))\n\n    pre_activation = resnet_version == 2\n\n    # We do not include batch normalization or activation functions in V2\n    # for the initial conv1 because the first ResNet unit will perform these\n    # for both the shortcut and non-shortcut paths as part of the first\n    # block\'s projection. Cf. Appendix of [2].\n    if resnet_version == 1:\n        inputs = batch_norm(inputs, training, data_format)\n        inputs = tf.nn.relu(inputs)\n\n    if first_pool_size:\n        inputs = tf.layers.max_pooling2d(\n            inputs=inputs,\n            pool_size=first_pool_size,\n            strides=first_pool_stride,\n            padding=\'SAME\',\n            data_format=data_format)\n        inputs = tf.identity(inputs, \'initial_max_pool\')\n\n    for i, num_blocks in enumerate(block_sizes):\n        num_filters = num_filters * (2**i)\n        inputs = block_layer(\n            inputs=inputs,\n            filters=num_filters,\n            bottleneck=bottleneck,\n            block_fn=block_fn,\n            blocks=num_blocks,\n            strides=block_strides[i],\n            training=training,\n            name=\'block_layer{}\'.format(i + 1),\n            data_format=data_format)\n\n    # Only apply the BN and ReLU for model that does pre_activation in each\n    # building/bottleneck block, eg resnet V2.\n    if pre_activation:\n        inputs = batch_norm(inputs, training, data_format)\n        inputs = tf.nn.relu(inputs)\n\n    # The current top layer has shape\n    # `batch_size x pool_size x pool_size x final_size`.\n    # ResNet does an Average Pooling layer over pool_size,\n    # but that is the same as doing a reduce_mean. We do a reduce_mean\n    # here because it performs better than AveragePooling2D.\n    axes = [2, 3] if data_format == \'channels_first\' else [1, 2]\n    inputs = tf.reduce_mean(inputs, axes, keepdims=True)\n    inputs = tf.identity(inputs, \'final_reduce_mean\')\n\n    inputs = tf.squeeze(inputs, axes)\n    inputs = tf.layers.dense(inputs=inputs, units=num_classes)\n    inputs = tf.identity(inputs, \'final_dense\')\n    return inputs\n'"
pba/search.py,2,"b'""""""Run PBA Search.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nimport numpy as np\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.schedulers import PopulationBasedTraining\nimport tensorflow as tf\n\nfrom pba.setup import create_hparams\nfrom pba.setup import create_parser\nfrom pba.train import RayModel\n\n\ndef main(_):\n    FLAGS = create_parser(""search"")  # pylint: disable=invalid-name\n    hparams = create_hparams(""search"", FLAGS)\n    hparams_config = hparams.values()\n\n    train_spec = {\n        ""run"": RayModel,\n        ""resources_per_trial"": {\n            ""cpu"": FLAGS.cpu,\n            ""gpu"": FLAGS.gpu\n        },\n        ""stop"": {\n            ""training_iteration"": hparams.num_epochs,\n        },\n        ""config"": hparams_config,\n        ""local_dir"": FLAGS.local_dir,\n        ""checkpoint_freq"": FLAGS.checkpoint_freq,\n        ""num_samples"": FLAGS.num_samples\n    }\n\n    if FLAGS.restore:\n        train_spec[""restore""] = FLAGS.restore\n\n    def explore(config):\n        """"""Custom explore function.\n\n    Args:\n      config: dictionary containing ray config params.\n\n    Returns:\n      Copy of config with modified augmentation policy.\n    """"""\n        new_params = []\n        if config[""explore""] == ""cifar10"":\n            for i, param in enumerate(config[""hp_policy""]):\n                if random.random() < 0.2:\n                    if i % 2 == 0:\n                        new_params.append(random.randint(0, 10))\n                    else:\n                        new_params.append(random.randint(0, 9))\n                else:\n                    amt = np.random.choice(\n                        [0, 1, 2, 3], p=[0.25, 0.25, 0.25, 0.25])\n                    # Cast np.int64 to int for py3 json\n                    amt = int(amt)\n                    if random.random() < 0.5:\n                        new_params.append(max(0, param - amt))\n                    else:\n                        if i % 2 == 0:\n                            new_params.append(min(10, param + amt))\n                        else:\n                            new_params.append(min(9, param + amt))\n        else:\n            raise ValueError()\n        config[""hp_policy""] = new_params\n        return config\n\n    ray.init()\n\n    pbt = PopulationBasedTraining(\n        time_attr=""training_iteration"",\n        reward_attr=""val_acc"",\n        perturbation_interval=FLAGS.perturbation_interval,\n        custom_explore_fn=explore,\n        log_config=True)\n\n    run_experiments(\n        {\n            FLAGS.name: train_spec\n        },\n        scheduler=pbt,\n        reuse_actors=True,\n        verbose=True)\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
pba/setup.py,7,"b'""""""Parse flags and set up hyperparameters.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport random\nimport tensorflow as tf\n\nfrom pba.augmentation_transforms_hp import NUM_HP_TRANSFORM\n\n\ndef create_parser(state):\n    """"""Create arg parser for flags.""""""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--model_name\',\n        default=\'wrn\',\n        choices=(\'wrn_28_10\', \'wrn_40_2\', \'shake_shake_32\', \'shake_shake_96\',\n                 \'shake_shake_112\', \'pyramid_net\', \'resnet\'))\n    parser.add_argument(\n        \'--data_path\',\n        default=\'/tmp/datasets/\',\n        help=\'Directory where dataset is located.\')\n    parser.add_argument(\n        \'--dataset\',\n        default=\'cifar10\',\n        choices=(\'cifar10\', \'cifar100\', \'svhn\', \'svhn-full\', \'test\'))\n    parser.add_argument(\n        \'--recompute_dset_stats\',\n        action=\'store_true\',\n        help=\'Instead of using hardcoded mean/std, recompute from dataset.\')\n    parser.add_argument(\'--local_dir\', type=str, default=\'/tmp/ray_results/\',  help=\'Ray directory.\')\n    parser.add_argument(\'--restore\', type=str, default=None, help=\'If specified, tries to restore from given path.\')\n    parser.add_argument(\'--train_size\', type=int, default=5000, help=\'Number of training examples.\')\n    parser.add_argument(\'--val_size\', type=int, default=45000, help=\'Number of validation examples.\')\n    parser.add_argument(\'--checkpoint_freq\', type=int, default=50, help=\'Checkpoint frequency.\')\n    parser.add_argument(\n        \'--cpu\', type=float, default=4, help=\'Allocated by Ray\')\n    parser.add_argument(\n        \'--gpu\', type=float, default=1, help=\'Allocated by Ray\')\n    parser.add_argument(\n        \'--aug_policy\',\n        type=str,\n        default=\'cifar10\',\n        help=\n        \'which augmentation policy to use (in augmentation_transforms_hp.py)\')\n    # search-use only\n    parser.add_argument(\n        \'--explore\',\n        type=str,\n        default=\'cifar10\',\n        help=\'which explore function to use\')\n    parser.add_argument(\n        \'--epochs\',\n        type=int,\n        default=0,\n        help=\'Number of epochs, or <=0 for default\')\n    parser.add_argument(\n        \'--no_cutout\', action=\'store_true\', help=\'turn off cutout\')\n    parser.add_argument(\'--lr\', type=float, default=0.1, help=\'learning rate\')\n    parser.add_argument(\'--wd\', type=float, default=0.0005, help=\'weight decay\')\n    parser.add_argument(\'--bs\', type=int, default=128, help=\'batch size\')\n    parser.add_argument(\'--test_bs\', type=int, default=25, help=\'test batch size\')\n    parser.add_argument(\'--num_samples\', type=int, default=1, help=\'Number of Ray samples\')\n\n    if state == \'train\':\n        parser.add_argument(\n            \'--use_hp_policy\',\n            action=\'store_true\',\n            help=\'otherwise use autoaug policy\')\n        parser.add_argument(\n            \'--hp_policy\',\n            type=str,\n            default=None,\n            help=\'either a comma separated list of values or a file\')\n        parser.add_argument(\n            \'--hp_policy_epochs\',\n            type=int,\n            default=200,\n            help=\'number of epochs/iterations policy trained for\')\n        parser.add_argument(\n            \'--no_aug\',\n            action=\'store_true\',\n            help=\n            \'no additional augmentation at all (besides cutout if not toggled)\'\n        )\n        parser.add_argument(\n            \'--flatten\',\n            action=\'store_true\',\n            help=\'randomly select aug policy from schedule\')\n        parser.add_argument(\'--name\', type=str, default=\'autoaug\')\n\n    elif state == \'search\':\n        parser.add_argument(\'--perturbation_interval\', type=int, default=10)\n        parser.add_argument(\'--name\', type=str, default=\'autoaug_pbt\')\n    else:\n        raise ValueError(\'unknown state\')\n    args = parser.parse_args()\n    tf.logging.info(str(args))\n    return args\n\n\ndef create_hparams(state, FLAGS):  # pylint: disable=invalid-name\n    """"""Creates hyperparameters to pass into Ray config.\n\n  Different options depending on search or eval mode.\n\n  Args:\n    state: a string, \'train\' or \'search\'.\n    FLAGS: parsed command line flags.\n\n  Returns:\n    tf.hparams object.\n  """"""\n    epochs = 0\n    tf.logging.info(\'data path: {}\'.format(FLAGS.data_path))\n    hparams = tf.contrib.training.HParams(\n        train_size=FLAGS.train_size,\n        validation_size=FLAGS.val_size,\n        dataset=FLAGS.dataset,\n        data_path=FLAGS.data_path,\n        batch_size=FLAGS.bs,\n        gradient_clipping_by_global_norm=5.0,\n        explore=FLAGS.explore,\n        aug_policy=FLAGS.aug_policy,\n        no_cutout=FLAGS.no_cutout,\n        recompute_dset_stats=FLAGS.recompute_dset_stats,\n        lr=FLAGS.lr,\n        weight_decay_rate=FLAGS.wd,\n        test_batch_size=FLAGS.test_bs)\n\n    if state == \'train\':\n        hparams.add_hparam(\'no_aug\', FLAGS.no_aug)\n        hparams.add_hparam(\'use_hp_policy\', FLAGS.use_hp_policy)\n        if FLAGS.use_hp_policy:\n            if FLAGS.hp_policy == \'random\':\n                tf.logging.info(\'RANDOM SEARCH\')\n                parsed_policy = []\n                for i in range(NUM_HP_TRANSFORM * 4):\n                    if i % 2 == 0:\n                        parsed_policy.append(random.randint(0, 10))\n                    else:\n                        parsed_policy.append(random.randint(0, 9))\n            elif FLAGS.hp_policy.endswith(\'.txt\') or FLAGS.hp_policy.endswith(\n                    \'.p\'):\n                # will be loaded in in data_utils\n                parsed_policy = FLAGS.hp_policy\n            else:\n                # parse input into a fixed augmentation policy\n                parsed_policy = FLAGS.hp_policy.split(\', \')\n                parsed_policy = [int(p) for p in parsed_policy]\n            hparams.add_hparam(\'hp_policy\', parsed_policy)\n            hparams.add_hparam(\'hp_policy_epochs\', FLAGS.hp_policy_epochs)\n            hparams.add_hparam(\'flatten\', FLAGS.flatten)\n    elif state == \'search\':\n        hparams.add_hparam(\'no_aug\', False)\n        hparams.add_hparam(\'use_hp_policy\', True)\n        # default start value of 0\n        hparams.add_hparam(\'hp_policy\',\n                           [0 for _ in range(4 * NUM_HP_TRANSFORM)])\n    else:\n        raise ValueError(\'unknown state\')\n\n    if FLAGS.model_name == \'wrn_40_2\':\n        hparams.add_hparam(\'model_name\', \'wrn\')\n        epochs = 200\n        hparams.add_hparam(\'wrn_size\', 32)\n        hparams.add_hparam(\'wrn_depth\', 40)\n    elif FLAGS.model_name == \'wrn_28_10\':\n        hparams.add_hparam(\'model_name\', \'wrn\')\n        epochs = 200\n        hparams.add_hparam(\'wrn_size\', 160)\n        hparams.add_hparam(\'wrn_depth\', 28)\n    elif FLAGS.model_name == \'resnet\':\n        hparams.add_hparam(\'model_name\', \'resnet\')\n        epochs = 200\n        hparams.add_hparam(\'resnet_size\', 20)\n        hparams.add_hparam(\'num_filters\', 32)\n    elif FLAGS.model_name == \'shake_shake_32\':\n        hparams.add_hparam(\'model_name\', \'shake_shake\')\n        epochs = 1800\n        hparams.add_hparam(\'shake_shake_widen_factor\', 2)\n    elif FLAGS.model_name == \'shake_shake_96\':\n        hparams.add_hparam(\'model_name\', \'shake_shake\')\n        epochs = 1800\n        hparams.add_hparam(\'shake_shake_widen_factor\', 6)\n    elif FLAGS.model_name == \'shake_shake_112\':\n        hparams.add_hparam(\'model_name\', \'shake_shake\')\n        epochs = 1800\n        hparams.add_hparam(\'shake_shake_widen_factor\', 7)\n    elif FLAGS.model_name == \'pyramid_net\':\n        hparams.add_hparam(\'model_name\', \'pyramid_net\')\n        epochs = 1800\n        hparams.set_hparam(\'batch_size\', 64)\n    else:\n        raise ValueError(\'Not Valid Model Name: %s\' % FLAGS.model_name)\n    if FLAGS.epochs > 0:\n        tf.logging.info(\'overwriting with custom epochs\')\n        epochs = FLAGS.epochs\n    hparams.add_hparam(\'num_epochs\', epochs)\n    tf.logging.info(\'epochs: {}, lr: {}, wd: {}\'.format(\n        hparams.num_epochs, hparams.lr, hparams.weight_decay_rate))\n    return hparams\n'"
pba/train.py,9,"b'""""""Train and evaluate models using augmentation schedules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune import Trainable\nimport tensorflow as tf\n\nfrom pba.model import ModelTrainer\nfrom pba.setup import create_hparams\nfrom pba.setup import create_parser\n\n\nclass RayModel(Trainable):\n    """"""A Ray wrapper for Models to run search.""""""\n\n    def _setup(self, *args):\n        tf.logging.set_verbosity(tf.logging.INFO)\n        tf.logging.info(""calling setup"")\n        self.hparams = tf.contrib.training.HParams(**self.config)\n        self.trainer = ModelTrainer(self.hparams)\n\n    def _train(self):\n        """"""Runs one epoch of training, and returns current epoch accuracies.""""""\n        tf.logging.info(""training for iteration: {}"".format(self._iteration))\n        train_acc, val_acc = self.trainer.run_model(self._iteration)\n        test_acc = self.trainer._compute_final_accuracies(self._iteration)  # pylint: disable=protected-access\n        return {\n            ""val_acc"": val_acc,\n            ""train_acc"": train_acc,\n            ""test_acc"": test_acc\n        }\n\n    def _save(self, checkpoint_dir):\n        """"""Uses tf trainer object to checkpoint.""""""\n        save_name = self.trainer.save_model(checkpoint_dir, self._iteration)\n        tf.logging.info(""saved model {}"".format(save_name))\n        os.close(os.open(save_name, os.O_CREAT))\n        return save_name\n\n    def _restore(self, checkpoint):\n        """"""Restores model from checkpoint.""""""\n        tf.logging.info(""RESTORING: {}"".format(checkpoint))\n        self.trainer.extract_model_spec(checkpoint)\n\n    def reset_config(self, new_config):\n        """"""Resets trainer config for fast PBT implementation.""""""\n        self.config = new_config\n        self.hparams = tf.contrib.training.HParams(**new_config)\n        self.trainer.reset_config(self.hparams)\n        return True\n\n\ndef main(_):\n    FLAGS = create_parser(""train"")  # pylint: disable=invalid-name\n    hparams = create_hparams(""train"", FLAGS)\n\n    train_spec = {\n        ""run"": RayModel,\n        ""resources_per_trial"": {\n            ""cpu"": FLAGS.cpu,\n            ""gpu"": FLAGS.gpu\n        },\n        ""stop"": {\n            ""training_iteration"": hparams.num_epochs,\n        },\n        ""config"": hparams.values(),\n        ""local_dir"": FLAGS.local_dir,\n        ""checkpoint_freq"": FLAGS.checkpoint_freq,\n        ""num_samples"": FLAGS.num_samples\n    }\n\n    if FLAGS.restore:\n        train_spec[""restore""] = FLAGS.restore\n\n    ray.init()\n    run_experiments({FLAGS.name: train_spec})\n\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n'"
pba/utils.py,3,"b'""""""Utils for parsing PBA augmentation schedules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport ast\nimport collections\nimport tensorflow as tf\nimport json\n\nPbtUpdate = collections.namedtuple(\'PbtUpdate\', [\n    \'target_trial_name\', \'clone_trial_name\', \'target_trial_epochs\',\n    \'clone_trial_epochs\', \'old_config\', \'new_config\'\n])\n\n\ndef parse_log(file_path, epochs):\n    """"""Parses augmentation policy schedule from log file.\n\n  Args:\n    file_path: Path to policy generated by running search.py.\n    epochs: The number of epochs search was run for.\n\n  Returns:\n    A list containing the parsed policy of the form: [start epoch, start_epoch_clone, policy], where each element is a tuple of (num_epochs, policy list).\n  """"""\n    raw_policy_file = open(file_path, ""r"").readlines()\n    raw_policy = []\n    for line in raw_policy_file:\n        try:\n            raw_policy_line = json.loads(line)\n        except:\n            raw_policy_line = ast.literal_eval(line)\n        raw_policy.append(raw_policy_line)\n\n    # Depreciated use case has policy as list instead of dict config.\n    for r in raw_policy:\n        for i in [4, 5]:\n            if isinstance(r[i], list):\n                r[i] = {""hp_policy"": r[i]}\n    raw_policy = [PbtUpdate(*r) for r in raw_policy]\n    policy = []\n\n    # Sometimes files have extra lines in the beginning.\n    to_truncate = None\n    for i in range(len(raw_policy) - 1):\n        if raw_policy[i][0] != raw_policy[i + 1][1]:\n            to_truncate = i\n    if to_truncate is not None:\n        raw_policy = raw_policy[to_truncate + 1:]\n\n    # Initial policy for trial_to_clone_epochs.\n    policy.append([raw_policy[0][3], raw_policy[0][4][""hp_policy""]])\n\n    current = raw_policy[0][3]\n    for i in range(len(raw_policy) - 1):\n        # End at next line\'s trial epoch, start from this clone epoch.\n        this_iter = raw_policy[i + 1][3] - raw_policy[i][3]\n        assert this_iter >= 0, (i, raw_policy[i + 1][3], raw_policy[i][3])\n        assert raw_policy[i][0] == raw_policy[i + 1][1], (i, raw_policy[i][0],\n                                                          raw_policy[i + 1][1])\n        policy.append([this_iter, raw_policy[i][5][""hp_policy""]])\n        current += this_iter\n\n    # Last cloned trial policy is run for (end - clone iter of last logged line)\n    policy.append([epochs - raw_policy[-1][3], raw_policy[-1][5][""hp_policy""]])\n    current += epochs - raw_policy[-1][3]\n    assert epochs == sum([p[0] for p in policy])\n    return policy\n\n\ndef parse_log_schedule(file_path, epochs, multiplier=1):\n    """"""Parses policy schedule from log file.\n\n  Args:\n    file_path: Path to policy generated by running search.py.\n    epochs: The number of epochs search was run for.\n    multiplier: Multiplier on number of epochs for each policy in the schedule..\n\n  Returns:\n    List of length epochs, where index i contains the policy to use at epoch i.\n  """"""\n    policy = parse_log(file_path, epochs)\n    schedule = []\n    count = 0\n    for num_iters, pol in policy:\n        tf.logging.debug(""iters {} by multiplier {} result: {}"".format(\n            num_iters, multiplier, num_iters * multiplier))\n        for _ in range(int(num_iters * multiplier)):\n            schedule.append(pol)\n            count += 1\n    if int(epochs * multiplier) - count > 0:\n        tf.logging.info(""len: {}, remaining: {}"".format(\n            count, epochs * multiplier))\n    for _ in range(int(epochs * multiplier) - count):\n        schedule.append(policy[-1][1])\n    tf.logging.info(""final len {}"".format(len(schedule)))\n    return schedule\n\n\nif __name__ == ""__main__"":\n    schedule = parse_log(\'schedules/rsvhn_16_wrn.txt\', 160)\n    for s in schedule:\n        print(s)\n'"
pba/wrn.py,5,"b'""""""Builds the Wide-ResNet Model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nimport autoaugment.custom_ops as ops\nfrom autoaugment.wrn import residual_block, _res_add\n\n\ndef build_wrn_model(images, num_classes, wrn_size, depth=28):\n    """"""Builds the WRN model.\n\n  Build the Wide ResNet model from https://arxiv.org/abs/1605.07146.\n\n  Args:\n    images: Tensor of images that will be fed into the Wide ResNet Model.\n    num_classes: Number of classed that the model needs to predict.\n    wrn_size: Parameter that scales the number of filters in the Wide ResNet\n      model.\n    depth: Number of layers of model.\n\n  Returns:\n    The logits of the Wide ResNet model.\n\n  28-10 is wrn_size of 160 -> k = 10, and depth=28 -> blocks = 4\n  40-2 is wrn_size of 32 -> k = 2, and depth=40 -> blocks = 6\n  """"""\n    assert (depth - 4) % 6 == 0\n    kernel_size = wrn_size\n    filter_size = 3\n    num_blocks_per_resnet = (depth - 4) // 6  # 4\n    filters = [\n        min(kernel_size, 16), kernel_size, kernel_size * 2, kernel_size * 4\n    ]\n    strides = [1, 2, 2]  # stride for each resblock\n\n    # Run the first conv\n    with tf.variable_scope(\'init\'):\n        x = images\n        output_filters = filters[0]\n        x = ops.conv2d(x, output_filters, filter_size, scope=\'init_conv\')\n\n    first_x = x  # Res from the beginning\n    orig_x = x  # Res from previous block\n\n    for block_num in range(1, 4):\n        with tf.variable_scope(\'unit_{}_0\'.format(block_num)):\n            activate_before_residual = True if block_num == 1 else False\n            x = residual_block(\n                x,\n                filters[block_num - 1],\n                filters[block_num],\n                strides[block_num - 1],\n                activate_before_residual=activate_before_residual)\n        for i in range(1, num_blocks_per_resnet):\n            with tf.variable_scope(\'unit_{}_{}\'.format(block_num, i)):\n                x = residual_block(\n                    x,\n                    filters[block_num],\n                    filters[block_num],\n                    1,\n                    activate_before_residual=False)\n        x, orig_x = _res_add(filters[block_num - 1], filters[block_num],\n                             strides[block_num - 1], x, orig_x)\n    final_stride_val = np.prod(strides)\n    x, _ = _res_add(filters[0], filters[3], final_stride_val, x, first_x)\n    with tf.variable_scope(\'unit_last\'):\n        x = ops.batch_norm(x, scope=\'final_bn\')\n        x = tf.nn.relu(x)\n        x = ops.global_avg_pool(x)\n        logits = ops.fc(x, num_classes)\n    return logits\n'"
