file_path,api_count,code
dataset.py,0,"b'from __future__ import generators, division, absolute_import, with_statement, print_function, unicode_literals\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\n\nclass Dataset(object):\n\timages_train = np.array([])\n\timages_test = np.array([])\n\tlabels_train = np.array([])\n\tlabels_test = np.array([])\n\tunique_train_label = np.array([])\n\tmap_train_label_indices = dict()\n\n\tdef _get_siamese_similar_pair(self):\n\t\tlabel =np.random.choice(self.unique_train_label)\n\t\tl, r = np.random.choice(self.map_train_label_indices[label], 2, replace=False)\n\t\treturn l, r, 1\n\n\tdef _get_siamese_dissimilar_pair(self):\n\t\tlabel_l, label_r = np.random.choice(self.unique_train_label, 2, replace=False)\n\t\tl = np.random.choice(self.map_train_label_indices[label_l])\n\t\tr = np.random.choice(self.map_train_label_indices[label_r])\n\t\treturn l, r, 0\n\n\tdef _get_siamese_pair(self):\n\t\tif np.random.random() < 0.5:\n\t\t\treturn self._get_siamese_similar_pair()\n\t\telse:\n\t\t\treturn self._get_siamese_dissimilar_pair()\n\n\tdef get_siamese_batch(self, n):\n\t\tidxs_left, idxs_right, labels = [], [], []\n\t\tfor _ in range(n):\n\t\t\tl, r, x = self._get_siamese_pair()\n\t\t\tidxs_left.append(l)\n\t\t\tidxs_right.append(r)\n\t\t\tlabels.append(x)\n\t\treturn self.images_train[idxs_left,:], self.images_train[idxs_right, :], np.expand_dims(labels, axis=1)\n\nclass MNISTDataset(Dataset):\n\tdef __init__(self):\n\t\tprint(""===Loading MNIST Dataset==="")\n\t\t(self.images_train, self.labels_train), (self.images_test, self.labels_test) = mnist.load_data()\n\t\tself.images_train = np.expand_dims(self.images_train, axis=3) / 255.0\n\t\tself.images_test = np.expand_dims(self.images_test, axis=3) / 255.0\n\t\tself.labels_train = np.expand_dims(self.labels_train, axis=1)\n\t\tself.unique_train_label = np.unique(self.labels_train)\n\t\tself.map_train_label_indices = {label: np.flatnonzero(self.labels_train == label) for label in self.unique_train_label}\n\t\tprint(""Images train :"", self.images_train.shape)\n\t\tprint(""Labels train :"", self.labels_train.shape)\n\t\tprint(""Images test  :"", self.images_test.shape)\n\t\tprint(""Labels test  :"", self.labels_test.shape)\n\t\tprint(""Unique label :"", self.unique_train_label)\n\t\t# print(""Map label indices:"", self.map_train_label_indices)\n\t\t\nif __name__ == ""__main__"":\n\t# Test if it can load the dataset properly or not. use the train.py to run the training\n\ta = MNISTDataset()\n\tbatch_size = 4\n\tls, rs, xs = a.get_siamese_batch(batch_size)\n\tf, axarr = plt.subplots(batch_size, 2)\n\tfor idx, (l, r, x) in enumerate(zip(ls, rs, xs)):\n\t\tprint(""Row"", idx, ""Label:"", ""similar"" if x else ""dissimilar"")\n\t\tprint(""max:"", np.squeeze(l, axis=2).max())\n\t\taxarr[idx, 0].imshow(np.squeeze(l, axis=2))\n\t\taxarr[idx, 1].imshow(np.squeeze(r, axis=2))\n\tplt.show()'"
model.py,28,"b'from __future__ import generators, division, absolute_import, with_statement, print_function, unicode_literals\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow.contrib.slim as slim\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\ndef mnist_model(input, reuse=False):\n\twith tf.name_scope(""model""):\n\t\twith tf.variable_scope(""conv1"") as scope:\n\t\t\tnet = tf.contrib.layers.conv2d(input, 32, [7, 7], activation_fn=tf.nn.relu, padding=\'SAME\',\n\t\t        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n\t\t\tnet = tf.contrib.layers.max_pool2d(net, [2, 2], padding=\'SAME\')\n\n\t\twith tf.variable_scope(""conv2"") as scope:\n\t\t\tnet = tf.contrib.layers.conv2d(net, 64, [5, 5], activation_fn=tf.nn.relu, padding=\'SAME\',\n\t\t        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n\t\t\tnet = tf.contrib.layers.max_pool2d(net, [2, 2], padding=\'SAME\')\n\n\t\twith tf.variable_scope(""conv3"") as scope:\n\t\t\tnet = tf.contrib.layers.conv2d(net, 128, [3, 3], activation_fn=tf.nn.relu, padding=\'SAME\',\n\t\t        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n\t\t\tnet = tf.contrib.layers.max_pool2d(net, [2, 2], padding=\'SAME\')\n\n\t\twith tf.variable_scope(""conv4"") as scope:\n\t\t\tnet = tf.contrib.layers.conv2d(net, 256, [1, 1], activation_fn=tf.nn.relu, padding=\'SAME\',\n\t\t        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n\t\t\tnet = tf.contrib.layers.max_pool2d(net, [2, 2], padding=\'SAME\')\n\n\t\twith tf.variable_scope(""conv5"") as scope:\n\t\t\tnet = tf.contrib.layers.conv2d(net, 2, [1, 1], activation_fn=None, padding=\'SAME\',\n\t\t        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),scope=scope,reuse=reuse)\n\t\t\tnet = tf.contrib.layers.max_pool2d(net, [2, 2], padding=\'SAME\')\n\n\t\tnet = tf.contrib.layers.flatten(net)\n\t\n\treturn net\n\n\ndef contrastive_loss(model1, model2, y, margin):\n\twith tf.name_scope(""contrastive-loss""):\n\t\tdistance = tf.sqrt(tf.reduce_sum(tf.pow(model1 - model2, 2), 1, keepdims=True))\n\t\tsimilarity = y * tf.square(distance)                                           # keep the similar label (1) close to each other\n\t\tdissimilarity = (1 - y) * tf.square(tf.maximum((margin - distance), 0))        # give penalty to dissimilar label if the distance is bigger than margin\n\t\treturn tf.reduce_mean(dissimilarity + similarity) / 2\n'"
train.py,21,"b'from __future__ import generators, division, absolute_import, with_statement, print_function, unicode_literals\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom dataset import MNISTDataset\nfrom model import *\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(\'batch_size\', 512, \'Batch size.\')\nflags.DEFINE_integer(\'train_iter\', 2000, \'Total training iter\')\nflags.DEFINE_integer(\'step\', 50, \'Save after ... iteration\')\nflags.DEFINE_string(\'model\', \'mnist\', \'model to run\')\n\nif __name__ == ""__main__"":\n\t#setup dataset\n\tif FLAGS.model == \'mnist\':\n\t\tdataset = MNISTDataset()\n\t\tmodel = mnist_model\n\t\tplaceholder_shape = [None] + list(dataset.images_train.shape[1:])\n\t\tprint(""placeholder_shape"", placeholder_shape)\n\t\tcolors = [\'#ff0000\', \'#ffff00\', \'#00ff00\', \'#00ffff\', \'#0000ff\', \'#ff00ff\', \'#990000\', \'#999900\', \'#009900\', \'#009999\']\n\telse:\n\t\traise NotImplementedError(""Model for %s is not implemented yet"" % FLAGS.model)\n\n\t# Setup network\n\tnext_batch = dataset.get_siamese_batch\n\tleft = tf.placeholder(tf.float32, placeholder_shape, name=\'left\')\n\tright = tf.placeholder(tf.float32, placeholder_shape, name=\'right\')\n\twith tf.name_scope(""similarity""):\n\t\tlabel = tf.placeholder(tf.int32, [None, 1], name=\'label\') # 1 if same, 0 if different\n\t\tlabel_float = tf.to_float(label)\n\tmargin = 0.5\n\tleft_output = model(left, reuse=False)\n\tright_output = model(right, reuse=True)\n\tloss = contrastive_loss(left_output, right_output, label_float, margin)\n\n\t# Setup Optimizer\n\tglobal_step = tf.Variable(0, trainable=False)\n\n\t# starter_learning_rate = 0.0001\n\t# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n\t# tf.scalar_summary(\'lr\', learning_rate)\n\t# train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)\n\n\ttrain_step = tf.train.MomentumOptimizer(0.01, 0.99, use_nesterov=True).minimize(loss, global_step=global_step)\n\n\t# Start Training\n\tsaver = tf.train.Saver()\n\twith tf.Session() as sess:\n\t\tsess.run(tf.global_variables_initializer())\n\n\t\t#setup tensorboard\t\n\t\ttf.summary.scalar(\'step\', global_step)\n\t\ttf.summary.scalar(\'loss\', loss)\n\t\tfor var in tf.trainable_variables():\n\t\t\ttf.summary.histogram(var.op.name, var)\n\t\tmerged = tf.summary.merge_all()\n\t\twriter = tf.summary.FileWriter(\'train.log\', sess.graph)\n\n\t\t#train iter\n\t\tfor i in range(FLAGS.train_iter):\n\t\t\tbatch_left, batch_right, batch_similarity = next_batch(FLAGS.batch_size)\n\n\t\t\t_, l, summary_str = sess.run([train_step, loss, merged],\n\t\t\t\t\t\t\t\t\t\t feed_dict={left:batch_left, right:batch_right, label: batch_similarity})\n\t\t\t\n\t\t\twriter.add_summary(summary_str, i)\n\t\t\tprint(""\\r#%d - Loss""%i, l)\n\t\t\t\n\t\t\tif (i + 1) % FLAGS.step == 0:\n\t\t\t\t#generate test\n\t\t\t\t# TODO: create a test file and run per batch\n\t\t\t\tfeat = sess.run(left_output, feed_dict={left:dataset.images_test})\n\t\t\t\t\n\t\t\t\tlabels = dataset.labels_test\n\t\t\t\t# plot result\n\t\t\t\tf = plt.figure(figsize=(16,9))\n\t\t\t\tf.set_tight_layout(True)\n\t\t\t\tfor j in range(10):\n\t\t\t\t    plt.plot(feat[labels==j, 0].flatten(), feat[labels==j, 1].flatten(), \'.\', c=colors[j], alpha=0.8)\n\t\t\t\tplt.legend([\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\'])\n\t\t\t\tplt.savefig(\'img/%d.jpg\' % (i + 1))\n\n\t\tsaver.save(sess, ""model/model.ckpt"")\n\n\n\n\n\n'"
