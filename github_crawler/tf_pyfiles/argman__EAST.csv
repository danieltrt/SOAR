file_path,api_count,code
__init__.py,0,b''
data_util.py,0,"b'\'\'\'\nthis file is modified from keras implemention of data process multi-threading,\nsee https://github.com/fchollet/keras/blob/master/keras/utils/data_utils.py\n\'\'\'\nimport time\nimport numpy as np\nimport threading\nimport multiprocessing\ntry:\n    import queue\nexcept ImportError:\n    import Queue as queue\n\n\nclass GeneratorEnqueuer():\n    """"""Builds a queue out of a data generator.\n\n    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n\n    # Arguments\n        generator: a generator function which endlessly yields data\n        use_multiprocessing: use multiprocessing if True, otherwise threading\n        wait_time: time to sleep in-between calls to `put()`\n        random_seed: Initial seed for workers,\n            will be incremented by one for each workers.\n    """"""\n\n    def __init__(self, generator,\n                 use_multiprocessing=False,\n                 wait_time=0.05,\n                 random_seed=None):\n        self.wait_time = wait_time\n        self._generator = generator\n        self._use_multiprocessing = use_multiprocessing\n        self._threads = []\n        self._stop_event = None\n        self.queue = None\n        self.random_seed = random_seed\n\n    def start(self, workers=1, max_queue_size=10):\n        """"""Kicks off threads which add data from the generator into the queue.\n\n        # Arguments\n            workers: number of worker threads\n            max_queue_size: queue size\n                (when full, threads could block on `put()`)\n        """"""\n\n        def data_generator_task():\n            while not self._stop_event.is_set():\n                try:\n                    if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                        generator_output = next(self._generator)\n                        self.queue.put(generator_output)\n                    else:\n                        time.sleep(self.wait_time)\n                except Exception:\n                    self._stop_event.set()\n                    raise\n\n        try:\n            if self._use_multiprocessing:\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n                self._stop_event = multiprocessing.Event()\n            else:\n                self.queue = queue.Queue()\n                self._stop_event = threading.Event()\n\n            for _ in range(workers):\n                if self._use_multiprocessing:\n                    # Reset random seed else all children processes\n                    # share the same seed\n                    np.random.seed(self.random_seed)\n                    thread = multiprocessing.Process(target=data_generator_task)\n                    thread.daemon = True\n                    if self.random_seed is not None:\n                        self.random_seed += 1\n                else:\n                    thread = threading.Thread(target=data_generator_task)\n                self._threads.append(thread)\n                thread.start()\n        except:\n            self.stop()\n            raise\n\n    def is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()\n\n    def stop(self, timeout=None):\n        """"""Stops running threads and wait for them to exit, if necessary.\n\n        Should be called by the same thread which called `start()`.\n\n        # Arguments\n            timeout: maximum time to wait on `thread.join()`.\n        """"""\n        if self.is_running():\n            self._stop_event.set()\n\n        for thread in self._threads:\n            if thread.is_alive():\n                if self._use_multiprocessing:\n                    thread.terminate()\n                else:\n                    thread.join(timeout)\n\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n\n        self._threads = []\n        self._stop_event = None\n        self.queue = None\n\n    def get(self):\n        """"""Creates a generator to extract data from the queue.\n\n        Skip the data if it is `None`.\n\n        # Returns\n            A generator\n        """"""\n        while self.is_running():\n            if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n            else:\n                time.sleep(self.wait_time)'"
eval.py,14,"b""import cv2\nimport time\nimport math\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport locality_aware_nms as nms_locality\nimport lanms\n\ntf.app.flags.DEFINE_string('test_data_path', '/tmp/ch4_test_images/images/', '')\ntf.app.flags.DEFINE_string('gpu_list', '0', '')\ntf.app.flags.DEFINE_string('checkpoint_path', '/tmp/east_icdar2015_resnet_v1_50_rbox/', '')\ntf.app.flags.DEFINE_string('output_dir', '/tmp/ch4_test_images/images/', '')\ntf.app.flags.DEFINE_bool('no_write_images', False, 'do not write images')\n\nimport model\nfrom icdar import restore_rectangle\n\nFLAGS = tf.app.flags.FLAGS\n\ndef get_images():\n    '''\n    find image files in test data path\n    :return: list of files found\n    '''\n    files = []\n    exts = ['jpg', 'png', 'jpeg', 'JPG']\n    for parent, dirnames, filenames in os.walk(FLAGS.test_data_path):\n        for filename in filenames:\n            for ext in exts:\n                if filename.endswith(ext):\n                    files.append(os.path.join(parent, filename))\n                    break\n    print('Find {} images'.format(len(files)))\n    return files\n\n\ndef resize_image(im, max_side_len=2400):\n    '''\n    resize image to a size multiple of 32 which is required by the network\n    :param im: the resized image\n    :param max_side_len: limit of max image size to avoid out of memory in gpu\n    :return: the resized image and the resize ratio\n    '''\n    h, w, _ = im.shape\n\n    resize_w = w\n    resize_h = h\n\n    # limit the max side\n    if max(resize_h, resize_w) > max_side_len:\n        ratio = float(max_side_len) / resize_h if resize_h > resize_w else float(max_side_len) / resize_w\n    else:\n        ratio = 1.\n    resize_h = int(resize_h * ratio)\n    resize_w = int(resize_w * ratio)\n\n    resize_h = resize_h if resize_h % 32 == 0 else (resize_h // 32 - 1) * 32\n    resize_w = resize_w if resize_w % 32 == 0 else (resize_w // 32 - 1) * 32\n    resize_h = max(32, resize_h)\n    resize_w = max(32, resize_w)\n    im = cv2.resize(im, (int(resize_w), int(resize_h)))\n\n    ratio_h = resize_h / float(h)\n    ratio_w = resize_w / float(w)\n\n    return im, (ratio_h, ratio_w)\n\n\ndef detect(score_map, geo_map, timer, score_map_thresh=0.8, box_thresh=0.1, nms_thres=0.2):\n    '''\n    restore text boxes from score map and geo map\n    :param score_map:\n    :param geo_map:\n    :param timer:\n    :param score_map_thresh: threshhold for score map\n    :param box_thresh: threshhold for boxes\n    :param nms_thres: threshold for nms\n    :return:\n    '''\n    if len(score_map.shape) == 4:\n        score_map = score_map[0, :, :, 0]\n        geo_map = geo_map[0, :, :, ]\n    # filter the score map\n    xy_text = np.argwhere(score_map > score_map_thresh)\n    # sort the text boxes via the y axis\n    xy_text = xy_text[np.argsort(xy_text[:, 0])]\n    # restore\n    start = time.time()\n    text_box_restored = restore_rectangle(xy_text[:, ::-1]*4, geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2\n    print('{} text boxes before nms'.format(text_box_restored.shape[0]))\n    boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32)\n    boxes[:, :8] = text_box_restored.reshape((-1, 8))\n    boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]]\n    timer['restore'] = time.time() - start\n    # nms part\n    start = time.time()\n    # boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres)\n    boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thres)\n    timer['nms'] = time.time() - start\n\n    if boxes.shape[0] == 0:\n        return None, timer\n\n    # here we filter some low score boxes by the average score map, this is different from the orginal paper\n    for i, box in enumerate(boxes):\n        mask = np.zeros_like(score_map, dtype=np.uint8)\n        cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1)\n        boxes[i, 8] = cv2.mean(score_map, mask)[0]\n    boxes = boxes[boxes[:, 8] > box_thresh]\n\n    return boxes, timer\n\n\ndef sort_poly(p):\n    min_axis = np.argmin(np.sum(p, axis=1))\n    p = p[[min_axis, (min_axis+1)%4, (min_axis+2)%4, (min_axis+3)%4]]\n    if abs(p[0, 0] - p[1, 0]) > abs(p[0, 1] - p[1, 1]):\n        return p\n    else:\n        return p[[0, 3, 2, 1]]\n\n\ndef main(argv=None):\n    import os\n    os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list\n\n\n    try:\n        os.makedirs(FLAGS.output_dir)\n    except OSError as e:\n        if e.errno != 17:\n            raise\n\n    with tf.get_default_graph().as_default():\n        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images')\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n\n        f_score, f_geometry = model.model(input_images, is_training=False)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.997, global_step)\n        saver = tf.train.Saver(variable_averages.variables_to_restore())\n\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n            ckpt_state = tf.train.get_checkpoint_state(FLAGS.checkpoint_path)\n            model_path = os.path.join(FLAGS.checkpoint_path, os.path.basename(ckpt_state.model_checkpoint_path))\n            print('Restore from {}'.format(model_path))\n            saver.restore(sess, model_path)\n\n            im_fn_list = get_images()\n            for im_fn in im_fn_list:\n                im = cv2.imread(im_fn)[:, :, ::-1]\n                start_time = time.time()\n                im_resized, (ratio_h, ratio_w) = resize_image(im)\n\n                timer = {'net': 0, 'restore': 0, 'nms': 0}\n                start = time.time()\n                score, geometry = sess.run([f_score, f_geometry], feed_dict={input_images: [im_resized]})\n                timer['net'] = time.time() - start\n\n                boxes, timer = detect(score_map=score, geo_map=geometry, timer=timer)\n                print('{} : net {:.0f}ms, restore {:.0f}ms, nms {:.0f}ms'.format(\n                    im_fn, timer['net']*1000, timer['restore']*1000, timer['nms']*1000))\n\n                if boxes is not None:\n                    boxes = boxes[:, :8].reshape((-1, 4, 2))\n                    boxes[:, :, 0] /= ratio_w\n                    boxes[:, :, 1] /= ratio_h\n\n                duration = time.time() - start_time\n                print('[timing] {}'.format(duration))\n\n                # save to file\n                if boxes is not None:\n                    res_file = os.path.join(\n                        FLAGS.output_dir,\n                        '{}.txt'.format(\n                            os.path.basename(im_fn).split('.')[0]))\n\n                    with open(res_file, 'w') as f:\n                        for box in boxes:\n                            # to avoid submitting errors\n                            box = sort_poly(box.astype(np.int32))\n                            if np.linalg.norm(box[0] - box[1]) < 5 or np.linalg.norm(box[3]-box[0]) < 5:\n                                continue\n                            f.write('{},{},{},{},{},{},{},{}\\r\\n'.format(\n                                box[0, 0], box[0, 1], box[1, 0], box[1, 1], box[2, 0], box[2, 1], box[3, 0], box[3, 1],\n                            ))\n                            cv2.polylines(im[:, :, ::-1], [box.astype(np.int32).reshape((-1, 1, 2))], True, color=(255, 255, 0), thickness=1)\n                if not FLAGS.no_write_images:\n                    img_path = os.path.join(FLAGS.output_dir, os.path.basename(im_fn))\n                    cv2.imwrite(img_path, im[:, :, ::-1])\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
icdar.py,7,"b""# coding:utf-8\nimport glob\nimport csv\nimport cv2\nimport time\nimport os\nimport numpy as np\nimport scipy.optimize\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as Patches\nfrom shapely.geometry import Polygon\n\nimport tensorflow as tf\n\nfrom data_util import GeneratorEnqueuer\n\ntf.app.flags.DEFINE_string('training_data_path', '/data/ocr/icdar2015/',\n                           'training dataset to use')\ntf.app.flags.DEFINE_integer('max_image_large_side', 1280,\n                            'max image size of training')\ntf.app.flags.DEFINE_integer('max_text_size', 800,\n                            'if the text in the input image is bigger than this, then we resize'\n                            'the image according to this')\ntf.app.flags.DEFINE_integer('min_text_size', 10,\n                            'if the text size is smaller than this, we ignore it during training')\ntf.app.flags.DEFINE_float('min_crop_side_ratio', 0.1,\n                          'when doing random crop from input image, the'\n                          'min length of min(H, W')\ntf.app.flags.DEFINE_string('geometry', 'RBOX',\n                           'which geometry to generate, RBOX or QUAD')\n\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef get_images():\n    files = []\n    for ext in ['jpg', 'png', 'jpeg', 'JPG']:\n        files.extend(glob.glob(\n            os.path.join(FLAGS.training_data_path, '*.{}'.format(ext))))\n    return files\n\n\ndef load_annoataion(p):\n    '''\n    load annotation from the text file\n    :param p:\n    :return:\n    '''\n    text_polys = []\n    text_tags = []\n    if not os.path.exists(p):\n        return np.array(text_polys, dtype=np.float32)\n    with open(p, 'r') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            label = line[-1]\n            # strip BOM. \\ufeff for python3,  \\xef\\xbb\\bf for python2\n            line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line]\n\n            x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n            if label == '*' or label == '###':\n                text_tags.append(True)\n            else:\n                text_tags.append(False)\n        return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool)\n\n\ndef polygon_area(poly):\n    '''\n    compute area of a polygon\n    :param poly:\n    :return:\n    '''\n    edge = [\n        (poly[1][0] - poly[0][0]) * (poly[1][1] + poly[0][1]),\n        (poly[2][0] - poly[1][0]) * (poly[2][1] + poly[1][1]),\n        (poly[3][0] - poly[2][0]) * (poly[3][1] + poly[2][1]),\n        (poly[0][0] - poly[3][0]) * (poly[0][1] + poly[3][1])\n    ]\n    return np.sum(edge)/2.\n\n\ndef check_and_validate_polys(polys, tags, xxx_todo_changeme):\n    '''\n    check so that the text poly is in the same direction,\n    and also filter some invalid polygons\n    :param polys:\n    :param tags:\n    :return:\n    '''\n    (h, w) = xxx_todo_changeme\n    if polys.shape[0] == 0:\n        return polys\n    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w-1)\n    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h-1)\n\n    validated_polys = []\n    validated_tags = []\n    for poly, tag in zip(polys, tags):\n        p_area = polygon_area(poly)\n        if abs(p_area) < 1:\n            # print poly\n            print('invalid poly')\n            continue\n        if p_area > 0:\n            print('poly in wrong direction')\n            poly = poly[(0, 3, 2, 1), :]\n        validated_polys.append(poly)\n        validated_tags.append(tag)\n    return np.array(validated_polys), np.array(validated_tags)\n\n\ndef crop_area(im, polys, tags, crop_background=False, max_tries=50):\n    '''\n    make random crop from the input image\n    :param im:\n    :param polys:\n    :param tags:\n    :param crop_background:\n    :param max_tries:\n    :return:\n    '''\n    h, w, _ = im.shape\n    pad_h = h//10\n    pad_w = w//10\n    h_array = np.zeros((h + pad_h*2), dtype=np.int32)\n    w_array = np.zeros((w + pad_w*2), dtype=np.int32)\n    for poly in polys:\n        poly = np.round(poly, decimals=0).astype(np.int32)\n        minx = np.min(poly[:, 0])\n        maxx = np.max(poly[:, 0])\n        w_array[minx+pad_w:maxx+pad_w] = 1\n        miny = np.min(poly[:, 1])\n        maxy = np.max(poly[:, 1])\n        h_array[miny+pad_h:maxy+pad_h] = 1\n    # ensure the cropped area not across a text\n    h_axis = np.where(h_array == 0)[0]\n    w_axis = np.where(w_array == 0)[0]\n    if len(h_axis) == 0 or len(w_axis) == 0:\n        return im, polys, tags\n    for i in range(max_tries):\n        xx = np.random.choice(w_axis, size=2)\n        xmin = np.min(xx) - pad_w\n        xmax = np.max(xx) - pad_w\n        xmin = np.clip(xmin, 0, w-1)\n        xmax = np.clip(xmax, 0, w-1)\n        yy = np.random.choice(h_axis, size=2)\n        ymin = np.min(yy) - pad_h\n        ymax = np.max(yy) - pad_h\n        ymin = np.clip(ymin, 0, h-1)\n        ymax = np.clip(ymax, 0, h-1)\n        if xmax - xmin < FLAGS.min_crop_side_ratio*w or ymax - ymin < FLAGS.min_crop_side_ratio*h:\n            # area too small\n            continue\n        if polys.shape[0] != 0:\n            poly_axis_in_area = (polys[:, :, 0] >= xmin) & (polys[:, :, 0] <= xmax) \\\n                                & (polys[:, :, 1] >= ymin) & (polys[:, :, 1] <= ymax)\n            selected_polys = np.where(np.sum(poly_axis_in_area, axis=1) == 4)[0]\n        else:\n            selected_polys = []\n        if len(selected_polys) == 0:\n            # no text in this area\n            if crop_background:\n                return im[ymin:ymax+1, xmin:xmax+1, :], polys[selected_polys], tags[selected_polys]\n            else:\n                continue\n        im = im[ymin:ymax+1, xmin:xmax+1, :]\n        polys = polys[selected_polys]\n        tags = tags[selected_polys]\n        polys[:, :, 0] -= xmin\n        polys[:, :, 1] -= ymin\n        return im, polys, tags\n\n    return im, polys, tags\n\n\ndef shrink_poly(poly, r):\n    '''\n    fit a poly inside the origin poly, maybe bugs here...\n    used for generate the score map\n    :param poly: the text poly\n    :param r: r in the paper\n    :return: the shrinked poly\n    '''\n    # shrink ratio\n    R = 0.3\n    # find the longer pair\n    if np.linalg.norm(poly[0] - poly[1]) + np.linalg.norm(poly[2] - poly[3]) > \\\n                    np.linalg.norm(poly[0] - poly[3]) + np.linalg.norm(poly[1] - poly[2]):\n        # first move (p0, p1), (p2, p3), then (p0, p3), (p1, p2)\n        ## p0, p1\n        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n        poly[0][0] += R * r[0] * np.cos(theta)\n        poly[0][1] += R * r[0] * np.sin(theta)\n        poly[1][0] -= R * r[1] * np.cos(theta)\n        poly[1][1] -= R * r[1] * np.sin(theta)\n        ## p2, p3\n        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n        poly[3][0] += R * r[3] * np.cos(theta)\n        poly[3][1] += R * r[3] * np.sin(theta)\n        poly[2][0] -= R * r[2] * np.cos(theta)\n        poly[2][1] -= R * r[2] * np.sin(theta)\n        ## p0, p3\n        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n        poly[0][0] += R * r[0] * np.sin(theta)\n        poly[0][1] += R * r[0] * np.cos(theta)\n        poly[3][0] -= R * r[3] * np.sin(theta)\n        poly[3][1] -= R * r[3] * np.cos(theta)\n        ## p1, p2\n        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n        poly[1][0] += R * r[1] * np.sin(theta)\n        poly[1][1] += R * r[1] * np.cos(theta)\n        poly[2][0] -= R * r[2] * np.sin(theta)\n        poly[2][1] -= R * r[2] * np.cos(theta)\n    else:\n        ## p0, p3\n        # print poly\n        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n        poly[0][0] += R * r[0] * np.sin(theta)\n        poly[0][1] += R * r[0] * np.cos(theta)\n        poly[3][0] -= R * r[3] * np.sin(theta)\n        poly[3][1] -= R * r[3] * np.cos(theta)\n        ## p1, p2\n        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n        poly[1][0] += R * r[1] * np.sin(theta)\n        poly[1][1] += R * r[1] * np.cos(theta)\n        poly[2][0] -= R * r[2] * np.sin(theta)\n        poly[2][1] -= R * r[2] * np.cos(theta)\n        ## p0, p1\n        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n        poly[0][0] += R * r[0] * np.cos(theta)\n        poly[0][1] += R * r[0] * np.sin(theta)\n        poly[1][0] -= R * r[1] * np.cos(theta)\n        poly[1][1] -= R * r[1] * np.sin(theta)\n        ## p2, p3\n        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n        poly[3][0] += R * r[3] * np.cos(theta)\n        poly[3][1] += R * r[3] * np.sin(theta)\n        poly[2][0] -= R * r[2] * np.cos(theta)\n        poly[2][1] -= R * r[2] * np.sin(theta)\n    return poly\n\n\ndef point_dist_to_line(p1, p2, p3):\n    # compute the distance from p3 to p1-p2\n    return np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)\n\n\ndef fit_line(p1, p2):\n    # fit a line ax+by+c = 0\n    if p1[0] == p1[1]:\n        return [1., 0., -p1[0]]\n    else:\n        [k, b] = np.polyfit(p1, p2, deg=1)\n        return [k, -1., b]\n\n\ndef line_cross_point(line1, line2):\n    # line1 0= ax+by+c, compute the cross point of line1 and line2\n    if line1[0] != 0 and line1[0] == line2[0]:\n        print('Cross point does not exist')\n        return None\n    if line1[0] == 0 and line2[0] == 0:\n        print('Cross point does not exist')\n        return None\n    if line1[1] == 0:\n        x = -line1[2]\n        y = line2[0] * x + line2[2]\n    elif line2[1] == 0:\n        x = -line2[2]\n        y = line1[0] * x + line1[2]\n    else:\n        k1, _, b1 = line1\n        k2, _, b2 = line2\n        x = -(b1-b2)/(k1-k2)\n        y = k1*x + b1\n    return np.array([x, y], dtype=np.float32)\n\n\ndef line_verticle(line, point):\n    # get the verticle line from line across point\n    if line[1] == 0:\n        verticle = [0, -1, point[1]]\n    else:\n        if line[0] == 0:\n            verticle = [1, 0, -point[0]]\n        else:\n            verticle = [-1./line[0], -1, point[1] - (-1/line[0] * point[0])]\n    return verticle\n\n\ndef rectangle_from_parallelogram(poly):\n    '''\n    fit a rectangle from a parallelogram\n    :param poly:\n    :return:\n    '''\n    p0, p1, p2, p3 = poly\n    angle_p0 = np.arccos(np.dot(p1-p0, p3-p0)/(np.linalg.norm(p0-p1) * np.linalg.norm(p3-p0)))\n    if angle_p0 < 0.5 * np.pi:\n        if np.linalg.norm(p0 - p1) > np.linalg.norm(p0-p3):\n            # p0 and p2\n            ## p0\n            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n            p2p3_verticle = line_verticle(p2p3, p0)\n\n            new_p3 = line_cross_point(p2p3, p2p3_verticle)\n            ## p2\n            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n            p0p1_verticle = line_verticle(p0p1, p2)\n\n            new_p1 = line_cross_point(p0p1, p0p1_verticle)\n            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n        else:\n            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n            p1p2_verticle = line_verticle(p1p2, p0)\n\n            new_p1 = line_cross_point(p1p2, p1p2_verticle)\n            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n            p0p3_verticle = line_verticle(p0p3, p2)\n\n            new_p3 = line_cross_point(p0p3, p0p3_verticle)\n            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n    else:\n        if np.linalg.norm(p0-p1) > np.linalg.norm(p0-p3):\n            # p1 and p3\n            ## p1\n            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n            p2p3_verticle = line_verticle(p2p3, p1)\n\n            new_p2 = line_cross_point(p2p3, p2p3_verticle)\n            ## p3\n            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n            p0p1_verticle = line_verticle(p0p1, p3)\n\n            new_p0 = line_cross_point(p0p1, p0p1_verticle)\n            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n        else:\n            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n            p0p3_verticle = line_verticle(p0p3, p1)\n\n            new_p0 = line_cross_point(p0p3, p0p3_verticle)\n            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n            p1p2_verticle = line_verticle(p1p2, p3)\n\n            new_p2 = line_cross_point(p1p2, p1p2_verticle)\n            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n\n\ndef sort_rectangle(poly):\n    # sort the four coordinates of the polygon, points in poly should be sorted clockwise\n    # First find the lowest point\n    p_lowest = np.argmax(poly[:, 1])\n    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n        # \xe5\xba\x95\xe8\xbe\xb9\xe5\xb9\xb3\xe8\xa1\x8c\xe4\xba\x8eX\xe8\xbd\xb4, \xe9\x82\xa3\xe4\xb9\x88p0\xe4\xb8\xba\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92 - if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n        p0_index = np.argmin(np.sum(poly, axis=1))\n        p1_index = (p0_index + 1) % 4\n        p2_index = (p0_index + 2) % 4\n        p3_index = (p0_index + 3) % 4\n        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n    else:\n        # \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe4\xbd\x8e\xe7\x82\xb9\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x9a\x84\xe7\x82\xb9 - find the point that sits right to the lowest point\n        p_lowest_right = (p_lowest - 1) % 4\n        p_lowest_left = (p_lowest + 1) % 4\n        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\n        # assert angle > 0\n        if angle <= 0:\n            print(angle, poly[p_lowest], poly[p_lowest_right])\n        if angle/np.pi * 180 > 45:\n            # \xe8\xbf\x99\xe4\xb8\xaa\xe7\x82\xb9\xe4\xb8\xbap2 - this point is p2\n            p2_index = p_lowest\n            p1_index = (p2_index - 1) % 4\n            p0_index = (p2_index - 2) % 4\n            p3_index = (p2_index + 1) % 4\n            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\n        else:\n            # \xe8\xbf\x99\xe4\xb8\xaa\xe7\x82\xb9\xe4\xb8\xbap3 - this point is p3\n            p3_index = p_lowest\n            p0_index = (p3_index + 1) % 4\n            p1_index = (p3_index + 2) % 4\n            p2_index = (p3_index + 3) % 4\n            return poly[[p0_index, p1_index, p2_index, p3_index]], angle\n\n\ndef restore_rectangle_rbox(origin, geometry):\n    d = geometry[:, :4]\n    angle = geometry[:, 4]\n    # for angle > 0\n    origin_0 = origin[angle >= 0]\n    d_0 = d[angle >= 0]\n    angle_0 = angle[angle >= 0]\n    if origin_0.shape[0] > 0:\n        p = np.array([np.zeros(d_0.shape[0]), -d_0[:, 0] - d_0[:, 2],\n                      d_0[:, 1] + d_0[:, 3], -d_0[:, 0] - d_0[:, 2],\n                      d_0[:, 1] + d_0[:, 3], np.zeros(d_0.shape[0]),\n                      np.zeros(d_0.shape[0]), np.zeros(d_0.shape[0]),\n                      d_0[:, 3], -d_0[:, 2]])\n        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n\n        rotate_matrix_x = np.array([np.cos(angle_0), np.sin(angle_0)]).transpose((1, 0))\n        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n\n        rotate_matrix_y = np.array([-np.sin(angle_0), np.cos(angle_0)]).transpose((1, 0))\n        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n\n        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n\n        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n\n        p3_in_origin = origin_0 - p_rotate[:, 4, :]\n        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n\n        new_p_0 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n    else:\n        new_p_0 = np.zeros((0, 4, 2))\n    # for angle < 0\n    origin_1 = origin[angle < 0]\n    d_1 = d[angle < 0]\n    angle_1 = angle[angle < 0]\n    if origin_1.shape[0] > 0:\n        p = np.array([-d_1[:, 1] - d_1[:, 3], -d_1[:, 0] - d_1[:, 2],\n                      np.zeros(d_1.shape[0]), -d_1[:, 0] - d_1[:, 2],\n                      np.zeros(d_1.shape[0]), np.zeros(d_1.shape[0]),\n                      -d_1[:, 1] - d_1[:, 3], np.zeros(d_1.shape[0]),\n                      -d_1[:, 1], -d_1[:, 2]])\n        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n\n        rotate_matrix_x = np.array([np.cos(-angle_1), -np.sin(-angle_1)]).transpose((1, 0))\n        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n\n        rotate_matrix_y = np.array([np.sin(-angle_1), np.cos(-angle_1)]).transpose((1, 0))\n        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n\n        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n\n        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n\n        p3_in_origin = origin_1 - p_rotate[:, 4, :]\n        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n\n        new_p_1 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n    else:\n        new_p_1 = np.zeros((0, 4, 2))\n    return np.concatenate([new_p_0, new_p_1])\n\n\ndef restore_rectangle(origin, geometry):\n    return restore_rectangle_rbox(origin, geometry)\n\n\ndef generate_rbox(im_size, polys, tags):\n    h, w = im_size\n    poly_mask = np.zeros((h, w), dtype=np.uint8)\n    score_map = np.zeros((h, w), dtype=np.uint8)\n    geo_map = np.zeros((h, w, 5), dtype=np.float32)\n    # mask used during traning, to ignore some hard areas\n    training_mask = np.ones((h, w), dtype=np.uint8)\n    for poly_idx, poly_tag in enumerate(zip(polys, tags)):\n        poly = poly_tag[0]\n        tag = poly_tag[1]\n\n        r = [None, None, None, None]\n        for i in range(4):\n            r[i] = min(np.linalg.norm(poly[i] - poly[(i + 1) % 4]),\n                       np.linalg.norm(poly[i] - poly[(i - 1) % 4]))\n        # score map\n        shrinked_poly = shrink_poly(poly.copy(), r).astype(np.int32)[np.newaxis, :, :]\n        cv2.fillPoly(score_map, shrinked_poly, 1)\n        cv2.fillPoly(poly_mask, shrinked_poly, poly_idx + 1)\n        # if the poly is too small, then ignore it during training\n        poly_h = min(np.linalg.norm(poly[0] - poly[3]), np.linalg.norm(poly[1] - poly[2]))\n        poly_w = min(np.linalg.norm(poly[0] - poly[1]), np.linalg.norm(poly[2] - poly[3]))\n        if min(poly_h, poly_w) < FLAGS.min_text_size:\n            cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n        if tag:\n            cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n\n        xy_in_poly = np.argwhere(poly_mask == (poly_idx + 1))\n        # if geometry == 'RBOX':\n        # \xe5\xaf\xb9\xe4\xbb\xbb\xe6\x84\x8f\xe4\xb8\xa4\xe4\xb8\xaa\xe9\xa1\xb6\xe7\x82\xb9\xe7\x9a\x84\xe7\xbb\x84\xe5\x90\x88\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb9\xb3\xe8\xa1\x8c\xe5\x9b\x9b\xe8\xbe\xb9\xe5\xbd\xa2 - generate a parallelogram for any combination of two vertices\n        fitted_parallelograms = []\n        for i in range(4):\n            p0 = poly[i]\n            p1 = poly[(i + 1) % 4]\n            p2 = poly[(i + 2) % 4]\n            p3 = poly[(i + 3) % 4]\n            edge = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n            backward_edge = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n            forward_edge = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n            if point_dist_to_line(p0, p1, p2) > point_dist_to_line(p0, p1, p3):\n                # \xe5\xb9\xb3\xe8\xa1\x8c\xe7\xba\xbf\xe7\xbb\x8f\xe8\xbf\x87p2 - parallel lines through p2\n                if edge[1] == 0:\n                    edge_opposite = [1, 0, -p2[0]]\n                else:\n                    edge_opposite = [edge[0], -1, p2[1] - edge[0] * p2[0]]\n            else:\n                # \xe7\xbb\x8f\xe8\xbf\x87p3 - after p3\n                if edge[1] == 0:\n                    edge_opposite = [1, 0, -p3[0]]\n                else:\n                    edge_opposite = [edge[0], -1, p3[1] - edge[0] * p3[0]]\n            # move forward edge\n            new_p0 = p0\n            new_p1 = p1\n            new_p2 = p2\n            new_p3 = p3\n            new_p2 = line_cross_point(forward_edge, edge_opposite)\n            if point_dist_to_line(p1, new_p2, p0) > point_dist_to_line(p1, new_p2, p3):\n                # across p0\n                if forward_edge[1] == 0:\n                    forward_opposite = [1, 0, -p0[0]]\n                else:\n                    forward_opposite = [forward_edge[0], -1, p0[1] - forward_edge[0] * p0[0]]\n            else:\n                # across p3\n                if forward_edge[1] == 0:\n                    forward_opposite = [1, 0, -p3[0]]\n                else:\n                    forward_opposite = [forward_edge[0], -1, p3[1] - forward_edge[0] * p3[0]]\n            new_p0 = line_cross_point(forward_opposite, edge)\n            new_p3 = line_cross_point(forward_opposite, edge_opposite)\n            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n            # or move backward edge\n            new_p0 = p0\n            new_p1 = p1\n            new_p2 = p2\n            new_p3 = p3\n            new_p3 = line_cross_point(backward_edge, edge_opposite)\n            if point_dist_to_line(p0, p3, p1) > point_dist_to_line(p0, p3, p2):\n                # across p1\n                if backward_edge[1] == 0:\n                    backward_opposite = [1, 0, -p1[0]]\n                else:\n                    backward_opposite = [backward_edge[0], -1, p1[1] - backward_edge[0] * p1[0]]\n            else:\n                # across p2\n                if backward_edge[1] == 0:\n                    backward_opposite = [1, 0, -p2[0]]\n                else:\n                    backward_opposite = [backward_edge[0], -1, p2[1] - backward_edge[0] * p2[0]]\n            new_p1 = line_cross_point(backward_opposite, edge)\n            new_p2 = line_cross_point(backward_opposite, edge_opposite)\n            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n        areas = [Polygon(t).area for t in fitted_parallelograms]\n        parallelogram = np.array(fitted_parallelograms[np.argmin(areas)][:-1], dtype=np.float32)\n        # sort thie polygon\n        parallelogram_coord_sum = np.sum(parallelogram, axis=1)\n        min_coord_idx = np.argmin(parallelogram_coord_sum)\n        parallelogram = parallelogram[\n            [min_coord_idx, (min_coord_idx + 1) % 4, (min_coord_idx + 2) % 4, (min_coord_idx + 3) % 4]]\n\n        rectange = rectangle_from_parallelogram(parallelogram)\n        rectange, rotate_angle = sort_rectangle(rectange)\n\n        p0_rect, p1_rect, p2_rect, p3_rect = rectange\n        for y, x in xy_in_poly:\n            point = np.array([x, y], dtype=np.float32)\n            # top\n            geo_map[y, x, 0] = point_dist_to_line(p0_rect, p1_rect, point)\n            # right\n            geo_map[y, x, 1] = point_dist_to_line(p1_rect, p2_rect, point)\n            # down\n            geo_map[y, x, 2] = point_dist_to_line(p2_rect, p3_rect, point)\n            # left\n            geo_map[y, x, 3] = point_dist_to_line(p3_rect, p0_rect, point)\n            # angle\n            geo_map[y, x, 4] = rotate_angle\n    return score_map, geo_map, training_mask\n\n\ndef generator(input_size=512, batch_size=32,\n              background_ratio=3./8,\n              random_scale=np.array([0.5, 1, 2.0, 3.0]),\n              vis=False):\n    image_list = np.array(get_images())\n    print('{} training images in {}'.format(\n        image_list.shape[0], FLAGS.training_data_path))\n    index = np.arange(0, image_list.shape[0])\n    while True:\n        np.random.shuffle(index)\n        images = []\n        image_fns = []\n        score_maps = []\n        geo_maps = []\n        training_masks = []\n        for i in index:\n            try:\n                im_fn = image_list[i]\n                im = cv2.imread(im_fn)\n                # print im_fn\n                h, w, _ = im.shape\n                txt_fn = im_fn.replace(os.path.basename(im_fn).split('.')[1], 'txt')\n                if not os.path.exists(txt_fn):\n                    print('text file {} does not exists'.format(txt_fn))\n                    continue\n\n                text_polys, text_tags = load_annoataion(txt_fn)\n\n                text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w))\n                # if text_polys.shape[0] == 0:\n                #     continue\n                # random scale this image\n                rd_scale = np.random.choice(random_scale)\n                im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale)\n                text_polys *= rd_scale\n                # print rd_scale\n                # random crop a area from image\n                if np.random.rand() < background_ratio:\n                    # crop background\n                    im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True)\n                    if text_polys.shape[0] > 0:\n                        # cannot find background\n                        continue\n                    # pad and resize image\n                    new_h, new_w, _ = im.shape\n                    max_h_w_i = np.max([new_h, new_w, input_size])\n                    im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8)\n                    im_padded[:new_h, :new_w, :] = im.copy()\n                    im = cv2.resize(im_padded, dsize=(input_size, input_size))\n                    score_map = np.zeros((input_size, input_size), dtype=np.uint8)\n                    geo_map_channels = 5 if FLAGS.geometry == 'RBOX' else 8\n                    geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32)\n                    training_mask = np.ones((input_size, input_size), dtype=np.uint8)\n                else:\n                    im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False)\n                    if text_polys.shape[0] == 0:\n                        continue\n                    h, w, _ = im.shape\n\n                    # pad the image to the training input size or the longer side of image\n                    new_h, new_w, _ = im.shape\n                    max_h_w_i = np.max([new_h, new_w, input_size])\n                    im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8)\n                    im_padded[:new_h, :new_w, :] = im.copy()\n                    im = im_padded\n                    # resize the image to input size\n                    new_h, new_w, _ = im.shape\n                    resize_h = input_size\n                    resize_w = input_size\n                    im = cv2.resize(im, dsize=(resize_w, resize_h))\n                    resize_ratio_3_x = resize_w/float(new_w)\n                    resize_ratio_3_y = resize_h/float(new_h)\n                    text_polys[:, :, 0] *= resize_ratio_3_x\n                    text_polys[:, :, 1] *= resize_ratio_3_y\n                    new_h, new_w, _ = im.shape\n                    score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags)\n\n                if vis:\n                    fig, axs = plt.subplots(3, 2, figsize=(20, 30))\n                    # axs[0].imshow(im[:, :, ::-1])\n                    # axs[0].set_xticks([])\n                    # axs[0].set_yticks([])\n                    # for poly in text_polys:\n                    #     poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1]))\n                    #     poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0]))\n                    #     axs[0].add_artist(Patches.Polygon(\n                    #         poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True))\n                    #     axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4),\n                    #                    color='purple')\n                    # axs[1].imshow(score_map)\n                    # axs[1].set_xticks([])\n                    # axs[1].set_yticks([])\n                    axs[0, 0].imshow(im[:, :, ::-1])\n                    axs[0, 0].set_xticks([])\n                    axs[0, 0].set_yticks([])\n                    for poly in text_polys:\n                        poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1]))\n                        poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0]))\n                        axs[0, 0].add_artist(Patches.Polygon(\n                            poly, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True))\n                        axs[0, 0].text(poly[0, 0], poly[0, 1], '{:.0f}-{:.0f}'.format(poly_h, poly_w), color='purple')\n                    axs[0, 1].imshow(score_map[::, ::])\n                    axs[0, 1].set_xticks([])\n                    axs[0, 1].set_yticks([])\n                    axs[1, 0].imshow(geo_map[::, ::, 0])\n                    axs[1, 0].set_xticks([])\n                    axs[1, 0].set_yticks([])\n                    axs[1, 1].imshow(geo_map[::, ::, 1])\n                    axs[1, 1].set_xticks([])\n                    axs[1, 1].set_yticks([])\n                    axs[2, 0].imshow(geo_map[::, ::, 2])\n                    axs[2, 0].set_xticks([])\n                    axs[2, 0].set_yticks([])\n                    axs[2, 1].imshow(training_mask[::, ::])\n                    axs[2, 1].set_xticks([])\n                    axs[2, 1].set_yticks([])\n                    plt.tight_layout()\n                    plt.show()\n                    plt.close()\n\n                images.append(im[:, :, ::-1].astype(np.float32))\n                image_fns.append(im_fn)\n                score_maps.append(score_map[::4, ::4, np.newaxis].astype(np.float32))\n                geo_maps.append(geo_map[::4, ::4, :].astype(np.float32))\n                training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32))\n\n                if len(images) == batch_size:\n                    yield images, image_fns, score_maps, geo_maps, training_masks\n                    images = []\n                    image_fns = []\n                    score_maps = []\n                    geo_maps = []\n                    training_masks = []\n            except Exception as e:\n                import traceback\n                traceback.print_exc()\n                continue\n\n\ndef get_batch(num_workers, **kwargs):\n    try:\n        enqueuer = GeneratorEnqueuer(generator(**kwargs), use_multiprocessing=True)\n        print('Generator use 10 batches for buffering, this may take a while, you can tune this yourself.')\n        enqueuer.start(max_queue_size=10, workers=num_workers)\n        generator_output = None\n        while True:\n            while enqueuer.is_running():\n                if not enqueuer.queue.empty():\n                    generator_output = enqueuer.queue.get()\n                    break\n                else:\n                    time.sleep(0.01)\n            yield generator_output\n            generator_output = None\n    finally:\n        if enqueuer is not None:\n            enqueuer.stop()\n\n\n\nif __name__ == '__main__':\n    pass\n"""
locality_aware_nms.py,0,"b""import numpy as np\nfrom shapely.geometry import Polygon\n\n\ndef intersection(g, p):\n    g = Polygon(g[:8].reshape((4, 2)))\n    p = Polygon(p[:8].reshape((4, 2)))\n    if not g.is_valid or not p.is_valid:\n        return 0\n    inter = Polygon(g).intersection(Polygon(p)).area\n    union = g.area + p.area - inter\n    if union == 0:\n        return 0\n    else:\n        return inter/union\n\n\ndef weighted_merge(g, p):\n    g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8])\n    g[8] = (g[8] + p[8])\n    return g\n\n\ndef standard_nms(S, thres):\n    order = np.argsort(S[:, 8])[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n\n        inds = np.where(ovr <= thres)[0]\n        order = order[inds+1]\n\n    return S[keep]\n\n\ndef nms_locality(polys, thres=0.3):\n    '''\n    locality aware nms of EAST\n    :param polys: a N*9 numpy array. first 8 coordinates, then prob\n    :return: boxes after nms\n    '''\n    S = []\n    p = None\n    for g in polys:\n        if p is not None and intersection(g, p) > thres:\n            p = weighted_merge(g, p)\n        else:\n            if p is not None:\n                S.append(p)\n            p = g\n    if p is not None:\n        S.append(p)\n\n    if len(S) == 0:\n        return np.array([])\n    return standard_nms(np.array(S), thres)\n\n\nif __name__ == '__main__':\n    # 343,350,448,135,474,143,369,359\n    print(Polygon(np.array([[343, 350], [448, 135],\n                            [474, 143], [369, 359]])).area)\n"""
model.py,24,"b""import tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib import slim\n\ntf.app.flags.DEFINE_integer('text_scale', 512, '')\n\nfrom nets import resnet_v1\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef unpool(inputs):\n    return tf.image.resize_bilinear(inputs, size=[tf.shape(inputs)[1]*2,  tf.shape(inputs)[2]*2])\n\n\ndef mean_image_subtraction(images, means=[123.68, 116.78, 103.94]):\n    '''\n    image normalization\n    :param images:\n    :param means:\n    :return:\n    '''\n    num_channels = images.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n      raise ValueError('len(means) must match the number of channels')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=images)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n\n\ndef model(images, weight_decay=1e-5, is_training=True):\n    '''\n    define the model, we use slim's implemention of resnet\n    '''\n    images = mean_image_subtraction(images)\n\n    with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)):\n        logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50')\n\n    with tf.variable_scope('feature_fusion', values=[end_points.values]):\n        batch_norm_params = {\n        'decay': 0.997,\n        'epsilon': 1e-5,\n        'scale': True,\n        'is_training': is_training\n        }\n        with slim.arg_scope([slim.conv2d],\n                            activation_fn=tf.nn.relu,\n                            normalizer_fn=slim.batch_norm,\n                            normalizer_params=batch_norm_params,\n                            weights_regularizer=slim.l2_regularizer(weight_decay)):\n            f = [end_points['pool5'], end_points['pool4'],\n                 end_points['pool3'], end_points['pool2']]\n            for i in range(4):\n                print('Shape of f_{} {}'.format(i, f[i].shape))\n            g = [None, None, None, None]\n            h = [None, None, None, None]\n            num_outputs = [None, 128, 64, 32]\n            for i in range(4):\n                if i == 0:\n                    h[i] = f[i]\n                else:\n                    c1_1 = slim.conv2d(tf.concat([g[i-1], f[i]], axis=-1), num_outputs[i], 1)\n                    h[i] = slim.conv2d(c1_1, num_outputs[i], 3)\n                if i <= 2:\n                    g[i] = unpool(h[i])\n                else:\n                    g[i] = slim.conv2d(h[i], num_outputs[i], 3)\n                print('Shape of h_{} {}, g_{} {}'.format(i, h[i].shape, i, g[i].shape))\n\n            # here we use a slightly different way for regression part,\n            # we first use a sigmoid to limit the regression range, and also\n            # this is do with the angle map\n            F_score = slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None)\n            # 4 channel of axis aligned bbox and 1 channel rotation angle\n            geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale\n            angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45]\n            F_geometry = tf.concat([geo_map, angle_map], axis=-1)\n\n    return F_score, F_geometry\n\n\ndef dice_coefficient(y_true_cls, y_pred_cls,\n                     training_mask):\n    '''\n    dice loss\n    :param y_true_cls:\n    :param y_pred_cls:\n    :param training_mask:\n    :return:\n    '''\n    eps = 1e-5\n    intersection = tf.reduce_sum(y_true_cls * y_pred_cls * training_mask)\n    union = tf.reduce_sum(y_true_cls * training_mask) + tf.reduce_sum(y_pred_cls * training_mask) + eps\n    loss = 1. - (2 * intersection / union)\n    tf.summary.scalar('classification_dice_loss', loss)\n    return loss\n\n\n\ndef loss(y_true_cls, y_pred_cls,\n         y_true_geo, y_pred_geo,\n         training_mask):\n    '''\n    define the loss used for training, contraning two part,\n    the first part we use dice loss instead of weighted logloss,\n    the second part is the iou loss defined in the paper\n    :param y_true_cls: ground truth of text\n    :param y_pred_cls: prediction os text\n    :param y_true_geo: ground truth of geometry\n    :param y_pred_geo: prediction of geometry\n    :param training_mask: mask used in training, to ignore some text annotated by ###\n    :return:\n    '''\n    classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask)\n    # scale classification loss to match the iou loss part\n    classification_loss *= 0.01\n\n    # d1 -> top, d2->right, d3->bottom, d4->left\n    d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3)\n    d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3)\n    area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt)\n    area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred)\n    w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred)\n    h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred)\n    area_intersect = w_union * h_union\n    area_union = area_gt + area_pred - area_intersect\n    L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0))\n    L_theta = 1 - tf.cos(theta_pred - theta_gt)\n    tf.summary.scalar('geometry_AABB', tf.reduce_mean(L_AABB * y_true_cls * training_mask))\n    tf.summary.scalar('geometry_theta', tf.reduce_mean(L_theta * y_true_cls * training_mask))\n    L_g = L_AABB + 20 * L_theta\n\n    return tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss\n"""
multigpu_train.py,58,"b""import time\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\ntf.app.flags.DEFINE_integer('input_size', 512, '')\ntf.app.flags.DEFINE_integer('batch_size_per_gpu', 14, '')\ntf.app.flags.DEFINE_integer('num_readers', 16, '')\ntf.app.flags.DEFINE_float('learning_rate', 0.0001, '')\ntf.app.flags.DEFINE_integer('max_steps', 100000, '')\ntf.app.flags.DEFINE_float('moving_average_decay', 0.997, '')\ntf.app.flags.DEFINE_string('gpu_list', '1', '')\ntf.app.flags.DEFINE_string('checkpoint_path', '/tmp/east_resnet_v1_50_rbox/', '')\ntf.app.flags.DEFINE_boolean('restore', False, 'whether to resotre from checkpoint')\ntf.app.flags.DEFINE_integer('save_checkpoint_steps', 1000, '')\ntf.app.flags.DEFINE_integer('save_summary_steps', 100, '')\ntf.app.flags.DEFINE_string('pretrained_model_path', None, '')\n\nimport model\nimport icdar\n\nFLAGS = tf.app.flags.FLAGS\n\ngpus = list(range(len(FLAGS.gpu_list.split(','))))\n\n\ndef tower_loss(images, score_maps, geo_maps, training_masks, reuse_variables=None):\n    # Build inference graph\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n        f_score, f_geometry = model.model(images, is_training=True)\n\n    model_loss = model.loss(score_maps, f_score,\n                            geo_maps, f_geometry,\n                            training_masks)\n    total_loss = tf.add_n([model_loss] + tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n    # add summary\n    if reuse_variables is None:\n        tf.summary.image('input', images)\n        tf.summary.image('score_map', score_maps)\n        tf.summary.image('score_map_pred', f_score * 255)\n        tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1])\n        tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1])\n        tf.summary.image('training_masks', training_masks)\n        tf.summary.scalar('model_loss', model_loss)\n        tf.summary.scalar('total_loss', total_loss)\n\n    return total_loss, model_loss\n\n\ndef average_gradients(tower_grads):\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        grads = []\n        for g, _ in grad_and_vars:\n            expanded_g = tf.expand_dims(g, 0)\n            grads.append(expanded_g)\n\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n\n    return average_grads\n\n\ndef main(argv=None):\n    import os\n    os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list\n    if not tf.gfile.Exists(FLAGS.checkpoint_path):\n        tf.gfile.MkDir(FLAGS.checkpoint_path)\n    else:\n        if not FLAGS.restore:\n            tf.gfile.DeleteRecursively(FLAGS.checkpoint_path)\n            tf.gfile.MkDir(FLAGS.checkpoint_path)\n\n    input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images')\n    input_score_maps = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_score_maps')\n    if FLAGS.geometry == 'RBOX':\n        input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 5], name='input_geo_maps')\n    else:\n        input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 8], name='input_geo_maps')\n    input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks')\n\n    global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n    learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True)\n    # add summary\n    tf.summary.scalar('learning_rate', learning_rate)\n    opt = tf.train.AdamOptimizer(learning_rate)\n    # opt = tf.train.MomentumOptimizer(learning_rate, 0.9)\n\n\n    # split\n    input_images_split = tf.split(input_images, len(gpus))\n    input_score_maps_split = tf.split(input_score_maps, len(gpus))\n    input_geo_maps_split = tf.split(input_geo_maps, len(gpus))\n    input_training_masks_split = tf.split(input_training_masks, len(gpus))\n\n    tower_grads = []\n    reuse_variables = None\n    for i, gpu_id in enumerate(gpus):\n        with tf.device('/gpu:%d' % gpu_id):\n            with tf.name_scope('model_%d' % gpu_id) as scope:\n                iis = input_images_split[i]\n                isms = input_score_maps_split[i]\n                igms = input_geo_maps_split[i]\n                itms = input_training_masks_split[i]\n                total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables)\n                batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope))\n                reuse_variables = True\n\n                grads = opt.compute_gradients(total_loss)\n                tower_grads.append(grads)\n\n    grads = average_gradients(tower_grads)\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    summary_op = tf.summary.merge_all()\n    # save moving average\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_average_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    # batch norm updates\n    with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]):\n        train_op = tf.no_op(name='train_op')\n\n    saver = tf.train.Saver(tf.global_variables())\n    summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph())\n\n    init = tf.global_variables_initializer()\n\n    if FLAGS.pretrained_model_path is not None:\n        variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(),\n                                                             ignore_missing_vars=True)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        if FLAGS.restore:\n            print('continue training from previous checkpoint')\n            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n            saver.restore(sess, ckpt)\n        else:\n            sess.run(init)\n            if FLAGS.pretrained_model_path is not None:\n                variable_restore_op(sess)\n\n        data_generator = icdar.get_batch(num_workers=FLAGS.num_readers,\n                                         input_size=FLAGS.input_size,\n                                         batch_size=FLAGS.batch_size_per_gpu * len(gpus))\n\n        start = time.time()\n        for step in range(FLAGS.max_steps):\n            data = next(data_generator)\n            ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0],\n                                                                                input_score_maps: data[2],\n                                                                                input_geo_maps: data[3],\n                                                                                input_training_masks: data[4]})\n            if np.isnan(tl):\n                print('Loss diverged, stop training')\n                break\n\n            if step % 10 == 0:\n                avg_time_per_step = (time.time() - start)/10\n                avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start)\n                start = time.time()\n                print('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format(\n                    step, ml, tl, avg_time_per_step, avg_examples_per_second))\n\n            if step % FLAGS.save_checkpoint_steps == 0:\n                saver.save(sess, FLAGS.checkpoint_path + 'model.ckpt', global_step=global_step)\n\n            if step % FLAGS.save_summary_steps == 0:\n                _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0],\n                                                                                             input_score_maps: data[2],\n                                                                                             input_geo_maps: data[3],\n                                                                                             input_training_masks: data[4]})\n                summary_writer.add_summary(summary_str, global_step=step)\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
run_demo_server.py,6,"b'#!/usr/bin/env python3\n\nimport os\n\nimport time\nimport datetime\nimport cv2\nimport numpy as np\nimport uuid\nimport json\n\nimport functools\nimport logging\nimport collections\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\n@functools.lru_cache(maxsize=1)\ndef get_host_info():\n    ret = {}\n    with open(\'/proc/cpuinfo\') as f:\n        ret[\'cpuinfo\'] = f.read()\n\n    with open(\'/proc/meminfo\') as f:\n        ret[\'meminfo\'] = f.read()\n\n    with open(\'/proc/loadavg\') as f:\n        ret[\'loadavg\'] = f.read()\n\n    return ret\n\n\n@functools.lru_cache(maxsize=100)\ndef get_predictor(checkpoint_path):\n    logger.info(\'loading model\')\n    import tensorflow as tf\n    import model\n    from icdar import restore_rectangle\n    import lanms\n    from eval import resize_image, sort_poly, detect\n\n    input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\'input_images\')\n    global_step = tf.get_variable(\'global_step\', [], initializer=tf.constant_initializer(0), trainable=False)\n\n    f_score, f_geometry = model.model(input_images, is_training=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(0.997, global_step)\n    saver = tf.train.Saver(variable_averages.variables_to_restore())\n\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\n    ckpt_state = tf.train.get_checkpoint_state(checkpoint_path)\n    model_path = os.path.join(checkpoint_path, os.path.basename(ckpt_state.model_checkpoint_path))\n    logger.info(\'Restore from {}\'.format(model_path))\n    saver.restore(sess, model_path)\n\n    def predictor(img):\n        """"""\n        :return: {\n            \'text_lines\': [\n                {\n                    \'score\': ,\n                    \'x0\': ,\n                    \'y0\': ,\n                    \'x1\': ,\n                    ...\n                    \'y3\': ,\n                }\n            ],\n            \'rtparams\': {  # runtime parameters\n                \'image_size\': ,\n                \'working_size\': ,\n            },\n            \'timing\': {\n                \'net\': ,\n                \'restore\': ,\n                \'nms\': ,\n                \'cpuinfo\': ,\n                \'meminfo\': ,\n                \'uptime\': ,\n            }\n        }\n        """"""\n        start_time = time.time()\n        rtparams = collections.OrderedDict()\n        rtparams[\'start_time\'] = datetime.datetime.now().isoformat()\n        rtparams[\'image_size\'] = \'{}x{}\'.format(img.shape[1], img.shape[0])\n        timer = collections.OrderedDict([\n            (\'net\', 0),\n            (\'restore\', 0),\n            (\'nms\', 0)\n        ])\n\n        im_resized, (ratio_h, ratio_w) = resize_image(img)\n        rtparams[\'working_size\'] = \'{}x{}\'.format(\n            im_resized.shape[1], im_resized.shape[0])\n        start = time.time()\n        score, geometry = sess.run(\n            [f_score, f_geometry],\n            feed_dict={input_images: [im_resized[:,:,::-1]]})\n        timer[\'net\'] = time.time() - start\n\n        boxes, timer = detect(score_map=score, geo_map=geometry, timer=timer)\n        logger.info(\'net {:.0f}ms, restore {:.0f}ms, nms {:.0f}ms\'.format(\n            timer[\'net\']*1000, timer[\'restore\']*1000, timer[\'nms\']*1000))\n\n        if boxes is not None:\n            scores = boxes[:,8].reshape(-1)\n            boxes = boxes[:, :8].reshape((-1, 4, 2))\n            boxes[:, :, 0] /= ratio_w\n            boxes[:, :, 1] /= ratio_h\n\n        duration = time.time() - start_time\n        timer[\'overall\'] = duration\n        logger.info(\'[timing] {}\'.format(duration))\n\n        text_lines = []\n        if boxes is not None:\n            text_lines = []\n            for box, score in zip(boxes, scores):\n                box = sort_poly(box.astype(np.int32))\n                if np.linalg.norm(box[0] - box[1]) < 5 or np.linalg.norm(box[3]-box[0]) < 5:\n                    continue\n                tl = collections.OrderedDict(zip(\n                    [\'x0\', \'y0\', \'x1\', \'y1\', \'x2\', \'y2\', \'x3\', \'y3\'],\n                    map(float, box.flatten())))\n                tl[\'score\'] = float(score)\n                text_lines.append(tl)\n        ret = {\n            \'text_lines\': text_lines,\n            \'rtparams\': rtparams,\n            \'timing\': timer,\n        }\n        ret.update(get_host_info())\n        return ret\n\n\n    return predictor\n\n\n### the webserver\nfrom flask import Flask, request, render_template\nimport argparse\n\n\nclass Config:\n    SAVE_DIR = \'static/results\'\n\n\nconfig = Config()\n\n\napp = Flask(__name__)\n\n@app.route(\'/\')\ndef index():\n    return render_template(\'index.html\', session_id=\'dummy_session_id\')\n\n\ndef draw_illu(illu, rst):\n    for t in rst[\'text_lines\']:\n        d = np.array([t[\'x0\'], t[\'y0\'], t[\'x1\'], t[\'y1\'], t[\'x2\'],\n                      t[\'y2\'], t[\'x3\'], t[\'y3\']], dtype=\'int32\')\n        d = d.reshape(-1, 2)\n        cv2.polylines(illu, [d], isClosed=True, color=(255, 255, 0))\n    return illu\n\n\ndef save_result(img, rst):\n    session_id = str(uuid.uuid1())\n    dirpath = os.path.join(config.SAVE_DIR, session_id)\n    os.makedirs(dirpath)\n\n    # save input image\n    output_path = os.path.join(dirpath, \'input.png\')\n    cv2.imwrite(output_path, img)\n\n    # save illustration\n    output_path = os.path.join(dirpath, \'output.png\')\n    cv2.imwrite(output_path, draw_illu(img.copy(), rst))\n\n    # save json data\n    output_path = os.path.join(dirpath, \'result.json\')\n    with open(output_path, \'w\') as f:\n        json.dump(rst, f)\n\n    rst[\'session_id\'] = session_id\n    return rst\n\n\n\ncheckpoint_path = \'./east_icdar2015_resnet_v1_50_rbox\'\n\n\n@app.route(\'/\', methods=[\'POST\'])\ndef index_post():\n    global predictor\n    import io\n    bio = io.BytesIO()\n    request.files[\'image\'].save(bio)\n    img = cv2.imdecode(np.frombuffer(bio.getvalue(), dtype=\'uint8\'), 1)\n    rst = get_predictor(checkpoint_path)(img)\n\n    save_result(img, rst)\n    return render_template(\'index.html\', session_id=rst[\'session_id\'])\n\n\ndef main():\n    global checkpoint_path\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--port\', default=8769, type=int)\n    parser.add_argument(\'--checkpoint_path\', default=checkpoint_path)\n    args = parser.parse_args()\n    checkpoint_path = args.checkpoint_path\n\n    if not os.path.exists(args.checkpoint_path):\n        raise RuntimeError(\n            \'Checkpoint `{}` not found\'.format(args.checkpoint_path))\n\n    app.debug = False  # change this to True if you want to debug\n    app.run(\'0.0.0.0\', args.port)\n\nif __name__ == \'__main__\':\n    main()\n\n'"
lanms/.ycm_extra_conf.py,0,"b'#!/usr/bin/env python\n#\n# Copyright (C) 2014  Google Inc.\n#\n# This file is part of YouCompleteMe.\n#\n# YouCompleteMe is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# YouCompleteMe is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport sys\nimport glob\nimport ycm_core\n\n# These are the compilation flags that will be used in case there\'s no\n# compilation database set (by default, one is not set).\n# CHANGE THIS LIST OF FLAGS. YES, THIS IS THE DROID YOU HAVE BEEN LOOKING FOR.\nsys.path.append(os.path.dirname(__file__))\n\n\nBASE_DIR = os.path.dirname(os.path.realpath(__file__))\n\nfrom plumbum.cmd import python_config\n\n\nflags = [\n    \'-Wall\',\n    \'-Wextra\',\n    \'-Wnon-virtual-dtor\',\n    \'-Winvalid-pch\',\n    \'-Wno-unused-local-typedefs\',\n    \'-std=c++11\',\n    \'-x\', \'c++\',\n    \'-Iinclude\',\n] + python_config(\'--cflags\').split()\n\n\n# Set this to the absolute path to the folder (NOT the file!) containing the\n# compile_commands.json file to use that instead of \'flags\'. See here for\n# more details: http://clang.llvm.org/docs/JSONCompilationDatabase.html\n#\n# Most projects will NOT need to set this to anything; you can just change the\n# \'flags\' list of compilation flags.\ncompilation_database_folder = \'\'\n\nif os.path.exists( compilation_database_folder ):\n  database = ycm_core.CompilationDatabase( compilation_database_folder )\nelse:\n  database = None\n\nSOURCE_EXTENSIONS = [ \'.cpp\', \'.cxx\', \'.cc\', \'.c\', \'.m\', \'.mm\' ]\n\ndef DirectoryOfThisScript():\n  return os.path.dirname( os.path.abspath( __file__ ) )\n\n\ndef MakeRelativePathsInFlagsAbsolute( flags, working_directory ):\n  if not working_directory:\n    return list( flags )\n  new_flags = []\n  make_next_absolute = False\n  path_flags = [ \'-isystem\', \'-I\', \'-iquote\', \'--sysroot=\' ]\n  for flag in flags:\n    new_flag = flag\n\n    if make_next_absolute:\n      make_next_absolute = False\n      if not flag.startswith( \'/\' ):\n        new_flag = os.path.join( working_directory, flag )\n\n    for path_flag in path_flags:\n      if flag == path_flag:\n        make_next_absolute = True\n        break\n\n      if flag.startswith( path_flag ):\n        path = flag[ len( path_flag ): ]\n        new_flag = path_flag + os.path.join( working_directory, path )\n        break\n\n    if new_flag:\n      new_flags.append( new_flag )\n  return new_flags\n\n\ndef IsHeaderFile( filename ):\n  extension = os.path.splitext( filename )[ 1 ]\n  return extension in [ \'.h\', \'.hxx\', \'.hpp\', \'.hh\' ]\n\n\ndef GetCompilationInfoForFile( filename ):\n  # The compilation_commands.json file generated by CMake does not have entries\n  # for header files. So we do our best by asking the db for flags for a\n  # corresponding source file, if any. If one exists, the flags for that file\n  # should be good enough.\n  if IsHeaderFile( filename ):\n    basename = os.path.splitext( filename )[ 0 ]\n    for extension in SOURCE_EXTENSIONS:\n      replacement_file = basename + extension\n      if os.path.exists( replacement_file ):\n        compilation_info = database.GetCompilationInfoForFile(\n          replacement_file )\n        if compilation_info.compiler_flags_:\n          return compilation_info\n    return None\n  return database.GetCompilationInfoForFile( filename )\n\n\n# This is the entry point; this function is called by ycmd to produce flags for\n# a file.\ndef FlagsForFile( filename, **kwargs ):\n  if database:\n    # Bear in mind that compilation_info.compiler_flags_ does NOT return a\n    # python list, but a ""list-like"" StringVec object\n    compilation_info = GetCompilationInfoForFile( filename )\n    if not compilation_info:\n      return None\n\n    final_flags = MakeRelativePathsInFlagsAbsolute(\n      compilation_info.compiler_flags_,\n      compilation_info.compiler_working_dir_ )\n  else:\n    relative_to = DirectoryOfThisScript()\n    final_flags = MakeRelativePathsInFlagsAbsolute( flags, relative_to )\n\n  return {\n    \'flags\': final_flags,\n    \'do_cache\': True\n  }\n\n'"
lanms/__init__.py,0,"b""import subprocess\nimport os\nimport numpy as np\n\nBASE_DIR = os.path.dirname(os.path.realpath(__file__))\n\nif subprocess.call(['make', '-C', BASE_DIR]) != 0:  # return value\n    raise RuntimeError('Cannot compile lanms: {}'.format(BASE_DIR))\n\n\ndef merge_quadrangle_n9(polys, thres=0.3, precision=10000):\n    from .adaptor import merge_quadrangle_n9 as nms_impl\n    if len(polys) == 0:\n        return np.array([], dtype='float32')\n    p = polys.copy()\n    p[:,:8] *= precision\n    ret = np.array(nms_impl(p, thres), dtype='float32')\n    ret[:,:8] /= precision\n    return ret\n\n"""
lanms/__main__.py,0,"b""import numpy as np\n\n\nfrom . import merge_quadrangle_n9\n\nif __name__ == '__main__':\n    # unit square with confidence 1\n    q = np.array([0, 0, 0, 1, 1, 1, 1, 0, 1], dtype='float32')\n\n    print(merge_quadrangle_n9(np.array([q, q + 0.1, q + 2])))\n"""
nets/__init__.py,0,b''
nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\n\n\n\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef subsample(inputs, factor, scope=None):\n    """"""Subsamples the input along the spatial dimensions.\n\n    Args:\n      inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n      factor: The subsampling factor.\n      scope: Optional variable_scope.\n\n    Returns:\n      output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n        input, either intact (if factor == 1) or subsampled (if factor > 1).\n    """"""\n    if factor == 1:\n        return inputs\n    else:\n        return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n    """"""Strided 2-D convolution with \'SAME\' padding.\n\n    When stride > 1, then we do explicit zero-padding, followed by conv2d with\n    \'VALID\' padding.\n\n    Note that\n\n       net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n    is equivalent to\n\n       net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n       net = subsample(net, factor=stride)\n\n    whereas\n\n       net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n    is different when the input\'s height or width is even, which is why we add the\n    current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n    Args:\n      inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n      num_outputs: An integer, the number of output filters.\n      kernel_size: An int with the kernel_size of the filters.\n      stride: An integer, the output stride.\n      rate: An integer, rate for atrous convolution.\n      scope: Scope.\n\n    Returns:\n      output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n        the convolution output.\n    """"""\n    if stride == 1:\n        return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                           padding=\'SAME\', scope=scope)\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tf.pad(inputs,\n                        [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n        return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                           rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n    """"""Stacks ResNet `Blocks` and controls output feature density.\n\n    First, this function creates scopes for the ResNet in the form of\n    \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n    Second, this function allows the user to explicitly control the ResNet\n    output_stride, which is the ratio of the input to output spatial resolution.\n    This is useful for dense prediction tasks such as semantic segmentation or\n    object detection.\n\n    Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n    factor of 2 when transitioning between consecutive ResNet blocks. This results\n    to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n    half the nominal network stride (e.g., output_stride=4), then we compute\n    responses twice.\n\n    Control of the output feature density is implemented by atrous convolution.\n\n    Args:\n      net: A `Tensor` of size [batch, height, width, channels].\n      blocks: A list of length equal to the number of ResNet `Blocks`. Each\n        element is a ResNet `Block` object describing the units in the `Block`.\n      output_stride: If `None`, then the output will be computed at the nominal\n        network stride. If output_stride is not `None`, it specifies the requested\n        ratio of input to output spatial resolution, which needs to be equal to\n        the product of unit strides from the start up to some level of the ResNet.\n        For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n        then valid values for the output_stride are 1, 2, 6, 24 or None (which\n        is equivalent to output_stride=24).\n      outputs_collections: Collection to add the ResNet block outputs.\n\n    Returns:\n      net: Output tensor with stride equal to the specified output_stride.\n\n    Raises:\n      ValueError: If the target output_stride is not valid.\n    """"""\n    # The current_stride variable keeps track of the effective stride of the\n    # activations. This allows us to invoke atrous convolution whenever applying\n    # the next residual unit would result in the activations having stride larger\n    # than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    for block in blocks:\n        with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n            for i, unit in enumerate(block.args):\n                if output_stride is not None and current_stride > output_stride:\n                    raise ValueError(\'The target output_stride cannot be reached.\')\n\n                with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n                    unit_depth, unit_depth_bottleneck, unit_stride = unit\n                    # If we have reached the target output_stride, then we need to employ\n                    # atrous convolution with stride=1 and multiply the atrous rate by the\n                    # current unit\'s stride for use in subsequent layers.\n                    if output_stride is not None and current_stride == output_stride:\n                        net = block.unit_fn(net,\n                                            depth=unit_depth,\n                                            depth_bottleneck=unit_depth_bottleneck,\n                                            stride=1,\n                                            rate=rate)\n                        rate *= unit_stride\n\n                    else:\n                        net = block.unit_fn(net,\n                                            depth=unit_depth,\n                                            depth_bottleneck=unit_depth_bottleneck,\n                                            stride=unit_stride,\n                                            rate=1)\n                        current_stride *= unit_stride\n            print(sc.name, net.shape)\n            net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n    if output_stride is not None and current_stride != output_stride:\n        raise ValueError(\'The target output_stride cannot be reached.\')\n\n    return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n    """"""Defines the default ResNet arg scope.\n\n    TODO(gpapan): The batch-normalization related default values above are\n      appropriate for use in conjunction with the reference ResNet models\n      released at https://github.com/KaimingHe/deep-residual-networks. When\n      training ResNets from scratch, they might need to be tuned.\n\n    Args:\n      weight_decay: The weight decay to use for regularizing the model.\n      batch_norm_decay: The moving average decay when estimating layer activation\n        statistics in batch normalization.\n      batch_norm_epsilon: Small constant to prevent division by zero when\n        normalizing activations by their variance in batch normalization.\n      batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n        activations in the batch normalization layer.\n\n    Returns:\n      An `arg_scope` to use for the resnet models.\n    """"""\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n        \'scale\': batch_norm_scale,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n    }\n\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n            # The following implies padding=\'SAME\' for pool1, which makes feature\n            # alignment easier for dense prediction tasks. This is also used in\n            # https://github.com/facebook/fb.resnet.torch. However the accompanying\n            # code of \'Deep Residual Learning for Image Recognition\' uses\n            # padding=\'VALID\' for pool1. You can switch to that choice by setting\n            # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n            with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n                return arg_sc\n'"
nets/resnet_v1.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\nfrom . import resnet_utils\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n    """"""Bottleneck residual unit variant with BN after convolutions.\n\n    This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n    its definition. Note that we use here the bottleneck variant which has an\n    extra bottleneck layer.\n\n    When putting together two consecutive ResNet blocks that use this unit, one\n    should use stride = 2 in the last unit of the first block.\n\n    Args:\n      inputs: A tensor of size [batch, height, width, channels].\n      depth: The depth of the ResNet unit output.\n      depth_bottleneck: The depth of the bottleneck layers.\n      stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n        the units output compared to its input.\n      rate: An integer, rate for atrous convolution.\n      outputs_collections: Collection to add the ResNet unit output.\n      scope: Optional variable_scope.\n\n    Returns:\n      The ResNet unit\'s output.\n    """"""\n    with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                                   activation_fn=None, scope=\'shortcut\')\n\n        residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                               scope=\'conv1\')\n        residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                            rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                               activation_fn=None, scope=\'conv3\')\n\n        output = tf.nn.relu(shortcut + residual)\n\n        return slim.utils.collect_named_outputs(outputs_collections,\n                                                sc.original_name_scope,\n                                                output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n    """"""Generator for v1 ResNet models.\n\n    This function generates a family of ResNet v1 models. See the resnet_v1_*()\n    methods for specific model instantiations, obtained by selecting different\n    block instantiations that produce ResNets of various depths.\n\n    Training for image classification on Imagenet is usually done with [224, 224]\n    inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n    block for the ResNets defined in [1] that have nominal stride equal to 32.\n    However, for dense prediction tasks we advise that one uses inputs with\n    spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n    this case the feature maps at the ResNet output will have spatial shape\n    [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n    and corners exactly aligned with the input image corners, which greatly\n    facilitates alignment of the features to the image. Using as input [225, 225]\n    images results in [8, 8] feature maps at the output of the last ResNet block.\n\n    For dense prediction tasks, the ResNet needs to run in fully-convolutional\n    (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n    have nominal stride equal to 32 and a good choice in FCN mode is to use\n    output_stride=16 in order to increase the density of the computed features at\n    small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n    Args:\n      inputs: A tensor of size [batch, height_in, width_in, channels].\n      blocks: A list of length equal to the number of ResNet blocks. Each element\n        is a resnet_utils.Block object describing the units in the block.\n      num_classes: Number of predicted classes for classification tasks. If None\n        we return the features before the logit layer.\n      is_training: whether is training or not.\n      global_pool: If True, we perform global average pooling before computing the\n        logits. Set to True for image classification, False for dense prediction.\n      output_stride: If None, then the output will be computed at the nominal\n        network stride. If output_stride is not None, it specifies the requested\n        ratio of input to output spatial resolution.\n      include_root_block: If True, include the initial convolution followed by\n        max-pooling, if False excludes it.\n      spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n          of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n\n    Returns:\n      net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n        If global_pool is False, then height_out and width_out are reduced by a\n        factor of output_stride compared to the respective height_in and width_in,\n        else both height_out and width_out equal one. If num_classes is None, then\n        net is the output of the last ResNet block, potentially after global\n        average pooling. If num_classes is not None, net contains the pre-softmax\n        activations.\n      end_points: A dictionary from components of the network to the corresponding\n        activation.\n\n    Raises:\n      ValueError: If the target output_stride is not valid.\n    """"""\n    with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.name + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck,\n                             resnet_utils.stack_blocks_dense],\n                            outputs_collections=end_points_collection):\n            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n                net = inputs\n                if include_root_block:\n                    if output_stride is not None:\n                        if output_stride % 4 != 0:\n                            raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                        output_stride /= 4\n                    net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n\n                    net = slim.utils.collect_named_outputs(end_points_collection, \'pool2\', net)\n\n                net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n\n                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n                # end_points[\'pool2\'] = end_points[\'resnet_v1_50/pool1/MaxPool:0\']\n                try:\n                    end_points[\'pool3\'] = end_points[\'resnet_v1_50/block1\']\n                    end_points[\'pool4\'] = end_points[\'resnet_v1_50/block2\']\n                except:\n                    end_points[\'pool3\'] = end_points[\'Detection/resnet_v1_50/block1\']\n                    end_points[\'pool4\'] = end_points[\'Detection/resnet_v1_50/block2\']\n                end_points[\'pool5\'] = net\n                # if global_pool:\n                #     # Global average pooling.\n                #     net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n                # if num_classes is not None:\n                #     net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                #                       normalizer_fn=None, scope=\'logits\')\n                # if spatial_squeeze:\n                #     logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n                # else:\n                #     logits = net\n                # # Convert end_points_collection into a dictionary of end_points.\n                # end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n                # if num_classes is not None:\n                #     end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n                return net, end_points\n\n\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n    ]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n    return resnet_v1(inputs, blocks, num_classes, is_training,\n                     global_pool=global_pool, output_stride=output_stride,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n\n\nif __name__ == \'__main__\':\n    input = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name=\'input\')\n    with slim.arg_scope(resnet_arg_scope()) as sc:\n        logits = resnet_v1_50(input)'"
