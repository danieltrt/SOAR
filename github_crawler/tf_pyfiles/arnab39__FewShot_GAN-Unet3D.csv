file_path,api_count,code
lib/operations.py,74,"b'import tensorflow as tf\n\nF = tf.flags.FLAGS\n\n\ndef gaussian_nll(mu, log_sigma, noise):\n    NLL = tf.reduce_sum(log_sigma, 1) + \\\n              tf.reduce_sum(((noise - mu)/(1e-8 + tf.exp(log_sigma)))**2,1)/2.\n    return tf.reduce_mean(NLL)\n\n\ndef conv3d(input_, output_dim,k_d=3, k_h=3, k_w=3, \n                  s_d=1, s_h=1, s_w=1, stddev=0.05, name=""conv3d""):\n  with tf.variable_scope(name):\n    w = tf.get_variable(\'w\', [k_d, k_h, k_w, input_.get_shape()[-1], output_dim], \n                              initializer=tf.truncated_normal_initializer(stddev=stddev))\n    conv = tf.nn.conv3d(input_, w, strides=[1, s_d, s_h, s_w, 1], padding=\'SAME\')\n    biases = tf.get_variable(\'biases\', [output_dim], \n                                    initializer=tf.constant_initializer(0.0))\n    conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n    return conv\n\n\ndef deconv3d(input_, output_shape,k_d=2, k_h=2, k_w=2, \n                s_d=2, s_h=2, s_w=2, stddev=0.05, name=""deconv3d""):\n  with tf.variable_scope(name):\n    w = tf.get_variable(\'w\', [k_d, k_h, k_w, output_shape[-1], input_.get_shape()[-1]], \n                                    initializer=tf.random_normal_initializer(stddev=stddev))\n    deconv = tf.nn.conv3d_transpose(input_, w, output_shape=output_shape, \n                                          strides=[1, s_d, s_h, s_w, 1], padding=""SAME"")\n    biases = tf.get_variable(\'biases\', [output_shape[-1]], \n                                            initializer=tf.constant_initializer(0.0))\n    deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n    return deconv\n\n\ndef relu(x, name=""relu""):\n  return tf.nn.relu(x)\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n  return tf.maximum(x, leak*x)\n\ndef max_pool3D(input_,k_d=2, k_h=2, k_w=2, s_d=2, s_h=2, s_w=2):\n  return tf.nn.max_pool3d(input_,[1, k_d, k_h, k_w, 1],strides=[1, s_d, s_h, s_w, 1] , padding=\'SAME\')\n\ndef avg_pool3D(input_,k_d=2, k_h=2, k_w=2, s_d=2, s_h=2, s_w=2):\n  return tf.nn.avg_pool3d(input_,[1, k_d, k_h, k_w, 1],strides=[1, s_d, s_h, s_w, 1] , padding=\'SAME\')\n\n\ndef linear(input_, output_size, scope=None, stddev=0.05, bias_start=0.0):\n  shape = input_.get_shape().as_list()\n  with tf.variable_scope(scope or ""Linear""):\n    matrix = tf.get_variable(""Matrix"", [shape[1], output_size], tf.float32,\n                 tf.random_normal_initializer(stddev=stddev))\n    bias = tf.get_variable(""bias"", [output_size],\n                  initializer=tf.constant_initializer(bias_start))\n    return tf.matmul(input_, matrix) + bias\n\n\n\ndef instance_norm(x,phase=False,name=""instance_norm""):\n  epsilon = 1e-9\n  mean, var = tf.nn.moments(x, [1, 2, 3], keep_dims=True)\n  return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, epsilon)))\n\nclass batch_norm(object):\n  def __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n    with tf.variable_scope(name):\n      self.epsilon  = epsilon\n      self.momentum = momentum\n      self.name = name\n\n  def __call__(self, x, train=True):\n    return tf.contrib.layers.batch_norm(x,\n                      decay=self.momentum, \n                      updates_collections=None,\n                      epsilon=self.epsilon,\n                      scale=True,\n                      is_training=train,\n                      scope=self.name)\n\ndef int_shape(x):\n  return list(map(int, x.get_shape()))\n\ndef get_var_maybe_avg(var_name, ema, **kwargs):\n    \'\'\' utility for retrieving polyak averaged params \'\'\'\n    v = tf.get_variable(var_name, **kwargs)\n    if ema is not None:\n        v = ema.average(v)\n    return v\n\ndef conv3d_WN(x, num_filters, filter_size=[3,3,3], stride=[1,1,1], pad=\'SAME\', init_scale=1., name=""conv_WN"", init=False, ema=None, **kwargs):\n    \'\'\' convolutional layer \'\'\'\n    with tf.variable_scope(name):\n        V = get_var_maybe_avg(\'V\', ema, shape=filter_size+[int(x.get_shape()[-1]),num_filters], dtype=tf.float32,\n                              initializer=tf.random_normal_initializer(0, 0.05), trainable=True)\n        g = get_var_maybe_avg(\'g\', ema, shape=[num_filters], dtype=tf.float32,\n                              initializer=tf.constant_initializer(1.), trainable=True)\n        b = get_var_maybe_avg(\'b\', ema, shape=[num_filters], dtype=tf.float32,\n                              initializer=tf.constant_initializer(0.), trainable=True)\n\n        # use weight normalization (Salimans & Kingma, 2016)\n        W = tf.reshape(g, [1, 1, 1, 1, num_filters]) * tf.nn.l2_normalize(V, [0, 1, 2, 3])\n\n        # calculate convolutional layer output\n        x = tf.nn.bias_add(tf.nn.conv3d(x, W, [1] + stride + [1], pad), b)\n\n        if init:  # normalize x\n            m_init, v_init = tf.nn.moments(x, [0,1,2,3])\n            scale_init = init_scale / tf.sqrt(v_init + 1e-10)\n            with tf.control_dependencies([g.assign(g * scale_init), b.assign_add(-m_init * scale_init)]):\n                x = tf.identity(x)\n        return x\n\ndef deconv3d_WN(x, num_filters, filter_size=[2,2,2], stride=[2,2,2], pad=\'SAME\', init_scale=1., name=""deconv_WN"", init=False, ema=None, **kwargs):\n    \'\'\' transposed convolutional layer \'\'\'\n    xs = int_shape(x)\n    if pad==\'SAME\':\n        target_shape = [xs[0], xs[1]*stride[0], xs[2]*stride[1], xs[3]*stride[2], num_filters]\n    else:\n        target_shape = [xs[0], xs[1]*stride[0] + filter_size[0]-1, xs[2]*stride[1] + filter_size[1]-1,xs[3]*stride[2] + filter_size[2]-1, num_filters]\n    with tf.variable_scope(name):\n        V = get_var_maybe_avg(\'V\', ema, shape=filter_size+[num_filters,int(x.get_shape()[-1])], dtype=tf.float32,\n                              initializer=tf.random_normal_initializer(0, 0.05), trainable=True)\n        g = get_var_maybe_avg(\'g\', ema, shape=[num_filters], dtype=tf.float32,\n                              initializer=tf.constant_initializer(1.), trainable=True)\n        b = get_var_maybe_avg(\'b\', ema, shape=[num_filters], dtype=tf.float32,\n                              initializer=tf.constant_initializer(0.), trainable=True)\n\n        # use weight normalization (Salimans & Kingma, 2016)\n        W = tf.reshape(g, [1, 1, 1, num_filters, 1]) * tf.nn.l2_normalize(V, [0, 1, 2, 4])\n\n        # calculate convolutional layer output\n        x = tf.nn.conv3d_transpose(x, W, target_shape, [1] + stride + [1], padding=pad)\n        x = tf.nn.bias_add(x, b)\n\n        if init:  # normalize x\n            m_init, v_init = tf.nn.moments(x, [0,1,2,3])\n            scale_init = init_scale / tf.sqrt(v_init + 1e-10)\n            with tf.control_dependencies([g.assign(g * scale_init), b.assign_add(-m_init * scale_init)]):\n                x = tf.identity(x)\n        return x\n\ndef linear_WN(x, num_units, name=""linear_WN"", init_scale=1., init=False, ema=None, **kwargs):\n    \'\'\' fully connected layer \'\'\'\n    with tf.variable_scope(name):\n        V = get_var_maybe_avg(\'V\', ema, shape=[int(x.get_shape()[1]),num_units], dtype=tf.float32,\n                              initializer=tf.random_normal_initializer(0, 0.05), trainable=True)\n        g = get_var_maybe_avg(\'g\', ema, shape=[num_units], dtype=tf.float32,\n                              initializer=tf.constant_initializer(1.), trainable=True)\n        b = get_var_maybe_avg(\'b\', ema, shape=[num_units], dtype=tf.float32,\n                              initializer=tf.constant_initializer(0.), trainable=True)\n\n        # use weight normalization (Salimans & Kingma, 2016)\n        x = tf.matmul(x, V)\n        scaler = g / tf.sqrt(tf.reduce_sum(tf.square(V), [0]))\n        x = tf.reshape(scaler, [1, num_units]) * x + tf.reshape(b, [1, num_units])\n\n        if init: # normalize x\n            m_init, v_init = tf.nn.moments(x, [0])\n            scale_init = init_scale/tf.sqrt(v_init + 1e-10)\n            with tf.control_dependencies([g.assign(g*scale_init), b.assign_add(-m_init*scale_init)]):\n                x = tf.identity(x)\n        return x'"
lib/utils.py,2,"b'import os\nimport numpy as np\nimport tensorflow as tf\n#import pdb\n\nF = tf.app.flags.FLAGS\n\n\n""""""\nSave tensorflow model\nParameters:\n* checkpoint_dir - name of the directory where model is to be saved\n* sess - current tensorflow session\n* saver - tensorflow saver\n""""""\ndef save_model(checkpoint_dir, sess, saver):\n  model_name = ""model.ckpt""\n  if not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n  saver.save(sess, os.path.join(checkpoint_dir, model_name))\n\n\n""""""\nLoad tensorflow model\nParameters:\n* checkpoint_dir - name of the directory where model is to be loaded from\n* sess - current tensorflow session\n* saver - tensorflow saver\nReturns: True if the model loaded successfully, else False\n""""""\ndef load_model(checkpoint_dir, sess, saver):\n  print("" [*] Reading checkpoints..."")\n  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n  if ckpt and ckpt.model_checkpoint_path:\n    ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n    saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n    return True\n  else:\n    return False\n\n""""""\nTo recompose an array of 3D images from patches\n""""""\ndef recompose3D_overlap(preds, img_h, img_w, img_d, stride_h, stride_w, stride_d):\n  patch_h = preds.shape[1]\n  patch_w = preds.shape[2]\n  patch_d = preds.shape[3]\n  N_patches_h = (img_h-patch_h)//stride_h+1\n  N_patches_w = (img_w-patch_w)//stride_w+1\n  N_patches_d = (img_d-patch_d)//stride_d+1\n  N_patches_img = N_patches_h * N_patches_w * N_patches_d\n  print(""N_patches_h: "" ,N_patches_h)\n  print(""N_patches_w: "" ,N_patches_w)\n  print(""N_patches_d: "" ,N_patches_d)\n  print(""N_patches_img: "",N_patches_img)\n  assert(preds.shape[0]%N_patches_img==0)\n  N_full_imgs = preds.shape[0]//N_patches_img\n  print(""According to the dimension inserted, there are "" \\\n          +str(N_full_imgs) +"" full images (of "" +str(img_h)+""x"" +str(img_w)+""x"" +str(img_d) +"" each)"")\n  # itialize to zero mega array with sum of Probabilities\n  raw_pred_martrix = np.zeros((N_full_imgs,img_h,img_w,img_d)) \n  raw_sum = np.zeros((N_full_imgs,img_h,img_w,img_d))\n  final_matrix = np.zeros((N_full_imgs,img_h,img_w,img_d),dtype=\'uint16\')\n\n  k = 0 \n  # iterator over all the patches\n  for i in range(N_full_imgs):\n    for h in range((img_h-patch_h)//stride_h+1):\n      for w in range((img_w-patch_w)//stride_w+1):\n        for d in range((img_d-patch_d)//stride_d+1):\n          raw_pred_martrix[i,h*stride_h:(h*stride_h)+patch_h,\\\n                                w*stride_w:(w*stride_w)+patch_w,\\\n                                  d*stride_d:(d*stride_d)+patch_d]+=preds[k]\n          raw_sum[i,h*stride_h:(h*stride_h)+patch_h,\\\n                          w*stride_w:(w*stride_w)+patch_w,\\\n                            d*stride_d:(d*stride_d)+patch_d]+=1.0\n          k+=1\n  assert(k==preds.shape[0])\n  #To check for non zero sum matrix\n  assert(np.min(raw_sum)>=1.0)\n  final_matrix = np.around(raw_pred_martrix/raw_sum)\n  return final_matrix'"
preprocess/preprocess.py,1,"b'import os\nimport glob\nimport nibabel as nib\nimport numpy as np\nimport SimpleITK as sitk\nimport shutil\nfrom nipype.interfaces.ants import N4BiasFieldCorrection\nfrom sklearn.feature_extraction.image import extract_patches as sk_extract_patches\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nimport scipy.misc\n#import pdb\n\nF = tf.app.flags.FLAGS\n\n\nseed = 7\nnp.random.seed(seed)\n\nall_modalities={\'T1\',\'T2\'}\n\n\ndef get_filename(set_name, case_idx, input_name, loc):\n    pattern = \'{0}/{1}/{3}/subject-{2}-{3}.nii\'\n    return pattern.format(loc, set_name, case_idx, input_name)\n\ndef get_set_name(case_idx):\n    return \'Training\' if case_idx < 11 else \'Testing\'\n\ndef read_data(case_idx, input_name, loc):\n    set_name = get_set_name(case_idx)\n\n    image_path = get_filename(set_name, case_idx, input_name, loc)\n    print(image_path)\n\n    return nib.load(image_path)\n\ndef read_vol(case_idx, input_name, dir):\n    image_data = read_data(case_idx, input_name, dir)\n    return image_data.get_data()\n\ndef correct_bias(in_file, out_file):\n    correct = N4BiasFieldCorrection()\n    correct.inputs.input_image = in_file\n    correct.inputs.output_image = out_file\n    done = correct.run()\n    return done.outputs.output_image\n\ndef normalise(case_idx, input_name, in_dir, out_dir,copy=False):\n\tset_name = get_set_name(case_idx)\n\timage_in_path = get_filename(set_name, case_idx, input_name, in_dir)\n\timage_out_path = get_filename(set_name, case_idx, input_name, out_dir)\n\tif copy:\n\t\tshutil.copy(image_in_path, image_out_path)\n\telse:\n\t\tcorrect_bias(image_in_path, image_out_path)\n\tprint(image_in_path + "" done."")\n\n\n\n""""""\nTo extract patches from a 3D image\n""""""\ndef extract_patches(volume, patch_shape, extraction_step,datype=\'float32\'):\n  patch_h, patch_w, patch_d = patch_shape[0], patch_shape[1], patch_shape[2]\n  stride_h, stride_w, stride_d = extraction_step[0], extraction_step[1], extraction_step[2]\n  img_h, img_w, img_d = volume.shape[0],volume.shape[1],volume.shape[2]\n  N_patches_h = (img_h-patch_h)//stride_h+1\n  N_patches_w = (img_w-patch_w)//stride_w+1\n  N_patches_d = (img_d-patch_d)//stride_d+1\n  N_patches_img = N_patches_h * N_patches_w * N_patches_d\n  raw_patch_martrix = np.zeros((N_patches_img,patch_h,patch_w,patch_d),dtype=datype)\n  k=0\n\n  #iterator over all the patches\n  for h in range((img_h-patch_h)//stride_h+1):\n    for w in range((img_w-patch_w)//stride_w+1):\n      for d in range((img_d-patch_d)//stride_d+1):\n        raw_patch_martrix[k]=volume[h*stride_h:(h*stride_h)+patch_h,\\\n                            \t\t\tw*stride_w:(w*stride_w)+patch_w,\\\n                              \t\t\t\td*stride_d:(d*stride_d)+patch_d]\n        k+=1\n  assert(k==N_patches_img)\n  return raw_patch_martrix\n\n""""""\nTo extract labeled patches from array of 3D labeled images\n""""""\ndef get_patches_lab(T1_vols, T2_vols, label_vols, extraction_step,\n                    patch_shape,validating,testing,num_images_training):\n    patch_shape_1d=patch_shape[0]\n    # Extract patches from input volumes and ground truth\n    x = np.zeros((0, patch_shape_1d, patch_shape_1d, patch_shape_1d, 2),dtype=""float32"")\n    y = np.zeros((0, patch_shape_1d, patch_shape_1d, patch_shape_1d),dtype=""uint8"")\n    for idx in range(len(T1_vols)) :\n        y_length = len(y)\n        if testing:\n            print((""Extracting Patches from Image %2d ...."")%(num_images_training+idx+2))\n        elif validating:\n            print((""Extracting Patches from Image %2d ...."")%(num_images_training+idx+1))\n        else:\n            print((""Extracting Patches from Image %2d ...."")%(1+idx))\n        label_patches = extract_patches(label_vols[idx], patch_shape, extraction_step,\n        \t\t\t\t\t\t\t\t\t\t\t\t\t\tdatype=""uint8"")\n\n        # Select only those who are important for processing\n        if testing or validating:\n            valid_idxs = np.where(np.sum(label_patches, axis=(1, 2, 3)) != -1)\n        else:\n            valid_idxs = np.where(np.count_nonzero(label_patches, axis=(1, 2, 3)) > 6000)\n\n        # Filtering extracted patches\n        label_patches = label_patches[valid_idxs]\n\n        x = np.vstack((x, np.zeros((len(label_patches), patch_shape_1d, \n                                                patch_shape_1d, patch_shape_1d, 2),dtype=""float32"")))\n        y = np.vstack((y, np.zeros((len(label_patches), patch_shape_1d, \n                                                patch_shape_1d, patch_shape_1d),dtype=""uint8"")))\n\n        y[y_length:, :, :, :] = label_patches\n\n        # Sampling strategy: reject samples which labels are mostly 0 and have less than 6000 nonzero elements\n        T1_train = extract_patches(T1_vols[idx], patch_shape, extraction_step,datype=""float32"")\n        x[y_length:, :, :, :, 0] = T1_train[valid_idxs]\n        \n        # Sampling strategy: reject samples which labels are mostly 0 and have less than 6000 nonzero elements\n        T2_train = extract_patches(T2_vols[idx], patch_shape, extraction_step,datype=""float32"")\n        x[y_length:, :, :, :, 1] = T2_train[valid_idxs]\n    return x, y\n\n""""""\nTo preprocess the labeled training data\n""""""\ndef preprocess_dynamic_lab(dir,num_classes, extraction_step,patch_shape,num_images_training=2,\n                                validating=False,testing=False,num_images_testing=7):\n    if testing:\n        print(""Testing"")\n        r1=num_images_training+2\n        r2=num_images_training+num_images_testing+2\n        c=num_images_training+1\n        T1_vols = np.empty((num_images_testing, 144, 192, 256),dtype=""float32"")\n        T2_vols = np.empty((num_images_testing, 144, 192, 256),dtype=""float32"")\n        label_vols = np.empty((num_images_testing, 144, 192, 256),dtype=""uint8"")\n    elif validating:\n        print(""Validating"")\n        r1=num_images_training+1\n        r2=num_images_training+2\n        c=num_images_training\n        T1_vols = np.empty((1, 144, 192, 256),dtype=""float32"")\n        T2_vols = np.empty((1, 144, 192, 256),dtype=""float32"")\n        label_vols = np.empty((1, 144, 192, 256),dtype=""uint8"")\n    else:\n        print(""Training"")\n        r1=1\n        r2=num_images_training+1\n        c=0\n        T1_vols = np.empty((num_images_training, 144, 192, 256),dtype=""float32"")\n        T2_vols = np.empty((num_images_training, 144, 192, 256),dtype=""float32"")\n        label_vols = np.empty((num_images_training, 144, 192, 256),dtype=""uint8"")\n    for case_idx in range(r1, r2) :\n        print(case_idx)\n        T1_vols[(case_idx-c-1), :, :, :] = read_vol(case_idx, \'T1\', dir)\n        T2_vols[(case_idx-c-1), :, :, :] = read_vol(case_idx, \'T2\', dir)\n        label_vols[(case_idx-c-1), :, :, :] = read_vol(case_idx, \'label\', dir)\n    T1_mean = T1_vols.mean()\n    T1_std = T1_vols.std()\n    T1_vols = (T1_vols - T1_mean) / T1_std\n    T2_mean = T2_vols.mean()\n    T2_std = T2_vols.std()\n    T2_vols = (T2_vols - T2_mean) / T2_std\n\n    for i in range(T1_vols.shape[0]):\n        T1_vols[i] = ((T1_vols[i] - np.min(T1_vols[i])) / \n                                    (np.max(T1_vols[i])-np.min(T1_vols[i])))*255\n    for i in range(T2_vols.shape[0]):\n        T2_vols[i] = ((T2_vols[i] - np.min(T2_vols[i])) / \n                                    (np.max(T2_vols[i])-np.min(T2_vols[i])))*255    \n    T1_vols = T1_vols/127.5 -1.\n    T2_vols = T2_vols/127.5 -1.\n    x,y=get_patches_lab(T1_vols,T2_vols,label_vols,extraction_step,patch_shape,validating=validating,\n                                testing=testing,num_images_training=num_images_training)\n    print(""Total Extracted Labelled Patches Shape:"",x.shape,y.shape)\n    if testing:\n        return x, label_vols\n    elif validating:\n        return x, y, label_vols\n    else:\n        return x, y\n\n\n""""""\nTo extract labeled patches from array of 3D ulabeled images\n""""""\ndef get_patches_unlab(T1_vols, T2_vols, extraction_step,patch_shape,dir):\n    patch_shape_1d=patch_shape[0]\n    # Extract patches from input volumes and ground truth\n    label_ref= np.empty((1, 144, 192, 256),dtype=""uint8"")\n    x = np.zeros((0, patch_shape_1d, patch_shape_1d, patch_shape_1d, 2))\n    label_ref = read_vol(1, \'label\', dir)\n    for idx in range(len(T1_vols)) :\n\n        x_length = len(x)\n        print((""Processing the Image %2d ...."")%(idx+11))\n        label_patches = extract_patches(label_ref, patch_shape, extraction_step)\n\n        # Select only those who are important for processing\n        # Sampling strategy: reject samples which labels are mostly 0 and have less than 6000 nonzero elements\n        valid_idxs = np.where(np.count_nonzero(label_patches, axis=(1, 2, 3)) > 6000)\n\n        label_patches = label_patches[valid_idxs]\n        x = np.vstack((x, np.zeros((len(label_patches), patch_shape_1d, \n                                            patch_shape_1d, patch_shape_1d, 2))))\n\n        T1_train = extract_patches(T1_vols[idx], patch_shape, extraction_step,datype=""float32"")\n        x[x_length:, :, :, :, 0] = T1_train[valid_idxs]\n        \n        T2_train = extract_patches(T2_vols[idx], patch_shape, extraction_step,datype=""float32"")\n        x[x_length:, :, :, :, 1] = T2_train[valid_idxs]\n    return x\n\n""""""\nTo preprocess the unlabeled training data\n""""""\ndef preprocess_dynamic_unlab( dir,extraction_step,patch_shape,num_images_training_unlab):\n    T1_vols = np.empty((num_images_training_unlab, 144, 192, 256),dtype=""float32"")\n    T2_vols = np.empty((num_images_training_unlab, 144, 192, 256),dtype=""float32"")\n    for case_idx in range(11, 11+num_images_training_unlab) :\n        T1_vols[(case_idx - 11), :, :, :] = read_vol(case_idx, \'T1\', dir)\n        T2_vols[(case_idx - 11), :, :, :] = read_vol(case_idx, \'T2\', dir)\n        #print(read_vol(case_idx, \'T2\', dir).shape)\n    T1_mean = T1_vols.mean()\n    T1_std = T1_vols.std()\n    T1_vols = (T1_vols - T1_mean) / T1_std\n    T2_mean = T2_vols.mean()\n    T2_std = T2_vols.std()\n    T2_vols = (T2_vols - T2_mean) / T2_std\n    for i in range(T1_vols.shape[0]):\n        T1_vols[i] = ((T1_vols[i] - np.min(T1_vols[i])) / \n                                        (np.max(T1_vols[i])-np.min(T1_vols[i])))*255\n    for i in range(T2_vols.shape[0]):\n        T2_vols[i] = ((T2_vols[i] - np.min(T2_vols[i])) / \n                                        (np.max(T2_vols[i])-np.min(T2_vols[i])))*255  \n    T1_vols = T1_vols/127.5 -1.\n    T2_vols = T2_vols/127.5 -1.\n    x=get_patches_unlab(T1_vols, T2_vols, extraction_step, patch_shape,dir)\n    print(""Total Extracted Unlabelled Patches Shape:"",x.shape)\n    return x\n\n\ndef preprocess_static( org_dir, prepro_dir, dataset=""labeled"", overwrite=False):\n    if not os.path.exists(prepro_dir):\n        os.makedirs(prepro_dir)\n    for subject_folder in glob.glob(os.path.join(org_dir, ""*"", ""*"")):\n        if os.path.isdir(subject_folder):\n            subject = os.path.basename(subject_folder)\n            new_subject_folder = os.path.join(prepro_dir, \n                os.path.basename(os.path.dirname(subject_folder)),subject)\n            if not os.path.exists(new_subject_folder) or overwrite:\n                if not os.path.exists(new_subject_folder):\n                    os.makedirs(new_subject_folder)\n    if(dataset==""labeled""):\n        for case_idx in range(1, 11) :\n            normalise(case_idx, \'T1\',org_dir,prepro_dir)\n            normalise(case_idx, \'T2\',org_dir,prepro_dir)\n            normalise(case_idx, \'label\',org_dir,prepro_dir,\n                           copy=True)\n    else:\n        for case_idx in range(11, 24) :\n            normalise(case_idx, \'T1\',org_dir,prepro_dir)\n            normalise(case_idx, \'T2\',org_dir,prepro_dir)\n            \n""""""\ndataset class for preparing training data of basic U-Net\n""""""\nclass dataset(object):\n  def __init__(self,num_classes, extraction_step, number_images_training, batch_size, patch_shape,data_directory):\n    # Extract labelled and unlabelled patches\n    self.batch_size=batch_size\n    self.data_lab, self.label = preprocess_dynamic_lab(\n                                        data_directory,num_classes,extraction_step,\n                                        patch_shape,number_images_training)\n\n    self.data_lab, self.label = shuffle(self.data_lab, \n                                                    self.label, random_state=0)\n    print(""Data_shape:"",self.data_lab.shape)\n    print(""Data lab max and min:"",np.max(self.data_lab),np.min(self.data_lab))\n    print(""Label unique:"",np.unique(self.label))\n\n  def batch_train(self):\n    self.num_batches = len(self.data_lab) // self.batch_size\n    for i in range(self.num_batches):\n      yield self.data_lab[i*self.batch_size:(i+1)*self.batch_size],\\\n             self.label[i*self.batch_size:(i+1)*self.batch_size]\n\n\n""""""\ndataset_badGAN class for preparing data of our model\n""""""\nclass dataset_badGAN(object):\n  def __init__(self,num_classes, extraction_step, number_images_training, batch_size, \n                    patch_shape, number_unlab_images_training,data_directory):\n    # Extract labelled and unlabelled patches,\n    self.batch_size=batch_size\n    self.data_lab, self.label = preprocess_dynamic_lab(\n                                data_directory,num_classes,extraction_step,\n                                        patch_shape,number_images_training)\n\n    self.data_lab, self.label = shuffle(self.data_lab, self.label, random_state=0)\n    self.data_unlab = preprocess_dynamic_unlab(data_directory,extraction_step,\n                                                patch_shape, number_unlab_images_training)\n    self.data_unlab = shuffle(self.data_unlab, random_state=0)\n\n    # If training, repeat labelled data to make its size equal to unlabelled data\n    factor = len(self.data_unlab) // len(self.data_lab)\n    print(""Factor for labeled images:"",factor)\n    rem = len(self.data_unlab)%len(self.data_lab)\n    temp = self.data_lab[:rem]\n    self.data_lab = np.concatenate((np.repeat(self.data_lab, factor, axis=0), temp), axis=0)\n    temp = self.label[:rem]\n    self.label = np.concatenate((np.repeat(self.label, factor, axis=0), temp), axis=0)\n    assert(self.data_lab.shape == self.data_unlab.shape)\n    print(""Data_shape:"",self.data_lab.shape,self.data_unlab.shape)\n    print(""Data lab max and min:"",np.max(self.data_lab),np.min(self.data_lab))\n    print(""Data unlab max and min:"",np.max(self.data_unlab),np.min(self.data_unlab))\n    print(""Label unique:"",np.unique(self.label))\n\n  def batch_train(self):\n    self.num_batches = len(self.data_lab) // self.batch_size\n    for i in range(self.num_batches):\n      yield self.data_lab[i*self.batch_size:(i+1)*self.batch_size],\\\n             self.data_unlab[i*self.batch_size:(i+1)*self.batch_size],\\\n                self.label[i*self.batch_size:(i+1)*self.batch_size]\n\n\n#preprocess_static( actual_data_directory, preprocesses_data_directory, overwrite=True)'"
proposed_model/main.py,4,"b'import os\nimport pprint\nimport tensorflow as tf\n\nfrom model import model\nfrom test import *\n\n\n# Define flags\nflags = tf.app.flags\nflags.DEFINE_integer(""epoch"", 300, ""Number of training epochs (default: 300)"")\nflags.DEFINE_float(""learning_rate_D"", 0.0001, ""Learning rate of Adam optimizer for Discriminator (default: 0.0001)"")\nflags.DEFINE_float(""learning_rate_G"", 0.0001, ""Learning rate of Adam optimizer for Generator (default: 0.0001)"")\nflags.DEFINE_float(""learning_rate_E"", 0.0001, ""Learning rate of Adam optimizer for Encoder (default: 0.0001)"")\nflags.DEFINE_float(""beta1D"", 0.5, ""Momentum term of Adam optimizer for Discriminator (default: 0.5)"")\nflags.DEFINE_float(""beta1G"", 0.5, ""Momentum term of Adam optimizer for Generator (default: 0.5)"")\nflags.DEFINE_float(""beta1E"", 0.5, ""Momentum term of Adam optimizer for Encoder (default: 0.5)"")\n\nflags.DEFINE_float(""gpu_frac"", 0.95, ""Gpu fraction"")\nflags.DEFINE_float(""tlw"", 0.5, ""True loss weight"")\nflags.DEFINE_float(""flw"", 0.5, ""Fake loss weight"")\nflags.DEFINE_float(""vi_weight"", 0.01, ""Weight of variational inference loss"")\n\nflags.DEFINE_integer(""number_train_images"", 1, ""No. of labeled images for training"")\nflags.DEFINE_integer(""number_train_unlab_images"", 1, ""No. of unlabeled images for training"")\nflags.DEFINE_integer(""number_test_images"", 2, ""No. of images for testing"")\n\nflags.DEFINE_string(""data_directory"", ""../data/iSEG_preprocessed"", ""Directory name containing the dataset"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint/current"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_string(""best_checkpoint_dir"", ""checkpoint/best"", ""Directory name to save the best checkpoints [checkpoint]"")\nflags.DEFINE_string(""results_dir"", ""results/"", ""Directory name to save the results [results]"")\n\nflags.DEFINE_boolean(""load_chkpt"", False, ""True for loading saved checkpoint"")\nflags.DEFINE_boolean(""training"", False, ""True for Training "")\nflags.DEFINE_boolean(""testing"", False, ""True for Testing "")\nflags.DEFINE_boolean(""badGAN"", False, ""True if you want to run badGAN based model "")\n\nflags.DEFINE_integer(""batch_size"", 30, ""The size of batch images [64]"")\n\nflags.DEFINE_integer(""num_mod"", 2, ""Number of modalities of the input 3-D image"")\nflags.DEFINE_integer(""num_classes"", 4, ""Number of output classes to segment"")\nflags.DEFINE_integer(""noise_dim"", 200, ""Dimension of noise vector"")\n\n\n\nFLAGS = flags.FLAGS\n\ndef main(_):\n  # Create required directories\n  if not os.path.exists(FLAGS.checkpoint_dir):\n    os.makedirs(FLAGS.checkpoint_dir)\n\n  if not os.path.exists(FLAGS.results_dir):\n    os.makedirs(FLAGS.results_dir)\n\n  if not os.path.exists(FLAGS.best_checkpoint_dir):\n    os.makedirs(FLAGS.best_checkpoint_dir)\n\n\n  # To configure the GPU fraction\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_frac)\n\n  # Parameters of extracted training and testing patches\n  patch_shape=(32,32,32)\n  extraction_step=(8,8,8)\n  testing_extraction_shape=(8,8,8)\n\n  if FLAGS.training:\n    # For training the network\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n      network = model(sess,patch_shape,extraction_step)\n      network.build_model()\n      network.train()\n  if FLAGS.testing:\n      # For testing the trained network\n      test(patch_shape,testing_extraction_shape)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
proposed_model/model.py,37,"b'from __future__ import division\nimport os\nimport pickle \nimport tensorflow as tf\n\nimport sys\nsys.path.insert(0, \'../preprocess/\')\nsys.path.insert(0, \'../lib/\')\n\nfrom operations import *\nfrom utils import *\nfrom preprocess import *\nimport numpy as np\nfrom six.moves import xrange\nfrom sklearn.metrics import f1_score\n\n\nF = tf.app.flags.FLAGS\n\n\n""""""\nModel class\n\n""""""\nclass model(object):\n  def __init__(self, sess, patch_shape, extraction_step):\n    self.sess = sess\n    self.patch_shape = patch_shape\n    self.extraction_step = extraction_step\n    self.g_bns = [batch_norm(name=\'g_bn{}\'.format(i,)) for i in range(4)]\n    if F.badGAN:\n      self.e_bns = [batch_norm(name=\'e_bn{}\'.format(i,)) for i in range(3)]\n\n\n  def discriminator(self, patch, reuse=False):\n    """"""\n    Parameters:\n    * patch - input image for the network\n    * reuse - boolean variable to reuse weights\n    Returns: \n    * logits\n    * softmax of logits\n    * features extracted from encoding path \n    """"""\n    with tf.variable_scope(\'D\') as scope:\n      if reuse:\n        scope.reuse_variables()\n\n      h0 = lrelu(conv3d_WN(patch, 32, name=\'d_h0_conv\'))\n      h1 = lrelu(conv3d_WN(h0, 32, name=\'d_h1_conv\'))\n      p1 = avg_pool3D(h1)\n\n      h2 = lrelu(conv3d_WN(p1, 64, name=\'d_h2_conv\'))\n      h3 = lrelu(conv3d_WN(h2, 64, name=\'d_h3_conv\'))\n      p3 = avg_pool3D(h3)\n\n      h4 = lrelu(conv3d_WN(p3, 128, name=\'d_h4_conv\'))\n      h5 = lrelu(conv3d_WN(h4, 128, name=\'d_h5_conv\'))\n      p5 = avg_pool3D(h5)\n\n      h6 = lrelu(conv3d_WN(p5, 256, name=\'d_h6_conv\'))\n      h7 = lrelu(conv3d_WN(h6, 256, name=\'d_h7_conv\'))\n\n      up1 = deconv3d_WN(h7,256,name=\'d_up1_deconv\')\n      up1 = tf.concat([h5,up1],4)\n      h8 = lrelu(conv3d_WN(up1, 128, name=\'d_h8_conv\'))\n      h9 = lrelu(conv3d_WN(h8, 128, name=\'d_h9_conv\'))\n      \n      up2 = deconv3d_WN(h9,128,name=\'d_up2_deconv\')\n      up2 = tf.concat([h3,up2],4)\n      h10 = lrelu(conv3d_WN(up2, 64, name=\'d_h10_conv\'))\n      h11 = lrelu(conv3d_WN(h10, 64, name=\'d_h11_conv\'))\n\n      up3 = deconv3d_WN(h11,64,name=\'d_up3_deconv\')\n      up3 = tf.concat([h1,up3],4)\n      h12 = lrelu(conv3d_WN(up3, 32, name=\'d_h12_conv\'))\n      h13 = lrelu(conv3d_WN(h12, 32, name=\'d_h13_conv\'))\n\n      h14 = conv3d_WN(h13, F.num_classes,name=\'d_h14_conv\')\n\n      return h14,tf.nn.softmax(h14),h6\n\n  def generator(self, z, phase):\n    """"""\n    Parameters:\n    * z - Noise vector for generating 3D patches\n    * phase - boolean variable to represent phase of operation of batchnorm\n    Returns: \n    * generated 3D patches\n    """"""\n    with tf.variable_scope(\'G\') as scope:\n      sh1, sh2, sh3, sh4 = int(self.patch_shape[0]/16), int(self.patch_shape[0]/8),\\\n                           int(self.patch_shape[0]/4), int(self.patch_shape[0]/2)\n\n      h0 = linear(z, sh1*sh1*sh1*512,\'g_h0_lin\')\n      h0 = tf.reshape(h0, [F.batch_size, sh1, sh1, sh1, 512])\n      h0 = relu(self.g_bns[0](h0,phase))\n\n      h1 = relu(self.g_bns[1](deconv3d(h0, [F.batch_size,sh2,sh2,sh2,256], \n                                                          name=\'g_h1_deconv\'),phase))\n\n      h2 = relu(self.g_bns[2](deconv3d(h1, [F.batch_size,sh3,sh3,sh3,128], \n                                                          name=\'g_h2_deconv\'),phase))   \n\n      h3 = relu(self.g_bns[3](deconv3d(h2, [F.batch_size,sh4,sh4,sh4,64], \n                                                          name=\'g_h3_deconv\'),phase))\n\n      h4 = deconv3d_WN(h3, F.num_mod, name=\'g_h4_deconv\')\n\n      return tf.nn.tanh(h4)\n\n  def encoder(self, patch, phase):\n    """"""\n    Parameters:\n    * patch - patches generated from the generator\n    * phase - boolean variable to represent phase of operation of batchnorm\n    Returns: \n    * splitted logits\n    """"""\n    with tf.variable_scope(\'E\') as scope:\n      h0 = relu(self.e_bns[0](conv3d(patch, 128, 5,5,5, 2,2,2, name=\'e_h0_conv\'),phase))\n      h1 = relu(self.e_bns[1](conv3d(h0, 256, 5,5,5, 2,2,2, name=\'e_h1_conv\'),phase))\n      h2 = relu(self.e_bns[2](conv3d(h1, 512, 5,5,5, 2,2,2, name=\'e_h2_conv\'),phase))\n\n      h2 = tf.reshape(h2, [h2.shape[0],h2.shape[1]*h2.shape[2]*h2.shape[3]*h2.shape[4]])\n      h3 = linear_WN(h2, F.noise_dim*2,\'e_h3_lin\')\n      \n      h3 = tf.split(h3,2,1)\n      return h3\n\n\n  """"""\n  Defines the Few shot GAN U-Net model and the corresponding losses\n\n  """"""\n  def build_model(self):\n    self.patches_lab = tf.placeholder(tf.float32, [F.batch_size, self.patch_shape[0], \n                                self.patch_shape[1], self.patch_shape[2], F.num_mod], name=\'real_images_l\')\n    self.patches_unlab = tf.placeholder(tf.float32, [F.batch_size, self.patch_shape[0], \n                                self.patch_shape[1], self.patch_shape[2], F.num_mod], name=\'real_images_unl\')\n\n    self.z_gen = tf.placeholder(tf.float32, [None, F.noise_dim], name=\'noise\')\n    self.labels = tf.placeholder(tf.uint8, [F.batch_size, self.patch_shape[0], self.patch_shape[1],\n                                                         self.patch_shape[2]], name=\'image_labels\')\n    self.phase = tf.placeholder(tf.bool)\n\n    #To make one hot of labels\n    self.labels_1hot = tf.one_hot(self.labels, depth=F.num_classes)\n\n    # To generate samples from noise\n    self.patches_fake = self.generator(self.z_gen, self.phase)\n\n    # Forward pass through network with different kinds of training patches \n    self.D_logits_lab, self.D_probdist, _= self.discriminator(self.patches_lab, reuse=False)\n    self.D_logits_unlab, _, self.features_unlab\\\n                       = self.discriminator(self.patches_unlab, reuse=True)\n    self.D_logits_fake, _, self.features_fake\\\n                       = self.discriminator(self.patches_fake, reuse=True)\n\n\n    # To obtain Validation Output\n    self.Val_output = tf.argmax(self.D_probdist, axis=-1)\n\n    # Supervised loss\n    # Weighted cross entropy loss (You can play with these values)\n    # Weights of different class are: Background- 0.33, CSF- 1.5, GM- 0.83, WM- 1.33\n    class_weights = tf.constant([[0.33, 1.5, 0.83, 1.33]])\n    weights = tf.reduce_sum(class_weights * self.labels_1hot, axis=-1)\n    unweighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.D_logits_lab, labels=self.labels_1hot)\n    weighted_losses = unweighted_losses * weights\n    self.d_loss_lab = tf.reduce_mean(weighted_losses)\n\n    # Unsupervised loss\n    self.unl_lsexp = tf.reduce_logsumexp(self.D_logits_unlab,-1)\n    self.fake_lsexp = tf.reduce_logsumexp(self.D_logits_fake,-1)\n    # Unlabeled loss\n    self.true_loss = - F.tlw * tf.reduce_mean(self.unl_lsexp) + F.tlw * tf.reduce_mean(tf.nn.softplus(self.unl_lsexp))\n    # Fake loss\n    self.fake_loss = F.flw * tf.reduce_mean(tf.nn.softplus(self.fake_lsexp))\n    self.d_loss_unlab = self.true_loss + self.fake_loss\n\n    #Total discriminator loss\n    self.d_loss = self.d_loss_lab + self.d_loss_unlab\n\n    #Feature matching loss\n    self.g_loss_fm = tf.reduce_mean(tf.abs(tf.reduce_mean(self.features_unlab,0) \\\n                                                  - tf.reduce_mean(self.features_fake,0)))\n\n    if F.badGAN:\n      # Mean and standard deviation for variational inference loss\n      self.mu, self.log_sigma = self.encoder(self.patches_fake, self.phase)\n      # Generator Loss via variational inference\n      self.vi_loss = gaussian_nll(self.mu, self.log_sigma, self.z_gen)\n      # Total Generator Loss\n      self.g_loss = self.g_loss_fm + F.vi_weight * self.vi_loss\n    else:\n      # Total Generator Loss\n      self.g_loss = self.g_loss_fm \n\n\n    t_vars = tf.trainable_variables()\n    \n    #define the trainable variables\n    self.d_vars = [var for var in t_vars if \'d_\' in var.name]\n    self.g_vars = [var for var in t_vars if \'g_\' in var.name]\n    if F.badGAN:\n      self.e_vars = [var for var in t_vars if \'e_\' in var.name]\n\n    self.saver = tf.train.Saver()\n\n\n  """"""\n  Train function\n  Defines learning rates and optimizers.\n  Performs Network update and saves the losses\n  """"""\n  def train(self):\n\n    # Instantiate the dataset class\n    data = dataset_badGAN(num_classes=F.num_classes,extraction_step=self.extraction_step, \n              number_images_training=F.number_train_images,batch_size=F.batch_size, \n              patch_shape=self.patch_shape,number_unlab_images_training=F.number_train_unlab_images,\n              data_directory=F.data_directory)\n\n    # Optimizer operations\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      d_optim = tf.train.AdamOptimizer(F.learning_rate_D, beta1=F.beta1D)\\\n                  .minimize(self.d_loss,var_list=self.d_vars)\n      g_optim = tf.train.AdamOptimizer(F.learning_rate_G, beta1=F.beta1G)\\\n                  .minimize(self.g_loss,var_list=self.g_vars)\n      if F.badGAN:\n        e_optim = tf.train.AdamOptimizer(F.learning_rate_E, beta1=F.beta1E)\\\n                  .minimize(self.g_loss,var_list=self.e_vars)\n\n    tf.global_variables_initializer().run()\n\n    # Load checkpoints if required\n    if F.load_chkpt:\n      try:\n        load_model(F.checkpoint_dir, self.sess, self.saver)\n        print(""\\n [*] Checkpoint loaded succesfully!"")\n      except:\n        print(""\\n [!] Checkpoint loading failed!"")\n    else:\n      print(""\\n [*] Checkpoint load not required."")\n\n    # Load the validation data\n    patches_val, labels_val_patch, labels_val = preprocess_dynamic_lab(F.data_directory,\n                                    F.num_classes,self.extraction_step,self.patch_shape,\n                                    F.number_train_images,validating=F.training,\n                                    testing=F.testing,num_images_testing=F.number_test_images)\n    \n    predictions_val = np.zeros((patches_val.shape[0],self.patch_shape[0],self.patch_shape[1],\n                                                              self.patch_shape[2]),dtype=""uint8"")\n    max_par=0.0\n    max_loss=100\n    for epoch in xrange(int(F.epoch)):\n      idx = 0\n      batch_iter_train = data.batch_train()\n      total_val_loss=0\n      total_train_loss_CE=0\n      total_train_loss_UL=0\n      total_train_loss_FK=0\n      total_gen_FMloss =0\n\n      for patches_lab, patches_unlab, labels in batch_iter_train:\n        # Network update\n        sample_z_gen = np.random.uniform(-1, 1, [F.batch_size, F.noise_dim]).astype(np.float32)\n\n        _ = self.sess.run(d_optim,feed_dict={self.patches_lab:patches_lab,self.patches_unlab:patches_unlab,\n                                    self.z_gen:sample_z_gen,self.labels:labels, self.phase: True})  \n\n        if F.badGAN:\n          _, _ = self.sess.run([e_optim,g_optim],feed_dict={self.patches_unlab:patches_unlab, self.z_gen:sample_z_gen,\n                                                                  self.z_gen:sample_z_gen,self.phase: True})\n        else:\n          _ = self.sess.run(g_optim,feed_dict={self.patches_unlab:patches_unlab, self.z_gen:sample_z_gen,\n                                                                  self.z_gen:sample_z_gen,self.phase: True})\n\n        feed_dict = {self.patches_lab:patches_lab,self.patches_unlab:patches_unlab,\n                                    self.z_gen:sample_z_gen,self.labels:labels, self.phase: True} \n\n        # Evaluate losses for plotting/printing purposes   \n        d_loss_lab = self.d_loss_lab.eval(feed_dict)\n        d_loss_unlab_true = self.true_loss.eval(feed_dict)\n        d_loss_unlab_fake = self.fake_loss.eval(feed_dict)\n        g_loss_fm = self.g_loss_fm.eval(feed_dict)\n\n        total_train_loss_CE=total_train_loss_CE+d_loss_lab\n        total_train_loss_UL=total_train_loss_UL+d_loss_unlab_true\n        total_train_loss_FK=total_train_loss_FK+d_loss_unlab_fake\n        total_gen_FMloss=total_gen_FMloss+g_loss_fm\n\n        idx += 1\n        if F.badGAN:\n          vi_loss = self.vi_loss.eval(feed_dict)\n          print((""Epoch:[%2d] [%4d/%4d] Labeled loss:%.2e Unlabeled loss:%.2e Fake loss:%.2e Generator FM loss:%.8f Generator VI loss:%.8f\\n"")%\n                      (epoch, idx,data.num_batches,d_loss_lab,d_loss_unlab_true,d_loss_unlab_fake,g_loss_fm,vi_loss))\n        else:\n          print((""Epoch:[%2d] [%4d/%4d] Labeled loss:%.2e Unlabeled loss:%.2e Fake loss:%.2e Generator loss:%.8f \\n"")%\n                      (epoch, idx,data.num_batches,d_loss_lab,d_loss_unlab_true,d_loss_unlab_fake,g_loss_fm))\n\n      # Save the curret model\n      save_model(F.checkpoint_dir, self.sess, self.saver)\n\n      avg_train_loss_CE=total_train_loss_CE/(idx*1.0)\n      avg_train_loss_UL=total_train_loss_UL/(idx*1.0)\n      avg_train_loss_FK=total_train_loss_FK/(idx*1.0)\n      avg_gen_FMloss=total_gen_FMloss/(idx*1.0)\n\n      print(\'\\n\\n\')\n\n      total_batches = int(patches_val.shape[0]/F.batch_size)\n      print(""Total number of batches for validation: "",total_batches)\n\n      # Prediction of validation patches\n      for batch in range(total_batches):\n        patches_feed = patches_val[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:,:]\n        labels_feed = labels_val_patch[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:]\n        feed_dict={self.patches_lab:patches_feed,\n                                              self.labels:labels_feed, self.phase:False}\n        preds = self.Val_output.eval(feed_dict)\n        val_loss = self.d_loss_lab.eval(feed_dict)\n\n        predictions_val[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:]=preds\n        print((""Validated Patch:[%8d/%8d]"")%(batch,total_batches))\n        total_val_loss=total_val_loss+val_loss\n\n      # To compute average patchvise validation loss(cross entropy loss)\n      avg_val_loss=total_val_loss/(total_batches*1.0)\n\n      print(""All validation patches Predicted"")\n\n      print(""Shape of predictions_val, min and max:"",predictions_val.shape,np.min(predictions_val),\n                                                                        np.max(predictions_val))\n\n      # To stitch back the patches into an entire image\n      val_image_pred = recompose3D_overlap(predictions_val,144, 192, 256, self.extraction_step[0],\n                                         self.extraction_step[1], self.extraction_step[2])\n      val_image_pred = val_image_pred.astype(\'uint8\')\n\n      print(""Shape of Predicted Output Groundtruth Images:"",val_image_pred.shape,\n                                                np.unique(val_image_pred),\n                                                np.unique(labels_val),\n                                                np.mean(val_image_pred),np.mean(labels_val))\n\n\n      pred2d=np.reshape(val_image_pred,(val_image_pred.shape[0]*144*192*256))\n      lab2d=np.reshape(labels_val,(labels_val.shape[0]*144*192*256))\n\n      # For printing the validation results\n      F1_score = f1_score(lab2d, pred2d,[0,1,2,3],average=None)\n      print(""Validation Dice Coefficient.... "")\n      print(""Background:"",F1_score[0])\n      print(""CSF:"",F1_score[1])\n      print(""GM:"",F1_score[2])\n      print(""WM:"",F1_score[3])\n\n      # To Save the best model\n      if(max_par<(F1_score[2]+F1_score[3])):\n        max_par=(F1_score[2]+F1_score[3])\n        save_model(F.best_checkpoint_dir, self.sess, self.saver)\n        print(""Best checkpoint updated from validation results."")\n\n      # To save the losses for plotting \n      print(""Average Validation Loss:"",avg_val_loss)\n      with open(\'Val_loss_GAN.txt\', \'a\') as f:\n        f.write(\'%.2e \\n\' % avg_val_loss)\n      with open(\'Train_loss_CE.txt\', \'a\') as f:\n        f.write(\'%.2e \\n\' % avg_train_loss_CE)\n      with open(\'Train_loss_UL.txt\', \'a\') as f:\n        f.write(\'%.2e \\n\' % avg_train_loss_UL)\n      with open(\'Train_loss_FK.txt\', \'a\') as f:\n        f.write(\'%.2e \\n\' % avg_train_loss_FK)\n      with open(\'Train_loss_FM.txt\', \'a\') as f:\n        f.write(\'%.2e \\n\' % avg_gen_FMloss)\n    return'"
proposed_model/test.py,11,"b'from __future__ import division\nimport os\nimport pickle \nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nimport sys\nsys.path.insert(0, \'../preprocess/\')\nsys.path.insert(0, \'../lib/\')\n\nfrom operations import *\nfrom utils import *\nfrom preprocess import *\n\n\n\nF = tf.app.flags.FLAGS\n\n\n# Function to save predicted images as .nii.gz file in results folder\ndef save_image(direc,i,num):\n  img = nib.Nifti1Image(i, None)\n  imgname = \'outputimage_GANbasedunet_\'+str(num)+\'.nii.gz\'\n  nib.save(img, os.path.join(direc,imgname))\n\n\n# Same discriminator network as in model file\ndef trained_dis_network(patch, reuse=False):\n    """"""\n    Parameters:\n    * patch - input image for the network\n    * reuse - boolean variable to reuse weights\n    Returns: \n    * softmax of logits \n    """"""\n    with tf.variable_scope(\'D\') as scope:\n      if reuse:\n        scope.reuse_variables()\n\n      h0 = lrelu(conv3d_WN(patch, 32, name=\'d_h0_conv\'))\n      h1 = lrelu(conv3d_WN(h0, 32, name=\'d_h1_conv\'))\n      p1 = avg_pool3D(h1)\n\n      h2 = lrelu(conv3d_WN(p1, 64, name=\'d_h2_conv\'))\n      h3 = lrelu(conv3d_WN(h2, 64, name=\'d_h3_conv\'))\n      p3 = avg_pool3D(h3)\n\n      h4 = lrelu(conv3d_WN(p3, 128, name=\'d_h4_conv\'))\n      h5 = lrelu(conv3d_WN(h4, 128, name=\'d_h5_conv\'))\n      p5 = avg_pool3D(h5)\n\n      h6 = lrelu(conv3d_WN(p5, 256, name=\'d_h6_conv\'))\n      h7 = lrelu(conv3d_WN(h6, 256, name=\'d_h7_conv\'))\n\n      up1 = deconv3d_WN(h7,256,name=\'d_up1_deconv\')\n      up1 = tf.concat([h5,up1],4)\n      h8 = lrelu(conv3d_WN(up1, 128, name=\'d_h8_conv\'))\n      h9 = lrelu(conv3d_WN(h8, 128, name=\'d_h9_conv\'))\n      \n      up2 = deconv3d_WN(h9,128,name=\'d_up2_deconv\')\n      up2 = tf.concat([h3,up2],4)\n      h10 = lrelu(conv3d_WN(up2, 64, name=\'d_h10_conv\'))\n      h11 = lrelu(conv3d_WN(h10, 64, name=\'d_h11_conv\'))\n\n      up3 = deconv3d_WN(h11,64,name=\'d_up3_deconv\')\n      up3 = tf.concat([h1,up3],4)\n      h12 = lrelu(conv3d_WN(up3, 32, name=\'d_h12_conv\'))\n      h13 = lrelu(conv3d_WN(h12, 32, name=\'d_h13_conv\'))\n\n      h14 = conv3d_WN(h13, F.num_classes,name=\'d_h14_conv\')\n\n      return tf.nn.softmax(h14)\n\n""""""\n Function to test the model and evaluate the predicted images\n Parameters:\n * patch_shape - shape of the patch\n * extraction_step - stride while extracting patches\n""""""\ndef test(patch_shape,extraction_step):\n  \n  with tf.Graph().as_default():\n    test_patches = tf.placeholder(tf.float32, [F.batch_size, patch_shape[0], patch_shape[1],\n                                             patch_shape[2], F.num_mod], name=\'real_patches\')\n\n    # Define the network\n    output_soft = trained_dis_network(test_patches, reuse=None)\n\n    # To convert from one hat form\n    output=tf.argmax(output_soft, axis=-1)\n    print(""Output Patch Shape:"",output.get_shape())\n\n    # To load the saved checkpoint\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n      try:\n        load_model(F.best_checkpoint_dir, sess, saver)\n        print("" Checkpoint loaded succesfully!....\\n"")\n      except:\n        print("" [!] Checkpoint loading failed!....\\n"")\n        return\n\n      # Get patches from test images\n      patches_test, labels_test = preprocess_dynamic_lab(F.data_directory,\n                                    F.num_classes,extraction_step,patch_shape,\n                                    F.number_train_images,validating=F.training,\n                                    testing=F.testing,num_images_testing=F.number_test_images)\n      total_batches = int(patches_test.shape[0]/F.batch_size)\n\n      # Array to store the prediction results\n      predictions_test = np.zeros((patches_test.shape[0],patch_shape[0], patch_shape[1],\n                                             patch_shape[2]))\n\n      print(""max and min of patches_test:"",np.min(patches_test),np.max(patches_test))\n\n      print(""Total number of Batches: "",total_batches)\n\n      # Batch wise prediction\n      for batch in range(total_batches):\n        patches_feed = patches_test[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:,:]\n        preds = sess.run(output, feed_dict={test_patches:patches_feed})\n        predictions_test[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:]=preds\n        print((""Processed_batch:[%8d/%8d]"")%(batch,total_batches))\n\n      print(""All patches Predicted"")\n\n      print(""Shape of predictions_test, min and max:"",predictions_test.shape,np.min(predictions_test),\n                                                                        np.max(predictions_test))\n\n      # To stitch the image back\n      images_pred = recompose3D_overlap(predictions_test,144, 192, 256, extraction_step[0],\n                                                        extraction_step[1],extraction_step[2])\n\n      print(""Shape of Predicted Output Groundtruth Images:"",images_pred.shape,\n                                                np.min(images_pred), np.max(images_pred),\n                                                np.mean(images_pred),np.mean(labels_test))\n\n\n      # To save the images\n      for i in range(F.number_test_images):\n        pred2d=np.reshape(images_pred[i],(144*192*256))\n        lab2d=np.reshape(labels_test[i],(144*192*256))\n        save_image(F.results_dir,images_pred[i],F.number_train_images+i+2)\n\n\n      # Evaluation\n      pred2d=np.reshape(images_pred,(images_pred.shape[0]*144*192*256))\n      lab2d=np.reshape(labels_test,(labels_test.shape[0]*144*192*256))\n\n      F1_score = f1_score(lab2d, pred2d,[0,1,2,3],average=None)\n      print(""Testing Dice Coefficient.... "")\n      print(""Background:"",F1_score[0])\n      print(""CSF:"",F1_score[1])\n      print(""GM:"",F1_score[2])\n      print(""WM:"",F1_score[3])\n\n\n\n  return\n\n\n\n\n\n\n\n'"
unet3D/main_unet.py,4,"b'import os\nimport pprint\nimport tensorflow as tf\n\nfrom model_unet import UNET\nfrom testing_unet import *\n\n\n# Define flags\nflags = tf.app.flags\nflags.DEFINE_integer(""epoch"", 100000, ""Number of training epochs (default: 100000)"")\nflags.DEFINE_float(""learning_rate_"", 0.0001, ""Learning rate of Adam optimizer for Discriminator (default: 0.0001)"")\nflags.DEFINE_float(""beta1"", 0.9, ""Momentum term of Adam optimizer for Discriminator (default: 0.5)"")\n\nflags.DEFINE_float(""gpu_frac"", 0.95, ""Gpu fraction"")\n\n\nflags.DEFINE_integer(""number_train_images"", 2, ""No. of images for training"")\nflags.DEFINE_integer(""number_test_images"", 1, ""No. of images for testing"")\n\nflags.DEFINE_string(""data_directory"", ""../data/iSEG_preprocessed"", ""Directory name containing the preprocessed dataset"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint/current"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_string(""best_checkpoint_dir"", ""checkpoint/best"", ""Directory name to save the best checkpoints [checkpoint]"")\nflags.DEFINE_string(""results_dir"", ""results/"", ""Directory name to save the results [results]"")\n\nflags.DEFINE_boolean(""load_chkpt"", False, ""True for loading saved checkpoint"")\nflags.DEFINE_boolean(""training"", False, ""True for Training "")\nflags.DEFINE_boolean(""testing"", False, ""True for Testing "")\n\n\nflags.DEFINE_integer(""batch_size"", 30, ""The size of batch images(30 for data1 and 20 for data2"")\n\nflags.DEFINE_integer(""num_mod"", 2, ""Number of channels in input image(2 for data1 and 1 for data2)"")\nflags.DEFINE_integer(""num_classes"", 4, ""Number of output classes(4 for data1 and 15 for data2)"")\n\nFLAGS = flags.FLAGS\n\ndef main(_):\n  # Create required directories\n  if not os.path.exists(FLAGS.checkpoint_dir):\n    os.makedirs(FLAGS.checkpoint_dir)\n\n  if not os.path.exists(FLAGS.results_dir):\n    os.makedirs(FLAGS.results_dir)\n\n  if not os.path.exists(FLAGS.best_checkpoint_dir):\n    os.makedirs(FLAGS.best_checkpoint_dir)\n\n  # To configure the GPU fraction\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_frac)\n\n  # Parameters of extracted training and testing patches\n  patch_shape=(32,32,32)\n  extraction_step=(8,8,8)\n  testing_extraction_shape=(8,8,8)\n\n  if FLAGS.training:\n    # For training the network\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n      network = UNET(sess,patch_shape,extraction_step)\n      network.build_model()\n      network.train()\n  if FLAGS.testing:\n      # For testing the network\n      test(patch_shape,testing_extraction_shape)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
unet3D/model_unet.py,25,"b'from __future__ import division\nimport os\nimport pickle \nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import xrange\nfrom sklearn.metrics import f1_score\n\nimport sys\nsys.path.insert(0, \'../preprocess/\')\nsys.path.insert(0, \'../lib/\')\n\nfrom operations import *\nfrom utils import *\nfrom preprocess import * \n\n\nF = tf.app.flags.FLAGS\n\n\n""""""\nModel class\n""""""\nclass UNET(object):\n  def __init__(self, sess, patch_shape, extraction_step):\n    self.sess = sess\n    self.patch_shape = patch_shape\n    self.extraction_step = extraction_step\n    self.d_bns = [batch_norm(name=\'u_bn{}\'.format(i,)) for i in range(14)]\n\n\n  def network_dis(self, patch, reuse=False):\n    """"""\n    Parameters:\n    * patch - input image for the network\n    * reuse - boolean variable to reuse weights\n    Returns: \n    * logits\n    * softmax of logits\n    * features extracted from encoding path \n    """"""\n    with tf.variable_scope(\'U\') as scope:\n      if reuse:\n        scope.reuse_variables()\n\n      h0 = lrelu(conv3d_WN(patch, 32, name=\'u_h0_conv\'))\n      h1 = lrelu(conv3d_WN(h0, 32, name=\'u_h1_conv\'))\n      p1 = avg_pool3D(h1)\n\n      h2 = lrelu(conv3d_WN(p1, 64, name=\'u_h2_conv\'))\n      h3 = lrelu(conv3d_WN(h2, 64, name=\'u_h3_conv\'))\n      p3 = avg_pool3D(h3)\n\n      h4 = lrelu(conv3d_WN(p3, 128, name=\'u_h4_conv\'))\n      h5 = lrelu(conv3d_WN(h4, 128, name=\'u_h5_conv\'))\n      p5 = avg_pool3D(h5)\n\n      h6 = lrelu(conv3d_WN(p5, 256, name=\'u_h6_conv\'))\n      h7 = lrelu(conv3d_WN(h6, 256, name=\'u_h7_conv\'))\n\n      up1 = deconv3d_WN(h7,256,name=\'u_up1_deconv\')\n      up1 = tf.concat([h5,up1],4)\n      h8 = lrelu(conv3d_WN(up1, 128, name=\'u_h8_conv\'))\n      h9 = lrelu(conv3d_WN(h8, 128, name=\'u_h9_conv\'))\n      \n      up2 = deconv3d_WN(h9,128,name=\'u_up2_deconv\')\n      up2 = tf.concat([h3,up2],4)\n      h10 = lrelu(conv3d_WN(up2, 64, name=\'u_h10_conv\'))\n      h11 = lrelu(conv3d_WN(h10, 64, name=\'u_h11_conv\'))\n\n      up3 = deconv3d_WN(h11,64,name=\'u_up3_deconv\')\n      up3 = tf.concat([h1,up3],4)\n      h12 = lrelu(conv3d_WN(up3, 32, name=\'u_h12_conv\'))\n      h13 = lrelu(conv3d_WN(h12, 32, name=\'u_h13_conv\'))\n\n      h14 = conv3d_WN(h13, F.num_classes,name=\'u_h14_conv\')\n\n      return h14,tf.nn.softmax(h14)\n\n  """"""\n  Network model\n  Parameters:\n  * image - input image for the network\n  * reuse - boolean variable to reuse weights\n  Returns: logits \n  """"""\n  def network(self, patch, phase,pshape, reuse=False):\n    with tf.variable_scope(\'U\') as scope:\n      if reuse:\n        scope.reuse_variables()\n\n      sh1, sh2, sh3 = int(pshape[0]/4),\\\n                           int(pshape[0]/2), int(pshape[0])\n\n      h0 = relu(self.d_bns[0](conv3d(patch, 32, name=\'u_h0_conv\'),phase))\n      h1 = relu(self.d_bns[1](conv3d(h0, 32, name=\'u_h1_conv\'),phase))\n      p1 = max_pool3D(h1)\n\n      h2 = relu(self.d_bns[2](conv3d(p1, 64, name=\'u_h2_conv\'),phase))\n      h3 = relu(self.d_bns[3](conv3d(h2, 64, name=\'u_h3_conv\'),phase))\n      p3 = max_pool3D(h3)\n\n      h4 = relu(self.d_bns[4](conv3d(p3, 128, name=\'u_h4_conv\'),phase))\n      h5 = relu(self.d_bns[5](conv3d(h4, 128, name=\'u_h5_conv\'),phase))\n      p5 = max_pool3D(h5)\n\n      h6 = relu(self.d_bns[6](conv3d(p5, 256, name=\'u_h6_conv\'),phase))\n      h7 = relu(self.d_bns[7](conv3d(h6, 256, name=\'u_h7_conv\'),phase))\n\n      up1 = deconv3d(h7,[F.batch_size,sh1,sh1,sh1,256],name=\'d_up1_deconv\')\n      up1 = tf.concat([h5,up1],4)\n      h8 = relu(self.d_bns[8](conv3d(up1, 128, name=\'u_h8_conv\'),phase))\n      h9 = relu(self.d_bns[9](conv3d(h8, 128, name=\'u_h9_conv\'),phase))\n      \n      up2 = deconv3d(h9,[F.batch_size,sh2,sh2,sh2,128],name=\'d_up2_deconv\')\n      up2 = tf.concat([h3,up2],4)\n      h10 = relu(self.d_bns[10](conv3d(up2, 64, name=\'u_h10_conv\'),phase))\n      h11 = relu(self.d_bns[11](conv3d(h10, 64, name=\'u_h11_conv\'),phase))\n\n      up3 = deconv3d(h11,[F.batch_size,sh3,sh3,sh3,64],name=\'d_up3_deconv\')\n      up3 = tf.concat([h1,up3],4)\n      h12 = relu(self.d_bns[12](conv3d(up3, 32, name=\'u_h12_conv\'),phase))\n      h13 = relu(self.d_bns[13](conv3d(h12, 32, name=\'u_h13_conv\'),phase))\n\n      h14 = conv3d(h13, F.num_classes, name=\'u_h14_conv\')\n\n      return h14,tf.nn.softmax(h14)\n\n\n  """"""\n  Defines the UNET model and losses\n  """"""\n  def build_model(self):\n    self.patches_labeled = tf.placeholder(tf.float32, [F.batch_size, self.patch_shape[0], \n                                self.patch_shape[1], self.patch_shape[2], F.num_mod], name=\'real_images_l\')\n\n    self.labels = tf.placeholder(tf.uint8, [F.batch_size, self.patch_shape[0], self.patch_shape[1],\n                                                         self.patch_shape[2]], name=\'image_labels\')\n    self.labels_1hot = tf.one_hot(self.labels, depth=F.num_classes)\n    self.phase = tf.placeholder(tf.bool)\n\n    # Forward pass through network\n    # To use original 3D U-Net use ***network*** function and don\'t forget to change the testing file\n    #self._logits_labeled, self._probdist = self.network(self.patches_labeled, self.phase, self.patch_shape, reuse=False)\n    self._logits_labeled, self._probdist = self.network_dis(self.patches_labeled, reuse=False)\n\n    #Validation Output\n    self.Val_output = tf.argmax(self._probdist, axis=-1)\n\n    # Weighted ross entropy loss\n    # Weights of different class are: Background- 0.33, CSF- 1.5, GM- 0.83, WM- 1.33\n    class_weights = tf.constant([[0.33, 1.5, 0.83, 1.33]])\n    weights = tf.reduce_sum(class_weights * self.labels_1hot, axis=-1)\n    unweighted_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self._logits_labeled, labels=self.labels_1hot)\n    weighted_losses = unweighted_losses * weights\n    self.u_loss = tf.reduce_mean(weighted_losses)\n\n    #define the trainable variables\n    t_vars = tf.trainable_variables()\n    self.u_vars = [var for var in t_vars if \'u_\' in var.name]\n\n    self.saver = tf.train.Saver()\n\n\n  """"""\n  Train function\n  Defines learning rates and optimizers.\n  Performs Network update and saves the losses\n  """"""\n  def train(self):\n    data = dataset(num_classes=F.num_classes,extraction_step=self.extraction_step, number_images_training=\n                    F.number_train_images,batch_size=F.batch_size, patch_shape=self.patch_shape,data_directory=F.data_directory)\n    \n    global_step = tf.placeholder(tf.int32, [], name=""global_step_epochs"")\n\n\n    # Optimizer operation\n    _optim = tf.train.AdamOptimizer(F.learning_rate_, beta1=F.beta1).minimize(self.u_loss,\n                                                                       var_list=self.u_vars)\n\n    tf.global_variables_initializer().run()\n\n    # Load checkpoints if required\n    if F.load_chkpt:\n      try:\n        load_model(F.checkpoint_dir, self.sess, self.saver)\n        print(""\\n [*] Checkpoint loaded succesfully!"")\n      except:\n        print(""\\n [!] Checkpoint loading failed!"")\n    else:\n      print(""\\n [*] Checkpoint load not required."")\n\n\n    patches_val, labels_val_patch, labels_val = preprocess_dynamic_lab(F.data_directory,\n                                    F.num_classes,self.extraction_step,self.patch_shape,\n                                    F.number_train_images,validating=F.training,\n                                    testing=F.testing,num_images_testing=F.number_test_images)\n    \n    predictions_val = np.zeros((patches_val.shape[0],self.patch_shape[0],self.patch_shape[1],\n                                                        self.patch_shape[2]),dtype=\'uint8\')\n    max_par=0.0\n    max_loss=100\n    for epoch in xrange(int(F.epoch)):\n      idx = 0\n      batch_iter_train = data.batch_train()\n      total_val_loss=0\n      total_train_loss=0\n\n      for patches_lab, labels in batch_iter_train:\n        # Network update\n        feed_dict = {self.patches_labeled:patches_lab,self.labels:labels,\n                                        self.phase:True, global_step: epoch}\n        _optim.run(feed_dict)\n\n        # Evaluate loss for plotting/printing purposes\n        feed_dict = {self.patches_labeled:patches_lab,self.labels:labels,\n                                        self.phase:True, global_step: epoch}    \n        u_loss = self.u_loss.eval(feed_dict)\n        total_train_loss=total_train_loss+u_loss\n\n        idx += 1\n        print((""Epoch:[%2d] [%4d/%4d] Loss:%.2e \\n"")%(epoch, idx,data.num_batches,u_loss))\n\n\n      # Save model\n      save_model(F.checkpoint_dir, self.sess, self.saver)\n\n\n      # Validation runs every third epoch\n      if epoch%3==0:\n        avg_train_loss=total_train_loss/(idx*1.0)\n        print(\'\\n\\n\')\n\n        total_batches = int(patches_val.shape[0]/F.batch_size)\n        print(""Total number of Patches: "",patches_val.shape[0])\n        print(""Total number of Batches: "",total_batches)\n\n        for batch in range(total_batches):\n          patches_feed = patches_val[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:,:]\n          labels_feed = labels_val_patch[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:]\n          feed_dict={self.patches_labeled:patches_feed,\n                                                self.labels:labels_feed, self.phase:False}\n          preds = self.Val_output.eval(feed_dict)\n          val_loss = self.u_loss.eval(feed_dict)\n\n          predictions_val[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:]=preds\n          print((""Validated Patch:[%8d/%8d]"")%(batch,total_batches))\n          total_val_loss=total_val_loss+val_loss\n\n        avg_val_loss=total_val_loss/(total_batches*1.0)\n\n        print(""All validation patches Predicted"")\n\n\n        print(""Shape of predictions_val, min and max:"",predictions_val.shape,np.min(predictions_val),\n                                                                          np.max(predictions_val))\n\n\n        val_image_pred = recompose3D_overlap(predictions_val,144, 192, 256, self.extraction_step[0],\n                                                        self.extraction_step[1],self.extraction_step[2])\n        val_image_pred = val_image_pred.astype(\'uint8\')\n\n        print(""Shape of Predicted Output Groundtruth Images:"",val_image_pred.shape,\n                                                  np.unique(val_image_pred),\n                                                  np.unique(labels_val),\n                                                  np.mean(val_image_pred),np.mean(labels_val))\n\n        \n        pred2d=np.reshape(val_image_pred,(val_image_pred.shape[0]*144*192*256))\n        lab2d=np.reshape(labels_val,(labels_val.shape[0]*144*192*256))\n        F1_score = f1_score(lab2d, pred2d,[0,1,2,3],average=None)\n        print(""Validation Dice Coefficient.... "")\n        print(""Background:"",F1_score[0])\n        print(""CSF:"",F1_score[1])\n        print(""GM:"",F1_score[2])\n        print(""WM:"",F1_score[3])\n\n        # To save the best model based on validation\n        if(max_par<(F1_score[2]+F1_score[3])):\n          max_par=(F1_score[2]+F1_score[3])\n          save_model(F.best_checkpoint_dir, self.sess, self.saver)\n          print(""Best checkpoint got updated from validation results."")\n\n        # To save losses for plotting \n        \'\'\'\n        print(""Average Validation Loss:"",avg_val_loss)\n        print(""Average Training Loss"",avg_train_loss)\n        with open(\'Val_loss.txt\', \'a\') as f:\n          f.write(\'%.2e \\n\' % avg_val_loss)\n        with open(\'Train_loss.txt\', \'a\') as f:\n          f.write(\'%.2e \\n\' % avg_train_loss)\n        \'\'\'\n\n    return'"
unet3D/testing_unet.py,17,"b'from __future__ import division\nimport os\nimport pickle \nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nimport sys\nsys.path.insert(0, \'../preprocess/\')\nsys.path.insert(0, \'../lib/\')\n\nfrom operations import *\nfrom utils import *\nfrom preprocess import *\n\n\nF = tf.app.flags.FLAGS\n\nd_bns = [batch_norm(name=\'u_bn{}\'.format(i,)) for i in range(14)]\n\n# Function to save predicted images as .nii.gz file in results folder\ndef save_image(direc,i,num):\n  img = nib.Nifti1Image(i, None)\n  imgname = \'outputimage_unet_\'+str(num)+\'.nii.gz\'\n  nib.save(img, os.path.join(direc,imgname))\n\n""""""\n Modified 3D U-Net \n""""""\ndef trained_network_dis(patch, reuse=False):\n    """"""\n    Parameters:\n    * patch - input image for the network\n    * reuse - boolean variable to reuse weights\n    Returns: \n    * softmax of logits\n    """"""\n    with tf.variable_scope(\'U\') as scope:\n      if reuse:\n        scope.reuse_variables()\n\n      h0 = lrelu(conv3d_WN(patch, 32, name=\'u_h0_conv\'))\n      h1 = lrelu(conv3d_WN(h0, 32, name=\'u_h1_conv\'))\n      p1 = avg_pool3D(h1)\n\n      h2 = lrelu(conv3d_WN(p1, 64, name=\'u_h2_conv\'))\n      h3 = lrelu(conv3d_WN(h2, 64, name=\'u_h3_conv\'))\n      p3 = avg_pool3D(h3)\n\n      h4 = lrelu(conv3d_WN(p3, 128, name=\'u_h4_conv\'))\n      h5 = lrelu(conv3d_WN(h4, 128, name=\'u_h5_conv\'))\n      p5 = avg_pool3D(h5)\n\n      h6 = lrelu(conv3d_WN(p5, 256, name=\'u_h6_conv\'))\n      h7 = lrelu(conv3d_WN(h6, 256, name=\'u_h7_conv\'))\n\n      up1 = deconv3d_WN(h7,256,name=\'u_up1_deconv\')\n      up1 = tf.concat([h5,up1],4)\n      h8 = lrelu(conv3d_WN(up1, 128, name=\'u_h8_conv\'))\n      h9 = lrelu(conv3d_WN(h8, 128, name=\'u_h9_conv\'))\n      \n      up2 = deconv3d_WN(h9,128,name=\'u_up2_deconv\')\n      up2 = tf.concat([h3,up2],4)\n      h10 = lrelu(conv3d_WN(up2, 64, name=\'u_h10_conv\'))\n      h11 = lrelu(conv3d_WN(h10, 64, name=\'u_h11_conv\'))\n\n      up3 = deconv3d_WN(h11,64,name=\'u_up3_deconv\')\n      up3 = tf.concat([h1,up3],4)\n      h12 = lrelu(conv3d_WN(up3, 32, name=\'u_h12_conv\'))\n      h13 = lrelu(conv3d_WN(h12, 32, name=\'u_h13_conv\'))\n\n      h14 = conv3d_WN(h13, F.num_classes,name=\'u_h14_conv\')\n\n      return tf.nn.softmax(h14)\n\n""""""\n Actual 3D U-Net \n""""""\ndef trained_network( patch, phase, pshape, reuse=None):\n  """"""\n    Parameters:\n    * patch - input image for the network\n    * phase - phase for batchnorm\n    * pshape - shape of the patch\n    * reuse - boolean variable to reuse weights\n    Returns: \n    * softmax of logits\n    """"""\n  with tf.variable_scope(\'U\') as scope:\n    if reuse:\n      scope.reuse_variables()\n\n    sh1, sh2, sh3 = int(pshape[0]/4),\\\n                           int(pshape[0]/2), int(pshape[0])\n\n    h0 = relu(d_bns[0](conv3d(patch, 32, name=\'u_h0_conv\'),phase))\n    h1 = relu(d_bns[1](conv3d(h0, 32, name=\'u_h1_conv\'),phase))\n    p1 = max_pool3D(h1)\n\n    h2 = relu(d_bns[2](conv3d(p1, 64, name=\'u_h2_conv\'),phase))\n    h3 = relu(d_bns[3](conv3d(h2, 64, name=\'u_h3_conv\'),phase))\n    p3 = max_pool3D(h3)\n\n    h4 = relu(d_bns[4](conv3d(p3, 128, name=\'u_h4_conv\'),phase))\n    h5 = relu(d_bns[5](conv3d(h4, 128, name=\'u_h5_conv\'),phase))\n    p5 = max_pool3D(h5)\n\n    h6 = relu(d_bns[6](conv3d(p5, 256, name=\'u_h6_conv\'),phase))\n    h7 = relu(d_bns[7](conv3d(h6, 256, name=\'u_h7_conv\'),phase))\n\n    up1 = deconv3d(h7,[F.batch_size,sh1,sh1,sh1,256],name=\'d_up1_deconv\')\n    up1 = tf.concat([h5,up1],4)\n    h8 = relu(d_bns[8](conv3d(up1, 128, name=\'u_h8_conv\'),phase))\n    h9 = relu(d_bns[9](conv3d(h8, 128, name=\'u_h9_conv\'),phase))\n    \n    up2 = deconv3d(h9,[F.batch_size,sh2,sh2,sh2,128],name=\'d_up2_deconv\')\n    up2 = tf.concat([h3,up2],4)\n    h10 = relu(d_bns[10](conv3d(up2, 64, name=\'u_h10_conv\'),phase))\n    h11 = relu(d_bns[11](conv3d(h10, 64, name=\'u_h11_conv\'),phase))\n\n    up3 = deconv3d(h11,[F.batch_size,sh3,sh3,sh3,64],name=\'d_up3_deconv\')\n    up3 = tf.concat([h1,up3],4)\n    h12 = relu(d_bns[12](conv3d(up3, 32, name=\'u_h12_conv\'),phase))\n    h13 = relu(d_bns[13](conv3d(h12, 32, name=\'u_h13_conv\'),phase))\n\n    h14 = conv3d(h13, F.num_classes, name=\'u_h14_conv\')\n\n    return tf.nn.softmax(h14)\n\n\n""""""\n Function to test the model and evaluate the predicted images\n Parameters:\n * patch_shape - shape of the patch\n * extraction_step - stride while extracting patches\n""""""\ndef test(patch_shape,extraction_step):\n  \n  with tf.Graph().as_default():\n    test_patches = tf.placeholder(tf.float32, [F.batch_size, patch_shape[0], patch_shape[1],\n                                             patch_shape[2], F.num_mod], name=\'real_patches\')\n    phase = tf.placeholder(tf.bool)\n\n    # Define the network\n    # For using actual 3-D U-Net change ***trained_network*** function both in training and testing\n    #output_soft = trained_network(test_patches, phase, patch_shape, reuse=None)\n    output_soft = trained_network_dis(test_patches, reuse=None)\n\n    # To convert from one hat form\n    output=tf.argmax(output_soft, axis=-1)\n    print(""Output Patch Shape:"",output.get_shape())\n\n    # To load the saved checkpoint\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n      try:\n        load_model(F.best_checkpoint_dir, sess, saver)\n        print("" Checkpoint loaded succesfully!....\\n"")\n      except:\n        print("" [!] Checkpoint loading failed!....\\n"")\n        return\n\n      # Get patches from test images\n      patches_test, labels_test = preprocess_dynamic_lab(F.data_directory,\n                                    F.num_classes,extraction_step,patch_shape,\n                                    F.number_train_images,validating=F.training,\n                                    testing=F.testing,num_images_testing=F.number_test_images)\n      total_batches = int(patches_test.shape[0]/F.batch_size)\n\n      # Array to store the prediction results\n      predictions_test = np.zeros((patches_test.shape[0],patch_shape[0], patch_shape[1],\n                                             patch_shape[2]))\n\n      print(""max and min of patches_test:"",np.min(patches_test),np.max(patches_test))\n\n      # Batch wise prediction\n      print(""Total number of Batches: "",total_batches)\n      for batch in range(total_batches):\n        patches_feed = patches_test[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:,:]\n        preds = sess.run(output, feed_dict={test_patches:patches_feed,phase:False})\n        predictions_test[batch*F.batch_size:(batch+1)*F.batch_size,:,:,:]=preds\n        print((""Processed_batch:[%8d/%8d]"")%(batch,total_batches))\n\n      print(""All patches Predicted"")\n\n      print(""Shape of predictions_test, min and max:"",predictions_test.shape,np.min(predictions_test),\n                                                                        np.max(predictions_test))\n\n      #To stitch the image back\n      images_pred = recompose3D_overlap(predictions_test,144, 192, 256, extraction_step[0],\n                                                        extraction_step[1],extraction_step[2])\n\n      print(""Shape of Predicted Output Groundtruth Images:"",images_pred.shape,\n                                                np.min(images_pred), np.max(images_pred),\n                                                np.mean(images_pred),np.mean(labels_test))\n      \n      # To save the images\n      for i in range(F.number_test_images):\n        pred2d=np.reshape(images_pred[i],(144*192*256))\n        lab2d=np.reshape(labels_test[i],(144*192*256))\n        save_image(F.results_dir,images_pred[i],F.number_train_images+i+2)\n        F1_score = f1_score(lab2d, pred2d,[0,1,2,3],average=None)\n\n      # Evaluation\n      pred2d=np.reshape(images_pred,(images_pred.shape[0]*144*192*256))\n      lab2d=np.reshape(labels_test,(labels_test.shape[0]*144*192*256))\n\n      F1_score = f1_score(lab2d, pred2d,[0,1,2,3],average=None)\n      print(""Testing Dice Coefficient.... "")\n      print(""Background:"",F1_score[0])\n      print(""CSF:"",F1_score[1])\n      print(""GM:"",F1_score[2])\n      print(""WM:"",F1_score[3])\n\n  return\n\n\n\n\n\n\n'"
