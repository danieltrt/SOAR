file_path,api_count,code
setup.py,0,"b'__author__ = \'Alex Rogozhnikov\'\n\nfrom setuptools import setup\n\nsetup(\n    name=""einops"",\n    version=\'0.2.0\',\n    description=""A new flavour of deep learning operations"",\n    long_description=open(\'README.md\', encoding=\'utf-8\').read(),\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/arogozhnikov/einops\',\n    author=\'Alex Rogozhnikov\',\n\n    packages=[\'einops\', \'einops.layers\'],\n\n    classifiers=[\n        \'Intended Audience :: Science/Research\',\n        \'Programming Language :: Python :: 3 \',\n    ],\n    keywords=\'deep learning, neural networks, tensor manipulation, machine learning, \'\n             \'scientific computations, einops\',\n    install_requires=[\n        # no run-time or installation-time dependencies\n    ],\n)\n'"
test.py,0,"b'""""""\nUsage: python test.py\n1. Installs part of dependencies (make sure `which pip` points to correct location)\n2. Installs current version of einops in editable mode\n3. Runs tests\n""""""\n\nimport os\nfrom subprocess import Popen, PIPE\nfrom pathlib import Path\n\n__author__ = \'Alex Rogozhnikov\'\n\n\ndef run(cmd, **env):\n    # keeps printing output when testing\n    cmd = cmd.split(\' \') if isinstance(cmd, str) else cmd\n    p = Popen(cmd, cwd=str(Path(__file__).parent), env={**os.environ, **env})\n    p.communicate()\n    return p.returncode\n\n\n# check we have nvidia-smi\noutput, _ = Popen(\'which nvidia-smi\'.split(\' \'), stdout=PIPE).communicate()\nhave_cuda = b\'nvidia\' in output\n\n# install cupy. It can\'t be installed without cuda available (with compilers).\nif have_cuda:\n    return_code = run(\'pip install cupy --pre --progress-bar off\')\n    assert return_code == 0\n\n# install dependencies\ndependencies = [\n    # TODO remove numpy version\n    \'numpy==1.18.4\',\n    \'mxnet\',\n    \'torch\',\n    \'tensorflow\',\n    \'chainer\',\n    \'keras\',\n    \'jax\',\n    \'jaxlib\',\n    \'nbformat\',\n    \'nbconvert\',\n    \'jupyter\',\n    \'pillow\',\n    \'nose\',\n]\nassert 0 == run(\'pip install {} --pre --progress-bar off\'.format(\' \'.join(dependencies)))\n# install einops\nassert 0 == run(\'pip install -e .\')\n\n\n# we need to run tests twice\n# - once for tensorflow eager\nreturn_code1 = run(\'python -m nose tests -vds\', TF_EAGER=\'1\', EINOPS_SKIP_CUPY=\'0\' if have_cuda else \'1\')\nprint(\'\\n\' * 5)\n# - and once for symbolic tensorflow\nreturn_code2 = run(\'python -m nose tests -vds\', TF_EAGER=\'0\', EINOPS_SKIP_CUPY=\'0\' if have_cuda else \'1\')\n\nassert return_code1 == 0 and return_code2 == 0\n'"
einops/__init__.py,0,"b""__author__ = 'Alex Rogozhnikov'\n__version__ = '0.2.0'\n__all__ = ['rearrange', 'reduce', 'parse_shape', 'asnumpy', 'EinopsError']\n\nfrom .einops import rearrange, reduce, repeat, parse_shape, asnumpy, EinopsError\n"""
einops/_backends.py,16,"b'""""""\nBackends in `einops` are organized to meet the following requirements\n- backends are not imported unless those are actually needed, because\n    - backends may not be installed\n    - importing all available backends will drive to significant memory footprint\n    - backends may by present but installed with errors (but never used),\n      importing may drive to crashes\n- backend should be either symbolic or imperative (tensorflow is for both, but that causes problems)\n    - this determines which methods (from_numpy/to_numpy or create_symbol/eval_symbol) should be defined\n- if backend can\'t (temporarily) provide symbols for shape dimensions, UnknownSize objects are used\n""""""\n\nimport sys\nimport warnings\n\n__author__ = \'Alex Rogozhnikov\'\n\n_backends = {}\n_debug_importing = False\n\n\ndef get_backend(tensor) -> \'AbstractBackend\':\n    """"""\n    Takes a correct backend (e.g. numpy backend if tensor is numpy.ndarray) for a tensor.\n    If needed, imports package and creates backend\n    """"""\n    for framework_name, backend in _backends.items():\n        if backend.is_appropriate_type(tensor):\n            return backend\n\n    # Find backend subclasses recursively\n    backend_subclasses = []\n    backends = AbstractBackend.__subclasses__()\n    while backends:\n        backend = backends.pop()\n        backends += backend.__subclasses__()\n        backend_subclasses.append(backend)\n\n    for BackendSubclass in backend_subclasses:\n        if _debug_importing:\n            print(\'Testing for subclass of \', BackendSubclass)\n        if BackendSubclass.framework_name not in _backends:\n            # check that module was already imported. Otherwise it can\'t be imported\n            if BackendSubclass.framework_name in sys.modules:\n                if _debug_importing:\n                    print(\'Imported backend for \', BackendSubclass.framework_name)\n                backend = BackendSubclass()\n                _backends[backend.framework_name] = backend\n                if backend.is_appropriate_type(tensor):\n                    return backend\n\n    raise RuntimeError(\'Tensor type unknown to einops {}\'.format(type(tensor)))\n\n\nclass AbstractBackend:\n    """""" Base backend class, major part of methods are only for debugging purposes. """"""\n    framework_name = None\n\n    def is_appropriate_type(self, tensor):\n        """""" helper method should recognize tensors it can handle """"""\n        raise NotImplementedError()\n\n    def from_numpy(self, x):\n        raise NotImplementedError(""framework doesn\'t support imperative execution"")\n\n    def to_numpy(self, x):\n        raise NotImplementedError(""framework doesn\'t support imperative execution"")\n\n    def create_symbol(self, shape):\n        raise NotImplementedError(""framework doesn\'t support symbolic computations"")\n\n    def eval_symbol(self, symbol, input_dict):\n        raise NotImplementedError(""framework doesn\'t support symbolic computations"")\n\n    def arange(self, start, stop):\n        # supplementary method used only in testing, so should implement CPU version\n        raise NotImplementedError(""framework doesn\'t implement arange"")\n\n    def shape(self, x):\n        """"""shape should return a tuple with integers or ""shape symbols"" (which will evaluate to actual size)""""""\n        return x.shape\n\n    def reshape(self, x, shape):\n        return x.reshape(shape)\n\n    def transpose(self, x, axes):\n        return x.transpose(axes)\n\n    def reduce(self, x, operation, axes):\n        return getattr(x, operation)(axis=axes)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        raise NotImplementedError()\n\n    def add_axis(self, x, new_position):\n        raise NotImplementedError()\n\n    def add_axes(self, x, n_axes, pos2len):\n        repeats = [1] * n_axes\n        for axis_position, axis_length in pos2len.items():\n            x = self.add_axis(x, axis_position)\n            repeats[axis_position] = axis_length\n        return self.tile(x, tuple(repeats))\n\n    def tile(self, x, repeats):\n        """"""repeats is a number of  """"""\n        raise NotImplementedError()\n\n    def is_float_type(self, x):\n        # some backends (torch) can\'t compute average for non-floating types.\n        # Decided to drop average for all backends if type is not floating\n        raise NotImplementedError()\n\n    def layers(self, x):\n        raise NotImplementedError(""backend does not provide layers"")\n\n    def __repr__(self):\n        return ""<einops backend for {}>"".format(self.framework_name)\n\n\nclass UnknownSize:\n    """""" pseudo-symbol for symbolic frameworks which do not provide symbols for shape elements """"""\n\n    def __floordiv__(self, other):\n        return self\n\n    def __eq__(self, other):\n        return True  # we don\'t know actual size\n\n    def __mul__(self, other):\n        return self\n\n    def __rmul__(self, other):\n        return self\n\n    def __hash__(self):\n        return None.__hash__()\n\n\nclass NumpyBackend(AbstractBackend):\n    framework_name = \'numpy\'\n\n    def __init__(self):\n        import numpy\n        self.np = numpy\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, self.np.ndarray)\n\n    def from_numpy(self, x):\n        return x\n\n    def to_numpy(self, x):\n        return x\n\n    def arange(self, start, stop):\n        return self.np.arange(start, stop)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.np.stack(tensors)\n\n    def tile(self, x, repeats):\n        return self.np.tile(x, repeats)\n\n    def is_float_type(self, x):\n        return x.dtype in (\'float16\', \'float32\', \'float64\', \'float128\')\n\n    def add_axis(self, x, new_position):\n        return self.np.expand_dims(x, new_position)\n\n\nclass JaxBackend(NumpyBackend):\n    framework_name = \'jax\'\n\n    def __init__(self):\n        super(JaxBackend, self).__init__()\n        self.onp = self.np\n\n        import jax.numpy\n        self.np = jax.numpy\n\n    def from_numpy(self, x):\n        return self.np.asarray(x)\n\n    def to_numpy(self, x):\n        return self.onp.asarray(x)\n\n\nclass GluonBackend(AbstractBackend):\n    framework_name = \'mxnet.ndarray\'\n\n    def __init__(self):\n        import mxnet\n        self.mx = mxnet\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, self.mx.nd.NDArray)\n\n    def from_numpy(self, x):\n        if len(x.shape) == 0:\n            x = x[None]  # poor support of scalars in mxnet, otherwise mxnet can\'t attach gradients\n        var = self.mx.nd.array(x, dtype=x.dtype)\n        var.attach_grad()\n        return var\n\n    def to_numpy(self, x):\n        return self.mx.nd.NDArray.asnumpy(x)\n\n    def reshape(self, x, shape):\n        if len(shape) == 0:\n            return x  # poor support of scalars in mxnet\n        return x.reshape(shape)\n\n    def arange(self, start, stop):\n        return self.mx.nd.arange(start, stop)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.mx.nd.stack(*tensors)\n\n    def tile(self, x, repeats):\n        return self.mx.nd.tile(x, repeats)\n\n    def add_axis(self, x, new_position):\n        return self.mx.nd.expand_dims(x, new_position)\n\n    def is_float_type(self, x):\n        return \'float\' in str(x.dtype)\n\n    def layers(self):\n        from .layers import gluon\n        return gluon\n\n\nclass MXNetBackend(AbstractBackend):\n    framework_name = \'mxnet.symbol\'\n\n    def __init__(self):\n        import mxnet\n        self.mx = mxnet\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, self.mx.symbol.Symbol)\n\n    def create_symbol(self, shape, dtype=\'float32\'):\n        # mxnet accepts zeros as undefined dimensions\n        shape = tuple(0 if d is None else d for d in shape)\n        var = self.mx.symbol.Variable(\'input\', shape=shape, dtype=dtype)\n        return var\n\n    def eval_symbol(self, symbol, input_dict):\n        args = {var.name: self.mx.nd.array(val) for var, val in input_dict}\n        ex = symbol.bind(ctx=self.mx.cpu(), args=args)\n        ex.forward()\n        return ex.outputs[0].asnumpy()\n\n    def shape(self, x):\n        # mxnet has problems with shape inference - it does not provide shape symbols\n        # shape_array seems to be impossible to use in shape inference\n        # infer_shape_partial returns empty tuple if was not able to infer shape\n        # reductions such as sum can\'t return scalars, but return 1-element vectors\n        shape = x.infer_shape_partial()[1][0]\n        if len(shape) == 0:\n            warnings.warn(\'mxnet inferred shape to be (), which probably means it could not be inferred\')\n        shape = tuple(UnknownSize() if d == 0 else d for d in shape)\n        return shape\n\n    def reshape(self, x, shape):\n        if len(shape) == 0:\n            return x  # poor support of scalars in mxnet\n        if any(isinstance(dimension, UnknownSize) for dimension in shape):\n            from .einops import EinopsError\n            raise EinopsError(""Mxnet could\'t infer all dimensions statically, please provide those with axes_lengths"")\n        return x.reshape(shape)\n\n    def arange(self, start, stop):\n        return self.mx.symbol.arange(start, stop)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.mx.symbol.stack(*tensors)\n\n    def tile(self, x, repeats):\n        return self.mx.symbol.tile(x, repeats)\n\n    def add_axis(self, x, new_position):\n        return self.mx.symbol.expand_dims(x, new_position)\n\n    def is_float_type(self, x):\n        return \'float\' in str(x.infer_type()[1][0])\n\n    def layers(self):\n        from .layers import gluon\n        return gluon\n\n\nclass TorchBackend(AbstractBackend):\n    framework_name = \'torch\'\n\n    def __init__(self):\n        import torch\n        self.torch = torch\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, self.torch.Tensor)\n\n    def from_numpy(self, x):\n        variable = self.torch.from_numpy(x)\n        if self.is_float_type(variable):\n            # attach grad only to floating types\n            variable.requires_grad = True\n        return variable\n\n    def to_numpy(self, x):\n        return x.detach().cpu().numpy()\n\n    def arange(self, start, stop):\n        return self.torch.arange(start, stop, dtype=self.torch.int64)\n\n    def reduce(self, x, operation, reduced_axes):\n        for axis in sorted(reduced_axes, reverse=True):\n            if operation == \'min\':\n                x, _ = x.min(dim=axis)\n            elif operation == \'max\':\n                x, _ = x.max(dim=axis)\n            elif operation in [\'sum\', \'mean\', \'prod\']:\n                x = getattr(x, operation)(dim=axis)\n            else:\n                raise NotImplementedError(\'Unknown reduction \', operation)\n        return x\n\n    def transpose(self, x, axes):\n        return x.permute(axes)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.torch.stack(tensors)\n\n    def tile(self, x, repeats):\n        return x.repeat(repeats)\n\n    def add_axis(self, x, new_position):\n        return self.torch.unsqueeze(x, new_position)\n\n    def is_float_type(self, x):\n        return x.dtype in [self.torch.float16, self.torch.float32, self.torch.float64]\n\n    def layers(self):\n        from .layers import torch\n        return torch\n\n\nclass CupyBackend(AbstractBackend):\n    framework_name = \'cupy\'\n\n    def __init__(self):\n        import cupy\n        self.cupy = cupy\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, self.cupy.ndarray)\n\n    def from_numpy(self, x):\n        return self.cupy.asarray(x)\n\n    def to_numpy(self, x):\n        return self.cupy.asnumpy(x)\n\n    def arange(self, start, stop):\n        return self.cupy.arange(start, stop)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.cupy.stack(tensors)\n\n    def tile(self, x, repeats):\n        return self.cupy.tile(x, repeats)\n\n    def add_axis(self, x, new_position):\n        return self.cupy.expand_dims(x, new_position)\n\n    def is_float_type(self, x):\n        return x.dtype in (\'float16\', \'float32\', \'float64\', \'float128\')\n\n\nclass ChainerBackend(AbstractBackend):\n    framework_name = \'chainer\'\n\n    def __init__(self):\n        import chainer\n        import numpy\n        self.numpy = numpy\n        self.chainer = chainer\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, self.chainer.Variable)\n\n    def from_numpy(self, x):\n        return self.chainer.Variable(x.astype(\'float32\'))\n\n    def to_numpy(self, x):\n        if isinstance(x, self.chainer.Variable):\n            x = x.data\n        return x\n\n    def arange(self, start, stop):\n        return self.numpy.arange(start, stop)\n\n    def reduce(self, x, operation, axes):\n        return getattr(self.chainer.functions, operation)(x, axis=axes)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.chainer.functions.stack(tensors)\n\n    def tile(self, x, repeats):\n        return self.chainer.functions.tile(x, repeats)\n\n    def add_axis(self, x, new_position):\n        return self.chainer.functions.expand_dims(x, new_position)\n\n    def is_float_type(self, x):\n        return x.dtype in (\'float16\', \'float32\', \'float64\', \'float128\')\n\n    def layers(self):\n        from .layers import chainer\n        return chainer\n\n\nclass TensorflowBackend(AbstractBackend):\n    framework_name = \'tensorflow\'\n\n    def __init__(self):\n        import tensorflow\n        self.tf = tensorflow\n\n    def is_appropriate_type(self, tensor):\n        return isinstance(tensor, (self.tf.Tensor, self.tf.Variable))\n\n    def from_numpy(self, x):\n        assert self.tf.executing_eagerly()\n        return self.tf.convert_to_tensor(x)\n\n    def to_numpy(self, x):\n        assert self.tf.executing_eagerly()\n        return x.numpy()\n\n    def create_symbol(self, shape, dtype=\'float32\'):\n        assert not self.tf.executing_eagerly()\n        return self.tf.placeholder(dtype=dtype, shape=shape, name=\'einops_placeholder\')\n\n    def eval_symbol(self, symbol, input_dict):\n        assert not self.tf.executing_eagerly()\n        with self.tf.Session() as sess:\n            return sess.run(symbol, feed_dict=dict(input_dict))\n\n    def arange(self, start, stop):\n        return self.tf.range(start, stop)\n\n    def shape(self, x):\n        if self.tf.executing_eagerly():\n            return tuple(int(d) for d in x.shape)\n        else:\n            static_shape = x.shape.as_list()\n            tf_shape = self.tf.shape(x)\n\n            # use the static shape where known, otherwise use the TF shape\n            shape = [s or tf_shape[dim] for dim, s in enumerate(static_shape)]\n            return tuple(shape)\n\n    def reduce(self, x, operation, axes):\n        return getattr(self.tf, \'reduce_\' + operation)(x, axis=axes)\n\n    def reshape(self, x, shape):\n        return self.tf.reshape(x, shape)\n\n    def transpose(self, x, axes):\n        return self.tf.transpose(x, axes)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.tf.stack(tensors)\n\n    def tile(self, x, repeats):\n        return self.tf.tile(x, repeats)\n\n    def add_axis(self, x, new_position):\n        return self.tf.expand_dims(x, new_position)\n\n    def is_float_type(self, x):\n        return x.dtype in (\'float16\', \'float32\', \'float64\', \'float128\')\n\n\nclass KerasBackend(AbstractBackend):\n    framework_name = \'keras\'\n\n    def __init__(self):\n        import keras\n        self.keras = keras\n        self.K = keras.backend\n\n    def is_appropriate_type(self, tensor):\n        return self.K.is_tensor(tensor) and self.K.is_keras_tensor(tensor)\n\n    def create_symbol(self, shape):\n        return self.keras.Input(batch_shape=shape)\n\n    def eval_symbol(self, symbol, input_dict):\n        (variable, value), = input_dict\n        model = self.keras.models.Model(variable, symbol)\n        return model.predict_on_batch(value)\n\n    def arange(self, start, stop):\n        return self.K.arange(start, stop)\n\n    def shape(self, x):\n        shape = self.K.shape(x)  # tf tensor (if tf is backend)\n        return tuple(shape[i] for i in range(shape.shape[0]))\n\n    def reduce(self, x, operation, axes):\n        return getattr(self.K, operation)(x, axis=axes)\n\n    def reshape(self, x, shape):\n        return self.K.reshape(x, shape)\n\n    def transpose(self, x, axes):\n        return self.K.permute_dimensions(x, axes)\n\n    def stack_on_zeroth_dimension(self, tensors: list):\n        return self.K.stack(tensors)\n\n    def tile(self, x, repeats):\n        return self.K.tile(x, repeats)\n\n    def add_axis(self, x, new_position):\n        return self.K.expand_dims(x, new_position)\n\n    def is_float_type(self, x):\n        return \'float\' in self.K.dtype(x)\n\n    def layers(self):\n        from .layers import keras\n        return keras\n'"
einops/einops.py,0,"b'import functools\nimport itertools\nfrom collections import OrderedDict\nfrom typing import Tuple, List, Set, Dict\n\nimport math\n\nfrom ._backends import get_backend\n\n_reductions = (\'min\', \'max\', \'sum\', \'mean\', \'prod\')\n_ellipsis = \'\xe2\x80\xa6\'  # NB, this is a single unicode symbol. String is used as it is not a list, but can be iterated\n\n\ndef _product(sequence):\n    # minimalistic product that works both with numbers and symbols. Supports empty lists\n    result = 1\n    for element in sequence:\n        result *= element\n    return result\n\n\nclass EinopsError(RuntimeError):\n    """""" Runtime error thrown by einops """"""\n    pass\n\n\ndef _reduce_axes(tensor, reduction_type: str, reduced_axes: Tuple[int], backend):\n    reduced_axes = tuple(reduced_axes)\n    if len(reduced_axes) == 0:\n        return tensor\n    assert reduction_type in _reductions\n    if reduction_type == \'mean\':\n        if not backend.is_float_type(tensor):\n            raise NotImplementedError(\'reduce_mean is not available for non-floating tensors\')\n    return backend.reduce(tensor, reduction_type, reduced_axes)\n\n\ndef _optimize_transformation(init_shapes, reduced_axes, axes_reordering, final_shapes):\n    # TODO this method is very slow\n    assert len(axes_reordering) + len(reduced_axes) == len(init_shapes)\n    # joining consecutive axes that will be reduced\n    # possibly we can skip this if all backends can optimize this (not sure)\n    reduced_axes = tuple(sorted(reduced_axes))\n    for i in range(len(reduced_axes) - 1)[::-1]:\n        if reduced_axes[i] + 1 == reduced_axes[i + 1]:\n            removed_axis = reduced_axes[i + 1]\n            removed_length = init_shapes[removed_axis]\n            init_shapes = init_shapes[:removed_axis] + init_shapes[removed_axis + 1:]\n            init_shapes[removed_axis - 1] *= removed_length\n            reduced_axes = reduced_axes[:i + 1] + tuple(axis - 1 for axis in reduced_axes[i + 2:])\n\n    # removing axes that are moved together during reshape\n    def build_mapping():\n        init_to_final = {}\n        for axis in range(len(init_shapes)):\n            if axis in reduced_axes:\n                init_to_final[axis] = None\n            else:\n                after_reduction = sum(x is not None for x in init_to_final.values())\n                init_to_final[axis] = list(axes_reordering).index(after_reduction)\n        return init_to_final\n\n    init_axis_to_final_axis = build_mapping()\n\n    for init_axis in range(len(init_shapes) - 1)[::-1]:\n        if init_axis_to_final_axis[init_axis] is None:\n            continue\n        if init_axis_to_final_axis[init_axis + 1] is None:\n            continue\n        if init_axis_to_final_axis[init_axis] + 1 == init_axis_to_final_axis[init_axis + 1]:\n            removed_axis = init_axis + 1\n            removed_length = init_shapes[removed_axis]\n            removed_axis_after_reduction = sum(x not in reduced_axes for x in range(removed_axis))\n\n            reduced_axes = tuple(axis if axis < removed_axis else axis - 1 for axis in reduced_axes)\n            init_shapes = init_shapes[:removed_axis] + init_shapes[removed_axis + 1:]\n            init_shapes[removed_axis - 1] *= removed_length\n            old_reordering = axes_reordering\n            axes_reordering = []\n            for axis in old_reordering:\n                if axis == removed_axis_after_reduction:\n                    pass\n                elif axis < removed_axis_after_reduction:\n                    axes_reordering.append(axis)\n                else:\n                    axes_reordering.append(axis - 1)\n            init_axis_to_final_axis = build_mapping()\n\n    return init_shapes, reduced_axes, axes_reordering, final_shapes\n\n\nclass TransformRecipe:\n    """"""\n    Recipe describes actual computation pathway.\n    Recipe can be applied to a tensor or variable.\n    """"""\n    # structure is non-mutable. In future, this can be non-mutable dataclass (python 3.7+)\n\n    def __init__(self,\n                 # list of expressions (or just sizes) for elementary axes as they appear in left expression.\n                 # this is what (after computing unknown parts) will be a shape after first transposition.\n                 # If ellipsis is present, it forms one dimension here (in the right position).\n                 elementary_axes_lengths: List,\n                 # each dimension in input can help to reconstruct length of one elementary axis\n                 # or verify one of dimensions. Each element points to element of elementary_axes_lengths\n                 input_composite_axes: List[Tuple[List[int], List[int]]],\n                 # indices of axes to be squashed\n                 reduced_elementary_axes: Tuple[int],\n                 # in which order should axes be reshuffled after reduction\n                 axes_permutation: Tuple[int],\n                 # at which positions which of elementary axes should appear\n                 added_axes: Dict[int, int],\n                 # ids of axes as they appear in result, again pointers to elementary_axes_lengths,\n                 # only used to infer result dimensions\n                 output_composite_axes: List[List[int]],\n                 reduction_type: str = \'rearrange\',\n                 # positions of ellipsis in lhs and rhs of expression\n                 ellipsis_positions: Tuple[int, int] = (math.inf, math.inf),\n                 ):\n        self.elementary_axes_lengths = elementary_axes_lengths\n        self.input_composite_axes = input_composite_axes\n        self.output_composite_axes = output_composite_axes\n        # self.final_axes_grouping_flat = list(itertools.chain(*output_composite_axes))\n        self.axes_permutation = axes_permutation\n        self.added_axes = added_axes\n        self.reduction_type = reduction_type\n        # This is redundant information, but more convenient during to use in reconstruction\n        self.reduced_elementary_axes = reduced_elementary_axes\n        self.ellipsis_positions = ellipsis_positions\n\n    @functools.lru_cache(maxsize=1024)\n    def reconstruct_from_shape(self, shape, optimize=False):\n        """"""\n        Reconstruct all actual parameters using shape.\n        Shape is a tuple that may contain integers, shape symbols (tf, keras, theano) and UnknownSize (keras, mxnet)\n        known axes can be integers or symbols, but not Nones.\n        """"""\n        axes_lengths = list(self.elementary_axes_lengths)\n        if self.ellipsis_positions != (math.inf, math.inf):\n            if len(shape) < len(self.input_composite_axes) - 1:\n                raise EinopsError(\'Expected at least {} dimensions, got {}\'.format(\n                    len(self.input_composite_axes) - 1, len(shape)))\n        else:\n            if len(shape) != len(self.input_composite_axes):\n                raise EinopsError(\'Expected {} dimensions, got {}\'.format(len(self.input_composite_axes), len(shape)))\n        for input_axis, (known_axes, unknown_axes) in enumerate(self.input_composite_axes):\n            before_ellipsis = input_axis\n            after_ellipsis = input_axis + len(shape) - len(self.input_composite_axes)\n            if input_axis == self.ellipsis_positions[0]:\n                assert len(known_axes) == 0 and len(unknown_axes) == 1\n                unknown_axis, = unknown_axes\n                ellipsis_shape = shape[before_ellipsis:after_ellipsis + 1]\n                if any(d is None for d in ellipsis_shape):\n                    raise EinopsError(""Couldn\'t infer shape for one or more axes represented by ellipsis"")\n                axes_lengths[unknown_axis] = _product(ellipsis_shape)\n            else:\n                if input_axis < self.ellipsis_positions[0]:\n                    length = shape[before_ellipsis]\n                else:\n                    length = shape[after_ellipsis]\n                known_product = 1\n                for axis in known_axes:\n                    known_product *= axes_lengths[axis]\n\n                if len(unknown_axes) == 0:\n                    if isinstance(length, int) and isinstance(known_product, int) and length != known_product:\n                        raise EinopsError(\'Shape mismatch, {} != {}\'.format(length, known_product))\n                else:\n                    if isinstance(length, int) and isinstance(known_product, int) and length % known_product != 0:\n                        raise EinopsError(""Shape mismatch, can\'t divide axis of length {} in chunks of {}"".format(\n                            length, known_product))\n\n                    unknown_axis, = unknown_axes\n                    axes_lengths[unknown_axis] = length // known_product\n\n        # at this point all axes_lengths are computed (either have values or variables, but not Nones)\n\n        # TODO more readable expression. and confirm we don\'t want to deal with ellipsis\n        init_shapes = axes_lengths[:len(axes_lengths) - len(self.added_axes)]\n        # reduced_axes_lengths = [dim for i, dim in enumerate(axes_lengths) if i not in self.reduced_elementary_axes]\n        final_shapes = []\n        for output_axis, grouping in enumerate(self.output_composite_axes):\n            if output_axis == self.ellipsis_positions[1]:\n                final_shapes.extend(ellipsis_shape)\n            else:\n                lengths = [axes_lengths[elementary_axis] for elementary_axis in grouping]\n                final_shapes.append(_product(lengths))\n        reduced_axes = self.reduced_elementary_axes\n        axes_reordering = self.axes_permutation\n        added_axes = {pos: axes_lengths[pos_in_elementary] for pos, pos_in_elementary in self.added_axes.items()}\n        if optimize:\n            assert len(self.added_axes) == 0\n            return _optimize_transformation(init_shapes, reduced_axes, axes_reordering, final_shapes)\n        else:\n            return init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes\n\n    def apply(self, tensor):\n        backend = get_backend(tensor)\n        init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes = self.reconstruct_from_shape(\n            backend.shape(tensor))\n        tensor = backend.reshape(tensor, init_shapes)\n        tensor = _reduce_axes(tensor, reduction_type=self.reduction_type, reduced_axes=reduced_axes, backend=backend)\n        tensor = backend.transpose(tensor, axes_reordering)\n        if len(added_axes) > 0:\n            tensor = backend.add_axes(tensor, n_axes=len(axes_reordering) + len(added_axes), pos2len=added_axes)\n        return backend.reshape(tensor, final_shapes)\n\n\nCompositeAxis = List[str]\n\n\ndef parse_expression(expression: str) -> Tuple[Set[str], List[CompositeAxis]]:\n    """"""\n    Parses an indexing expression (for a single tensor).\n    Checks uniqueness of names, checks usage of \'...\' (allowed only once)\n    Returns set of all used identifiers and a list of axis groups.\n    """"""\n    identifiers = set()\n    composite_axes = []\n    if \'.\' in expression:\n        if \'...\' not in expression:\n            raise EinopsError(\'Expression may contain dots only inside ellipsis (...)\')\n        if str.count(expression, \'...\') != 1 or str.count(expression, \'.\') != 3:\n            raise EinopsError(\'Expression may contain dots only inside ellipsis (...); only one ellipsis for tensor \')\n        expression = expression.replace(\'...\', _ellipsis)\n\n    bracket_group = None\n\n    def add_axis_name(x):\n        if x is not None:\n            if x in identifiers:\n                raise ValueError(\'Indexing expression contains duplicate dimension ""{}""\'.format(x))\n            identifiers.add(x)\n            if bracket_group is None:\n                composite_axes.append([x])\n            else:\n                bracket_group.append(x)\n\n    current_identifier = None\n    for char in expression:\n        if char in \'() \' + _ellipsis:\n            add_axis_name(current_identifier)\n            current_identifier = None\n            if char == _ellipsis:\n                if bracket_group is not None:\n                    raise EinopsError(""Ellipsis can\'t be used inside the composite axis (inside brackets)"")\n                composite_axes.append(_ellipsis)\n                identifiers.add(_ellipsis)\n            elif char == \'(\':\n                if bracket_group is not None:\n                    raise EinopsError(""Axis composition is one-level (brackets inside brackets not allowed)"")\n                bracket_group = []\n            elif char == \')\':\n                if bracket_group is None:\n                    raise EinopsError(\'Brackets are not balanced\')\n                composite_axes.append(bracket_group)\n                bracket_group = None\n        elif \'0\' <= char <= \'9\':\n            if current_identifier is None:\n                raise EinopsError(""Axis name can\'t start with a digit"")\n            current_identifier += char\n        elif \'a\' <= char <= \'z\':\n            if current_identifier is None:\n                current_identifier = char\n            else:\n                current_identifier += char\n        else:\n            if \'A\' <= char <= \'Z\':\n                raise EinopsError(""Only lower-case latin letters allowed in names, not \'{}\'"".format(char))\n            raise EinopsError(""Unknown character \'{}\'"".format(char))\n\n    if bracket_group is not None:\n        raise EinopsError(\'Imbalanced parentheses in expression: ""{}""\'.format(expression))\n    add_axis_name(current_identifier)\n    return identifiers, composite_axes\n\n\ndef _parse_composite_axis(composite_axis_name: str):\n    axes_names = [axis for axis in composite_axis_name.split(\' \') if len(axis) > 0]\n    for axis in axes_names:\n        if axis == \'_\':\n            continue\n        assert \'a\' <= axis[0] <= \'z\'\n        for letter in axis:\n            assert str.isdigit(letter) or \'a\' <= letter <= \'z\'\n    return axes_names\n\n\ndef _check_elementary_axis_name(name: str) -> bool:\n    """"""\n    Valid elementary axes contain only lower latin letters and digits and start with a letter.\n    """"""\n    if len(name) == 0:\n        return False\n    if not \'a\' <= name[0] <= \'z\':\n        return False\n    for letter in name:\n        if (not letter.isdigit()) and not (\'a\' <= letter <= \'z\'):\n            return False\n    return True\n\n\n# TODO parenthesis within brackets\n# TODO add logaddexp, std, var, ptp, l1, l2\n@functools.lru_cache(256)\ndef _prepare_transformation_recipe(pattern: str, operation: str, axes_lengths: Tuple[Tuple]) -> TransformRecipe:\n    """""" Perform initial parsing of pattern and provided supplementary info\n    axes_lengths is a tuple of tuples (axis_name, axis_length)\n    """"""\n    left, right = pattern.split(\'->\')\n    identifiers_left, composite_axes_left = parse_expression(left)\n    identifiers_rght, composite_axes_rght = parse_expression(right)\n\n    # checking that both have similar letters\n    if operation == \'rearrange\':\n        difference = set.symmetric_difference(identifiers_left, identifiers_rght)\n        if len(difference) > 0:\n            raise EinopsError(\'Identifiers only on one side of expression (should be on both): {}\'.format(difference))\n    elif operation == \'repeat\':\n        difference = set.difference(identifiers_left, identifiers_rght)\n        if len(difference) > 0:\n            raise EinopsError(\'Unexpected identifiers on the left side of repeat: {}\'.format(difference))\n        axes_without_size = set.difference(identifiers_rght, {*identifiers_left, *(ax for ax, _ in axes_lengths)})\n        if len(axes_without_size) > 0:\n            raise EinopsError(\'Specify sizes for new axes in repeat: {}\'.format(axes_without_size))\n    elif operation in _reductions:\n        difference = set.difference(identifiers_rght, identifiers_left)\n        if len(difference) > 0:\n            raise EinopsError(\'Unexpected identifiers on the right side of reduce {}: {}\'.format(operation, difference))\n    else:\n        raise EinopsError(\'Unknown reduction {}. Expect one of {}.\'.format(operation, _reductions))\n\n    # parsing all dimensions to find out lengths\n    axis_name2known_length = OrderedDict()\n    for composite_axis in composite_axes_left:\n        for axis_name in composite_axis:\n            axis_name2known_length[axis_name] = None\n\n    # axis_ids_after_first_reshape = range(len(axis_name2known_length))\n    # position_lookup_after_reduction = {}\n\n    repeat_axes_names = []\n    for axis_name in identifiers_rght:\n        if axis_name not in axis_name2known_length:\n            axis_name2known_length[axis_name] = None\n            repeat_axes_names.append(axis_name)\n\n    axis_name2position = {name: position for position, name in enumerate(axis_name2known_length)}\n    reduced_axes = [position for axis, position in axis_name2position.items() if axis not in identifiers_rght]\n\n    for elementary_axis, axis_length in axes_lengths:\n        if not _check_elementary_axis_name(elementary_axis):\n            raise EinopsError(\'Invalid name for an axis\', elementary_axis)\n        if elementary_axis not in axis_name2known_length:\n            raise EinopsError(\'Axis {} is not used in transform\'.format(elementary_axis))\n        # check that element was not set, this can be deleted\n        assert axis_name2known_length[elementary_axis] is None\n        axis_name2known_length[elementary_axis] = axis_length\n\n    input_axes_known_unknown = []\n    # some of shapes will be inferred later - all information is prepared to know\n    for composite_axis in composite_axes_left:\n        known = {axis for axis in composite_axis if axis_name2known_length[axis] is not None}\n        unknown = {axis for axis in composite_axis if axis_name2known_length[axis] is None}\n        if len(unknown) > 1:\n            raise EinopsError(\'Could not infer sizes for {}\'.format(unknown))\n        assert len(unknown) + len(known) == len(composite_axis)\n        input_axes_known_unknown.append(\n            ([axis_name2position[axis] for axis in known], [axis_name2position[axis] for axis in unknown]))\n\n    axis_position_after_reduction = {}\n    for axis_name in itertools.chain(*composite_axes_left):\n        if axis_name in identifiers_rght:\n            axis_position_after_reduction[axis_name] = len(axis_position_after_reduction)\n\n    result_axes_grouping = [[axis_name2position[axis] for axis in composite_axis]\n                            for composite_axis in composite_axes_rght]\n    ordered_axis_right = list(itertools.chain(*composite_axes_rght))\n    axes_permutation = tuple(\n        axis_position_after_reduction[axis] for axis in ordered_axis_right if axis in identifiers_left)\n    added_axes = {i: axis_name2position[axis_name] for i, axis_name in enumerate(ordered_axis_right)\n                  if axis_name not in identifiers_left}\n\n    ellipsis_left = math.inf if _ellipsis not in composite_axes_left else composite_axes_left.index(_ellipsis)\n    ellipsis_rght = math.inf if _ellipsis not in composite_axes_rght else composite_axes_rght.index(_ellipsis)\n\n    return TransformRecipe(\n        elementary_axes_lengths=list(axis_name2known_length.values()),\n        input_composite_axes=input_axes_known_unknown,\n        reduced_elementary_axes=tuple(reduced_axes),\n        axes_permutation=axes_permutation,\n        added_axes=added_axes,\n        output_composite_axes=result_axes_grouping,\n        reduction_type=operation,\n        # TODO get rid of ellipses position on right side, put marks directly\n        ellipsis_positions=(ellipsis_left, ellipsis_rght)\n    )\n\n\ndef reduce(tensor, pattern: str, reduction: str, **axes_lengths: int):\n    """"""\n    einops.reduce provides combination of reordering and reduction using reader-friendly notation.\n    \n    Examples for reduce operation:\n    \n    >>> x = np.random.randn(100, 32, 64)\n    >>> # perform max-reduction on the first axis\n    >>> y = reduce(x, \'t b c -> b c\', \'max\')\n    >>> # same as previous, but with clearer axes meaning\n    >>> y = reduce(x, \'time batch channel -> batch channel\', \'max\')\n\n    >>> x = np.random.randn(10, 20, 30, 40)\n    >>> # 2d max-pooling with kernel size = 2 * 2 for image processing\n    >>> y1 = reduce(x, \'b c (h1 h2) (w1 w2) -> b c h1 w1\', \'max\', h2=2, w2=2)\n    >>> # if one wants to go back to the original height and width, depth-to-space trick can be applied\n    >>> y2 = rearrange(y1, \'b (c h2 w2) h1 w1 -> b c (h1 h2) (w1 w2)\', h2=2, w2=2)\n    >>> assert parse_shape(x, \'b _ h w\') == parse_shape(y2, \'b _ h w\')\n    >>> # Adaptive 2d max-pooling to 3 * 4 grid\n    >>> reduce(x, \'b c (h1 h2) (w1 w2) -> b c h1 w1\', \'max\', h1=3, w1=4).shape\n    (10, 20, 3, 4)\n    >>> # Global average pooling\n    >>> reduce(x, \'b c h w -> b c\', \'mean\').shape\n    (10, 20)\n    >>> # Subtracting mean over batch for each channel\n    >>> y = x - reduce(x, \'b c h w -> () c () ()\', \'mean\')\n    >>> # Subtracting per-image mean for each channel\n    >>> y = x - reduce(x, \'b c h w -> b c () ()\', \'mean\') \n    \n    :param tensor: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray).\n            list of tensors is also accepted, those should be of the same type and shape\n    :param pattern: string, reduction pattern\n    :param reduction: one of available reductions (\'min\', \'max\', \'sum\', \'mean\', \'prod\'), case-sensitive\n    :param axes_lengths: any additional specifications for dimensions\n    :return: tensor of the same type as input\n    """"""\n    try:\n        hashable_axes_lengths = tuple(sorted(axes_lengths.items()))\n        recipe = _prepare_transformation_recipe(pattern, reduction, axes_lengths=hashable_axes_lengths)\n        return recipe.apply(tensor)\n    except EinopsError as e:\n        message = \' Error while processing {}-reduction pattern ""{}"".\'.format(reduction, pattern)\n        if not isinstance(tensor, list):\n            message += \'\\n Input tensor shape: {}. \'.format(get_backend(tensor).shape(tensor))\n        else:\n            message += \'\\n Input is list. \'\n        message += \'Additional info: {}.\'.format(axes_lengths)\n        raise EinopsError(message + \'\\n {}\'.format(e))\n\n\ndef rearrange(tensor, pattern, **axes_lengths):\n    """"""\n    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\n    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\n    stack, concatenate and other operations.\n\n    Examples for rearrange operation:\n\n    >>> # suppose we have a set of 32 images in ""h w c"" format (height-width-channel)\n    >>> images = [np.random.randn(30, 40, 3) for _ in range(32)]\n    >>> # stack along first (batch) axis, output is a single array\n    >>> rearrange(images, \'b h w c -> b h w c\').shape\n    (32, 30, 40, 3)\n    >>> # concatenate images along height (vertical axis), 960 = 32 * 30\n    >>> rearrange(images, \'b h w c -> (b h) w c\').shape\n    (960, 40, 3)\n    >>> # concatenated images along horizontal axis, 1280 = 32 * 40\n    >>> rearrange(images, \'b h w c -> h (b w) c\').shape\n    (30, 1280, 3)\n    >>> # reordered axes to ""b c h w"" format for deep learning\n    >>> rearrange(images, \'b h w c -> b c h w\').shape\n    (32, 3, 30, 40)\n    >>> # flattened each image into a vector, 3600 = 30 * 40 * 3\n    >>> rearrange(images, \'b h w c -> b (c h w)\').shape\n    (32, 3600)\n    >>> # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2\n    >>> rearrange(images, \'b (h1 h) (w1 w) c -> (b h1 w1) h w c\', h1=2, w1=2).shape\n    (128, 15, 20, 3)\n    >>> # space-to-depth operation\n    >>> rearrange(images, \'b (h h1) (w w1) c -> b h w (c h1 w1)\', h1=2, w1=2).shape\n    (32, 15, 20, 12)\n\n    :param tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray).\n            list of tensors is also accepted, those should be of the same type and shape\n    :param pattern: string, rearrangement pattern\n    :param axes_lengths: any additional specifications for dimensions\n    :return: tensor of the same type as input. If possible, a view to the original tensor is returned.\n\n    When composing axes, C-order enumeration used (consecutive elements have different last axis)\n    Find more examples in einops tutorial.\n    """"""\n    if isinstance(tensor, list):\n        if len(tensor) == 0:\n            raise TypeError(""Rearrange can\'t be applied to an empty list"")\n        tensor = get_backend(tensor[0]).stack_on_zeroth_dimension(tensor)\n    return reduce(tensor, pattern, reduction=\'rearrange\', **axes_lengths)\n\n\ndef repeat(tensor, pattern, **axes_lengths):\n    """"""\n    einops.repeat allows reordering elements and repeating them in arbitrary combinations.\n    This operation includes functionality of repeat, tile, broadcast functions.\n\n    Examples for repeat operation:\n    >>> # a grayscale image (of shape height x width)\n    >>> image = np.random.randn(30, 40)\n    >>> # change it to RGB format by repeating in each channel\n    >>> repeat(image, \'h w -> h w c\', c=3).shape\n    (30, 40, 3)\n    >>> # repeat image 2 times along height (vertical axis)\n    >>> repeat(image, \'h w -> (repeat h) w\', repeat=2).shape\n    (60, 40)\n    >>> # repeat image 2 time along height and 3 times along width\n    >>> repeat(image, \'h w -> h (repeat w)\', repeat=3).shape\n    (30, 120)\n    >>> # convert each pixel to a small square 2x2. Upsample image by 2x\n    >>> repeat(image, \'h w -> (h h2) (w w2)\', h2=2, w2=2).shape\n    (60, 80)\n    >>> # pixelate image first by downsampling by 2x, then upsampling\n    >>> downsampled = reduce(image, \'(h h2) (w w2) -> h w\', \'mean\', h2=2, w2=2)\n    >>> repeat(downsampled, \'h w -> (h h2) (w w2)\', h2=2, w2=2).shape\n    (30, 40)\n\n    :param tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray).\n            list of tensors is also accepted, those should be of the same type and shape\n    :param pattern: string, rearrangement pattern\n    :param axes_lengths: any additional specifications for dimensions\n    :return: tensor of the same type as input. If possible, a view to the original tensor is returned.\n\n    When composing axes, C-order enumeration used (consecutive elements have different last axis)\n    Find more examples in einops tutorial.\n    """"""\n    return reduce(tensor, pattern, reduction=\'repeat\', **axes_lengths)\n\n\ndef parse_shape(x, pattern: str):\n    """"""\n    Parse a tensor shape to dictionary mapping axes names to their lengths.\n    Use underscore to skip the dimension in parsing.\n    >>> x = np.zeros([2, 3, 5, 7])\n    >>> parse_shape(x, \'batch _ h w\')\n    {\'batch\': 2, \'h\': 5, \'w\': 7}\n\n    parse_shape output can be used to specify axes_lengths for other operations\n    >>> y = np.zeros([700])\n    >>> rearrange(y, \'(b c h w) -> b c h w\', **parse_shape(x, \'b _ h w\')).shape\n    (2, 10, 5, 7)\n\n    For symbolic frameworks may return symbols, not integers.\n    :param x: tensor of any of supported frameworks\n    :param pattern: str, space separated names for axes, underscore means skip axis\n    :return: dict, maps axes names to their lengths\n    """"""\n    names = [elementary_axis for elementary_axis in pattern.split(\' \') if len(elementary_axis) > 0]\n    shape = get_backend(x).shape(x)\n    if len(shape) != len(names):\n        raise RuntimeError(""Can\'t parse shape with different number of dimensions: {pattern} {shape}"".format(\n            pattern=pattern, shape=shape))\n    result = {}\n    for axis_name, axis_length in zip(names, shape):\n        if axis_name != \'_\':\n            result[axis_name] = axis_length\n    return result\n\n\n# this one is probably not needed in the public API\ndef _enumerate_directions(x):\n    """"""\n    For an n-dimensional tensor, returns tensors to enumerate each axis.\n    >>> x = np.zeros([2, 3, 4]) # or any other tensor\n    >>> i, j, k = _enumerate_directions(x)\n    >>> result = i + 2 * j + 3 * k\n\n    result[i, j, k] = i + 2 * j + 3 * k, and also has the same shape as result\n    Works very similarly to numpy.ogrid (open indexing grid)\n    """"""\n    backend = get_backend(x)\n    shape = backend.shape(x)\n    result = []\n    for axis_id, axis_length in enumerate(shape):\n        shape = [1] * len(shape)\n        shape[axis_id] = axis_length\n        result.append(backend.reshape(backend.arange(0, axis_length), shape))\n    return result\n\n\ndef asnumpy(tensor):\n    """"""\n    Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/gluon/etc.) to numpy.ndarray\n\n    :param tensor: tensor of any of known imperative framework\n    :return: numpy.ndarray, converted to numpy\n    """"""\n    return get_backend(tensor).to_numpy(tensor)\n'"
tests/__init__.py,0,"b'import os\n\nfrom einops import _backends\nimport warnings\n\n__author__ = \'Alex Rogozhnikov\'\n\nimport logging\n\n# minimize noise in tests logging\nlogging.getLogger(\'tensorflow\').disabled = True\nlogging.getLogger(\'matplotlib\').disabled = True\n\nassert os.environ.get(\'TF_EAGER\', \'\') in [\'\', \'1\', \'0\']\nassert os.environ.get(\'EINOPS_SKIP_CUPY\', \'\') in [\'\', \'1\', \'0\']\nskip_cupy = os.environ.get(\'EINOPS_SKIP_CUPY\', \'\') == \'1\'\n\n\nif os.environ.get(\'TF_EAGER\', \'\') == \'1\':\n    try:\n        import tensorflow\n        tensorflow.enable_eager_execution()\n        print(\'testing with eager execution\')\n    except:\n        pass\n\n\ndef collect_test_backends(symbolic=False, layers=False):\n    """"""\n    :param symbolic: symbolic or imperative frameworks?\n    :param layers: layers or operation?\n    :return: list of backends satisfying set conditions\n    """"""\n    tf_running_eagerly = True\n    try:\n        import tensorflow\n        tf_running_eagerly = tensorflow.executing_eagerly()\n    except ImportError:\n        print(""Couldn\'t import tensorflow for testing"")\n\n    if not symbolic:\n        if not layers:\n            backend_types = [_backends.NumpyBackend,\n                             _backends.JaxBackend,\n                             _backends.TorchBackend,\n                             _backends.GluonBackend,\n                             _backends.ChainerBackend,\n                             ]\n            if tf_running_eagerly:\n                backend_types += [_backends.TensorflowBackend]\n            if not skip_cupy:\n                backend_types += [_backends.CupyBackend]\n        else:\n            backend_types = [_backends.TorchBackend,\n                             _backends.GluonBackend,\n                             _backends.ChainerBackend]\n    else:\n        if not layers:\n            backend_types = [_backends.MXNetBackend]\n            if not tf_running_eagerly:\n                backend_types += [_backends.TensorflowBackend]\n        else:\n            backend_types = [_backends.MXNetBackend]\n            if not tf_running_eagerly:\n                backend_types += [_backends.KerasBackend]\n\n    result = []\n    for backend_type in backend_types:\n        try:\n            result.append(backend_type())\n        except ImportError:\n            warnings.warn(\'backend could not be initialized for tests: {}\'.format(backend_type))\n    return result\n'"
tests/test_layers.py,0,"b""import pickle\nimport tempfile\nfrom collections import namedtuple\n\nimport numpy\n\nfrom einops import rearrange, reduce\nfrom einops.einops import _reductions\nfrom . import collect_test_backends\n\n__author__ = 'Alex Rogozhnikov'\n\ntestcase = namedtuple('testcase', ['pattern', 'axes_lengths', 'input_shape', 'wrong_shapes'])\n\nrearrangement_patterns = [\n    testcase('b c h w -> b (c h w)', dict(c=20), (10, 20, 30, 40),\n             [(), (10,), (10, 10, 10), (10, 21, 30, 40), [1, 20, 1, 1, 1]]),\n    testcase('b c (h1 h2) (w1 w2) -> b (c h2 w2) h1 w1', dict(h2=2, w2=2), (10, 20, 30, 40),\n             [(), (1, 1, 1, 1), (1, 10, 3), ()]),\n    testcase('b ... c -> c b ...', dict(b=10), (10, 20, 30),\n             [(), (10,), (5, 10)]),\n]\n\n\ndef test_rearrange_imperative():\n    for backend in collect_test_backends(symbolic=False, layers=True):\n        print('Test layer for ', backend.framework_name)\n\n        for pattern, axes_lengths, input_shape, wrong_shapes in rearrangement_patterns:\n            x = numpy.arange(numpy.prod(input_shape), dtype='float32').reshape(input_shape)\n            result_numpy = rearrange(x, pattern, **axes_lengths)\n            layer = backend.layers().Rearrange(pattern, **axes_lengths)\n            for shape in wrong_shapes:\n                try:\n                    layer(backend.from_numpy(numpy.zeros(shape, dtype='float32')))\n                except:\n                    pass\n                else:\n                    raise AssertionError('Failure expected')\n\n            # simple pickling / unpickling\n            layer2 = pickle.loads(pickle.dumps(layer))\n            result1 = backend.to_numpy(layer(backend.from_numpy(x)))\n            result2 = backend.to_numpy(layer2(backend.from_numpy(x)))\n            assert numpy.allclose(result_numpy, result1)\n            assert numpy.allclose(result1, result2)\n\n            just_sum = backend.layers().Reduce('...->', reduction='sum')\n\n            if 'mxnet' in backend.framework_name:\n                with backend.mx.autograd.record():\n                    variable = backend.from_numpy(x)\n                    result = just_sum(layer(variable))\n            else:\n                variable = backend.from_numpy(x)\n                result = just_sum(layer(variable))\n\n            result.backward()\n            assert numpy.allclose(backend.to_numpy(variable.grad), 1)\n\n\ndef test_rearrange_symbolic():\n    for backend in collect_test_backends(symbolic=True, layers=True):\n        print('Test layer for ', backend.framework_name)\n\n        for pattern, axes_lengths, input_shape, wrong_shapes in rearrangement_patterns:\n            x = numpy.arange(numpy.prod(input_shape), dtype='float32').reshape(input_shape)\n            result_numpy = rearrange(x, pattern, **axes_lengths)\n            layer = backend.layers().Rearrange(pattern, **axes_lengths)\n            shapes = [input_shape]\n            if 'mxnet' not in backend.framework_name:\n                shapes.append([None] * len(input_shape))\n\n            for shape in shapes:\n                symbol = backend.create_symbol(shape)\n                eval_inputs = [(symbol, x)]\n\n                result_symbol1 = layer(symbol)\n                result1 = backend.eval_symbol(result_symbol1, eval_inputs)\n                assert numpy.allclose(result_numpy, result1)\n\n                if 'keras' not in backend.framework_name:\n                    # simple pickling / unpickling\n                    # keras bug - fails for pickling\n                    layer2 = pickle.loads(pickle.dumps(layer))\n                    result_symbol2 = layer2(symbol)\n                    result2 = backend.eval_symbol(result_symbol2, eval_inputs)\n                    assert numpy.allclose(result1, result2)\n                else:\n                    import keras\n                    import einops.layers.keras\n                    model = keras.models.Model(symbol, result_symbol1)\n                    result2 = model.predict_on_batch(x)\n\n                    # create a temporary file using a context manager\n                    with tempfile.NamedTemporaryFile(mode='r+b') as fp:\n                        keras.models.save_model(model, fp.name)\n                        model2 = keras.models.load_model(fp.name,\n                                                         custom_objects=einops.layers.keras.keras_custom_objects)\n\n                    result3 = model2.predict_on_batch(x)\n                    assert numpy.allclose(result1, result2)\n                    assert numpy.allclose(result1, result3)\n\n                # now testing back-propagation\n                just_sum = backend.layers().Reduce('...->', reduction='sum')\n\n                result_sum1 = backend.eval_symbol(just_sum(result_symbol1), eval_inputs)\n                result_sum2 = numpy.sum(x)\n\n                assert numpy.allclose(result_sum1, result_sum2)\n\n\nreduction_patterns = rearrangement_patterns + [\n    testcase('b c h w -> b ()', dict(b=10), (10, 20, 30, 40),\n             [(10,), (10, 20, 30)]),\n    testcase('b c (h1 h2) (w1 w2) -> b c h1 w1', dict(h1=15, h2=2, w2=2), (10, 20, 30, 40),\n             [(10, 20, 31, 40)]),\n    testcase('b ... c -> b', dict(b=10), (10, 20, 30, 40),\n             [(10,), (11, 10)]),\n]\n\n\ndef test_reduce_imperative():\n    for backend in collect_test_backends(symbolic=False, layers=True):\n        print('Test layer for ', backend.framework_name)\n        for reduction in _reductions:\n            for pattern, axes_lengths, input_shape, wrong_shapes in reduction_patterns:\n                print(backend, reduction, pattern, axes_lengths, input_shape, wrong_shapes)\n                x = numpy.arange(1, 1 + numpy.prod(input_shape), dtype='float32').reshape(input_shape)\n                x /= x.mean()\n                result_numpy = reduce(x, pattern, reduction, **axes_lengths)\n                layer = backend.layers().Reduce(pattern, reduction, **axes_lengths)\n                for shape in wrong_shapes:\n                    try:\n                        layer(backend.from_numpy(numpy.zeros(shape, dtype='float32')))\n                    except:\n                        pass\n                    else:\n                        raise AssertionError('Failure expected')\n\n                # simple pickling / unpickling\n                layer2 = pickle.loads(pickle.dumps(layer))\n                result1 = backend.to_numpy(layer(backend.from_numpy(x)))\n                result2 = backend.to_numpy(layer2(backend.from_numpy(x)))\n                assert numpy.allclose(result_numpy, result1)\n                assert numpy.allclose(result1, result2)\n\n                just_sum = backend.layers().Reduce('...->', reduction='sum')\n\n                if 'mxnet' in backend.framework_name:\n                    with backend.mx.autograd.record():\n                        variable = backend.from_numpy(x)\n                        result = just_sum(layer(variable))\n                else:\n                    variable = backend.from_numpy(x)\n                    result = just_sum(layer(variable))\n\n                result.backward()\n                grad = backend.to_numpy(variable.grad)\n                if reduction == 'sum':\n                    assert numpy.allclose(grad, 1)\n                if reduction == 'mean':\n                    assert numpy.allclose(grad, grad.min())\n                if reduction in ['max', 'min']:\n                    assert numpy.all(numpy.in1d(grad, [0, 1]))\n                    assert numpy.sum(grad) > 0.5\n\n\ndef test_reduce_symbolic():\n    for backend in collect_test_backends(symbolic=True, layers=True):\n        print('Test layer for ', backend.framework_name)\n        for reduction in _reductions:\n            for pattern, axes_lengths, input_shape, wrong_shapes in reduction_patterns:\n                x = numpy.arange(1, 1 + numpy.prod(input_shape), dtype='float32').reshape(input_shape)\n                x /= x.mean()\n                result_numpy = reduce(x, pattern, reduction, **axes_lengths)\n                layer = backend.layers().Reduce(pattern, reduction, **axes_lengths)\n                shapes = [input_shape]\n                if 'mxnet' not in backend.framework_name:\n                    shapes.append([None] * len(input_shape))\n\n                for shape in shapes:\n                    symbol = backend.create_symbol(shape)\n                    eval_inputs = [(symbol, x)]\n\n                    result_symbol1 = layer(symbol)\n                    result1 = backend.eval_symbol(result_symbol1, eval_inputs)\n                    assert numpy.allclose(result_numpy, result1)\n\n                    if 'keras' not in backend.framework_name:\n                        # simple pickling / unpickling\n                        # keras bug - fails for pickling\n                        layer2 = pickle.loads(pickle.dumps(layer))\n                        result_symbol2 = layer2(symbol)\n                        result2 = backend.eval_symbol(result_symbol2, eval_inputs)\n                        assert numpy.allclose(result1, result2)\n                    else:\n                        import keras\n                        import einops.layers.keras\n                        model = keras.models.Model(symbol, result_symbol1)\n                        result2 = model.predict_on_batch(x)\n\n                        # create a temporary file using a context manager\n                        with tempfile.NamedTemporaryFile(mode='r+b') as fp:\n                            keras.models.save_model(model, fp.name)\n                            model2 = keras.models.load_model(fp.name,\n                                                             custom_objects=einops.layers.keras.keras_custom_objects)\n\n                        result3 = model2.predict_on_batch(x)\n                        assert numpy.allclose(result1, result2)\n                        assert numpy.allclose(result1, result3)\n\n\ndef test_torch_layer():\n    if any(backend.framework_name == 'torch' for backend in collect_test_backends(symbolic=False, layers=True)):\n        # checked that torch present\n        import torch\n        from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\n        from einops.layers.torch import Rearrange, Reduce\n\n        def create_model(use_reduce=False):\n            return Sequential(\n                Conv2d(3, 6, kernel_size=5),\n                Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2) if use_reduce else MaxPool2d(kernel_size=2),\n                Conv2d(6, 16, kernel_size=5),\n                Reduce('b c (h h2) (w w2) -> b (c h w)', 'max', h2=2, w2=2),\n                Linear(16 * 5 * 5, 120),\n                ReLU(),\n                Linear(120, 84),\n                ReLU(),\n                Linear(84, 10),\n            )\n\n        model1 = create_model(use_reduce=True)\n        model2 = create_model(use_reduce=False)\n        input = torch.randn([10, 3, 32, 32])\n        # random models have different predictions\n        assert not torch.allclose(model1(input), model2(input))\n        model2.load_state_dict(pickle.loads(pickle.dumps(model1.state_dict())))\n        assert torch.allclose(model1(input), model2(input))\n\n        # tracing (freezing)\n        model3 = torch.jit.trace(model2, example_inputs=input)\n        torch.testing.assert_allclose(model1(input), model3(input), atol=1e-3, rtol=1e-3)\n        torch.testing.assert_allclose(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3)\n\n        model4 = torch.jit.trace(model2, example_inputs=input, optimize=True)\n        torch.testing.assert_allclose(model1(input), model4(input), atol=1e-3, rtol=1e-3)\n        torch.testing.assert_allclose(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3)\n\n\ndef test_keras_layer():\n    if any(backend.framework_name == 'keras' for backend in collect_test_backends(symbolic=True, layers=True)):\n        # checked that keras present\n\n        import keras\n        from keras.models import Sequential\n        from keras.layers import MaxPool2D as MaxPool2d, Conv2D as Conv2d, Dense as Linear, ReLU\n        from einops.layers.keras import Rearrange, Reduce, keras_custom_objects\n\n        def create_model():\n            return Sequential([\n                Conv2d(6, kernel_size=5, input_shape=[32, 32, 3]),\n                Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2),\n                Conv2d(16, kernel_size=5),\n                Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2),\n                Rearrange('b c h w -> b (c h w)'),\n                Linear(120),\n                ReLU(),\n                Linear(84),\n                ReLU(),\n                Linear(10),\n            ])\n\n        model1 = create_model()\n        model2 = create_model()\n        input = numpy.random.normal(size=[10, 32, 32, 3]).astype('float32')\n        assert not numpy.allclose(model1.predict_on_batch(input), model2.predict_on_batch(input))\n\n        # save arch + weights\n        with tempfile.NamedTemporaryFile(mode='r+b') as fp:\n            keras.models.save_model(model1, fp.name)\n            model3 = keras.models.load_model(fp.name, custom_objects=keras_custom_objects)\n        assert numpy.allclose(model1.predict_on_batch(input), model3.predict_on_batch(input))\n\n        # save arch as json, md5\n        model4 = keras.models.model_from_json(model1.to_json(), custom_objects=keras_custom_objects)\n        with tempfile.NamedTemporaryFile(mode='r+b') as fp:\n            model1.save_weights(fp.name)\n            model4.load_weights(fp.name)\n            model2.load_weights(fp.name)\n        assert numpy.allclose(model1.predict_on_batch(input), model4.predict_on_batch(input))\n        assert numpy.allclose(model1.predict_on_batch(input), model2.predict_on_batch(input))\n\n\ndef test_gluon_layer():\n    if any('mxnet' in backend.framework_name for backend in collect_test_backends(symbolic=False, layers=True)):\n        # checked that gluon present\n        import mxnet\n        from mxnet.gluon.nn import HybridSequential, Dense, Conv2D, LeakyReLU\n        from einops.layers.gluon import Rearrange, Reduce\n        from einops import asnumpy\n\n        def create_model():\n            model = HybridSequential()\n            layers = [\n                Conv2D(6, kernel_size=5),\n                Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2),\n                Conv2D(16, kernel_size=5),\n                Reduce('b c (h h2) (w w2) -> b c h w', 'max', h2=2, w2=2),\n                Rearrange('b c h w -> b (c h w)'),\n                Dense(120),\n                LeakyReLU(alpha=0.0),\n                Dense(84),\n                LeakyReLU(alpha=0.0),\n                Dense(10),\n            ]\n            for layer in layers:\n                model.add(layer)\n            model.initialize(mxnet.init.Xavier(), ctx=mxnet.cpu())\n            return model\n\n        model1 = create_model()\n        model2 = create_model()\n        x = mxnet.ndarray.random_normal(shape=[10, 3, 32, 32])\n        assert not numpy.allclose(asnumpy(model1(x)), asnumpy(model2(x)))\n\n        with tempfile.NamedTemporaryFile(mode='r+b') as fp:\n            model1.save_parameters(fp.name)\n            model2.load_parameters(fp.name)\n\n        assert numpy.allclose(asnumpy(model1(x)), asnumpy(model2(x)))\n\n        # testing with symbolic (NB with fixed dimensions!)\n        input = mxnet.sym.Variable('data', shape=x.shape)\n        json = model1(input).tojson()\n        model3 = mxnet.gluon.SymbolBlock(outputs=mxnet.sym.load_json(json), inputs=input)\n        model4 = mxnet.gluon.SymbolBlock(outputs=mxnet.sym.load_json(json), inputs=input)\n        model3.initialize(ctx=mxnet.cpu())\n        model3(x)\n\n        with tempfile.NamedTemporaryFile(mode='r+b') as fp:\n            model3.save_parameters(fp.name)\n            model4.load_parameters(fp.name)\n        assert numpy.allclose(asnumpy(model3(x)), asnumpy(model4(x)))\n\n        try:\n            # hybridization doesn't work\n            model1.hybridize(static_alloc=True, static_shape=True)\n            model1(x)\n        except:\n            pass\n"""
tests/test_notebooks.py,0,"b'from typing import Dict\n\nfrom io import StringIO\n\nfrom tests import collect_test_backends\n\n__author__ = \'Alex Rogozhnikov\'\n\nfrom pathlib import Path\nimport nbformat\nfrom nbconvert.preprocessors import ExecutePreprocessor\n\n\ndef render_notebook(filename: Path, replacements: Dict[str, str]) -> str:\n    """""" Takes path to the notebook, returns executed and rendered version\n    :param filename: notebook\n    :param replacements: dictionary with text replacements done before executing\n    :return: notebook, rendered as string\n    """"""\n    with filename.open(\'r\') as f:\n        nb_as_str = f.read()\n    for original, replacement in replacements.items():\n        nb_as_str = nb_as_str.replace(original, replacement)\n\n    nb = nbformat.read(StringIO(nb_as_str), nbformat.NO_CONVERT)\n    ep = ExecutePreprocessor(timeout=60, kernel_name=\'python3\')\n    ep.preprocess(nb, {\'metadata\': {\'path\': str(filename.parent.absolute())}})\n\n    result_as_stream = StringIO()\n    nbformat.write(nb, result_as_stream)\n    return result_as_stream.getvalue()\n\n\ndef test_all_notebooks():\n    notebooks = Path(__file__).parent.with_name(\'docs\').glob(\'*.ipynb\')\n    for notebook in notebooks:\n        render_notebook(notebook, replacements={})\n\n\ndef test_dl_notebook_with_all_backends():\n    notebook, = Path(__file__).parent.with_name(\'docs\').glob(\'2-*.ipynb\')\n    backends = [\'chainer\', \'gluon\', \'pytorch\']\n    if \'tensorflow\' in collect_test_backends(symbolic=False, layers=False):\n        backends += [\'tensorflow\']\n    for backend in backends:\n        print(\'Testing {} with backend {}\'.format(notebook, backend))\n        replacements = {""flavour = \'pytorch\'"": ""flavour = \'{}\'"".format(backend)}\n        expected_string = \'selected {} backend\'.format(backend)\n        result = render_notebook(notebook, replacements=replacements)\n        assert expected_string in result\n'"
tests/test_ops.py,0,"b'import itertools\n\nimport numpy\n\nfrom einops.einops import (rearrange, reduce, parse_shape, _enumerate_directions, _reductions)\nfrom . import collect_test_backends\n\nimp_op_backends = collect_test_backends(symbolic=False, layers=False)\nsym_op_backends = collect_test_backends(symbolic=True, layers=False)\n\nidentity_patterns = [\n    \'...->...\',\n    \'a b c d e-> a b c d e\',\n    \'a b c d e ...-> ... a b c d e\',\n    \'a b c d e ...-> a ... b c d e\',\n    \'... a b c d e -> ... a b c d e\',\n    \'a ... e-> a ... e\',\n    \'a ... -> a ... \',\n]\n\nequivalent_rearrange_patterns = [\n    (\'a b c d e -> (a b) c d e\', \'a b ... -> (a b) ... \'),\n    (\'a b c d e -> a b (c d) e\', \'... c d e -> ... (c d) e\'),\n    (\'a b c d e -> a b c d e\', \'... -> ... \'),\n]\n\nequivalent_reduction_patterns = [\n    (\'a b c d e -> \', \' ... ->  \'),\n    (\'a b c d e -> (e a)\', \'a ... e -> (e a)\'),\n    (\'a b c d e -> d (a e)\', \' a b c d e ... -> d (a e) \'),\n]\n\n\ndef test_ellipsis_ops_numpy():\n    x = numpy.arange(2 * 3 * 4 * 5 * 6).reshape([2, 3, 4, 5, 6])\n    for pattern in identity_patterns:\n        assert numpy.array_equal(x, rearrange(x, pattern)), pattern\n\n    for pattern1, pattern2 in equivalent_rearrange_patterns:\n        assert numpy.array_equal(rearrange(x, pattern1), rearrange(x, pattern2))\n\n    for reduction in [\'min\', \'max\', \'sum\']:\n        for pattern1, pattern2 in equivalent_reduction_patterns:\n            assert numpy.array_equal(reduce(x, pattern1, reduction=reduction),\n                                     reduce(x, pattern2, reduction=reduction))\n\n    # now just check coincidence with numpy\n    all_rearrange_patterns = [*identity_patterns]\n    for pattern_pairs in equivalent_rearrange_patterns:\n        all_rearrange_patterns.extend(pattern_pairs)\n\n\ndef check_op_against_numpy(backend, numpy_input, pattern, axes_lengths, reduction=\'rearrange\', is_symbolic=False):\n    """"""\n    Helper to test result of operation (rearrange or transpose) against numpy\n    if reduction == \'rearrange\', rearrange op is tested, otherwise reduce\n    """"""\n\n    def operation(x):\n        if reduction == \'rearrange\':\n            return rearrange(x, pattern, **axes_lengths)\n        else:\n            return reduce(x, pattern, reduction, **axes_lengths)\n\n    numpy_result = operation(numpy_input)\n    check_equal = numpy.array_equal\n    p_none_dimension = 0.5\n    if \'mxnet\' in backend.framework_name:\n        # known mxnet bug cant work with scalars - allclose\n        check_equal = numpy.allclose\n        # mxnet can\'t work unless shape is completely specified\n        p_none_dimension = 0\n    if is_symbolic:\n        symbol_shape = [d if numpy.random.random() >= p_none_dimension else None for d in numpy_input.shape]\n        symbol = backend.create_symbol(shape=symbol_shape)\n        result_symbol = operation(symbol)\n        backend_result = backend.eval_symbol(result_symbol, [(symbol, numpy_input)])\n    else:\n        backend_result = operation(backend.from_numpy(numpy_input))\n        backend_result = backend.to_numpy(backend_result)\n\n    check_equal(numpy_result, backend_result)\n\n\ndef test_ellipsis_ops_imperative():\n    """""" Checking various patterns against numpy """"""\n    x = numpy.arange(2 * 3 * 4 * 5 * 6).reshape([2, 3, 4, 5, 6])\n    for is_symbolic in [True, False]:\n        for backend in collect_test_backends(symbolic=is_symbolic, layers=False):\n            for pattern in identity_patterns + list(itertools.chain(*equivalent_rearrange_patterns)):\n                check_op_against_numpy(backend, x, pattern, axes_lengths={}, is_symbolic=is_symbolic)\n\n            for reduction in [\'min\', \'max\', \'sum\']:\n                for pattern in itertools.chain(*equivalent_reduction_patterns):\n                    check_op_against_numpy(backend, x, pattern,\n                                           axes_lengths={}, reduction=reduction, is_symbolic=is_symbolic)\n\n\ndef test_rearrange_consistency_numpy():\n    shape = [1, 2, 3, 5, 7, 11]\n    x = numpy.arange(numpy.prod(shape)).reshape(shape)\n    for pattern in [\n        \'a b c d e f -> a b c d e f\',\n        \'b a c d e f -> a b d e f c\',\n        \'a b c d e f -> f e d c b a\',\n        \'a b c d e f -> (f e) d (c b a)\',\n        \'a b c d e f -> (f e d c b a)\',\n    ]:\n        result = rearrange(x, pattern)\n        assert len(numpy.setdiff1d(x, result)) == 0\n        assert result.dtype == x.dtype\n\n    result = rearrange(x, \'a b c d e f -> a (b) (c d e) f\')\n    assert numpy.array_equal(x.flatten(), result.flatten())\n\n    result = rearrange(x, \'a aa aa1 a1a1 aaaa a11 -> a aa aa1 a1a1 aaaa a11\')\n    assert numpy.array_equal(x, result)\n\n    result1 = rearrange(x, \'a b c d e f -> f e d c b a\')\n    result2 = rearrange(x, \'f e d c b a -> a b c d e f\')\n    assert numpy.array_equal(result1, result2)\n\n    result = rearrange(rearrange(x, \'a b c d e f -> (f d) c (e b) a\'), \'(f d) c (e b) a -> a b c d e f\', b=2, d=5)\n    assert numpy.array_equal(x, result)\n\n    sizes = dict(zip(\'abcdef\', shape))\n    temp = rearrange(x, \'a b c d e f -> (f d) c (e b) a\', **sizes)\n    result = rearrange(temp, \'(f d) c (e b) a -> a b c d e f\', **sizes)\n    assert numpy.array_equal(x, result)\n\n    x2 = numpy.arange(2 * 3 * 4).reshape([2, 3, 4])\n    result = rearrange(x2, \'a b c -> b c a\')\n    assert x2[1, 2, 3] == result[2, 3, 1]\n    assert x2[0, 1, 2] == result[1, 2, 0]\n\n\ndef test_rearrange_numpy_element_wise():\n    for n_axes in range(1, 10):\n        input = numpy.arange(2 ** n_axes).reshape([2] * n_axes)\n        permutation = numpy.random.permutation(n_axes)\n        left_expression = \' \'.join(\'i\' + str(axis) for axis in range(n_axes))\n        right_expression = \' \'.join(\'i\' + str(axis) for axis in permutation)\n        expression = left_expression + \' -> \' + right_expression\n        result = rearrange(input, expression)\n\n        for pick in numpy.random.randint(0, 2, [10, n_axes]):\n            assert input[tuple(pick)] == result[tuple(pick[permutation])]\n\n    for n_axes in range(1, 10):\n        input = numpy.arange(2 ** n_axes).reshape([2] * n_axes)\n        permutation = numpy.random.permutation(n_axes)\n        left_expression = \' \'.join(\'i\' + str(axis) for axis in range(n_axes)[::-1])\n        right_expression = \' \'.join(\'i\' + str(axis) for axis in permutation[::-1])\n        expression = left_expression + \' -> \' + right_expression\n        result = rearrange(input, expression)\n        assert result.shape == input.shape\n        expected_result = numpy.zeros_like(input)\n        for original_axis, result_axis in enumerate(permutation):\n            expected_result |= ((input >> original_axis) & 1) << result_axis\n\n        assert numpy.array_equal(result, expected_result)\n\n\ndef test_reduction_imperatives():\n    for backend in imp_op_backends:\n        print(\'Reduction tests for \', backend.framework_name)\n        for reduction in _reductions:\n            input = numpy.arange(2 * 3 * 4 * 5 * 6, dtype=\'int64\').reshape(2, 3, 4, 5, 6)\n            if reduction in [\'mean\', \'prod\']:\n                input = input / input.astype(\'float64\').mean()\n            test_cases = [\n                [\'a b c d e -> \', {}, getattr(input, reduction)()],\n                [\'... -> \', {}, getattr(input, reduction)()],\n                [\'(a1 a2) ... (e1 e2) -> \', dict(a1=1, e2=2), getattr(input, reduction)()],\n                [\'a b c d e -> (e c) a\', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1, 2)],\n                [\'a ... c d e -> (e c) a\', {},\n                 getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1, 2)],\n                [\'a b c d e ... -> (e c) a\', {},\n                 getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1, 2)],\n                [\'a b c d e -> (e c a)\', {}, getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1)],\n                [\'(a1 a2) ... -> (a2 a1) ...\', dict(a2=1), input],\n            ]\n            for pattern, axes_lengths, expected_result in test_cases:\n                result = reduce(backend.from_numpy(input.copy()), pattern, reduction=reduction, **axes_lengths)\n                result = backend.to_numpy(result)\n                assert numpy.allclose(result, expected_result)\n\n\ndef test_reduction_symbolic():\n    for backend in sym_op_backends:\n        print(\'Reduction tests for \', backend.framework_name)\n        for reduction in _reductions:\n            input = numpy.arange(2 * 3 * 4 * 5 * 6, dtype=\'int64\').reshape(2, 3, 4, 5, 6)\n            input = input / input.astype(\'float64\').mean()\n            test_cases = [\n                [\'a b c d e -> \', {},\n                 getattr(input, reduction)()],\n                [\'a ... -> \', {},\n                 getattr(input, reduction)()],\n                [\'(a a2) ... (e e2) -> \', dict(a2=1, e2=1),\n                 getattr(input, reduction)()],\n                [\'a b c d e -> (e c) a\', {},\n                 getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1, 2)],\n                [\'a ... c d e -> (e c) a\', {},\n                 getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1, 2)],\n                [\'a b c d e ... -> (e c) a\', {},\n                 getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1, 2)],\n                [\'a b c d e -> (e c a)\', {},\n                 getattr(input, reduction)(axis=(1, 3)).transpose(2, 1, 0).reshape(-1)],\n                [\'(a a2) ... -> (a2 a) ...\', dict(a2=1),\n                 input],\n            ]\n            for pattern, axes_lengths, expected_result in test_cases:\n                shapes = [input.shape]\n                if backend.framework_name != \'mxnet.symbol\':\n                    # mxnet can\'t handle non-specified shapes\n                    shapes.append([None for _ in input.shape])\n                for shape in shapes:\n                    sym = backend.create_symbol(shape)\n                    result_sym = reduce(sym, pattern, reduction=reduction, **axes_lengths)\n                    result = backend.eval_symbol(result_sym, [(sym, input)])\n                    assert numpy.allclose(result, expected_result)\n\n                if True:\n                    shape = []\n                    _axes_lengths = {**axes_lengths}\n                    for axis, length in zip(\'abcde\', input.shape):\n                        # filling as much as possible with Nones\n                        if axis in pattern:\n                            shape.append(None)\n                            _axes_lengths[axis] = length\n                        else:\n                            shape.append(length)\n                    sym = backend.create_symbol(shape)\n                    result_sym = reduce(sym, pattern, reduction=reduction, **_axes_lengths)\n                    result = backend.eval_symbol(result_sym, [(sym, input)])\n                    assert numpy.allclose(result, expected_result)\n\n\ndef test_reduction_stress_imperatives():\n    for backend in imp_op_backends:\n        print(\'Stress-testing reduction for \', backend.framework_name)\n        for reduction in _reductions + (\'rearrange\',):\n            dtype = \'int64\'\n            coincide = numpy.array_equal\n            if reduction in [\'mean\', \'prod\']:\n                dtype = \'float64\'\n                coincide = numpy.allclose\n            for n_axes in range(6 if \'mxnet\' in backend.framework_name else 11):\n                shape = numpy.random.randint(2, 4, size=n_axes)\n                permutation = numpy.random.permutation(n_axes)\n                skipped = 0 if reduction == \'rearrange\' else numpy.random.randint(n_axes + 1)\n                left = \' \'.join(\'x\' + str(i) for i in range(n_axes))\n                right = \' \'.join(\'x\' + str(i) for i in permutation[skipped:])\n                pattern = left + \'->\' + right\n                x = numpy.arange(1, 1 + numpy.prod(shape), dtype=dtype).reshape(shape)\n                if reduction == \'prod\':\n                    x /= x.mean()\n                result1 = reduce(x, pattern, reduction=reduction)\n                result2 = x.transpose(permutation)\n                if skipped > 0:\n                    result2 = getattr(result2, reduction)(axis=tuple(range(skipped)))\n                assert coincide(result1, result2)\n                if n_axes == 0 and \'mxnet\' in backend.framework_name:\n                    # known mxnet bug, cant attach gradients to scalar\n                    continue\n                check_op_against_numpy(backend, x, pattern, reduction=reduction, axes_lengths={}, is_symbolic=False)\n\n\ndef test_rearrange_examples():\n    def test1(x):\n        # transpose\n        y = rearrange(x, \'b c h w -> b h w c\')\n        assert y.shape == (10, 30, 40, 20)\n        return y\n\n    def test2(x):\n        # view / reshape\n        y = rearrange(x, \'b c h w -> b (c h w)\')\n        assert y.shape == (10, 20 * 30 * 40)\n        return y\n\n    def test3(x):\n        # depth-to-space\n        y = rearrange(x, \'b (c h1 w1) h w -> b c (h h1) (w w1)\', h1=2, w1=2)\n        assert y.shape == (10, 5, 30 * 2, 40 * 2)\n        return y\n\n    def test4(x):\n        # space-to-depth\n        y = rearrange(x, \'b c (h h1) (w w1) -> b (h1 w1 c) h w\', h1=2, w1=2)\n        assert y.shape == (10, 20 * 4, 30 // 2, 40 // 2)\n        return y\n\n    def test5(x):\n        # simple transposition\n        y = rearrange(x, \'b1 sound b2 letter -> b1 b2 sound letter\')\n        assert y.shape == (10, 30, 20, 40)\n        return y\n\n    def test6(x):\n        # parsing parameters\n        t = rearrange(x, \'b c h w -> (b h w) c\')\n        t = t[:, ::2]  # replacement for dot-product, just changes size of second axis\n        assert t.shape == (10 * 30 * 40, 10)\n\n        y = rearrange(t, \'(b h w) c2 -> b c2 h w\', **parse_shape(x, \'b _ h w\'))\n        assert y.shape == (10, 10, 30, 40)\n        return y\n\n    def test7(x):\n        # split of embedding into groups\n        y1, y2 = rearrange(x, \'b (c g) h w -> g b c h w\', g=2)\n        assert y1.shape == (10, 10, 30, 40)\n        assert y2.shape == (10, 10, 30, 40)\n        return y1 + y2  # only one tensor is expected in output\n\n    def test8(x):\n        # max-pooling\n        y = reduce(x, \'b c (h h1) (w w1) -> b c h w\', reduction=\'max\', h1=2, w1=2)\n        assert y.shape == (10, 20, 30 // 2, 40 // 2)\n        return y\n\n    def test9(x):\n        # squeeze - unsqueeze\n        y = reduce(x, \'b c h w -> b c () ()\', reduction=\'max\')\n        assert y.shape == (10, 20, 1, 1)\n        y = rearrange(y, \'b c () () -> c b\')\n        assert y.shape == (20, 10)\n        return y\n\n    def test10(x):\n        # stack\n        tensors = list(x + 0)  # 0 is needed https://github.com/tensorflow/tensorflow/issues/23185\n        tensors = rearrange(tensors, \'b c h w -> b h w c\')\n        assert tensors.shape == (10, 30, 40, 20)\n        return tensors\n\n    def test11(x):\n        # concatenate\n        tensors = list(x + 0)  # 0 is needed https://github.com/tensorflow/tensorflow/issues/23185\n        tensors = rearrange(tensors, \'b c h w -> h (b w) c\')\n        assert tensors.shape == (30, 10 * 40, 20)\n        return tensors\n\n    def shufflenet(x, convolve, c1, c2):\n        # shufflenet reordering example\n        x = convolve(x)\n        x = rearrange(x, \'b (c1 c2) h w-> b (c2 c1) h w\', c1=c1, c2=c2)\n        x = convolve(x)\n        return x\n\n    def convolve_strided_1d(x, stride, usual_convolution):\n        x = rearrange(x, \'b c t1 t2 -> b c (t1 t2)\')  # reduce dimensionality\n        x = rearrange(x, \'b c (t stride) -> (stride b) c t\', stride=stride)\n        x = usual_convolution(x)\n        x = rearrange(x, \'(stride b) c t -> b c (t stride)\', stride=stride)\n        return x\n\n    def convolve_strided_2d(x, h_stride, w_stride, usual_convolution):\n        x = rearrange(x, \'b c (h hs) (w ws) -> (hs ws b) c h w\', hs=h_stride, ws=w_stride)\n        x = usual_convolution(x)\n        x = rearrange(x, \'(hs ws b) c h w -> b c (h hs) (w ws)\', hs=h_stride, ws=w_stride)\n        return x\n\n    def unet_like_1d(x, usual_convolution):\n        # u-net like steps for increasing / reducing dimensionality\n        x = rearrange(x, \'b c t1 t2 -> b c (t1 t2)\')  # reduce dimensionality\n        y = rearrange(x, \'b c (t dt) -> b (dt c) t\', dt=2)\n        y = usual_convolution(y)\n        x = x + rearrange(y, \'b (dt c) t -> b c (t dt)\', dt=2)\n        return x\n\n    # mock for convolution (works for all backends)\n    convolve_mock = lambda x: x\n\n    tests = [test1, test2, test3, test4, test5, test6, test7, test8, test9, test10, test11,\n             lambda x: shufflenet(x, convolve=convolve_mock, c1=4, c2=5),\n             lambda x: convolve_strided_1d(x, stride=2, usual_convolution=convolve_mock),\n             lambda x: convolve_strided_2d(x, h_stride=2, w_stride=2, usual_convolution=convolve_mock),\n             lambda x: unet_like_1d(x, usual_convolution=convolve_mock),\n             ]\n\n    for backend in imp_op_backends:\n        print(\'testing source_examples for \', backend.framework_name)\n        for test in tests:\n            x = numpy.arange(10 * 20 * 30 * 40).reshape([10, 20, 30, 40])\n            result1 = test(x)\n            result2 = backend.to_numpy(test(backend.from_numpy(x)))\n            assert numpy.array_equal(result1, result2)\n\n            # now with strides\n            x = numpy.arange(10 * 2 * 20 * 3 * 30 * 1 * 40).reshape([10 * 2, 20 * 3, 30 * 1, 40 * 1])\n            # known torch bug - torch doesn\'t support negative steps\n            last_step = -1 if backend.framework_name != \'torch\' else 1\n            indexing_expression = numpy.index_exp[::2, ::3, ::1, ::last_step]\n            result1 = test(x[indexing_expression])\n            result2 = backend.to_numpy(test(backend.from_numpy(x)[indexing_expression]))\n            assert numpy.array_equal(result1, result2)\n\n\ndef test_enumerating_directions():\n    for backend in imp_op_backends:\n        print(\'testing directions for\', backend.framework_name)\n        for shape in [[], [1], [1, 1, 1], [2, 3, 5, 7]]:\n            if backend.framework_name == \'mxnet.ndarray\' and len(shape) == 0:\n                # known bug of mxnet\n                continue\n            x = numpy.arange(numpy.prod(shape)).reshape(shape)\n            axes1 = _enumerate_directions(x)\n            axes2 = _enumerate_directions(backend.from_numpy(x))\n            for axe1, axe2 in zip(axes1, axes2):\n                axe2 = backend.to_numpy(axe2)\n                assert axe1.shape == axe2.shape\n                assert numpy.allclose(axe1, axe2)\n\n\ndef test_concatenations_and_stacking():\n    for backend in imp_op_backends:\n        print(\'testing shapes for \', backend.framework_name)\n        for n_arrays in [1, 2, 5]:\n            shapes = [[], [1], [1, 1], [2, 3, 5, 7], [1] * 6]\n            for shape in shapes:\n                if backend.framework_name == \'mxnet.ndarray\' and len(shape) == 0:\n                    # known bug of mxnet\n                    continue\n                arrays1 = [numpy.arange(i, i + numpy.prod(shape)).reshape(shape) for i in range(n_arrays)]\n                arrays2 = [backend.from_numpy(array) for array in arrays1]\n                result0 = numpy.asarray(arrays1)\n                result1 = rearrange(arrays1, \'...->...\')\n                result2 = rearrange(arrays2, \'...->...\')\n                assert numpy.array_equal(result0, result1)\n                assert numpy.array_equal(result1, backend.to_numpy(result2))\n\n                result1 = rearrange(arrays1, \'b ... -> ... b\')\n                result2 = rearrange(arrays2, \'b ... -> ... b\')\n                assert numpy.array_equal(result1, backend.to_numpy(result2))\n\n\ndef test_gradients_imperatives():\n    # lazy - just checking reductions\n    for reduction in _reductions:\n        x = numpy.arange(1, 1 + 2 * 3 * 4).reshape(2, 3, 4).astype(\'float32\')\n        results = {}\n        for backend in imp_op_backends:\n            y0 = backend.from_numpy(x)\n            if not hasattr(y0, \'grad\'):\n                continue\n            if \'mxnet\' in backend.framework_name:\n                backend.mx.autograd.set_recording(True)\n            y1 = reduce(y0, \'a b c -> c a\', reduction=reduction)\n            y2 = reduce(y1, \'c a -> a c\', reduction=reduction)\n            y3 = reduce(y2, \'a (c1 c2) -> a\', reduction=reduction, c1=2)\n            y4 = reduce(y3, \'... -> \', reduction=reduction)\n            if \'mxnet\' in backend.framework_name:\n                backend.mx.autograd.set_recording(False)\n            y4.backward()\n            grad = backend.to_numpy(y0.grad)\n            results[backend.framework_name] = grad\n\n        print(\'comparing gradients for\', results.keys())\n        for name1, grad1 in results.items():\n            for name2, grad2 in results.items():\n                assert numpy.allclose(grad1, grad2), [name1, name2, \'provided different gradients\']\n\n\ndef tensor_train_example_numpy():\n    # kept here just for a collection, only tested for numpy\n    # https://arxiv.org/pdf/1509.06569.pdf, (5)\n    x = numpy.ones([3, 4, 5, 6])\n    rank = 4\n    if numpy.__version__ < \'1.15.0\':\n        # numpy.einsum fails here, skip test\n        return\n    # creating appropriate Gs\n    Gs = [numpy.ones([d, d, rank, rank]) for d in x.shape]\n    Gs[0] = Gs[0][:, :, :1, :]\n    Gs[-1] = Gs[-1][:, :, :, :1]\n\n    # einsum way\n    y = x.reshape((1,) + x.shape)\n    for G in Gs:\n        # taking partial results left-to-right\n        # y = numpy.einsum(\'i j alpha beta, alpha i ...  -> beta ... j\', G, y)\n        y = numpy.einsum(\'i j a b, a i ...  -> b ... j\', G, y)\n    y1 = y.reshape(-1)\n\n    # alternative way\n    y = x.reshape(-1)\n    for G in Gs:\n        i, j, alpha, beta = G.shape\n        y = rearrange(y, \'(i rest alpha) -> rest (alpha i)\', alpha=alpha, i=i)\n        y = y @ rearrange(G, \'i j alpha beta -> (alpha i) (j beta)\')\n        y = rearrange(y, \'rest (beta j) -> (beta rest j)\', beta=beta, j=j)\n    y2 = y\n    assert numpy.allclose(y1, y2)\n\n    # yet another way\n    y = x\n    for G in Gs:\n        i, j, alpha, beta = G.shape\n        y = rearrange(y, \'i ... (j alpha) -> ... j (alpha i)\', alpha=alpha, i=i)\n        y = y @ rearrange(G, \'i j alpha beta -> (alpha i) (j beta)\')\n    y3 = y.reshape(-1)\n    assert numpy.allclose(y1, y3)\n\n\ndef test_pytorch_yolo_fragment():\n    if not any(b.framework_name == \'torch\' for b in collect_test_backends(symbolic=False, layers=False)):\n        return\n    import torch\n\n    def old_way(input, num_classes, num_anchors, anchors, stride_h, stride_w):\n        # https://github.com/BobLiu20/YOLOv3_PyTorch/blob/c6b483743598b5f64d520d81e7e5f47ba936d4c9/nets/yolo_loss.py#L28-L44\n        bs = input.size(0)\n        in_h = input.size(2)\n        in_w = input.size(3)\n        scaled_anchors = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in anchors]\n\n        prediction = input.view(bs, num_anchors,\n                                5 + num_classes, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous()\n        # Get outputs\n        x = torch.sigmoid(prediction[..., 0])  # Center x\n        y = torch.sigmoid(prediction[..., 1])  # Center y\n        w = prediction[..., 2]  # Width\n        h = prediction[..., 3]  # Height\n        conf = torch.sigmoid(prediction[..., 4])  # Conf\n        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n\n        # https://github.com/BobLiu20/YOLOv3_PyTorch/blob/c6b483743598b5f64d520d81e7e5f47ba936d4c9/nets/yolo_loss.py#L70-L92\n        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n        # Calculate offsets for each grid\n        grid_x = torch.linspace(0, in_w - 1, in_w).repeat(in_w, 1).repeat(\n            bs * num_anchors, 1, 1).view(x.shape).type(FloatTensor)\n        grid_y = torch.linspace(0, in_h - 1, in_h).repeat(in_h, 1).t().repeat(\n            bs * num_anchors, 1, 1).view(y.shape).type(FloatTensor)\n        # Calculate anchor w, h\n        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))\n        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))\n        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(w.shape)\n        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, in_h * in_w).view(h.shape)\n        # Add offset and scale with anchors\n        pred_boxes = FloatTensor(prediction[..., :4].shape)\n        pred_boxes[..., 0] = x.data + grid_x\n        pred_boxes[..., 1] = y.data + grid_y\n        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n        # Results\n        _scale = torch.Tensor([stride_w, stride_h] * 2).type(FloatTensor)\n        output = torch.cat((pred_boxes.view(bs, -1, 4) * _scale,\n                            conf.view(bs, -1, 1), pred_cls.view(bs, -1, num_classes)), -1)\n        return output\n\n    def new_way(input, num_classes, num_anchors, anchors, stride_h, stride_w):\n        raw_predictions = rearrange(input, \' b (anchor prediction) h w -> prediction b anchor h w\', anchor=num_anchors)\n\n        anchors = torch.FloatTensor(anchors).to(input.device)\n        anchor_sizes = rearrange(anchors, \'anchor dim -> dim () anchor () ()\')\n\n        _, _, _, in_h, in_w = raw_predictions.shape\n        grid_h = rearrange(torch.arange(in_h).float(), \'h -> () () h ()\').to(input.device)\n        grid_w = rearrange(torch.arange(in_w).float(), \'w -> () () () w\').to(input.device)\n\n        predicted_bboxes = torch.zeros_like(raw_predictions)\n        predicted_bboxes[0] = (raw_predictions[0].sigmoid() + grid_h) * stride_h  # center y\n        predicted_bboxes[1] = (raw_predictions[1].sigmoid() + grid_w) * stride_w  # center x\n        predicted_bboxes[2:4] = (raw_predictions[2:4].exp()) * anchor_sizes  # bbox width and height\n        predicted_bboxes[4] = raw_predictions[4].sigmoid()  # confidence\n        predicted_bboxes[5:] = raw_predictions[5:].sigmoid()  # class predictions\n        # only to match results of original code, not needed\n        return rearrange(predicted_bboxes, \'prediction b anchor h w -> b anchor h w prediction\')\n\n    stride_h = 4\n    stride_w = 4\n    batch_size = 5\n    num_classes = 12\n    anchors = [[50, 100], [100, 50], [75, 75]]\n    num_anchors = len(anchors)\n\n    input = torch.randn([batch_size, num_anchors * (5 + num_classes), 1, 1])\n    result1 = old_way(input=input, num_anchors=num_anchors, num_classes=num_classes,\n                      stride_h=stride_h, stride_w=stride_w, anchors=anchors)\n    result2 = new_way(input=input, num_anchors=num_anchors, num_classes=num_classes,\n                      stride_h=stride_h, stride_w=stride_w, anchors=anchors)\n    result1 = result1.reshape(result2.shape)\n    assert torch.allclose(result1, result2)\n\n\ndef test_tiling_imperatives():\n    for backend in imp_op_backends:\n        print(\'Tiling tests for \', backend.framework_name)\n        input = numpy.arange(2 * 3 * 5, dtype=\'int64\').reshape([2, 1, 3, 1, 5])\n        test_cases = [\n            (1, 1, 1, 1, 1),\n            (1, 2, 1, 3, 1),\n            (3, 1, 1, 4, 1),\n        ]\n        for repeats in test_cases:\n            expected = numpy.tile(input, repeats)\n            converted = backend.from_numpy(input)\n            repeated = backend.tile(converted, repeats)\n            result = backend.to_numpy(repeated)\n            assert numpy.array_equal(result, expected)\n\n\ndef test_tiling_symbolic():\n    for backend in sym_op_backends:\n        print(\'Tiling tests for \', backend.framework_name)\n        input = numpy.arange(2 * 3 * 5, dtype=\'int64\').reshape([2, 1, 3, 1, 5])\n        test_cases = [\n            (1, 1, 1, 1, 1),\n            (1, 2, 1, 3, 1),\n            (3, 1, 1, 4, 1),\n        ]\n        for repeats in test_cases:\n            expected = numpy.tile(input, repeats)\n            sym = backend.create_symbol(input.shape)\n            result = backend.eval_symbol(backend.tile(sym, repeats), [[sym, input]])\n            assert numpy.array_equal(result, expected)\n\n            sym = backend.create_symbol([None] * len(input.shape))\n            result = backend.eval_symbol(backend.tile(sym, repeats), [[sym, input]])\n            assert numpy.array_equal(result, expected)\n\n\nrepeat_test_cases = [\n    # all assume that input has shape [2, 3, 5]\n    (\'a b c -> c a b\', dict()),\n    (\'a b c -> (c copy a b)\', dict(copy=2, a=2, b=3, c=5)),\n    (\'a b c -> (a copy) b c \', dict(copy=1)),\n    (\'a b c -> (c a) (copy1 b copy2)\', dict(a=2, copy1=1, copy2=2)),\n    (\'a ...  -> a ... copy\', dict(copy=4)),\n    (\'... c -> ... (copy1 c copy2)\', dict(copy1=1, copy2=2)),\n    (\'...  -> ... \', dict()),\n    (\' ...  -> copy1 ... copy2 \', dict(copy1=2, copy2=3)),\n    (\'a b c  -> copy1 a copy2 b c () \', dict(copy1=2, copy2=1)),\n]\n\n\ndef test_repeat_numpy():\n    x = numpy.arange(2 * 3 * 5).reshape(2, 3, 5)\n    x1 = reduce(x, \'a b c -> copy a b c \', reduction=\'repeat\', copy=1)\n    assert numpy.array_equal(x[None], x1)\n\n    def check_reversion(x, repeat_pattern, **sizes):\n        left, right = repeat_pattern.split(\'->\')\n        reduce_pattern = right + \'->\' + left\n        repeated = reduce(x, repeat_pattern, reduction=\'repeat\', **sizes)\n        reduced_min = reduce(repeated, reduce_pattern, reduction=\'min\', **sizes)\n        reduced_max = reduce(repeated, reduce_pattern, reduction=\'max\', **sizes)\n        assert numpy.array_equal(x, reduced_min)\n        assert numpy.array_equal(x, reduced_max)\n\n    for pattern, axis_dimensions in repeat_test_cases:\n        check_reversion(x, pattern, **axis_dimensions)\n\n\ndef test_repeat_imperatives():\n    x = numpy.arange(2 * 3 * 5).reshape(2, 3, 5)\n    for backend in imp_op_backends:\n        print(\'Repeat tests for \', backend.framework_name)\n\n        for pattern, axis_dimensions in repeat_test_cases:\n            expected = reduce(x, pattern, reduction=\'repeat\', **axis_dimensions)\n            converted = backend.from_numpy(x)\n            repeated = reduce(converted, pattern, reduction=\'repeat\', **axis_dimensions)\n            result = backend.to_numpy(repeated)\n            assert numpy.array_equal(result, expected)\n\n\ndef test_repeat_symbolic():\n    x = numpy.arange(2 * 3 * 5).reshape(2, 3, 5)\n\n    for backend in sym_op_backends:\n        print(\'Repeat tests for \', backend.framework_name)\n\n        for pattern, axis_dimensions in repeat_test_cases:\n            expected = reduce(x, pattern, reduction=\'repeat\', **axis_dimensions)\n\n            sym = backend.create_symbol(x.shape)\n            result = backend.eval_symbol(reduce(sym, pattern, reduction=\'repeat\', **axis_dimensions), [[sym, x]])\n            assert numpy.array_equal(result, expected)\n'"
tests/test_other.py,0,"b'import sys\nimport numpy\n\nimport einops\nimport einops.layers\n\nfrom doctest import testmod\nfrom einops.einops import (rearrange, parse_shape, _optimize_transformation, _check_elementary_axis_name)\nfrom einops._backends import AbstractBackend\nfrom . import collect_test_backends\n\n__author__ = \'Alex Rogozhnikov\'\n\n\ndef test_doctests_examples():\n    if sys.version_info >= (3, 6):\n        # python 3.5 and lower do not keep ordered dictionaries\n        testmod(einops.layers, raise_on_error=True, extraglobs=dict(np=numpy))\n        testmod(einops.einops, raise_on_error=True, extraglobs=dict(np=numpy))\n\n\ndef test_backends_installed():\n    """"""\n    This test will fail if some of backends are not installed or can\'t be imported\n    Other tests will just work and only test installed backends.\n    """"""\n    from . import skip_cupy\n    errors = []\n    for backend_type in AbstractBackend.__subclasses__():\n        if skip_cupy and backend_type.framework_name == \'cupy\':\n            continue\n        try:\n            # instantiate\n            backend_type()\n        except Exception as e:\n            errors.append(e)\n    assert len(errors) == 0, errors\n\n\ndef test_optimize_transformations_numpy():\n    print(\'Testing optimizations\')\n    shapes = [[2] * n_dimensions for n_dimensions in range(14)]\n    shapes += [[3] * n_dimensions for n_dimensions in range(6)]\n    shapes += [[2, 3, 5, 7]]\n    shapes += [[2, 3, 5, 7, 11, 17]]\n\n    for shape in shapes:\n        for attempt in range(5):\n            n_dimensions = len(shape)\n            x = numpy.random.randint(0, 2 ** 12, size=shape).reshape([-1])\n            init_shape = shape[:]\n            n_reduced = numpy.random.randint(0, n_dimensions + 1)\n            reduced_axes = tuple(numpy.random.permutation(n_dimensions)[:n_reduced])\n            axes_reordering = numpy.random.permutation(n_dimensions - n_reduced)\n            final_shape = numpy.random.randint(0, 1024, size=333)  # just random\n\n            init_shape2, reduced_axes2, axes_reordering2, final_shape2 = combination2 = \\\n                _optimize_transformation(init_shape, reduced_axes, axes_reordering, final_shape)\n\n            assert numpy.array_equal(final_shape, final_shape2)\n            result1 = x.reshape(init_shape).sum(axis=reduced_axes).transpose(axes_reordering).reshape([-1])\n            result2 = x.reshape(init_shape2).sum(axis=reduced_axes2).transpose(axes_reordering2).reshape([-1])\n            assert numpy.array_equal(result1, result2)\n\n            # testing we can\'t optimize this formula again\n            combination3 = _optimize_transformation(*combination2)\n            for a, b in zip(combination2, combination3):\n                assert numpy.array_equal(a, b)\n\n\ndef test_elementary_axis_name():\n    for name in [\'a\', \'b\', \'h\', \'dx\', \'h1\', \'zz\', \'i9123\', \'somelongname\']:\n        assert _check_elementary_axis_name(name)\n    for name in [\'\', \'2b\', \'Alex\', \'camelCase\', \'under_score\', \'12\']:\n        assert not _check_elementary_axis_name(name)\n\n\ndef test_parse_shape_imperative():\n    backends = collect_test_backends(symbolic=False, layers=False)\n    backends += collect_test_backends(symbolic=False, layers=True)\n    for backend in backends:\n        print(\'Shape parsing for \', backend.framework_name)\n        x = numpy.zeros([10, 20, 30, 40])\n        parsed1 = parse_shape(x, \'a b c d\')\n        parsed2 = parse_shape(backend.from_numpy(x), \'a b c d\')\n        assert parsed1 == parsed2 == dict(a=10, b=20, c=30, d=40)\n        assert parsed1 != dict(a=1, b=20, c=30, d=40) != parsed2\n\n        parsed1 = parse_shape(x, \'_ _ _ _\')\n        parsed2 = parse_shape(backend.from_numpy(x), \'_ _ _ _\')\n        assert parsed1 == parsed2 == dict()\n\n        parsed1 = parse_shape(x, \'_ _ _ hello\')\n        parsed2 = parse_shape(backend.from_numpy(x), \'_ _ _ hello\')\n        assert parsed1 == parsed2 == dict(hello=40)\n\n        parsed1 = parse_shape(x, \'_ _ a1 a1a111a\')\n        parsed2 = parse_shape(backend.from_numpy(x), \'_ _ a1 a1a111a\')\n        assert parsed1 == parsed2 == dict(a1=30, a1a111a=40)\n\n\ndef test_parse_shape_symbolic():\n    backends = collect_test_backends(symbolic=True, layers=False)\n    backends += collect_test_backends(symbolic=True, layers=True)\n    for backend in backends:\n        if backend.framework_name == \'keras\':\n            # need special way to compile, shape vars can be used only inside layers\n            continue\n        print(\'special shape parsing for\', backend.framework_name)\n        input_symbols = [\n            backend.create_symbol([10, 20, 30, 40]),\n            backend.create_symbol([10, 20, None, None]),\n            backend.create_symbol([None, None, None, None]),\n        ]\n        if backend.framework_name in [\'mxnet.symbol\']:\n            # mxnet can\'t normally run inference\n            input_symbols = [backend.create_symbol([10, 20, 30, 40])]\n\n        for input_symbol in input_symbols:\n            shape_placeholder = parse_shape(input_symbol, \'a b c d\')\n            shape = {}\n            for name, symbol in shape_placeholder.items():\n                shape[name] = symbol if isinstance(symbol, int) \\\n                    else backend.eval_symbol(symbol, [(input_symbol, numpy.zeros([10, 20, 30, 40]))])\n            print(shape)\n            result_placeholder = rearrange(input_symbol, \'a b (c1 c2) (d1 d2) -> (a b d1) c1 (c2 d2)\',\n                                           **parse_shape(input_symbol, \'a b c1 _\'), d2=2)\n            result = backend.eval_symbol(result_placeholder, [(input_symbol, numpy.zeros([10, 20, 30, 40]))])\n            print(result.shape)\n            assert result.shape == (10 * 20 * 20, 30, 1 * 2)\n            assert numpy.allclose(result, 0)\n\n\ndef test_is_float_type():\n    backends = collect_test_backends(symbolic=False, layers=False)\n    backends += collect_test_backends(symbolic=False, layers=True)\n    for backend in backends:\n        for dtype in [\'int32\', \'int64\', \'float32\', \'float64\']:\n            is_float = \'float\' in dtype\n            input = numpy.zeros([3, 4, 5], dtype=dtype)\n            input = backend.from_numpy(input)\n            if \'chainer\' in backend.framework_name and not is_float:\n                continue  # chainer doesn\'t allow non-floating tensors\n            assert backend.is_float_type(input) == is_float, (dtype, backend, input.dtype)\n'"
docs/source_examples/converter.py,0,"b'""""""\njust run this script with python converter.py .\nIt will convert pytorch.ipynb to html page docs/pytorch-examples.html\n\n""""""\nimport nbformat\nimport markdown\n\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import HtmlFormatter\n\nnotebook = nbformat.read(\'Pytorch.ipynb\', as_version=nbformat.NO_CONVERT)\n\ncontent = \'\'\ncache = \'\'\n\nfor cell in notebook[\'cells\']:\n    if cell[\'cell_type\'] == \'code\':\n        source = cell[\'source\']\n        if source.startswith(\'#left\') or source.startswith(\'#right\'):\n            trimmed_source = source[source.index(\'\\n\') + 1:]\n            cache += ""<div>{}</div>"".format(highlight(trimmed_source, PythonLexer(), HtmlFormatter()))\n        if source.startswith(\'#right\'):\n            content += ""<div class=\'leftright-wrapper\'><div class=\'leftright-cells\'>{}</div></div> "".format(cache)\n            cache = \'\'\n\n    elif cell[\'cell_type\'] == \'markdown\':\n        content += ""<div class=\'markdown-cell\'>{}</div>"".format(markdown.markdown(cell[\'source\']))\n    else:\n        raise RuntimeError(\'not expected type of cell\' + cell[\'cell_type\'])\n\nstyles = HtmlFormatter().get_style_defs(\'.highlight\')\n\nstyles += \'\'\'\n    body {\n        padding: 50px 10px;\n    }\n    .leftright-wrapper {\n        text-align: center;\n        overflow-x: auto;\n    }\n    .leftright-cells {\n        display: inline-flex;\n        text-align: left;\n    }\n    .leftright-cells > div {\n        padding: 0px 10px;\n        min-width: 350px;\n    }\n    .markdown-cell{\n        max-width: 700px;\n        margin: 0px auto;\n    }\n    h1 {\n        text-align: center;\n        padding: 10px 0px 0px;\n    }\n\'\'\'\n\nmeta_tags = \'\'\'\n<meta property=""og:title"" content=""Writing better code with pytorch and einops"">\n<meta property=""og:description"" content=""Learning by example: rewriting and fixing popular code fragments"">\n<meta property=""og:image"" content=""http://arogozhnikov.github.io/images/einops/einops_video.gif"">\n<meta property=""og:video"" content=""http://arogozhnikov.github.io/images/einops/einops_video.mp4"" />\n<meta property=""og:url"" content=""https://arogozhnikov.github.io/einops/pytorch-examples.html"">\n<meta name=""twitter:card"" content=""summary_large_image"">\n\n<!--  Non-Essential, But Recommended -->\n\n<meta property=""og:site_name"" content=""Writing better code with pytorch and einops"">\n<meta name=""twitter:image:alt"" content=""Learning by example: rewriting and fixing popular code fragments"">\n\'\'\'\n\ngithub_ribbon = \'\'\'\n<a href=""https://github.com/arogozhnikov/einops"" class=""github-corner"" aria-label=""View source on GitHub"">\n<svg width=""80"" height=""80"" viewBox=""0 0 250 250"" style=""fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;"" aria-hidden=""true"">\n    <path d=""M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z""></path><path d=""M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"" fill=""currentColor"" style=""transform-origin: 130px 106px;"" class=""octo-arm""></path>\n    <path d=""M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"" fill=""currentColor"" class=""octo-body""></path>\n</svg></a>\n<style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>\n\'\'\'\n\nresult = f\'\'\'\n<!DOCTYPE html>\n<html lang=""en"">\n  <head>\n    <meta charset=""utf-8"">\n    {meta_tags}\n    <title>Writing better code with pytorch+einops</title>\n    <style>{styles}</style>\n  </head>\n  <body>\n    {github_ribbon}\n    {content}\n  </body>\n\n</html>\n\'\'\'\n\nwith open(\'../pytorch-examples.html\', \'w\') as f:\n    f.write(result)\n'"
einops/layers/__init__.py,0,"b'__author__ = \'Alex Rogozhnikov\'\n\nimport functools\n\nfrom ..einops import TransformRecipe, _prepare_transformation_recipe, EinopsError\n\n\nclass RearrangeMixin:\n    """"""\n    Rearrange layer behaves identically to einops.rearrange operation.\n\n    :param pattern: str, rearrangement pattern\n    :param axes_lengths: any additional specification of dimensions\n\n    See einops.rearrange for source_examples.\n    """"""\n\n    def __init__(self, pattern, **axes_lengths):\n        super().__init__()\n        self.pattern = pattern\n        self.axes_lengths = axes_lengths\n        self.recipe()  # checking parameters\n\n    def __repr__(self):\n        params = repr(self.pattern)\n        for axis, length in self.axes_lengths.items():\n            params += \', {}={}\'.format(axis, length)\n        return \'{}({})\'.format(self.__class__.__name__, params)\n\n    @functools.lru_cache(maxsize=1024)\n    def recipe(self) -> TransformRecipe:\n        try:\n            hashable_lengths = tuple(sorted(self.axes_lengths.items()))\n            return _prepare_transformation_recipe(self.pattern, operation=\'rearrange\', axes_lengths=hashable_lengths)\n        except EinopsError as e:\n            raise EinopsError(\' Error while preparing {!r}\\n {}\'.format(self, e))\n\n    def _apply_recipe(self, x):\n        try:\n            return self.recipe().apply(x)\n        except EinopsError as e:\n            raise EinopsError(\' Error while computing {!r}\\n {}\'.format(self, e))\n\n\nclass ReduceMixin:\n    """"""\n    Reduce layer behaves identically to einops.reduce operation.\n\n    :param pattern: str, rearrangement pattern\n    :param reduction: one of available reductions (\'min\', \'max\', \'sum\', \'mean\', \'prod\'), case-sensitive\n    :param axes_lengths: any additional specification of dimensions\n\n    See einops.reduce for source_examples.\n    """"""\n\n    def __init__(self, pattern, reduction, **axes_lengths):\n        super().__init__()\n        self.pattern = pattern\n        self.reduction = reduction\n        self.axes_lengths = axes_lengths\n        self.recipe()  # checking parameters\n\n    def __repr__(self):\n        params = \'{!r}, {!r}\'.format(self.pattern, self.reduction)\n        for axis, length in self.axes_lengths.items():\n            params += \', {}={}\'.format(axis, length)\n        return \'{}({})\'.format(self.__class__.__name__, params)\n\n    @functools.lru_cache(maxsize=1024)\n    def recipe(self) -> TransformRecipe:\n        try:\n            hashable_lengths = tuple(sorted(self.axes_lengths.items()))\n            return _prepare_transformation_recipe(self.pattern, operation=self.reduction, axes_lengths=hashable_lengths)\n        except EinopsError as e:\n            raise EinopsError(\' Error while preparing {!r}\\n {}\'.format(self, e))\n\n    def _apply_recipe(self, x):\n        try:\n            return self.recipe().apply(x)\n        except EinopsError as e:\n            raise EinopsError(\' Error while computing {!r}\\n {}\'.format(self, e))\n'"
einops/layers/chainer.py,0,"b""import chainer\n\nfrom . import RearrangeMixin, ReduceMixin\n\n__author__ = 'Alex Rogozhnikov'\n\n\nclass Rearrange(RearrangeMixin, chainer.Link):\n    def __call__(self, x):\n        return self._apply_recipe(x)\n\n\nclass Reduce(ReduceMixin, chainer.Link):\n    def __call__(self, x):\n        return self._apply_recipe(x)"""
einops/layers/gluon.py,0,"b""import mxnet\n\nfrom . import RearrangeMixin, ReduceMixin\n\n__author__ = 'Alex Rogozhnikov'\n\n\nclass Rearrange(RearrangeMixin, mxnet.gluon.HybridBlock):\n    def hybrid_forward(self, F, x):\n        return self._apply_recipe(x)\n\n\nclass Reduce(ReduceMixin, mxnet.gluon.HybridBlock):\n    def hybrid_forward(self, F, x):\n        return self._apply_recipe(x)\n"""
einops/layers/keras.py,0,"b""from keras.engine import Layer\n\nfrom .._backends import UnknownSize\nfrom . import RearrangeMixin, ReduceMixin\n\n__author__ = 'Alex Rogozhnikov'\n\n\nclass Rearrange(RearrangeMixin, Layer):\n    def compute_output_shape(self, input_shape):\n        input_shape = tuple(UnknownSize() if d is None else int(d) for d in input_shape)\n        init_shapes, reduced_axes, axes_reordering, final_shape = self.recipe().reconstruct_from_shape(input_shape)\n        final_shape = tuple(None if isinstance(d, UnknownSize) else int(d) for d in final_shape)\n        return final_shape\n\n    def call(self, inputs):\n        return self._apply_recipe(inputs)\n\n    def get_config(self):\n        return {'pattern': self.pattern, **self.axes_lengths}\n\n\nclass Reduce(ReduceMixin, Layer):\n    def compute_output_shape(self, input_shape):\n        input_shape = tuple(UnknownSize() if d is None else int(d) for d in input_shape)\n        init_shapes, reduced_axes, axes_reordering, final_shape = self.recipe().reconstruct_from_shape(input_shape)\n        final_shape = tuple(None if isinstance(d, UnknownSize) else int(d) for d in final_shape)\n        return final_shape\n\n    def call(self, inputs):\n        return self._apply_recipe(inputs)\n\n    def get_config(self):\n        return {'pattern': self.pattern, 'reduction': self.reduction, **self.axes_lengths}\n\n\nkeras_custom_objects = {Rearrange.__name__: Rearrange, Reduce.__name__: Reduce}"""
einops/layers/tensorflow.py,0,"b""from tensorflow.keras.layers import Layer\n\nfrom .._backends import UnknownSize\nfrom . import RearrangeMixin, ReduceMixin\n\n__author__ = 'Alex Rogozhnikov'\n\n\nclass Rearrange(RearrangeMixin, Layer):\n    def compute_output_shape(self, input_shape):\n        input_shape = tuple(UnknownSize() if d.value is None else int(d) for d in input_shape)\n        init_shapes, reduced_axes, axes_reordering, final_shape = self.recipe().reconstruct_from_shape(input_shape)\n        final_shape = tuple(None if isinstance(d, UnknownSize) else int(d) for d in final_shape)\n        return final_shape\n\n    def call(self, inputs):\n        return self._apply_recipe(inputs)\n\n    def get_config(self):\n        return {'pattern': self.pattern, **self.axes_lengths}\n\n\nclass Reduce(ReduceMixin, Layer):\n    def compute_output_shape(self, input_shape):\n        input_shape = tuple(UnknownSize() if d.value is None else int(d) for d in input_shape)\n        init_shapes, reduced_axes, axes_reordering, final_shape = self.recipe().reconstruct_from_shape(input_shape)\n        final_shape = tuple(None if isinstance(d, UnknownSize) else int(d) for d in final_shape)\n        return final_shape\n\n    def call(self, inputs):\n        return self._apply_recipe(inputs)\n\n    def get_config(self):\n        return {'pattern': self.pattern, 'reduction': self.reduction, **self.axes_lengths}\n\n"""
einops/layers/torch.py,0,"b""import torch\n\nfrom . import RearrangeMixin, ReduceMixin\n\n__author__ = 'Alex Rogozhnikov'\n\n\nclass Rearrange(RearrangeMixin, torch.nn.Module):\n    def forward(self, input):\n        return self._apply_recipe(input)\n\n\nclass Reduce(ReduceMixin, torch.nn.Module):\n    def forward(self, input):\n        return self._apply_recipe(input)"""
