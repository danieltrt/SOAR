file_path,api_count,code
create_dataset.py,0,"b'import argparse\nimport os\nfrom pathlib import Path\n\nimport librosa\nimport numpy as np\nimport tqdm\nimport ruamel.yaml\n\nfrom preprocessing.text_processing import Phonemizer, TextCleaner\nfrom utils.audio import Audio\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--config\', dest=\'CONFIG\', type=str, required=True)\nparser.add_argument(\'--dont_cache_phonemes\', dest=\'CACHE_PHON\', action=\'store_false\')\nparser.add_argument(\'--njobs\', dest=\'NJOBS\', type=int, default=16)\nparser.add_argument(\'--col_sep\', dest=\'COLUMN_SEP\', type=str, default=\'|\')\nparser.add_argument(\'--recompute_phon\', dest=\'RECOMPUTE_PHON\', action=\'store_true\')\nargs = parser.parse_args()\nfor arg in vars(args):\n    print(\'{}: {}\'.format(arg, getattr(args, arg)))\nyaml = ruamel.yaml.YAML()\nwith open(str(Path(args.CONFIG) / \'data_config.yaml\'), \'rb\') as conf_yaml:\n    config = yaml.load(conf_yaml)\nargs.DATA_DIR = config[\'data_directory\']\nargs.META_FILE = os.path.join(args.DATA_DIR, config[\'metadata_filename\'])\nargs.WAV_DIR = os.path.join(args.DATA_DIR, config[\'wav_subdir_name\'])\nargs.TARGET_DIR = config[\'train_data_directory\']\nif args.TARGET_DIR is None:\n    args.TARGET_DIR = args.DATA_DIR\n\nmel_dir = os.path.join(args.TARGET_DIR, \'mels\')\nif not os.path.exists(mel_dir):\n    os.makedirs(mel_dir)\n\nphon_path = os.path.join(args.TARGET_DIR, \'phonemes.npy\')\nif os.path.exists(phon_path) and not args.RECOMPUTE_PHON:\n    print(""using cached phonemes"")\n    audio_data = np.load(phon_path)\nelse:\n    print(\'\\nLoading and cleaning text\')\n    text_cleaner = TextCleaner()\n    audio_data = []\n    with open(args.META_FILE, \'r\', encoding=\'utf-8\') as f:\n        for l in f.readlines():\n            l_split = l.split(args.COLUMN_SEP)\n            filename, text = l_split[0], l_split[-1]\n            if filename.endswith(\'.wav\'):\n                filename = filename.split(\'.\')[-1]\n            text = text_cleaner.clean(text)\n            audio_data.append((filename, text))\n    audio_data = np.array(audio_data)\n    print(\'\\nPhonemizing\')\n    \n    phonemizer = Phonemizer(config[\'phoneme_language\'])\n    texts = audio_data[:, 1]\n    batch_size = 250  # batch phonemization to avoid memory issues.\n    phonemes = []\n    for i in tqdm.tqdm(range(0, len(audio_data), batch_size)):\n        batch = texts[i: i + batch_size]\n        batch = phonemizer.encode(batch, njobs=args.NJOBS, clean=False)\n        phonemes.extend(batch)\n    audio_data = np.concatenate([np.array(audio_data), np.expand_dims(phonemes, axis=1)], axis=1)\n    if args.CACHE_PHON:\n        np.save(phon_path, audio_data, allow_pickle=True)\n\nprint(\'\\nBuilding dataset and writing files\')\nnp.random.seed(42)\nnp.random.shuffle(audio_data)\ntest_metafile = os.path.join(args.TARGET_DIR, \'test_metafile.txt\')\ntrain_metafile = os.path.join(args.TARGET_DIR, \'train_metafile.txt\')\n\ntest_lines = [\'\'.join([filename, \'|\', text, \'|\', phon, \'\\n\']) for filename, text, phon in\n              audio_data[:config[\'n_test\']]]\ntrain_lines = [\'\'.join([filename, \'|\', text, \'|\', phon, \'\\n\']) for filename, text, phon in\n               audio_data[config[\'n_test\']:-1]]\n\nwith open(test_metafile, \'w+\', encoding=\'utf-8\') as test_f:\n    test_f.writelines(test_lines)\nwith open(train_metafile, \'w+\', encoding=\'utf-8\') as train_f:\n    train_f.writelines(train_lines)\n\naudio = Audio(config)\nfor i in tqdm.tqdm(range(len(audio_data))):\n    filename, _, _ = audio_data[i]\n    wav_path = os.path.join(args.WAV_DIR, filename + \'.wav\')\n    y, sr = librosa.load(wav_path, sr=config[\'sampling_rate\'])\n    mel = audio.mel_spectrogram(y)\n    mel_path = os.path.join(mel_dir, filename)\n    np.save(mel_path, mel.T)\nprint(\'\\nDone\')\n'"
extract_durations.py,7,"b'import argparse\nimport traceback\nimport pickle\n\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom utils.config_manager import ConfigManager\nfrom utils.logging import SummaryManager\nfrom preprocessing.data_handling import load_files, Dataset, DataPrepper\nfrom model.transformer_utils import create_mel_padding_mask\nfrom utils.alignments import get_durations_from_alignment\n\n# dynamically allocate GPU\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n        print(len(gpus), \'Physical GPUs,\', len(logical_gpus), \'Logical GPUs\')\n    except Exception:\n        traceback.print_exc()\n\n# consuming CLI, creating paths and directories, load data\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--config\', dest=\'config\', type=str)\nparser.add_argument(\'--session_name\', dest=\'session_name\', default=None)\nparser.add_argument(\'--recompute_pred\', dest=\'recompute_pred\', action=\'store_true\',\n                    help=\'Recompute the model predictions.\')\nparser.add_argument(\'--best\', dest=\'best\', action=\'store_true\',\n                    help=\'Use best head instead of weighted average of heads.\')\nparser.add_argument(\'--binary\', dest=\'binary\', action=\'store_true\',\n                    help=\'Use attention peak instead of all attentio values.\')\nparser.add_argument(\'--fix_jumps\', dest=\'fix_jumps\', action=\'store_true\',\n                    help=\'Scan attention peaks and try to fix jumps. Only with binary.\')\nparser.add_argument(\'--fill_mode_max\', dest=\'fill_mode_max\', action=\'store_true\',\n                    help=\'Fill zero durations with ones. Reduces phoneme duration with maximum value in sequence to compensate.\')\nparser.add_argument(\'--fill_mode_next\', dest=\'fill_mode_next\', action=\'store_true\',\n                    help=\'Fill zero durations with ones. Reduces next non-zero phoneme duration in sequence to compensate.\')\nparser.add_argument(\'--use_GT\', action=\'store_true\',\n                    help=\'Use ground truth mel instead of predicted mel to train forward model.\')\nargs = parser.parse_args()\nassert (args.fill_mode_max is False) or (args.fill_mode_next is False), \'Choose one gap filling mode.\'\nweighted = not args.best\nbinary = args.binary\nfill_gaps = args.fill_mode_max or args.fill_mode_next\nfix_jumps = args.fix_jumps\nfill_mode = f""{f\'max\' * args.fill_mode_max}{f\'next\' * args.fill_mode_next}""\nfilling_tag = f""{f\'(max)\' * args.fill_mode_max}{f\'(next)\' * args.fill_mode_next}""\ntag_description = \'\'.join(\n    [f\'{""_weighted"" * weighted}{""_best"" * (not weighted)}\',\n     f\'{""_binary"" * binary}\',\n     f\'{""_filled"" * fill_gaps}{filling_tag}\',\n     f\'{""_fix_jumps"" * fix_jumps}\'])\nwriter_tag = f\'DurationExtraction{tag_description}\'\nprint(writer_tag)\nconfig_manager = ConfigManager(config_path=args.config, model_kind=\'autoregressive\', session_name=args.session_name)\nconfig = config_manager.config\n\nmeldir = config_manager.train_datadir / \'mels\'\ntarget_dir = config_manager.train_datadir / f\'forward_data\'\ntrain_target_dir = target_dir / \'train\'\nval_target_dir = target_dir / \'val\'\ntrain_predictions_dir = target_dir / f\'train_predictions_{config_manager.session_name}\'\nval_predictions_dir = target_dir / f\'val_predictions_{config_manager.session_name}\'\ntarget_dir.mkdir(exist_ok=True)\ntrain_target_dir.mkdir(exist_ok=True)\nval_target_dir.mkdir(exist_ok=True)\ntrain_predictions_dir.mkdir(exist_ok=True)\nval_predictions_dir.mkdir(exist_ok=True)\nconfig_manager.dump_config()\nscript_batch_size = 5 * config[\'batch_size\']\nval_has_files = len([batch_file for batch_file in val_predictions_dir.iterdir() if batch_file.suffix == \'.npy\'])\ntrain_has_files = len([batch_file for batch_file in train_predictions_dir.iterdir() if batch_file.suffix == \'.npy\'])\nmodel = config_manager.load_model()\nif args.recompute_pred or (val_has_files == 0) or (train_has_files == 0):\n    train_meta = config_manager.train_datadir / \'train_metafile.txt\'\n    test_meta = config_manager.train_datadir / \'test_metafile.txt\'\n    train_samples, _ = load_files(metafile=str(train_meta),\n                                  meldir=str(meldir),\n                                  num_samples=config[\'n_samples\'])  # (phonemes, mel)\n    val_samples, _ = load_files(metafile=str(test_meta),\n                                meldir=str(meldir),\n                                num_samples=config[\'n_samples\'])  # (phonemes, text, mel)\n    \n    # get model, prepare data for model, create datasets\n    \n    data_prep = DataPrepper(config=config,\n                            tokenizer=model.tokenizer)\n    script_batch_size = 5 * config[\'batch_size\']  # faster parallel computation\n    train_dataset = Dataset(samples=train_samples,\n                            preprocessor=data_prep,\n                            batch_size=script_batch_size,\n                            shuffle=False,\n                            drop_remainder=False)\n    val_dataset = Dataset(samples=val_samples,\n                          preprocessor=data_prep,\n                          batch_size=script_batch_size,\n                          shuffle=False,\n                          drop_remainder=False)\n    if model.r != 1:\n        print(f""ERROR: model\'s reduction factor is greater than 1, check config. (r={model.r}"")\n    # identify last decoder block\n    n_layers = len(config_manager.config[\'decoder_num_heads\'])\n    n_dense = int(config_manager.config[\'decoder_dense_blocks\'])\n    n_convs = int(n_layers - n_dense)\n    if n_convs > 0:\n        last_layer_key = f\'Decoder_ConvBlock{n_convs}_CrossfAttention\'\n    else:\n        last_layer_key = f\'Decoder_DenseBlock{n_dense}_CrossAttention\'\n    print(f\'Extracting attention from layer {last_layer_key}\')\n    \n    iterator = tqdm(enumerate(val_dataset.all_batches()))\n    for c, (val_mel, val_text, val_stop) in iterator:\n        iterator.set_description(f\'Processing validation set\')\n        outputs = model.val_step(inp=val_text,\n                                 tar=val_mel,\n                                 stop_prob=val_stop)\n        if args.use_GT:\n            batch = (val_mel.numpy(), val_text.numpy(), outputs[\'decoder_attention\'][last_layer_key].numpy())\n        else:\n            mask = create_mel_padding_mask(val_mel)\n            out_val = tf.expand_dims(1 - tf.squeeze(create_mel_padding_mask(val_mel[:, 1:, :])), -1) * outputs[\n                \'final_output\'].numpy()\n            batch = (out_val.numpy(), val_text.numpy(), outputs[\'decoder_attention\'][last_layer_key].numpy())\n        with open(str(val_predictions_dir / f\'{c}_batch_prediction.npy\'), \'wb\') as file:\n            pickle.dump(batch, file)\n    \n    iterator = tqdm(enumerate(train_dataset.all_batches()))\n    for c, (train_mel, train_text, train_stop) in iterator:\n        iterator.set_description(f\'Processing training set\')\n        outputs = model.val_step(inp=train_text,\n                                 tar=train_mel,\n                                 stop_prob=train_stop)\n        if args.use_GT:\n            batch = (train_mel.numpy(), train_text.numpy(), outputs[\'decoder_attention\'][last_layer_key].numpy())\n        else:\n            mask = create_mel_padding_mask(train_mel)\n            out_train = tf.expand_dims(1 - tf.squeeze(create_mel_padding_mask(train_mel[:, 1:, :])), -1) * outputs[\n                \'final_output\'].numpy()\n            batch = (out_train.numpy(), train_text.numpy(), outputs[\'decoder_attention\'][last_layer_key].numpy())\n        with open(str(train_predictions_dir / f\'{c}_batch_prediction.npy\'), \'wb\') as file:\n            pickle.dump(batch, file)\n\nsummary_manager = SummaryManager(model=model, log_dir=config_manager.log_dir / writer_tag, config=config,\n                                 default_writer=writer_tag)\nval_batch_files = [batch_file for batch_file in val_predictions_dir.iterdir() if batch_file.suffix == \'.npy\']\niterator = tqdm(enumerate(val_batch_files))\nall_val_durations = np.array([])\nnew_alignments = []\ntotal_val_samples = 0\nfor c, batch_file in iterator:\n    iterator.set_description(f\'Extracting validation alignments\')\n    val_mel, val_text, val_alignments = np.load(str(batch_file), allow_pickle=True)\n    durations, unpad_mels, unpad_phonemes, final_align = get_durations_from_alignment(\n        batch_alignments=val_alignments,\n        mels=val_mel,\n        phonemes=val_text,\n        weighted=weighted,\n        binary=binary,\n        fill_gaps=fill_gaps,\n        fill_mode=fill_mode,\n        fix_jumps=fix_jumps)\n    batch_size = len(val_mel)\n    for i in range(batch_size):\n        sample_idx = total_val_samples + i\n        all_val_durations = np.append(all_val_durations, durations[i])\n        new_alignments.append(final_align[i])\n        sample = (unpad_mels[i], unpad_phonemes[i], durations[i])\n        np.save(str(val_target_dir / f\'{sample_idx}_mel_phon_dur.npy\'), sample)\n    total_val_samples += batch_size\nall_val_durations[all_val_durations >= 20] = 20\nbuckets = len(set(all_val_durations))\nsummary_manager.add_histogram(values=all_val_durations, tag=\'ValidationDurations\', buckets=buckets)\nfor i, alignment in enumerate(new_alignments):\n    summary_manager.add_image(tag=\'ExtractedValidationAlignments\',\n                              image=tf.expand_dims(tf.expand_dims(alignment, 0), -1),\n                              step=i)\n\ntrain_batch_files = [batch_file for batch_file in train_predictions_dir.iterdir() if batch_file.suffix == \'.npy\']\niterator = tqdm(enumerate(train_batch_files))\nall_train_durations = np.array([])\nnew_alignments = []\ntotal_train_samples = 0\nfor c, batch_file in iterator:\n    iterator.set_description(f\'Extracting training alignments\')\n    train_mel, train_text, train_alignments = np.load(str(batch_file), allow_pickle=True)\n    durations, unpad_mels, unpad_phonemes, final_align = get_durations_from_alignment(\n        batch_alignments=train_alignments,\n        mels=train_mel,\n        phonemes=train_text,\n        weighted=weighted,\n        binary=binary,\n        fill_gaps=fill_gaps,\n        fill_mode=fill_mode,\n        fix_jumps=fix_jumps)\n    batch_size = len(train_mel)\n    for i in range(batch_size):\n        sample_idx = total_train_samples + i\n        sample = (unpad_mels[i], unpad_phonemes[i], durations[i])\n        new_alignments.append(final_align[i])\n        all_train_durations = np.append(all_train_durations, durations[i])\n        np.save(str(train_target_dir / f\'{sample_idx}_mel_phon_dur.npy\'), sample)\n    total_train_samples += batch_size\nall_train_durations[all_train_durations >= 20] = 20\nbuckets = len(set(all_train_durations))\nsummary_manager.add_histogram(values=all_train_durations, tag=\'TrainDurations\', buckets=buckets)\nfor i, alignment in enumerate(new_alignments):\n    summary_manager.add_image(tag=\'ExtractedTrainingAlignments\', image=tf.expand_dims(tf.expand_dims(alignment, 0), -1),\n                              step=i)\nprint(\'Done.\')\n'"
train_autoregressive.py,7,"b'import argparse\nimport traceback\n\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import trange\n\nfrom utils.config_manager import ConfigManager\nfrom preprocessing.data_handling import load_files, Dataset, DataPrepper\nfrom utils.decorators import ignore_exception, time_it\nfrom utils.scheduling import piecewise_linear_schedule, reduction_schedule\nfrom utils.logging import SummaryManager\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# dynamically allocate GPU\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n        print(len(gpus), \'Physical GPUs,\', len(logical_gpus), \'Logical GPUs\')\n    except Exception:\n        traceback.print_exc()\n\n\n@ignore_exception\n@time_it\ndef validate(model,\n             val_dataset,\n             summary_manager):\n    val_loss = {\'loss\': 0.}\n    norm = 0.\n    for val_mel, val_text, val_stop in val_dataset.all_batches():\n        model_out = model.val_step(inp=val_text,\n                                   tar=val_mel,\n                                   stop_prob=val_stop)\n        norm += 1\n        val_loss[\'loss\'] += model_out[\'loss\']\n    val_loss[\'loss\'] /= norm\n    summary_manager.display_loss(model_out, tag=\'Validation\', plot_all=True)\n    summary_manager.display_attention_heads(model_out, tag=\'ValidationAttentionHeads\')\n    summary_manager.display_mel(mel=model_out[\'mel_linear\'][0], tag=f\'Validation/linear_mel_out\')\n    summary_manager.display_mel(mel=model_out[\'final_output\'][0], tag=f\'Validation/predicted_mel\')\n    residual = abs(model_out[\'mel_linear\'] - model_out[\'final_output\'])\n    summary_manager.display_mel(mel=residual[0], tag=f\'Validation/conv-linear_residual\')\n    summary_manager.display_mel(mel=val_mel[0], tag=f\'Validation/target_mel\')\n    return val_loss[\'loss\']\n\n\n# consuming CLI, creating paths and directories, load data\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--config\', dest=\'config\', type=str)\nparser.add_argument(\'--reset_dir\', dest=\'clear_dir\', action=\'store_true\',\n                    help=""deletes everything under this config\'s folder."")\nparser.add_argument(\'--reset_logs\', dest=\'clear_logs\', action=\'store_true\',\n                    help=""deletes logs under this config\'s folder."")\nparser.add_argument(\'--reset_weights\', dest=\'clear_weights\', action=\'store_true\',\n                    help=""deletes weights under this config\'s folder."")\nparser.add_argument(\'--session_name\', dest=\'session_name\', default=None)\nargs = parser.parse_args()\nconfig_manager = ConfigManager(config_path=args.config, model_kind=\'autoregressive\', session_name=args.session_name)\nconfig = config_manager.config\nconfig_manager.create_remove_dirs(clear_dir=args.clear_dir,\n                                  clear_logs=args.clear_logs,\n                                  clear_weights=args.clear_weights)\nconfig_manager.dump_config()\nconfig_manager.print_config()\n\ntrain_samples, _ = load_files(metafile=str(config_manager.train_datadir / \'train_metafile.txt\'),\n                              meldir=str(config_manager.train_datadir / \'mels\'),\n                              num_samples=config[\'n_samples\'])  # (phonemes, mel)\nval_samples, _ = load_files(metafile=str(config_manager.train_datadir / \'test_metafile.txt\'),\n                            meldir=str(config_manager.train_datadir / \'mels\'),\n                            num_samples=config[\'n_samples\'])  # (phonemes, text, mel)\n\n# get model, prepare data for model, create datasets\nmodel = config_manager.get_model()\nconfig_manager.compile_model(model)\ndata_prep = DataPrepper(config=config,\n                        tokenizer=model.tokenizer)\n\ntest_list = [data_prep(s) for s in val_samples]\ntrain_dataset = Dataset(samples=train_samples,\n                        preprocessor=data_prep,\n                        batch_size=config[\'batch_size\'],\n                        mel_channels=config[\'mel_channels\'],\n                        shuffle=True)\nval_dataset = Dataset(samples=val_samples,\n                      preprocessor=data_prep,\n                      batch_size=config[\'batch_size\'],\n                      mel_channels=config[\'mel_channels\'],\n                      shuffle=False)\n\n# create logger and checkpointer and restore latest model\n\nsummary_manager = SummaryManager(model=model, log_dir=config_manager.log_dir, config=config)\ncheckpoint = tf.train.Checkpoint(step=tf.Variable(1),\n                                 optimizer=model.optimizer,\n                                 net=model)\nmanager = tf.train.CheckpointManager(checkpoint, str(config_manager.weights_dir),\n                                     max_to_keep=config[\'keep_n_weights\'],\n                                     keep_checkpoint_every_n_hours=config[\'keep_checkpoint_every_n_hours\'])\ncheckpoint.restore(manager.latest_checkpoint)\nif manager.latest_checkpoint:\n    print(f\'\\nresuming training from step {model.step} ({manager.latest_checkpoint})\')\nelse:\n    print(f\'\\nstarting training from scratch\')\n# main event\nprint(\'\\nTRAINING\')\nlosses = []\n_ = train_dataset.next_batch()\nt = trange(model.step, config[\'max_steps\'], leave=True)\nfor _ in t:\n    t.set_description(f\'step {model.step}\')\n    mel, phonemes, stop = train_dataset.next_batch()\n    decoder_prenet_dropout = piecewise_linear_schedule(model.step, config[\'decoder_prenet_dropout_schedule\'])\n    learning_rate = piecewise_linear_schedule(model.step, config[\'learning_rate_schedule\'])\n    reduction_factor = reduction_schedule(model.step, config[\'reduction_factor_schedule\'])\n    drop_n_heads = tf.cast(reduction_schedule(model.step, config[\'head_drop_schedule\']), tf.int32)\n    t.display(f\'reduction factor {reduction_factor}\', pos=10)\n    model.set_constants(decoder_prenet_dropout=decoder_prenet_dropout,\n                        learning_rate=learning_rate,\n                        reduction_factor=reduction_factor,\n                        drop_n_heads=drop_n_heads)\n    output = model.train_step(inp=phonemes,\n                              tar=mel,\n                              stop_prob=stop)\n    losses.append(float(output[\'loss\']))\n    \n    t.display(f\'step loss: {losses[-1]}\', pos=1)\n    for pos, n_steps in enumerate(config[\'n_steps_avg_losses\']):\n        if len(losses) > n_steps:\n            t.display(f\'{n_steps}-steps average loss: {sum(losses[-n_steps:]) / n_steps}\', pos=pos + 2)\n    \n    summary_manager.display_loss(output, tag=\'Train\')\n    summary_manager.display_scalar(tag=\'Meta/decoder_prenet_dropout\', scalar_value=model.decoder_prenet.rate)\n    summary_manager.display_scalar(tag=\'Meta/learning_rate\', scalar_value=model.optimizer.lr)\n    summary_manager.display_scalar(tag=\'Meta/reduction_factor\', scalar_value=model.r)\n    summary_manager.display_scalar(tag=\'Meta/drop_n_heads\', scalar_value=model.drop_n_heads)\n    if model.step % config[\'train_images_plotting_frequency\'] == 0:\n        summary_manager.display_attention_heads(output, tag=\'TrainAttentionHeads\')\n        summary_manager.display_mel(mel=output[\'mel_linear\'][0], tag=f\'Train/linear_mel_out\')\n        summary_manager.display_mel(mel=output[\'final_output\'][0], tag=f\'Train/predicted_mel\')\n        residual = abs(output[\'mel_linear\'] - output[\'final_output\'])\n        summary_manager.display_mel(mel=residual[0], tag=f\'Train/conv-linear_residual\')\n        summary_manager.display_mel(mel=mel[0], tag=f\'Train/target_mel\')\n    \n    if model.step % config[\'weights_save_frequency\'] == 0:\n        save_path = manager.save()\n        t.display(f\'checkpoint at step {model.step}: {save_path}\', pos=len(config[\'n_steps_avg_losses\']) + 2)\n    \n    if model.step % config[\'validation_frequency\'] == 0:\n        val_loss, time_taken = validate(model=model,\n                                        val_dataset=val_dataset,\n                                        summary_manager=summary_manager)\n        t.display(f\'validation loss at step {model.step}: {val_loss} (took {time_taken}s)\',\n                  pos=len(config[\'n_steps_avg_losses\']) + 3)\n    \n    if model.step % config[\'prediction_frequency\'] == 0 and (model.step >= config[\'prediction_start_step\']):\n        for j in range(config[\'n_predictions\']):\n            mel, phonemes, stop = test_list[j]\n            t.display(f\'Predicting {j}\', pos=len(config[\'n_steps_avg_losses\']) + 4)\n            pred = model.predict(phonemes,\n                                 max_length=mel.shape[0] + 50,\n                                 encode=False,\n                                 verbose=False)\n            pred_mel = pred[\'mel\']\n            target_mel = mel\n            summary_manager.display_attention_heads(outputs=pred, tag=f\'TestAttentionHeads/sample {j}\')\n            summary_manager.display_mel(mel=pred_mel, tag=f\'Test/sample {j}/predicted_mel\')\n            summary_manager.display_mel(mel=target_mel, tag=f\'Test/sample {j}/target_mel\')\n            if model.step > config[\'audio_start_step\']:\n                summary_manager.display_audio(tag=f\'Target/sample {j}\', mel=target_mel)\n                summary_manager.display_audio(tag=f\'Prediction/sample {j}\', mel=pred_mel)\n\nprint(\'Done.\')\n'"
train_forward.py,11,"b'import argparse\nimport traceback\nfrom pathlib import Path\nfrom time import time\n\nimport tensorflow as tf\nimport numpy as np\nfrom tqdm import trange\n\nfrom utils.config_manager import ConfigManager\nfrom preprocessing.data_handling import Dataset, ForwardDataPrepper\nfrom utils.decorators import ignore_exception, time_it\nfrom utils.scheduling import piecewise_linear_schedule, reduction_schedule\nfrom utils.logging import SummaryManager\nfrom model.transformer_utils import create_mel_padding_mask\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# dynamically allocate GPU\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n        print(len(gpus), \'Physical GPUs,\', len(logical_gpus), \'Logical GPUs\')\n    except Exception:\n        traceback.print_exc()\n\n\ndef build_file_list(data_dir: Path):\n    sample_paths = []\n    for item in data_dir.iterdir():\n        if item.suffix == \'.npy\':\n            sample_paths.append(str(item))\n    return sample_paths\n\n\n@ignore_exception\n@time_it\ndef validate(model,\n             val_dataset,\n             summary_manager):\n    val_loss = {\'loss\': 0.}\n    norm = 0.\n    for mel, phonemes, durations in val_dataset.all_batches():\n        model_out = model.val_step(input_sequence=phonemes,\n                                   target_sequence=mel,\n                                   target_durations=durations)\n        norm += 1\n        val_loss[\'loss\'] += model_out[\'loss\']\n    val_loss[\'loss\'] /= norm\n    summary_manager.display_loss(model_out, tag=\'Validation\', plot_all=True)\n    summary_manager.display_attention_heads(model_out, tag=\'ValidationAttentionHeads\')\n    summary_manager.add_histogram(tag=f\'Validation/Predicted durations\', values=model_out[\'duration\'])\n    summary_manager.add_histogram(tag=f\'Validation/Target durations\', values=durations)\n    summary_manager.display_mel(mel=model_out[\'mel\'][0], tag=f\'Validation/predicted_mel\')\n    summary_manager.display_mel(mel=mel[0], tag=f\'Validation/target_mel\')\n    return val_loss[\'loss\']\n\n\n# consuming CLI, creating paths and directories, load data\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--config\', dest=\'config\', type=str)\nparser.add_argument(\'--reset_dir\', dest=\'clear_dir\', action=\'store_true\',\n                    help=""deletes everything under this config\'s folder."")\nparser.add_argument(\'--reset_logs\', dest=\'clear_logs\', action=\'store_true\',\n                    help=""deletes logs under this config\'s folder."")\nparser.add_argument(\'--reset_weights\', dest=\'clear_weights\', action=\'store_true\',\n                    help=""deletes weights under this config\'s folder."")\nparser.add_argument(\'--session_name\', dest=\'session_name\', default=None)\nargs = parser.parse_args()\n\nconfig_manager = ConfigManager(config_path=args.config, model_kind=\'forward\', session_name=args.session_name)\nconfig = config_manager.config\nconfig_manager.create_remove_dirs(clear_dir=args.clear_dir,\n                                  clear_logs=args.clear_logs,\n                                  clear_weights=args.clear_weights)\nconfig_manager.dump_config()\nconfig_manager.print_config()\n\ntrain_data_list = build_file_list(config_manager.train_datadir / \'forward_data/train\')\ndataprep = ForwardDataPrepper()\ntrain_dataset = Dataset(samples=train_data_list,\n                        mel_channels=config[\'mel_channels\'],\n                        preprocessor=dataprep,\n                        batch_size=config[\'batch_size\'],\n                        shuffle=True)\nval_data_list = build_file_list(config_manager.train_datadir / \'forward_data/val\')\nval_dataset = Dataset(samples=val_data_list,\n                      mel_channels=config[\'mel_channels\'],\n                      preprocessor=dataprep,\n                      batch_size=config[\'batch_size\'],\n                      shuffle=False)\n\n# get model, prepare data for model, create datasets\nmodel = config_manager.get_model()\nconfig_manager.compile_model(model)\n\n# create logger and checkpointer and restore latest model\nsummary_manager = SummaryManager(model=model, log_dir=config_manager.log_dir, config=config)\ncheckpoint = tf.train.Checkpoint(step=tf.Variable(1),\n                                 optimizer=model.optimizer,\n                                 net=model)\nmanager = tf.train.CheckpointManager(checkpoint, config_manager.weights_dir,\n                                     max_to_keep=config[\'keep_n_weights\'],\n                                     keep_checkpoint_every_n_hours=config[\'keep_checkpoint_every_n_hours\'])\ncheckpoint.restore(manager.latest_checkpoint)\nif manager.latest_checkpoint:\n    print(f\'\\nresuming training from step {model.step} ({manager.latest_checkpoint})\')\nelse:\n    print(f\'\\nstarting training from scratch\')\n# main event\nprint(\'\\nTRAINING\')\nlosses = []\ntest_batch = val_dataset.next_batch()\nt = trange(model.step, config[\'max_steps\'], leave=True)\nfor _ in t:\n    t.set_description(f\'step {model.step}\')\n    mel, phonemes, durations = train_dataset.next_batch()\n    learning_rate = piecewise_linear_schedule(model.step, config[\'learning_rate_schedule\'])\n    decoder_prenet_dropout = piecewise_linear_schedule(model.step, config[\'decoder_prenet_dropout_schedule\'])\n    drop_n_heads = tf.cast(reduction_schedule(model.step, config[\'head_drop_schedule\']), tf.int32)\n    model.set_constants(decoder_prenet_dropout=decoder_prenet_dropout,\n                        learning_rate=learning_rate,\n                        drop_n_heads=drop_n_heads)\n    output = model.train_step(input_sequence=phonemes,\n                              target_sequence=mel,\n                              target_durations=durations)\n    losses.append(float(output[\'loss\']))\n    \n    t.display(f\'step loss: {losses[-1]}\', pos=1)\n    for pos, n_steps in enumerate(config[\'n_steps_avg_losses\']):\n        if len(losses) > n_steps:\n            t.display(f\'{n_steps}-steps average loss: {sum(losses[-n_steps:]) / n_steps}\', pos=pos + 2)\n    \n    summary_manager.display_loss(output, tag=\'Train\')\n    summary_manager.display_scalar(tag=\'Meta/learning_rate\', scalar_value=model.optimizer.lr)\n    summary_manager.display_scalar(tag=\'Meta/decoder_prenet_dropout\', scalar_value=model.decoder_prenet.rate)\n    summary_manager.display_scalar(tag=\'Meta/drop_n_heads\', scalar_value=model.drop_n_heads)\n    if model.step % config[\'train_images_plotting_frequency\'] == 0:\n        summary_manager.display_attention_heads(output, tag=\'TrainAttentionHeads\')\n        summary_manager.display_mel(mel=output[\'mel\'][0], tag=f\'Train/predicted_mel\')\n        summary_manager.display_mel(mel=mel[0], tag=f\'Train/target_mel\')\n        summary_manager.add_histogram(tag=f\'Train/Predicted durations\', values=output[\'duration\'])\n        summary_manager.add_histogram(tag=f\'Train/Target durations\', values=durations)\n    \n    if model.step % config[\'weights_save_frequency\'] == 0:\n        save_path = manager.save()\n        t.display(f\'checkpoint at step {model.step}: {save_path}\', pos=len(config[\'n_steps_avg_losses\']) + 2)\n    \n    if model.step % config[\'validation_frequency\'] == 0:\n        t.display(f\'Validating\', pos=len(config[\'n_steps_avg_losses\']) + 3)\n        val_loss, time_taken = validate(model=model,\n                                        val_dataset=val_dataset,\n                                        summary_manager=summary_manager)\n        t.display(f\'validation loss at step {model.step}: {val_loss} (took {time_taken}s)\',\n                  pos=len(config[\'n_steps_avg_losses\']) + 3)\n    \n    if model.step % config[\'prediction_frequency\'] == 0 and (model.step >= config[\'prediction_start_step\']):\n        tar_mel, phonemes, durs = test_batch\n        t.display(f\'Predicting\', pos=len(config[\'n_steps_avg_losses\']) + 4)\n        timed_pred = time_it(model.predict)\n        model_out, time_taken = timed_pred(phonemes, encode=False)\n        summary_manager.display_attention_heads(model_out, tag=\'TestAttentionHeads\')\n        summary_manager.add_histogram(tag=f\'Test/Predicted durations\', values=model_out[\'duration\'])\n        summary_manager.add_histogram(tag=f\'Test/Target durations\', values=durs)\n        pred_lengths = tf.cast(tf.reduce_sum(1 - model_out[\'expanded_mask\'], axis=-1), tf.int32)\n        pred_lengths = tf.squeeze(pred_lengths)\n        tar_lengths = tf.cast(tf.reduce_sum(1 - create_mel_padding_mask(tar_mel), axis=-1), tf.int32)\n        tar_lengths = tf.squeeze(tar_lengths)\n        display_start = time()\n        for j, pred_mel in enumerate(model_out[\'mel\']):\n            predval = pred_mel[:pred_lengths[j], :]\n            tar_value = tar_mel[j, :tar_lengths[j], :]\n            summary_manager.display_mel(mel=predval, tag=f\'Test/sample {j}/predicted_mel\')\n            summary_manager.display_mel(mel=tar_value, tag=f\'Test/sample {j}/target_mel\')\n            if j < config[\'n_predictions\']:\n                if model.step >= config[\'audio_start_step\'] and (\n                        model.step % config[\'audio_prediction_frequency\'] == 0):\n                    summary_manager.display_audio(tag=f\'Target/sample {j}\', mel=tar_value)\n                    summary_manager.display_audio(tag=f\'Prediction/sample {j}\', mel=predval)\n            else:\n                break\n        display_end = time()\n        t.display(f\'Predictions took {time_taken}. Displaying took {display_end - display_start}.\',\n                  pos=len(config[\'n_steps_avg_losses\']) + 4)\nprint(\'Done.\')\n'"
model/__init__.py,0,b''
model/layers.py,84,"b'import tensorflow as tf\n\nfrom model.transformer_utils import positional_encoding, scaled_dot_product_attention\n\n\nclass CNNResNorm(tf.keras.layers.Layer):\n    def __init__(self,\n                 out_size: int,\n                 n_layers: int,\n                 hidden_size: int,\n                 kernel_size: int,\n                 inner_activation: str,\n                 last_activation: str,\n                 padding: str,\n                 normalization: str,\n                 **kwargs):\n        super(CNNResNorm, self).__init__(**kwargs)\n        self.convolutions = [tf.keras.layers.Conv1D(filters=hidden_size,\n                                                    kernel_size=kernel_size,\n                                                    padding=padding)\n                             for _ in range(n_layers - 1)]\n        self.inner_activations = [tf.keras.layers.Activation(inner_activation) for _ in range(n_layers - 1)]\n        self.last_conv = tf.keras.layers.Conv1D(filters=out_size,\n                                                kernel_size=kernel_size,\n                                                padding=padding)\n        self.last_activation = tf.keras.layers.Activation(last_activation)\n        if normalization == \'layer\':\n            self.normalization = [tf.keras.layers.LayerNormalization(epsilon=1e-6) for _ in range(n_layers + 1)]\n        elif normalization == \'batch\':\n            self.normalization = [tf.keras.layers.BatchNormalization() for _ in range(n_layers + 1)]\n        else:\n            assert False is True, f\'normalization must be either ""layer"" or ""batch"", not {normalization}.\'\n    \n    def call_convs(self, x, training):\n        for i in range(0, len(self.convolutions)):\n            x = self.convolutions[i](x)\n            x = self.normalization[i](x, training=training)\n            x = self.inner_activations[i](x)\n        return x\n    \n    def call(self, inputs, training):\n        x = self.call_convs(inputs, training=training)\n        x = self.last_conv(x)\n        x = self.normalization[-2](x, training=training)\n        x = self.last_activation(x)\n        return self.normalization[-1](inputs + x)\n\n\nclass FFNResNorm(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 dense_hidden_units: int,\n                 dropout_rate: float,\n                 **kwargs):\n        super(FFNResNorm, self).__init__(**kwargs)\n        self.d1 = tf.keras.layers.Dense(dense_hidden_units)\n        self.activation = tf.keras.layers.Activation(\'relu\')\n        self.d2 = tf.keras.layers.Dense(model_dim)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.last_ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    def call(self, x, training):\n        ffn_out = self.d1(x)\n        ffn_out = self.d2(ffn_out)  # (batch_size, input_seq_len, model_dim)\n        ffn_out = self.ln(ffn_out)  # (batch_size, input_seq_len, model_dim)\n        ffn_out = self.activation(ffn_out)\n        ffn_out = self.dropout(ffn_out, training=training)\n        return self.last_ln(ffn_out + x)\n\n\nclass HeadDrop(tf.keras.layers.Layer):\n    """""" Randomly drop n heads. """"""\n    \n    def __init__(self, **kwargs):\n        super(HeadDrop, self).__init__(**kwargs)\n    \n    def call(self, batch, training: bool, drop_n_heads: int):\n        if not training or (drop_n_heads == 0):\n            return batch\n        if len(tf.shape(batch)) != 4:\n            raise Exception(\'attention values must be 4 dimensional\')\n        batch_size = tf.shape(batch)[0]\n        head_n = tf.shape(batch)[1]\n        if head_n == 1:\n            return batch\n        # assert drop_n_heads < head_n, \'drop_n_heads must less than number of heads\'\n        keep_head_batch = tf.TensorArray(tf.float32, size=batch_size)\n        keep_mask = tf.concat([tf.ones(head_n - drop_n_heads), tf.zeros(drop_n_heads)], axis=0)\n        for i in range(batch_size):\n            t = tf.random.shuffle(keep_mask)\n            keep_head_batch = keep_head_batch.write(i, t)\n        keep_head_batch = keep_head_batch.stack()\n        keep_head_batch = keep_head_batch[:, :, tf.newaxis, tf.newaxis]\n        return batch * keep_head_batch * tf.cast(head_n / (head_n - drop_n_heads), tf.float32)\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    \n    def __init__(self, model_dim: int, num_heads: int, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.model_dim = model_dim\n        self.head_drop = HeadDrop()\n        \n        assert model_dim % self.num_heads == 0\n        \n        self.depth = model_dim // self.num_heads\n        \n        self.wq = tf.keras.layers.Dense(model_dim)\n        self.wk = tf.keras.layers.Dense(model_dim)\n        self.wv = tf.keras.layers.Dense(model_dim)\n        \n        self.dense = tf.keras.layers.Dense(model_dim)\n    \n    def split_heads(self, x, batch_size: int):\n        """""" Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        """"""\n        \n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q_in, mask, training, drop_n_heads):\n        batch_size = tf.shape(q_in)[0]\n        \n        q = self.wq(q_in)  # (batch_size, seq_len, model_dim)\n        k = self.wk(k)  # (batch_size, seq_len, model_dim)\n        v = self.wv(v)  # (batch_size, seq_len, model_dim)\n        \n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n        \n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = self.head_drop(scaled_attention, training=training, drop_n_heads=drop_n_heads)\n        \n        scaled_attention = tf.transpose(scaled_attention,\n                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concat_attention = tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.model_dim))  # (batch_size, seq_len_q, model_dim)\n        concat_query = tf.concat([q_in, concat_attention], axis=-1)\n        output = self.dense(concat_query)  # (batch_size, seq_len_q, model_dim)\n        \n        return output, attention_weights\n\n\nclass SelfAttentionResNorm(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 num_heads: int,\n                 dropout_rate: float,\n                 **kwargs):\n        super(SelfAttentionResNorm, self).__init__(**kwargs)\n        self.mha = MultiHeadAttention(model_dim, num_heads)\n        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.last_ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    def call(self, x, training, mask, drop_n_heads):\n        attn_out, attn_weights = self.mha(x, x, x, mask, training=training,\n                                          drop_n_heads=drop_n_heads)  # (batch_size, input_seq_len, model_dim)\n        attn_out = self.ln(attn_out)  # (batch_size, input_seq_len, model_dim)\n        out = self.dropout(attn_out, training=training)\n        return self.last_ln(out + x), attn_weights\n\n\nclass SelfAttentionDenseBlock(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 num_heads: int,\n                 dense_hidden_units: int,\n                 dropout_rate: float,\n                 **kwargs):\n        super(SelfAttentionDenseBlock, self).__init__(**kwargs)\n        self.sarn = SelfAttentionResNorm(model_dim, num_heads, dropout_rate=dropout_rate)\n        self.ffn = FFNResNorm(model_dim, dense_hidden_units, dropout_rate=dropout_rate)\n    \n    def call(self, x, training, mask, drop_n_heads):\n        attn_out, attn_weights = self.sarn(x, mask=mask, training=training, drop_n_heads=drop_n_heads)\n        return self.ffn(attn_out, training=training), attn_weights\n\n\nclass SelfAttentionConvBlock(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 num_heads: int,\n                 dropout_rate: float,\n                 conv_filters: int,\n                 kernel_size: int,\n                 conv_activation: str,\n                 **kwargs):\n        super(SelfAttentionConvBlock, self).__init__(**kwargs)\n        self.sarn = SelfAttentionResNorm(model_dim, num_heads, dropout_rate=dropout_rate)\n        self.conv = CNNResNorm(out_size=model_dim,\n                               n_layers=2,\n                               hidden_size=conv_filters,\n                               kernel_size=kernel_size,\n                               inner_activation=conv_activation,\n                               last_activation=conv_activation,\n                               padding=\'same\',\n                               normalization=\'batch\')\n    \n    def call(self, x, training, mask, drop_n_heads):\n        attn_out, attn_weights = self.sarn(x, mask=mask, training=training, drop_n_heads=drop_n_heads)\n        conv = self.conv(attn_out)\n        return conv, attn_weights\n\n\nclass SelfAttentionBlocks(tf.keras.layers.Layer):\n    def __init__(self,\n                 model_dim: int,\n                 feed_forward_dimension: int,\n                 num_heads: list,\n                 maximum_position_encoding: int,\n                 conv_filters: int,\n                 dropout_rate: float,\n                 dense_blocks: int,\n                 kernel_size: int,\n                 conv_activation: str,\n                 **kwargs):\n        super(SelfAttentionBlocks, self).__init__(**kwargs)\n        self.model_dim = model_dim\n        self.pos_encoding_scalar = tf.Variable(1.)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, model_dim)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.encoder_SADB = [\n            SelfAttentionDenseBlock(model_dim=model_dim, dropout_rate=dropout_rate, num_heads=n_heads,\n                                    dense_hidden_units=feed_forward_dimension, name=f\'{self.name}_SADB_{i}\')\n            for i, n_heads in enumerate(num_heads[:dense_blocks])]\n        self.encoder_SACB = [\n            SelfAttentionConvBlock(model_dim=model_dim, dropout_rate=dropout_rate, num_heads=n_heads,\n                                   name=f\'{self.name}_SACB_{i}\', kernel_size=kernel_size,\n                                   conv_activation=conv_activation, conv_filters=conv_filters)\n            for i, n_heads in enumerate(num_heads[dense_blocks:])]\n    \n    def call(self, inputs, training, padding_mask, drop_n_heads, reduction_factor=1):\n        seq_len = tf.shape(inputs)[1]\n        x = inputs * tf.math.sqrt(tf.cast(self.model_dim, tf.float32))\n        x += self.pos_encoding_scalar * self.pos_encoding[:, :seq_len * reduction_factor:reduction_factor, :]\n        x = self.dropout(x, training=training)\n        attention_weights = {}\n        for i, block in enumerate(self.encoder_SADB):\n            x, attn_weights = block(x, training=training, mask=padding_mask, drop_n_heads=drop_n_heads)\n            attention_weights[f\'{self.name}_DenseBlock{i + 1}_SelfAttention\'] = attn_weights\n        for i, block in enumerate(self.encoder_SACB):\n            x, attn_weights = block(x, training=training, mask=padding_mask, drop_n_heads=drop_n_heads)\n            attention_weights[f\'{self.name}_ConvBlock{i + 1}_SelfAttention\'] = attn_weights\n        \n        return x, attention_weights\n\n\nclass CrossAttentionResnorm(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 num_heads: int,\n                 dropout_rate: float,\n                 **kwargs):\n        super(CrossAttentionResnorm, self).__init__(**kwargs)\n        self.mha = MultiHeadAttention(model_dim, num_heads)\n        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n    \n    def call(self, q, k, v, training, mask, drop_n_heads):\n        attn_values, attn_weights = self.mha(v, k=k, q_in=q, mask=mask, training=training, drop_n_heads=drop_n_heads)\n        attn_values = self.dropout(attn_values, training=training)\n        out = self.layernorm(attn_values + q)\n        return out, attn_weights\n\n\nclass CrossAttentionDenseBlock(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 num_heads: int,\n                 dense_hidden_units: int,\n                 dropout_rate: float,\n                 **kwargs):\n        super(CrossAttentionDenseBlock, self).__init__(**kwargs)\n        self.sarn = SelfAttentionResNorm(model_dim, num_heads, dropout_rate=dropout_rate)\n        self.carn = CrossAttentionResnorm(model_dim, num_heads, dropout_rate=dropout_rate)\n        self.ffn = FFNResNorm(model_dim, dense_hidden_units, dropout_rate=dropout_rate)\n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask, drop_n_heads):\n        attn1, attn_weights_block1 = self.sarn(x, mask=look_ahead_mask, training=training, drop_n_heads=drop_n_heads)\n        \n        attn2, attn_weights_block2 = self.carn(attn1, v=enc_output, k=enc_output,\n                                               mask=padding_mask, training=training, drop_n_heads=drop_n_heads)\n        ffn_out = self.ffn(attn2, training=training)\n        return ffn_out, attn_weights_block1, attn_weights_block2\n\n\nclass CrossAttentionConvBlock(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 num_heads: int,\n                 conv_filters: int,\n                 dropout_rate: float,\n                 kernel_size: int,\n                 conv_padding: str,\n                 conv_activation: str,\n                 **kwargs):\n        super(CrossAttentionConvBlock, self).__init__(**kwargs)\n        self.sarn = SelfAttentionResNorm(model_dim, num_heads, dropout_rate=dropout_rate)\n        self.carn = CrossAttentionResnorm(model_dim, num_heads, dropout_rate=dropout_rate)\n        self.conv = CNNResNorm(out_size=model_dim,\n                               n_layers=2,\n                               hidden_size=conv_filters,\n                               kernel_size=kernel_size,\n                               inner_activation=conv_activation,\n                               last_activation=conv_activation,\n                               padding=conv_padding,\n                               normalization=\'batch\')\n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask, drop_n_heads):\n        attn1, attn_weights_block1 = self.sarn(x, mask=look_ahead_mask, training=training, drop_n_heads=drop_n_heads)\n        \n        attn2, attn_weights_block2 = self.carn(attn1, v=enc_output, k=enc_output,\n                                               mask=padding_mask, training=training, drop_n_heads=drop_n_heads)\n        ffn_out = self.conv(attn2, training=training)\n        return ffn_out, attn_weights_block1, attn_weights_block2\n\n\nclass CrossAttentionBlocks(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 feed_forward_dimension: int,\n                 num_heads: list,\n                 maximum_position_encoding: int,\n                 dropout_rate: float,\n                 dense_blocks: int,\n                 conv_filters: int,\n                 conv_activation: str,\n                 conv_padding: str,\n                 conv_kernel: int,\n                 **kwargs):\n        super(CrossAttentionBlocks, self).__init__(**kwargs)\n        self.model_dim = model_dim\n        self.pos_encoding_scalar = tf.Variable(1.)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, model_dim)\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.CADB = [\n            CrossAttentionDenseBlock(model_dim=model_dim, dropout_rate=dropout_rate, num_heads=n_heads,\n                                     dense_hidden_units=feed_forward_dimension, name=f\'{self.name}_CADB_{i}\')\n            for i, n_heads in enumerate(num_heads[:dense_blocks])]\n        self.CACB = [\n            CrossAttentionConvBlock(model_dim=model_dim, dropout_rate=dropout_rate, num_heads=n_heads,\n                                    name=f\'{self.name}_CACB_{i}\', conv_filters=conv_filters,\n                                    conv_activation=conv_activation, conv_padding=conv_padding, kernel_size=conv_kernel)\n            for i, n_heads in enumerate(num_heads[dense_blocks:])]\n    \n    def call(self, inputs, enc_output, training, decoder_padding_mask, encoder_padding_mask, drop_n_heads,\n             reduction_factor=1):\n        seq_len = tf.shape(inputs)[1]\n        x = inputs * tf.math.sqrt(tf.cast(self.model_dim, tf.float32))\n        x += self.pos_encoding_scalar * self.pos_encoding[:, :seq_len * reduction_factor:reduction_factor, :]\n        x = self.dropout(x, training=training)\n        attention_weights = {}\n        for i, block in enumerate(self.CADB):\n            x, _, attn_weights = block(x, enc_output, training, decoder_padding_mask, encoder_padding_mask,\n                                       drop_n_heads)\n            attention_weights[f\'{self.name}_DenseBlock{i + 1}_CrossAttention\'] = attn_weights\n        for i, block in enumerate(self.CACB):\n            x, _, attn_weights = block(x, enc_output, training, decoder_padding_mask, encoder_padding_mask,\n                                       drop_n_heads)\n            attention_weights[f\'{self.name}_ConvBlock{i + 1}_CrossAttention\'] = attn_weights\n        \n        return x, attention_weights\n\n\nclass DecoderPrenet(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 model_dim: int,\n                 dense_hidden_units: int,\n                 dropout_rate: float,\n                 **kwargs):\n        super(DecoderPrenet, self).__init__(**kwargs)\n        self.d1 = tf.keras.layers.Dense(dense_hidden_units,\n                                        activation=\'relu\')  # (batch_size, seq_len, dense_hidden_units)\n        self.d2 = tf.keras.layers.Dense(model_dim, activation=\'relu\')  # (batch_size, seq_len, model_dim)\n        self.rate = tf.Variable(dropout_rate, trainable=False)\n        self.dropout_1 = tf.keras.layers.Dropout(self.rate)\n        self.dropout_2 = tf.keras.layers.Dropout(self.rate)\n    \n    def call(self, x):\n        self.dropout_1.rate = self.rate\n        self.dropout_2.rate = self.rate\n        x = self.d1(x)\n        # use dropout also in inference for positional encoding relevance\n        x = self.dropout_1(x, training=True)\n        x = self.d2(x)\n        x = self.dropout_2(x, training=True)\n        return x\n\n\nclass Postnet(tf.keras.layers.Layer):\n    \n    def __init__(self, mel_channels: int,\n                 conv_filters: int,\n                 conv_layers: int,\n                 kernel_size: int,\n                 **kwargs):\n        super(Postnet, self).__init__(**kwargs)\n        self.mel_channels = mel_channels\n        self.stop_linear = tf.keras.layers.Dense(3)\n        self.conv_blocks = CNNResNorm(out_size=mel_channels,\n                                      kernel_size=kernel_size,\n                                      padding=\'causal\',\n                                      inner_activation=\'tanh\',\n                                      last_activation=\'linear\',\n                                      hidden_size=conv_filters,\n                                      n_layers=conv_layers,\n                                      normalization=\'batch\')\n        self.add_layer = tf.keras.layers.Add()\n    \n    def call(self, x, training):\n        stop = self.stop_linear(x)\n        conv_out = self.conv_blocks(x, training=training)\n        return {\n            \'mel_linear\': x,\n            \'final_output\': conv_out,\n            \'stop_prob\': stop,\n        }\n\n\nclass DurationPredictor(tf.keras.layers.Layer):\n    def __init__(self,\n                 model_dim: int,\n                 kernel_size: int,\n                 conv_padding: str,\n                 conv_activation: str,\n                 conv_block_n: int,\n                 dense_activation: str,\n                 **kwargs):\n        super(DurationPredictor, self).__init__(**kwargs)\n        self.conv_blocks = CNNResNorm(out_size=model_dim,\n                                      kernel_size=kernel_size,\n                                      padding=conv_padding,\n                                      inner_activation=conv_activation,\n                                      last_activation=conv_activation,\n                                      hidden_size=model_dim,\n                                      n_layers=conv_block_n,\n                                      normalization=\'layer\')\n        self.linear = tf.keras.layers.Dense(1, activation=dense_activation,\n                                            bias_initializer=tf.keras.initializers.Constant(value=1))\n    \n    def call(self, x, training):\n        x = self.conv_blocks(x, training=training)\n        x = self.linear(x)\n        return x\n\n\nclass Expand(tf.keras.layers.Layer):\n    """""" Expands a 3D tensor on its second axis given a list of dimensions.\n        Tensor should be:\n            batch_size, seq_len, dimension\n        \n        E.g:\n        input = tf.Tensor([[[0.54710746 0.8943467 ]\n                          [0.7140938  0.97968304]\n                          [0.5347662  0.15213418]]], shape=(1, 3, 2), dtype=float32)\n        dimensions = tf.Tensor([1 3 2], shape=(3,), dtype=int32)\n        output = tf.Tensor([[[0.54710746 0.8943467 ]\n                           [0.7140938  0.97968304]\n                           [0.7140938  0.97968304]\n                           [0.7140938  0.97968304]\n                           [0.5347662  0.15213418]\n                           [0.5347662  0.15213418]]], shape=(1, 6, 2), dtype=float32)\n    """"""\n    \n    def __init__(self, model_dim, **kwargs):\n        super(Expand, self).__init__(**kwargs)\n        self.model_dimension = model_dim\n    \n    def call(self, x, dimensions):\n        dimensions = tf.squeeze(dimensions, axis=-1)\n        dimensions = tf.cast(tf.math.round(dimensions), tf.int32)\n        seq_len = tf.shape(x)[1]\n        batch_size = tf.shape(x)[0]\n        # build masks from dimensions\n        max_dim = tf.math.reduce_max(dimensions)\n        tot_dim = tf.math.reduce_sum(dimensions)\n        index_masks = tf.RaggedTensor.from_row_lengths(tf.ones(tot_dim), tf.reshape(dimensions, [-1])).to_tensor()\n        index_masks = tf.cast(tf.reshape(index_masks, (batch_size, seq_len * max_dim)), tf.float32)\n        non_zeros = seq_len * max_dim - tf.reduce_sum(max_dim - dimensions, axis=1)\n        # stack and mask\n        tiled = tf.tile(x, [1, 1, max_dim])\n        reshaped = tf.reshape(tiled, (batch_size, seq_len * max_dim, self.model_dimension))\n        mask_reshape = tf.multiply(reshaped, index_masks[:, :, tf.newaxis])\n        ragged = tf.RaggedTensor.from_row_lengths(mask_reshape[index_masks > 0], non_zeros)\n        return ragged.to_tensor()\n'"
model/models.py,46,"b""import sys\n\nimport tensorflow as tf\n\nfrom model.transformer_utils import create_encoder_padding_mask, create_mel_padding_mask, create_look_ahead_mask\nfrom utils.losses import weighted_sum_losses\nfrom model.layers import DecoderPrenet, Postnet\nfrom utils.losses import masked_mean_absolute_error, new_scaled_crossentropy\nfrom preprocessing.data_handling import Tokenizer\nfrom preprocessing.text_processing import _phonemes, Phonemizer, _punctuations\nfrom model.layers import DurationPredictor, Expand, SelfAttentionBlocks, CrossAttentionBlocks, CNNResNorm\n\n\nclass AutoregressiveTransformer(tf.keras.models.Model):\n    \n    def __init__(self,\n                 encoder_model_dimension: int,\n                 decoder_model_dimension: int,\n                 encoder_num_heads: list,\n                 decoder_num_heads: list,\n                 encoder_maximum_position_encoding: int,\n                 decoder_maximum_position_encoding: int,\n                 encoder_dense_blocks: int,\n                 decoder_dense_blocks: int,\n                 encoder_prenet_dimension: int,\n                 decoder_prenet_dimension: int,\n                 postnet_conv_filters: int,\n                 postnet_conv_layers: int,\n                 postnet_kernel_size: int,\n                 dropout_rate: float,\n                 mel_start_value: int,\n                 mel_end_value: int,\n                 mel_channels: int,\n                 phoneme_language: str,\n                 encoder_attention_conv_filters: int = None,\n                 decoder_attention_conv_filters: int = None,\n                 encoder_attention_conv_kernel: int = None,\n                 decoder_attention_conv_kernel: int = None,\n                 encoder_feed_forward_dimension: int = None,\n                 decoder_feed_forward_dimension: int = None,\n                 decoder_prenet_dropout=0.5,\n                 max_r: int = 10,\n                 debug=False,\n                 **kwargs):\n        super(AutoregressiveTransformer, self).__init__(**kwargs)\n        self.start_vec = tf.ones((1, mel_channels), dtype=tf.float32) * mel_start_value\n        self.end_vec = tf.ones((1, mel_channels), dtype=tf.float32) * mel_end_value\n        self.stop_prob_index = 2\n        self.max_r = max_r\n        self.r = max_r\n        self.mel_channels = mel_channels\n        self.drop_n_heads = 0\n        \n        self.tokenizer = Tokenizer(sorted(list(_phonemes) + list(_punctuations)), add_start_end=True)\n        self.phonemizer = Phonemizer(language=phoneme_language)\n        self.encoder_prenet = tf.keras.layers.Embedding(self.tokenizer.vocab_size, encoder_prenet_dimension,\n                                                        name='Embedding')\n        self.encoder = SelfAttentionBlocks(model_dim=encoder_model_dimension,\n                                           dropout_rate=dropout_rate,\n                                           num_heads=encoder_num_heads,\n                                           feed_forward_dimension=encoder_feed_forward_dimension,\n                                           maximum_position_encoding=encoder_maximum_position_encoding,\n                                           dense_blocks=encoder_dense_blocks,\n                                           conv_filters=encoder_attention_conv_filters,\n                                           kernel_size=encoder_attention_conv_kernel,\n                                           conv_activation='relu',\n                                           name='Encoder')\n        self.decoder_prenet = DecoderPrenet(model_dim=decoder_model_dimension,\n                                            dense_hidden_units=decoder_prenet_dimension,\n                                            dropout_rate=decoder_prenet_dropout,\n                                            name='DecoderPrenet')\n        self.decoder = CrossAttentionBlocks(model_dim=decoder_model_dimension,\n                                            dropout_rate=dropout_rate,\n                                            num_heads=decoder_num_heads,\n                                            feed_forward_dimension=decoder_feed_forward_dimension,\n                                            maximum_position_encoding=decoder_maximum_position_encoding,\n                                            dense_blocks=decoder_dense_blocks,\n                                            conv_filters=decoder_attention_conv_filters,\n                                            conv_kernel=decoder_attention_conv_kernel,\n                                            conv_activation='relu',\n                                            conv_padding='causal',\n                                            name='Decoder')\n        self.final_proj_mel = tf.keras.layers.Dense(self.mel_channels * self.max_r, name='FinalProj')\n        self.decoder_postnet = Postnet(mel_channels=mel_channels,\n                                       conv_filters=postnet_conv_filters,\n                                       conv_layers=postnet_conv_layers,\n                                       kernel_size=postnet_kernel_size,\n                                       name='Postnet')\n        \n        self.training_input_signature = [\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(None, None, mel_channels), dtype=tf.float32),\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n        ]\n        self.forward_input_signature = [\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(None, None, mel_channels), dtype=tf.float32),\n        ]\n        self.encoder_signature = [\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n        ]\n        self.decoder_signature = [\n            tf.TensorSpec(shape=(None, None, encoder_model_dimension), dtype=tf.float32),\n            tf.TensorSpec(shape=(None, None, mel_channels), dtype=tf.float32),\n            tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32),\n        ]\n        self.debug = debug\n        self._apply_all_signatures()\n    \n    @property\n    def step(self):\n        return int(self.optimizer.iterations)\n    \n    def _apply_signature(self, function, signature):\n        if self.debug:\n            return function\n        else:\n            return tf.function(input_signature=signature)(function)\n    \n    def _apply_all_signatures(self):\n        self.forward = self._apply_signature(self._forward, self.forward_input_signature)\n        self.train_step = self._apply_signature(self._train_step, self.training_input_signature)\n        self.val_step = self._apply_signature(self._val_step, self.training_input_signature)\n        self.forward_encoder = self._apply_signature(self._forward_encoder, self.encoder_signature)\n        self.forward_decoder = self._apply_signature(self._forward_decoder, self.decoder_signature)\n    \n    def _call_encoder(self, inputs, training):\n        padding_mask = create_encoder_padding_mask(inputs)\n        enc_input = self.encoder_prenet(inputs)\n        enc_output, attn_weights = self.encoder(enc_input,\n                                                training=training,\n                                                padding_mask=padding_mask,\n                                                drop_n_heads=self.drop_n_heads)\n        return enc_output, padding_mask, attn_weights\n    \n    def _call_decoder(self, encoder_output, targets, encoder_padding_mask, training):\n        dec_target_padding_mask = create_mel_padding_mask(targets)\n        look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n        dec_input = self.decoder_prenet(targets)\n        dec_output, attention_weights = self.decoder(inputs=dec_input,\n                                                     enc_output=encoder_output,\n                                                     training=training,\n                                                     decoder_padding_mask=combined_mask,\n                                                     encoder_padding_mask=encoder_padding_mask,\n                                                     drop_n_heads=self.drop_n_heads,\n                                                     reduction_factor=self.r)\n        out_proj = self.final_proj_mel(dec_output)[:, :, :self.r * self.mel_channels]\n        b = int(tf.shape(out_proj)[0])\n        t = int(tf.shape(out_proj)[1])\n        mel = tf.reshape(out_proj, (b, t * self.r, self.mel_channels))\n        model_output = self.decoder_postnet(mel, training=training)\n        model_output.update(\n            {'decoder_attention': attention_weights, 'decoder_output': dec_output, 'out_proj': out_proj})\n        return model_output\n    \n    def _forward(self, inp, output):\n        model_out = self.__call__(inputs=inp,\n                                  targets=output,\n                                  training=False)\n        return model_out\n    \n    def _forward_encoder(self, inputs):\n        return self._call_encoder(inputs, training=False)\n    \n    def _forward_decoder(self, encoder_output, targets, encoder_padding_mask):\n        return self._call_decoder(encoder_output, targets, encoder_padding_mask, training=False)\n    \n    def _gta_forward(self, inp, tar, stop_prob, training):\n        tar_inp = tar[:, :-1]\n        tar_real = tar[:, 1:]\n        tar_stop_prob = stop_prob[:, 1:]\n        \n        mel_len = int(tf.shape(tar_inp)[1])\n        tar_mel = tar_inp[:, 0::self.r, :]\n        \n        with tf.GradientTape() as tape:\n            model_out = self.__call__(inputs=inp,\n                                      targets=tar_mel,\n                                      training=training)\n            loss, loss_vals = weighted_sum_losses((tar_real,\n                                                   tar_stop_prob,\n                                                   tar_real),\n                                                  (model_out['final_output'][:, :mel_len, :],\n                                                   model_out['stop_prob'][:, :mel_len, :],\n                                                   model_out['mel_linear'][:, :mel_len, :]),\n                                                  self.loss,\n                                                  self.loss_weights)\n        model_out.update({'loss': loss})\n        model_out.update({'losses': {'output': loss_vals[0], 'stop_prob': loss_vals[1], 'mel_linear': loss_vals[2]}})\n        model_out.update({'reduced_target': tar_mel})\n        return model_out, tape\n    \n    def _train_step(self, inp, tar, stop_prob):\n        model_out, tape = self._gta_forward(inp, tar, stop_prob, training=True)\n        gradients = tape.gradient(model_out['loss'], self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        return model_out\n    \n    def _val_step(self, inp, tar, stop_prob):\n        model_out, _ = self._gta_forward(inp, tar, stop_prob, training=False)\n        return model_out\n    \n    def _compile(self, stop_scaling, optimizer):\n        self.loss_weights = [1., 1., 1.]\n        self.compile(loss=[masked_mean_absolute_error,\n                           new_scaled_crossentropy(index=2, scaling=stop_scaling),\n                           masked_mean_absolute_error],\n                     loss_weights=self.loss_weights,\n                     optimizer=optimizer)\n    \n    def _set_r(self, r):\n        if self.r == r:\n            return\n        self.r = r\n        self._apply_all_signatures()\n    \n    def _set_heads(self, heads):\n        if self.drop_n_heads == heads:\n            return\n        self.drop_n_heads = heads\n        self._apply_all_signatures()\n    \n    def call(self, inputs, targets, training):\n        encoder_output, padding_mask, encoder_attention = self._call_encoder(inputs, training)\n        model_out = self._call_decoder(encoder_output, targets, padding_mask, training)\n        model_out.update({'encoder_attention': encoder_attention})\n        return model_out\n    \n    def predict(self, inp, max_length=1000, encode=True, verbose=True):\n        if encode:\n            inp = self.encode_text(inp)\n        inp = tf.cast(tf.expand_dims(inp, 0), tf.int32)\n        output = tf.cast(tf.expand_dims(self.start_vec, 0), tf.float32)\n        output_concat = tf.cast(tf.expand_dims(self.start_vec, 0), tf.float32)\n        out_dict = {}\n        encoder_output, padding_mask, encoder_attention = self.forward_encoder(inp)\n        for i in range(int(max_length // self.r) + 1):\n            model_out = self.forward_decoder(encoder_output, output, padding_mask)\n            output = tf.concat([output, model_out['final_output'][:1, -1:, :]], axis=-2)\n            output_concat = tf.concat([tf.cast(output_concat, tf.float32), model_out['final_output'][:1, -self.r:, :]],\n                                      axis=-2)\n            stop_pred = model_out['stop_prob'][:, -1]\n            out_dict = {'mel': output_concat[0, 1:, :],\n                        'decoder_attention': model_out['decoder_attention'],\n                        'encoder_attention': encoder_attention}\n            if verbose:\n                sys.stdout.write(f'\\rpred text mel: {i} stop out: {float(stop_pred[0, 2])}')\n            if int(tf.argmax(stop_pred, axis=-1)) == self.stop_prob_index:\n                if verbose:\n                    print('Stopping')\n                break\n        return out_dict\n    \n    def set_constants(self, decoder_prenet_dropout: float = None, learning_rate: float = None,\n                      reduction_factor: float = None, drop_n_heads: int = None):\n        if decoder_prenet_dropout is not None:\n            self.decoder_prenet.rate.assign(decoder_prenet_dropout)\n        if learning_rate is not None:\n            self.optimizer.lr.assign(learning_rate)\n        if reduction_factor is not None:\n            self._set_r(reduction_factor)\n        if drop_n_heads is not None:\n            self._set_heads(drop_n_heads)\n    \n    def encode_text(self, text):\n        phons = self.phonemizer.encode(text, clean=True)\n        return self.tokenizer.encode(phons)\n\n\nclass ForwardTransformer(tf.keras.models.Model):\n    def __init__(self,\n                 encoder_model_dimension: int,\n                 decoder_model_dimension: int,\n                 dropout_rate: float,\n                 decoder_num_heads: list,\n                 encoder_num_heads: list,\n                 encoder_maximum_position_encoding: int,\n                 decoder_maximum_position_encoding: int,\n                 postnet_conv_filters: int,\n                 postnet_conv_layers: int,\n                 postnet_kernel_size: int,\n                 encoder_dense_blocks: int,\n                 decoder_dense_blocks: int,\n                 mel_channels: int,\n                 phoneme_language: str,\n                 encoder_attention_conv_filters: int = None,\n                 decoder_attention_conv_filters: int = None,\n                 encoder_attention_conv_kernel: int = None,\n                 decoder_attention_conv_kernel: int = None,\n                 encoder_feed_forward_dimension: int = None,\n                 decoder_feed_forward_dimension: int = None,\n                 debug=False,\n                 decoder_prenet_dropout=0.,\n                 **kwargs):\n        super(ForwardTransformer, self).__init__(**kwargs)\n        self.tokenizer = Tokenizer(sorted(list(_phonemes) + list(_punctuations)), add_start_end=False)\n        self.phonemizer = Phonemizer(language=phoneme_language)\n        self.drop_n_heads = 0\n        self.mel_channels = mel_channels\n        self.encoder_prenet = tf.keras.layers.Embedding(self.tokenizer.vocab_size, encoder_model_dimension,\n                                                        name='Embedding')\n        self.encoder = SelfAttentionBlocks(model_dim=encoder_model_dimension,\n                                           dropout_rate=dropout_rate,\n                                           num_heads=encoder_num_heads,\n                                           feed_forward_dimension=encoder_feed_forward_dimension,\n                                           maximum_position_encoding=encoder_maximum_position_encoding,\n                                           dense_blocks=encoder_dense_blocks,\n                                           conv_filters=encoder_attention_conv_filters,\n                                           kernel_size=encoder_attention_conv_kernel,\n                                           conv_activation='relu',\n                                           name='Encoder')\n        self.dur_pred = DurationPredictor(model_dim=encoder_model_dimension,\n                                          kernel_size=3,\n                                          conv_padding='same',\n                                          conv_activation='relu',\n                                          conv_block_n=2,\n                                          dense_activation='relu',\n                                          name='dur_pred')\n        self.expand = Expand(name='expand', model_dim=encoder_model_dimension)\n        self.decoder_prenet = DecoderPrenet(model_dim=decoder_model_dimension,\n                                            dense_hidden_units=decoder_feed_forward_dimension,\n                                            dropout_rate=decoder_prenet_dropout,\n                                            name='DecoderPrenet')\n        self.decoder = SelfAttentionBlocks(model_dim=decoder_model_dimension,\n                                           dropout_rate=dropout_rate,\n                                           num_heads=decoder_num_heads,\n                                           feed_forward_dimension=decoder_feed_forward_dimension,\n                                           maximum_position_encoding=decoder_maximum_position_encoding,\n                                           dense_blocks=decoder_dense_blocks,\n                                           conv_filters=decoder_attention_conv_filters,\n                                           kernel_size=decoder_attention_conv_kernel,\n                                           conv_activation='relu',\n                                           name='Decoder')\n        self.out = tf.keras.layers.Dense(mel_channels)\n        self.decoder_postnet = CNNResNorm(out_size=mel_channels,\n                                          kernel_size=postnet_kernel_size,\n                                          padding='same',\n                                          inner_activation='tanh',\n                                          last_activation='linear',\n                                          hidden_size=postnet_conv_filters,\n                                          n_layers=postnet_conv_layers,\n                                          normalization='batch',\n                                          name='Postnet')\n        self.training_input_signature = [\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(None, None, mel_channels), dtype=tf.float32),\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n        ]\n        self.forward_input_signature = [\n            tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(), dtype=tf.float32),\n        ]\n        self.debug = debug\n        self._apply_all_signatures()\n    \n    def _apply_signature(self, function, signature):\n        if self.debug:\n            return function\n        else:\n            return tf.function(input_signature=signature)(function)\n    \n    def _apply_all_signatures(self):\n        self.forward = self._apply_signature(self._forward, self.forward_input_signature)\n        self.train_step = self._apply_signature(self._train_step, self.training_input_signature)\n        self.val_step = self._apply_signature(self._val_step, self.training_input_signature)\n    \n    def _set_heads(self, heads):\n        if self.drop_n_heads == heads:\n            return\n        self.drop_n_heads = heads\n        self._apply_all_signatures()\n    \n    def _train_step(self, input_sequence, target_sequence, target_durations):\n        target_durations = tf.expand_dims(target_durations, -1)\n        mel_len = int(tf.shape(target_sequence)[1])\n        with tf.GradientTape() as tape:\n            model_out = self.__call__(input_sequence, target_durations, training=True)\n            loss, loss_vals = weighted_sum_losses((target_sequence,\n                                                   target_durations),\n                                                  (model_out['mel'][:, :mel_len, :],\n                                                   model_out['duration']),\n                                                  self.loss,\n                                                  self.loss_weights)\n        model_out.update({'loss': loss})\n        model_out.update({'losses': {'mel': loss_vals[0], 'duration': loss_vals[1]}})\n        gradients = tape.gradient(model_out['loss'], self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        return model_out\n    \n    def _compile(self, optimizer):\n        self.loss_weights = [3., 1.]\n        self.compile(loss=[masked_mean_absolute_error,\n                           masked_mean_absolute_error],\n                     loss_weights=self.loss_weights,\n                     optimizer=optimizer)\n    \n    def _val_step(self, input_sequence, target_sequence, target_durations):\n        target_durations = tf.expand_dims(target_durations, -1)\n        mel_len = int(tf.shape(target_sequence)[1])\n        model_out = self.__call__(input_sequence, target_durations, training=False)\n        loss, loss_vals = weighted_sum_losses((target_sequence,\n                                               target_durations),\n                                              (model_out['mel'][:, :mel_len, :],\n                                               model_out['duration']),\n                                              self.loss,\n                                              self.loss_weights)\n        model_out.update({'loss': loss})\n        model_out.update({'losses': {'mel': loss_vals[0], 'duration': loss_vals[1]}})\n        return model_out\n    \n    def _forward(self, input_sequence, durations_scalar):\n        return self.__call__(input_sequence, target_durations=None, training=False, durations_scalar=durations_scalar)\n    \n    @property\n    def step(self):\n        return int(self.optimizer.iterations)\n    \n    def call(self, x, target_durations, training, durations_scalar=1.):\n        padding_mask = create_encoder_padding_mask(x)\n        x = self.encoder_prenet(x)\n        x, encoder_attention = self.encoder(x, training=training, padding_mask=padding_mask,\n                                            drop_n_heads=self.drop_n_heads)\n        durations = self.dur_pred(x, training=training) * durations_scalar\n        durations = (1. - tf.reshape(padding_mask, tf.shape(durations))) * durations\n        if target_durations is not None:\n            mels = self.expand(x, target_durations)\n        else:\n            mels = self.expand(x, durations)\n        expanded_mask = create_mel_padding_mask(mels)\n        mels = self.decoder_prenet(mels)\n        mels, decoder_attention = self.decoder(mels, training=training, padding_mask=expanded_mask,\n                                               drop_n_heads=self.drop_n_heads, reduction_factor=1)\n        mels = self.out(mels)\n        mels = self.decoder_postnet(mels, training=training)\n        model_out = {'mel': mels,\n                     'duration': durations,\n                     'expanded_mask': expanded_mask,\n                     'encoder_attention': encoder_attention,\n                     'decoder_attention': decoder_attention}\n        return model_out\n    \n    def set_constants(self, decoder_prenet_dropout: float = None, learning_rate: float = None,\n                      drop_n_heads: int = None, **kwargs):\n        if decoder_prenet_dropout is not None:\n            self.decoder_prenet.rate.assign(decoder_prenet_dropout)\n        if learning_rate is not None:\n            self.optimizer.lr.assign(learning_rate)\n        if drop_n_heads is not None:\n            self._set_heads(drop_n_heads)\n    \n    def encode_text(self, text):\n        phons = self.phonemizer.encode(text, clean=True)\n        return self.tokenizer.encode(phons)\n    \n    def predict(self, inp, encode=True, speed_regulator=1.):\n        if encode:\n            inp = self.encode_text(inp)\n            inp = tf.cast(tf.expand_dims(inp, 0), tf.int32)\n        duration_scalar = tf.cast(1. / speed_regulator, tf.float32)\n        out = self.forward(inp, durations_scalar=duration_scalar)\n        out['mel'] = tf.squeeze(out['mel'])\n        return out\n"""
model/transformer_utils.py,12,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef get_angles(pos, i, model_dim):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(model_dim))\n    return pos * angle_rates\n\n\ndef positional_encoding(position, model_dim):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(model_dim)[np.newaxis, :], model_dim)\n    \n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    \n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    pos_encoding = angle_rads[np.newaxis, ...]\n    \n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    """""" Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  """"""\n    \n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n    \n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    \n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += mask * -1e9\n    \n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n    \n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n    \n    return output, attention_weights\n\n\ndef create_encoder_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, y, x)\n\n\ndef create_mel_padding_mask(seq):\n    seq = tf.reduce_sum(tf.math.abs(seq), axis=-1)\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, y, x)\n\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n'"
preprocessing/__init__.py,0,b''
preprocessing/data_handling.py,2,"b'import os\nfrom random import Random\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Dataset:\n    """""" Model digestible dataset. """"""\n    \n    def __init__(self,\n                 samples,\n                 preprocessor,\n                 batch_size,\n                 shuffle=True,\n                 drop_remainder=True,\n                 mel_channels=80,\n                 seed=42):\n        self._random = Random(seed)\n        self._samples = samples[:]\n        self.preprocessor = preprocessor\n        output_types = (tf.float32, tf.int32, tf.int32)\n        padded_shapes = ([-1, mel_channels], [-1], [-1])\n        dataset = tf.data.Dataset.from_generator(lambda: self._datagen(shuffle),\n                                                 output_types=output_types)\n        dataset = dataset.padded_batch(batch_size,\n                                       padded_shapes=padded_shapes,\n                                       drop_remainder=drop_remainder)\n        self.dataset = dataset\n        self.data_iter = iter(dataset.repeat(-1))\n    \n    def next_batch(self):\n        return next(self.data_iter)\n    \n    def all_batches(self):\n        return iter(self.dataset)\n    \n    def _datagen(self, shuffle):\n        """"""\n        Shuffle once before generating to avoid buffering\n        """"""\n        samples = self._samples[:]\n        if shuffle:\n            # print(f\'shuffling files\')\n            self._random.shuffle(samples)\n        return (self.preprocessor(s) for s in samples)\n\n\ndef load_files(metafile,\n               meldir,\n               num_samples=None):\n    samples = []\n    count = 0\n    alphabet = set()\n    with open(metafile, \'r\', encoding=\'utf-8\') as f:\n        for l in f.readlines():\n            l_split = l.split(\'|\')\n            mel_file = os.path.join(str(meldir), l_split[0] + \'.npy\')\n            text = l_split[1].strip().lower()\n            phonemes = l_split[2].strip()\n            samples.append((phonemes, text, mel_file))\n            alphabet.update(list(text))\n            count += 1\n            if num_samples is not None and count > num_samples:\n                break\n        alphabet = sorted(list(alphabet))\n        return samples, alphabet\n\n\nclass Tokenizer:\n    \n    def __init__(self, alphabet, start_token=\'>\', end_token=\'<\', pad_token=\'/\', add_start_end=True):\n        self.alphabet = alphabet\n        self.idx_to_token = {i: s for i, s in enumerate(self.alphabet, start=1)}\n        self.idx_to_token[0] = pad_token\n        self.token_to_idx = {s: i for i, s in self.idx_to_token.items()}\n        self.vocab_size = len(self.alphabet) + 1\n        self.add_start_end = add_start_end\n        if add_start_end:\n            self.start_token_index = len(self.alphabet) + 1\n            self.end_token_index = len(self.alphabet) + 2\n            self.vocab_size += 2\n            self.idx_to_token[self.start_token_index] = start_token\n            self.idx_to_token[self.end_token_index] = end_token\n    \n    def encode(self, sentence):\n        sequence = [self.token_to_idx[c] for c in sentence if c in self.token_to_idx]\n        if self.add_start_end:\n            sequence = [self.start_token_index] + sequence + [self.end_token_index]\n        return sequence\n    \n    def decode(self, sequence):\n        return \'\'.join([self.idx_to_token[int(t)] for t in sequence if int(t) in self.idx_to_token])\n\n\nclass DataPrepper:\n    \n    def __init__(self,\n                 config,\n                 tokenizer: Tokenizer):\n        self.start_vec = np.ones((1, config[\'mel_channels\'])) * config[\'mel_start_value\']\n        self.end_vec = np.ones((1, config[\'mel_channels\'])) * config[\'mel_end_value\']\n        self.tokenizer = tokenizer\n    \n    def __call__(self, sample):\n        phonemes, text, mel_path = sample\n        mel = np.load(mel_path)\n        return self._run(phonemes, text, mel)\n    \n    def _run(self, phonemes, text, mel):\n        encoded_phonemes = self.tokenizer.encode(phonemes)\n        norm_mel = np.concatenate([self.start_vec, mel, self.end_vec], axis=0)\n        stop_probs = np.ones((norm_mel.shape[0]))\n        stop_probs[-1] = 2\n        return norm_mel, encoded_phonemes, stop_probs\n\n\nclass ForwardDataPrepper:\n    \n    def __call__(self, sample):\n        mel, encoded_phonemes, durations = np.load(str(sample), allow_pickle=True)\n        return mel, encoded_phonemes, durations\n'"
preprocessing/text_processing.py,0,"b""import re\n\nfrom phonemizer.phonemize import phonemize\n\n_vowels = 'iy\xc9\xa8\xca\x89\xc9\xafu\xc9\xaa\xca\x8f\xca\x8ae\xc3\xb8\xc9\x98\xc9\x99\xc9\xb5\xc9\xa4o\xc9\x9b\xc5\x93\xc9\x9c\xc9\x9e\xca\x8c\xc9\x94\xc3\xa6\xc9\x90a\xc9\xb6\xc9\x91\xc9\x92\xe1\xb5\xbb'\n_non_pulmonic_consonants = '\xca\x98\xc9\x93\xc7\x80\xc9\x97\xc7\x83\xca\x84\xc7\x82\xc9\xa0\xc7\x81\xca\x9b'\n_pulmonic_consonants = 'pbtd\xca\x88\xc9\x96c\xc9\x9fk\xc9\xa1q\xc9\xa2\xca\x94\xc9\xb4\xc5\x8b\xc9\xb2\xc9\xb3n\xc9\xb1m\xca\x99r\xca\x80\xe2\xb1\xb1\xc9\xbe\xc9\xbd\xc9\xb8\xce\xb2fv\xce\xb8\xc3\xb0sz\xca\x83\xca\x92\xca\x82\xca\x90\xc3\xa7\xca\x9dx\xc9\xa3\xcf\x87\xca\x81\xc4\xa7\xca\x95h\xc9\xa6\xc9\xac\xc9\xae\xca\x8b\xc9\xb9\xc9\xbbj\xc9\xb0l\xc9\xad\xca\x8e\xca\x9f'\n_suprasegmentals = '\xcb\x88\xcb\x8c\xcb\x90\xcb\x91'\n_other_symbols = '\xca\x8dw\xc9\xa5\xca\x9c\xca\xa2\xca\xa1\xc9\x95\xca\x91\xc9\xba\xc9\xa7'\n_diacrilics = '\xc9\x9a\xcb\x9e\xc9\xab'\n_phonemes = sorted(list(\n    _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics))\n_punctuations = '!,-.:;? '\n_alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\xc3\xa4\xc3\xbc\xc3\xb6\xc3\x9f'\n_not_end_punctuation = ',-.:; '\n_ad_hoc_replace = {\n    'Mrs.': 'Mrs',\n    'Mr.': 'Mr',\n    'Dr.': 'Dr',\n    'St.': 'St',\n    'Co.': 'Co',\n    'Jr.': 'Jr',\n    'Maj.': 'Maj',\n    'Gen.': 'Gen',\n    'Drs.': 'Drs',\n    'Rev.': 'Rev',\n    'Lt.': 'Lt',\n    'Hon.': 'Hon',\n    'Sgt.': 'Sgt',\n    'Capt.': 'Capt',\n    'Esq.': 'Esq',\n    'Ltd.': 'Ltd',\n    'Col.': 'Col',\n    'Ft.': 'Ft',\n    'a.m.': 'a m',\n    'p.m.': 'p m',\n    'e.g.': 'e g',\n    'i.e.': 'i e',\n    ';': ',',\n    ':': ','}\n_ad_hoc_pattern = '|'.join(sorted(re.escape(k) for k in _ad_hoc_replace))\n\n\nclass TextCleaner:\n    def __init__(self, alphabet=None):\n        if not alphabet:\n            self.accepted_chars = list(_alphabet) + list(_punctuations)\n    \n    def clean(self, text):\n        if type(text) is list:\n            return [self.clean_line(t) for t in text]\n        elif type(text) is str:\n            return self.clean_line(text)\n        else:\n            raise TypeError(f'TextCleaner.clean() input must be list or str, not {type(text)}')\n    \n    def clean_line(self, text):\n        text = ''.join([c for c in text if c in self.accepted_chars])\n        text = re.sub(_ad_hoc_pattern, lambda m: _ad_hoc_replace.get(m.group(0)), text)\n        if text.endswith(tuple(_not_end_punctuation)):\n            text = text[:-1]\n        return text + ' '\n\n\nclass Phonemizer:\n    def __init__(self, language, alphabet=None):\n        self.language = language\n        self.cleaner = TextCleaner(alphabet)\n    \n    def encode(self, text, strip=True, preserve_punctuation=True, with_stress=False, njobs=4, clean=True):\n        if clean:\n            text = self.cleaner.clean(text)\n        phonemes = phonemize(text,\n                             language=self.language,\n                             backend='espeak',\n                             strip=strip,\n                             preserve_punctuation=preserve_punctuation,\n                             with_stress=with_stress,\n                             njobs=njobs,\n                             language_switch='remove-flags')\n        return phonemes\n"""
tests/__init__.py,0,b''
tests/test_char_tokenizer.py,1,"b""import unittest\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom preprocessing.data_handling import Tokenizer\n\n\nclass TestCharTokenizer(unittest.TestCase):\n    \n    def test_tokenizer(self):\n        tokenizer = Tokenizer(alphabet=list('ab c'))\n        self.assertEqual(5, tokenizer.start_token_index)\n        self.assertEqual(6, tokenizer.end_token_index)\n        self.assertEqual(7, tokenizer.vocab_size)\n        \n        seq = tokenizer.encode('a b d')\n        self.assertEqual([5, 1, 3, 2, 3, 6], seq)\n        \n        seq = np.array([5, 1, 3, 2, 8, 6])\n        seq = tf.convert_to_tensor(seq)\n        text = tokenizer.decode(seq)\n        self.assertEqual('>a b<', text)\n"""
tests/test_loss.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom utils.losses import new_scaled_crossentropy, masked_crossentropy\n\n\nclass TestCharTokenizer(unittest.TestCase):\n    \n    def test_crossentropy(self):\n        scaled_crossent = new_scaled_crossentropy(index=2, scaling=5)\n        \n        targets = np.array([[0, 1, 2]])\n        logits = np.array([[[.3, .2, .1], [.3, .2, .1], [.3, .2, .1]]])\n        \n        loss = scaled_crossent(targets, logits)\n        self.assertAlmostEqual(2.3705523014068604, float(loss))\n        \n        scaled_crossent = new_scaled_crossentropy(index=2, scaling=1)\n        loss = scaled_crossent(targets, logits)\n        self.assertAlmostEqual(0.7679619193077087, float(loss))\n        \n        loss = masked_crossentropy(targets, logits)\n        self.assertAlmostEqual(0.7679619193077087, float(loss))\n'"
utils/__init__.py,0,b''
utils/alignments.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom model.transformer_utils import create_mel_padding_mask, create_encoder_padding_mask\n\nlogger = tf.get_logger()\nlogger.setLevel(\'ERROR\')\n\n\ndef duration_to_alignment_matrix(durations):\n    starts = np.cumsum(np.append([0], durations[:-1]))\n    tot_duration = np.sum(durations)\n    pads = tot_duration - starts - durations\n    alignments = [np.concatenate([np.zeros(starts[i]), np.ones(durations[i]), np.zeros(pads[i])]) for i in\n                  range(len(durations))]\n    return np.array(alignments)\n\n\ndef clean_attention(binary_attention, jump_threshold):\n    phon_idx = 0\n    clean_attn = np.zeros(binary_attention.shape)\n    for i, av in enumerate(binary_attention):\n        next_phon_idx = np.argmax(av)\n        if abs(next_phon_idx - phon_idx) > jump_threshold:\n            next_phon_idx = phon_idx\n        phon_idx = next_phon_idx\n        clean_attn[i, min(phon_idx, clean_attn.shape[1] - 1)] = 1\n    return clean_attn\n\n\ndef weight_mask(attention_weights):\n    """""" Exponential loss mask based on distance from approximate diagonal""""""\n    max_m, max_n = attention_weights.shape\n    I = np.tile(np.arange(max_n), (max_m, 1)) / max_n\n    J = np.swapaxes(np.tile(np.arange(max_m), (max_n, 1)), 0, 1) / max_m\n    return np.sqrt(np.square(I - J))\n\n\ndef fill_zeros(duration, take_from=\'next\'):\n    """""" Fills zeros with one. Takes either from the next non-zero duration, or max.""""""\n    for i in range(len(duration)):\n        if i < (len(duration) - 1):\n            if duration[i] == 0:\n                if take_from == \'next\':\n                    next_avail = np.where(duration[i:] > 1)[0]\n                    if len(next_avail) > 1:\n                        next_avail = next_avail[0]\n                elif take_from == \'max\':\n                    next_avail = np.argmax(duration[i:])\n                if next_avail:\n                    duration[i] = 1\n                    duration[i + next_avail] -= 1\n    return duration\n\n\ndef fix_attention_jumps(binary_attn, alignments_weights, binary_score):\n    """""" Scans for jumps in attention and attempts to fix. If score decreases, a collapse\n        is likely so it tries to relax the jump size.\n        Lower jumps size is more accurate, but more prone to collapse.\n    """"""\n    clean_scores = []\n    clean_attns = []\n    for jumpth in [2, 3, 4, 5]:\n        cl_at = clean_attention(binary_attention=binary_attn, jump_threshold=jumpth)\n        clean_attns.append(cl_at)\n        sclean_score = np.sum(alignments_weights * cl_at)\n        clean_scores.append(sclean_score)\n    best_idx = np.argmin(clean_scores)\n    best_score = clean_scores[best_idx]\n    best_cleaned_attention = clean_attns[best_idx]\n    while ((best_score - binary_score) > 2.) and (jumpth < 20):\n        jumpth += 1\n        best_cleaned_attention = clean_attention(binary_attention=binary_attn, jump_threshold=jumpth)\n        best_score = np.sum(alignments_weights * best_cleaned_attention)\n    return best_cleaned_attention\n\n\ndef binary_attention(attention_weights):\n    attention_peak_per_phoneme = attention_weights.max(axis=1)\n    binary_attn = (attention_weights.T == attention_peak_per_phoneme).astype(int).T\n    assert np.sum(\n        np.sum(attention_weights.T == attention_peak_per_phoneme, axis=0) != 1) == 0  # single peak per mel step\n    binary_score = np.sum(attention_weights * binary_attn)\n    return binary_attn, binary_score\n\n\ndef get_durations_from_alignment(batch_alignments, mels, phonemes, weighted=False, binary=False, fill_gaps=False,\n                                 fix_jumps=False, fill_mode=\'max\'):\n    """"""\n    \n    :param batch_alignments: attention weights from autoregressive model.\n    :param mels: mel spectrograms.\n    :param phonemes: phoneme sequence.\n    :param weighted: if True use weighted average of durations of heads, best head if False.\n    :param binary: if True take maximum attention peak, sum if False.\n    :param fill_gaps: if True fills zeros durations with ones.\n    :param fix_jumps: if True, tries to scan alingments for attention jumps and interpolate.\n    :param fill_mode: used only if fill_gaps is True. Is either \'max\' or \'next\'. Defines where to take the duration\n        needed to fill the gap. Next takes it from the next non-zeros duration value, max from the sequence maximum.\n    :return:\n    """"""\n    assert (binary is True) or (fix_jumps is False), \'Cannot fix jumps in non-binary attention.\'\n    mel_pad_mask = create_mel_padding_mask(mels)\n    phon_pad_mask = create_encoder_padding_mask(phonemes)\n    durations = []\n    # remove start end token or vector\n    unpad_mels = []\n    unpad_phonemes = []\n    final_alignment = []\n    for i, al in enumerate(batch_alignments):\n        mel_len = int(mel_pad_mask[i].shape[-1] - np.sum(mel_pad_mask[i]))\n        phon_len = int(phon_pad_mask[i].shape[-1] - np.sum(phon_pad_mask[i]))\n        unpad_alignments = al[:, 1:mel_len - 1, 1:phon_len - 1]  # first dim is heads\n        unpad_mels.append(mels[i, 1:mel_len - 1, :])\n        unpad_phonemes.append(phonemes[i, 1:phon_len - 1])\n        alignments_weights = weight_mask(unpad_alignments[0])\n        heads_scores = []\n        scored_attention = []\n        for _, attention_weights in enumerate(unpad_alignments):\n            score = np.sum(alignments_weights * attention_weights)\n            scored_attention.append(attention_weights / score)\n            heads_scores.append(score)\n        \n        if weighted:\n            ref_attention_weights = np.sum(scored_attention, axis=0)\n        else:\n            best_head = np.argmin(heads_scores)\n            ref_attention_weights = unpad_alignments[best_head]\n        \n        if binary:  # pick max attention for each mel time-step\n            binary_attn, binary_score = binary_attention(ref_attention_weights)\n            if fix_jumps:\n                binary_attn = fix_attention_jumps(\n                    binary_attn=binary_attn,\n                    alignments_weights=alignments_weights,\n                    binary_score=binary_score)\n            integer_durations = binary_attn.sum(axis=0)\n        \n        else:  # takes actual attention values and normalizes to mel_len\n            attention_durations = np.sum(ref_attention_weights, axis=0)\n            normalized_durations = attention_durations * ((mel_len - 2) / np.sum(attention_durations))\n            integer_durations = np.round(normalized_durations)\n            tot_duration = np.sum(integer_durations)\n            duration_diff = tot_duration - (mel_len - 2)\n            while duration_diff != 0:\n                rounding_diff = integer_durations - normalized_durations\n                if duration_diff > 0:  # duration is too long -> reduce highest (positive) rounding difference\n                    max_error_idx = np.argmax(rounding_diff)\n                    integer_durations[max_error_idx] -= 1\n                elif duration_diff < 0:  # duration is too short -> increase lowest (negative) rounding difference\n                    min_error_idx = np.argmin(rounding_diff)\n                    integer_durations[min_error_idx] += 1\n                tot_duration = np.sum(integer_durations)\n                duration_diff = tot_duration - (mel_len - 2)\n        \n        if fill_gaps:  # fill zeros durations\n            integer_durations = fill_zeros(integer_durations, take_from=fill_mode)\n        \n        assert np.sum(integer_durations) == mel_len - 2, f\'{np.sum(integer_durations)} vs {mel_len - 2}\'\n        new_alignment = duration_to_alignment_matrix(integer_durations.astype(int))\n        best_head = np.argmin(heads_scores)\n        best_attention = unpad_alignments[best_head]\n        final_alignment.append(best_attention.T + new_alignment)\n        durations.append(integer_durations)\n    return durations, unpad_mels, unpad_phonemes, final_alignment\n'"
utils/audio.py,0,"b'import sys\n\nimport librosa\nimport numpy as np\nimport librosa.display\nfrom matplotlib import pyplot as plt\n\n\nclass Audio():\n    def __init__(self, config: dict):\n        self.config = config\n        self.normalizer = getattr(sys.modules[__name__], config[\'normalizer\'])(config)\n    \n    def _normalize(self, S):\n        return self.normalizer.normalize(S)\n    \n    def _denormalize(self, S):\n        return self.normalizer.denormalize(S)\n    \n    def _linear_to_mel(self, spectrogram):\n        return librosa.feature.melspectrogram(\n            S=spectrogram,\n            sr=self.config[\'sampling_rate\'],\n            n_fft=self.config[\'n_fft\'],\n            n_mels=self.config[\'mel_channels\'],\n            fmin=self.config[\'f_min\'],\n            fmax=self.config[\'f_max\'])\n    \n    def _stft(self, y):\n        return librosa.stft(\n            y=y,\n            n_fft=self.config[\'n_fft\'],\n            hop_length=self.config[\'hop_length\'],\n            win_length=self.config[\'win_length\'])\n    \n    def mel_spectrogram(self, wav):\n        """""" This is what the model is trained to reproduce. """"""\n        D = self._stft(wav)\n        S = self._linear_to_mel(np.abs(D))\n        return self._normalize(S)\n    \n    def reconstruct_waveform(self, mel, n_iter=32):\n        """""" Uses Griffin-Lim phase reconstruction to convert from a normalized\n        mel spectrogram back into a waveform. """"""\n        amp_mel = self._denormalize(mel)\n        S = librosa.feature.inverse.mel_to_stft(\n            amp_mel,\n            power=1,\n            sr=self.config[\'sampling_rate\'],\n            n_fft=self.config[\'n_fft\'],\n            fmin=self.config[\'f_min\'],\n            fmax=self.config[\'f_max\'])\n        wav = librosa.core.griffinlim(\n            S,\n            n_iter=n_iter,\n            hop_length=self.config[\'hop_length\'],\n            win_length=self.config[\'win_length\'])\n        return wav\n    \n    def display_mel(self, mel, is_normal=True):\n        if is_normal:\n            mel = self._denormalize(mel)\n        f = plt.figure(figsize=(10, 4))\n        s_db = librosa.power_to_db(mel, ref=np.max)\n        ax = librosa.display.specshow(s_db,\n                                      x_axis=\'time\',\n                                      y_axis=\'mel\',\n                                      sr=self.config[\'sampling_rate\'],\n                                      fmin=self.config[\'f_min\'],\n                                      fmax=self.config[\'f_max\'])\n        f.add_subplot(ax)\n        return f\n\n\nclass Normalizer:\n    def __init__(self, config: dict):\n        self.config = config\n    \n    def normalize(self):\n        raise NotImplementedError\n    \n    def denormalize(self):\n        raise NotImplementedError\n\n\nclass MelGAN(Normalizer):\n    def __init__(self, config):\n        super().__init__(config)\n        self.clip_min = 1.0e-5\n    \n    def normalize(self, S):\n        S = np.clip(S, a_min=self.clip_min, a_max=None)\n        return np.log(S)\n    \n    def denormalize(self, S):\n        return np.exp(S)\n\n\nclass WaveRNN(Normalizer):\n    def __init__(self, config: dict):\n        super().__init__(config)\n        self.min_level_db = - 100\n        self.max_norm = 4\n    \n    def normalize(self, S):\n        S = self.amp_to_db(S)\n        S = np.clip((S - self.min_level_db) / -self.min_level_db, 0, 1)\n        return (S * 2 * self.max_norm) - self.max_norm\n    \n    def denormalize(self, S):\n        S = (S + self.max_norm) / (2 * self.max_norm)\n        S = (np.clip(S, 0, 1) * -self.min_level_db) + self.min_level_db\n        return self.db_to_amp(S)\n    \n    def amp_to_db(self, x):\n        return 20 * np.log10(np.maximum(1e-5, x))\n    \n    def db_to_amp(self, x):\n        return np.power(10.0, x * 0.05)\n'"
utils/config_manager.py,3,"b'import subprocess\nimport shutil\nfrom pathlib import Path\n\nimport numpy as np\nimport tensorflow as tf\nimport ruamel.yaml\n\nfrom model.models import AutoregressiveTransformer, ForwardTransformer\nfrom utils.scheduling import piecewise_linear_schedule, reduction_schedule\n\n\nclass ConfigManager:\n    \n    def __init__(self, config_path: str, model_kind: str, session_name: str = None):\n        if model_kind not in [\'autoregressive\', \'forward\']:\n            raise TypeError(f""model_kind must be in {[\'autoregressive\', \'forward\']}"")\n        self.config_path = Path(config_path)\n        self.model_kind = model_kind\n        self.yaml = ruamel.yaml.YAML()\n        self.config, self.data_config, self.model_config = self._load_config()\n        self.git_hash = self._get_git_hash()\n        if session_name is None:\n            if self.config[\'session_name\'] is None:\n                session_name = self.git_hash\n        self.session_name = \'_\'.join(filter(None, [self.config_path.name, session_name]))\n        self.base_dir, self.log_dir, self.train_datadir, self.weights_dir = self._make_folder_paths()\n        self.learning_rate = np.array(self.config[\'learning_rate_schedule\'])[0, 1].astype(np.float32)\n        if model_kind == \'autoregressive\':\n            self.max_r = np.array(self.config[\'reduction_factor_schedule\'])[0, 1].astype(np.int32)\n            self.stop_scaling = self.config.get(\'stop_loss_scaling\', 1.)\n    \n    def _load_config(self):\n        with open(str(self.config_path / \'data_config.yaml\'), \'rb\') as data_yaml:\n            data_config = self.yaml.load(data_yaml)\n        with open(str(self.config_path / f\'{self.model_kind}_config.yaml\'), \'rb\') as model_yaml:\n            model_config = self.yaml.load(model_yaml)\n        all_config = {}\n        all_config.update(model_config)\n        all_config.update(data_config)\n        return all_config, data_config, model_config\n    \n    @staticmethod\n    def _get_git_hash():\n        try:\n            return subprocess.check_output([""git"", ""describe"", ""--always""]).strip().decode()\n        except Exception as e:\n            print(f""WARNING: could not retrieve git hash. {e}"")\n    \n    def _check_hash(self):\n        try:\n            git_hash = subprocess.check_output([""git"", ""describe"", ""--always""]).strip().decode()\n            if self.config[\'git_hash\'] != git_hash:\n                print(f""WARNING: git hash mismatch. Current: {git_hash}. Config hash: {self.config[\'git_hash\']}"")\n        except Exception as e:\n            print(f""WARNING: could not check git hash. {e}"")\n    \n    def _make_folder_paths(self):\n        base_dir = Path(self.config[\'log_directory\']) / self.session_name\n        log_dir = base_dir / f\'{self.model_kind}_logs\'\n        weights_dir = base_dir / f\'{self.model_kind}_weights\'\n        train_datadir = self.config[\'train_data_directory\']\n        if train_datadir is None:\n            train_datadir = self.config[\'data_directory\']\n        train_datadir = Path(train_datadir)\n        return base_dir, log_dir, train_datadir, weights_dir\n    \n    @staticmethod\n    def _print_dict_values(values, key_name, level=0, tab_size=2):\n        tab = level * tab_size * \' \'\n        print(tab + \'-\', key_name, \':\', values)\n    \n    def _print_dictionary(self, dictionary, recursion_level=0):\n        for key in dictionary.keys():\n            if isinstance(key, dict):\n                recursion_level += 1\n                self._print_dictionary(dictionary[key], recursion_level)\n            else:\n                self._print_dict_values(dictionary[key], key_name=key, level=recursion_level)\n    \n    def print_config(self):\n        print(\'\\nCONFIGURATION\', self.session_name)\n        self._print_dictionary(self.config)\n    \n    def update_config(self):\n        self.config[\'git_hash\'] = self.git_hash\n        self.model_config[\'git_hash\'] = self.git_hash\n        self.data_config[\'session_name\'] = self.session_name\n        self.model_config[\'session_name\'] = self.session_name\n        self.config[\'session_name\'] = self.session_name\n    \n    def get_model(self, ignore_hash=False):\n        if not ignore_hash:\n            self._check_hash()\n        if self.model_kind == \'autoregressive\':\n            return AutoregressiveTransformer(mel_channels=self.config[\'mel_channels\'],\n                                             encoder_model_dimension=self.config[\'encoder_model_dimension\'],\n                                             decoder_model_dimension=self.config[\'decoder_model_dimension\'],\n                                             encoder_num_heads=self.config[\'encoder_num_heads\'],\n                                             decoder_num_heads=self.config[\'decoder_num_heads\'],\n                                             encoder_feed_forward_dimension=self.config[\n                                                 \'encoder_feed_forward_dimension\'],\n                                             decoder_feed_forward_dimension=self.config[\n                                                 \'decoder_feed_forward_dimension\'],\n                                             encoder_maximum_position_encoding=self.config[\n                                                 \'encoder_max_position_encoding\'],\n                                             decoder_maximum_position_encoding=self.config[\n                                                 \'decoder_max_position_encoding\'],\n                                             encoder_dense_blocks=self.config[\'encoder_dense_blocks\'],\n                                             decoder_dense_blocks=self.config[\'decoder_dense_blocks\'],\n                                             decoder_prenet_dimension=self.config[\'decoder_prenet_dimension\'],\n                                             encoder_prenet_dimension=self.config[\'encoder_prenet_dimension\'],\n                                             postnet_conv_filters=self.config[\'postnet_conv_filters\'],\n                                             postnet_conv_layers=self.config[\'postnet_conv_layers\'],\n                                             postnet_kernel_size=self.config[\'postnet_kernel_size\'],\n                                             dropout_rate=self.config[\'dropout_rate\'],\n                                             max_r=self.max_r,\n                                             mel_start_value=self.config[\'mel_start_value\'],\n                                             mel_end_value=self.config[\'mel_end_value\'],\n                                             phoneme_language=self.config[\'phoneme_language\'],\n                                             debug=self.config[\'debug\'])\n        \n        else:\n            return ForwardTransformer(encoder_model_dimension=self.config[\'encoder_model_dimension\'],\n                                      decoder_model_dimension=self.config[\'decoder_model_dimension\'],\n                                      dropout_rate=self.config[\'dropout_rate\'],\n                                      decoder_num_heads=self.config[\'decoder_num_heads\'],\n                                      encoder_num_heads=self.config[\'encoder_num_heads\'],\n                                      encoder_maximum_position_encoding=self.config[\'encoder_max_position_encoding\'],\n                                      decoder_maximum_position_encoding=self.config[\'decoder_max_position_encoding\'],\n                                      encoder_feed_forward_dimension=self.config[\'encoder_feed_forward_dimension\'],\n                                      decoder_feed_forward_dimension=self.config[\'decoder_feed_forward_dimension\'],\n                                      encoder_attention_conv_filters=self.config[\n                                          \'encoder_attention_conv_filters\'],\n                                      decoder_attention_conv_filters=self.config[\n                                          \'decoder_attention_conv_filters\'],\n                                      encoder_attention_conv_kernel=self.config[\'encoder_attention_conv_kernel\'],\n                                      decoder_attention_conv_kernel=self.config[\'decoder_attention_conv_kernel\'],\n                                      mel_channels=self.config[\'mel_channels\'],\n                                      postnet_conv_filters=self.config[\'postnet_conv_filters\'],\n                                      postnet_conv_layers=self.config[\'postnet_conv_layers\'],\n                                      postnet_kernel_size=self.config[\'postnet_kernel_size\'],\n                                      encoder_dense_blocks=self.config[\'encoder_dense_blocks\'],\n                                      decoder_dense_blocks=self.config[\'decoder_dense_blocks\'],\n                                      phoneme_language=self.config[\'phoneme_language\'],\n                                      debug=self.config[\'debug\'])\n    \n    def compile_model(self, model):\n        if self.model_kind == \'autoregressive\':\n            model._compile(stop_scaling=self.stop_scaling, optimizer=self.new_adam(self.learning_rate))\n        else:\n            model._compile(optimizer=self.new_adam(self.learning_rate))\n    \n    # TODO: move to model\n    @staticmethod\n    def new_adam(learning_rate):\n        return tf.keras.optimizers.Adam(learning_rate,\n                                        beta_1=0.9,\n                                        beta_2=0.98,\n                                        epsilon=1e-9)\n    \n    def dump_config(self):\n        self.update_config()\n        with open(self.base_dir / f\'{self.model_kind}_config.yaml\', \'w\') as model_yaml:\n            self.yaml.dump(self.model_config, model_yaml)\n        with open(self.base_dir / \'data_config.yaml\', \'w\') as data_yaml:\n            self.yaml.dump(self.data_config, data_yaml)\n    \n    def create_remove_dirs(self, clear_dir: False, clear_logs: False, clear_weights: False):\n        self.base_dir.mkdir(exist_ok=True)\n        if clear_dir:\n            delete = input(f\'Delete {self.log_dir} AND {self.weights_dir}? (y/[n])\')\n            if delete == \'y\':\n                shutil.rmtree(self.log_dir, ignore_errors=True)\n                shutil.rmtree(self.weights_dir, ignore_errors=True)\n        if clear_logs:\n            delete = input(f\'Delete {self.log_dir}? (y/[n])\')\n            if delete == \'y\':\n                shutil.rmtree(self.log_dir, ignore_errors=True)\n        if clear_weights:\n            delete = input(f\'Delete {self.weights_dir}? (y/[n])\')\n            if delete == \'y\':\n                shutil.rmtree(self.weights_dir, ignore_errors=True)\n        self.log_dir.mkdir(exist_ok=True)\n        self.weights_dir.mkdir(exist_ok=True)\n    \n    def load_model(self, checkpoint_path: str = None, verbose=True):\n        model = self.get_model()\n        self.compile_model(model)\n        ckpt = tf.train.Checkpoint(net=model)\n        manager = tf.train.CheckpointManager(ckpt, self.weights_dir,\n                                             max_to_keep=None)\n        if checkpoint_path:\n            ckpt.restore(checkpoint_path)\n            if verbose:\n                print(f\'restored weights from {checkpoint_path} at step {model.step}\')\n        else:\n            if manager.latest_checkpoint is None:\n                print(f\'WARNING: could not find weights file. Trying to load from \\n {self.weights_dir}.\')\n                print(\'Edit data_config.yaml to point at the right log directory.\')\n            ckpt.restore(manager.latest_checkpoint)\n            if verbose:\n                print(f\'restored weights from {manager.latest_checkpoint} at step {model.step}\')\n        decoder_prenet_dropout = piecewise_linear_schedule(model.step, self.config[\'decoder_prenet_dropout_schedule\'])\n        reduction_factor = None\n        if self.model_kind == \'autoregressive\':\n            reduction_factor = reduction_schedule(model.step, self.config[\'reduction_factor_schedule\'])\n        model.set_constants(reduction_factor=reduction_factor, decoder_prenet_dropout=decoder_prenet_dropout)\n        return model\n'"
utils/decorators.py,0,"b""import traceback\nfrom time import time\n\n\ndef ignore_exception(f):\n    def apply_func(*args, **kwargs):\n        try:\n            result = f(*args, **kwargs)\n            return result\n        except Exception:\n            print(f'Catched exception in {f}:')\n            traceback.print_exc()\n            return None\n    \n    return apply_func\n\n\ndef time_it(f):\n    def apply_func(*args, **kwargs):\n        t_start = time()\n        result = f(*args, **kwargs)\n        t_end = time()\n        dur = round(t_end - t_start, ndigits=2)\n        return result, dur\n    \n    return apply_func\n"""
utils/display.py,0,"b""import io\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef buffer_image(figure):\n    buf = io.BytesIO()\n    figure.savefig(buf, format='png')\n    buf.seek(0)\n    plt.close('all')\n    return buf\n\n\ndef tight_grid(images):\n    images = np.array(images)\n    images = np.pad(images, [[0, 0], [1, 1], [1, 1]], 'constant', constant_values=1)  # add borders\n    if len(images.shape) != 3:\n        raise Exception\n    else:\n        n, y, x = images.shape\n    ratio = y / x\n    if ratio > 1:\n        ny = max(int(np.sqrt(n / ratio)), 1)\n        nx = int(n / ny)\n        nx += n - (nx * ny)\n        extra = nx * ny - n\n    else:\n        nx = max(int(np.sqrt(n * ratio)), 1)\n        ny = int(n / nx)\n        ny += n - (nx * ny)\n        extra = nx * ny - n\n    tot = np.append(images, np.zeros((extra, y, x)), axis=0)\n    img = np.block([[*tot[i * nx:(i + 1) * nx]] for i in range(ny)])\n    return img\n"""
utils/logging.py,14,"b'from pathlib import Path\n\nimport tensorflow as tf\n\nfrom utils.audio import Audio\nfrom utils.display import tight_grid, buffer_image\nfrom utils.vec_ops import norm_tensor\nfrom utils.decorators import ignore_exception\n\n\ndef control_frequency(f):\n    def apply_func(*args, **kwargs):\n        # args[0] is self\n        plot_all = (\'plot_all\' in kwargs) and kwargs[\'plot_all\']\n        if (args[0].global_step % args[0].plot_frequency == 0) or plot_all:\n            result = f(*args, **kwargs)\n            return result\n        else:\n            return None\n    \n    return apply_func\n\n\nclass SummaryManager:\n    """""" Writes tensorboard logs during training.\n    \n        :arg model: model object that is trained\n        :arg log_dir: base directory where logs of a config are created\n        :arg config: configuration dictionary\n        :arg max_plot_frequency: every how many steps to plot\n    """"""\n    \n    def __init__(self,\n                 model: tf.keras.models.Model,\n                 log_dir: str,\n                 config: dict,\n                 max_plot_frequency=10,\n                 default_writer=\'log_dir\'):\n        self.model = model\n        self.log_dir = Path(log_dir)\n        self.config = config\n        self.audio = Audio(config)\n        self.plot_frequency = max_plot_frequency\n        self.default_writer = default_writer\n        self.writers = {}\n        self.add_writer(tag=default_writer, path=self.log_dir, default=True)\n    \n    def add_writer(self, path, tag=None, default=False):\n        """""" Adds a writer to self.writers if the writer does not exist already.\n            To avoid spamming writers on disk.\n            \n            :returns the writer on path with tag tag or path\n        """"""\n        if not tag:\n            tag = path\n        if tag not in self.writers.keys():\n            self.writers[tag] = tf.summary.create_file_writer(str(path))\n        if default:\n            self.default_writer = tag\n        return self.writers[tag]\n    \n    @property\n    def global_step(self):\n        return self.model.step\n    \n    def add_scalars(self, tag, dictionary):\n        for k in dictionary.keys():\n            with self.add_writer(str(self.log_dir / k)).as_default():\n                tf.summary.scalar(name=tag, data=dictionary[k], step=self.global_step)\n    \n    def add_scalar(self, tag, scalar_value):\n        with self.writers[self.default_writer].as_default():\n            tf.summary.scalar(name=tag, data=scalar_value, step=self.global_step)\n    \n    def add_image(self, tag, image, step=None):\n        if step is None:\n            step = self.global_step\n        with self.writers[self.default_writer].as_default():\n            tf.summary.image(name=tag, data=image, step=step, max_outputs=4)\n    \n    def add_histogram(self, tag, values, buckets=None):\n        with self.writers[self.default_writer].as_default():\n            tf.summary.histogram(name=tag, data=values, step=self.global_step, buckets=buckets)\n    \n    def add_audio(self, tag, wav, sr):\n        with self.writers[self.default_writer].as_default():\n            tf.summary.audio(name=tag,\n                             data=wav,\n                             sample_rate=sr,\n                             step=self.global_step)\n    \n    @ignore_exception\n    def display_attention_heads(self, outputs, tag=\'\'):\n        for layer in [\'encoder_attention\', \'decoder_attention\']:\n            for k in outputs[layer].keys():\n                image = tight_grid(norm_tensor(outputs[layer][k][0]))\n                # dim 0 of image_batch is now number of heads\n                batch_plot_path = f\'{tag}/{layer}/{k}\'\n                self.add_image(str(batch_plot_path), tf.expand_dims(tf.expand_dims(image, 0), -1))\n    \n    @ignore_exception\n    def display_mel(self, mel, tag=\'\'):\n        img = tf.transpose(mel)\n        figure = self.audio.display_mel(img, is_normal=True)\n        buf = buffer_image(figure)\n        img_tf = tf.image.decode_png(buf.getvalue(), channels=3)\n        self.add_image(tag, tf.expand_dims(img_tf, 0))\n    \n    @control_frequency\n    @ignore_exception\n    def display_loss(self, output, tag=\'\', plot_all=False):\n        self.add_scalars(tag=f\'{tag}/losses\', dictionary=output[\'losses\'])\n        self.add_scalar(tag=f\'{tag}/loss\', scalar_value=output[\'loss\'])\n    \n    @control_frequency\n    @ignore_exception\n    def display_scalar(self, tag, scalar_value, plot_all=False):\n        self.add_scalar(tag=tag, scalar_value=scalar_value)\n    \n    @ignore_exception\n    def display_audio(self, tag, mel):\n        wav = tf.transpose(mel)\n        wav = self.audio.reconstruct_waveform(wav)\n        wav = tf.expand_dims(wav, 0)\n        wav = tf.expand_dims(wav, -1)\n        self.add_audio(tag, wav.numpy(), sr=self.config[\'sampling_rate\'])\n'"
utils/losses.py,26,"b'import tensorflow as tf\n\n\ndef new_scaled_crossentropy(index=2, scaling=1.0):\n    """"""\n    Returns masked crossentropy with extra scaling:\n    Scales the loss for given stop_index by stop_scaling\n    """"""\n    \n    def masked_crossentropy(targets: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n        crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        padding_mask = tf.math.equal(targets, 0)\n        padding_mask = tf.math.logical_not(padding_mask)\n        padding_mask = tf.cast(padding_mask, dtype=tf.float32)\n        stop_mask = tf.math.equal(targets, index)\n        stop_mask = tf.cast(stop_mask, dtype=tf.float32) * (scaling - 1.)\n        combined_mask = padding_mask + stop_mask\n        loss = crossentropy(targets, logits, sample_weight=combined_mask)\n        return loss\n    \n    return masked_crossentropy\n\n\ndef masked_crossentropy(targets: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n    mask = tf.cast(mask, dtype=tf.int32)\n    loss = crossentropy(targets, logits, sample_weight=mask)\n    return loss\n\n\ndef masked_mean_squared_error(targets: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n    mse = tf.keras.losses.MeanSquaredError()\n    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n    mask = tf.cast(mask, dtype=tf.int32)\n    mask = tf.reduce_max(mask, axis=-1)\n    loss = mse(targets, logits, sample_weight=mask)\n    return loss\n\ndef masked_mean_absolute_error(targets: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n    mae = tf.keras.losses.MeanAbsoluteError()\n    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n    mask = tf.cast(mask, dtype=tf.int32)\n    mask = tf.reduce_max(mask, axis=-1)\n    loss = mae(targets, logits, sample_weight=mask)\n    return loss\n\n\ndef masked_binary_crossentropy(targets: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n    bc = tf.keras.losses.BinaryCrossentropy(reduction=\'none\')\n    mask = tf.math.logical_not(tf.math.equal(targets, -1))\n    mask = tf.cast(mask, dtype=tf.int32)\n    loss_ = bc(targets, logits)\n    loss_ *= mask\n    return tf.reduce_mean(loss_)\n\n\ndef weighted_sum_losses(targets, pred, loss_functions, coeffs):\n    total_loss = 0\n    loss_vals = []\n    for i in range(len(loss_functions)):\n        loss = loss_functions[i](targets[i], pred[i])\n        loss_vals.append(loss)\n        total_loss += coeffs[i] * loss\n    return total_loss, loss_vals\n'"
utils/scheduling.py,1,"b'import tensorflow as tf\nimport numpy as np\n\n\ndef linear_function(x, x0, x1, y0, y1):\n    m = (y1 - y0) / (x1 - x0)\n    b = y0 - m * x0\n    return m * x + b\n\n\ndef piecewise_linear(step, X, Y):\n    """"""\n    Piecewise linear function.\n    \n    :param step: current step.\n    :param X: list of breakpoints\n    :param Y: list of values at breakpoints\n    :return: value of piecewise linear function with values Y_i at step X_i\n    """"""\n    assert len(X) == len(Y)\n    X = np.array(X)\n    if step < X[0]:\n        return Y[0]\n    idx = np.where(step >= X)[0][-1]\n    if idx == (len(Y) - 1):\n        return Y[-1]\n    else:\n        return linear_function(step, X[idx], X[idx + 1], Y[idx], Y[idx + 1])\n\n\ndef piecewise_linear_schedule(step, schedule):\n    schedule = np.array(schedule)\n    x_schedule = schedule[:, 0]\n    y_schedule = schedule[:, 1]\n    value = piecewise_linear(step, x_schedule, y_schedule)\n    return tf.cast(value, tf.float32)\n\n\ndef reduction_schedule(step, schedule):\n    schedule = np.array(schedule)\n    r = schedule[0, 0]\n    for i in range(schedule.shape[0]):\n        if schedule[i, 0] <= step:\n            r = schedule[i, 1]\n        else:\n            break\n    return int(r)\n'"
utils/vec_ops.py,6,"b'import tensorflow as tf\n\n\ndef norm_tensor(tensor):\n    return tf.math.divide(\n        tf.math.subtract(\n            tensor,\n            tf.math.reduce_min(tensor)\n        ),\n        tf.math.subtract(\n            tf.math.reduce_max(tensor),\n            tf.math.reduce_min(tensor)\n        )\n    )\n'"
