file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\nlong_description = '''\nHeadliner is a sequence modeling library that eases the training and \n**in particular, the deployment of custom sequence models** for both researchers and developers. \nYou can very easily deploy your models in a few lines of code. It was originally \nbuilt for our own research to generate headlines from news articles. \nThat's why we chose the name, Headliner. Although this library was created internally to \ngenerate headlines, you can also use it for other tasks like machine translations,\ntext summarization and many more.\n\nRead the documentation at: https://as-ideas.github.io/headliner/\n\nHeadliner is compatible with Python 3.6+ and is distributed under the MIT license.\n'''\n\nsetup(\n    name='headliner',\n    version='1.0.2',\n    author='Christian Sch\xc3\xa4fer',\n    author_email='c.schaefer.home@gmail.com',\n    description='Easy training and deployment of seq2seq models.',\n    long_description=long_description,\n    license='MIT',\n    install_requires=['scikit-learn', 'nltk', 'pyyaml', 'transformers>=2.2.2', 'spacy>=2.2.2'],\n    extras_require={\n        'tests': ['pytest', 'pytest-cov', 'codecov', 'tensorflow==2.0.0'],\n        'docs': ['mkdocs', 'mkdocs-material'],\n        'dev': ['bumpversion']\n    },\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: POSIX :: Linux',\n        'Operating System :: MacOS :: MacOS X',\n        'Operating System :: Microsoft :: Windows',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    packages=find_packages(exclude=('tests',)),\n)\n"""
headliner/__init__.py,0,"b""__version__ = '1.0.2'\n"""
headliner/embeddings.py,0,"b'from typing import Dict\n\nimport numpy as np\n\n\ndef read_embedding(file_path: str, vector_dim: int) -> Dict[str, np.array]:\n    """"""\n    Reads an embedding file in glove format into a dictionary mapping tokens to vectors.\n    """"""\n\n    glove = {}\n    with open(file_path, encoding=\'utf-8\') as f:\n        for line in f:\n            values = line.split()\n            # handle weird whitespaces in tokens\n            if len(values[1:]) > vector_dim:\n                wordcount = len(values) - vector_dim\n                vec = np.asarray(values[wordcount:], dtype=\'float32\')\n            else:\n                vec = np.asarray(values[1:], dtype=\'float32\')\n            token = values[0]\n            glove[token] = vec\n    return glove\n\n\ndef embedding_to_matrix(embedding: Dict[str, np.array],\n                        token_index: Dict[str, int],\n                        embedding_dim: int) -> np.array:\n    """"""\n    Converts an embedding dictionary into a weights matrix used to initialize an embedding layer.\n    It ensures that all tokens in the token_index dictionare are mapped to a row, even those that are\n    not contained in the provided embedding dictionary. Unknown tokens are initialized with a random\n    vector with entries between -1 and 1.\n\n    Args:\n        embedding: dictionary mapping tokens to embedding vectors\n        token_index: dictionary mapping tokens to indices that are fed into the embedding layer\n        embedding_dim: size of the embedding vectors\n\n    Returns: embedding weights as numpy array\n    """"""\n    np.random.seed(42)\n    embedding_matrix = 2. * np.random.rand(len(token_index) + 1, embedding_dim) - 1.\n    for token, index in token_index.items():\n        embedding_vec = embedding.get(token)\n        if embedding_vec is not None:\n            embedding_matrix[index] = embedding_vec\n    return embedding_matrix\n'"
headliner/losses.py,4,"b'import tensorflow as tf\n\n\ndef masked_crossentropy(targets: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n    mask = tf.cast(mask, dtype=tf.int64)\n    loss = crossentropy(targets, logits, sample_weight=mask)\n    return loss\n'"
headliner/trainer.py,1,"b'import datetime\nimport logging\nimport os\nimport tempfile\nfrom collections import Counter\nfrom typing import Tuple, List, Iterable, Callable, Dict, Union\n\nimport tensorflow as tf\nimport yaml\n\nfrom headliner.callbacks.evaluation_callback import EvaluationCallback\nfrom headliner.callbacks.model_checkpoint_callback import ModelCheckpointCallback\nfrom headliner.callbacks.tensorboard_callback import TensorboardCallback\nfrom headliner.callbacks.validation_callback import ValidationCallback\nfrom headliner.embeddings import read_embedding, embedding_to_matrix\nfrom headliner.evaluation.scorer import Scorer\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.bert_summarizer import BertSummarizer\nfrom headliner.model.summarizer import Summarizer\nfrom headliner.preprocessing.bucket_generator import BucketGenerator\nfrom headliner.preprocessing.dataset_generator import DatasetGenerator\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\nfrom headliner.utils.logger import get_logger\n\nSTART_TOKEN = \'<start>\'\nEND_TOKEN = \'<end>\'\nOOV_TOKEN = \'<unk>\'\n\n\nclass Trainer:\n\n    def __init__(self,\n                 max_input_len=None,\n                 max_output_len=None,\n                 batch_size=16,\n                 max_vocab_size_encoder=200000,\n                 max_vocab_size_decoder=200000,\n                 embedding_path_encoder=None,\n                 embedding_path_decoder=None,\n                 steps_per_epoch=500,\n                 tensorboard_dir=None,\n                 model_save_path=None,\n                 shuffle_buffer_size=100000,\n                 use_bucketing=False,\n                 bucketing_buffer_size_batches=10000,\n                 bucketing_batches_to_bucket=100,\n                 logging_level=logging.INFO,\n                 num_print_predictions=5,\n                 steps_to_log=10,\n                 preprocessor: Union[Preprocessor, None] = None) -> None:\n        """"""\n        Initializes the trainer.\n\n        Args:\n            max_input_len (output): Maximum length of input sequences, longer sequences will be truncated.\n            max_output_len (output): Maximum length of output sequences, longer sequences will be truncated.\n            batch_size: Size of mini-batches for stochastic gradient descent.\n            max_vocab_size_encoder: Maximum number of unique tokens to consider for encoder embeddings.\n            max_vocab_size_decoder: Maximum number of unique tokens to consider for decoder embeddings.\n            embedding_path_encoder: Path to embedding file for the encoder.\n            embedding_path_decoder: Path to embedding file for the decoder.\n            steps_per_epoch: Number of steps to train until callbacks are invoked.\n            tensorboard_dir: Directory for saving tensorboard logs.\n            model_save_path: Directory for saving the best model.\n            shuffle_buffer_size: Size of the buffer for shuffling the files before batching.\n            use_bucketing: Whether to bucket the sequences by length to reduce the amount of padding.\n            bucketing_buffer_size_batches: Number of batches to buffer when bucketing sequences.\n            bucketing_batches_to_bucket: Number of buffered batches from which sequences are collected for bucketing.\n            logging_level: Level of logging to use, e.g. logging.INFO or logging.DEBUG.\n            num_print_predictions: Number of sample predictions to print in each evaluation.\n            steps_to_log: Number of steps to wait for logging output.\n            preprocessor (optional): custom preprocessor, if None a standard preprocessor will be created.\n        """"""\n\n        self.max_input_len = max_input_len\n        self.max_output_len = max_output_len\n        self.batch_size = batch_size\n        self.max_vocab_size_encoder = max_vocab_size_encoder\n        self.max_vocab_size_decoder = max_vocab_size_decoder\n        self.bucketing_buffer_size_batches = bucketing_buffer_size_batches\n        self.bucketing_batches_to_bucket = bucketing_batches_to_bucket\n        self.embedding_path_encoder = embedding_path_encoder\n        self.embedding_path_decoder = embedding_path_decoder\n        self.steps_per_epoch = steps_per_epoch\n        self.tensorboard_dir = tensorboard_dir\n        self.model_save_path = model_save_path\n        self.loss_function = masked_crossentropy\n        self.use_bucketing = use_bucketing\n        self.shuffle_buffer_size = None if use_bucketing else shuffle_buffer_size\n\n        self.bucket_generator = None\n        if use_bucketing:\n            self.bucket_generator = BucketGenerator(element_length_function=lambda vecs: len(vecs[0]),\n                                                    batch_size=self.batch_size,\n                                                    buffer_size_batches=self.bucketing_buffer_size_batches,\n                                                    batches_to_bucket=self.bucketing_batches_to_bucket,\n                                                    shuffle=True,\n                                                    seed=42)\n        self.logger = get_logger(__name__)\n        self.logger.setLevel(logging_level)\n        self.num_print_predictions = num_print_predictions\n        self.steps_to_log = steps_to_log\n        self.preprocessor = preprocessor or Preprocessor(start_token=START_TOKEN, end_token=END_TOKEN)\n\n    @classmethod\n    def from_config(cls, file_path, **kwargs):\n        with open(file_path, \'r\', encoding=\'utf-8\') as f:\n            cfg = yaml.load(f)\n            batch_size = cfg[\'batch_size\']\n            max_vocab_size_encoder = cfg[\'max_vocab_size_encoder\']\n            max_vocab_size_decoder = cfg[\'max_vocab_size_decoder\']\n            glove_path_encoder = cfg[\'embedding_path_encoder\']\n            glove_path_decoder = cfg[\'embedding_path_decoder\']\n            steps_per_epoch = cfg[\'steps_per_epoch\']\n            tensorboard_dir = cfg[\'tensorboard_dir\']\n            model_save_path = cfg[\'model_save_path\']\n            use_bucketing = cfg[\'use_bucketing\']\n            shuffle_buffer_size = cfg[\'shuffle_buffer_size\']\n            bucketing_buffer_size_batches = cfg[\'bucketing_buffer_size_batches\']\n            bucketing_batches_to_bucket = cfg[\'bucketing_batches_to_bucket\']\n            steps_to_log = cfg[\'steps_to_log\']\n            logging_level = logging.INFO\n            logging_level_string = cfg[\'logging_level\']\n            max_input_len = cfg[\'max_input_len\']\n            max_output_len = cfg[\'max_output_len\']\n            if logging_level_string == \'debug\':\n                logging_level = logging.DEBUG\n            elif logging_level_string == \'error\':\n                logging_level = logging.ERROR\n            return Trainer(batch_size=batch_size,\n                           max_vocab_size_encoder=max_vocab_size_encoder,\n                           max_vocab_size_decoder=max_vocab_size_decoder,\n                           embedding_path_encoder=glove_path_encoder,\n                           embedding_path_decoder=glove_path_decoder,\n                           steps_per_epoch=steps_per_epoch,\n                           tensorboard_dir=tensorboard_dir,\n                           model_save_path=model_save_path,\n                           use_bucketing=use_bucketing,\n                           shuffle_buffer_size=shuffle_buffer_size,\n                           bucketing_buffer_size_batches=bucketing_buffer_size_batches,\n                           bucketing_batches_to_bucket=bucketing_batches_to_bucket,\n                           logging_level=logging_level,\n                           steps_to_log=steps_to_log,\n                           max_input_len=max_input_len,\n                           max_output_len=max_output_len,\n                           **kwargs)\n\n    def train(self,\n              summarizer: Summarizer,\n              train_data: Iterable[Tuple[str, str]],\n              val_data: Iterable[Tuple[str, str]] = None,\n              num_epochs=2500,\n              scorers: Dict[str, Scorer] = None,\n              callbacks: List[tf.keras.callbacks.Callback] = None) -> None:\n        """"""\n        Trains a summarizer or resumes training of a previously initialized summarizer.\n\n        Args:\n            summarizer: Model to train, can be either a freshly created model or a loaded model.\n            train_data: Data to train the model on.\n            val_data (optional): Validation data.\n            num_epochs: Number of epochs to train.\n            scorers (optional): Dictionary with {score_name, scorer} to add validation scores to the logs.\n            callbacks (optional): Additional custom callbacks.\n        """"""\n        if summarizer.preprocessor is None or summarizer.vectorizer is None:\n            self.logger.info(\'training a bare model, preprocessing data to init model...\')\n            self._init_model(summarizer, train_data)\n        else:\n            self.logger.info(\'training an already initialized model...\')\n        vectorize_train = self._vectorize_data(preprocessor=summarizer.preprocessor,\n                                               vectorizer=summarizer.vectorizer,\n                                               bucket_generator=self.bucket_generator)\n        vectorize_val = self._vectorize_data(preprocessor=summarizer.preprocessor,\n                                             vectorizer=summarizer.vectorizer,\n                                             bucket_generator=None)\n        train_gen, val_gen = self._create_dataset_generators(summarizer)\n        train_dataset = train_gen(lambda: vectorize_train(train_data))\n        val_dataset = val_gen(lambda: vectorize_val(val_data))\n\n        train_callbacks = callbacks or []\n        if val_data is not None:\n            train_callbacks.extend([\n                EvaluationCallback(summarizer=summarizer,\n                                   scorers=scorers or {},\n                                   val_data=val_data,\n                                   print_num_examples=self.num_print_predictions),\n                ValidationCallback(summarizer=summarizer,\n                                   val_dataset=val_dataset,\n                                   loss_function=self.loss_function,\n                                   batch_size=self.batch_size),\n            ])\n        loss_monitor = \'loss_val\' if val_data is not None else \'loss\'\n        train_callbacks.append(\n            ModelCheckpointCallback(file_path=self.model_save_path,\n                                    summarizer=summarizer,\n                                    monitor=loss_monitor,\n                                    mode=\'min\'))\n\n        if self.tensorboard_dir is not None:\n            tb_callback = TensorboardCallback(log_dir=self.tensorboard_dir)\n            train_callbacks.append(tb_callback)\n        logs = {}\n        epoch_count, batch_count, train_losses = 0, 0, []\n        train_step = summarizer.new_train_step(self.loss_function,\n                                               self.batch_size,\n                                               apply_gradients=True)\n        while epoch_count < num_epochs:\n            for train_batch in train_dataset.take(-1):\n                batch_count += 1\n                current_loss = train_step(*train_batch)\n                train_losses.append(current_loss)\n                logs[\'loss\'] = float(sum(train_losses)) / len(train_losses)\n                if batch_count % self.steps_to_log == 0:\n                    self.logger.info(\'epoch {epoch}, batch {batch}, \'\n                                     \'logs: {logs}\'.format(epoch=epoch_count,\n                                                           batch=batch_count,\n                                                           logs=logs))\n                if batch_count % self.steps_per_epoch == 0:\n                    train_losses.clear()\n                    for callback in train_callbacks:\n                        callback.on_epoch_end(epoch_count, logs=logs)\n                    epoch_count += 1\n                    if epoch_count >= num_epochs:\n                        break\n\n            self.logger.info(\'finished iterating over dataset, total batches: {}\'.format(batch_count))\n            if batch_count == 0:\n                raise ValueError(\'Iterating over the dataset yielded zero batches!\')\n\n    def _init_model(self,\n                    summarizer: Summarizer,\n                    train_data: Iterable[Tuple[str, str]]) -> None:\n\n        tokenizer_encoder, tokenizer_decoder = self._create_tokenizers(train_data)\n        self.logger.info(\'vocab encoder: {vocab_enc}, vocab decoder: {vocab_dec}\'.format(\n            vocab_enc=tokenizer_encoder.vocab_size, vocab_dec=tokenizer_decoder.vocab_size))\n        vectorizer = Vectorizer(tokenizer_encoder,\n                                tokenizer_decoder,\n                                max_input_len=self.max_input_len,\n                                max_output_len=self.max_output_len)\n        embedding_weights_encoder, embedding_weights_decoder = None, None\n\n        if self.embedding_path_encoder is not None:\n            self.logger.info(\'loading encoder embedding from {}\'.format(self.embedding_path_encoder))\n            embedding = read_embedding(self.embedding_path_encoder, summarizer.embedding_size)\n            embedding_weights_encoder = embedding_to_matrix(embedding=embedding,\n                                                            token_index=tokenizer_encoder.token_index,\n                                                            embedding_dim=summarizer.embedding_size)\n        if self.embedding_path_decoder is not None:\n            self.logger.info(\'loading decoder embedding from {}\'.format(self.embedding_path_decoder))\n            embedding = read_embedding(self.embedding_path_decoder, summarizer.embedding_size)\n            embedding_weights_decoder = embedding_to_matrix(embedding=embedding,\n                                                            token_index=tokenizer_decoder.token_index,\n                                                            embedding_dim=summarizer.embedding_size)\n        summarizer.init_model(preprocessor=self.preprocessor,\n                              vectorizer=vectorizer,\n                              embedding_weights_encoder=embedding_weights_encoder,\n                              embedding_weights_decoder=embedding_weights_decoder)\n\n    def _vectorize_data(self,\n                        preprocessor: Preprocessor,\n                        vectorizer: Vectorizer,\n                        bucket_generator: BucketGenerator = None) \\\n            -> Callable[[Iterable[Tuple[str, str]]],\n                        Iterable[Tuple[List[int], List[int]]]]:\n\n        def vectorize(raw_data: Iterable[Tuple[str, str]]):\n            data_preprocessed = (preprocessor(d) for d in raw_data)\n            data_vectorized = (vectorizer(d) for d in data_preprocessed)\n            if bucket_generator is None:\n                return data_vectorized\n            else:\n                return bucket_generator(data_vectorized)\n\n        return vectorize\n\n    def _create_tokenizers(self,\n                           train_data: Iterable[Tuple[str, str]]\n                           ) -> Tuple[KerasTokenizer, KerasTokenizer]:\n\n        self.logger.info(\'fitting tokenizers...\')\n        counter_encoder = Counter()\n        counter_decoder = Counter()\n        train_preprocessed = (self.preprocessor(d) for d in train_data)\n        for text_encoder, text_decoder in train_preprocessed:\n            counter_encoder.update(text_encoder.split())\n            counter_decoder.update(text_decoder.split())\n        tokens_encoder = {token_count[0] for token_count\n                          in counter_encoder.most_common(self.max_vocab_size_encoder)}\n        tokens_decoder = {token_count[0] for token_count\n                          in counter_decoder.most_common(self.max_vocab_size_decoder)}\n        tokens_encoder.update({self.preprocessor.start_token, self.preprocessor.end_token})\n        tokens_decoder.update({self.preprocessor.start_token, self.preprocessor.end_token})\n        tokenizer_encoder = KerasTokenizer(oov_token=OOV_TOKEN, lower=False, filters=\'\')\n        tokenizer_decoder = KerasTokenizer(oov_token=OOV_TOKEN, lower=False, filters=\'\')\n        tokenizer_encoder.fit(sorted(list(tokens_encoder)))\n        tokenizer_decoder.fit(sorted(list(tokens_decoder)))\n        return tokenizer_encoder, tokenizer_decoder\n\n    def _create_dataset_generators(self, summarizer):\n        data_rank = 3 if isinstance(summarizer, BertSummarizer) else 2\n        train_gen = DatasetGenerator(batch_size=self.batch_size,\n                                     shuffle_buffer_size=self.shuffle_buffer_size,\n                                     rank=data_rank)\n        val_gen = DatasetGenerator(batch_size=self.batch_size,\n                                   shuffle_buffer_size=None,\n                                   rank=data_rank)\n        return train_gen, val_gen\n'"
mkdocs/autogen.py,0,"b'# Heavily borrowed from the Auto-Keras project:\n# https://github.com/jhfjhfj1/autokeras/blob/master/mkdocs/autogen.py\n\nimport ast\nimport os\nimport re\n\n\ndef delete_space(parts, start, end):\n    if start > end or end >= len(parts):\n        return None\n    count = 0\n    while count < len(parts[start]):\n        if parts[start][count] == \' \':\n            count += 1\n        else:\n            break\n    return \'\\n\'.join(y for y in [x[count:] for x in parts[start : end + 1] if len(x) > count])\n\n\ndef change_args_to_dict(string):\n    if string is None:\n        return None\n    ans = []\n    strings = string.split(\'\\n\')\n    ind = 1\n    start = 0\n    while ind <= len(strings):\n        if ind < len(strings) and strings[ind].startswith("" ""):\n            ind += 1\n        else:\n            if start < ind:\n                ans.append(\'\\n\'.join(strings[start:ind]))\n            start = ind\n            ind += 1\n    d = {}\n    for line in ans:\n        if "":"" in line and len(line) > 0:\n            lines = line.split("":"")\n            d[lines[0]] = lines[1].strip()\n    return d\n\n\ndef remove_next_line(comments):\n    for x in comments:\n        if comments[x] is not None and \'\\n\' in comments[x]:\n            comments[x] = \' \'.join(comments[x].split(\'\\n\'))\n    return comments\n\n\ndef skip_space_line(parts, ind):\n    while ind < len(parts):\n        if re.match(r\'^\\s*$\', parts[ind]):\n            ind += 1\n        else:\n            break\n    return ind\n\n\n# check if comment is None or len(comment) == 0 return {}\ndef parse_func_string(comment):\n    if comment is None or len(comment) == 0:\n        return {}\n    comments = {}\n    paras = (\'Args\', \'Attributes\', \'Returns\', \'Raises\')\n    comment_parts = [\n        \'short_description\',\n        \'long_description\',\n        \'Args\',\n        \'Attributes\',\n        \'Returns\',\n        \'Raises\',\n    ]\n    for x in comment_parts:\n        comments[x] = None\n\n    parts = re.split(r\'\\n\', comment)\n    ind = 1\n    while ind < len(parts):\n        if re.match(r\'^\\s*$\', parts[ind]):\n            break\n        else:\n            ind += 1\n\n    comments[\'short_description\'] = \'\\n\'.join(\n        [\'\\n\'.join(re.split(\'\\n\\s+\', x.strip())) for x in parts[0:ind]]\n    ).strip(\':\\n\\t \')\n    ind = skip_space_line(parts, ind)\n\n    start = ind\n    while ind < len(parts):\n        if parts[ind].strip().startswith(paras):\n            break\n        else:\n            ind += 1\n    long_description = \'\\n\'.join(\n        [\'\\n\'.join(re.split(\'\\n\\s+\', x.strip())) for x in parts[start:ind]]\n    ).strip(\':\\n\\t \')\n    comments[\'long_description\'] = long_description\n\n    ind = skip_space_line(paras, ind)\n    while ind < len(parts):\n        if parts[ind].strip().startswith(paras):\n            start = ind\n            start_with = parts[ind].strip()\n            ind += 1\n            while ind < len(parts):\n                if parts[ind].strip().startswith(paras):\n                    break\n                else:\n                    ind += 1\n            part = delete_space(parts, start + 1, ind - 1)\n            if start_with.startswith(paras[0]):\n                comments[paras[0]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[1]):\n                comments[paras[1]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[2]):\n                comments[paras[2]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[3]):\n                comments[paras[3]] = part\n            ind = skip_space_line(parts, ind)\n        else:\n            ind += 1\n\n    remove_next_line(comments)\n    return comments\n\n\ndef md_parse_line_break(comment):\n    comment = comment.replace(\'  \', \'\\n\\n\')\n    return comment.replace(\' - \', \'\\n\\n- \')\n\n\ndef to_md(comment_dict):\n    doc = \'\'\n    if \'short_description\' in comment_dict:\n        doc += comment_dict[\'short_description\']\n        doc += \'\\n\\n\'\n\n    if \'long_description\' in comment_dict:\n        doc += md_parse_line_break(comment_dict[\'long_description\'])\n        doc += \'\\n\'\n\n    if \'Args\' in comment_dict and comment_dict[\'Args\'] is not None:\n        doc += \'##### Args\\n\'\n        for arg, des in comment_dict[\'Args\'].items():\n            doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Attributes\' in comment_dict and comment_dict[\'Attributes\'] is not None:\n        doc += \'##### Attributes\\n\'\n        for arg, des in comment_dict[\'Attributes\'].items():\n            doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Returns\' in comment_dict and comment_dict[\'Returns\'] is not None:\n        doc += \'##### Returns\\n\'\n        if isinstance(comment_dict[\'Returns\'], str):\n            doc += comment_dict[\'Returns\']\n            doc += \'\\n\'\n        else:\n            for arg, des in comment_dict[\'Returns\'].items():\n                doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n    return doc\n\n\ndef parse_func_args(function):\n    args = [a.arg for a in function.args.args if a.arg != \'self\']\n    kwargs = []\n    if function.args.kwarg:\n        kwargs = [\'**\' + function.args.kwarg.arg]\n\n    return \'(\' + \', \'.join(args + kwargs) + \')\'\n\n\ndef get_func_comments(function_definitions):\n    doc = \'\'\n    for f in function_definitions:\n        temp_str = to_md(parse_func_string(ast.get_docstring(f)))\n        doc += \'\'.join(\n            [\n                \'### \',\n                f.name.replace(\'_\', \'\\\\_\'),\n                \'\\n\',\n                \'```python\',\n                \'\\n\',\n                \'def \',\n                f.name,\n                parse_func_args(f),\n                \'\\n\',\n                \'```\',\n                \'\\n\',\n                temp_str,\n                \'\\n\',\n            ]\n        )\n\n    return doc\n\n\ndef get_comments_str(file_name):\n    with open(file_name) as fd:\n        file_contents = fd.read()\n    module = ast.parse(file_contents)\n\n    function_definitions = [node for node in module.body if isinstance(node, ast.FunctionDef)]\n\n    doc = get_func_comments(function_definitions)\n\n    class_definitions = [node for node in module.body if isinstance(node, ast.ClassDef)]\n    for class_def in class_definitions:\n        temp_str = to_md(parse_func_string(ast.get_docstring(class_def)))\n\n        # excludes private methods (start with \'_\')\n        method_definitions = [\n            node\n            for node in class_def.body\n            if isinstance(node, ast.FunctionDef) and (node.name[0] != \'_\' or node.name[:2] == \'__\')\n        ]\n\n        temp_str += get_func_comments(method_definitions)\n        doc += \'## class \' + class_def.name + \'\\n\' + temp_str\n    return doc\n\n\ndef extract_comments(directory):\n    for parent, dir_names, file_names in os.walk(directory):\n        for file_name in file_names:\n            if os.path.splitext(file_name)[1] == \'.py\' and file_name != \'__init__.py\':\n                # with open\n                doc = get_comments_str(os.path.join(parent, file_name))\n                directory = os.path.join(\'docs\', parent.replace(\'../headliner/\', \'\'))\n                if not os.path.exists(directory):\n                    os.makedirs(directory)\n\n                output_file = open(os.path.join(directory, file_name[:-3] + \'.md\'), \'w\')\n                output_file.write(doc)\n                output_file.close()\n\n\nextract_comments(\'../headliner/\')'"
tests/__init__.py,0,b''
tests/test_bert_training.py,3,"b""import unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom spacy.lang.en import English\nfrom transformers import BertTokenizer\n\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.bert_summarizer import BertSummarizer\nfrom headliner.preprocessing.bert_preprocessor import BertPreprocessor\nfrom headliner.preprocessing.bert_vectorizer import BertVectorizer\nfrom headliner.preprocessing.dataset_generator import DatasetGenerator\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\n\n\nclass TestBertTraining(unittest.TestCase):\n\n    def setUp(self) -> None:\n        tf.random.set_seed(42)\n        np.random.seed(42)\n        data = [('I love dogs.', 'Dogs.'),\n                ('I love cats.', 'Cats.')]\n        tokenizer_encoder = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.preprocessor = BertPreprocessor(nlp=English())\n        data_prep = [self.preprocessor(d) for d in data]\n        tokenizer_decoder = KerasTokenizer(lower=False, filters='')\n        tokenizer_decoder.fit([d[1] for d in data_prep])\n        self.vectorizer = BertVectorizer(tokenizer_encoder=tokenizer_encoder,\n                                         tokenizer_decoder=tokenizer_decoder,\n                                         max_output_len=10)\n        batch_generator = DatasetGenerator(2, rank=3)\n        data_vecs = [self.vectorizer(d) for d in data_prep]\n        self.dataset = batch_generator(lambda: data_vecs)\n        self.loss_func = masked_crossentropy\n\n    def test_training_summarizer_bert(self):\n        bert_summarizer = BertSummarizer(num_heads=4,\n                                         num_layers_encoder=0,\n                                         num_layers_decoder=1,\n                                         feed_forward_dim=20,\n                                         embedding_size_encoder=768,\n                                         embedding_size_decoder=64,\n                                         bert_embedding_encoder='bert-base-uncased',\n                                         embedding_encoder_trainable=True,\n                                         dropout_rate=0,\n                                         max_prediction_len=10)\n        bert_summarizer.optimizer_encoder = tf.keras.optimizers.Adam(learning_rate=3e-5)\n        bert_summarizer.optimizer_decoder = tf.keras.optimizers.Adam(learning_rate=1e-4)\n        bert_summarizer.init_model(preprocessor=self.preprocessor,\n                                   vectorizer=self.vectorizer,\n                                   embedding_weights_encoder=None,\n                                   embedding_weights_decoder=None)\n\n        loss_bert = 0\n        train_step = bert_summarizer.new_train_step(loss_function=self.loss_func,\n                                                    batch_size=2)\n        for e in range(0, 10):\n            for token_ids, sent_ids, target_ids in self.dataset.take(-1):\n                loss_bert = train_step(token_ids, sent_ids, target_ids)\n                print(str(loss_bert))\n\n        self.assertAlmostEqual(0.07848279923200607, float(loss_bert), 6)\n        model_output = bert_summarizer.predict_vectors('I love dogs.', '')\n        expected_first_logits = np.array([-1.881355, -1.493431,  1.358053,  3.050439,  0.636483])\n        np.testing.assert_allclose(expected_first_logits, model_output['logits'][0], atol=1e-3)\n        self.assertEqual('[CLS] I love dogs. [SEP]', model_output['preprocessed_text'][0])\n        self.assertEqual('Dogs. [SEP]', model_output['predicted_text'])\n"""
tests/test_embeddings.py,0,"b""import os\nimport unittest\n\nimport numpy as np\nfrom numpy import array\nfrom numpy.testing import assert_array_equal\n\nfrom headliner.embeddings import read_embedding, embedding_to_matrix\n\n\nclass TestEmbeddings(unittest.TestCase):\n\n    def test_read_embedding(self):\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        file_path = os.path.join(current_dir, 'resources/small_glove.txt')\n        glove = read_embedding(file_path, vector_dim=3)\n        assert_array_equal(array([1, 2, 3]), glove['a'])\n\n    def test_embedding_to_matrix(self):\n        embedding = {'a': np.array(2), 'b': np.array(3), 'c': np.array(4)}\n        token_index = {'a': 1, 'b': 2, 'd': 3}\n        matrix = embedding_to_matrix(embedding, token_index, 1)\n        np.testing.assert_array_equal(matrix[1], np.array(2))\n        np.testing.assert_array_equal(matrix[2], np.array(3))\n        # random values for zero index and tokens not in embedding\n        self.assertTrue(-1 < float(matrix[0]) < 1)\n        self.assertTrue(-1 < float(matrix[3]) < 1)\n"""
tests/test_trainer.py,1,"b""import tensorflow as tf\nimport logging\nimport os\nimport unittest\nimport numpy as np\nfrom tensorflow.python.keras.callbacks import Callback\nfrom headliner.model import AttentionSummarizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.trainer import Trainer\n\n\nclass TestTrainer(unittest.TestCase):\n\n    def setUp(self) -> None:\n        tf.random.set_seed(42)\n        np.random.seed(42)\n\n    def test_init_from_config(self) -> None:\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        config_path = os.path.join(current_dir, 'resources/trainer_test_config.yaml')\n        trainer = Trainer.from_config(config_path)\n        self.assertEqual(10, trainer.max_input_len)\n        self.assertEqual(9, trainer.max_output_len)\n        self.assertEqual(1, trainer.batch_size)\n        self.assertEqual(7, trainer.max_vocab_size_encoder)\n        self.assertEqual(6, trainer.max_vocab_size_decoder)\n        self.assertEqual('glove.txt', trainer.embedding_path_encoder)\n        self.assertEqual(None, trainer.embedding_path_decoder)\n        self.assertEqual(4, trainer.steps_per_epoch)\n        self.assertEqual('tensor_dir', trainer.tensorboard_dir)\n        self.assertEqual('model_save_path', trainer.model_save_path)\n        self.assertTrue(trainer.use_bucketing)\n        self.assertIsNone(trainer.shuffle_buffer_size)\n        self.assertEqual(5, trainer.bucketing_buffer_size_batches)\n        self.assertEqual(6, trainer.bucketing_batches_to_bucket)\n        self.assertEqual(7, trainer.steps_to_log)\n        self.assertEqual(logging.DEBUG, trainer.logger.level)\n\n    def test_init(self) -> None:\n        preprocessor = Preprocessor(start_token='<custom_start_token>', lower_case=False, hash_numbers=False)\n        trainer = Trainer(max_output_len=9,\n                          batch_size=1,\n                          max_vocab_size_encoder=2,\n                          max_vocab_size_decoder=3,\n                          embedding_path_encoder='glove.txt',\n                          steps_per_epoch=4,\n                          tensorboard_dir='tensor_dir',\n                          model_save_path='model_save_path',\n                          shuffle_buffer_size=10,\n                          bucketing_buffer_size_batches=5,\n                          bucketing_batches_to_bucket=6,\n                          steps_to_log=7,\n                          logging_level=logging.DEBUG,\n                          preprocessor=preprocessor)\n\n        self.assertEqual(1, trainer.batch_size)\n        self.assertEqual(2, trainer.max_vocab_size_encoder)\n        self.assertEqual(3, trainer.max_vocab_size_decoder)\n        self.assertEqual('glove.txt', trainer.embedding_path_encoder)\n        self.assertIsNone(trainer.embedding_path_decoder)\n        self.assertEqual(4, trainer.steps_per_epoch)\n        self.assertEqual('tensor_dir', trainer.tensorboard_dir)\n        self.assertEqual('model_save_path', trainer.model_save_path)\n        self.assertFalse(trainer.use_bucketing)\n        self.assertEqual(10, trainer.shuffle_buffer_size)\n        self.assertEqual(5, trainer.bucketing_buffer_size_batches)\n        self.assertEqual(6, trainer.bucketing_batches_to_bucket)\n        self.assertEqual(7, trainer.steps_to_log)\n        self.assertEqual(9, trainer.max_output_len)\n        self.assertEqual(logging.DEBUG, trainer.logger.level)\n        self.assertEqual('<custom_start_token>', trainer.preprocessor.start_token)\n        self.assertEqual(False, trainer.preprocessor.lower_case)\n        self.assertEqual(False, trainer.preprocessor.hash_numbers)\n\n    def test_init_model(self) -> None:\n        logging.basicConfig(level=logging.INFO)\n        data = [('a b', 'a'), ('a b c', 'b')]\n        summarizer = AttentionSummarizer(lstm_size=16,\n                                         embedding_size=10)\n        trainer = Trainer(batch_size=2,\n                          steps_per_epoch=10,\n                          max_vocab_size_encoder=10,\n                          max_vocab_size_decoder=10,\n                          model_save_path=None,\n                          max_input_len=5,\n                          max_output_len=3)\n        trainer.train(summarizer, data, num_epochs=1)\n        # encoding dim and decoding dim are num unique tokens + 4 (pad, start, end, oov)\n        self.assertIsNotNone(summarizer.vectorizer)\n        self.assertEqual(7, summarizer.vectorizer.encoding_dim)\n        self.assertEqual(6, summarizer.vectorizer.decoding_dim)\n        self.assertEqual(5, summarizer.vectorizer.max_input_len)\n        self.assertEqual(3, summarizer.vectorizer.max_output_len)\n\n    def test_train(self) -> None:\n\n        class LogCallback(Callback):\n\n            def __init__(self):\n                super().__init__()\n\n            def on_epoch_end(self, epoch, logs=None):\n                self.logs = logs\n\n        data = [('a b', 'a'), ('a b c', 'b')]\n\n        summarizer = AttentionSummarizer(lstm_size=16,\n                                         embedding_size=10)\n        log_callback = LogCallback()\n        trainer = Trainer(batch_size=2,\n                          steps_per_epoch=10,\n                          max_vocab_size_encoder=10,\n                          max_vocab_size_decoder=10,\n                          model_save_path=None,\n                          max_output_len=3)\n\n        trainer.train(summarizer,\n                      data,\n                      num_epochs=2,\n                      callbacks=[log_callback])\n\n        logs = log_callback.logs\n        self.assertAlmostEqual(1.7135955810546875, logs['loss'], 6)\n"""
tests/test_training.py,1,"b""import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.basic_summarizer import BasicSummarizer\nfrom headliner.model.attention_summarizer import AttentionSummarizer\nfrom headliner.model.transformer_summarizer import TransformerSummarizer\nfrom headliner.preprocessing.dataset_generator import DatasetGenerator\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass TestTraining(unittest.TestCase):\n\n    def setUp(self) -> None:\n        tf.random.set_seed(42)\n        np.random.seed(42)\n        self.data = [('a b', 'c'), ('a b c', 'd')]\n        tokenizer_encoder = KerasTokenizer(lower=False, filters='')\n        tokenizer_decoder = KerasTokenizer(lower=False, filters='')\n        tokenizer_encoder.fit(['a b c <start> <end>'])\n        tokenizer_decoder.fit(['c d <start> <end>'])\n        self.vectorizer = Vectorizer(tokenizer_encoder=tokenizer_encoder,\n                                     tokenizer_decoder=tokenizer_decoder,\n                                     max_output_len=3)\n        self.preprocessor = Preprocessor()\n        batch_generator = DatasetGenerator(2)\n        data_prep = [self.preprocessor(d) for d in self.data]\n        data_vecs = [self.vectorizer(d) for d in data_prep]\n        self.dataset = batch_generator(lambda: data_vecs)\n        self.loss_func = masked_crossentropy\n\n    def test_training_summarizer_attention(self) -> None:\n        attention_summarizer = AttentionSummarizer(lstm_size=10,\n                                                   embedding_size=10)\n        attention_summarizer.init_model(preprocessor=self.preprocessor,\n                                        vectorizer=self.vectorizer,\n                                        embedding_weights_encoder=None,\n                                        embedding_weights_decoder=None)\n        loss_attention = 0\n        train_step = attention_summarizer.new_train_step(loss_function=self.loss_func,\n                                                         batch_size=2)\n        for _ in range(10):\n            for source_seq, target_seq in self.dataset.take(-1):\n                loss_attention = train_step(source_seq, target_seq)\n                print(str(loss_attention))\n\n        self.assertAlmostEqual(1.577033519744873, float(loss_attention), 5)\n        output_attention = attention_summarizer.predict_vectors('a c', '')\n        expected_first_logits = np.array([-0.077805,  0.012667,  0.021359, -0.04872,  0.014989])\n        np.testing.assert_allclose(expected_first_logits, output_attention['logits'][0], atol=1e-6)\n        self.assertEqual('<start> a c <end>', output_attention['preprocessed_text'][0])\n        self.assertEqual('d <end>', output_attention['predicted_text'])\n\n    def test_training_summarizer_basic(self) -> None:\n        basic_summarizer = BasicSummarizer(lstm_size=10,\n                                     embedding_size=10)\n        basic_summarizer.init_model(preprocessor=self.preprocessor,\n                              vectorizer=self.vectorizer,\n                              embedding_weights_encoder=None,\n                              embedding_weights_decoder=None)\n        loss = 0\n        train_step = basic_summarizer.new_train_step(loss_function=self.loss_func,\n                                               batch_size=2)\n        for e in range(0, 10):\n            for source_seq, target_seq in self.dataset.take(-1):\n                loss = train_step(source_seq, target_seq)\n\n        self.assertAlmostEqual(1.5850255489349365, float(loss), 5)\n        output = basic_summarizer.predict_vectors('a c', '')\n        expected_first_logits = np.array([-0.00621 ,  0.007277,  0.015851, -0.034298,  0.044253])\n        np.testing.assert_allclose(expected_first_logits, output['logits'][0], atol=1e-6)\n        self.assertEqual('<start> a c <end>', output['preprocessed_text'][0])\n        self.assertEqual('<end>', output['predicted_text'])\n\n    def test_training_summarizer_transformer(self):\n        transformer_summarizer = TransformerSummarizer(num_heads=1,\n                                                       num_layers=1,\n                                                       feed_forward_dim=20,\n                                                       embedding_size=10,\n                                                       dropout_rate=0,\n                                                       max_prediction_len=3)\n        transformer_summarizer.init_model(preprocessor=self.preprocessor,\n                                          vectorizer=self.vectorizer,\n                                          embedding_weights_encoder=None,\n                                          embedding_weights_decoder=None)\n        loss_transformer = 0\n        train_step = transformer_summarizer.new_train_step(loss_function=self.loss_func,\n                                                           batch_size=2)\n        for e in range(0, 10):\n            for source_seq, target_seq in self.dataset.take(-1):\n                loss_transformer = train_step(source_seq, target_seq)\n                print(str(loss_transformer))\n\n        self.assertAlmostEqual(1.3421446084976196, float(loss_transformer), 5)\n        output_transformer = transformer_summarizer.predict_vectors('a c', '')\n        expected_first_logits = np.array([-0.514366,  1.416978, -0.679771, -0.488442, -0.022602])\n        np.testing.assert_allclose(expected_first_logits, output_transformer['logits'][0], atol=1e-6)\n        self.assertEqual('<start> a c <end>', output_transformer['preprocessed_text'][0])\n        self.assertEqual('c c c', output_transformer['predicted_text'])\n"""
headliner/callbacks/__init__.py,0,b'from .evaluation_callback import EvaluationCallback\nfrom .model_checkpoint_callback import ModelCheckpointCallback\nfrom .validation_callback import ValidationCallback\n'
headliner/callbacks/evaluation_callback.py,1,"b'from typing import Dict, Callable, Iterable, Tuple\n\nimport tensorflow as tf\n\nfrom headliner.model.summarizer import Summarizer\nfrom headliner.utils.logger import get_logger\n\n\nclass EvaluationCallback(tf.keras.callbacks.Callback):\n    """"""\n    Callback for custom scoring methods.\n    """"""\n\n    def __init__(self,\n                 summarizer: Summarizer,\n                 scorers: Dict[str, Callable[[Dict], float]],\n                 val_data: Iterable[Tuple[str, str]],\n                 print_num_examples=5) -> None:\n        """"""\n        Initializes the Callback.\n\n        Args:\n            summarizer: Summarizer that predicts over the validation data.\n            scorers: Dictionary of {scorer_name: scorer}, where each scorer maps a prediction to a score.\n            val_data: Raw validation data to predict on.\n            print_num_examples: Number of prediction examples to output for eyeballing the prediction quality.\n        """"""\n\n        super().__init__()\n        self.summarizer = summarizer\n        self.scorers = scorers\n        self.val_data = val_data\n        self.logger = get_logger(__name__)\n        self.print_num_examples = print_num_examples\n\n    def on_epoch_end(self, batch, logs=None) -> None:\n        if logs is None:\n            logs = {}\n        val_scores = {score_name: 0. for score_name in self.scorers.keys()}\n        count_val = 0\n        for d in self.val_data:\n            count_val += 1\n            input_text, target_text = d\n            prediction = self.summarizer.predict_vectors(input_text, target_text)\n            if count_val <= self.print_num_examples:\n                self.logger.info(\'\\n(input) {} \\n(target) {} \\n(prediction) {}\\n\'.format(\n                    prediction[\'preprocessed_text\'][0],\n                    prediction[\'preprocessed_text\'][1],\n                    prediction[\'predicted_text\']\n                ))\n            elif len(self.scorers) == 0:\n                break\n            for score_name, scorer in self.scorers.items():\n                score = scorer(prediction)\n                val_scores[score_name] += score\n        for score_name, score in val_scores.items():\n            logs[score_name] = float(score / count_val)\n'"
headliner/callbacks/model_checkpoint_callback.py,1,"b'import tensorflow as tf\n\nfrom headliner.model.summarizer import Summarizer\n\n\nclass ModelCheckpointCallback(tf.keras.callbacks.Callback):\n    """"""\n    Callback for checkpointing summarizer models.\n    """"""\n\n    def __init__(self,\n                 file_path: str,\n                 summarizer: Summarizer,\n                 monitor=\'loss_val\',\n                 mode=\'min\') -> None:\n\n        """"""\n        Initializes the Callback.\n\n        Args:\n            file_path: Path for saving the model (a directory). If existing, the model will be overwritten.\n            summarizer: Summarizer to checkpoint.\n            monitor: Name of the score monitor for improvements.\n            mode: If set to \'min\' a decrease of the monitored score is seen as an improvement, otherwise an increase.\n        """"""\n\n        super().__init__()\n        self.file_path = file_path\n        self.summarizer = summarizer\n        self.monitor = monitor\n        self.mode = mode\n        self.best_score = None\n\n    def on_epoch_end(self, batch, logs=None) -> None:\n        if logs is None:\n            logs = {}\n        if self.file_path is None:\n            return\n        score = logs[self.monitor]\n        score_is_better = False\n        if self.best_score is None:\n            score_is_better = True\n        else:\n            if self.mode == \'min\' and score < self.best_score:\n                score_is_better = True\n            if self.mode == \'max\' and score > self.best_score:\n                score_is_better = True\n        if score_is_better:\n            print(\'{score_name} improved from {prev} to {current}, \'\n                  \'saving summarizer to {path}\'.format(score_name=self.monitor,\n                                                       prev=self.best_score,\n                                                       current=score,\n                                                       path=self.file_path))\n            self.best_score = score\n            self.summarizer.save(self.file_path)\n        else:\n            print(\'{score_name} did not improve.\'.format(score_name=self.monitor))\n'"
headliner/callbacks/tensorboard_callback.py,3,"b'import tensorflow as tf\n\n\nclass TensorboardCallback(tf.keras.callbacks.Callback):\n    """"""\n    Callback for validation loss.\n    """"""\n\n    def __init__(self,\n                 log_dir: str) -> None:\n        """"""\n        Initializes the Callback.\n\n        Args:\n            log_dir: Tensorboard log directory to write to.\n        """"""\n\n        super().__init__()\n        self.summary_writer = tf.summary.create_file_writer(log_dir)\n\n    def on_epoch_end(self, batch, logs=None) -> None:\n        if logs is not None:\n            for key, val in logs.items():\n                with self.summary_writer.as_default():\n                    tf.summary.scalar(key, val, step=batch)\n'"
headliner/callbacks/validation_callback.py,3,"b'from typing import Callable\n\nimport tensorflow as tf\n\nfrom headliner.model.summarizer import Summarizer\n\n\nclass ValidationCallback(tf.keras.callbacks.Callback):\n    """"""\n    Callback for validation loss.\n    """"""\n\n    def __init__(self,\n                 summarizer: Summarizer,\n                 val_dataset: tf.data.Dataset,\n                 loss_function: Callable[[tf.Tensor, tf.Tensor], tf.Tensor],\n                 batch_size: int) -> None:\n        """"""\n        Initializes the Callback.\n\n        Args:\n            summarizer: Summarizer to validate.\n            val_dataset: Validation dataset to validate the model on.\n            loss_function: Loss function to apply to calculate the validation score.\n            batch_size: Batch size of the validation dataset, needed for initializing the model.\n        """"""\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.summarizer = summarizer\n        self.loss_function = loss_function\n        self.val_dataset = val_dataset\n        self.train_step = summarizer.new_train_step(self.loss_function,\n                                                    self.batch_size,\n                                                    apply_gradients=False)\n\n    def on_epoch_end(self, batch, logs=None) -> None:\n        if logs is None:\n            logs = {}\n        val_loss, count_batches_val = 0, 0\n        for batch in self.val_dataset.take(-1):\n            val_loss_batch = self.train_step(*batch)\n            val_loss += val_loss_batch\n            count_batches_val += 1\n        if count_batches_val == 0:\n            raise ValueError(\'Tried to validate on empty validation dataset, possibly due to batch size \'\n                             \'exceeding validation data size.\')\n        logs[\'loss_val\'] = float(val_loss / count_batches_val)\n'"
headliner/evaluation/__init__.py,0,b'from .bleu_scorer import BleuScorer\n'
headliner/evaluation/bleu_scorer.py,0,"b'from typing import Dict, Union\n\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom headliner.evaluation.scorer import Scorer\n\n\nclass BleuScorer(Scorer):\n    """"""\n    Provides BLEU score for a model prediction.\n    """"""\n\n    def __init__(self, tokens_to_ignore=None, weights=(0.25, 0.25, 0.25, 0.25)) -> None:\n        """"""\n        Initializes the scorer.\n\n        Args:\n            tokens_to_ignore: Tokens to be removed before comparing input and output text.\n            weights: Custom weights for 1,2,3,4 grams, e.g. (1, 0, 0, 0) will only measure 1-gram overlaps.\n        """"""\n        self.tokens_to_exclude = tokens_to_ignore or []\n        self.weights = weights\n\n    def __call__(self, prediction: [Dict[str, Union[str, np.array]]]) -> float:\n        tokens_predicted = prediction[\'predicted_text\'].split()\n        tokens_output = prediction[\'preprocessed_text\'][1].split()\n        tokens_predicted_filtered = [t for t in tokens_predicted if t not in self.tokens_to_exclude]\n        tokens_output_filtered = [t for t in tokens_output if t not in self.tokens_to_exclude]\n        return sentence_bleu([tokens_output_filtered], tokens_predicted_filtered, weights=self.weights)\n'"
headliner/evaluation/scorer.py,0,"b'import abc\nfrom typing import Dict, Union\n\nimport numpy as np\n\n\nclass Scorer(abc.ABC):\n\n    def __call__(self, prediction: [Dict[str, Union[str, np.array]]]) -> float:\n        """"""\n        Evaluates prediction.\n\n        Args:\n            prediction: Dictionary providing all information about a model prediction such as\n            output string, logits etc.\n\n        Returns: Prediction score as float.\n        """"""\n\n        raise NotImplementedError()\n'"
headliner/model/__init__.py,0,b'from .attention_summarizer import AttentionSummarizer\nfrom .basic_summarizer import BasicSummarizer\n'
headliner/model/attention_model.py,21,"b""from typing import Tuple\n\nimport tensorflow as tf\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self,\n                 embedding_shape: Tuple[int, int],\n                 lstm_size=50,\n                 embedding_trainable=True,\n                 embedding_weights=None) -> None:\n        super(Encoder, self).__init__()\n        vocab_size, vec_dim = embedding_shape\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                   vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.lstm = tf.keras.layers.LSTM(lstm_size,\n                                         return_sequences=True,\n                                         return_state=True)\n        self.lstm_size = lstm_size\n\n    def call(self,\n             sequence: tf.Tensor,\n             states: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n        embed = self.embedding(sequence)\n        output, state_h, state_c = self.lstm(embed, initial_state=states)\n        return output, state_h, state_c\n\n    def init_states(self, batch_size: int) -> Tuple[tf.Tensor, tf.Tensor]:\n        return tf.zeros([batch_size, self.lstm_size]), \\\n               tf.zeros([batch_size, self.lstm_size])\n\n\nclass LuongAttention(tf.keras.Model):\n\n    def __init__(self, rnn_size):\n        super(LuongAttention, self).__init__()\n        self.wa = tf.keras.layers.Dense(rnn_size)\n\n    def call(self, decoder_output, encoder_output):\n        score = tf.matmul(decoder_output, self.wa(encoder_output), transpose_b=True)\n        alignment = tf.nn.softmax(score, axis=2)\n        context = tf.matmul(alignment, encoder_output)\n        return context, alignment\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self,\n                 embedding_shape: Tuple[int, int],\n                 lstm_size=50,\n                 embedding_trainable=True,\n                 embedding_weights=None) -> None:\n        super(Decoder, self).__init__()\n        self.lstm_size = lstm_size\n        vocab_size, vec_dim = embedding_shape\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding = tf.keras.layers.Embedding(vocab_size, vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.lstm_size = lstm_size\n        self.attention = LuongAttention(lstm_size)\n        self.lstm = tf.keras.layers.LSTM(lstm_size,\n                                         return_sequences=True,\n                                         return_state=True)\n        self.wc = tf.keras.layers.Dense(lstm_size, activation='tanh')\n        self.ws = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, sequence, state, encoder_output):\n        embed = self.embedding(sequence)\n        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n        context, alignment = self.attention(lstm_out, encoder_output)\n        lstm_out = tf.concat([tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n        lstm_out = self.wc(lstm_out)\n        logits = self.ws(lstm_out)\n        return logits, state_h, state_c, alignment\n\n    def init_states(self, batch_size):\n        return (tf.zeros([batch_size, self.lstm_size]),\n                tf.zeros([batch_size, self.lstm_size]))\n"""
headliner/model/attention_summarizer.py,12,"b'import os\nimport pickle\nfrom typing import Callable, Dict, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.model.attention_model import Encoder, Decoder\nfrom headliner.model.summarizer import Summarizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass AttentionSummarizer(Summarizer):\n\n    def __init__(self, lstm_size=50, max_prediction_len=20, embedding_size=50, embedding_encoder_trainable=True,\n                 embedding_decoder_trainable=True):\n        super().__init__()\n        self.lstm_size = lstm_size\n        self.max_prediction_len = max_prediction_len\n        self.embedding_size = embedding_size\n        self.embedding_encoder_trainable = embedding_encoder_trainable\n        self.embedding_decoder_trainable = embedding_decoder_trainable\n        self.optimizer = AttentionSummarizer._new_optimizer()\n        self.encoder = None\n        self.decoder = None\n        self.embedding_shape_in = None\n        self.embedding_shape_out = None\n\n    def init_model(self,\n                   preprocessor: Preprocessor,\n                   vectorizer: Vectorizer,\n                   embedding_weights_encoder=None,\n                   embedding_weights_decoder=None) -> None:\n        self.preprocessor = preprocessor\n        self.vectorizer = vectorizer\n        self.embedding_shape_in = (self.vectorizer.encoding_dim, self.embedding_size)\n        self.embedding_shape_out = (self.vectorizer.decoding_dim, self.embedding_size)\n        self.encoder = Encoder(self.embedding_shape_in,\n                               self.lstm_size,\n                               embedding_trainable=self.embedding_encoder_trainable,\n                               embedding_weights=embedding_weights_encoder)\n        self.decoder = Decoder(self.embedding_shape_out,\n                               self.lstm_size,\n                               embedding_trainable=self.embedding_decoder_trainable,\n                               embedding_weights=embedding_weights_decoder)\n        self.encoder.compile(optimizer=self.optimizer)\n        self.decoder.compile(optimizer=self.optimizer)\n\n    def __getstate__(self):\n        """""" Prevents pickle from serializing encoder and decoder """"""\n        state = self.__dict__.copy()\n        del state[\'encoder\']\n        del state[\'decoder\']\n        del state[\'optimizer\']\n        return state\n\n    def predict(self, text: str) -> str:\n        return self.predict_vectors(text, \'\')[\'predicted_text\']\n\n    def predict_vectors(self, input_text: str, target_text: str) -> Dict[str, Union[str, np.array]]:\n        text_preprocessed = self.preprocessor((input_text, target_text))\n        en_inputs, _ = self.vectorizer(text_preprocessed)\n        en_initial_states = self.encoder.init_states(1)\n        en_outputs = self.encoder(tf.constant([en_inputs]), en_initial_states)\n        start_end_seq = self.vectorizer.encode_output(\n            \' \'.join([self.preprocessor.start_token, self.preprocessor.end_token]))\n        de_start_index, de_end_index = start_end_seq[:1], start_end_seq[-1:]\n        de_input = tf.constant([de_start_index])\n        de_state_h, de_state_c = en_outputs[1:]\n        output = {\'preprocessed_text\': text_preprocessed,\n                  \'logits\': [],\n                  \'alignment\': [],\n                  \'predicted_sequence\': []}\n        for _ in range(self.max_prediction_len):\n            de_output, de_state_h, de_state_c, alignment = self.decoder(de_input, (de_state_h, de_state_c),\n                                                                        en_outputs[0])\n            de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n            pred_token_index = de_input.numpy()[0][0]\n            if pred_token_index != 0:\n                output[\'logits\'].append(np.squeeze(de_output.numpy()))\n                output[\'alignment\'].append(np.squeeze(alignment.numpy()))\n                output[\'predicted_sequence\'].append(pred_token_index)\n                if pred_token_index == de_end_index:\n                    break\n        output[\'predicted_text\'] = self.vectorizer.decode_output(output[\'predicted_sequence\'])\n        return output\n\n    def new_train_step(self,\n                       loss_function: Callable[[tf.Tensor], tf.Tensor],\n                       batch_size: int,\n                       apply_gradients=True) -> Callable[[tf.Tensor, tf.Tensor], float]:\n\n        train_step_signature = [\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(batch_size, self.vectorizer.max_output_len), dtype=tf.int32),\n        ]\n        encoder = self.encoder\n        decoder = self.decoder\n        optimizer = self.optimizer\n\n        def train_step(source_seq, target_seq):\n            loss = 0\n            en_initial_states = encoder.init_states(batch_size)\n            with tf.GradientTape() as tape:\n                en_outputs = encoder(source_seq, en_initial_states)\n                en_states = en_outputs[1:]\n                de_state_h, de_state_c = en_states\n                for i in range(target_seq.shape[1] - 1):\n                    decoder_in = tf.expand_dims(target_seq[:, i], 1)\n                    logit, de_state_h, de_state_c, _ = decoder(\n                        decoder_in, (de_state_h, de_state_c), en_outputs[0])\n                    loss += loss_function(target_seq[:, i + 1], logit)\n            if apply_gradients is True:\n                variables = encoder.trainable_variables + decoder.trainable_variables\n                gradients = tape.gradient(loss, variables)\n                optimizer.apply_gradients(zip(gradients, variables))\n            return loss / (target_seq.shape[1] - 1)\n\n        if self.vectorizer.max_output_len is not None:\n            return tf.function(train_step, input_signature=train_step_signature)\n        else:\n            return train_step\n\n    def save(self, out_path: str) -> None:\n        if not os.path.exists(out_path):\n            os.mkdir(out_path)\n        summarizer_path = os.path.join(out_path, \'summarizer.pkl\')\n        encoder_path = os.path.join(out_path, \'encoder\')\n        decoder_path = os.path.join(out_path, \'decoder\')\n        with open(summarizer_path, \'wb+\') as handle:\n            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        self.encoder.save_weights(encoder_path, save_format=\'tf\')\n        self.decoder.save_weights(decoder_path, save_format=\'tf\')\n\n    @staticmethod\n    def load(in_path: str):\n        summarizer_path = os.path.join(in_path, \'summarizer.pkl\')\n        encoder_path = os.path.join(in_path, \'encoder\')\n        decoder_path = os.path.join(in_path, \'decoder\')\n        with open(summarizer_path, \'rb\') as handle:\n            summarizer = pickle.load(handle)\n        summarizer.encoder = Encoder(summarizer.embedding_shape_in,\n                                     summarizer.lstm_size,\n                                     embedding_trainable=summarizer.embedding_encoder_trainable)\n        summarizer.decoder = Decoder(summarizer.embedding_shape_out,\n                                     summarizer.lstm_size,\n                                     embedding_trainable=summarizer.embedding_decoder_trainable)\n        optimizer = AttentionSummarizer._new_optimizer()\n        summarizer.encoder.compile(optimizer=optimizer)\n        summarizer.decoder.compile(optimizer=optimizer)\n        summarizer.encoder.load_weights(encoder_path)\n        summarizer.decoder.load_weights(decoder_path)\n        summarizer.optimizer = summarizer.encoder.optimizer\n        return summarizer\n\n    @staticmethod\n    def _new_optimizer() -> tf.keras.optimizers.Optimizer:\n        return tf.keras.optimizers.Adam()\n'"
headliner/model/basic_model.py,12,"b'from typing import Tuple\n\nimport tensorflow as tf\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self,\n                 embedding_shape: Tuple[int, int],\n                 lstm_size=50,\n                 embedding_weights=None,\n                 embedding_trainable=True) -> None:\n        super(Encoder, self).__init__()\n        vocab_size, vec_dim = embedding_shape\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                   vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True, go_backwards=True)\n        self.lstm_size = lstm_size\n\n    def call(self,\n             sequence: tf.Tensor,\n             states: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n        embed = self.embedding(sequence)\n        output, state_h, state_c = self.lstm(embed, initial_state=states)\n        return output, state_h, state_c\n\n    def init_states(self, batch_size: int) -> Tuple[tf.Tensor, tf.Tensor]:\n        return tf.zeros([batch_size, self.lstm_size]), tf.zeros([batch_size, self.lstm_size])\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self,\n                 embedding_shape: Tuple[int, int],\n                 lstm_size=50,\n                 embedding_weights=None,\n                 embedding_trainable=True) -> None:\n        super(Decoder, self).__init__()\n        self.lstm_size = lstm_size\n        vocab_size, vec_dim = embedding_shape\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                   vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, sequence: tf.Tensor, state: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n        embed = self.embedding(sequence)\n        lstm_out, state_h, state_c = self.lstm(embed, state)\n        logits = self.dense(lstm_out)\n        return logits, state_h, state_c\n'"
headliner/model/basic_summarizer.py,13,"b'import os\nimport pickle\nfrom typing import Callable, Dict, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.model.basic_model import Encoder, Decoder\nfrom headliner.model.summarizer import Summarizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass BasicSummarizer(Summarizer):\n\n    def __init__(self, lstm_size=50, max_prediction_len=20, embedding_size=50, embedding_encoder_trainable=True,\n                 embedding_decoder_trainable=True):\n\n        super().__init__()\n        self.lstm_size = lstm_size\n        self.max_prediction_len = max_prediction_len\n        self.embedding_size = embedding_size\n        self.embedding_encoder_trainable = embedding_encoder_trainable\n        self.embedding_decoder_trainable = embedding_decoder_trainable\n        self.optimizer = BasicSummarizer._new_optimizer()\n        self.encoder = None\n        self.decoder = None\n        self.embedding_shape_in = None\n        self.embedding_shape_out = None\n\n    def init_model(self,\n                   preprocessor: Preprocessor,\n                   vectorizer: Vectorizer,\n                   embedding_weights_encoder=None,\n                   embedding_weights_decoder=None) -> None:\n        self.preprocessor = preprocessor\n        self.vectorizer = vectorizer\n        self.embedding_shape_in = (self.vectorizer.encoding_dim, self.embedding_size)\n        self.embedding_shape_out = (self.vectorizer.decoding_dim, self.embedding_size)\n        self.encoder = Encoder(self.embedding_shape_in,\n                               self.lstm_size,\n                               embedding_trainable=self.embedding_encoder_trainable,\n                               embedding_weights=embedding_weights_encoder)\n        self.decoder = Decoder(self.embedding_shape_out,\n                               self.lstm_size,\n                               embedding_trainable=self.embedding_decoder_trainable,\n                               embedding_weights=embedding_weights_decoder)\n        self.encoder.compile(optimizer=self.optimizer)\n        self.decoder.compile(optimizer=self.optimizer)\n\n    def __getstate__(self):\n        """""" Prevents pickle from serializing encoder and decoder. """"""\n        state = self.__dict__.copy()\n        del state[\'encoder\']\n        del state[\'decoder\']\n        del state[\'optimizer\']\n        return state\n\n    def predict(self, text: str) -> str:\n        return self.predict_vectors(text, \'\')[\'predicted_text\']\n\n    def predict_vectors(self, input_text: str, target_text: str) -> Dict[str, Union[str, np.array]]:\n        text_preprocessed = self.preprocessor((input_text, target_text))\n        en_inputs, _ = self.vectorizer(text_preprocessed)\n        en_initial_states = self.encoder.init_states(1)\n        en_outputs = self.encoder(tf.constant([en_inputs]), en_initial_states)\n        start_end_seq = self.vectorizer.encode_output(\n            \' \'.join([self.preprocessor.start_token, self.preprocessor.end_token]))\n        de_start_index, de_end_index = start_end_seq[:1], start_end_seq[-1:]\n        de_input = tf.constant([de_start_index])\n        de_state_h, de_state_c = en_outputs[1:]\n        output = {\'preprocessed_text\': text_preprocessed,\n                  \'logits\': [],\n                  \'alignment\': [],\n                  \'predicted_sequence\': []}\n        for _ in range(self.max_prediction_len):\n            de_output, de_state_h, de_state_c = self.decoder(de_input, (de_state_h, de_state_c))\n            de_input = tf.argmax(de_output, -1)\n            pred_token_index = de_input.numpy()[0][0]\n            if pred_token_index != 0:\n                output[\'logits\'].append(np.squeeze(de_output.numpy()))\n                output[\'predicted_sequence\'].append(pred_token_index)\n                if pred_token_index == de_end_index:\n                    break\n        output[\'predicted_text\'] = self.vectorizer.decode_output(output[\'predicted_sequence\'])\n        return output\n\n    def new_train_step(self,\n                       loss_function: Callable[[tf.Tensor], tf.Tensor],\n                       batch_size: int,\n                       apply_gradients=True) -> Callable[[tf.Tensor, tf.Tensor], float]:\n\n        train_step_signature = [\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n        ]\n        encoder = self.encoder\n        decoder = self.decoder\n        optimizer = self.optimizer\n\n        @tf.function(input_signature=train_step_signature)\n        def train_step(source_seq: tf.Tensor,\n                       target_seq: tf.Tensor) -> float:\n            en_initial_states = self.encoder.init_states(source_seq.get_shape()[0])\n            with tf.GradientTape() as tape:\n                en_outputs = encoder(source_seq, en_initial_states)\n                en_states = en_outputs[1:]\n                de_states = en_states\n                de_outputs = decoder(target_seq[:, :-1], de_states)\n                logits = de_outputs[0]\n                loss = loss_function(target_seq[:, 1:], logits)\n            if apply_gradients is True:\n                variables = encoder.trainable_variables + decoder.trainable_variables\n                gradients = tape.gradient(loss, variables)\n                optimizer.apply_gradients(zip(gradients, variables))\n            return float(loss)\n\n        return train_step\n\n    def save(self, out_path):\n        if not os.path.exists(out_path):\n            os.mkdir(out_path)\n        summarizer_path = os.path.join(out_path, \'summarizer.pkl\')\n        encoder_path = os.path.join(out_path, \'encoder\')\n        decoder_path = os.path.join(out_path, \'decoder\')\n        with open(summarizer_path, \'wb+\') as handle:\n            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        self.encoder.save_weights(encoder_path, save_format=\'tf\')\n        self.decoder.save_weights(decoder_path, save_format=\'tf\')\n\n    @staticmethod\n    def load(in_path):\n        summarizer_path = os.path.join(in_path, \'summarizer.pkl\')\n        encoder_path = os.path.join(in_path, \'encoder\')\n        decoder_path = os.path.join(in_path, \'decoder\')\n        with open(summarizer_path, \'rb\') as handle:\n            summarizer = pickle.load(handle)\n        summarizer.encoder = Encoder(summarizer.embedding_shape_in,\n                                     summarizer.lstm_size,\n                                     embedding_trainable=summarizer.embedding_encoder_trainable)\n        summarizer.decoder = Decoder(summarizer.embedding_shape_out,\n                                     summarizer.lstm_size,\n                                     embedding_trainable=summarizer.embedding_decoder_trainable)\n        optimizer = BasicSummarizer._new_optimizer()\n        summarizer.encoder.compile(optimizer=optimizer)\n        summarizer.decoder.compile(optimizer=optimizer)\n        summarizer.encoder.load_weights(encoder_path)\n        summarizer.decoder.load_weights(decoder_path)\n        summarizer.optimizer = summarizer.encoder.optimizer\n        return summarizer\n\n    @staticmethod\n    def _new_optimizer() -> tf.keras.optimizers.Optimizer:\n        return tf.keras.optimizers.Adam()\n'"
headliner/model/bert_model.py,13,"b""from typing import Tuple\n\nfrom transformers import TFBertModel\n\nfrom headliner.model.transformer_util import *\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self,\n                 num_layers: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 embedding_shape: Tuple[int, int],\n                 bert_embedding_name=None,\n                 embedding_trainable=True,\n                 embedding_weights=None,\n                 dropout_rate=0.1,\n                 max_seq_len=10000) -> None:\n        super(Encoder, self).__init__()\n\n        self.num_layers = num_layers\n        vocab_size, vec_dim = embedding_shape\n        self.embedding_size = vec_dim\n        self.bert_embedding_name = bert_embedding_name\n        if bert_embedding_name is not None:\n            self.embedding = TFBertModel.from_pretrained(bert_embedding_name)\n            self.embedding.trainable = embedding_trainable\n        else:\n            weights = None if embedding_weights is None else [embedding_weights]\n            self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                       vec_dim,\n                                                       weights=weights,\n                                                       trainable=embedding_trainable)\n        self.pos_encoding = positional_encoding(max_seq_len, self.embedding_size)\n        self.enc_layers = [EncoderLayer(vec_dim, num_heads, feed_forward_dim, dropout_rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, x, sent_ids, training, mask):\n        seq_len = tf.shape(x)[1]\n        # make attention mask consumable for huggingface transformes\n        # 1 for non-masked tokens, 0 for masked tokens\n        mask = mask[:, 0, 0, :]\n        attention_mask = tf.cast(tf.math.equal(mask, 0), tf.int32)\n        if self.bert_embedding_name is not None:\n            x = {'input_ids': x,\n                 'token_type_ids': sent_ids,\n                 'attention_mask': attention_mask}\n            x = self.embedding(x, training=training)[0]\n        else:\n            x = self.embedding(x)\n            x *= tf.math.sqrt(tf.cast(self.embedding_size, tf.float32))\n            x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n        return x\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self,\n                 num_layers: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 embedding_shape: Tuple[int, int],\n                 embedding_trainable=True,\n                 embedding_weights=None,\n                 dropout_rate=0.1,\n                 max_seq_len=10000) -> None:\n        super(Decoder, self).__init__()\n\n        self.num_layers = num_layers\n        vocab_size, vec_dim = embedding_shape\n        self.embedding_size = vec_dim\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                   vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.pos_encoding = positional_encoding(max_seq_len, vec_dim)\n        self.dec_layers = [DecoderLayer(vec_dim, num_heads, feed_forward_dim, dropout_rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.final_layer = tf.keras.layers.Dense(embedding_shape[0])\n\n    def call(self,\n             x,\n             enc_output,\n             training,\n             look_ahead_mask,\n             padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.embedding_size, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                   look_ahead_mask, padding_mask)\n            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n        x = self.final_layer(x)\n        return x, attention_weights\n\n\nclass Transformer(tf.keras.Model):\n\n    def __init__(self,\n                 num_layers_encoder: int,\n                 num_layers_decoder: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 embedding_shape_encoder: Tuple[int, int],\n                 embedding_shape_decoder: Tuple[int, int],\n                 bert_embedding_encoder=None,\n                 embedding_encoder_trainable=True,\n                 embedding_decoder_trainable=True,\n                 embedding_weights_encoder=None,\n                 embedding_weights_decoder=None,\n                 dropout_rate=0.1,\n                 max_seq_len=10000) -> None:\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers_encoder,\n                               num_heads,\n                               feed_forward_dim,\n                               embedding_shape_encoder,\n                               bert_embedding_name=bert_embedding_encoder,\n                               embedding_trainable=embedding_encoder_trainable,\n                               embedding_weights=embedding_weights_encoder,\n                               dropout_rate=dropout_rate,\n                               max_seq_len=max_seq_len)\n\n        self.decoder = Decoder(num_layers_decoder,\n                               num_heads,\n                               feed_forward_dim,\n                               embedding_shape_decoder,\n                               embedding_trainable=embedding_decoder_trainable,\n                               embedding_weights=embedding_weights_decoder,\n                               dropout_rate=dropout_rate,\n                               max_seq_len=max_seq_len)\n\n    def call(self,\n             inp,\n             sent_ids,\n             tar,\n             training,\n             enc_padding_mask,\n             look_ahead_mask,\n             dec_padding_mask):\n        enc_output = self.encoder(inp, sent_ids, training, enc_padding_mask)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        return dec_output, attention_weights\n"""
headliner/model/bert_summarizer.py,17,"b'import os\nimport pickle\nfrom typing import Callable\nfrom typing import Dict, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.model.bert_model import Transformer, create_masks\nfrom headliner.model.summarizer import Summarizer\nfrom headliner.preprocessing.bert_vectorizer import BertVectorizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.utils.logger import get_logger\n\n\nclass BertSummarizer(Summarizer):\n\n    def __init__(self,\n                 max_prediction_len=20,\n                 num_layers_encoder=1,\n                 num_layers_decoder=1,\n                 num_heads=2,\n                 feed_forward_dim=512,\n                 dropout_rate=0,\n                 embedding_size_encoder=768,\n                 embedding_size_decoder=64,\n                 bert_embedding_encoder=None,\n                 bert_embedding_decoder=None,\n                 embedding_encoder_trainable=True,\n                 embedding_decoder_trainable=True,\n                 max_sequence_len=10000):\n\n        super().__init__()\n        self.max_prediction_len = max_prediction_len\n        self.embedding_size_encoder = embedding_size_encoder\n        self.embedding_size_decoder = embedding_size_decoder\n        self.num_layers_encoder = num_layers_encoder\n        self.num_layers_decoder = num_layers_decoder\n        self.num_heads = num_heads\n        self.dropout_rate = dropout_rate\n        self.feed_forward_dim = feed_forward_dim\n        self.embedding_encoder_trainable = embedding_encoder_trainable\n        self.embedding_decoder_trainable = embedding_decoder_trainable\n        self.bert_embedding_encoder = bert_embedding_encoder\n        self.bert_embedding_decoder = bert_embedding_decoder\n        self.optimizer_encoder = BertSummarizer.new_optimizer_encoder()\n        self.optimizer_decoder = BertSummarizer.new_optimizer_decoder()\n        self.transformer = None\n        self.embedding_shape_in = None\n        self.embedding_shape_out = None\n        self.max_sequence_len = max_sequence_len\n        self.logger = get_logger(__name__)\n\n    def __getstate__(self):\n        """""" Prevents pickle from serializing the transformer and optimizer """"""\n        state = self.__dict__.copy()\n        del state[\'transformer\']\n        del state[\'logger\']\n        del state[\'optimizer_encoder\']\n        del state[\'optimizer_decoder\']\n        return state\n\n    def init_model(self,\n                   preprocessor: Preprocessor,\n                   vectorizer: BertVectorizer,\n                   embedding_weights_encoder=None,\n                   embedding_weights_decoder=None\n                   ) -> None:\n        self.preprocessor = preprocessor\n        self.vectorizer = vectorizer\n        self.embedding_shape_in = (self.vectorizer.encoding_dim, self.embedding_size_encoder)\n        self.embedding_shape_out = (self.vectorizer.decoding_dim, self.embedding_size_decoder)\n        self.transformer = Transformer(num_layers_encoder=self.num_layers_encoder,\n                                       num_layers_decoder=self.num_layers_decoder,\n                                       num_heads=self.num_heads,\n                                       feed_forward_dim=self.feed_forward_dim,\n                                       embedding_shape_encoder=self.embedding_shape_in,\n                                       embedding_shape_decoder=self.embedding_shape_out,\n                                       bert_embedding_encoder=self.bert_embedding_encoder,\n                                       embedding_encoder_trainable=self.embedding_encoder_trainable,\n                                       embedding_decoder_trainable=self.embedding_decoder_trainable,\n                                       embedding_weights_encoder=embedding_weights_encoder,\n                                       embedding_weights_decoder=embedding_weights_decoder,\n                                       dropout_rate=self.dropout_rate,\n                                       max_seq_len=self.max_sequence_len)\n        self.transformer.encoder.compile(optimizer=self.optimizer_encoder)\n        self.transformer.decoder.compile(optimizer=self.optimizer_decoder)\n\n    def new_train_step(self,\n                       loss_function: Callable[[tf.Tensor], tf.Tensor],\n                       batch_size: int,\n                       apply_gradients=True):\n\n        transformer = self.transformer\n        optimizer_encoder = self.optimizer_encoder\n        optimizer_decoder = self.optimizer_decoder\n\n        train_step_signature = [\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n        ]\n\n        @tf.function(input_signature=train_step_signature)\n        def train_step(inp, sent_ids, tar):\n            tar_inp = tar[:, :-1]\n            tar_real = tar[:, 1:]\n            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n            with tf.GradientTape(persistent=True) as tape:\n                predictions, _ = transformer(inp,\n                                             sent_ids,\n                                             tar_inp,\n                                             True,\n                                             enc_padding_mask,\n                                             combined_mask,\n                                             dec_padding_mask)\n                loss = loss_function(tar_real, predictions)\n            if apply_gradients:\n                gradients_decoder = tape.gradient(loss, transformer.decoder.trainable_variables)\n                gradients_encoder = tape.gradient(loss, transformer.encoder.trainable_variables)\n                optimizer_decoder.apply_gradients(zip(gradients_decoder, transformer.decoder.trainable_variables))\n                optimizer_encoder.apply_gradients(zip(gradients_encoder, transformer.encoder.trainable_variables))\n\n            return loss\n\n        return train_step\n\n    def predict(self, text: str) -> str:\n        return self.predict_vectors(text, \'\')[\'predicted_text\']\n\n    def predict_vectors(self, input_text: str, target_text: str) -> Dict[str, Union[str, np.array]]:\n        text_preprocessed = self.preprocessor((input_text, target_text))\n        en_inputs, sent_ids, _ = self.vectorizer(text_preprocessed)\n        en_inputs = tf.expand_dims(en_inputs, 0)\n        sent_ids = tf.expand_dims(sent_ids, 0)\n        start_end_seq = self.vectorizer.encode_output(\n            \' \'.join([self.preprocessor.start_token, self.preprocessor.end_token]))\n        de_start_index, de_end_index = start_end_seq[:1], start_end_seq[-1:]\n        decoder_output = tf.expand_dims(de_start_index, 0)\n        output = {\'preprocessed_text\': text_preprocessed,\n                  \'logits\': [],\n                  \'attention_weights\': [],\n                  \'predicted_sequence\': []}\n        for _ in range(self.max_prediction_len):\n            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n                en_inputs, decoder_output)\n            predictions, attention_weights = self.transformer(en_inputs,\n                                                              sent_ids,\n                                                              decoder_output,\n                                                              False,\n                                                              enc_padding_mask,\n                                                              combined_mask,\n                                                              dec_padding_mask)\n\n            predictions = predictions[:, -1:, :]\n            pred_token_index = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n            decoder_output = tf.concat([decoder_output, pred_token_index], axis=-1)\n            if pred_token_index != 0:\n                output[\'logits\'].append(np.squeeze(predictions.numpy()))\n                output[\'attention_weights\'] = attention_weights\n                output[\'predicted_sequence\'].append(int(pred_token_index))\n                if pred_token_index == de_end_index:\n                    break\n        output[\'predicted_text\'] = self.vectorizer.decode_output(output[\'predicted_sequence\'])\n        return output\n\n    def save(self, out_path: str) -> None:\n        if not os.path.exists(out_path):\n            os.mkdir(out_path)\n        summarizer_path = os.path.join(out_path, \'summarizer.pkl\')\n        encoder_path = os.path.join(out_path, \'encoder\')\n        decoder_path = os.path.join(out_path, \'decoder\')\n        with open(summarizer_path, \'wb+\') as handle:\n            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        self.transformer.encoder.save_weights(encoder_path, save_format=\'tf\')\n        self.transformer.decoder.save_weights(decoder_path, save_format=\'tf\')\n\n    @staticmethod\n    def load(in_path: str):\n        summarizer_path = os.path.join(in_path, \'summarizer.pkl\')\n        encoder_path = os.path.join(in_path, \'encoder\')\n        decoder_path = os.path.join(in_path, \'decoder\')\n        with open(summarizer_path, \'rb\') as handle:\n            summarizer = pickle.load(handle)\n        summarizer.logger = get_logger(__name__)\n        summarizer.transformer = Transformer(num_layers_encoder=summarizer.num_layers_encoder,\n                                             num_layers_decoder=summarizer.num_layers_decoder,\n                                             num_heads=summarizer.num_heads,\n                                             feed_forward_dim=summarizer.feed_forward_dim,\n                                             embedding_shape_encoder=summarizer.embedding_shape_in,\n                                             embedding_shape_decoder=summarizer.embedding_shape_out,\n                                             bert_embedding_encoder=summarizer.bert_embedding_encoder,\n                                             embedding_encoder_trainable=summarizer.embedding_encoder_trainable,\n                                             embedding_decoder_trainable=summarizer.embedding_decoder_trainable,\n                                             dropout_rate=summarizer.dropout_rate)\n        optimizer_encoder = BertSummarizer.new_optimizer_encoder()\n        optimizer_decoder = BertSummarizer.new_optimizer_decoder()\n        summarizer.transformer.encoder.compile(optimizer=optimizer_encoder)\n        summarizer.transformer.decoder.compile(optimizer=optimizer_decoder)\n        summarizer.transformer.encoder.load_weights(encoder_path)\n        summarizer.transformer.decoder.load_weights(decoder_path)\n        summarizer.optimizer_encoder = summarizer.transformer.encoder.optimizer\n        summarizer.optimizer_decoder = summarizer.transformer.decoder.optimizer\n        return summarizer\n\n    @staticmethod\n    def new_optimizer_decoder(learning_rate_start=0.02, warmup_steps=10000) -> tf.keras.optimizers.Optimizer:\n        learning_rate = CustomSchedule(warmup_steps=warmup_steps, learning_rate_start=learning_rate_start)\n        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999, )\n        return optimizer\n\n    @staticmethod\n    def new_optimizer_encoder(learning_rate_start=5e-4, warmup_steps=20000) -> tf.keras.optimizers.Optimizer:\n        learning_rate = CustomSchedule(warmup_steps=warmup_steps, learning_rate_start=learning_rate_start)\n        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999, )\n        return optimizer\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n    def __init__(self, warmup_steps=10000, learning_rate_start=1e-1):\n        super(CustomSchedule, self).__init__()\n        self.warmup_steps = warmup_steps\n        self.learning_rate_start = learning_rate_start\n\n    def __call__(self, step):\n        arg1 = step ** -0.5\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return self.learning_rate_start * tf.math.minimum(arg1, arg2)\n'"
headliner/model/summarizer.py,2,"b'import abc\nfrom abc import abstractmethod\nfrom typing import Callable, Dict, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.preprocessing import Preprocessor, Vectorizer\n\n\nclass Summarizer(abc.ABC):\n\n    def __init__(self):\n        self.vectorizer: Union[Vectorizer, None] = None\n        self.preprocessor: Union[Preprocessor, None] = None\n        self.embedding_size: Union[int, None] = None\n\n    @abstractmethod\n    def init_model(self,\n                   preprocessor: Preprocessor,\n                   vectorizer: Vectorizer,\n                   embedding_weights_encoder=None,\n                   embedding_weights_decoder=None) -> None:\n        """"""\n        Initializes the model and provides necessary information for compilation.\n\n        Args:\n            preprocessor: Preprocessor object that preprocesses text for training and prediction.\n            vectorizer: Vectorizer object that performs vectorization of the text.\n            embedding_weights_encoder (optional): Matrix to initialize the encoder embedding.\n            embedding_weights_decoder (optional): Matrix to initialize the decoder embedding.\n        """"""\n\n        pass\n\n    @abstractmethod\n    def predict(self, text: str) -> str:\n        """"""\n        Predicts summary of an input text.\n        """"""\n\n        pass\n\n    @abstractmethod\n    def predict_vectors(self, input_text: str, target_text: str) -> Dict[str, Union[str, np.array]]:\n        """"""\n        Predicts summary of an input text and outputs information needed for evaluation:\n        output logits, input tokens, output tokens, predicted tokens, preprocessed text,\n        attention alignment.\n\n        Args:\n            input_text: Text used as input for prediction.\n            target_text: Text used for evaluation.\n\n        Returns: Dictionary with prediction information such as\n            preprocessed_text, logits, alignment, predicted_sequence, predicted_text.\n\n        """"""\n\n        pass\n\n    @abstractmethod\n    def new_train_step(self,\n                       loss_function: Callable[[tf.Tensor], tf.Tensor],\n                       batch_size: int,\n                       apply_gradients=True) -> Callable[[tf.Tensor, tf.Tensor], float]:\n        """"""\n        Initializes the train_step function to train the model on batches of data.\n\n        Args:\n            loss_function: Loss function to perform backprop on.\n            batch_size: Batch size to use for training.\n            apply_gradients: Whether to apply the gradients, i.e.\n                False if you want to validate the model on test data.\n\n        Returns: Train step function that is applied to a batch and returns the loss.\n\n        """"""\n\n        pass\n\n    @abstractmethod\n    def save(self, out_path: str) -> None:\n        """"""\n        Saves the model to a file.\n\n        Args:\n            out_path: Path to directory for saving the model.\n\n        """"""\n\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def load(in_path: str):\n        """"""\n        Loads the model from a file.\n\n        Args:\n            in_path: Path to the model directory.\n\n        Returns: Instance of the loaded summarizer.\n\n        """"""\n\n        pass\n'"
headliner/model/transformer_model.py,12,"b""from typing import Tuple\n\nfrom headliner.model.transformer_util import *\n\n\nclass Encoder(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 num_layers: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 embedding_shape: Tuple[int, int],\n                 embedding_trainable=True,\n                 embedding_weights=None,\n                 dropout_rate=0.1,\n                 max_seq_len=10000) -> None:\n        super(Encoder, self).__init__()\n\n        self.num_layers = num_layers\n        vocab_size, vec_dim = embedding_shape\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding_size = vec_dim\n        self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                   vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.pos_encoding = positional_encoding(max_seq_len, self.embedding_size)\n        self.enc_layers = [EncoderLayer(vec_dim, num_heads, feed_forward_dim, dropout_rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.embedding_size, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n        return x\n\n\nclass Decoder(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 num_layers: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 embedding_shape: Tuple[int, int],\n                 embedding_trainable=True,\n                 embedding_weights=None,\n                 dropout_rate=0.1,\n                 max_seq_len=10000) -> None:\n        super(Decoder, self).__init__()\n\n        self.num_layers = num_layers\n        vocab_size, vec_dim = embedding_shape\n        weights = None if embedding_weights is None else [embedding_weights]\n        self.embedding_size = vec_dim\n        self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                   vec_dim,\n                                                   weights=weights,\n                                                   trainable=embedding_trainable)\n        self.pos_encoding = positional_encoding(max_seq_len, vec_dim)\n        self.dec_layers = [DecoderLayer(vec_dim, num_heads, feed_forward_dim, dropout_rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self,\n             x,\n             enc_output,\n             training,\n             look_ahead_mask,\n             padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.embedding_size, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                   look_ahead_mask, padding_mask)\n            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n\n        return x, attention_weights\n\n\nclass Transformer(tf.keras.Model):\n\n    def __init__(self,\n                 num_layers: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 embedding_shape_encoder: Tuple[int, int],\n                 embedding_shape_decoder: Tuple[int, int],\n                 embedding_encoder_trainable=True,\n                 embedding_decoder_trainable=True,\n                 embedding_weights_encoder=None,\n                 embedding_weights_decoder=None,\n                 dropout_rate=0.1,\n                 max_sequence_len=10000) -> None:\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers,\n                               num_heads,\n                               feed_forward_dim,\n                               embedding_shape_encoder,\n                               embedding_trainable=embedding_encoder_trainable,\n                               embedding_weights=embedding_weights_encoder,\n                               dropout_rate=dropout_rate,\n                               max_seq_len=max_sequence_len)\n\n        self.decoder = Decoder(num_layers,\n                               num_heads,\n                               feed_forward_dim,\n                               embedding_shape_decoder,\n                               embedding_trainable=embedding_decoder_trainable,\n                               embedding_weights=embedding_weights_decoder,\n                               dropout_rate=dropout_rate,\n                               max_seq_len=max_sequence_len)\n\n        self.final_layer = tf.keras.layers.Dense(embedding_shape_decoder[0])\n\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n        return final_output, attention_weights\n"""
headliner/model/transformer_summarizer.py,11,"b'import os\nimport pickle\nfrom typing import Callable\nfrom typing import Dict, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.model.summarizer import Summarizer\nfrom headliner.model.transformer_model import Transformer, create_masks\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass TransformerSummarizer(Summarizer):\n\n    def __init__(self,\n                 max_prediction_len=20,\n                 num_layers=1,\n                 num_heads=2,\n                 feed_forward_dim=512,\n                 dropout_rate=0,\n                 embedding_size=128,\n                 embedding_encoder_trainable=True,\n                 embedding_decoder_trainable=True,\n                 max_sequence_len=10000):\n\n        super().__init__()\n        self.max_prediction_len = max_prediction_len\n        self.embedding_size = embedding_size\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.dropout_rate = dropout_rate\n        self.feed_forward_dim = feed_forward_dim\n        self.embedding_encoder_trainable = embedding_encoder_trainable\n        self.embedding_decoder_trainable = embedding_decoder_trainable\n        self.optimizer = TransformerSummarizer.new_optimizer()\n        self.transformer = None\n        self.embedding_shape_in = None\n        self.embedding_shape_out = None\n        self.max_sequence_len = max_sequence_len\n\n    def __getstate__(self):\n        """""" Prevents pickle from serializing the transformer and optimizer """"""\n        state = self.__dict__.copy()\n        del state[\'transformer\']\n        del state[\'optimizer\']\n        return state\n\n    def init_model(self,\n                   preprocessor: Preprocessor,\n                   vectorizer: Vectorizer,\n                   embedding_weights_encoder=None,\n                   embedding_weights_decoder=None) -> None:\n        self.preprocessor = preprocessor\n        self.vectorizer = vectorizer\n        self.embedding_shape_in = (self.vectorizer.encoding_dim, self.embedding_size)\n        self.embedding_shape_out = (self.vectorizer.decoding_dim, self.embedding_size)\n        self.transformer = Transformer(num_layers=self.num_layers,\n                                       num_heads=self.num_heads,\n                                       feed_forward_dim=self.feed_forward_dim,\n                                       embedding_shape_encoder=self.embedding_shape_in,\n                                       embedding_shape_decoder=self.embedding_shape_out,\n                                       embedding_encoder_trainable=self.embedding_encoder_trainable,\n                                       embedding_decoder_trainable=self.embedding_decoder_trainable,\n                                       embedding_weights_encoder=embedding_weights_encoder,\n                                       embedding_weights_decoder=embedding_weights_decoder,\n                                       dropout_rate=self.dropout_rate,\n                                       max_sequence_len=self.max_sequence_len)\n        self.transformer.compile(optimizer=self.optimizer)\n\n    def new_train_step(self,\n                       loss_function: Callable[[tf.Tensor], tf.Tensor],\n                       batch_size: int,\n                       apply_gradients=True):\n\n        transformer = self.transformer\n        optimizer = self.optimizer\n\n        train_step_signature = [\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n            tf.TensorSpec(shape=(batch_size, None), dtype=tf.int32),\n        ]\n\n        @tf.function(input_signature=train_step_signature)\n        def train_step(inp, tar):\n            tar_inp = tar[:, :-1]\n            tar_real = tar[:, 1:]\n            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n            with tf.GradientTape() as tape:\n                predictions, _ = transformer(inp, tar_inp,\n                                             True,\n                                             enc_padding_mask,\n                                             combined_mask,\n                                             dec_padding_mask)\n                loss = loss_function(tar_real, predictions)\n            if apply_gradients:\n                gradients = tape.gradient(loss, transformer.trainable_variables)\n                optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n            return loss\n\n        return train_step\n\n    def predict(self, text: str) -> str:\n        return self.predict_vectors(text, \'\')[\'predicted_text\']\n\n    def predict_vectors(self, input_text: str, target_text: str) -> Dict[str, Union[str, np.array]]:\n        text_preprocessed = self.preprocessor((input_text, target_text))\n        en_inputs, _ = self.vectorizer(text_preprocessed)\n        en_inputs = tf.expand_dims(en_inputs, 0)\n        start_end_seq = self.vectorizer.encode_output(\n            \' \'.join([self.preprocessor.start_token, self.preprocessor.end_token]))\n        de_start_index, de_end_index = start_end_seq[:1], start_end_seq[-1:]\n        decoder_output = tf.expand_dims(de_start_index, 0)\n        output = {\'preprocessed_text\': text_preprocessed,\n                  \'logits\': [],\n                  \'attention_weights\': [],\n                  \'predicted_sequence\': []}\n        for _ in range(self.max_prediction_len):\n            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n                en_inputs, decoder_output)\n            predictions, attention_weights = self.transformer(en_inputs,\n                                                              decoder_output,\n                                                              False,\n                                                              enc_padding_mask,\n                                                              combined_mask,\n                                                              dec_padding_mask)\n\n            predictions = predictions[:, -1:, :]\n            pred_token_index = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n            decoder_output = tf.concat([decoder_output, pred_token_index], axis=-1)\n            if pred_token_index != 0:\n                output[\'logits\'].append(np.squeeze(predictions.numpy()))\n                output[\'attention_weights\'] = attention_weights\n                output[\'predicted_sequence\'].append(int(pred_token_index))\n                if pred_token_index == de_end_index:\n                    break\n        output[\'predicted_text\'] = self.vectorizer.decode_output(output[\'predicted_sequence\'])\n        return output\n\n    def save(self, out_path: str) -> None:\n        if not os.path.exists(out_path):\n            os.mkdir(out_path)\n        summarizer_path = os.path.join(out_path, \'summarizer.pkl\')\n        transformer_path = os.path.join(out_path, \'transformer\')\n        with open(summarizer_path, \'wb+\') as handle:\n            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        self.transformer.save_weights(transformer_path, save_format=\'tf\')\n\n    @staticmethod\n    def load(in_path: str):\n        summarizer_path = os.path.join(in_path, \'summarizer.pkl\')\n        transformer_path = os.path.join(in_path, \'transformer\')\n        with open(summarizer_path, \'rb\') as handle:\n            summarizer = pickle.load(handle)\n        summarizer.transformer = Transformer(num_layers=summarizer.num_layers,\n                                             num_heads=summarizer.num_heads,\n                                             feed_forward_dim=summarizer.feed_forward_dim,\n                                             embedding_shape_encoder=summarizer.embedding_shape_in,\n                                             embedding_shape_decoder=summarizer.embedding_shape_out,\n                                             embedding_encoder_trainable=summarizer.embedding_encoder_trainable,\n                                             embedding_decoder_trainable=summarizer.embedding_decoder_trainable,\n                                             dropout_rate=summarizer.dropout_rate)\n        optimizer = TransformerSummarizer.new_optimizer()\n        summarizer.transformer.compile(optimizer=optimizer)\n        summarizer.transformer.load_weights(transformer_path)\n        summarizer.optimizer = summarizer.transformer.optimizer\n        return summarizer\n\n    @staticmethod\n    def new_optimizer() -> tf.keras.optimizers.Optimizer:\n        return tf.keras.optimizers.Adam()\n'"
headliner/model/transformer_util.py,36,"b""import numpy as np\nimport tensorflow as tf\n\n\ndef get_angles(pos, i, embedding_size):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embedding_size))\n    return pos * angle_rates\n\n\ndef positional_encoding(max_len, embedding_size):\n    angle_rads = get_angles(np.arange(max_len)[:, np.newaxis],\n                            np.arange(embedding_size)[np.newaxis, :],\n                            embedding_size)\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]\n\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n\n    return output, attention_weights\n\n\ndef point_wise_feed_forward_network(embedding_size, feed_forward_dim):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(feed_forward_dim, activation='relu'),\n        tf.keras.layers.Dense(embedding_size)\n    ])\n\n\ndef create_masks(inp, tar):\n    enc_padding_mask = create_padding_mask(inp)\n    dec_padding_mask = create_padding_mask(inp)\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n    return enc_padding_mask, combined_mask, dec_padding_mask\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 embedding_size: int,\n                 num_heads: int) -> None:\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.embedding_size = embedding_size\n        assert embedding_size % self.num_heads == 0\n        self.depth = embedding_size // self.num_heads\n        self.wq = tf.keras.layers.Dense(embedding_size)\n        self.wk = tf.keras.layers.Dense(embedding_size)\n        self.wv = tf.keras.layers.Dense(embedding_size)\n        self.dense = tf.keras.layers.Dense(embedding_size)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_size))\n        output = self.dense(concat_attention)\n        return output, attention_weights\n\n\nclass EncoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 embedding_size: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 dropout_rate=0.1) -> None:\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(embedding_size, num_heads)\n        self.ffn = point_wise_feed_forward_network(embedding_size, feed_forward_dim)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, embedding_size)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, embedding_size)\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, embedding_size)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, embedding_size)\n        return out2\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 embedding_size: int,\n                 num_heads: int,\n                 feed_forward_dim: int,\n                 dropout_rate=0.1) -> None:\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(embedding_size, num_heads)\n        self.mha2 = MultiHeadAttention(embedding_size, num_heads)\n        self.ffn = point_wise_feed_forward_network(embedding_size, feed_forward_dim)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self,\n             x,\n             enc_output,\n             training,\n             look_ahead_mask,\n             padding_mask):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n\n        return out3, attn_weights_block1, attn_weights_block2\n"""
headliner/preprocessing/__init__.py,0,b'from .bucket_generator import BucketGenerator\nfrom .dataset_generator import DatasetGenerator\nfrom .preprocessor import Preprocessor\nfrom .vectorizer import Vectorizer\n'
headliner/preprocessing/bert_preprocessor.py,0,"b'from typing import Tuple\n\nfrom spacy.pipeline.pipes import Language\n\n\nclass BertPreprocessor:\n\n    def __init__(self,\n                 nlp: Language):\n        """"""\n        Initializes the preprocessor.\n\n        Args:\n            nlp: Spacy natural language processing pipeline.\n        """"""\n        self.nlp = nlp\n        pipe = self.nlp.create_pipe(\'sentencizer\')\n        self.nlp.add_pipe(pipe)\n        self.start_token = \'[CLS]\'\n        self.end_token = \'[SEP]\'\n\n    def __call__(self, data: Tuple[str, str]) -> Tuple[str, str]:\n        """""" Splits input text into sentences and adds start and end token to each sentence. """"""\n        text_encoder, text_decoder = data[0], data[1]\n        doc = self.nlp(text_encoder)\n        sentences = [self._process_sentence(s) for s in doc.sents]\n        text_encoder = \' \'.join(sentences)\n        text_decoder = self.start_token + \' \' + text_decoder + \' \' + self.end_token\n        return text_encoder, text_decoder\n\n    def _process_sentence(self, sentence):\n        return self.start_token + \' \' + sentence.string.strip() + \' \' + self.end_token\n'"
headliner/preprocessing/bert_vectorizer.py,0,"b'from typing import Tuple, List\n\nfrom transformers import BertTokenizer\n\nfrom headliner.preprocessing.tokenizer import Tokenizer\n\n\nclass BertVectorizer:\n    """"""\n    Transforms tuples of text into tuples of vector sequences.\n    """"""\n\n    def __init__(self,\n                 tokenizer_encoder: BertTokenizer,\n                 tokenizer_decoder: Tokenizer,\n                 max_input_len=512,\n                 max_output_len=None) -> None:\n        """"""\n        Initializes the vectorizer.\n\n        Args:\n            tokenizer_encoder: Tokenizer that encodes the input text.\n            tokenizer_decoder: Tokenizer that encodes the target text.\n            max_input_len (optional): Maximum length of input sequence,\n                longer sequences will be truncated.\n            max_output_len (optional): Maximum length of target sequence,\n                longer sequences will be truncated and shorter sequences\n                will be padded to max len.\n        """"""\n        self.encoding_dim = tokenizer_encoder.vocab_size + 1\n        self.decoding_dim = tokenizer_decoder.vocab_size + 1\n        self.max_input_len = max_input_len\n        self.max_output_len = max_output_len\n        self._tokenizer_encoder = tokenizer_encoder\n        self._tokenizer_decoder = tokenizer_decoder\n\n    def __call__(self, data: Tuple[str, str]) -> Tuple[List[int], List[int], List[int]]:\n        """"""\n        Encodes preprocessed strings into sequences of one-hot indices.\n        """"""\n        text_encoder, text_decoder = data[0], data[1]\n        sentences = text_encoder.split(\'[SEP]\')[:-1]\n        vec_encoder = []\n        sentence_ids = []\n        for i, sent in enumerate(sentences):\n            sent = sent + \'[SEP]\'\n            vec = self._tokenizer_encoder.encode(sent, add_special_tokens=False)\n            if len(vec_encoder) + len(vec) < self.max_input_len:\n                vec_encoder.extend(vec)\n                ids = [i % 2] * len(vec)\n                sentence_ids.extend(ids)\n\n        vec_decoder = self._tokenizer_decoder.encode(text_decoder)\n        if self.max_output_len is not None:\n            if len(vec_decoder) > self.max_output_len:\n                vec_decoder = vec_decoder[:self.max_output_len - 1] + [vec_decoder[-1]]\n            else:\n                vec_decoder = vec_decoder + [0] * (self.max_output_len - len(vec_decoder))\n\n        return vec_encoder, sentence_ids, vec_decoder\n\n    def encode_input(self, text: str) -> List[int]:\n        return self._tokenizer_encoder.encode(text, add_special_tokens=False)\n\n    def encode_output(self, text: str) -> List[int]:\n        return self._tokenizer_decoder.encode(text)\n\n    def decode_input(self, sequence: List[int]) -> str:\n        return self._tokenizer_encoder.decode(sequence)\n\n    def decode_output(self, sequence: List[int]) -> str:\n        return self._tokenizer_decoder.decode(sequence)\n'"
headliner/preprocessing/bucket_generator.py,0,"b'from random import Random\nfrom typing import Iterable, Callable, Iterator, List\n\n\nclass BucketGenerator:\n    """"""\n    Performs bucketing of elements in a dataset by length.\n    """"""\n\n    def __init__(self,\n                 element_length_function: Callable[..., int],\n                 batch_size: int,\n                 buffer_size_batches=1000,\n                 batches_to_bucket=10,\n                 shuffle=True,\n                 seed=None) -> None:\n        """"""\n        Initializes the BucketGenerator.\n\n        Args:\n            element_length_function: Element_length_function: function from element in the dataset to int that\n            determines the length of the element.\n            batch_size: The size of the batches to bucket the sequences into\n                buffer_size_batches: buffer_size_batches: number of batches to keep in internal memory.\n            batches_to_bucket: Number of batches in buffer to use for bucketing.\n                If set to buffer_size_batches, the resulting batches will be deterministic.\n            shuffle: Whether to shuffle elements across batches and the resulting buckets.\n            seed: Seed for shuffling.\n        """"""\n\n        self.sequence_length_function = element_length_function\n        self.batch_size = batch_size\n        self.buffer_size_batches = buffer_size_batches\n        self.batches_to_shuffle = batches_to_bucket\n        self.shuffle = shuffle\n        self.random = Random(seed)\n\n    def __call__(self, data: Iterable) -> Iterable:\n        """"""\n        Returns iterable of data with elements ordered by bucketed sequence lengths, e.g for batch size = 2 the\n        transformation could look like this:\n        [1], [3, 3, 3], [1], [4, 4, 4, 4] -> [1], [1], [3, 3, 3], [4, 4, 4, 4]\n        """"""\n        data_iter = iter(data)\n        bucket_gen = self._generate_buckets(data_iter)\n        return bucket_gen\n\n    def _generate_buckets(self, data_iter: Iterator) -> List[List]:\n        buffered_data = self._fetch_buffered_data(data_iter)\n        while len(buffered_data) > 0:\n            buckets = self._to_buckets(buffered_data)\n            for bucket in buckets:\n                for element in bucket:\n                    yield element\n            buffered_data = self._fetch_buffered_data(data_iter)\n            del buckets\n\n    def _to_buckets(self, buffered_data: List) -> List[List]:\n        self._shuffle_if_required(buffered_data)\n        buffered_data = self._sort_blocks(buffered_data)\n        buckets = []\n        for i in range(0, len(buffered_data), self.batch_size):\n            bucket = buffered_data[i:i + self.batch_size]\n            if len(bucket) == self.batch_size:\n                buckets.append(bucket)\n        self._shuffle_if_required(buckets)\n        return buckets\n\n    def _sort_blocks(self, buffered_data: List) -> List:\n        block_size = self.batches_to_shuffle * self.batch_size\n        buffered_data_sorted = []\n        for i in range(0, len(buffered_data), block_size):\n            sorted_block = buffered_data[i:i + block_size]\n            sorted_block.sort(key=self.sequence_length_function)\n            buffered_data_sorted.extend(sorted_block)\n        return buffered_data_sorted\n\n    def _fetch_buffered_data(self, data_iter: Iterator):\n        buffered_data = []\n        for _ in range(self.buffer_size_batches * self.batch_size):\n            try:\n                buffered_data.append(next(data_iter))\n            except StopIteration:\n                pass\n        return buffered_data\n\n    def _shuffle_if_required(self, list_to_shuffle):\n        if self.shuffle:\n            self.random.shuffle(list_to_shuffle)\n'"
headliner/preprocessing/dataset_generator.py,6,"b'from typing import Iterable, Callable\n\nimport tensorflow as tf\n\n\nclass DatasetGenerator:\n\n    def __init__(self,\n                 batch_size: int,\n                 shuffle_buffer_size=None,\n                 rank=2):\n        self.batch_size = batch_size\n        self.shuffle_buffer_size = shuffle_buffer_size\n        if rank == 2:\n            self.tensor_types = (tf.int32, tf.int32)\n            self.tensor_shapes = (tf.TensorShape([None]), tf.TensorShape([None]))\n        elif rank == 3:\n            self.tensor_types = (tf.int32, tf.int32, tf.int32)\n            self.tensor_shapes = (tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None]))\n        else:\n            raise ValueError(\'Rank must be either 2 or 3, but was: {}\'.format(rank))\n\n    def __call__(self, data_generator_func: Callable[..., Iterable]) -> tf.data.Dataset:\n        """"""\n        Initializes a dataset generator.\n\n        Args:\n            data_generator_func: Callable that returns an iterable over the data to be batched, e.g. lambda: [1, 2, 3].\n        """"""\n\n        dataset = tf.data.Dataset.from_generator(data_generator_func,\n                                                 self.tensor_types,\n                                                 self.tensor_shapes)\n        if self.shuffle_buffer_size is not None:\n            dataset = dataset.shuffle(self.shuffle_buffer_size)\n        dataset = dataset.padded_batch(batch_size=self.batch_size,\n                                       padded_shapes=self.tensor_shapes,\n                                       drop_remainder=True)\n        return dataset\n'"
headliner/preprocessing/keras_tokenizer.py,0,"b'from typing import List, Iterable, Dict\n\nfrom keras_preprocessing.text import Tokenizer as KTokenizer\n\nfrom headliner.preprocessing.tokenizer import Tokenizer\n\n\nclass KerasTokenizer(Tokenizer):\n\n    def __init__(self, **kwargs):\n        self._keras_tokenizer = KTokenizer(**kwargs)\n\n    def encode(self, text: str) -> List[int]:\n        return self._keras_tokenizer.texts_to_sequences([text])[0]\n\n    def decode(self, sequence: List[int]) -> str:\n        return self._keras_tokenizer.sequences_to_texts([sequence])[0]\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self._keras_tokenizer.word_index)\n\n    def fit(self, texts: Iterable[str]):\n        self._keras_tokenizer.fit_on_texts(texts)\n\n    @property\n    def token_index(self) -> Dict[str, int]:\n        return self._keras_tokenizer.word_index\n'"
headliner/preprocessing/preprocessor.py,0,"b'import re\nfrom typing import Tuple\n\n\nclass Preprocessor:\n\n    def __init__(self,\n                 start_token=\'<start>\',\n                 end_token=\'<end>\',\n                 punctuation_pattern=\'([!.?,])\',\n                 filter_pattern=\'([""#$%&()*+/:;<=>@[\\\\]^_`{|}~\\t\\n])\',\n                 add_input_start_end=True,\n                 lower_case=True,\n                 hash_numbers=True):\n        """"""\n        Initializes the preprocessor.\n\n        Args:\n            start_token: Unique start token to be inserted at the beginning of the target text.\n            end_token: Unique end token to be attached at the end of a target text.\n            punctuation_pattern: Regex pattern for punktuation that is splitted from the tokens.\n            filter_pattern: Regex pattern for characters to be removed from the text.\n            add_input_start_end: Whether to add start and end token to input sequence.\n            lower_case: Whether to perform lower casing.\n            hash_numbers: Whether to replace numbers by a #.\n        """"""\n        self.start_token = start_token\n        self.end_token = end_token\n        self.punctuation_pattern = punctuation_pattern\n        self.filter_pattern = filter_pattern\n        self.add_input_start_end = add_input_start_end\n        self.lower_case = lower_case\n        self.hash_numbers = hash_numbers\n\n    def __call__(self, data: Tuple[str, str]) -> Tuple[str, str]:\n        """""" Performs regex logic for string cleansing and attaches start and end tokens to the text. """"""\n        text_encoder, text_decoder = self._normalize_string(data[0]), self._normalize_string(data[1])\n        if self.add_input_start_end:\n            text_encoder = self.start_token + \' \' + text_encoder + \' \' + self.end_token\n        text_decoder = self.start_token + \' \' + text_decoder + \' \' + self.end_token\n        return text_encoder, text_decoder\n\n    def _normalize_string(self, s: str) -> str:\n        if self.lower_case:\n            s = s.lower()\n        if self.filter_pattern is not None:\n            s = re.sub(self.filter_pattern, \'\', s)\n        if self.hash_numbers:\n            s = re.sub(r\'\\d+\', \'#\', s)\n        if self.punctuation_pattern is not None:\n            s = re.sub(self.punctuation_pattern, r\' \\1\', s)\n        s = re.sub(r\'\\s+\', r\' \', s)\n        return s\n'"
headliner/preprocessing/tokenizer.py,0,"b'import abc\nfrom abc import abstractmethod\nfrom typing import List\n\n\nclass Tokenizer(abc.ABC):\n    """"""\n    Encodes text to sequences and decodes sequences to text.\n    """"""\n\n    @abstractmethod\n    def encode(self, text: str) -> List[int]:\n        """"""\n        Encodes a given string into a sequence of indices.\n\n        Args:\n            text: Text to encode.\n\n        Returns: Encoded sequence.\n        """"""\n        pass\n\n    @abstractmethod\n    def decode(self, sequence: List[int]) -> str:\n        """"""\n        Decodees a given sequence into a text.\n\n        Args:\n            sequence: Sequence to decode.\n\n        Returns: Decoded text.\n\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def vocab_size(self) -> int:\n        """"""\n        Size of token vocab.\n        """"""\n        pass\n'"
headliner/preprocessing/vectorizer.py,0,"b'from typing import Tuple, List\n\nfrom headliner.preprocessing.tokenizer import Tokenizer\n\n\nclass Vectorizer:\n    """"""\n    Transforms tuples of text into tuples of vector sequences.\n    """"""\n\n    def __init__(self,\n                 tokenizer_encoder: Tokenizer,\n                 tokenizer_decoder: Tokenizer,\n                 max_input_len=None,\n                 max_output_len=None) -> None:\n        """"""\n        Initializes the vectorizer.\n\n        Args:\n            tokenizer_encoder: Tokenizer that encodes the input text.\n            tokenizer_decoder: Tokenizer that encodes the target text.\n            max_input_len (optional): Maximum length of input sequence,\n                longer sequences will be truncated.\n            max_output_len (optional): Maximum length of target sequence,\n                longer sequences will be truncated and shorter sequences\n                will be padded to max len.\n        """"""\n        self.encoding_dim = tokenizer_encoder.vocab_size + 1\n        self.decoding_dim = tokenizer_decoder.vocab_size + 1\n        self.max_input_len = max_input_len\n        self.max_output_len = max_output_len\n        self._tokenizer_encoder = tokenizer_encoder\n        self._tokenizer_decoder = tokenizer_decoder\n\n    def __call__(self, data: Tuple[str, str]) -> Tuple[List[int], List[int]]:\n        """"""\n        Encodes preprocessed strings into sequences of one-hot indices.\n        """"""\n        text_encoder, text_decoder = data[0], data[1]\n        vec_encoder = self._tokenizer_encoder.encode(text_encoder)\n        vec_decoder = self._tokenizer_decoder.encode(text_decoder)\n        if self.max_input_len is not None:\n            if len(vec_encoder) > self.max_input_len:\n                vec_encoder = vec_encoder[:self.max_input_len - 1] + [vec_encoder[-1]]\n        if self.max_output_len is not None:\n            if len(vec_decoder) > self.max_output_len:\n                vec_decoder = vec_decoder[:self.max_output_len - 1] + [vec_decoder[-1]]\n            else:\n                vec_decoder = vec_decoder + [0] * (self.max_output_len - len(vec_decoder))\n\n        return vec_encoder, vec_decoder\n\n    def encode_input(self, text: str) -> List[int]:\n        return self._tokenizer_encoder.encode(text)\n\n    def encode_output(self, text: str) -> List[int]:\n        return self._tokenizer_decoder.encode(text)\n\n    def decode_input(self, sequence: List[int]) -> str:\n        return self._tokenizer_encoder.decode(sequence)\n\n    def decode_output(self, sequence: List[int]) -> str:\n        return self._tokenizer_decoder.decode(sequence)\n'"
headliner/utils/__init__.py,0,b''
headliner/utils/logger.py,0,"b""import logging\nimport sys\n\n\ndef get_logger(name: str) -> logging.Logger:\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.propagate = False\n\n    if not logger.handlers:\n        # stream handler ensures that logging events are passed to stdout\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(logging.INFO)\n        ch_formatter = logging.Formatter('%(message)s')\n        ch.setFormatter(ch_formatter)\n        logger.addHandler(ch)\n\n    return logger\n"""
tests/callbacks/__init__.py,0,b''
tests/callbacks/test_evaluation_callback.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom headliner.callbacks.evaluation_callback import EvaluationCallback\n\n\nclass TestEvaluationCallback(unittest.TestCase):\n\n    def test_on_epoch_end(self):\n        mock_summarizer = Mock()\n        mock_summarizer.predict_vectors.return_value = None\n        mock_scorer_a, mock_scorer_b = Mock(), Mock()\n        mock_scorer_a.return_value = 1\n        mock_scorer_b.return_value = 2\n        val_data = [('a', 'b'), ('c', 'd')]\n        evaluation_callback = EvaluationCallback(summarizer=mock_summarizer,\n                                                 scorers={'mock_score_a': mock_scorer_a,\n                                                          'mock_score_b': mock_scorer_b},\n                                                 val_data=val_data,\n                                                 print_num_examples=0)\n        logs = {}\n        evaluation_callback.on_epoch_end(0, logs=logs)\n        self.assertEqual({'mock_score_a', 'mock_score_b'}, logs.keys())\n        self.assertAlmostEqual(1.0, logs['mock_score_a'], places=10)\n        self.assertAlmostEqual(2.0, logs['mock_score_b'], places=10)\n"""
tests/callbacks/test_model_checkpointing_callback.py,0,"b""import shutil\nimport tempfile\nimport unittest\nimport os\nfrom unittest.mock import Mock, call\n\nfrom headliner.callbacks.model_checkpoint_callback import ModelCheckpointCallback\n\n\nclass TestModelCheckpointCallback(unittest.TestCase):\n\n    def setUp(self) -> None:\n        self.temp_dir = tempfile.mkdtemp(prefix='TestModelCheckpointingCallback')\n\n    def tearDown(self) -> None:\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_on_epoch_end(self):\n        mock_summarizer = Mock()\n        model_save_path = os.path.join(self.temp_dir, 'summarizer_save')\n        model_checkoint_callback = ModelCheckpointCallback(file_path=model_save_path,\n                                                           summarizer=mock_summarizer,\n                                                           monitor='loss_val',\n                                                           mode='min')\n\n        logs = {'loss_val': 10}\n        model_checkoint_callback.on_epoch_end(0, logs=logs)\n        mock_summarizer.save.assert_called_with(model_save_path)\n        logs = {'loss_val': 20}\n        model_checkoint_callback.on_epoch_end(0, logs=logs)\n        mock_summarizer.save.assert_called_with(model_save_path)\n        logs = {'loss_val': 5}\n        model_checkoint_callback.on_epoch_end(0, logs=logs)\n        mock_summarizer.save.assert_has_calls([call(model_save_path), call(model_save_path)])\n"""
tests/callbacks/test_validation_callback.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom headliner.callbacks.validation_callback import ValidationCallback\n\n\nclass TestValidationCallback(unittest.TestCase):\n\n    def test_on_epoch_end(self):\n        mock_summarizer = Mock()\n        mock_summarizer.new_train_step.return_value = lambda input_seq, output_seq: 0.5\n        mock_scorer_a, mock_scorer_b = Mock(), Mock()\n        mock_scorer_a.return_value = 1\n        mock_scorer_b.return_value = 2\n        val_dataset_mock = Mock()\n        val_dataset_mock.take.return_value = [(1, 2), (1, 2)]\n        loss_function_mock = Mock()\n        validation_callback = ValidationCallback(summarizer=mock_summarizer,\n                                                 val_dataset=val_dataset_mock,\n                                                 loss_function=loss_function_mock,\n                                                 batch_size=1)\n        logs = {}\n        validation_callback.on_epoch_end(0, logs=logs)\n        self.assertEqual({'loss_val'}, logs.keys())\n        self.assertAlmostEqual(0.5, logs['loss_val'], places=10)\n"""
tests/evaluation/__init__.py,0,b''
tests/evaluation/test_bleu_scorer.py,0,"b""import unittest\n\nfrom headliner.evaluation.bleu_scorer import BleuScorer\n\n\nclass TestBleuScorer(unittest.TestCase):\n\n    def test_score(self):\n        bleu_scorer = BleuScorer(tokens_to_ignore={'<end>', '<unk>'})\n\n        text_preprocessed = ('', 'this is a test')\n        pred = 'this is a test'\n        score = bleu_scorer({'preprocessed_text': text_preprocessed, 'predicted_text': pred})\n        self.assertAlmostEqual(1, score, 5)\n\n        text_preprocessed = ('', 'this is a test')\n        pred = 'this is a test <end>'\n        score = bleu_scorer({'preprocessed_text': text_preprocessed, 'predicted_text': pred})\n        self.assertAlmostEqual(1, score, 5)\n\n        text_preprocessed = ('', 'it is a guide to action which ensures that the military '\n                                 'always obeys the commands of the party')\n        pred = 'it is a guide to action that ensures that the military will ' \\\n               'forever <unk> <unk> <unk> heed Party commands <end>'\n        score = bleu_scorer({'preprocessed_text': text_preprocessed, 'predicted_text': pred})\n        self.assertAlmostEqual(0.4138, score, 3)\n"""
tests/model/__init__.py,0,b''
tests/model/test_summarizer.py,2,"b""import os\nimport shutil\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\n\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.basic_summarizer import BasicSummarizer\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass TestSummarizer(unittest.TestCase):\n\n    def setUp(self) -> None:\n        np.random.seed(42)\n        tf.random.set_seed(42)\n        self.temp_dir = tempfile.mkdtemp(prefix='TestSummarizerTmp')\n\n    def tearDown(self) -> None:\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_serde_happy_path(self) -> None:\n        preprocessor = Preprocessor()\n        tokenizer = KerasTokenizer(oov_token='<unk>')\n        tokenizer.fit(['a b c {} {}'.format(\n            preprocessor.start_token, preprocessor.end_token)])\n        vectorizer = Vectorizer(tokenizer, tokenizer)\n        summarizer = BasicSummarizer(lstm_size=10,\n                                     max_prediction_len=10,\n                                     embedding_decoder_trainable=False,\n                                     embedding_size=10)\n        summarizer.init_model(preprocessor=preprocessor,\n                              vectorizer=vectorizer)\n\n        # we need at least a train step to init the weights\n        train_step = summarizer.new_train_step(masked_crossentropy, batch_size=1, apply_gradients=True)\n        train_seq = tf.convert_to_tensor(np.array([[1, 1, 1]]), dtype=tf.int32)\n        train_step(train_seq, train_seq)\n\n        save_dir = os.path.join(self.temp_dir, 'summarizer_serde_happy_path')\n        summarizer.save(save_dir)\n        summarizer_loaded = BasicSummarizer.load(save_dir)\n        self.assertEqual(10, summarizer_loaded.lstm_size)\n        self.assertEqual(10, summarizer_loaded.max_prediction_len)\n        self.assertIsNotNone(summarizer_loaded.preprocessor)\n        self.assertIsNotNone(summarizer_loaded.vectorizer)\n        self.assertIsNotNone(summarizer_loaded.encoder)\n        self.assertIsNotNone(summarizer_loaded.decoder)\n        self.assertTrue(summarizer_loaded.encoder.embedding.trainable)\n        self.assertFalse(summarizer_loaded.decoder.embedding.trainable)\n        self.assertIsNotNone(summarizer_loaded.optimizer)\n\n        pred = summarizer.predict_vectors('a c', '')\n        pred_loaded = summarizer_loaded.predict_vectors('a c', '')\n        np.testing.assert_almost_equal(\n            pred['logits'], pred_loaded['logits'], decimal=6)"""
tests/model/test_summarizer_attention.py,2,"b""import os\nimport shutil\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras_preprocessing.text import Tokenizer\n\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.attention_summarizer import AttentionSummarizer\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass TestSummarizerAttention(unittest.TestCase):\n\n    def setUp(self) -> None:\n        np.random.seed(42)\n        tf.random.set_seed(42)\n        self.temp_dir = tempfile.mkdtemp(prefix='TestSummarizerAttentionTmp')\n\n    def tearDown(self) -> None:\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_serde_happy_path(self) -> None:\n        preprocessor = Preprocessor()\n        tokenizer = KerasTokenizer(oov_token='<unk>')\n        tokenizer.fit(['a b c {} {}'.format(\n            preprocessor.start_token, preprocessor.end_token)])\n        vectorizer = Vectorizer(tokenizer, tokenizer)\n        summarizer = AttentionSummarizer(lstm_size=10,\n                                         max_prediction_len=10,\n                                         embedding_size=10,\n                                         embedding_encoder_trainable=False)\n        summarizer.init_model(preprocessor=preprocessor,\n                              vectorizer=vectorizer)\n\n        # we need at least a train step to init the weights\n        train_step = summarizer.new_train_step(masked_crossentropy, batch_size=1, apply_gradients=True)\n        train_seq = tf.convert_to_tensor(np.array([[1, 1, 1]]), dtype=tf.int32)\n        train_step(train_seq, train_seq)\n\n        save_dir = os.path.join(self.temp_dir, 'summarizer_serde_happy_path')\n        summarizer.save(save_dir)\n        summarizer_loaded = AttentionSummarizer.load(save_dir)\n        self.assertEqual(10, summarizer_loaded.lstm_size)\n        self.assertEqual(10, summarizer_loaded.max_prediction_len)\n        self.assertIsNotNone(summarizer_loaded.preprocessor)\n        self.assertIsNotNone(summarizer_loaded.vectorizer)\n        self.assertIsNotNone(summarizer_loaded.encoder)\n        self.assertIsNotNone(summarizer_loaded.decoder)\n        self.assertFalse(summarizer_loaded.encoder.embedding.trainable)\n        self.assertTrue(summarizer_loaded.decoder.embedding.trainable)\n        self.assertIsNotNone(summarizer_loaded.optimizer)\n\n        pred = summarizer.predict_vectors('a c', '')\n        pred_loaded = summarizer_loaded.predict_vectors('a c', '')\n        np.testing.assert_almost_equal(pred['logits'], pred_loaded['logits'], decimal=6)\n"""
tests/model/test_summarizer_bert.py,2,"b""import os\nimport shutil\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizer\n\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.bert_summarizer import BertSummarizer\nfrom headliner.preprocessing.bert_vectorizer import BertVectorizer\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\n\n\nclass TestSummarizerBert(unittest.TestCase):\n\n    def setUp(self) -> None:\n        np.random.seed(42)\n        tf.random.set_seed(42)\n        self.temp_dir = tempfile.mkdtemp(prefix='TestSummarizerBertTmp')\n\n    def tearDown(self) -> None:\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_serde_happy_path(self) -> None:\n        preprocessor = Preprocessor(start_token='[CLS]', end_token='[SEP]')\n\n        tokenizer_encoder = BertTokenizer.from_pretrained('bert-base-uncased')\n        tokenizer_decoder = KerasTokenizer(oov_token='<unk>')\n        tokenizer_decoder.fit(['a b c {} {}'.format(\n            preprocessor.start_token, preprocessor.end_token)])\n        vectorizer = BertVectorizer(tokenizer_encoder, tokenizer_decoder)\n        summarizer = BertSummarizer(num_layers_encoder=1,\n                                    num_layers_decoder=1,\n                                    bert_embedding_encoder='bert-base-uncased',\n                                    num_heads=2,\n                                    max_prediction_len=3,\n                                    embedding_size_encoder=768,\n                                    embedding_size_decoder=10,\n                                    embedding_encoder_trainable=False)\n        summarizer.init_model(preprocessor=preprocessor,\n                              vectorizer=vectorizer)\n\n        # we need at least a train step to init the weights\n        train_step = summarizer.new_train_step(masked_crossentropy, batch_size=1, apply_gradients=True)\n        train_seq = tf.convert_to_tensor(np.array([[1, 1, 1]]), dtype=tf.int32)\n        train_step(train_seq, train_seq, train_seq)\n\n        save_dir = os.path.join(self.temp_dir, 'summarizer_serde_happy_path')\n        summarizer.save(save_dir)\n        summarizer_loaded = BertSummarizer.load(save_dir)\n        self.assertEqual(1, summarizer_loaded.num_layers_encoder)\n        self.assertEqual(1, summarizer_loaded.num_layers_decoder)\n        self.assertEqual(2, summarizer_loaded.num_heads)\n        self.assertEqual(3, summarizer_loaded.max_prediction_len)\n        self.assertEqual(768, summarizer_loaded.embedding_size_encoder)\n        self.assertEqual(10, summarizer_loaded.embedding_size_decoder)\n        self.assertIsNotNone(summarizer_loaded.preprocessor)\n        self.assertIsNotNone(summarizer_loaded.vectorizer)\n        self.assertIsNotNone(summarizer_loaded.transformer)\n        self.assertFalse(summarizer_loaded.transformer.encoder.embedding.trainable)\n        self.assertTrue(summarizer_loaded.transformer.decoder.embedding.trainable)\n        self.assertIsNotNone(summarizer_loaded.optimizer_encoder)\n        self.assertIsNotNone(summarizer_loaded.optimizer_decoder)\n\n        pred = summarizer.predict_vectors('a c', '')\n        pred_loaded = summarizer_loaded.predict_vectors('a c', '')\n        np.testing.assert_almost_equal(pred['logits'], pred_loaded['logits'], decimal=6)\n"""
tests/model/test_summarizer_transformer.py,2,"b""import os\nimport shutil\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom headliner.losses import masked_crossentropy\nfrom headliner.model.transformer_summarizer import TransformerSummarizer\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.preprocessor import Preprocessor\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass TestSummarizerTransformer(unittest.TestCase):\n\n    def setUp(self) -> None:\n        np.random.seed(42)\n        tf.random.set_seed(42)\n        self.temp_dir = tempfile.mkdtemp(prefix='TestSummarizerTransformerTmp')\n\n    def tearDown(self) -> None:\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_serde_happy_path(self) -> None:\n        preprocessor = Preprocessor()\n        tokenizer = KerasTokenizer(oov_token='<unk>')\n        tokenizer.fit(['a b c {} {}'.format(\n            preprocessor.start_token, preprocessor.end_token)])\n        vectorizer = Vectorizer(tokenizer, tokenizer)\n        summarizer = TransformerSummarizer(num_layers=1,\n                                           num_heads=2,\n                                           max_prediction_len=3,\n                                           embedding_size=10,\n                                           embedding_encoder_trainable=False)\n        summarizer.init_model(preprocessor=preprocessor,\n                              vectorizer=vectorizer)\n\n        # we need at least a train step to init the weights\n        train_step = summarizer.new_train_step(masked_crossentropy, batch_size=1, apply_gradients=True)\n        train_seq = tf.convert_to_tensor(np.array([[1, 1, 1]]), dtype=tf.int32)\n        train_step(train_seq, train_seq)\n\n        save_dir = os.path.join(self.temp_dir, 'summarizer_serde_happy_path')\n        summarizer.save(save_dir)\n        summarizer_loaded = TransformerSummarizer.load(save_dir)\n        self.assertEqual(1, summarizer_loaded.num_layers)\n        self.assertEqual(2, summarizer_loaded.num_heads)\n        self.assertEqual(3, summarizer_loaded.max_prediction_len)\n        self.assertEqual(10, summarizer_loaded.embedding_size)\n        self.assertIsNotNone(summarizer_loaded.preprocessor)\n        self.assertIsNotNone(summarizer_loaded.vectorizer)\n        self.assertIsNotNone(summarizer_loaded.transformer)\n        self.assertFalse(summarizer_loaded.transformer.encoder.embedding.trainable)\n        self.assertTrue(summarizer_loaded.transformer.decoder.embedding.trainable)\n        self.assertIsNotNone(summarizer_loaded.optimizer)\n\n        pred = summarizer.predict_vectors('a c', '')\n        pred_loaded = summarizer_loaded.predict_vectors('a c', '')\n        np.testing.assert_almost_equal(pred['logits'], pred_loaded['logits'], decimal=6)\n"""
tests/preprocessing/__init__.py,0,b''
tests/preprocessing/test_bert_preprocessor.py,0,"b""import unittest\n\nfrom spacy.lang.en import English\nfrom headliner.preprocessing.bert_preprocessor import BertPreprocessor\n\n\nclass TestBertPreprocessor(unittest.TestCase):\n\n    def test_preprocessing(self):\n        nlp = English()\n        preprocessor = BertPreprocessor(nlp=nlp)\n        data = ('I love my dog. He is the best. He eats and poops.', 'Me and my dog.')\n        data_preprocessed = preprocessor(data)\n        self.assertEqual(('[CLS] I love my dog. [SEP] [CLS] He is the best. [SEP] [CLS] He eats and poops. [SEP]',\n                          '[CLS] Me and my dog. [SEP]'), data_preprocessed)\n"""
tests/preprocessing/test_bert_vectorizer.py,0,"b""import unittest\n\nfrom transformers import BertTokenizer\n\nfrom headliner.preprocessing.bert_vectorizer import BertVectorizer\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\n\n\nclass TestBertVectorizer(unittest.TestCase):\n\n    def test_vectorize(self):\n        data = ('[CLS] I love my dog. [SEP] [CLS] He is the best. [SEP]', '[CLS] Dog. [SEP]')\n        tokenizer_encoder = BertTokenizer.from_pretrained('bert-base-uncased')\n        tokenizer_decoder = KerasTokenizer()\n        tokenizer_decoder.fit([data[1]])\n        vectorizer = BertVectorizer(tokenizer_encoder,\n                                    tokenizer_decoder,\n                                    max_input_len=50,\n                                    max_output_len=3)\n\n        data_vectorized = vectorizer(data)\n        expected = ([101, 1045, 2293, 2026, 3899, 1012, 102, 101, 2002, 2003, 1996, 2190, 1012, 102],\n                    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 2, 3])\n        self.assertEqual(expected, data_vectorized)\n        input_decoded = vectorizer.decode_input(expected[0])\n        expected = '[CLS] i love my dog. [SEP] [CLS] he is the best. [SEP]'\n        self.assertEqual(expected, input_decoded)\n\n\n"""
tests/preprocessing/test_bucket_generator.py,0,"b'import itertools\nimport random\nimport unittest\n\nfrom headliner.preprocessing.bucket_generator import BucketGenerator\n\n\nclass TestBucketGenerator(unittest.TestCase):\n\n    def test_generate_batches_nonrandom(self):\n        data = [[i] * i for i in range(10, 0, -1)]\n        bucket_generator = BucketGenerator(lambda e: len(e),\n                                           batch_size=2,\n                                           buffer_size_batches=100,\n                                           shuffle=False)\n        buckets_gen = bucket_generator(data)\n        result = [el for el in buckets_gen]\n        expected = [[i] * i for i in range(1, 11)]\n        self.assertEqual(expected, result)\n\n    def test_generate_batches_random(self):\n        data = [[i] * i for i in range(100, 0, -1)]\n        random.shuffle(data)\n        bucket_generator = BucketGenerator(lambda e: len(e),\n                                           batch_size=2,\n                                           buffer_size_batches=100,\n                                           batches_to_bucket=10,\n                                           shuffle=True,\n                                           seed=42)\n        buckets_gen = bucket_generator(data)\n        result = [el for el in buckets_gen]\n\n        # check whether all elements are returned\n        expected_elements = list(itertools.chain.from_iterable(data))\n        expected_elements.sort()\n        result_elements = list(itertools.chain.from_iterable(result))\n        result_elements.sort()\n        self.assertEqual(expected_elements, result_elements)\n\n        # check whether sequences of similar length are bucketed together\n        # -> compare sum of length difference within batches against non-bucketed data\n        raw_total_length_diff = 0\n        result_total_length_diff = 0\n        for i in range(0, len(data), 2):\n            first_seq_raw, second_seq_raw = data[i:i + 2]\n            first_seq_bucket, second_seq_bucket = result[i:i + 2]\n            raw_total_length_diff += abs(len(second_seq_raw) - len(first_seq_raw))\n            result_total_length_diff += abs(len(second_seq_bucket) - len(first_seq_bucket))\n        self.assertTrue(result_total_length_diff < raw_total_length_diff / 4)\n'"
tests/preprocessing/test_dataset_generator.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom headliner.preprocessing.dataset_generator import DatasetGenerator\n\n\nclass TestDatasetGenerator(unittest.TestCase):\n\n    def test_generate_dataset(self):\n        data = [([1, 1], [2, 2]), ([1, 1, 1], [2, 2])]\n        batch_generator = DatasetGenerator(batch_size=1)\n\n        # batch size = 1\n        batches_iter = iter(batch_generator(lambda: data))\n        batches = next(batches_iter)\n        print(batches[0].numpy().tolist())\n        expected = [[[1, 1]], [[2, 2]]]\n        np.testing.assert_array_equal(expected[0], batches[0].numpy().tolist())\n        np.testing.assert_array_equal(expected[1], batches[1].numpy().tolist())\n\n        # batch size = 2\n        batch_generator = DatasetGenerator(batch_size=2)\n        batches_iter = iter(batch_generator(lambda: data))\n        batches = next(batches_iter)\n        expected = [[[1, 1, 0], [1, 1, 1]], [[2, 2], [2, 2]]]\n        np.testing.assert_array_equal(expected[0], batches[0].numpy().tolist())\n        np.testing.assert_array_equal(expected[1], batches[1].numpy().tolist())\n\n        # batch size = 2, rank = 3\n        data = [([1, 1], [0, 1], [2, 2]),\n                ([1, 1, 1], [1, 1, 1], [3, 3, 3])]\n        batch_generator = DatasetGenerator(batch_size=2, rank=3)\n        batches_iter = iter(batch_generator(lambda: data))\n        batches = next(batches_iter)\n        expected = [[[1, 1, 0], [1, 1, 1]], [[0, 1, 0], [1, 1, 1]], [[2, 2, 0], [3, 3, 3]]]\n        np.testing.assert_array_equal(expected[0], batches[0].numpy().tolist())\n        np.testing.assert_array_equal(expected[1], batches[1].numpy().tolist())\n'"
tests/preprocessing/test_keras_tokenizer.py,0,"b""import unittest\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\n\n\nclass TestKerasTokenizer(unittest.TestCase):\n\n    def test_keras_tokenizer(self):\n        tokenizer = KerasTokenizer(filters='', lower=False, oov_token='<unk>')\n        tokenizer.fit(['a b c d'])\n        encoded = tokenizer.encode('a b e')\n        self.assertEqual([2, 3, 1], encoded)\n        decoded = tokenizer.decode(encoded)\n        self.assertEqual('a b <unk>', decoded)\n        self.assertEqual(5, tokenizer.vocab_size)\n        self.assertEqual({'a', 'b', 'c', 'd', '<unk>'}, tokenizer.token_index.keys())\n        self.assertEqual({1, 2, 3, 4, 5}, set(tokenizer.token_index.values()))\n\n"""
tests/preprocessing/test_preprocessor.py,0,"b""import unittest\n\nfrom headliner.preprocessing.preprocessor import Preprocessor\n\n\nclass TestPreprocessor(unittest.TestCase):\n\n    def test_preprocessing(self):\n        preprocessor = Preprocessor()\n        data = (('First text!', 'first head'), ('2-nd t\xc3\xa4xt', 'Second head'))\n        data_preprocessed = [preprocessor(d) for d in data]\n        self.assertEqual(('<start> first text ! <end>', '<start> first head <end>'), data_preprocessed[0])\n        self.assertEqual(('<start> #-nd t\xc3\xa4xt <end>', '<start> second head <end>'), data_preprocessed[1])\n\n        preprocessor = Preprocessor(start_token='<start>',\n                                    end_token='<end>',\n                                    lower_case=True,\n                                    hash_numbers=False)\n        data_preprocessed = [preprocessor(d) for d in data]\n        self.assertEqual(('<start> 2-nd t\xc3\xa4xt <end>', '<start> second head <end>'), data_preprocessed[1])\n\n        preprocessor = Preprocessor(start_token='<start>',\n                                    end_token='<end>',\n                                    lower_case=False,\n                                    hash_numbers=True)\n        data_preprocessed = [preprocessor(d) for d in data]\n        self.assertEqual(('<start> #-nd t\xc3\xa4xt <end>', '<start> Second head <end>'), data_preprocessed[1])\n\n"""
tests/preprocessing/test_vectorizer.py,0,"b""import unittest\n\nfrom headliner.preprocessing.keras_tokenizer import KerasTokenizer\nfrom headliner.preprocessing.vectorizer import Vectorizer\n\n\nclass TestVectorizer(unittest.TestCase):\n\n    def test_vectorize(self):\n        data = [('a b c', 'd')]\n        tokenizer_encoder = KerasTokenizer()\n        tokenizer_decoder = KerasTokenizer()\n        tokenizer_encoder.fit([data[0][0]])\n        tokenizer_decoder.fit([data[0][1]])\n        vectorizer = Vectorizer(tokenizer_encoder,\n                                tokenizer_decoder,\n                                max_input_len=5,\n                                max_output_len=3)\n        data_vectorized = [vectorizer(d) for d in data]\n        self.assertEqual([([1, 2, 3], [1, 0, 0])], data_vectorized)\n\n        data = [('a b c', 'd d d d')]\n        data_vectorized = [vectorizer(d) for d in data]\n        self.assertEqual([([1, 2, 3], [1, 1, 1])], data_vectorized)\n\n        data = [('a a b b b b b', 'd d d d')]\n        data_vectorized = [vectorizer(d) for d in data]\n        self.assertEqual([([1, 1, 2, 2, 2], [1, 1, 1])], data_vectorized)\n"""
tests/resources/__init__.py,0,b''
