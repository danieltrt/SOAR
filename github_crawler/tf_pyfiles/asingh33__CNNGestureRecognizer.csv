file_path,api_count,code
gestureCNN.py,0,"b'#!/usr/bin/env python2\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Apr  6 01:01:43 2017\n\n@author: abhisheksingh\n""""""\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\nfrom keras.optimizers import SGD,RMSprop,adam\nfrom keras.utils import np_utils\n\nfrom keras import backend as K\nif K.backend() == \'tensorflow\':\n    import tensorflow\n    #K.set_image_dim_ordering(\'tf\')\nelse:\n    import theano\n    #K.set_image_dim_ordering(\'th\')\n\n\'\'\'Ideally we should have changed image dim ordering based on Theano or Tensorflow, but for some reason I get following error when I switch it to \'tf\' for Tensorflow.\n\tHowever, the outcome of the prediction doesnt seem to get affected due to this and Tensorflow gives me similar result as Theano.\n\tI didnt spend much time on this behavior, but if someone has answer to this then please do comment and let me know.\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for \'conv2d_1/convolution\' (op: \'Conv2D\') with input shapes: [?,1,200,200], [3,3,200,32].\n\'\'\'\nK.set_image_dim_ordering(\'th\')\n\t\n\t\nimport numpy as np\n#import matplotlib.pyplot as plt\nimport os\n\nfrom PIL import Image\n# SKLEARN\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport json\n\nimport cv2\nimport matplotlib\n#matplotlib.use(""TkAgg"")\nfrom matplotlib import pyplot as plt\n\n# input image dimensions\nimg_rows, img_cols = 200, 200\n\n# number of channels\n# For grayscale use 1 value and for color images use 3 (R,G,B channels)\nimg_channels = 1\n\n\n# Batch_size to train\nbatch_size = 32\n\n## Number of output classes (change it accordingly)\n## eg: In my case I wanted to predict 4 types of gestures (Ok, Peace, Punch, Stop)\n## NOTE: If you change this then dont forget to change Labels accordingly\nnb_classes = 5\n\n# Number of epochs to train (change it accordingly)\nnb_epoch = 15  #25\n\n# Total number of convolutional filters to use\nnb_filters = 32\n# Max pooling\nnb_pool = 2\n# Size of convolution kernel\nnb_conv = 3\n\n#%%\n#  data\npath = ""./""\npath1 = ""./gestures""    #path of folder of images\n\n## Path2 is the folder which is fed in to training model\npath2 = \'./imgfolder_b\'\n\nWeightFileName = []\n\n# outputs\noutput = [""OK"", ""NOTHING"",""PEACE"", ""PUNCH"", ""STOP""]\n#output = [""PEACE"", ""STOP"", ""THUMBSDOWN"", ""THUMBSUP""]\n\njsonarray = {}\n\n#%%\ndef update(plot):\n    global jsonarray\n    h = 450\n    y = 30\n    w = 45\n    font = cv2.FONT_HERSHEY_SIMPLEX\n\n    #plot = np.zeros((512,512,3), np.uint8)\n    \n    #array = {""OK"": 65.79261422157288, ""NOTHING"": 0.7953541353344917, ""PEACE"": 5.33270463347435, ""PUNCH"": 0.038031660369597375, ""STOP"": 28.04129719734192}\n    \n    for items in jsonarray:\n        mul = (jsonarray[items]) / 100\n        #mul = random.randint(1,100) / 100\n        cv2.line(plot,(0,y),(int(h * mul),y),(255,0,0),w)\n        cv2.putText(plot,items,(0,y+5), font , 0.7,(0,255,0),2,1)\n        y = y + w + 30\n\n    return plot\n\n#%% For debug trace\ndef debugme():\n    import pdb\n    pdb.set_trace()\n\n#%%\n# This function can be used for converting colored img to Grayscale img\n# while copying images from path1 to path2\ndef convertToGrayImg(path1, path2):\n    listing = os.listdir(path1)\n    for file in listing:\n        if file.startswith(\'.\'):\n            continue\n        img = Image.open(path1 +\'/\' + file)\n        #img = img.resize((img_rows,img_cols))\n        grayimg = img.convert(\'L\')\n        grayimg.save(path2 + \'/\' +  file, ""PNG"")\n\n#%%\ndef modlistdir(path, pattern = None):\n    listing = os.listdir(path)\n    retlist = []\n    for name in listing:\n        #This check is to ignore any hidden files/folders\n        if pattern == None:\n            if name.startswith(\'.\'):\n                continue\n            else:\n                retlist.append(name)\n        elif name.endswith(pattern):\n            retlist.append(name)\n            \n    return retlist\n\n\n# Load CNN model\ndef loadCNN(bTraining = False):\n    global get_output\n    model = Sequential()\n    \n    \n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv),\n                        padding=\'valid\',\n                        input_shape=(img_channels, img_rows, img_cols)))\n    convout1 = Activation(\'relu\')\n    model.add(convout1)\n    model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\n    convout2 = Activation(\'relu\')\n    model.add(convout2)\n    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.5))\n\n    model.add(Flatten())\n    model.add(Dense(128))\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    \n    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(loss=\'categorical_crossentropy\', optimizer=\'adadelta\', metrics=[\'accuracy\'])\n    \n    # Model summary\n    model.summary()\n    # Model conig details\n    model.get_config()\n    \n    if not bTraining :\n        #List all the weight files available in current directory\n        WeightFileName = modlistdir(\'.\',\'.hdf5\')\n        if len(WeightFileName) == 0:\n            print(\'Error: No pretrained weight file found. Please either train the model or download one from the https://github.com/asingh33/CNNGestureRecognizer\')\n            return 0\n        else:\n            print(\'Found these weight files - {}\'.format(WeightFileName))\n        #Load pretrained weights\n        w = int(input(""Which weight file to load (enter the INDEX of it, which starts from 0): ""))\n        fname = WeightFileName[int(w)]\n        print(""loading "", fname)\n        model.load_weights(fname)\n\n    # refer the last layer here\n    layer = model.layers[-1]\n    get_output = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\n    \n    \n    return model\n\n# This function does the guessing work based on input images\ndef guessGesture(model, img):\n    global output, get_output, jsonarray\n    #Load image and flatten it\n    image = np.array(img).flatten()\n    \n    # reshape it\n    image = image.reshape(img_channels, img_rows,img_cols)\n    \n    # float32\n    image = image.astype(\'float32\') \n    \n    # normalize it\n    image = image / 255\n    \n    # reshape for NN\n    rimage = image.reshape(1, img_channels, img_rows, img_cols)\n    \n    # Now feed it to the NN, to fetch the predictions\n    #index = model.predict_classes(rimage)\n    #prob_array = model.predict_proba(rimage)\n    \n    prob_array = get_output([rimage, 0])[0]\n    \n    #print prob_array\n    \n    d = {}\n    i = 0\n    for items in output:\n        d[items] = prob_array[0][i] * 100\n        i += 1\n    \n    # Get the output with maximum probability\n    import operator\n    \n    guess = max(d.items(), key=operator.itemgetter(1))[0]\n    prob  = d[guess]\n\n    if prob > 60.0:\n        #print(guess + ""  Probability: "", prob)\n\n        #Enable this to save the predictions in a json file,\n        #Which can be read by plotter app to plot bar graph\n        #dump to the JSON contents to the file\n        \n        #with open(\'gesturejson.txt\', \'w\') as outfile:\n        #    json.dump(d, outfile)\n        jsonarray = d\n                \n        return output.index(guess)\n\n    else:\n        # Lets return index 1 for \'Nothing\' \n        return 1\n\n#%%\ndef initializers():\n    imlist = modlistdir(path2)\n    \n    image1 = np.array(Image.open(path2 +\'/\' + imlist[0])) # open one image to get size\n    #plt.imshow(im1)\n    \n    m,n = image1.shape[0:2] # get the size of the images\n    total_images = len(imlist) # get the \'total\' number of images\n    \n    # create matrix to store all flattened images\n    immatrix = np.array([np.array(Image.open(path2+ \'/\' + images).convert(\'L\')).flatten()\n                         for images in sorted(imlist)], dtype = \'f\')\n    \n\n    \n    print(immatrix.shape)\n    \n    input(""Press any key"")\n    \n    #########################################################\n    ## Label the set of images per respective gesture type.\n    ##\n    label=np.ones((total_images,),dtype = int)\n    \n    samples_per_class = int(total_images / nb_classes)\n    print(""samples_per_class - "",samples_per_class)\n    s = 0\n    r = samples_per_class\n    for classIndex in range(nb_classes):\n        label[s:r] = classIndex\n        s = r\n        r = s + samples_per_class\n    \n    \'\'\'\n    # eg: For 301 img samples/gesture for 4 gesture types\n    label[0:301]=0\n    label[301:602]=1\n    label[602:903]=2\n    label[903:]=3\n    \'\'\'\n    \n    data,Label = shuffle(immatrix,label, random_state=2)\n    train_data = [data,Label]\n     \n    (X, y) = (train_data[0],train_data[1])\n     \n     \n    # Split X and y into training and testing sets\n     \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n     \n    X_train = X_train.reshape(X_train.shape[0], img_channels, img_rows, img_cols)\n    X_test = X_test.reshape(X_test.shape[0], img_channels, img_rows, img_cols)\n     \n    X_train = X_train.astype(\'float32\')\n    X_test = X_test.astype(\'float32\')\n     \n    # normalize\n    X_train /= 255\n    X_test /= 255\n     \n    # convert class vectors to binary class matrices\n    Y_train = np_utils.to_categorical(y_train, nb_classes)\n    Y_test = np_utils.to_categorical(y_test, nb_classes)\n    return X_train, X_test, Y_train, Y_test\n\n\n\ndef trainModel(model):\n\n    # Split X and y into training and testing sets\n    X_train, X_test, Y_train, Y_test = initializers()\n\n    # Now start the training of the loaded model\n    hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n                 verbose=1, validation_split=0.2)\n\n    visualizeHis(hist)\n\n    ans = input(""Do you want to save the trained weights - y/n ?"")\n    if ans == \'y\':\n        filename = input(""Enter file name - "")\n        fname = path + str(filename) + "".hdf5""\n        model.save_weights(fname,overwrite=True)\n    else:\n        model.save_weights(""newWeight.hdf5"",overwrite=True)\n\n    # Save model as well\n    # model.save(""newModel.hdf5"")\n#%%\n\ndef visualizeHis(hist):\n    # visualizing losses and accuracy\n\n    train_loss=hist.history[\'loss\']\n    val_loss=hist.history[\'val_loss\']\n    train_acc=hist.history[\'acc\']\n    val_acc=hist.history[\'val_acc\']\n    xc=range(nb_epoch)\n\n    plt.figure(1,figsize=(7,5))\n    plt.plot(xc,train_loss)\n    plt.plot(xc,val_loss)\n    plt.xlabel(\'num of Epochs\')\n    plt.ylabel(\'loss\')\n    plt.title(\'train_loss vs val_loss\')\n    plt.grid(True)\n    plt.legend([\'train\',\'val\'])\n    #print plt.style.available # use bmh, classic,ggplot for big pictures\n    #plt.style.use([\'classic\'])\n\n    plt.figure(2,figsize=(7,5))\n    plt.plot(xc,train_acc)\n    plt.plot(xc,val_acc)\n    plt.xlabel(\'num of Epochs\')\n    plt.ylabel(\'accuracy\')\n    plt.title(\'train_acc vs val_acc\')\n    plt.grid(True)\n    plt.legend([\'train\',\'val\'],loc=4)\n\n    plt.show()\n\n#%%\ndef visualizeLayers(model):\n    imlist = modlistdir(\'./imgs\')\n    if len(imlist) == 0:\n        print(\'Error: No sample image file found under \\\'./imgs\\\' folder.\')\n        return\n    else:\n        print(\'Found these sample image files - {}\'.format(imlist))\n\n    img = int(input(""Which sample image file to load (enter the INDEX of it, which starts from 0): ""))\n    layerIndex = int(input(""Enter which layer to visualize. Enter -1 to visualize all layers possible: ""))\n    \n    if img <= len(imlist):\n        \n        image = np.array(Image.open(\'./imgs/\' + imlist[img]).convert(\'L\')).flatten()\n        \n        ## Predict\n        print(\'Guessed Gesture is {}\'.format(output[guessGesture(model,image)]))\n        \n        # reshape it\n        image = image.reshape(img_channels, img_rows,img_cols)\n        \n        # float32\n        image = image.astype(\'float32\')\n        \n        # normalize it\n        image = image / 255\n        \n        # reshape for NN\n        input_image = image.reshape(1, img_channels, img_rows, img_cols)\n    else:\n        print(\'Wrong file index entered !!\')\n        return\n    \n    \n    \n        \n    # visualizing intermediate layers\n    #output_layer = model.layers[layerIndex].output\n    #output_fn = theano.function([model.layers[0].input], output_layer)\n    #output_image = output_fn(input_image)\n    \n    if layerIndex >= 1:\n        visualizeLayer(model,img,input_image, layerIndex)\n    else:\n        tlayers = len(model.layers[:])\n        print(""Total layers - {}"".format(tlayers))\n        for i in range(1,tlayers):\n             visualizeLayer(model,img, input_image,i)\n\n#%%\ndef visualizeLayer(model, img, input_image, layerIndex):\n\n    layer = model.layers[layerIndex]\n    \n    get_activations = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\n    activations = get_activations([input_image, 0])[0]\n    output_image = activations\n    \n    \n    ## If 4 dimensional then take the last dimension value as it would be no of filters\n    if output_image.ndim == 4:\n        # Rearrange dimension so we can plot the result\n        #o1 = np.rollaxis(output_image, 3, 1)\n        #output_image = np.rollaxis(o1, 3, 1)\n        output_image = np.moveaxis(output_image, 1, 3)\n        \n        print(""Dumping filter data of layer{} - {}"".format(layerIndex,layer.__class__.__name__))\n        filters = len(output_image[0,0,0,:])\n        \n        fig=plt.figure(figsize=(8,8))\n        # This loop will plot the 32 filter data for the input image\n        for i in range(filters):\n            ax = fig.add_subplot(6, 6, i+1)\n            #ax.imshow(output_image[img,:,:,i],interpolation=\'none\' ) #to see the first filter\n            ax.imshow(output_image[0,:,:,i],\'gray\')\n            #ax.set_title(""Feature map of layer#{} \\ncalled \'{}\' \\nof type {} "".format(layerIndex,\n            #                layer.name,layer.__class__.__name__))\n            plt.xticks(np.array([]))\n            plt.yticks(np.array([]))\n        plt.tight_layout()\n        #plt.show()\n        savedfilename = ""img_"" + str(img) + ""_layer"" + str(layerIndex)+""_""+layer.__class__.__name__+"".png""\n        fig.savefig(savedfilename)\n        print(""Create file - {}"".format(savedfilename))\n        #plt.close(fig)\n    else:\n        print(""Can\'t dump data of this layer{}- {}"".format(layerIndex, layer.__class__.__name__))\n\n\n'"
trackgesture.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Mar  23 01:01:43 2017\n\n@author: abhisheksingh\n""""""\n\n#%%\nimport cv2\nimport numpy as np\nimport os\nimport time\n\nimport threading\n\nimport gestureCNN as myNN\n\nminValue = 70\n\nx0 = 400\ny0 = 200\nheight = 200\nwidth = 200\n\nsaveImg = False\nguessGesture = False\nvisualize = False\n\nkernel = np.ones((15,15),np.uint8)\nkernel2 = np.ones((1,1),np.uint8)\nskinkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))\n\n# Which mask mode to use BinaryMask, SkinMask (True|False) OR BkgrndSubMask (\'x\' key)\nbinaryMode = True\nbkgrndSubMode = False\nmask = 0\nbkgrnd = 0\ncounter = 0\n# This parameter controls number of image samples to be taken PER gesture\nnumOfSamples = 301\ngestname = """"\npath = """"\nmod = 0\n\nbanner =  \'\'\'\\nWhat would you like to do ?\n    1- Use pretrained model for gesture recognition & layer visualization\n    2- Train the model (you will require image samples for training under .\\imgfolder)\n    3- Visualize feature maps of different layers of trained model\n    4- Exit\t\n    \'\'\'\n\n\n#%%\ndef saveROIImg(img):\n    global counter, gestname, path, saveImg\n    if counter > (numOfSamples - 1):\n        # Reset the parameters\n        saveImg = False\n        gestname = \'\'\n        counter = 0\n        return\n    \n    counter = counter + 1\n    name = gestname + str(counter)\n    print(""Saving img:"",name)\n    cv2.imwrite(path+name + "".png"", img)\n    time.sleep(0.04 )\n\n\n#%%\ndef skinMask(frame, x0, y0, width, height, framecount, plot):\n    global guessGesture, visualize, mod, saveImg\n    # HSV values\n    low_range = np.array([0, 50, 80])\n    upper_range = np.array([30, 200, 255])\n    \n    cv2.rectangle(frame, (x0,y0),(x0+width,y0+height),(0,255,0),1)\n    #roi = cv2.UMat(frame[y0:y0+height, x0:x0+width])\n    roi = frame[y0:y0+height, x0:x0+width]\n    \n    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n    \n    #Apply skin color range\n    mask = cv2.inRange(hsv, low_range, upper_range)\n    \n    mask = cv2.erode(mask, skinkernel, iterations = 1)\n    mask = cv2.dilate(mask, skinkernel, iterations = 1)\n    \n    #blur\n    mask = cv2.GaussianBlur(mask, (15,15), 1)\n    \n    #bitwise and mask original frame\n    res = cv2.bitwise_and(roi, roi, mask = mask)\n    # color to grayscale\n    res = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n    \n    if saveImg == True:\n        saveROIImg(res)\n    elif guessGesture == True and (framecount % 5) == 4:\n        #res = cv2.UMat.get(res)\n        t = threading.Thread(target=myNN.guessGesture, args = [mod, res])\n        t.start()\n    elif visualize == True:\n        layer = int(input(""Enter which layer to visualize ""))\n        cv2.waitKey(0)\n        myNN.visualizeLayers(mod, res, layer)\n        visualize = False\n    \n    return res\n\n\n#%%\ndef binaryMask(frame, x0, y0, width, height, framecount, plot ):\n    global guessGesture, visualize, mod, saveImg\n    \n    cv2.rectangle(frame, (x0,y0),(x0+width,y0+height),(0,255,0),1)\n    #roi = cv2.UMat(frame[y0:y0+height, x0:x0+width])\n    roi = frame[y0:y0+height, x0:x0+width]\n    \n    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray,(5,5),2)\n    \n    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n    \n    if saveImg == True:\n        saveROIImg(res)\n    elif guessGesture == True and (framecount % 5) == 4:\n        #ores = cv2.UMat.get(res)\n        t = threading.Thread(target=myNN.guessGesture, args = [mod, res])\n        t.start()\n    elif visualize == True:\n        layer = int(input(""Enter which layer to visualize ""))\n        cv2.waitKey(1)\n        myNN.visualizeLayers(mod, res, layer)\n        visualize = False\n\n    return res\n\n#%%\n# This is the new mask mode. It simply tries to remove the background content by taking a image of the \n# background and subtracts it from the new frame contents of the ROI window.\n# So in order to use it correctly, keep the contents of ROI window stable and without your hand in it \n# and then press \'x\' key. If you can see the contents of ROI window all blank then it means you are\n# good to go for gesture prediction\ndef bkgrndSubMask(frame, x0, y0, width, height, framecount, plot):\n    global guessGesture, takebkgrndSubMask, visualize, mod, bkgrnd, saveImg\n        \n    cv2.rectangle(frame, (x0,y0),(x0+width,y0+height),(0,255,0),1)\n    roi = frame[y0:y0+height, x0:x0+width]\n    #roi = cv2.UMat(frame[y0:y0+height, x0:x0+width])\n    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n        \n    #Take background image\n    if takebkgrndSubMask == True:\n        bkgrnd = roi\n        takebkgrndSubMask = False\n        print(""Refreshing background image for mask..."")\t\t\n\n    \n    #Take a diff between roi & bkgrnd image contents\n    diff = cv2.absdiff(roi, bkgrnd)\n\n    _, diff = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n        \n    mask = cv2.GaussianBlur(diff, (3,3), 5)\n    mask = cv2.erode(diff, skinkernel, iterations = 1)\n    mask = cv2.dilate(diff, skinkernel, iterations = 1)\n    res = cv2.bitwise_and(roi, roi, mask = mask)\n    \n    if saveImg == True:\n        saveROIImg(res)\n    elif guessGesture == True and (framecount % 5) == 4:\n        t = threading.Thread(target=myNN.guessGesture, args = [mod, res])\n        t.start()\n        #t.join()\n        #myNN.update(plot)\n        \n    elif visualize == True:\n        layer = int(input(""Enter which layer to visualize ""))\n        cv2.waitKey(0)\n        myNN.visualizeLayers(mod, res, layer)\n        visualize = False\n    \n    \n    return res\n\t\n\t\n\t\n#%%\ndef Main():\n    global guessGesture, visualize, mod, binaryMode, bkgrndSubMode, mask, takebkgrndSubMask, x0, y0, width, height, saveImg, gestname, path\n    quietMode = False\n    \n    font = cv2.FONT_HERSHEY_SIMPLEX\n    size = 0.5\n    fx = 10\n    fy = 350\n    fh = 18\n\n        \n    #Call CNN model loading callback\n    while True:\n        ans = int(input( banner))\n        if ans == 1:\n            mod = myNN.loadCNN()\n            break\n        elif ans == 2:\n            mod = myNN.loadCNN(True)\n            myNN.trainModel(mod)\n            input(""Press any key to continue"")\n            break\n        elif ans == 3:\n            if not mod:\n                mod = myNN.loadCNN()\n            else:\n                print(""Will load default weight file"")\n            \n            myNN.visualizeLayers(mod)\n            input(""Press any key to continue"")\n            continue\n        \n        else:\n            print(""Get out of here!!!"")\n            return 0\n        \n    ## Grab camera input\n    cap = cv2.VideoCapture(0)\n    cv2.namedWindow(\'Original\', cv2.WINDOW_NORMAL)\n\n    # set rt size as 640x480\n    ret = cap.set(3,640)\n    ret = cap.set(4,480)\n\n    framecount = 0\n    fps = """"\n    start = time.time()\n\n    plot = np.zeros((512,512,3), np.uint8)\n    \n    while(True):\n        ret, frame = cap.read()\n        max_area = 0\n        \n        frame = cv2.flip(frame, 3)\n        frame = cv2.resize(frame, (640,480))\n                      \n        if ret == True:\n            if bkgrndSubMode == True:\n                roi = bkgrndSubMask(frame, x0, y0, width, height, framecount, plot)\n            elif binaryMode == True:\n                roi = binaryMask(frame, x0, y0, width, height, framecount, plot)\n            else:\n                roi = skinMask(frame, x0, y0, width, height, framecount, plot)\n\n            \n            framecount = framecount + 1\n            end  = time.time()\n            timediff = (end - start)\n            if( timediff >= 1):\n                #timediff = end - start\n                fps = \'FPS:%s\' %(framecount)\n                start = time.time()\n                framecount = 0\n\n        cv2.putText(frame,fps,(10,20), font, 0.7,(0,255,0),2,1)\n        cv2.putText(frame,\'Options:\',(fx,fy), font, 0.7,(0,255,0),2,1)\n        cv2.putText(frame,\'b - Toggle Binary/SkinMask\',(fx,fy + fh), font, size,(0,255,0),1,1)\n        cv2.putText(frame,\'x - Toggle Background Sub Mask\',(fx,fy + 2*fh), font, size,(0,255,0),1,1)\t\t\n        cv2.putText(frame,\'g - Toggle Prediction Mode\',(fx,fy + 3*fh), font, size,(0,255,0),1,1)\n        cv2.putText(frame,\'q - Toggle Quiet Mode\',(fx,fy + 4*fh), font, size,(0,255,0),1,1)\n        cv2.putText(frame,\'n - To enter name of new gesture folder\',(fx,fy + 5*fh), font, size,(0,255,0),1,1)\n        cv2.putText(frame,\'s - To start capturing new gestures for training\',(fx,fy + 6*fh), font, size,(0,255,0),1,1)\n        cv2.putText(frame,\'ESC - Exit\',(fx,fy + 7*fh), font, size,(0,255,0),1,1)\n        \n        \n        ## If enabled will stop updating the main openCV windows\n        ## Way to reduce some processing power :)\n        if not quietMode:\n            cv2.imshow(\'Original\',frame)\n            cv2.imshow(\'ROI\', roi)\n\n            if guessGesture == True:\n                plot = np.zeros((512,512,3), np.uint8)\n                plot = myNN.update(plot)\n            \n            cv2.imshow(\'Gesture Probability\',plot)\n            #plot = np.zeros((512,512,3), np.uint8)\n        \n        ############## Keyboard inputs ##################\n        key = cv2.waitKey(5) & 0xff\n        \n        ## Use Esc key to close the program\n        if key == 27:\n            break\n        \n        ## Use b key to toggle between binary threshold or skinmask based filters\n        elif key == ord(\'b\'):\n            binaryMode = not binaryMode\n            bkgrndSubMode = False\n            if binaryMode:\n                print(""Binary Threshold filter active"")\n            else:\n                print(""SkinMask filter active"")\n        \n\t## Use x key to use and refresh Background SubMask filter\n        elif key == ord(\'x\'):\n            takebkgrndSubMask = True\n            bkgrndSubMode = True\n            print(""BkgrndSubMask filter active"")\n        \n\t\t\n        ## Use g key to start gesture predictions via CNN\n        elif key == ord(\'g\'):\n            guessGesture = not guessGesture\n            print(""Prediction Mode - {}"".format(guessGesture))\n        \n        ## This option is not yet complete. So disabled for now\n        ## Use v key to visualize layers\n        #elif key == ord(\'v\'):\n        #    visualize = True\n\n        ## Use i,j,k,l to adjust ROI window\n        elif key == ord(\'i\'):\n            y0 = y0 - 5\n        elif key == ord(\'k\'):\n            y0 = y0 + 5\n        elif key == ord(\'j\'):\n            x0 = x0 - 5\n        elif key == ord(\'l\'):\n            x0 = x0 + 5\n\n        ## Quiet mode to hide gesture window\n        elif key == ord(\'q\'):\n            quietMode = not quietMode\n            print(""Quiet Mode - {}"".format(quietMode))\n\n        ## Use s key to start/pause/resume taking snapshots\n        ## numOfSamples controls number of snapshots to be taken PER gesture\n        elif key == ord(\'s\'):\n            saveImg = not saveImg\n            \n            if gestname != \'\':\n                saveImg = True\n            else:\n                print(""Enter a gesture group name first, by pressing \'n\'"")\n                saveImg = False\n        \n        ## Use n key to enter gesture name\n        elif key == ord(\'n\'):\n            gestname = input(""Enter the gesture folder name: "")\n            try:\n                os.makedirs(gestname)\n            except OSError as e:\n                # if directory already present\n                if e.errno != 17:\n                    print(\'Some issue while creating the directory named -\' + gestname)\n            \n            path = ""./""+gestname+""/""\n        \n        #elif key != 255:\n        #    print key\n\n    #Realse & destroy\n    cap.release()\n    cv2.destroyAllWindows()\n\nif __name__ == ""__main__"":\n    Main()\n\n'"
