file_path,api_count,code
code/lip_tracking/VisualizeLip.py,0,"b'import numpy as np\nimport cv2\nimport dlib\nimport math\nimport sys\nimport pickle\nimport argparse\nimport os\nimport skvideo.io\n\n\n""""""\nPART1: Construct the argument parse and parse the arguments\n""""""\nap = argparse.ArgumentParser()\nap.add_argument(""-i"", ""--input"", required=True,\n                help=""path to input video file"")\nap.add_argument(""-o"", ""--output"", required=True,\n                help=""path to output video file"")\nap.add_argument(""-f"", ""--fps"", type=int, default=30,\n                help=""FPS of output video"")\nap.add_argument(""-c"", ""--codec"", type=str, default=""MJPG"",\n                help=""codec of output video"")\nargs = vars(ap.parse_args())\n\n""""""\nPART2: Calling and defining required parameters for:\n\n       1 - Processing video for extracting each frame.\n       2 - Lip extraction from frames.\n""""""\n\n# Dlib requirements.\npredictor_path = \'dlib/shape_predictor_68_face_landmarks.dat\'\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\nmouth_destination_path = os.path.dirname(args[""output""]) + \'/\' + \'mouth\'\nif not os.path.exists(mouth_destination_path):\n    os.makedirs(mouth_destination_path)\n\ninputparameters = {}\noutputparameters = {}\nreader = skvideo.io.FFmpegReader(args[""input""],\n                inputdict=inputparameters,\n                outputdict=outputparameters)\nvideo_shape = reader.getShape()\n(num_frames, h, w, c) = video_shape\nprint(num_frames, h, w, c)\n\n# The required parameters\nactivation = []\nmax_counter = 150\ntotal_num_frames = int(video_shape[0])\nnum_frames = min(total_num_frames,max_counter)\ncounter = 0\nfont = cv2.FONT_HERSHEY_SIMPLEX\n\n# Define the writer\nwriter = skvideo.io.FFmpegWriter(args[""output""])\n\n\n# Required parameters for mouth extraction.\nwidth_crop_max = 0\nheight_crop_max = 0\n\n\n\'\'\'\nProcessing parameters.\n\n    activation: set to one if the full mouth can be extracted and set to zero otherwise.\n    max_counter: How many frames will be processed.\n    total_num_frames: Total number of frames for the video.\n    num_frames: The number of frames which are subjected to be processed.\n    counter: The frame counter.\n\'\'\'\n\n""""""\nPART3: Processing the video.\n\nProcedure:\n     1 - Extracting each frame.\n     2 - Detect the mouth in the frame.\n     3 - Define a boarder around the mouth.\n     4 - Crop and save the mouth.\n\nTechnical considerations:\n     * - For the first frame the mouth is detected and by using a boarder the mouth is extracted and cropped.\n     * - After the first frame the size of the cropped windows remains fixed unless for the subsequent frames\n          a bigger windows is required. In such a case the windows size will be increased and it will be held\n          fixed again unless increasing the size becoming necessary again too.\n""""""\n# Loop over all frames.\nfor frame in reader.nextFrame():\n    print(\'frame_shape:\', frame.shape)\n\n    # Process the video and extract the frames up to a certain number and then stop processing.\n    if counter > num_frames:\n        break\n\n    # Detection of the frame\n    frame.setflags(write=True)\n    detections = detector(frame, 1)\n\n    # 20 mark for mouth\n    marks = np.zeros((2, 20))\n\n    # All unnormalized face features.\n    Features_Abnormal = np.zeros((190, 1))\n\n    # If the face is detected.\n    print(len(detections))\n    if len(detections) > 0:\n        for k, d in enumerate(detections):\n\n            # Shape of the face.\n            shape = predictor(frame, d)\n\n            co = 0\n            # Specific for the mouth.\n            for ii in range(48, 68):\n                """"""\n                This for loop is going over all mouth-related features.\n                X and Y coordinates are extracted and stored separately.\n                """"""\n                X = shape.part(ii)\n                A = (X.x, X.y)\n                marks[0, co] = X.x\n                marks[1, co] = X.y\n                co += 1\n\n            # Get the extreme points(top-left & bottom-right)\n            X_left, Y_left, X_right, Y_right = [int(np.amin(marks, axis=1)[0]), int(np.amin(marks, axis=1)[1]),\n                                                int(np.amax(marks, axis=1)[0]),\n                                                int(np.amax(marks, axis=1)[1])]\n\n            # Find the center of the mouth.\n            X_center = (X_left + X_right) / 2.0\n            Y_center = (Y_left + Y_right) / 2.0\n\n            # Make a boarder for cropping.\n            border = 30\n            X_left_new = X_left - border\n            Y_left_new = Y_left - border\n            X_right_new = X_right + border\n            Y_right_new = Y_right + border\n\n            # Width and height for cropping(before and after considering the border).\n            width_new = X_right_new - X_left_new\n            height_new = Y_right_new - Y_left_new\n            width_current = X_right - X_left\n            height_current = Y_right - Y_left\n\n            # Determine the cropping rectangle dimensions(the main purpose is to have a fixed area).\n            if width_crop_max == 0 and height_crop_max == 0:\n                width_crop_max = width_new\n                height_crop_max = height_new\n            else:\n                width_crop_max += 1.5 * np.maximum(width_current - width_crop_max, 0)\n                height_crop_max += 1.5 * np.maximum(height_current - height_crop_max, 0)\n\n            # # # Uncomment if the lip area is desired to be rectangular # # # #\n            #########################################################\n            # Find the cropping points(top-left and bottom-right).\n            X_left_crop = int(X_center - width_crop_max / 2.0)\n            X_right_crop = int(X_center + width_crop_max / 2.0)\n            Y_left_crop = int(Y_center - height_crop_max / 2.0)\n            Y_right_crop = int(Y_center + height_crop_max / 2.0)\n            #########################################################\n\n            # # # # # Uncomment if the lip area is desired to be rectangular # # # #\n            # #######################################\n            # # Use this part if the cropped area should look like a square.\n            # crop_length_max = max(width_crop_max, height_crop_max) / 2\n            #\n            # # Find the cropping points(top-left and bottom-right).\n            # X_left_crop = int(X_center - crop_length_max)\n            # X_right_crop = int(X_center + crop_length_max)\n            # Y_left_crop = int(Y_center - crop_length_max)\n            # Y_right_crop = int(Y_center + crop_length_max)\n            #########################################\n\n            if X_left_crop >= 0 and Y_left_crop >= 0 and X_right_crop < w and Y_right_crop < h:\n                mouth = frame[Y_left_crop:Y_right_crop, X_left_crop:X_right_crop, :]\n\n                # Save the mouth area.\n                mouth_gray = cv2.cvtColor(mouth, cv2.COLOR_RGB2GRAY)\n                cv2.imwrite(mouth_destination_path + \'/\' + \'frame\' + \'_\' + str(counter) + \'.png\', mouth_gray)\n\n                print(""The cropped mouth is detected ..."")\n                activation.append(1)\n            else:\n                cv2.putText(frame, \'The full mouth is not detectable. \', (30, 30), font, 1, (0, 255, 255), 2)\n                print(""The full mouth is not detectable. ..."")\n                activation.append(0)\n\n    else:\n        cv2.putText(frame, \'Mouth is not detectable. \', (30, 30), font, 1, (0, 0, 255), 2)\n        print(""Mouth is not detectable. ..."")\n        activation.append(0)\n\n\n    if activation[counter] == 1:\n        # Demonstration of face.\n        cv2.rectangle(frame, (X_left_crop, Y_left_crop), (X_right_crop, Y_right_crop), (0, 255, 0), 2)\n\n    # cv2.imshow(\'frame\', frame)\n    print(\'frame number %d of %d\' % (counter, num_frames))\n\n    # write the output frame to file\n    print(""writing frame %d with activation %d"" % (counter + 1, activation[counter]))\n    writer.writeFrame(frame)\n    counter += 1\n\nwriter.close()\n\n""""""\nPART4: Save the activation vector as a list.\n\nThe python script for loading a list:\n    with open(the_filename, \'rb\') as f:\n        my_list = pickle.load(f)\n""""""\n\nthe_filename = os.path.dirname(args[""output""]) + \'/\' + \'activation\'\nmy_list = activation\nwith open(the_filename, \'wb\') as f:\n    pickle.dump(my_list, f)\n\n'"
code/speech-input/input_feature.py,0,"b'import os\nfrom scipy.io.wavfile import read\nimport scipy.io.wavfile as wav\nimport subprocess as sp\nimport numpy as np\nimport argparse\nimport random\nimport os\nimport sys\nfrom random import shuffle\nimport speechpy\nimport datetime\n\n\n######################################\n####### Define the dataset class #####\n######################################\nclass AudioDataset():\n    """"""Audio dataset.""""""\n\n    def __init__(self, files_path, audio_dir, transform=None):\n        """"""\n        Args:\n            files_path (string): Path to the .txt file which the address of files are saved in it.\n            root_dir (string): Directory with all the audio files.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n\n        # self.sound_files = [x.strip() for x in content]\n        self.audio_dir = audio_dir\n        self.transform = transform\n\n        # Open the .txt file and create a list from each line.\n        with open(files_path, \'r\') as f:\n            content = f.readlines()\n        # you may also want to remove whitespace characters like `\\n` at the end of each line\n        list_files = []\n        for x in content:\n            sound_file_path = os.path.join(self.audio_dir, x.strip().split()[1])\n            try:\n                with open(sound_file_path, \'rb\') as f:\n                    riff_size, _ = wav._read_riff_chunk(f)\n                    file_size = os.path.getsize(sound_file_path)\n\n                # Assertion error.\n                assert riff_size == file_size and os.path.getsize(sound_file_path) > 1000, ""Bad file!""\n\n                # Add to list if file is OK!\n                list_files.append(x.strip())\n            except OSError as err:\n                print(""OS error: {0}"".format(err))\n            except ValueError:\n                print(\'file %s is corrupted!\' % sound_file_path)\n            # except:\n            #     print(""Unexpected error:"", sys.exc_info()[0])\n            #     raise\n\n        # Save the correct and healthy sound files to a list.\n        self.sound_files = list_files\n\n    def __len__(self):\n        return len(self.sound_files)\n\n    def __getitem__(self, idx):\n        # Get the sound file path\n        sound_file_path = os.path.join(self.audio_dir, self.sound_files[idx].split()[1])\n\n        ##############################\n        ### Reading and processing ###\n        ##############################\n\n        # Reading .wav file\n        fs, signal = wav.read(sound_file_path)\n\n        # Reading .wav file\n        import soundfile as sf\n        signal, fs = sf.read(sound_file_path)\n\n        ###########################\n        ### Feature Extraction ####\n        ###########################\n\n        # DEFAULTS:\n        num_coefficient = 40\n\n        # Staching frames\n        frames = speechpy.processing.stack_frames(signal, sampling_frequency=fs, frame_length=0.02,\n                                                  frame_stride=0.02,\n                                                  zero_padding=True)\n\n        # # Extracting power spectrum (choosing 3 seconds and elimination of DC)\n        power_spectrum = speechpy.processing.power_spectrum(frames, fft_points=2 * num_coefficient)[:, 1:]\n\n        logenergy = speechpy.feature.lmfe(signal, sampling_frequency=fs, frame_length=0.02, frame_stride=0.02,\n                                          num_filters=num_coefficient, fft_length=1024, low_frequency=0,\n                                          high_frequency=None)\n        \n\n        ########################\n        ### Handling sample ####\n        ########################\n\n        # Label extraction\n        label = int(self.sound_files[idx].split()[0])\n\n        sample = {\'feature\': logenergy, \'label\': label}\n\n        ########################\n        ### Post Processing ####\n        ########################\n        if self.transform:\n            sample = self.transform(sample)\n        else:\n            feature, label = sample[\'feature\'], sample[\'label\']\n            sample = feature, label\n\n        return sample\n        # return sample\n\n\nclass CMVN(object):\n    """"""Cepstral mean variance normalization.\n\n    """"""\n\n    def __call__(self, sample):\n        feature, label = sample[\'feature\'], sample[\'label\']\n\n        # Mean variance normalization of the spectrum.\n        # The following line should be Uncommented if cepstral mean variance normalization is desired!\n        feature = speechpy.processing.cmvn(feature, variance_normalization=True)\n\n        return {\'feature\': feature, \'label\': label}\n\nclass Extract_Derivative(object):\n    """"""\n    Extract derivative features.\n\n    """"""\n\n    def __call__(self, sample):\n        feature, label = sample[\'feature\'], sample[\'label\']\n\n        # Extract derivative features\n        feature = speechpy.feature.extract_derivative_feature(feature)\n\n        return {\'feature\': feature, \'label\': label}\n    \n\nclass Feature_Cube(object):\n    """"""Return a feature cube of desired size.\n\n    Args:\n        cube_shape (tuple): The shape of the feature cube.\n        ex: cube_shape=(15,40,3)\n    """"""\n\n    def __init__(self, cube_shape):\n        \n        self.cube_shape = cube_shape\n        if self.cube_shape != None:\n            self.num_frames = cube_shape[0]\n            self.num_features = cube_shape[1]\n            self.num_channels = cube_shape[2]\n\n\n    def __call__(self, sample):\n        feature, label = sample[\'feature\'], sample[\'label\']         \n\n        if self.cube_shape != None:\n            feature_cube = np.zeros((self.num_frames, self.num_features, self.num_channels), dtype=np.float32)\n            feature_cube = feature[0:self.num_frames, :, :]\n        else:\n            feature_cube = feature\n                 \n        \n        # return {\'feature\': feature_cube, \'label\': label}\n        return {\'feature\': feature_cube[None, :, :, :], \'label\': label}\n\n\nclass ToOutput(object):\n    """"""Return the output.\n\n    """"""\n\n    def __call__(self, sample):\n        feature, label = sample[\'feature\'], sample[\'label\']\n\n        return feature, label\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> Compose([\n        >>>     CMVN(),\n        >>>     Feature_Cube(cube_shape=(20, 80, 40),\n        >>>     augmentation=True), ToOutput(),\n        >>>        ])\n        If necessary, for the details of this class, please refer to Pytorch documentation.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        for t in self.transforms:\n            format_string += \'\\n\'\n            format_string += \'    {0}\'.format(t)\n        format_string += \'\\n)\'\n        return format_string\n\n\nif __name__ == \'__main__\':\n    # add parser\n    parser = argparse.ArgumentParser(description=\'Input pipeline\')\n\n    # The text file in which the paths to the audio files are available.\n    # The path are relative to the directory of the audio files\n    # Format of each line of the txt file is ""class_label subject_dir/sound_file_name.ext""\n    # Example of each line: 0 subject/sound.wav\n    parser.add_argument(\'--file_path\',\n                        default=os.path.expanduser(\n                            \'~/github/3D-convolutional-speaker-recognition/code/0-input/file_path.txt\'),\n                        help=\'The file names for development phase\')\n\n    # The directory of the audio files separated by subject\n    parser.add_argument(\'--audio_dir\',\n                        default=os.path.expanduser(\'~/github/lip-reading-deeplearning/code/speech-input/Audio\'),\n                        help=\'Location of sound files\')\n    args = parser.parse_args()\n\n    dataset = AudioDataset(files_path=args.file_path, audio_dir=args.audio_dir,\n                           transform=Compose([Extract_Derivative(), Feature_Cube(cube_shape=None), ToOutput()]))\n    idx = 0\n    feature, label = dataset.__getitem__(idx)\n    print(feature.shape)\n    print(label)\n'"
code/training_evaluation/test.py,83,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport sys\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom tensorflow.python.ops import control_flow_ops\nfrom nets import nets_factory\nfrom auxiliary import losses\nfrom roc_curve import calculate_roc\nimport os\n# import matplotlib.pyplot as plt\nslim = tf.contrib.slim\n\n######################\n# Train Directory #\n######################\n\ntf.app.flags.DEFINE_string(\n    \'test_dir\', \'results/TRAIN_CNN_3D/test_logs\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_dir\', os.path.expanduser(\'~/results/\'),\n    \'Directory where checkpoints and event logs are written to.\')\n\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 1,\n    \'The frequency with which logs are print.\')\n\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'adam\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n \ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 5.0,\n    \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'model_speech_name\', \'lipread_speech\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_mouth_name\', \'lipread_mouth\', \'The name of the architecture to train.\')\n\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 128, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_epochs\', 20, \'The number of epochs for training.\')\n\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune. ex:/home/user/TRAIN/train_logs\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring\'\n    \'from a checkpoint. ex: vgg_19/fc8/biases,vgg_19/fc8/weights\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\n# Store all elemnts in FLAG structure!\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n    """"""Configures the learning rate.\n\n    Args:\n      num_samples_per_epoch: The number of samples in each epoch of training.\n      global_step: The global_step tensor.\n\n    Returns:\n      A `Tensor` representing the learning rate.\n\n    Raises:\n      ValueError: if\n    """"""\n    decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                      FLAGS.num_epochs_per_decay)\n    if FLAGS.sync_replicas:\n        decay_steps /= FLAGS.replicas_to_aggregate\n\n    if FLAGS.learning_rate_decay_type == \'exponential\':\n        return tf.train.exponential_decay(FLAGS.learning_rate,\n                                          global_step,\n                                          decay_steps,\n                                          FLAGS.learning_rate_decay_factor,\n                                          staircase=True,\n                                          name=\'exponential_decay_learning_rate\')\n    elif FLAGS.learning_rate_decay_type == \'fixed\':\n        return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n    elif FLAGS.learning_rate_decay_type == \'polynomial\':\n        return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                         global_step,\n                                         decay_steps,\n                                         FLAGS.end_learning_rate,\n                                         power=1.0,\n                                         cycle=False,\n                                         name=\'polynomial_decay_learning_rate\')\n    else:\n        raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                         FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n    """"""Configures the optimizer used for training.\n\n    Args:\n      learning_rate: A scalar or `Tensor` learning rate.\n\n    Returns:\n      An instance of an optimizer.\n\n    Raises:\n      ValueError: if FLAGS.optimizer is not recognized.\n    """"""\n\n    if FLAGS.optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate,\n            beta1=FLAGS.adam_beta1,\n            beta2=FLAGS.adam_beta2,\n            epsilon=FLAGS.opt_epsilon)\n    elif FLAGS.optimizer == \'sgd\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n    return optimizer\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n\n    Note that this function provides a synchronization point across all towers.\n\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef _get_variables_to_train():\n    """"""Returns a list of variables to train.\n\n    Returns:\n      A list of variables to train by the optimizer.\n    """"""\n    if FLAGS.trainable_scopes is None:\n        return tf.trainable_variables()\n    else:\n        scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n    variables_to_train = []\n    for scope in scopes:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        variables_to_train.extend(variables)\n    return variables_to_train\n\n\n# Definign arbitrary data\nnum_training_samples = 1000\nnum_testing_samples = 1000\ntrain_data = {}\n\ntrain_data = {\'mouth\': np.random.random_sample(size=(num_training_samples, 9, 60, 100, 1)),\n              \'speech\': np.random.random_sample(size=(num_training_samples, 15, 40, 1, 3))}\ntest_data = {\'mouth\': np.random.random_sample(size=(num_testing_samples, 9, 60, 100, 1)),\n             \'speech\': np.random.random_sample(size=(num_testing_samples, 15, 40, 1, 3))}\n\ntrain_label = np.random.randint(2, size=(num_training_samples, 1))\ntest_label = np.random.randint(2, size=(num_testing_samples, 1))\n\n\n# # Uncomment if data standardalization is required and the mean and std vectors have been calculated.\n# ############ Get the mean vectors ####################\n#\n# # mean mouth\n# mean_mouth = np.load(\'/path/to/mean/file/mouth.npy\')\n# # mean_mouth = np.tile(mean_mouth.reshape(47, 73, 1), (1, 1, 9))\n# mean_mouth = mean_mouth[None, :]\n# mean_channel_mouth = np.mean(mean_mouth)\n#\n# # mean speech\n# mean_speech = np.load(\'/path/to/mean/file/speech.npy\')\n# mean_speech = mean_speech[None, :]\n# # mean_channel_speech = np.hstack((\n# #     [np.mean(mean_speech[:, :, :, 0])], [np.mean(mean_speech[:, :, :, 1])], [np.mean(mean_speech[:, :, :, 2])]))\n#\n# ############ Get the std vectors ####################\n#\n# # mean std\n# std_mouth = np.load(\'/path/to/std/file/mouth.npy\')\n# std_mouth = np.tile(std_mouth.reshape(60, 100, 1), (1, 1, 9))\n# std_mouth = std_mouth[None, :]\n#\n# # mean speech\n# std_speech = np.load(\'/path/to/std/file/speech.npy\')\n# std_speech = std_speech[None, :]\n\n\n\n\ndef main(_):\n\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    graph = tf.Graph()\n    with graph.as_default(), tf.device(\'/cpu:0\'):\n        ######################\n        # Config model_deploy#\n        ######################\n\n        # required from data\n        num_samples_per_epoch = train_data[\'mouth\'].shape[0]\n        num_batches_per_epoch = int(num_samples_per_epoch / FLAGS.batch_size)\n\n        num_samples_per_epoch_test = test_data[\'mouth\'].shape[0]\n        num_batches_per_epoch_test = int(num_samples_per_epoch_test / FLAGS.batch_size)\n\n        # Create global_step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        #########################################\n        # Configure the larning rate. #\n        #########################################\n        learning_rate = _configure_learning_rate(num_samples_per_epoch, global_step)\n        opt = _configure_optimizer(learning_rate)\n\n        ######################\n        # Select the network #\n        ######################\n        is_training = tf.placeholder(tf.bool)\n\n        network_speech_fn = nets_factory.get_network_fn(\n            FLAGS.model_speech_name,\n            num_classes=2,\n            weight_decay=FLAGS.weight_decay,\n            is_training=is_training)\n\n        network_mouth_fn = nets_factory.get_network_fn(\n            FLAGS.model_mouth_name,\n            num_classes=2,\n            weight_decay=FLAGS.weight_decay,\n            is_training=is_training)\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n\n        # TODO: Do some preprocessing if necessary.\n\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        # with tf.device(deploy_config.inputs_device()):\n        """"""\n        Define the place holders and creating the batch tensor.\n        """"""\n\n        # Mouth spatial set\n        INPUT_SEQ_LENGTH = 9\n        INPUT_HEIGHT = 60\n        INPUT_WIDTH = 100\n        INPUT_CHANNELS = 1\n        batch_mouth = tf.placeholder(tf.float32, shape=(\n            [None, INPUT_SEQ_LENGTH, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS]))\n\n        # Speech spatial set\n        INPUT_SEQ_LENGTH_SPEECH = 15\n        INPUT_HEIGHT_SPEECH = 40\n        INPUT_WIDTH_SPEECH = 1\n        INPUT_CHANNELS_SPEECH = 3\n        batch_speech = tf.placeholder(tf.float32, shape=(\n            [None, INPUT_SEQ_LENGTH_SPEECH, INPUT_HEIGHT_SPEECH, INPUT_WIDTH_SPEECH, INPUT_CHANNELS_SPEECH]))\n\n        # Label\n        batch_labels = tf.placeholder(tf.uint8, (None, 1))\n        margin_imp_tensor = tf.placeholder(tf.float32, ())\n\n        ################################\n        ## Feed forwarding to network ##\n        ################################\n        tower_grads = []\n        with tf.variable_scope(tf.get_variable_scope()):\n            with tf.device(\'/gpu:%d\' % 0):\n                with tf.name_scope(\'%s_%d\' % (\'tower\', 0)) as scope:\n                    """"""\n                    Two distance metric are defined:\n                       1 - distance_weighted: which is a weighted average of the distance between two structures.\n                       2 - distance_l2: which is the regular l2-norm of the two networks outputs.\n                    Place holders\n\n                    """"""\n                    ########################################\n                    ######## Outputs of two networks #######\n                    ########################################\n\n                    logits_speech, end_points_speech = network_speech_fn(batch_speech)\n                    logits_mouth, end_points_mouth = network_mouth_fn(batch_mouth)\n\n                    # # Uncomment if the output embedding is desired to be as |f(x)| = 1\n                    # logits_speech = tf.nn.l2_normalize(logits_speech, dim=1, epsilon=1e-12, name=None)\n                    # logits_mouth = tf.nn.l2_normalize(logits_mouth, dim=1, epsilon=1e-12, name=None)\n\n                    #################################################\n                    ########### Loss Calculation ####################\n                    #################################################\n\n                    # ##### Weighted distance using a fully connected layer #####\n                    # distance_vector = tf.subtract(logits_speech, logits_mouth,  name=None)\n                    # distance_weighted = slim.fully_connected(distance_vector, 1, activation_fn=tf.nn.sigmoid,\n                    #                                          normalizer_fn=None,\n                    #                                          scope=\'fc_weighted\')\n\n                    ##### Euclidean distance ####\n                    distance_l2 = tf.sqrt(\n                        tf.reduce_sum(tf.pow(tf.subtract(logits_speech, logits_mouth), 2), 1, keep_dims=True))\n\n                    ##### Contrastive loss ######\n                    loss = losses.contrastive_loss(batch_labels, distance_l2, margin_imp=margin_imp_tensor,\n                                                   scope=scope)\n\n                    # ##### call the optimizer ######\n                    # # TODO: call optimizer object outside of this gpu environment\n                    #\n                    # Reuse variables for the next tower.\n                    tf.get_variable_scope().reuse_variables()\n\n                    # Calculate the gradients for the batch of data on this CIFAR tower.\n                    grads = opt.compute_gradients(loss)\n\n                    # Keep track of the gradients across all towers.\n                    tower_grads.append(grads)\n\n\n        # Calculate the mean of each gradient.\n        grads = average_gradients(tower_grads)\n\n        # Apply the gradients to adjust the shared variables.\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n        # Track the moving averages of all trainable variables.\n        MOVING_AVERAGE_DECAY = 0.9999\n        variable_averages = tf.train.ExponentialMovingAverage(\n            MOVING_AVERAGE_DECAY, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        # Group all updates to into a single train op.\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n        #################################################\n        ########### Summary Section #####################\n        #################################################\n\n        # Gather initial summaries.\n        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        # Add summaries for all end_points.\n        for end_point in end_points_speech:\n            x = end_points_speech[end_point]\n            # summaries.add(tf.summary.histogram(\'activations_speech/\' + end_point, x))\n            summaries.add(tf.summary.scalar(\'sparsity_speech/\' + end_point,\n                                            tf.nn.zero_fraction(x)))\n\n        for end_point in end_points_mouth:\n            x = end_points_mouth[end_point]\n            # summaries.add(tf.summary.histogram(\'activations_mouth/\' + end_point, x))\n            summaries.add(tf.summary.scalar(\'sparsity_mouth/\' + end_point,\n                                            tf.nn.zero_fraction(x)))\n\n        # Add summaries for variables.\n        for variable in slim.get_model_variables():\n            summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n        # Add to parameters to summaries\n        summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n        summaries.add(tf.summary.scalar(\'eval/Loss\', loss))\n        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        # Merge all summaries together.\n        summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n    ###########################\n    ######## Training #########\n    ###########################\n\n    with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n\n        # Initialization of the network.\n        variables_to_restore = slim.get_variables_to_restore()\n        saver = tf.train.Saver(variables_to_restore, max_to_keep=20)\n        coord = tf.train.Coordinator()\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n        # Restore the model\n        print(\'Loading from:\',FLAGS.checkpoint_dir)\n        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir=FLAGS.checkpoint_dir)\n        saver.restore(sess, latest_checkpoint)\n\n        # op to write logs to Tensorboard\n        summary_writer = tf.summary.FileWriter(FLAGS.test_dir, graph=graph)\n\n        ###################################################\n        ############################ TEST  ################\n        ###################################################\n        score_dissimilarity_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\n        label_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\n\n        # Loop over all batches\n        for i in range(num_batches_per_epoch_test):\n            start_idx = i * FLAGS.batch_size\n            end_idx = (i + 1) * FLAGS.batch_size\n            speech_test, mouth_test, label_test = test_data[\'speech\'][start_idx:end_idx], test_data[\'mouth\'][\n                                                                                          start_idx:end_idx], test_label[\n                                                                                                              start_idx:end_idx]\n\n            # # # Uncomment if standardalization is needed\n            # # mean subtraction if necessary\n            # speech_test = (speech_test - mean_speech) / std_speech\n            # mouth_test = (mouth_test - mean_mouth) / std_mouth\n\n            # Evaluation phase\n            # WARNING: margin_imp_tensor has no effect here but it needs to be there because its tensor required a value to feed in!!\n            loss_value, score_dissimilarity, _ = sess.run([loss, distance_l2, is_training],\n                                                          feed_dict={is_training: False,\n                                                                     margin_imp_tensor: 50,\n                                                                     batch_speech: speech_test,\n                                                                     batch_mouth: mouth_test,\n                                                                     batch_labels: label_test.reshape(\n                                                                         [FLAGS.batch_size, 1])})\n            if (i + 1) % FLAGS.log_every_n_steps == 0:\n                print(""TESTING:"" + "", Minibatch "" + str(\n                    i + 1) + "" of %d "" % num_batches_per_epoch_test)\n            score_dissimilarity_vector[start_idx:end_idx] = score_dissimilarity\n            label_vector[start_idx:end_idx] = label_test\n\n        ##############################\n        ##### K-fold validation ######\n        ##############################\n        K = 10\n        EER = np.zeros((K, 1))\n        AUC = np.zeros((K, 1))\n        AP = np.zeros((K, 1))\n        batch_k_validation = int(label_vector.shape[0] / float(K))\n\n        for i in range(K):\n            EER[i, :], AUC[i, :], AP[i, :], fpr, tpr = calculate_roc.calculate_eer_auc_ap(\n                label_vector[i * batch_k_validation:(i + 1) * batch_k_validation],\n                score_dissimilarity_vector[i * batch_k_validation:(i + 1) * batch_k_validation])\n\n        # Printing Equal Error Rate(EER), Area Under the Curve(AUC) and Average Precision(AP)\n        print(""TESTING:"" +"", EER= "" + str(np.mean(EER, axis=0)) + "", AUC= "" + str(\n            np.mean(AUC, axis=0)) + "", AP= "" + str(np.mean(AP, axis=0)))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
code/training_evaluation/train.py,82,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nimport sys\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom tensorflow.python.ops import control_flow_ops\nfrom nets import nets_factory\nfrom auxiliary import losses\nfrom roc_curve import calculate_roc\nimport os\n\nslim = tf.contrib.slim\n\n######################\n# Train Directory #\n######################\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', os.path.expanduser(\'~/results/TRAIN_CNN_3D\'),\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 1,\n    \'The frequency with which logs are print.\')\n\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'adam\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 5.0,\n    \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'model_speech_name\', \'lipread_speech\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_mouth_name\', \'lipread_mouth\', \'The name of the architecture to train.\')\n\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_epochs\', 1, \'The number of epochs for training.\')\n\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune. ex:/home/sina/TRAIN_CASIA/train_logs/vgg_19.cpkt\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring\'\n    \'from a checkpoint. ex: vgg_19/fc8/biases,vgg_19/fc8/weights\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\n# Store all elemnts in FLAG structure!\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n    """"""Configures the learning rate.\n\n    Args:\n      num_samples_per_epoch: The number of samples in each epoch of training.\n      global_step: The global_step tensor.\n\n    Returns:\n      A `Tensor` representing the learning rate.\n\n    Raises:\n      ValueError: if\n    """"""\n    decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                      FLAGS.num_epochs_per_decay)\n    if FLAGS.sync_replicas:\n        decay_steps /= FLAGS.replicas_to_aggregate\n\n    if FLAGS.learning_rate_decay_type == \'exponential\':\n        return tf.train.exponential_decay(FLAGS.learning_rate,\n                                          global_step,\n                                          decay_steps,\n                                          FLAGS.learning_rate_decay_factor,\n                                          staircase=True,\n                                          name=\'exponential_decay_learning_rate\')\n    elif FLAGS.learning_rate_decay_type == \'fixed\':\n        return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n    elif FLAGS.learning_rate_decay_type == \'polynomial\':\n        return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                         global_step,\n                                         decay_steps,\n                                         FLAGS.end_learning_rate,\n                                         power=1.0,\n                                         cycle=False,\n                                         name=\'polynomial_decay_learning_rate\')\n    else:\n        raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                         FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n    """"""Configures the optimizer used for training.\n\n    Args:\n      learning_rate: A scalar or `Tensor` learning rate.\n\n    Returns:\n      An instance of an optimizer.\n\n    Raises:\n      ValueError: if FLAGS.optimizer is not recognized.\n    """"""\n\n    if FLAGS.optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate,\n            beta1=FLAGS.adam_beta1,\n            beta2=FLAGS.adam_beta2,\n            epsilon=FLAGS.opt_epsilon)\n    elif FLAGS.optimizer == \'sgd\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n    return optimizer\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n\n    Note that this function provides a synchronization point across all towers.\n\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef _get_variables_to_train():\n    """"""Returns a list of variables to train.\n\n    Returns:\n      A list of variables to train by the optimizer.\n    """"""\n    if FLAGS.trainable_scopes is None:\n        return tf.trainable_variables()\n    else:\n        scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n    variables_to_train = []\n    for scope in scopes:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        variables_to_train.extend(variables)\n    return variables_to_train\n\n\n# Definign arbitrary data\nnum_training_samples = 1000\nnum_testing_samples = 1000\ntrain_data = {}\n\ntrain_data = {\'mouth\': np.random.random_sample(size=(num_training_samples, 9, 60, 100, 1)),\n              \'speech\': np.random.random_sample(size=(num_training_samples, 15, 40, 1, 3))}\ntest_data = {\'mouth\': np.random.random_sample(size=(num_testing_samples, 9, 60, 100, 1)),\n             \'speech\': np.random.random_sample(size=(num_testing_samples, 15, 40, 1, 3))}\n\ntrain_label = np.random.randint(2, size=(num_training_samples, 1))\ntest_label = np.random.randint(2, size=(num_testing_samples, 1))\n\n\n# # Uncomment if data standardalization is required and the mean and std vectors have been calculated.\n# ############ Get the mean vectors ####################\n#\n# # mean mouth\n# mean_mouth = np.load(\'/path/to/mean/file/mouth.npy\')\n# # mean_mouth = np.tile(mean_mouth.reshape(47, 73, 1), (1, 1, 9))\n# mean_mouth = mean_mouth[None, :]\n# mean_channel_mouth = np.mean(mean_mouth)\n#\n# # mean speech\n# mean_speech = np.load(\'/path/to/mean/file/speech.npy\')\n# mean_speech = mean_speech[None, :]\n# # mean_channel_speech = np.hstack((\n# #     [np.mean(mean_speech[:, :, :, 0])], [np.mean(mean_speech[:, :, :, 1])], [np.mean(mean_speech[:, :, :, 2])]))\n#\n# ############ Get the std vectors ####################\n#\n# # mean std\n# std_mouth = np.load(\'/path/to/std/file/mouth.npy\')\n# std_mouth = np.tile(std_mouth.reshape(60, 100, 1), (1, 1, 9))\n# std_mouth = std_mouth[None, :]\n#\n# # mean speech\n# std_speech = np.load(\'/path/to/std/file/speech.npy\')\n# std_speech = std_speech[None, :]\n\n\n\n\ndef main(_):\n\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    graph = tf.Graph()\n    with graph.as_default(), tf.device(\'/cpu:0\'):\n        ######################\n        # Config model_deploy#\n        ######################\n\n        # required from data\n        num_samples_per_epoch = train_data[\'mouth\'].shape[0]\n        num_batches_per_epoch = int(num_samples_per_epoch / FLAGS.batch_size)\n\n        num_samples_per_epoch_test = test_data[\'mouth\'].shape[0]\n        num_batches_per_epoch_test = int(num_samples_per_epoch_test / FLAGS.batch_size)\n\n        # Create global_step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        #########################################\n        # Configure the larning rate. #\n        #########################################\n        learning_rate = _configure_learning_rate(num_samples_per_epoch, global_step)\n        opt = _configure_optimizer(learning_rate)\n\n        ######################\n        # Select the network #\n        ######################\n        is_training = tf.placeholder(tf.bool)\n\n        network_speech_fn = nets_factory.get_network_fn(\n            FLAGS.model_speech_name,\n            num_classes=2,\n            weight_decay=FLAGS.weight_decay,\n            is_training=is_training)\n\n        network_mouth_fn = nets_factory.get_network_fn(\n            FLAGS.model_mouth_name,\n            num_classes=2,\n            weight_decay=FLAGS.weight_decay,\n            is_training=is_training)\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n\n        # TODO: Do some preprocessing if necessary.\n\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        # with tf.device(deploy_config.inputs_device()):\n        """"""\n        Define the place holders and creating the batch tensor.\n        """"""\n\n        # Mouth spatial set\n        INPUT_SEQ_LENGTH = 9\n        INPUT_HEIGHT = 60\n        INPUT_WIDTH = 100\n        INPUT_CHANNELS = 1\n        batch_mouth = tf.placeholder(tf.float32, shape=(\n            [None, INPUT_SEQ_LENGTH, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS]))\n\n        # Speech spatial set\n        INPUT_SEQ_LENGTH_SPEECH = 15\n        INPUT_HEIGHT_SPEECH = 40\n        INPUT_WIDTH_SPEECH = 1\n        INPUT_CHANNELS_SPEECH = 3\n        batch_speech = tf.placeholder(tf.float32, shape=(\n            [None, INPUT_SEQ_LENGTH_SPEECH, INPUT_HEIGHT_SPEECH, INPUT_WIDTH_SPEECH, INPUT_CHANNELS_SPEECH]))\n\n        # Label\n        batch_labels = tf.placeholder(tf.uint8, (None, 1))\n        margin_imp_tensor = tf.placeholder(tf.float32, ())\n\n        ################################\n        ## Feed forwarding to network ##\n        ################################\n        tower_grads = []\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(FLAGS.num_clones):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'%s_%d\' % (\'tower\', i)) as scope:\n                        """"""\n                        Two distance metric are defined:\n                           1 - distance_weighted: which is a weighted average of the distance between two structures.\n                           2 - distance_l2: which is the regular l2-norm of the two networks outputs.\n                        Place holders\n\n                        """"""\n                        ########################################\n                        ######## Outputs of two networks #######\n                        ########################################\n\n                        logits_speech, end_points_speech = network_speech_fn(batch_speech)\n                        logits_mouth, end_points_mouth = network_mouth_fn(batch_mouth)\n\n                        # # Uncomment if the output embedding is desired to be as |f(x)| = 1\n                        # logits_speech = tf.nn.l2_normalize(logits_speech, dim=1, epsilon=1e-12, name=None)\n                        # logits_mouth = tf.nn.l2_normalize(logits_mouth, dim=1, epsilon=1e-12, name=None)\n\n                        #################################################\n                        ########### Loss Calculation ####################\n                        #################################################\n\n                        # ##### Weighted distance using a fully connected layer #####\n                        # distance_vector = tf.subtract(logits_speech, logits_mouth,  name=None)\n                        # distance_weighted = slim.fully_connected(distance_vector, 1, activation_fn=tf.nn.sigmoid,\n                        #                                          normalizer_fn=None,\n                        #                                          scope=\'fc_weighted\')\n\n                        ##### Euclidean distance ####\n                        distance_l2 = tf.sqrt(\n                            tf.reduce_sum(tf.pow(tf.subtract(logits_speech, logits_mouth), 2), 1, keepdims=True))\n\n                        ##### Contrastive loss ######\n                        loss = losses.contrastive_loss(batch_labels, distance_l2, margin_imp=margin_imp_tensor,\n                                                       scope=scope)\n\n                        # ##### call the optimizer ######\n                        # # TODO: call optimizer object outside of this gpu environment\n                        #\n                        # Reuse variables for the next tower.\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this CIFAR tower.\n                        grads = opt.compute_gradients(loss)\n\n                        # Keep track of the gradients across all towers.\n                        tower_grads.append(grads)\n\n\n        # Calculate the mean of each gradient.\n        grads = average_gradients(tower_grads)\n\n        # Apply the gradients to adjust the shared variables.\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n        # Track the moving averages of all trainable variables.\n        MOVING_AVERAGE_DECAY = 0.9999\n        variable_averages = tf.train.ExponentialMovingAverage(\n            MOVING_AVERAGE_DECAY, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        # Group all updates to into a single train op.\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n        #################################################\n        ########### Summary Section #####################\n        #################################################\n\n        # Gather initial summaries.\n        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        # Add summaries for all end_points.\n        for end_point in end_points_speech:\n            x = end_points_speech[end_point]\n            # summaries.add(tf.summary.histogram(\'activations_speech/\' + end_point, x))\n            summaries.add(tf.summary.scalar(\'sparsity_speech/\' + end_point,\n                                            tf.nn.zero_fraction(x)))\n\n        for end_point in end_points_mouth:\n            x = end_points_mouth[end_point]\n            # summaries.add(tf.summary.histogram(\'activations_mouth/\' + end_point, x))\n            summaries.add(tf.summary.scalar(\'sparsity_mouth/\' + end_point,\n                                            tf.nn.zero_fraction(x)))\n\n        # Add summaries for variables.\n        for variable in slim.get_model_variables():\n            summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n        # Add to parameters to summaries\n        summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n        summaries.add(tf.summary.scalar(\'global_step\', global_step))\n        summaries.add(tf.summary.scalar(\'eval/Loss\', loss))\n        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        # Merge all summaries together.\n        summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n    ###########################\n    ######## Training #########\n    ###########################\n\n    with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n\n        # Initialization of the network.\n        variables_to_restore = slim.get_variables_to_restore()\n        saver = tf.train.Saver(variables_to_restore, max_to_keep=20)\n        coord = tf.train.Coordinator()\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n        # # Restore the model\n        # saver.restore(sess, \'/home/sina/TRAIN_LIPREAD/train_logs-1366\')\n\n        # op to write logs to Tensorboard\n        summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=graph)\n\n        #####################################\n        ############## TRAIN ################\n        #####################################\n\n        step = 1\n        for epoch in range(FLAGS.num_epochs):\n\n            # Loop over all batches\n\n            for batch_num in range(num_batches_per_epoch):\n                step += 1\n                start_idx = batch_num * FLAGS.batch_size\n                end_idx = (batch_num + 1) * FLAGS.batch_size\n                speech_train, mouth_train, label_train = train_data[\'speech\'][start_idx:end_idx], train_data[\'mouth\'][\n                                                                                                  start_idx:end_idx], train_label[\n                                                                                                                      start_idx:end_idx]\n\n                # # # Standardalization for speech if necessary\n                # speech_train = (speech_train - mean_speech) / std_speech\n                #\n                # # # Standardalization  for visual if necessary\n                # mouth_train = (mouth_train - mean_mouth) / std_mouth\n\n                #########################################################################\n                ################## Online Pair Selection Algorithm ######################\n                #########################################################################\n                online_pair_selection = True\n                if online_pair_selection:\n                    distance = sess.run(\n                        distance_l2,\n                        feed_dict={is_training: False, batch_speech: speech_train,\n                                   batch_mouth: mouth_train,\n                                   batch_labels: label_train.reshape([FLAGS.batch_size, 1])})\n                    label_keep = []\n\n                    ###############################\n                    hard_margin = 10\n\n                    # Max-Min distance in genuines\n                    max_gen = 0\n                    min_gen = 100\n                    for j in range(label_train.shape[0]):\n                        if label_train[j] == 1:\n                            if max_gen < distance[j, 0]:\n                                max_gen = distance[j, 0]\n                            if min_gen > distance[j, 0]:\n                                min_gen = distance[j, 0]\n\n                    # Min-Max distance in impostors\n                    min_imp = 100\n                    max_imp = 0\n                    for k in range(label_train.shape[0]):\n                        if label_train[k] == 0:\n                            if min_imp > distance[k, 0]:\n                                min_imp = distance[k, 0]\n                            if max_imp < distance[k, 0]:\n                                max_imp = distance[k, 0]\n\n                    ### Keeping hard impostors and genuines\n                    for i in range(label_train.shape[0]):\n                        # imposter\n                        if label_train[i] == 0:\n                            if distance[i, 0] < max_gen + hard_margin:\n                                label_keep.append(i)\n                        elif label_train[i] == 1:\n                            # if distance[i, 0] > min_imp - hard_margin:\n                            label_keep.append(i)\n\n                    #### Choosing the pairs ######\n                    speech_train = speech_train[label_keep]\n                    mouth_train = mouth_train[label_keep]\n                    label_train = label_train[label_keep]\n\n                ############################################\n                #### Running the training operation ########\n                _, loss_value, score_dissimilarity, summary, training_step, _ = sess.run(\n                    [train_op, loss, distance_l2, summary_op, global_step, is_training],\n                    feed_dict={is_training: True, margin_imp_tensor: 100,\n                               batch_speech: speech_train, batch_mouth: mouth_train,\n                               batch_labels: label_train.reshape([label_train.shape[0], 1])})\n                summary_writer.add_summary(summary, epoch * num_batches_per_epoch + i)\n\n                # try and error method is used to handle the error due to ROC calculation\n                try:\n                    # Calculation of ROC\n                    EER, AUC, AP, fpr, tpr = calculate_roc.calculate_eer_auc_ap(label_train, score_dissimilarity)\n\n                    if (batch_num + 1) % FLAGS.log_every_n_steps == 0:\n                        print(""Epoch "" + str(epoch + 1) + "", Minibatch "" + str(\n                            batch_num + 1) + "" of %d "" % num_batches_per_epoch + "", Minibatch Loss= "" + \\\n                              ""{:.6f}"".format(loss_value) + "", EER= "" + ""{:.5f}"".format(EER) + "", AUC= "" + ""{:.5f}"".format(\n                            AUC) + "", AP= "" + ""{:.5f}"".format(AP) + "", contrib = %d pairs"" % label_train.shape[0])\n                except:\n                    print(""Error: "" ,sys.exc_info()[0])\n                    print(""No contributing impostor pair!"")\n\n            # Save the model\n            saver.save(sess, FLAGS.train_dir, global_step=training_step)\n\n            ###################################################\n            ############## TEST PER EACH EPOCH ################\n            ###################################################\n            score_dissimilarity_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\n            label_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\n\n            # Loop over all batches\n            for i in range(num_batches_per_epoch_test):\n                start_idx = i * FLAGS.batch_size\n                end_idx = (i + 1) * FLAGS.batch_size\n                speech_test, mouth_test, label_test = test_data[\'speech\'][start_idx:end_idx], test_data[\'mouth\'][\n                                                                                              start_idx:end_idx], test_label[\n                                                                                                                  start_idx:end_idx]\n\n                # # # Uncomment if standardalization is needed\n                # # mean subtraction if necessary\n                # speech_test = (speech_test - mean_speech) / std_speech\n                # mouth_test = (mouth_test - mean_mouth) / std_mouth\n\n                # Evaluation phase\n                # WARNING: margin_imp_tensor has no effect here but it needs to be there because its tensor required a value to feed in!!\n                loss_value, score_dissimilarity, _ = sess.run([loss, distance_l2, is_training],\n                                                              feed_dict={is_training: False,\n                                                                         margin_imp_tensor: 50,\n                                                                         batch_speech: speech_test,\n                                                                         batch_mouth: mouth_test,\n                                                                         batch_labels: label_test.reshape(\n                                                                             [FLAGS.batch_size, 1])})\n                if (i + 1) % FLAGS.log_every_n_steps == 0:\n                    print(""TESTING: Epoch "" + str(epoch + 1) + "", Minibatch "" + str(\n                        i + 1) + "" of %d "" % num_batches_per_epoch_test)\n                score_dissimilarity_vector[start_idx:end_idx] = score_dissimilarity\n                label_vector[start_idx:end_idx] = label_test\n\n            ##############################\n            ##### K-fold validation ######\n            ##############################\n            K = 10\n            EER = np.zeros((K, 1))\n            AUC = np.zeros((K, 1))\n            AP = np.zeros((K, 1))\n            batch_k_validation = int(label_vector.shape[0] / float(K))\n\n            for i in range(K):\n                EER[i, :], AUC[i, :], AP[i, :], fpr, tpr = calculate_roc.calculate_eer_auc_ap(\n                    label_vector[i * batch_k_validation:(i + 1) * batch_k_validation],\n                    score_dissimilarity_vector[i * batch_k_validation:(i + 1) * batch_k_validation])\n\n            # Printing Equal Error Rate(EER), Area Under the Curve(AUC) and Average Precision(AP)\n            print(""TESTING: Epoch "" + str(epoch + 1) + "", EER= "" + str(np.mean(EER, axis=0)) + "", AUC= "" + str(\n                np.mean(AUC, axis=0)) + "", AP= "" + str(np.mean(AP, axis=0)))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
code/training_evaluation/auxiliary/__init__.py,0,b''
code/training_evaluation/auxiliary/losses.py,15,"b'""""""\nContrastive cost\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.util.deprecation import deprecated\n\nimport tensorflow as tf\n\n# def contrastive_loss(onehot_labels, logits, margin=1, scope=None):\n#     """"""With this definition the loss will be calculated.\n#         Args:\n#           y: The labels.\n#           distance: The distance vector between the output features..\n#           batch_size: the batch size is necessary because the loss calculation would be over each batch.\n#         Returns:\n#           The total loss.\n#     """"""\n#     with ops.name_scope(scope, ""contrastive_loss"", [onehot_labels, logits]) as scope:\n#         # logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\n#\n#         onehot_labels = math_ops.cast(onehot_labels, logits.dtype)\n#\n#         term_1 = tf.multiply(onehot_labels, tf.square(logits))[:,0:1]\n#         term_2 = tf.multiply(onehot_labels, tf.square(tf.maximum((margin - logits), 0)))[:,1:]\n#\n#         # Contrastive\n#         Contrastive_Loss = tf.add(term_1, term_2) / 2\n#         loss = tf.losses.compute_weighted_loss(Contrastive_Loss, scope=scope)\n#\n#         return tf.losses.compute_weighted_loss(Contrastive_Loss, scope=scope)\n\ndef contrastive_loss(labels, logits, margin_gen=0, margin_imp=1, scope=None):\n    """"""With this definition the loss will be calculated.\n        Args:\n          y: The labels.\n          distance: The distance vector between the output features..\n          batch_size: the batch size is necessary because the loss calculation would be over each batch.\n        Returns:\n          The total loss.\n    """"""\n    with ops.name_scope(scope, ""contrastive_loss"", [labels, logits]) as scope:\n        # logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\n\n        labels = math_ops.cast(labels, logits.dtype)\n\n        # term_1 = tf.multiply(labels, tf.square(logits))\n        term_1 = tf.multiply(labels, tf.square(tf.maximum((logits - margin_gen), 0)))\n        term_2 = tf.multiply(1 - labels, tf.square(tf.maximum((margin_imp - logits), 0)))\n\n        # Contrastive\n        Contrastive_Loss = tf.add(term_1, term_2) / 2\n        loss = tf.losses.compute_weighted_loss(Contrastive_Loss, scope=scope)\n\n        return loss\n\n\n# def contrastive_loss(onehot_labels, logits, batch_size, margin=1):\n#     """"""With this definition the loss will be calculated.\n#         Args:\n#           y: The labels.\n#           distance: The distance vector between the output features..\n#           batch_size: the batch size is necessary because the loss calculation would be over each batch.\n#         Returns:\n#           The total loss.\n#     """"""\n#     with ops.name_scope(scope, ""contrastive_loss"", [onehot_labels, logits]) as scope:\n#         logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\n#\n#         onehot_labels = math_ops.cast(onehot_labels, logits.dtype)\n#\n#         term_1 = tf.multiply(onehot_labels, tf.square(distance))[:,0:1]\n#         term_2 = tf.multiply(onehot_labels, tf.square(tf.maximum((margin - distance), 0)))[:,1:]\n#\n#         # Contrastive\n#         Contrastive_Loss = tf.add(term_1, term_2) / batch_size / 2\n#         tf.add_to_collection(\'losses\', Contrastive_Loss)\n#\n#         return tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n\n'"
code/training_evaluation/nets/__init__.py,0,b''
code/training_evaluation/nets/lipread_mouth.py,19,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\nLSTM_status = False\n\n\ndef lipread_mouth_arg_scope(is_training, weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  # Add normalizer_fn=slim.batch_norm if Batch Normalization is required!\n  with slim.arg_scope([slim.conv3d, slim.fully_connected],\n                      activation_fn=None,\n                      weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=\'FAN_AVG\'),\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      normalizer_fn=slim.batch_norm,\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv3d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\ndef PReLU(input,scope):\n  """"""\n  Similar to TFlearn implementation\n  :param input: input of the PReLU which is output of a layer.\n  :return: The output.\n  """"""\n  alphas = tf.get_variable(scope, input.get_shape()[-1],\n                       initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n\n  return tf.nn.relu(input) + alphas * (input - abs(input)) * 0.5\n\n\ndef mouth_cnn_lstm(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.8,\n          spatial_squeeze=True,\n          scope=\'mouth_cnn\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv3d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n\n  end_points = {}\n  with tf.variable_scope(scope, \'mouth_cnn\', [inputs]) as sc:\n    # end_points_collection = sc.name + \'_end_points\'\n    # # Collect outputs for conv3d, fully_connected and max_pool2d.\n    # with slim.arg_scope([slim.conv3d, slim.max_pool2d],\n    #                     outputs_collections=end_points_collection):\n\n    ##### Convolution Section #####\n    # Tensor(""batch:1"", shape=(?, 9, 60, 100, 1), dtype=float32, device=/device:CPU:0)\n    inputs = tf.to_float(inputs)\n    net = slim.repeat(inputs, 1, slim.conv3d, 16, [1, 3, 3], scope=\'conv1\')\n    net = PReLU(net, \'conv1_activation\')\n    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1],padding=\'VALID\', name=\'pool1\')\n    # net = slim.max_pool2d(net, [3, 3], scope=\'pool1\')\n    net = slim.repeat(net, 1, slim.conv3d, 32, [1, 3, 3], scope=\'conv2\')\n    net = PReLU(net, \'conv2_activation\')\n    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1], padding=\'VALID\', name=\'pool2\')\n\n    net = slim.conv3d(net, 64, [1, 3, 3], scope=\'conv31\')\n    net = PReLU(net, \'conv31_activation\')\n    net = slim.conv3d(net, 64, [1, 3, 3], scope=\'conv32\')\n    net = PReLU(net, \'conv32_activation\')\n\n    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1], padding=\'VALID\', name=\'pool3\')\n    net = slim.repeat(net, 1, slim.conv3d, 128, [1, 3, 3], scope=\'conv4\')\n    net = PReLU(net, \'conv4_activation\')\n    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1], padding=\'VALID\', name=\'pool4\')\n\n    ##### FC section #####\n    # Use conv3d instead of fully_connected layers.\n    net = slim.repeat(net, 1, slim.conv3d, 256, [1, 2, 5], padding=\'VALID\', scope=\'fc5\')\n    net = PReLU(net, \'fc5_activation\')\n    # net = PReLU(net)\n    # net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n    #                    scope=\'dropout5\')\n\n\n    if LSTM_status:\n\n      net = slim.conv3d(net, 64, [1, 1, 1], padding=\'VALID\', activation_fn=None, normalizer_fn=None, scope=\'fc6\')\n      net = PReLU(net, \'fc6_activation\')\n\n      # Tensor(""tower_0/speech_cnn/fc6/squeezed:0"", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\n      net = tf.squeeze(net, [2, 3], name=\'fc6/squeezed\')\n\n    else:\n      net = slim.conv3d(net, 64, [9, 1, 1],padding=\'VALID\', activation_fn=None, normalizer_fn=None, scope=\'fc5\')\n\n      # Tensor(""tower_0/speech_cnn/fc6/squeezed:0"", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\n      net = tf.squeeze(net, [1, 2, 3], name=\'fc6/squeezed\')\n\n    if LSTM_status:\n      ##### LSTM-1 #####\n      # use sequence_length=X_lengths argument in tf.nn.dynamic_rnn if necessary.\n      cell_1 = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\n      outputs, last_states = tf.nn.dynamic_rnn(\n        cell=cell_1,\n        dtype=tf.float32,\n        inputs=net,\n        scope=\'LSTM-mouth\')\n      net = last_states.h\n\n    return net, end_points\n\n'"
code/training_evaluation/nets/lipread_speech.py,17,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport sys\n\nslim = tf.contrib.slim\nLSTM_status = False\n\n\n\ndef lipread_speech_arg_scope(is_training, weight_decay=0.0005,):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  # Add normalizer_fn=slim.batch_norm if Batch Normalization is required!\n  with slim.arg_scope([slim.conv3d, slim.fully_connected],\n                      activation_fn=None,\n                      weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode=\'FAN_AVG\'),\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      normalizer_fn=slim.batch_norm,\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv3d], padding=\'VALID\') as arg_sc:\n      return arg_sc\n\ndef PReLU(input,scope):\n  """"""\n  Similar to TFlearn implementation\n  :param input: input of the PReLU which is output of a layer.\n  :return: The output.\n  """"""\n  alphas = tf.get_variable(scope, input.get_shape()[-1],\n                       initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n\n  return tf.nn.relu(input) + alphas * (input - abs(input)) * 0.5\n\nend_points = {}\n\ndef speech_cnn_lstm(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.8,\n          spatial_squeeze=True,\n          scope=\'speech_cnn\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv3d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'speech_cnn\', [inputs]) as sc:\n    ##### CNN part #####\n    # Tensor(""batch:0"", shape=(?, 15, 40, 1, 3), dtype=float32, device=/device:CPU:0)\n    inputs = tf.to_float(inputs)\n    net = slim.repeat(inputs, 1, slim.conv3d, 16, [1, 5, 1], scope=\'conv1\')\n    net = PReLU(net, \'conv1_activation\')\n    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 1, 1], ksize=[1, 1, 2, 1, 1], padding=\'VALID\', name=\'pool1\')\n\n    net = slim.conv3d(net, 32, [1, 4, 1], scope=\'conv21\')\n    net = PReLU(net, \'conv21_activation\')\n    net = slim.conv3d(net, 32, [1, 4, 1], scope=\'conv22\')\n    net = PReLU(net, \'conv22_activation\')\n    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 1, 1], ksize=[1, 1, 2, 1, 1], padding=\'VALID\', name=\'pool2\')\n\n    net = slim.conv3d(net, 64, [1, 3, 1], scope=\'conv31\')\n    net = PReLU(net, \'conv31_activation\')\n    net = slim.conv3d(net, 64, [1, 3, 1], scope=\'conv32\')\n    net = PReLU(net, \'conv32_activation\')\n\n    ##### FC part #####\n    # Use conv3d instead of fully_connected layers.\n    net = slim.conv3d(net, 128, [1, 2, 1], padding=\'VALID\', scope=\'fc4\')\n    net = PReLU(net, \'fc4_activation\')\n    # net = PReLU(net)\n    # net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n    #                    scope=\'dropout4\')\n\n    if LSTM_status:\n\n      net = slim.conv3d(net, 64, [1, 1, 1], padding=\'VALID\', activation_fn=None, normalizer_fn=None, scope=\'fc5\')\n      net = PReLU(net, \'fc5_activation\')\n\n      # Tensor(""tower_0/speech_cnn/fc6/squeezed:0"", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\n      net = tf.squeeze(net, [2, 3], name=\'fc5/squeezed\')\n\n    else:\n      net = slim.conv3d(net, 64, [15, 1, 1],padding=\'VALID\', activation_fn=None, normalizer_fn=None, scope=\'fc5\')\n\n      # Tensor(""tower_0/speech_cnn/fc6/squeezed:0"", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\n      net = tf.squeeze(net, [1, 2, 3], name=\'fc5/squeezed\')\n\n    if LSTM_status:\n      ##### LSTM-1 #####\n      # use sequence_length=X_lengths argument in tf.nn.dynamic_rnn if necessary.\n      cell_1 = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\n      outputs, last_states = tf.nn.dynamic_rnn(\n        cell=cell_1,\n        dtype=tf.float32,\n        inputs=net,\n        scope=\'LSTM-speech\')\n      net = last_states.h\n\n    return net, end_points\n\n'"
code/training_evaluation/nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\nfrom nets import lipread_mouth\nfrom nets import lipread_speech\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'lipread_mouth\':lipread_mouth.mouth_cnn_lstm,\n                \'lipread_speech\':lipread_speech.speech_cnn_lstm,\n\n               }\n\narg_scopes_map = {\'lipread_mouth\':lipread_mouth.lipread_mouth_arg_scope,\n                  \'lipread_speech\':lipread_speech.lipread_speech_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    arg_scope = arg_scopes_map[name](is_training, weight_decay=weight_decay)\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
code/training_evaluation/roc_curve/PlotHIST.py,0,"b""# Siamese Architecture for face recognition\n\nimport random\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport math\nimport pdb\nimport sys\nimport scipy.io as sio\nfrom sklearn import *\nimport matplotlib.pyplot as plt\n\ndef Plot_HIST_Fn(label,distance, phase, num_bins = 50):\n\n    dissimilarity = distance[:]\n    gen_dissimilarity_original = []\n    imp_dissimilarity_original = []\n    for i in range(len(label)):\n        if label[i] == 1:\n            gen_dissimilarity_original.append(dissimilarity[i])\n        else:\n            imp_dissimilarity_original.append(dissimilarity[i])\n\n    bins = np.linspace(np.amin(distance), np.amax(distance), num_bins)\n    fig = plt.figure()\n    plt.hist(gen_dissimilarity_original, bins, alpha=0.5, facecolor='blue', normed=False, label='gen_dist_original')\n    plt.hist(imp_dissimilarity_original, bins, alpha=0.5, facecolor='red', normed=False, label='imp_dist_original')\n    plt.legend(loc='upper right')\n    plt.title(phase + '_' + 'OriginalFeatures_Histogram.jpg')\n    plt.show()\n    fig.savefig(phase + '_' + 'OriginalFeatures_Histogram.jpg')\n"""
code/training_evaluation/roc_curve/PlotPR.py,0,"b'# Siamese Architecture for face recognition\n\nimport random\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport math\nimport pdb\nimport sys\nimport scipy.io as sio\nfrom sklearn import *\nimport matplotlib.pyplot as plt\n\ndef Plot_PR_Fn(label,distance,phase):\n\n    precision, recall, thresholds = metrics.precision_recall_curve(label, -distance, pos_label=1, sample_weight=None)\n    AP = metrics.average_precision_score(label, -distance, average=\'macro\', sample_weight=None)\n\n    # AP(average precision) calculation.\n    # This score corresponds to the area under the precision-recall curve.\n    print(""AP = "", float((""{0:.%ie}"" % 1).format(AP)))\n\n    # Plot the ROC\n    fig = plt.figure()\n    ax = fig.gca()\n    lines = plt.plot(recall, precision, label=\'ROC Curve\')\n    plt.setp(lines, linewidth=2, color=\'r\')\n    ax.set_xticks(np.arange(0, 1.1, 0.1))\n    ax.set_yticks(np.arange(0, 1.1, 0.1))\n    plt.title(phase + \'_\' + \'PR.jpg\')\n    plt.xlabel(\'Recall\')\n    plt.ylabel(\'Precision\')\n\n    # Cutting the floating number\n    AP = \'%.2f\' % AP\n\n    # Setting text to plot\n    # plt.text(0.5, 0.5, \'AP = \' + str(AP), fontdict=None)\n    plt.grid()\n    plt.show()\n    fig.savefig(phase + \'_\' + \'PR.jpg\')\n\n'"
code/training_evaluation/roc_curve/PlotROC.py,0,"b'# Siamese Architecture for face recognition\n\nimport random\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport math\nimport pdb\nimport sys\nimport scipy.io as sio\nfrom sklearn import *\nimport matplotlib.pyplot as plt\n\n\ndef Plot_ROC_Fn(label,distance,phase):\n\n    fpr, tpr, thresholds = metrics.roc_curve(label, -distance, pos_label=1)\n    AUC = metrics.roc_auc_score(label, -distance, average=\'macro\', sample_weight=None)\n    # AP = metrics.average_precision_score(label, -distance, average=\'macro\', sample_weight=None)\n\n    # Calculating EER\n    intersect_x = fpr[np.abs(fpr - (1 - tpr)).argmin(0)]\n    EER = intersect_x\n    print(""EER = "", float((""{0:.%ie}"" % 1).format(intersect_x)))\n\n    # AUC(area under the curve) calculation\n    print(""AUC = "", float((""{0:.%ie}"" % 1).format(AUC)))\n\n    # # AP(average precision) calculation.\n    # # This score corresponds to the area under the precision-recall curve.\n    # print(""AP = "", float((""{0:.%ie}"" % 1).format(AP)))\n\n    # Plot the ROC\n    fig = plt.figure()\n    ax = fig.gca()\n    lines = plt.plot(fpr, tpr, label=\'ROC Curve\')\n    plt.setp(lines, linewidth=2, color=\'r\')\n    ax.set_xticks(np.arange(0, 1.1, 0.1))\n    ax.set_yticks(np.arange(0, 1.1, 0.1))\n    plt.title(phase + \'_\' + \'ROC.jpg\')\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n\n    # # Cutting the floating number\n    # AUC = \'%.2f\' % AUC\n    # EER = \'%.2f\' % EER\n    # # AP = \'%.2f\' % AP\n    #\n    # # Setting text to plot\n    # # plt.text(0.5, 0.6, \'AP = \' + str(AP), fontdict=None)\n    # plt.text(0.5, 0.5, \'AUC = \' + str(AUC), fontdict=None)\n    # plt.text(0.5, 0.4, \'EER = \' + str(EER), fontdict=None)\n    plt.grid()\n    plt.show()\n    fig.savefig(phase + \'_\' + \'ROC.jpg\')\n\n\n\n'"
code/training_evaluation/roc_curve/__init__.py,0,b'\n'
code/training_evaluation/roc_curve/calculate_roc.py,0,"b""# Siamese Architecture for face recognition\n\nimport random\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport math\nimport pdb\nimport sys\nimport scipy.io as sio\nfrom sklearn import *\n\ndef calculate_eer_auc_ap(label,distance):\n\n    fpr, tpr, thresholds = metrics.roc_curve(label, -distance, pos_label=1)\n    AUC = metrics.roc_auc_score(label, -distance, average='macro', sample_weight=None)\n    AP = metrics.average_precision_score(label, -distance, average='macro', sample_weight=None)\n\n    # Calculating EER\n    intersect_x = fpr[np.abs(fpr - (1 - tpr)).argmin(0)]\n    EER = intersect_x\n\n    return EER,AUC,AP,fpr, tpr\n\n"""
