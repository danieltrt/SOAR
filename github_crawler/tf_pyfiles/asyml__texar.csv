file_path,api_count,code
setup.py,0,"b'import sys\nimport setuptools\n\n\nlong_description = \'\'\'\nTexar is an open-source toolkit based on TensorFlow,\naiming to support a broad set of machine learning especially text generation\ntasks, such as machine translation, dialog, summarization, content manipulation,\nlanguage modeling, and so on.\n\nTexar is designed for both researchers and practitioners for fast prototyping\nand experimentation. Checkout https://github.com/asyml/texar-pytorch for the\nPyTorch version which has the same functionalities and (mostly) the same\ninterfaces.\n\'\'\'\n\nif sys.version_info < (3, 6):\n    sys.exit(\'Python>=3.6 is required by Texar.\')\n\nsetuptools.setup(\n    name=""texar"",\n    version=""0.2.4"",\n    url=""https://github.com/asyml/texar"",\n\n    description=""Toolkit for Machine Learning and Text Generation"",\n    long_description=long_description,\n    license=\'Apache License Version 2.0\',\n\n    packages=setuptools.find_packages(),\n    platforms=\'any\',\n\n    install_requires=[\n        \'regex>=2018.01.10\',\n        \'numpy<1.17.0\',\n        \'pathlib>=1.0\',\n        \'pyyaml\',\n        \'requests\',\n        \'funcsigs>=1.0.2\',\n        \'sentencepiece>=0.1.8\',\n        \'packaging\'\n    ],\n    extras_require={\n        \'tensorflow-cpu\': [\n            \'tensorflow>=1.10.0,<2.0\',\n            \'tensorflow-probability>=0.3.0,<0.8.0\'\n        ],\n        \'tensorflow-gpu\': [\n            \'tensorflow-gpu>=1.10.0,<2.0\',\n            \'tensorflow-probability>=0.3.0,<0.8.0\'\n        ]\n    },\n    package_data={\n        ""texar"": [\n            ""../bin/utils/multi-bleu.perl"",\n        ]\n    },\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n)\n'"
bin/average_checkpoints.py,16,"b'""""""Checkpoint averaging script.""""""\n\n# This script is modified version of\n# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/bin/t2t_avg_all.py\n# which comes with the following license and copyright notice:\n\n# Copyright 2017 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport argparse\nimport six\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef main():\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(""--model_dir"", required=True,\n                      help=""The model directory containing the checkpoints."")\n  parser.add_argument(""--output_dir"", required=True,\n                      help=""The output directory where the averaged checkpoint will be saved."")\n  parser.add_argument(""--max_count"", type=int, default=8,\n                      help=""The maximal number of checkpoints to average."")\n  args = parser.parse_args()\n\n  if args.model_dir == args.output_dir:\n    raise ValueError(""Model and output directory must be different"")\n\n  checkpoints_path = tf.train.get_checkpoint_state(args.model_dir).all_model_checkpoint_paths\n  if len(checkpoints_path) > args.max_count:\n    checkpoints_path = checkpoints_path[-args.max_count:]\n  num_checkpoints = len(checkpoints_path)\n\n  tf.logging.info(""Averaging %d checkpoints..."" % num_checkpoints)\n  tf.logging.info(""Listing variables..."")\n\n  var_list = tf.train.list_variables(checkpoints_path[0])\n  avg_values = {}\n  for name, shape in var_list:\n    if not name.startswith(""global_step""):\n      avg_values[name] = np.zeros(shape)\n\n  for checkpoint_path in checkpoints_path:\n    tf.logging.info(""Loading checkpoint %s"" % checkpoint_path)\n    reader = tf.train.load_checkpoint(checkpoint_path)\n    for name in avg_values:\n      avg_values[name] += reader.get_tensor(name) / num_checkpoints\n\n  tf_vars = []\n  for name, value in six.iteritems(avg_values):\n    tf_vars.append(tf.get_variable(name, shape=value.shape))\n  placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n  assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n\n  latest_step = int(checkpoints_path[-1].split(""-"")[-1])\n  out_base_file = os.path.join(args.output_dir, ""model.ckpt"")\n  global_step = tf.get_variable(\n      ""global_step"",\n      initializer=tf.constant(latest_step, dtype=tf.int64),\n      trainable=False)\n  saver = tf.train.Saver(tf.global_variables())\n\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for p, assign_op, (name, value) in zip(placeholders, assign_ops, six.iteritems(avg_values)):\n      sess.run(assign_op, {p: value})\n    tf.logging.info(""Saving averaged checkpoint to %s-%d"" % (out_base_file, latest_step))\n    saver.save(sess, out_base_file, global_step=global_step)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
bin/train.py,27,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Main script for model training.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport tempfile\nimport yaml\n\nimport tensorflow as tf\n\nfrom texar.tf import utils\nfrom texar.tf.run import Executor\n\n\ntf.flags.DEFINE_string(""config_paths"", """",\n                       ""Paths to configuration files. This can be a path to a ""\n                       ""directory in which all files are loaded, or paths to ""\n                       ""multiple files separated by commas. Setting a key in ""\n                       ""these files is equivalent to setting the FLAG value ""\n                       ""with the same name. If a key is set in both config ""\n                       ""files and FLAG, the value in config files is used."")\n\ntf.flags.DEFINE_string(""model"", """",\n                       ""Name of the model class."")\ntf.flags.DEFINE_string(""model_hparams"", ""{}"",\n                       ""YAML configuration string for the model ""\n                       ""hyper-parameters."")\n\ntf.flags.DEFINE_string(""data_hparams_train"", ""{}"",\n                       ""YAML configuration string for the training data ""\n                       ""hyper-parameters."")\ntf.flags.DEFINE_string(""data_hparams_eval"", ""{}"",\n                       ""YAML configuration string for the evaluation data ""\n                       ""hyper-parameters."")\n\ntf.flags.DEFINE_integer(""max_train_steps"", None,\n                        ""Maximum number of training steps to run. ""\n                        ""If None, train forever or until the train data ""\n                        ""generates the OutOfRange exception. If OutOfRange ""\n\t\t\t""occurs in the middle, training stops before ""\n\t\t\t""max_train_steps steps."")\ntf.flags.DEFINE_integer(""eval_steps"", None,\n                        ""Maximum number of evaluation steps to run. ""\n                        ""If None, evaluate until the eval data raises an ""\n                        ""OutOfRange exception."")\n\n# RunConfig\ntf.flags.DEFINE_string(""model_dir"", None,\n                       ""The directory where model parameters, graph, ""\n                       ""summeries, etc are saved. If None, a local temporary ""\n                       ""directory is created."")\ntf.flags.DEFINE_integer(""tf_random_seed"", None,\n                        ""Random seed for TensorFlow initializers. Setting ""\n                        ""this value allows consistency between reruns."")\ntf.flags.DEFINE_integer(""save_summary_steps"", 100,\n                        ""Save summaries every this many steps."")\ntf.flags.DEFINE_integer(""save_checkpoints_steps"", None,\n                        ""Save checkpoints every this many steps. ""\n                        ""Can not be specified with save_checkpoints_secs."")\ntf.flags.DEFINE_integer(""save_checkpoints_secs"", None,\n                        ""Save checkpoints every this many seconds. ""\n                        ""Can not be specified with save_checkpoints_steps. ""\n                        ""Defaults to 600 seconds if both ""\n                        ""save_checkpoints_steps and save_checkpoints_secs ""\n                        ""are not set. If both are set to -1, then ""\n                        ""checkpoints are disabled."")\ntf.flags.DEFINE_integer(""keep_checkpoint_max"", 5,\n                        ""Maximum number of recent checkpoint files to keep. ""\n                        ""As new files are created, older files are deleted. ""\n                        ""If None or 0, all checkpoint files are kept."")\ntf.flags.DEFINE_integer(""keep_checkpoint_every_n_hours"", 10000,\n                        ""Number of hours between each checkpoint to be saved. ""\n                        ""The default value of 10,000 hours effectively ""\n                        ""disables the feature."")\ntf.flags.DEFINE_integer(""log_step_count_steps"", 100,\n                        ""The frequency, in number of global steps, that the ""\n                        ""global step/sec and the loss will be logged during ""\n                        ""training."")\n# Session config\ntf.flags.DEFINE_float(""per_process_gpu_memory_fraction"", 1.0,\n                      ""Fraction of the available GPU memory to allocate for ""\n                      ""each process."")\ntf.flags.DEFINE_boolean(""gpu_allow_growth"", False,\n                        ""If true, the allocator does not pre-allocate the ""\n                        ""entire specified GPU memory region, instead starting ""\n                        ""small and growing as needed."")\ntf.flags.DEFINE_boolean(""log_device_placement"", False,\n                        ""Whether device placements should be logged."")\n\nFLAGS = tf.flags.FLAGS\n\ndef _process_config():\n    # Loads configs\n    config = utils.load_config(FLAGS.config_paths)\n\n    # Parses YAML FLAGS\n    FLAGS.model_hparams = yaml.load(FLAGS.model_hparams)\n    FLAGS.data_hparams_train = yaml.load(FLAGS.data_hparams_train)\n    FLAGS.data_hparams_eval = yaml.load(FLAGS.data_hparams_eval)\n\n    # Merges\n    final_config = {}\n    for flag_key in dir(FLAGS):\n        if flag_key in {\'h\', \'help\', \'helpshort\'}: # Filters out help flags\n            continue\n        flag_value = getattr(FLAGS, flag_key)\n        config_value = config.get(flag_key, None)\n        if isinstance(flag_value, dict) and isinstance(config_value, dict):\n            final_config[flag_key] = utils.dict_patch(config_value, flag_value)\n        elif flag_key in config:\n            final_config[flag_key] = config_value\n        else:\n            final_config[flag_key] = flag_value\n\n    # Processes\n    if final_config[\'model_dir\'] is None:\n        final_config[\'model_dir\'] = tempfile.mkdtemp()\n\n    if final_config[\'save_checkpoints_steps\'] is None \\\n            and final_config[\'save_checkpoints_secs\'] is None:\n        final_config[\'save_checkpoints_secs\'] = 600\n    if final_config[\'save_checkpoints_steps\'] == -1 \\\n            and final_config[\'save_checkpoints_secs\'] == -1:\n        final_config[\'save_checkpoints_steps\'] = None\n        final_config[\'save_checkpoints_secs\'] = None\n\n    tf.logging.info(""Final Config:\\n%s"", yaml.dump(final_config))\n\n    return final_config\n\ndef _get_run_config(config):\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=\\\n                config[\'per_process_gpu_memory_fraction\'],\n        allow_growth=config[\'gpu_allow_growth\'])\n    sess_config = tf.ConfigProto(\n        gpu_options=gpu_options,\n        log_device_placement=config[\'log_device_placement\'])\n\n    run_config = tf.estimator.RunConfig(\n        model_dir=config[\'model_dir\'],\n        tf_random_seed=config[\'tf_random_seed\'],\n        save_summary_steps=config[\'save_summary_steps\'],\n        save_checkpoints_steps=config[\'save_checkpoints_steps\'],\n        save_checkpoints_secs=config[\'save_checkpoints_secs\'],\n        keep_checkpoint_max=config[\'keep_checkpoint_max\'],\n        keep_checkpoint_every_n_hours=config[\'keep_checkpoint_every_n_hours\'],\n        log_step_count_steps=config[\'log_step_count_steps\'],\n        session_config=sess_config)\n\n    return run_config\n\ndef main(_):\n    """"""The entrypoint.""""""\n\n    config = _process_config()\n\n    run_config = _get_run_config(config)\n\n    kwargs = {\n        \'data_hparams\': config[\'data_hparams_train\'],\n        \'hparams\': config[\'model_hparams\']\n    }\n    model = utils.check_or_get_instance_with_redundant_kwargs(\n        config[\'model\'], kwargs=kwargs,\n        module_paths=[\'texar.tf.models\', \'texar.tf.custom\'])\n\n    data_hparams = {\n        \'train\': config[\'data_hparams_train\'],\n        \'eval\': config[\'data_hparams_eval\']\n    }\n\n    exor = Executor(\n        model=model,\n        data_hparams=data_hparams,\n        config=run_config)\n\n    exor.train_and_evaluate(\n        max_train_steps=config[\'max_train_steps\'],\n        eval_steps=config[\'eval_steps\'])\n\nif __name__ == ""__main__"":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main=main)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# texar documentation build configuration file, created by\n# sphinx-quickstart on Mon Sep  4 21:15:05 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n#from unittest.mock import MagicMock\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'..\'))\nfrom texar.tf import __version__\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.extlinks\',\n    \'sphinx.ext.napoleon\',\n    \'recommonmark\',\n    \'sphinxcontrib.spelling\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.md\': \'markdown\',\n}\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Texar\'\ncopyright = u\'2019, Texar\'\nauthor = u\'Texar\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\n#version = u\'{}\'.format(__version_short__)\nversion = u\'{}\'.format(__version__)\n# The full version, including alpha/beta/rc tags.\nrelease = u\'{}\'.format(__version__)\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = \'any\'\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# html_theme = \'alabaster\'\n\nimport sphinx_rtd_theme\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\nhtml_title = u\'Texar\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'_static/img/logo_h.png\'\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_context = {\n    \'css_files\': [\n        \'https://fonts.googleapis.com/css?family=Lato\',\n        \'_static/css/custom_theme.css\'\n    ],\n}\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'texardoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'texar.tex\', u\'Texar Documentation\',\n     u\'Texar\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'texar\', u\'Texar Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'texar\', u\'Texar Documentation\',\n     author, \'Texar\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    # \'numpy\': (\'http://docs.scipy.org/docs/numpy/\', None),\n}\n\nextlinks = {\'tf_main\': (\n                \'https://www.tensorflow.org/api_docs/python/tf/%s\',\n                None),\n            \'tf_r0.12\': (\n                \'https://www.tensorflow.org/versions/r0.12/api_docs/python/%s\',\n                None),\n            \'tf_hmpg\': (\n                \'https://www.tensorflow.org/%s\',\n                None),\n            \'gym\': (\n                \'https://gym.openai.com/docs/%s\',\n                None),\n            }\n\n##### Customize ######\n\n# Snippet to insert at beginning of each RST file.\nrst_prolog = r""""""\n.. role:: python(code)\n    :language: python\n""""""\n\nautodoc_member_order = \'bysource\'\nautodoc_typehints = \'none\'\n\nnapoleon_numpy_docstring = False\n\nspelling_lang = \'en_US\'\nspelling_word_list_filename = \'spelling_wordlist.txt\'\n\n## Exclude imports\n#autodoc_mock_imports = [\n#    ""torch""\n#]\n\n# Addresses import errors. Refer to:\n# https://docs.readthedocs.io/en/latest/faq.html#i-get-import-errors-on-libraries-that-depend-on-c-modules\n#class Mock(MagicMock):\n#    @classmethod\n#    def __getattr__(cls, name):\n#        return MagicMock()\n#MOCK_MODULES = [\'gym\']\n#sys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n'"
tests/context_test.py,13,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for various context functionalities.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf import context\n\n# pylint: disable=protected-access\n\n\nclass ContextTest(tf.test.TestCase):\n    """"""Tests context.\n    """"""\n\n    def test_global_mode(self):\n        """"""Tests the mode context manager.\n        """"""\n        global_mode = context.global_mode()\n        self.assertIsInstance(global_mode, tf.Tensor)\n\n        mode_train = context.global_mode_train()\n        mode_eval = context.global_mode_eval()\n        mode_predict = context.global_mode_predict()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            global_mode_ = sess.run(global_mode)\n            self.assertEqual(tf.compat.as_str(global_mode_),\n                             tf.estimator.ModeKeys.TRAIN)\n\n            global_mode_, mode_train_, mode_eval_, mode_predict_ = sess.run(\n                [global_mode, mode_train, mode_eval, mode_predict],\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN})\n            self.assertEqual(global_mode_, tf.estimator.ModeKeys.TRAIN)\n            self.assertTrue(mode_train_)\n            self.assertFalse(mode_eval_)\n            self.assertFalse(mode_predict_)\n\n            global_mode_, mode_train_, mode_eval_, mode_predict_ = sess.run(\n                [global_mode, mode_train, mode_eval, mode_predict],\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.EVAL})\n            self.assertEqual(global_mode_, tf.estimator.ModeKeys.EVAL)\n            self.assertFalse(mode_train_)\n            self.assertTrue(mode_eval_)\n            self.assertFalse(mode_predict_)\n\n            global_mode_, mode_train_, mode_eval_, mode_predict_ = sess.run(\n                [global_mode, mode_train, mode_eval, mode_predict],\n                feed_dict={context.global_mode():\n                           tf.estimator.ModeKeys.PREDICT})\n            self.assertEqual(global_mode_, tf.estimator.ModeKeys.PREDICT)\n            self.assertFalse(mode_train_)\n            self.assertFalse(mode_eval_)\n            self.assertTrue(mode_predict_)\n\n        global_mode_values = tf.get_collection_ref(context._GLOBAL_MODE_KEY)\n        self.assertEqual(len(global_mode_values), 1)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/hyperparams_test.py,3,"b'""""""\nUnit tests of :class:`HParams`.\n""""""\n\nimport copy\nimport pickle\n\nimport tempfile\nimport tensorflow as tf\n\nfrom texar.tf.hyperparams import HParams\n\n# pylint: disable=no-member\n\n\nclass HParamsTest(tf.test.TestCase):\n    """"""Tests hyperparameter related operations.\n    """"""\n\n    def test_hparams(self):\n        """"""Tests the HParams class.\n        """"""\n        default_hparams = {\n            ""str"": ""str"",\n            ""list"": [\'item1\', \'item2\'],\n            ""dict"": {\n                ""key1"": ""value1"",\n                ""key2"": ""value2""\n            },\n            ""nested_dict"": {\n                ""dict_l2"": {\n                    ""key1_l2"": ""value1_l2""\n                }\n            },\n            ""type"": ""type"",\n            ""kwargs"": {\n                ""arg1"": ""argv1""\n            },\n        }\n\n        # Test HParams.items() function\n        hparams_ = HParams(None, default_hparams)\n        names = []\n        for name, _ in hparams_.items():\n            names.append(name)\n        self.assertEqual(set(names), set(default_hparams.keys()))\n\n        hparams = {\n            ""dict"": {""key1"": ""new_value""},\n            ""kwargs"": {""arg2"": ""argv2""}\n        }\n\n        hparams_ = HParams(hparams, default_hparams)\n\n        # Test HParams construction\n        self.assertEqual(hparams_.str, default_hparams[""str""])\n        self.assertEqual(hparams_.list, default_hparams[""list""])\n        self.assertEqual(hparams_.dict.key1, hparams[""dict""][""key1""])\n        self.assertEqual(hparams_.kwargs.arg2, hparams[""kwargs""][""arg2""])\n        self.assertEqual(hparams_.nested_dict.dict_l2.key1_l2,\n                         default_hparams[""nested_dict""][""dict_l2""][""key1_l2""])\n\n        self.assertEqual(len(hparams_), len(default_hparams))\n\n        new_hparams = copy.deepcopy(default_hparams)\n        new_hparams[""dict""][""key1""] = hparams[""dict""][""key1""]\n        new_hparams[""kwargs""].update(hparams[""kwargs""])\n        self.assertEqual(hparams_.todict(), new_hparams)\n\n        self.assertTrue(""dict"" in hparams_)\n\n        self.assertIsNone(hparams_.get(\'not_existed_name\', None))\n        self.assertEqual(hparams_.get(\'str\'), default_hparams[\'str\'])\n\n        # Test HParams update related operations\n        hparams_.str = ""new_str""\n        hparams_.dict = {""key3"": ""value3""}\n        self.assertEqual(hparams_.str, ""new_str"")\n        self.assertEqual(hparams_.dict.key3, ""value3"")\n\n        hparams_.add_hparam(""added_str"", ""added_str"")\n        hparams_.add_hparam(""added_dict"", {""key4"": ""value4""})\n        hparams_.kwargs.add_hparam(""added_arg"", ""added_argv"")\n        self.assertEqual(hparams_.added_str, ""added_str"")\n        self.assertEqual(hparams_.added_dict.todict(), {""key4"": ""value4""})\n        self.assertEqual(hparams_.kwargs.added_arg, ""added_argv"")\n\n        # Test HParams I/O\n        hparams_file = tempfile.NamedTemporaryFile()\n        pickle.dump(hparams_, hparams_file)\n        with open(hparams_file.name, \'rb\') as hparams_file:\n            hparams_loaded = pickle.load(hparams_file)\n        self.assertEqual(hparams_loaded.todict(), hparams_.todict())\n\n    def test_typecheck(self):\n        """"""Tests type-check functionality.\n        """"""\n        def _foo():\n            pass\n\n        def _bar():\n            pass\n\n        default_hparams = {\n            ""fn"": _foo,\n            ""fn_2"": _foo\n        }\n        hparams = {\n            ""fn"": _foo,\n            ""fn_2"": _bar\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.fn, default_hparams[""fn""])\n\n    def test_type_kwargs(self):\n        """"""The the special cases involving ""type"" and ""kwargs""\n        hyperparameters.\n        """"""\n        default_hparams = {\n            ""type"": ""type_name"",\n            ""kwargs"": {\n                ""arg1"": ""argv1""\n            }\n        }\n\n        hparams = {\n            ""type"": ""type_name""\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), default_hparams[""kwargs""])\n\n        hparams = {\n            ""type"": ""type_name"",\n            ""kwargs"": {\n                ""arg2"": ""argv2""\n            }\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        full_kwargs = {}\n        full_kwargs.update(default_hparams[""kwargs""])\n        full_kwargs.update(hparams[""kwargs""])\n        self.assertEqual(hparams_.kwargs.todict(), full_kwargs)\n\n        hparams = {\n            ""kwargs"": {\n                ""arg2"": ""argv2""\n            }\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), full_kwargs)\n\n        hparams = {\n            ""type"": ""type_name2""\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), {})\n\n        hparams = {\n            ""type"": ""type_name2"",\n            ""kwargs"": {\n                ""arg3"": ""argv3""\n            }\n        }\n        hparams_ = HParams(hparams, default_hparams)\n        self.assertEqual(hparams_.kwargs.todict(), hparams[""kwargs""])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
texar/__init__.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library.\n""""""\n\n# pylint: disable=wildcard-import\n\nimport sys\n\nif sys.version_info.major < 3:\n    # PY 2.x, import as is because Texar-PyTorch cannot be installed.\n    import texar.tf\n\nelse:\n    # Lazily load Texar-TF modules upon usage. This is to ensure that Texar-TF\n    # and TensorFlow will not be imported if the user only requires\n    # Texar-PyTorch modules from `texar.torch`.\n\n    import importlib\n\n    __import_modules__ = [\n        ""modules"", ""core"", ""losses"", ""models"", ""data"", ""evals"",\n        ""agents"", ""run"", ""utils"",\n    ]\n    __import_star_modules__ = [""module_base"", ""hyperparams"", ""context""]\n\n    def _import_all():\n        import warnings\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(""always"", DeprecationWarning)\n            warnings.warn(\n                ""Importing from `texar` is deprecated. Please import from ""\n                ""`texar.tf` instead, e.g. `import texar.tf as tx`"",\n                DeprecationWarning, stacklevel=3)\n\n        from texar.tf.version import VERSION\n        globals()[""__version__""] = VERSION\n\n        for module_name in __import_star_modules__:\n            # from ... import *. Requires manually handling `__all__`.\n            module = importlib.import_module(""."" + module_name, package=""texar.tf"")\n            try:\n                variables = module.__all__\n            except AttributeError:\n                variables = [name for name in module.__dict__\n                             if not name.startswith(""_"")]\n            globals().update({\n                name: module.__dict__[name] for name in variables})\n            globals()[module_name] = module\n\n        for module_name in __import_modules__:\n            # from ... import module\n            module = importlib.import_module(""."" + module_name, package=""texar.tf"")\n            globals()[module_name] = module\n\n    class _DummyTexarBaseModule:\n        # Credit: https://stackoverflow.com/a/7668273/4909228\n        def __getattr__(self, name):\n            if name in globals():\n                # Shortcut to global names.\n                return globals()[name]\n            if name in [""torch"", ""tf""]:\n                # To use `texar.torch`, Texar-TF and TensorFlow should not be\n                # imported; To use `texar.tf`, Texar-PyTorch and PyTorch should\n                # not be imported.\n                module = importlib.import_module(""."" + name, package=""texar"")\n                globals()[name] = module\n                return module\n\n            # The user tries to access Texar-TF modules, so we load all modules\n            # at this point, and restore the registered `texar` module.\n            sys.modules[__name__] = __module__\n            _import_all()\n            return globals()[name]\n\n    # Save `texar` module as `__module__`, ans replace the system-wide\n    # registered module with our dummy module.\n    __module__ = sys.modules[__name__]\n    sys.modules[__name__] = _DummyTexarBaseModule()\n'"
bin/utils/make_vocab.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Creates vocabulary from a set of data files.\n\nExample usage:\n\n$ python make_vocab.py --files \'./data/train*\'\n\nNote that if the file path is a pattern, wrap it with quotation masks.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# pylint: disable=invalid-name\n\nimport sys\nfrom io import open\n\nimport tensorflow as tf\n\nimport texar as tx\n\nPy3 = sys.version_info[0] == 3\n\nflags = tf.flags\n\nflags.DEFINE_string(""files"", ""./train.txt"",\n                    ""Path to the data files. Can be a pattern, e.g., ""\n                    ""\'/path/to/train*\', \'/path/to/train[12]\'. ""\n                    ""Caution: If the path is a pattern, you must wrap the path ""\n                    ""with quotation marks. e.g., ""\n                    ""python make_vocab.py --files \'./data/*\'"")\nflags.DEFINE_integer(""max_vocab_size"", -1,\n                     ""Maximum size of the vocabulary. Low frequency words ""\n                     ""that exceeding the limit will be discarded. ""\n                     ""Set to `-1` if no truncation is wanted."")\nflags.DEFINE_boolean(""count"", False, ""Whether to print word count in the ""\n                     ""output file. Note that Texar data modules require a ""\n                     ""vocab file without word count. But the functionality ""\n                     ""can be useful to decide vocab truncation."")\nflags.DEFINE_string(""output_path"", ""./vocab.txt"",\n                    ""Path of the output vocab file."")\nflags.DEFINE_string(""newline_token"", None,\n                    ""The token to replace the original newline token \'\\n\'. ""\n                    ""For example, `--newline_token \'<EOS>\'`. If not ""\n                    ""specified, no replacement is performed."")\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n    """"""Makes vocab.\n    """"""\n    filenames = tx.utils.get_files(FLAGS.files)\n\n    if FLAGS.count:\n        vocab, count = tx.data.make_vocab(\n            filenames,\n            max_vocab_size=FLAGS.max_vocab_size,\n            newline_token=FLAGS.newline_token,\n            return_count=True)\n\n        with open(FLAGS.output_path, ""w"", encoding=""utf-8"") as fout:\n            for v, c in zip(vocab, count):\n                fout.write(\'{}\\t{}\\n\'.format(v, c))\n    else:\n        vocab = tx.data.make_vocab(\n            filenames,\n            max_vocab_size=FLAGS.max_vocab_size,\n            newline_token=FLAGS.newline_token)\n\n        with open(FLAGS.output_path, ""w"", encoding=""utf-8"") as fout:\n            fout.write(\'\\n\'.join(vocab))\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
examples/bert/bert_classifier_main.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building a sentence classifier based on pre-trained BERT model.\n""""""\n\nimport os\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom utils import model_utils\n\n# pylint: disable=invalid-name, too-many-locals, too-many-statements\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    ""config_downstream"", ""config_classifier"",\n    ""Configuration of the downstream part of the model."")\nflags.DEFINE_string(\n    ""pretrained_model_name"", \'bert-base-uncased\',\n    ""The name of pre-trained BERT model. See the doc of ""\n    ""`texar.tf.modules.PretrainedBERTMixin for all supported models.`"")\nflags.DEFINE_string(\n    ""config_data"", ""config_data"",\n    ""The dataset config."")\nflags.DEFINE_string(\n    ""output_dir"", ""output/"",\n    ""The output directory where the model checkpoints will be written."")\nflags.DEFINE_string(\n    ""checkpoint"", None,\n    ""Path to a model checkpoint (including bert modules) to restore from."")\nflags.DEFINE_bool(""do_train"", False, ""Whether to run training."")\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\nflags.DEFINE_bool(""do_test"", False, ""Whether to run test on the test set."")\nflags.DEFINE_bool(""distributed"", False, ""Whether to run in distributed mode."")\n\nconfig_data = importlib.import_module(FLAGS.config_data)\nconfig_downstream = importlib.import_module(FLAGS.config_downstream)\n\n\ndef main(_):\n    """"""\n    Builds the model and runs.\n    """"""\n    if FLAGS.distributed:\n        import horovod.tensorflow as hvd\n        hvd.init()\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    tx.utils.maybe_create_dir(FLAGS.output_dir)\n\n    # Loads data\n    num_train_data = config_data.num_train_data\n\n    # Configures distribued mode\n    if FLAGS.distributed:\n        config_data.train_hparam[""dataset""][""num_shards""] = hvd.size()\n        config_data.train_hparam[""dataset""][""shard_id""] = hvd.rank()\n        config_data.train_hparam[""batch_size""] //= hvd.size()\n\n    train_dataset = tx.data.TFRecordData(hparams=config_data.train_hparam)\n    eval_dataset = tx.data.TFRecordData(hparams=config_data.eval_hparam)\n    test_dataset = tx.data.TFRecordData(hparams=config_data.test_hparam)\n\n    iterator = tx.data.FeedableDataIterator({\n        \'train\': train_dataset, \'eval\': eval_dataset, \'test\': test_dataset})\n    batch = iterator.get_next()\n    input_ids = batch[""input_ids""]\n    segment_ids = batch[""segment_ids""]\n    batch_size = tf.shape(input_ids)[0]\n    input_length = tf.reduce_sum(1 - tf.cast(tf.equal(input_ids, 0), tf.int32),\n                                 axis=1)\n    # Builds BERT\n    hparams = {\n        \'clas_strategy\': \'cls_time\'\n    }\n    model = tx.modules.BERTClassifier(\n        pretrained_model_name=FLAGS.pretrained_model_name,\n        hparams=hparams)\n    logits, preds = model(input_ids, input_length, segment_ids)\n\n    accu = tx.evals.accuracy(batch[\'label_ids\'], preds)\n\n    # Optimization\n    loss = tf.losses.sparse_softmax_cross_entropy(\n        labels=batch[""label_ids""], logits=logits)\n    global_step = tf.Variable(0, trainable=False)\n\n    # Builds learning rate decay scheduler\n    static_lr = config_downstream.lr[\'static_lr\']\n    num_train_steps = int(num_train_data / config_data.train_batch_size\n                          * config_data.max_train_epoch)\n    num_warmup_steps = int(num_train_steps * config_data.warmup_proportion)\n    lr = model_utils.get_lr(global_step, num_train_steps,  # lr is a Tensor\n                            num_warmup_steps, static_lr)\n\n    opt = tx.core.get_optimizer(\n        global_step=global_step,\n        learning_rate=lr,\n        hparams=config_downstream.opt\n    )\n\n    if FLAGS.distributed:\n        opt = hvd.DistributedOptimizer(opt)\n\n    train_op = tf.contrib.layers.optimize_loss(\n        loss=loss,\n        global_step=global_step,\n        learning_rate=None,\n        optimizer=opt)\n\n    # Train/eval/test routine\n\n    def _is_head():\n        if not FLAGS.distributed:\n            return True\n        return hvd.rank() == 0\n\n    def _train_epoch(sess):\n        """"""Trains on the training set, and evaluates on the dev set\n        periodically.\n        """"""\n        iterator.restart_dataset(sess, \'train\')\n\n        fetches = {\n            \'train_op\': train_op,\n            \'loss\': loss,\n            \'batch_size\': batch_size,\n            \'step\': global_step\n        }\n\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'train\'),\n                    tx.global_mode(): tf.estimator.ModeKeys.TRAIN,\n                }\n                rets = sess.run(fetches, feed_dict)\n                step = rets[\'step\']\n\n                dis_steps = config_data.display_steps\n                if _is_head() and dis_steps > 0 and step % dis_steps == 0:\n                    tf.logging.info(\'step:%d; loss:%f;\' % (step, rets[\'loss\']))\n\n                eval_steps = config_data.eval_steps\n                if _is_head() and eval_steps > 0 and step % eval_steps == 0:\n                    _eval_epoch(sess)\n\n            except tf.errors.OutOfRangeError:\n                break\n\n    def _eval_epoch(sess):\n        """"""Evaluates on the dev set.\n        """"""\n        iterator.restart_dataset(sess, \'eval\')\n\n        cum_acc = 0.0\n        cum_loss = 0.0\n        nsamples = 0\n        fetches = {\n            \'accu\': accu,\n            \'loss\': loss,\n            \'batch_size\': batch_size,\n        }\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'eval\'),\n                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,\n                }\n                rets = sess.run(fetches, feed_dict)\n\n                cum_acc += rets[\'accu\'] * rets[\'batch_size\']\n                cum_loss += rets[\'loss\'] * rets[\'batch_size\']\n                nsamples += rets[\'batch_size\']\n            except tf.errors.OutOfRangeError:\n                break\n\n        tf.logging.info(\'eval accu: {}; loss: {}; nsamples: {}\'.format(\n            cum_acc / nsamples, cum_loss / nsamples, nsamples))\n\n    def _test_epoch(sess):\n        """"""Does predictions on the test set.\n        """"""\n        iterator.restart_dataset(sess, \'test\')\n\n        _all_preds = []\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'test\'),\n                    tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT,\n                }\n                _preds = sess.run(preds, feed_dict=feed_dict)\n                _all_preds.extend(_preds.tolist())\n            except tf.errors.OutOfRangeError:\n                break\n\n        output_file = os.path.join(FLAGS.output_dir, ""test_results.tsv"")\n        with tf.gfile.GFile(output_file, ""w"") as writer:\n            writer.write(\'\\n\'.join(str(p) for p in _all_preds))\n\n    # Broadcasts global variables from rank-0 process\n    if FLAGS.distributed:\n        bcast = hvd.broadcast_global_variables(0)\n\n    session_config = tf.ConfigProto()\n    if FLAGS.distributed:\n        session_config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    with tf.Session(config=session_config) as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        if FLAGS.distributed:\n            bcast.run()\n\n        # Restores trained model if specified\n        saver = tf.train.Saver()\n        if FLAGS.checkpoint:\n            saver.restore(sess, FLAGS.checkpoint)\n\n        iterator.initialize_dataset(sess)\n\n        if FLAGS.do_train:\n            for i in range(config_data.max_train_epoch):\n                _train_epoch(sess)\n            saver.save(sess, FLAGS.output_dir + \'/model.ckpt\')\n\n        if FLAGS.do_eval:\n            _eval_epoch(sess)\n\n        if FLAGS.do_test:\n            _test_epoch(sess)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
examples/bert/config_classifier.py,0,"b""hidden_dim = 768\n\nopt = {\n    'optimizer': {\n        'type': 'AdamWeightDecayOptimizer',\n        'kwargs': {\n            'weight_decay_rate': 0.01,\n            'beta_1': 0.9,\n            'beta_2': 0.999,\n            'epsilon': 1e-6,\n            'exclude_from_weight_decay': ['LayerNorm', 'layer_norm', 'bias']\n        }\n    },\n    'gradient_clip': {\n        'type': 'clip_by_global_norm',\n        'kwargs': {\n            'clip_norm': 1.0,\n        }\n    }\n}\n\n# By default, we use warmup and linear decay for learinng rate\nlr = {\n    'static_lr': 2e-5,\n}\n"""
examples/bert/config_data.py,11,"b'tfrecord_data_dir = ""data/MRPC""\nmax_seq_length = 128\nnum_classes = 2\nnum_train_data = 3668\n\ntrain_batch_size = 32\nmax_train_epoch = 3\ndisplay_steps = 50  # Print training loss every display_steps; -1 to disable\neval_steps = -1    # Eval on the dev set every eval_steps; -1 to disable\n# Proportion of training to perform linear learning\n# rate warmup for. E.g., 0.1 = 10% of training.\nwarmup_proportion = 0.1\n\neval_batch_size = 8\ntest_batch_size = 8\n\n\nfeature_original_types = {\n    # Reading features from TFRecord data file.\n    # E.g., Reading feature ""input_ids"" as dtype `tf.int64`;\n    # ""FixedLenFeature"" indicates its length is fixed for all data instances;\n    # and the sequence length is limited by `max_seq_length`.\n    ""input_ids"": [""tf.int64"", ""FixedLenFeature"", max_seq_length],\n    ""input_mask"": [""tf.int64"", ""FixedLenFeature"", max_seq_length],\n    ""segment_ids"": [""tf.int64"", ""FixedLenFeature"", max_seq_length],\n    ""label_ids"": [""tf.int64"", ""FixedLenFeature""]\n}\n\nfeature_convert_types = {\n    # Converting feature dtype after reading. E.g.,\n    # Converting the dtype of feature ""input_ids"" from `tf.int64` (as above)\n    # to `tf.int32`\n    ""input_ids"": ""tf.int32"",\n    ""input_mask"": ""tf.int32"",\n    ""label_ids"": ""tf.int32"",\n    ""segment_ids"": ""tf.int32""\n}\n\ntrain_hparam = {\n    ""allow_smaller_final_batch"": False,\n    ""batch_size"": train_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_convert_types"": feature_convert_types,\n        ""feature_original_types"": feature_original_types,\n        ""files"": ""{}/train.tf_record"".format(tfrecord_data_dir)\n    },\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 100\n}\n\neval_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": eval_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_convert_types"": feature_convert_types,\n        ""feature_original_types"": feature_original_types,\n        ""files"": ""{}/eval.tf_record"".format(tfrecord_data_dir)\n    },\n    ""shuffle"": False\n}\n\ntest_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": test_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_convert_types"": feature_convert_types,\n        ""feature_original_types"": feature_original_types,\n        ""files"": ""{}/predict.tf_record"".format(tfrecord_data_dir)\n    },\n\n    ""shuffle"": False\n}\n'"
examples/bert/prepare_data.py,8,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Produces TFRecord files and modifies data configuration file\n""""""\n\nimport os\nimport tensorflow as tf\nimport texar.tf as tx\n\n# pylint: disable=no-name-in-module\nfrom utils import data_utils\n\n# pylint: disable=invalid-name, too-many-locals, too-many-statements\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\n    ""task"", ""MRPC"",\n    ""The task to run experiment on. One of ""\n    ""{\'COLA\', \'MNLI\', \'MRPC\', \'XNLI\', \'SST\'}."")\nflags.DEFINE_string(\n    ""pretrained_model_name"", \'bert-base-uncased\',\n    ""The name of pre-trained BERT model. See the doc of ""\n    ""`texar.tf.modules.PretrainedBERTMixin for all supported models.`"")\nflags.DEFINE_integer(\n    ""max_seq_length"", 128,\n    ""The maximum length of sequence, longer sequence will be trimmed."")\nflags.DEFINE_string(\n    ""tfrecord_output_dir"", None,\n    ""The output directory where the TFRecord files will be generated. ""\n    ""By default it will be set to \'data/{task}\'. E.g.: if ""\n    ""task is \'MRPC\', it will be set as \'data/MRPC\'"")\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef _modify_config_data(max_seq_length, num_train_data, num_classes):\n    # Modify the data configuration file\n    config_data_exists = os.path.isfile(\'./config_data.py\')\n    if config_data_exists:\n        with open(""./config_data.py"", \'r\') as file:\n            filedata = file.read()\n            filedata_lines = filedata.split(\'\\n\')\n            idx = 0\n            while True:\n                if idx >= len(filedata_lines):\n                    break\n                line = filedata_lines[idx]\n                if (line.startswith(\'num_classes =\') or\n                        line.startswith(\'num_train_data =\') or\n                        line.startswith(\'max_seq_length =\')):\n                    filedata_lines.pop(idx)\n                    idx -= 1\n                idx += 1\n\n            if len(filedata_lines) > 0:\n                insert_idx = 1\n            else:\n                insert_idx = 0\n            filedata_lines.insert(\n                insert_idx, \'{} = {}\'.format(\n                    ""num_train_data"", num_train_data))\n            filedata_lines.insert(\n                insert_idx, \'{} = {}\'.format(\n                    ""num_classes"", num_classes))\n            filedata_lines.insert(\n                insert_idx, \'{} = {}\'.format(\n                    ""max_seq_length"", max_seq_length))\n\n        with open(""./config_data.py"", \'w\') as file:\n            file.write(\'\\n\'.join(filedata_lines))\n        tf.logging.info(""config_data.py has been updated"")\n    else:\n        tf.logging.info(""config_data.py cannot be found"")\n\n    tf.logging.info(""Data preparation finished"")\n\n\ndef main():\n    """"""Prepares data.\n    """"""\n    # Loads data\n    tf.logging.info(""Loading data"")\n\n    task_datasets_rename = {\n        ""COLA"": ""CoLA"",\n        ""SST"": ""SST-2"",\n    }\n\n    data_dir = \'data/{}\'.format(FLAGS.task)\n    if FLAGS.task.upper() in task_datasets_rename:\n        data_dir = \'data/{}\'.format(\n            task_datasets_rename[FLAGS.task])\n\n    if FLAGS.tfrecord_output_dir is None:\n        tfrecord_output_dir = data_dir\n    else:\n        tfrecord_output_dir = FLAGS.tfrecord_output_dir\n    tx.utils.maybe_create_dir(tfrecord_output_dir)\n\n    processors = {\n        ""COLA"": data_utils.ColaProcessor,\n        ""MNLI"": data_utils.MnliProcessor,\n        ""MRPC"": data_utils.MrpcProcessor,\n        ""XNLI"": data_utils.XnliProcessor,\n        \'SST\': data_utils.SSTProcessor\n    }\n    processor = processors[FLAGS.task]()\n\n    num_classes = len(processor.get_labels())\n    num_train_data = len(processor.get_train_examples(data_dir))\n    tf.logging.info(\n        \'num_classes:%d; num_train_data:%d\' % (num_classes, num_train_data))\n\n    tokenizer = tx.data.BERTTokenizer(\n        pretrained_model_name=FLAGS.pretrained_model_name)\n\n    # Produces TFRecord files\n    data_utils.prepare_TFRecord_data(\n        processor=processor,\n        tokenizer=tokenizer,\n        data_dir=data_dir,\n        max_seq_length=FLAGS.max_seq_length,\n        output_dir=tfrecord_output_dir)\n\n    _modify_config_data(FLAGS.max_seq_length, num_train_data, num_classes)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/distributed_gpu/config_large.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM large size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.04\nnum_epochs = 55\nhidden_size = 1500\nkeep_prob = 0.35\nbatch_size = 20\nnum_steps = 35\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 10.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 1. / 1.15,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 14\n    }\n}\n'"
examples/distributed_gpu/config_medium.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM medium size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.05\nnum_epochs = 39\nhidden_size = 650\nkeep_prob = 0.5\nbatch_size = 20\nnum_steps = 35\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.8,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 5\n    }\n}\n'"
examples/distributed_gpu/config_small.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM small size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.1\nnum_epochs = 13\nhidden_size = 200\nkeep_prob = 1.0\nbatch_size = 20\nnum_steps = 20\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.5,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 3\n    }\n}\n'"
examples/distributed_gpu/lm_ptb_distributed.py,26,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building the language model.\n\nThis is a reimpmentation of the TensorFlow official PTB example in:\ntensorflow/models/rnn/ptb\n\nModel and training are described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\n http://arxiv.org/abs/1409.2329\n\nThere are 3 provided model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\n\nThe data required for this example is in the `data/` dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\nIf data is not provided, the program will download from above automatically.\n\nTo run:\n\n$ python lm_ptb.py --data_path=simple-examples/data --config=config_small\n""""""\n\n# pylint: disable=invalid-name, no-member, too-many-locals\n\nimport time\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\nimport horovod.tensorflow as hvd\n\nfrom ptb_reader import prepare_data, ptb_iterator\n\nflags = tf.flags\n\nflags.DEFINE_string(""data_path"", ""./"",\n                    ""Directory containing PTB raw data (e.g., ptb.train.txt). ""\n                    ""E.g., ./simple-examples/data. If not exists, ""\n                    ""the directory will be created and PTB raw data will ""\n                    ""be downloaded."")\nflags.DEFINE_string(""config"", ""config_small"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    # Data\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # 1. initialize the horovod\n    hvd.init()\n\n    batch_size = config.batch_size\n    num_steps = config.num_steps\n    data = prepare_data(FLAGS.data_path)\n    vocab_size = data[""vocab_size""]\n\n    inputs = tf.placeholder(tf.int32, [None, num_steps],\n                            name=\'inputs\')\n    targets = tf.placeholder(tf.int32, [None, num_steps],\n                             name=\'targets\')\n\n    # Model architecture\n    initializer = tf.random_uniform_initializer(\n        -config.init_scale, config.init_scale)\n    with tf.variable_scope(""model"", initializer=initializer):\n        embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams=config.emb)\n        emb_inputs = embedder(inputs)\n        if config.keep_prob < 1:\n            emb_inputs = tf.nn.dropout(\n                emb_inputs, tx.utils.switch_dropout(config.keep_prob))\n\n        decoder = tx.modules.BasicRNNDecoder(\n            vocab_size=vocab_size, hparams={""rnn_cell"": config.cell})\n\n        # This _batch_size equals to batch_size // hvd.size() in\n        # distributed training.\n        # because the mini-batch is distributed to multiple GPUs\n\n        _batch_size = tf.shape(inputs)[0]\n        initial_state = decoder.zero_state(_batch_size,\n                                           tf.float32)\n        seq_length = tf.broadcast_to([num_steps], (_batch_size, ))\n        outputs, final_state, seq_lengths = decoder(\n            decoding_strategy=""train_greedy"",\n            impute_finished=True,\n            inputs=emb_inputs,\n            sequence_length=seq_length,\n            initial_state=initial_state)\n    # Losses & train ops\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=targets,\n        logits=outputs.logits,\n        sequence_length=seq_lengths)\n\n    # Use global_step to pass epoch, for lr decay\n    global_step = tf.placeholder(tf.int32)\n\n    opt = tx.core.get_optimizer(\n        global_step=global_step,\n        hparams=config.opt\n    )\n\n    # 2. wrap the optimizer\n    opt = hvd.DistributedOptimizer(opt)\n\n    train_op = tx.core.get_train_op(\n        loss=mle_loss,\n        optimizer=opt,\n        global_step=global_step,\n        learning_rate=None,\n        increment_global_step=False,\n        hparams=config.opt\n    )\n\n    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):\n        start_time = time.time()\n        loss = 0.\n        iters = 0\n\n        fetches = {\n            ""mle_loss"": mle_loss,\n            ""final_state"": final_state,\n        }\n        if is_train:\n            fetches[""train_op""] = train_op\n            epoch_size = (len(data[""train_text_id""]) // batch_size - 1)\\\n                // num_steps\n\n        mode = (tf.estimator.ModeKeys.TRAIN\n                if is_train\n                else tf.estimator.ModeKeys.EVAL)\n\n        for step, (x, y) in enumerate(data_iter):\n            if step == 0:\n                state = sess.run(initial_state,\n                                 feed_dict={inputs: x})\n\n            feed_dict = {\n                inputs: x, targets: y, global_step: epoch,\n                tx.global_mode(): mode,\n            }\n            for i, (c, h) in enumerate(initial_state):\n                feed_dict[c] = state[i].c\n                feed_dict[h] = state[i].h\n\n            rets = sess.run(fetches, feed_dict)\n            loss += rets[""mle_loss""]\n            state = rets[""final_state""]\n            iters += num_steps\n\n            ppl = np.exp(loss / iters)\n            if verbose and is_train and hvd.rank() == 0 \\\n                and (step + 1) % (epoch_size // 10) == 0:\n                tf.logging.info(""%.3f perplexity: %.3f speed: %.0f wps"" %\n                                ((step + 1) * 1.0 / epoch_size, ppl,\n                                 iters * batch_size / (\n                                         time.time() - start_time)))\n        _elapsed_time = time.time() - start_time\n        tf.logging.info(""epoch time elapsed: %f"" % (_elapsed_time))\n        ppl = np.exp(loss / iters)\n        return ppl, _elapsed_time\n\n    # 3. set broadcase global variables from rank-0 process\n    bcast = hvd.broadcast_global_variables(0)\n\n    # 4. set visible GPU\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    with tf.Session(config=session_config) as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        # 5. run the broadcast_global_variables node before training\n        bcast.run()\n\n        _times = []\n        for epoch in range(config.num_epochs):\n            # Train\n            train_data_iter = ptb_iterator(\n                data[""train_text_id""], config.batch_size, num_steps,\n                is_train=True)\n            train_ppl, train_time = _run_epoch(\n                sess, train_data_iter, epoch, is_train=True, verbose=True)\n            _times.append(train_time)\n            tf.logging.info(""Epoch: %d Train Perplexity: %.3f"" % (epoch, train_ppl))\n            # Valid in the main process\n            if hvd.rank() == 0:\n                valid_data_iter = ptb_iterator(\n                    data[""valid_text_id""], config.batch_size, num_steps)\n                valid_ppl, _ = _run_epoch(sess, valid_data_iter, epoch)\n                tf.logging.info(""Epoch: %d Valid Perplexity: %.3f""\n                                % (epoch, valid_ppl))\n\n        tf.logging.info(\'train times: %s\' % (_times))\n        tf.logging.info(\'average train time/epoch %f\'\n                        % np.mean(np.array(_times)))\n        # Test in the main process\n        if hvd.rank() == 0:\n            test_data_iter = ptb_iterator(\n                data[""test_text_id""], batch_size, num_steps)\n            test_ppl, _ = _run_epoch(sess, test_data_iter, 0)\n            tf.logging.info(""Test Perplexity: %.3f"" % (test_ppl))\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/distributed_gpu/ptb_reader.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for preprocessing and iterating over the PTB data.\n""""""\n\n# pylint: disable=invalid-name, too-many-locals\n\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nimport horovod.tensorflow as hvd\nimport texar.tf as tx\n\n\ndef ptb_iterator(data, batch_size, num_steps, is_train=False):\n    """"""Iterates through the ptb data.\n    """"""\n\n    data_length = len(data)\n\n    batch_length = data_length // batch_size\n    data = np.asarray(data[:batch_size * batch_length])\n    data = data.reshape([batch_size, batch_length])\n\n    epoch_size = (batch_length - 1) // num_steps\n    if epoch_size == 0:\n        raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n    def _sharded_data(data):\n        _batch_size = len(data)\n        _shard_size = _batch_size // hvd.size()\n        data = [data[i * _shard_size: (i + 1) * _shard_size]\n                for i in range(_shard_size)]\n        data = data[hvd.rank()]\n        return data\n\n    if is_train:\n        # split the dataset into shards to make sure\n        # different processed are loaded with different training data\n        data = _sharded_data(data)\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps: (i + 1) * num_steps]\n        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n        yield (x, y)\n\n\ndef prepare_data(data_path):\n    """"""Preprocess PTB data.\n    """"""\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    if not tf.gfile.Exists(train_path):\n        url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\'\n        tx.data.maybe_download(url, data_path, extract=True)\n        data_path = os.path.join(data_path, \'simple-examples\', \'data\')\n\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n    test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n    word_to_id = tx.data.make_vocab(\n        train_path, newline_token=""<EOS>"", return_type=""dict"")\n    assert len(word_to_id) == 10000\n\n    train_text = tx.data.read_words(\n        train_path, newline_token=""<EOS>"")\n    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]\n\n    valid_text = tx.data.read_words(\n        valid_path, newline_token=""<EOS>"")\n    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]\n\n    test_text = tx.data.read_words(\n        test_path, newline_token=""<EOS>"")\n    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]\n\n    data = {\n        ""train_text"": train_text,\n        ""valid_text"": valid_text,\n        ""test_text"": test_text,\n        ""train_text_id"": train_text_id,\n        ""valid_text_id"": valid_text_id,\n        ""test_text_id"": test_text_id,\n        ""vocab"": word_to_id,\n        ""vocab_size"": len(word_to_id)\n    }\n    return data\n'"
examples/gpt-2/gpt2_generate_main.py,16,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of building OpenAI GPT-2 language model for sample generation.\n""""""\n\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom utils import model_utils, processor\n\n# pylint: disable=invalid-name, too-many-locals, too-many-statements, no-member\n# pylint: disable=too-many-branches\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""checkpoint"", None,\n                    ""Model checkpoint to load model weights from. Use ""\n                    ""`--pretrain_checkpoint` instead if loading OpenAI ""\n                    ""pretrained checkpoint."")\nflags.DEFINE_string(""pretrain_checkpoint"",\n                    ""gpt2_pretrained_models/model_117M/model.ckpt"",\n                    ""OpenAI pretrained model checkpoint. Ignored if ""\n                    ""\'--checkpoint\' is specified."")\nflags.DEFINE_string(""pretrain_model_dir"", ""gpt2_pretrained_models/model_117M"",\n                     ""The directory of pretrained model, for loading ""\n                     ""vocabuary, etc."")\nflags.DEFINE_integer(""seed"", None, ""Random seed."")\nflags.DEFINE_integer(""nsamples"", 1, ""The number of samples per input."")\nflags.DEFINE_integer(""batch_size"", 1, ""The batch size of input."")\nflags.DEFINE_integer(""max_decoding_length"", 100,\n                     ""The maximun length of generated text."")\nflags.DEFINE_float(""temperature"", 0.7,\n                   ""Softmax temperature for top-k sample decoding. Must be ""\n                   ""strictly greater than 0. Defaults to 0.7."")\nflags.DEFINE_integer(""top_k"", 40,\n                     ""The number of top most likely candidates from a vocab ""\n                     ""distribution."")\nflags.DEFINE_boolean(""is_interactive"", False, ""Interactive mode or not."")\nflags.DEFINE_string(""config_type"", ""texar"",\n                    ""The configuration file type. Set to \'json\' if the GPT-2 ""\n                    ""config file is in the same type of the official GPT-2 ""\n                    ""config file. Set to \'texar\' if GPT-2 config file is in ""\n                    ""Texar type."")\nflags.DEFINE_string(""config_model"", ""configs.config_model_117M"",\n                    ""The model configuration file to configure the model. ""\n                    ""The config file type is define by the \'config_type\',""\n                    ""it be of texar type or json type.""\n                    ""For \'--config_type=json\', set the json config file path""\n                    ""like: \'--config_model gpt2_pretrained_models/model_117M/""\n                    ""hparams.json\';""\n                    ""For \'--config_type=texar\', set the texar config file ""\n                    ""like: \'--config_model configs.config_model_117M\'."")\n\n\ndef main(_):\n    """"""\n    Builds the model and runs\n    """"""\n    np.random.seed(FLAGS.seed)\n    tf.set_random_seed(FLAGS.seed)\n\n    nsamples = FLAGS.nsamples\n    batch_size = FLAGS.batch_size\n    max_decoding_length = FLAGS.max_decoding_length\n\n    # Load GPT-2 model configuration\n    if FLAGS.config_type == ""json"":\n        gpt2_config = model_utils.transform_gpt2_to_texar_config(\n            FLAGS.config_model)\n    elif FLAGS.config_type == ""texar"":\n        gpt2_config = importlib.import_module(\n            FLAGS.config_model)\n    else:\n        raise ValueError(""Unknown config_type."")\n\n    assert max_decoding_length <= gpt2_config.position_size, (\n        ""max_decoding_length should not be greater than position size"")\n    assert nsamples % batch_size == 0, (\n        ""nsamples must be dividable by batch_size"")\n\n    # Create a data pre-processor for, e.g., BPE encoding\n    proc = processor.get_encoder(\n        FLAGS.pretrain_model_dir)\n\n    context = tf.placeholder(tf.int32, [batch_size, None])\n    context_length = tf.placeholder(tf.int32, [batch_size])\n\n    end_token = proc.encoder[""<|endoftext|>""]\n    if FLAGS.is_interactive:\n        start_tokens = context[:, 0]\n    else:\n        start_tokens = tf.fill([batch_size], end_token)\n\n    # Build the GPT-2 model\n    word_embedder = tx.modules.WordEmbedder(\n        vocab_size=gpt2_config.vocab_size,\n        hparams=gpt2_config.embed)\n\n    pos_embedder = tx.modules.PositionEmbedder(\n        position_size=gpt2_config.position_size,\n        hparams=gpt2_config.pos_embed)\n\n    def _embedding_fn(x, y):\n        # `x` is token ids, `y` is time steps\n        return word_embedder(x) + pos_embedder(y)\n\n    helper = tx.modules.TopKSampleEmbeddingHelper(\n        embedding=_embedding_fn,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        top_k=FLAGS.top_k,\n        softmax_temperature=FLAGS.temperature)\n    output_layer = tf.transpose(word_embedder.embedding, (1, 0))\n\n    decoder = tx.modules.TransformerDecoder(\n        vocab_size=gpt2_config.vocab_size,\n        output_layer=output_layer,\n        hparams=gpt2_config.decoder)\n\n    with tf.Session() as sess:\n\n        if FLAGS.is_interactive:\n            # Generate continuations of context\n            lm_output, _ = decoder(\n                context=context,\n                context_sequence_length=context_length,\n                max_decoding_length=max_decoding_length,\n                helper=helper,\n                mode=tf.estimator.ModeKeys.PREDICT)\n\n            # Load model checkpoint\n            if FLAGS.checkpoint:\n                tf.logging.info(""Restore from {}"".format(FLAGS.checkpoint))\n                saver = tf.train.Saver()\n                saver.restore(sess, FLAGS.checkpoint)\n            elif FLAGS.pretrain_checkpoint:\n                model_utils.init_gpt2_checkpoint(\n                    sess, FLAGS.pretrain_checkpoint)\n                print(""\\nFinished loading\\n"")\n\n            # Enter interactive mode\n            while True:\n\n                raw_text = input(""Model input >>> "")\n\n                while not raw_text:\n                    print(""Input should not be empty!"")\n                    raw_text = input(""Model input >>> "")\n\n                context_tokens = proc.encode(raw_text)\n\n                feed_dict = {\n                    context: [context_tokens for _ in range(batch_size)],\n                    context_length:\n                        [len(context_tokens) for _ in range(batch_size)],\n                    tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT\n                }\n                generated = 0\n                for _ in range(nsamples // batch_size):\n\n                    output = sess.run(lm_output, feed_dict=feed_dict)\n\n                    sample_id = output.sample_id\n                    for i in range(batch_size):\n\n                        generated += 1\n                        print(""="" * 40 +\n                              "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)\n                        si = sample_id[i][len(context_tokens):]\n                        print(proc.decode(si))\n                print(""="" * 80)\n        else:\n            # Generate samples from scratch\n            lm_output, _ = decoder(\n                max_decoding_length=max_decoding_length,\n                helper=helper,\n                mode=tf.estimator.ModeKeys.PREDICT)\n\n            # Load model checkpoint\n            if FLAGS.checkpoint:\n                tf.logging.info(""Restore from {}"".format(FLAGS.checkpoint))\n                saver = tf.train.Saver()\n                saver.restore(sess, FLAGS.checkpoint)\n            elif FLAGS.pretrain_checkpoint:\n                model_utils.init_gpt2_checkpoint(\n                    sess, FLAGS.pretrain_checkpoint)\n                print(""\\nFinished loading\\n"")\n\n            feed_dict = {\n                tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT\n            }\n            generated = 0\n            while nsamples == 0 or generated < nsamples:\n\n                output = sess.run(lm_output, feed_dict=feed_dict)\n\n                sample_id = output.sample_id\n                for i in range(batch_size):\n\n                    generated += batch_size\n                    text = proc.decode(sample_id[i])\n                    print(""="" * 40 +\n                          "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)\n                    print(text)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
examples/gpt-2/gpt2_train_main.py,29,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example of fine-tuning OpenAI GPT-2 language model.\n""""""\n\nimport os\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom utils import model_utils, processor\n\n# pylint: disable=invalid-name, too-many-locals, too-many-statements, no-member\n# pylint: disable=too-many-branches\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""checkpoint"", None,\n                    ""Model checkpoint to resume training or for test."")\nflags.DEFINE_string(""pretrain_checkpoint"",\n                    ""gpt2_pretrained_models/model_117M/model.ckpt"",\n                    ""OpenAI pretrained model checkpoint. Ignored if ""\n                    ""\'--checkpoint\' is specified."")\nflags.DEFINE_string(""pretrain_model_dir"", ""gpt2_pretrained_models/model_117M"",\n                    ""The directory of pretrained model, for loading vocabuary, ""\n                    ""etc."")\nflags.DEFINE_float(""temperature"", 0.7,\n                   ""Softmax temperature for top-k sample decoding. Must be ""\n                   ""strictly greater than 0. Defaults to 0.7."")\nflags.DEFINE_integer(""top_k"", 40,\n                     ""The number of top most likely candidates from a vocab ""\n                     ""distribution."")\nflags.DEFINE_string(""config_train"", ""configs.config_train"",\n                    ""Configurations of GPT-2 training, including data and ""\n                    ""optimization hyperparameters."")\nflags.DEFINE_string(""config_type"", ""texar"",\n                    ""The configuration file type. Set to \'json\' if the GPT-2 ""\n                    ""config file is in the same type of the official GPT-2 ""\n                    ""config file. Set to \'texar\' if GPT-2 config file is in ""\n                    ""Texar type."")\nflags.DEFINE_string(""config_model"", ""configs.config_model_117M"",\n                    ""The model configuration file to configure the model. ""\n                    ""The config file type is define by the \'config_type\',""\n                    ""it be of texar type or json type.""\n                    ""For \'--config_type=json\', set the json config file path""\n                    ""like: \'--config_model gpt2_pretrained_models/model_117M/""\n                    ""hparams.json\';""\n                    ""For \'--config_type=texar\', set the texar config file ""\n                    ""like: \'--config_model configs.config_model_117M\'."")\nflags.DEFINE_string(""output_dir"", ""output/"",\n                    ""The output directory where the model checkpoints will be ""\n                    ""written."")\nflags.DEFINE_bool(""do_train"", False, ""Whether to run training."")\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\nflags.DEFINE_bool(""do_test"", False, ""Whether to run test on the test set."")\nflags.DEFINE_bool(""distributed"", False, ""Whether to run in distributed mode."")\n\nconfig_train = importlib.import_module(FLAGS.config_train)\n\n\ndef main(_):\n    """"""\n    Builds the model and runs\n    """"""\n    if FLAGS.distributed:\n        import horovod.tensorflow as hvd\n        hvd.init()\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    # Loads GPT-2 model configuration\n\n    if FLAGS.config_type == ""json"":\n        gpt2_config = model_utils.transform_gpt2_to_texar_config(\n            FLAGS.config_model)\n    elif FLAGS.config_type == \'texar\':\n        gpt2_config = importlib.import_module(\n            FLAGS.config_model)\n    else:\n        raise ValueError(\'Unknown config_type.\')\n\n    # Creates a data pre-processor for, e.g., BPE encoding\n    proc = processor.get_encoder(FLAGS.pretrain_model_dir)\n\n    max_decoding_length = config_train.max_decoding_length\n    assert max_decoding_length <= gpt2_config.position_size, (\n        ""max_decoding_length should not be greater than position_size. ""\n        ""{}>{}"".format(max_decoding_length, gpt2_config.position_size))\n\n    # Loads data\n\n    # Configures training data shard in distributed mode\n    if FLAGS.distributed:\n        config_train.train_hparam[""dataset""][""num_shards""] = hvd.size()\n        config_train.train_hparam[""dataset""][""shard_id""] = hvd.rank()\n        config_train.train_hparam[""batch_size""] //= hvd.size()\n\n    datasets = {}\n    if FLAGS.do_train:\n        train_dataset = tx.data.TFRecordData(hparams=config_train.train_hparam)\n        datasets[\'train\'] = train_dataset\n    if FLAGS.do_eval:\n        dev_dataset = tx.data.TFRecordData(hparams=config_train.dev_hparam)\n        datasets[\'dev\'] = dev_dataset\n    if FLAGS.do_test:\n        test_dataset = tx.data.TFRecordData(hparams=config_train.test_hparam)\n        datasets[\'test\'] = test_dataset\n    iterator = tx.data.FeedableDataIterator(datasets)\n    batch = iterator.get_next()\n    batch_size = tf.shape(batch[\'text_ids\'])[0]\n\n    # Builds the GPT-2 model\n\n    word_embedder = tx.modules.WordEmbedder(\n        vocab_size=gpt2_config.vocab_size,\n        hparams=gpt2_config.embed)\n\n    pos_embedder = tx.modules.PositionEmbedder(\n        position_size=gpt2_config.position_size,\n        hparams=gpt2_config.pos_embed)\n\n    # Ties output layer with input word embedding\n    output_layer = tf.transpose(word_embedder.embedding, (1, 0))\n\n    decoder = tx.modules.TransformerDecoder(\n        vocab_size=gpt2_config.vocab_size,\n        output_layer=output_layer,\n        hparams=gpt2_config.decoder)\n\n    # For training\n    seq_len = tf.fill([batch_size], tf.shape(batch[\'text_ids\'])[1])\n    pos_embeds = pos_embedder(sequence_length=seq_len)\n    input_embeds = word_embedder(batch[\'text_ids\']) + pos_embeds\n\n    outputs = decoder(inputs=input_embeds, decoding_strategy=\'train_greedy\')\n\n    loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=batch[\'text_ids\'][:, 1:],\n        logits=outputs.logits[:, :-1, :],\n        sequence_length=batch[\'length\'] - 1,\n        average_across_timesteps=True,\n        sum_over_timesteps=False)\n    ppl = tf.exp(loss)\n\n    global_step = tf.Variable(0, trainable=False)\n    opt = tx.core.get_optimizer(\n        global_step=global_step,\n        hparams=config_train.opt)\n\n    if FLAGS.distributed:\n        opt = hvd.DistributedOptimizer(opt)\n\n    train_op = tf.contrib.layers.optimize_loss(\n        loss=loss,\n        global_step=global_step,\n        learning_rate=None,\n        optimizer=opt)\n\n    # For generation: generates continuations of test text\n    def _embedding_fn(x, y):\n        # `x` is token ids, `y` is time steps\n        return word_embedder(x) + pos_embedder(y)\n\n    end_token = proc.encoder[\'<|endoftext|>\']\n    start_tokens = batch[\'text_ids\'][:, 0]\n    helper = tx.modules.TopKSampleEmbeddingHelper(\n        embedding=_embedding_fn,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        top_k=FLAGS.top_k,\n        softmax_temperature=FLAGS.temperature)\n\n    outputs_infer, _ = decoder(\n        context=batch[\'text_ids\'],\n        context_sequence_length=batch[\'length\'],\n        max_decoding_length=max_decoding_length,\n        helper=helper)\n    sample_id = outputs_infer.sample_id\n\n    # Train/eval/test routine\n    saver = tf.train.Saver()\n    saver_best = tf.train.Saver(max_to_keep=1)\n    dev_best = {\'loss\': 1e8, \'ppl\': 1e8}\n\n    def _is_head():\n        if not FLAGS.distributed:\n            return True\n        else:\n            return hvd.rank() == 0\n\n    def _train_epoch(sess):\n        """"""Trains on the training set, and evaluates on the dev set\n        periodically.\n        """"""\n        iterator.restart_dataset(sess, \'train\')\n\n        fetches = {\n            \'loss\': train_op,\n            \'step\': global_step\n        }\n\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'train\'),\n                    tx.global_mode(): tf.estimator.ModeKeys.TRAIN,\n                }\n                rets = sess.run(fetches, feed_dict)\n                step = rets[\'step\']\n\n                dis_steps = config_train.display_steps\n                if _is_head() and dis_steps > 0 and step % dis_steps == 0:\n                    tf.logging.info(\'step:%d; loss:%f\' % (step, rets[\'loss\']))\n\n                eval_steps = config_train.eval_steps\n                if _is_head() and eval_steps > 0 and step % eval_steps == 0:\n                    _dev_epoch(sess)\n\n                ckpt_steps = config_train.checkpoint_steps\n                if _is_head() and ckpt_steps > 0 and step % ckpt_steps == 0:\n                    ckpt_fn = os.path.join(FLAGS.output_dir, \'model.ckpt\')\n                    ckpt_fn = saver.save(sess, ckpt_fn, global_step=step)\n                    tf.logging.info(\'Checkpoint to {}\'.format(ckpt_fn))\n\n            except tf.errors.OutOfRangeError:\n                break\n\n    def _dev_epoch(sess):\n        """"""Evaluates on the dev set.\n        """"""\n        iterator.restart_dataset(sess, \'dev\')\n\n        cum_loss = 0.\n        cum_ppl = 0.\n        nsamples = 0\n        fetches = {\n            \'loss\': loss,\n            \'ppl\': ppl,\n            \'batch_size\': batch_size,\n        }\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'dev\'),\n                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,\n                }\n                rets = sess.run(fetches, feed_dict)\n\n                cum_loss += rets[\'loss\'] * rets[\'batch_size\']\n                cum_ppl += rets[\'ppl\'] * rets[\'batch_size\']\n                nsamples += rets[\'batch_size\']\n            except tf.errors.OutOfRangeError:\n                break\n\n        avg_loss = cum_loss / nsamples\n        avg_ppl = cum_ppl / nsamples\n        tf.logging.info(\'dev loss: {}; ppl: {}; nsamples: {}\'.format(\n            avg_loss, avg_ppl, nsamples))\n\n        if FLAGS.do_train and avg_loss < dev_best[\'loss\']:\n            dev_best[\'loss\'] = avg_loss\n            dev_best[\'ppl\'] = avg_ppl\n            ckpt_fn = os.path.join(FLAGS.output_dir, \'model_best.ckpt\')\n            ckpt_fn = saver_best.save(sess, ckpt_fn)\n            tf.logging.info(\'Checkpoint best to {}\'.format(ckpt_fn))\n\n    def _test_epoch(sess):\n        """"""Generates samples on the test set.\n        """"""\n        iterator.restart_dataset(sess, \'test\')\n\n        _all_inputs = []\n        _all_samples = []\n        fetches = {\n            \'inputs\': batch[\'text_ids\'],\n            \'length\': batch[\'length\'],\n            \'samples\': sample_id\n        }\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'test\'),\n                    tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT,\n                }\n                rets = sess.run(fetches, feed_dict=feed_dict)\n\n                _inputs = []\n                for i, l in zip(rets[\'inputs\'], rets[\'length\']):\n                    # Delete padding\n                    _inputs.append(i[:l].tolist())\n                _all_inputs.extend(_inputs)\n\n                _samples = []\n                for s, l in zip(rets[\'samples\'], rets[\'length\']):\n                    # Delete inputs from samples\n                    _samples.append(s[l:].tolist())\n                _all_samples.extend(_samples)\n\n            except tf.errors.OutOfRangeError:\n                break\n\n        # Parse samples and write to file\n\n        eos_token_id = proc.encoder[\'<|endoftext|>\']\n\n        _all_input_text = []\n        for i in _all_inputs:\n            if i[0] == eos_token_id:\n                # \'<|endoftext|>\' is used as the BOS token. Delete it here\n                i = i[1:]\n            i_text = proc.decode(i)\n            _all_input_text.append(i_text)\n        # \'<|endoftext|>\' is used as the PAD token. Delete them here\n        _all_input_text = tx.utils.strip_eos(_all_input_text,\n                                             eos_token=\'<|endoftext|>\')\n\n        _all_samples_text = []\n        for i, s in zip(_all_inputs, _all_samples):\n            s_text = proc.decode(s)\n            s_text = s_text.replace(\'\\n\', \' \')\n            _all_samples_text.append(s_text)\n        _all_samples_text = tx.utils.strip_eos(_all_samples_text,\n                                               eos_token=\'<|endoftext|>\')\n\n        output_file = os.path.join(FLAGS.output_dir, ""test_samples.tsv"")\n        tf.logging.info(\'Write samples to {}\'.format(output_file))\n        tx.utils.write_paired_text(\n            _all_input_text, _all_samples_text, output_file)\n\n    # Broadcasts global variables from rank-0 process\n    if FLAGS.distributed:\n        bcast = hvd.broadcast_global_variables(0)\n\n    session_config = tf.ConfigProto()\n    if FLAGS.distributed:\n        session_config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    with tf.Session(config=session_config) as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        if FLAGS.distributed:\n            bcast.run()\n\n        # Restores trained model if specified\n        if FLAGS.checkpoint:\n            tf.logging.info(\'Restore from {}\'.format(FLAGS.checkpoint))\n            saver.restore(sess, FLAGS.checkpoint)\n        elif FLAGS.pretrain_checkpoint:\n            tf.logging.info(\'Restore from {}\'.format(FLAGS.pretrain_checkpoint))\n            model_utils.init_gpt2_checkpoint(sess, FLAGS.pretrain_checkpoint)\n            print(""\\nFinished loading\\n"")\n\n        iterator.initialize_dataset(sess)\n\n        if FLAGS.do_train:\n            for _ in range(config_train.max_train_epoch):\n                _train_epoch(sess)\n            saver.save(sess, FLAGS.output_dir + \'/model.ckpt\')\n\n        if FLAGS.do_eval:\n            _dev_epoch(sess)\n\n        if FLAGS.do_test:\n            _test_epoch(sess)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
examples/gpt-2/prepare_data.py,2,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Preprocesses raw data and produces TFRecord files\n""""""\n\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom utils import data_utils, processor\n\n# pylint: disable=invalid-name, too-many-locals, too-many-statements\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\n    ""data_dir"", \'data/toy\',\n    ""The directory of raw data, wherein data files must be named as ""\n    ""\'train.txt\', \'dev.txt\', or \'test.txt\'."")\nflags.DEFINE_integer(\n    ""max_seq_length"", 128,\n    ""The maxium length of sequence, longer sequence will be trimmed."")\nflags.DEFINE_string(\n    ""tfrecord_output_dir"", None,\n    ""The output directory where the TFRecord files will be generated. ""\n    ""By default it is set to be the same as `--data_dir`."")\nflags.DEFINE_string(\n    ""pretrain_model_dir"", ""gpt2_pretrained_models/model_117M"",\n    ""The directory of pretrained model."")\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef prepare_data():\n    """"""\n    Builds the model and runs.\n    """"""\n    data_dir = FLAGS.data_dir\n    if FLAGS.tfrecord_output_dir is None:\n        tfrecord_output_dir = data_dir\n    else:\n        tfrecord_output_dir = FLAGS.tfrecord_output_dir\n    tx.utils.maybe_create_dir(tfrecord_output_dir)\n\n    # Creates a data pre-processor for, e.g., BPE encoding\n    proc = processor.get_encoder(FLAGS.pretrain_model_dir)\n\n    # Produces TFRecord files\n    data_utils.prepare_TFRecord_data(\n        data_dir=data_dir,\n        max_seq_length=FLAGS.max_seq_length,\n        encoder=proc,\n        output_dir=tfrecord_output_dir)\n\n\ndef main():\n    """"""Data preparation.\n    """"""\n    prepare_data()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/hierarchical_dialog/config_data.py,0,"b'import os\n\ndata_root = \'./data\'\nmax_utterance_cnt = 9\n\ndata_hparams = {\n    stage: {\n        ""num_epochs"": 1,\n        ""shuffle"": stage != \'test\',\n        ""batch_size"": 30,\n        ""datasets"": [\n            {  # source\n                ""variable_utterance"": True,\n                ""max_utterance_cnt"": max_utterance_cnt,\n                ""files"": [\n                    os.path.join(data_root,\n                                 \'{}-source.txt\'.format(stage))],\n                ""vocab_file"": os.path.join(data_root, \'vocab.txt\'),\n                ""embedding_init"": {\n                    ""file"": os.path.join(data_root, \'embedding.txt\'),\n                    ""dim"": 200,\n                    ""read_fn"": ""load_glove""\n                },\n                ""data_name"": ""source""\n            },\n            {  # target\n                ""files"": [\n                    os.path.join(data_root, \'{}-target.txt\'.format(stage))],\n                ""vocab_share_with"": 0,\n                ""data_name"": ""target""\n            },\n        ] + [{  # source speaker token\n                ""files"": os.path.join(data_root,\n                                      \'{}-source-spk-{}.txt\'.format(stage, i)),\n                ""data_type"": ""float"",\n                ""data_name"": ""spk_{}"".format(i)\n            } for i in range(max_utterance_cnt)\n        ] + [{  # target speaker token\n                ""files"": os.path.join(data_root,\n                                      \'{}-target-spk.txt\'.format(stage)),\n                ""data_type"": ""float"",\n                ""data_name"": ""spk_tgt""\n            }\n        ] + [{  # target refs for BLEU evaluation\n                ""variable_utterance"": True,\n                ""max_utterance_cnt"": 10,\n                ""files"": [os.path.join(data_root,\n                                       \'{}-target-refs.txt\'.format(stage))],\n                ""vocab_share_with"": 0,\n                ""data_name"": ""refs""\n            }]\n    }\n    for stage in [\'train\', \'val\', \'test\']\n}\n'"
examples/hierarchical_dialog/config_model_biminor.py,3,"b'\nimport tensorflow as tf\n\nnum_samples = 10  # Number of samples generated for each test data instance\nbeam_width = num_samples\n\nencoder_hparams = {\n    ""encoder_minor_type"": ""BidirectionalRNNEncoder"",\n    ""encoder_minor_hparams"": {\n        ""rnn_cell_fw"": {\n            ""type"": ""GRUCell"",\n            ""kwargs"": {\n                ""num_units"": 300,\n                ""kernel_initializer"": tf.initializers.random_uniform(-0.08, 0.08)\n            },\n            ""dropout"": {\n                ""input_keep_prob"": 0.5,\n            }\n        },\n        ""rnn_cell_share_config"": True\n    },\n    ""encoder_major_type"": ""UnidirectionalRNNEncoder"",\n    ""encoder_major_hparams"": {\n        ""rnn_cell"": {\n            ""type"": ""GRUCell"",\n            ""kwargs"": {\n                ""num_units"": 600,\n                ""kernel_initializer"": tf.initializers.random_uniform(-0.08, 0.08)\n            },\n            ""dropout"": {\n                ""output_keep_prob"": 0.3\n            }\n        }\n    }\n}\ndecoder_hparams = {\n    ""rnn_cell"": {\n        ""type"": ""GRUCell"",\n        ""kwargs"": {\n            ""num_units"": 400,\n            ""kernel_initializer"": tf.initializers.random_uniform(-0.08, 0.08),\n        },\n        ""dropout"": {\n            ""input_keep_prob"": 0.3\n        }\n    }\n}\nopt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.001,\n        }\n    },\n    # (It looks gradient clip does not affect the results a lot)\n    # ""gradient_clip"": {\n    #    ""type"": ""clip_by_global_norm"",\n    #    ""kwargs"": {""clip_norm"": 5.}\n    # },\n}\n'"
examples/hierarchical_dialog/config_model_uniminor.py,3,"b'\nimport tensorflow as tf\n\nnum_samples = 10  # Number of samples generated for each test data instance\nbeam_width = num_samples\n\nencoder_hparams = {\n    ""encoder_minor_type"": ""UnidirectionalRNNEncoder"",\n    ""encoder_minor_hparams"": {\n        ""rnn_cell"": {\n            ""type"": ""GRUCell"",\n            ""kwargs"": {\n                ""num_units"": 300,\n                ""kernel_initializer"": tf.initializers.random_uniform(-0.08, 0.08)\n            },\n            ""dropout"": {\n                ""input_keep_prob"": 0.5,\n            }\n        },\n    },\n    ""encoder_major_type"": ""UnidirectionalRNNEncoder"",\n    ""encoder_major_hparams"": {\n        ""rnn_cell"": {\n            ""type"": ""GRUCell"",\n            ""kwargs"": {\n                ""num_units"": 600,\n                ""kernel_initializer"": tf.initializers.random_uniform(-0.08, 0.08)\n            },\n            ""dropout"": {\n                ""input_keep_prob"": 0.3,\n            }\n        }\n    }\n}\ndecoder_hparams = {\n    ""rnn_cell"": {\n        ""type"": ""GRUCell"",\n        ""kwargs"": {\n            ""num_units"": 400,\n            ""kernel_initializer"": tf.initializers.random_uniform(-0.08, 0.08),\n        },\n        ""dropout"": {\n            ""output_keep_prob"": 0.3,\n        }\n    }\n}\nopt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.001,\n        }\n    }\n}\n'"
examples/hierarchical_dialog/hred.py,20,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Hierarchical Recurrent Encoder-Decoder (HRED) for dialog response\ngeneration.\n""""""\n\n# pylint: disable=invalid-name, too-many-locals\n\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n\nflags = tf.flags\n\nflags.DEFINE_string(\'config_data\', \'config_data\', \'The data config\')\nflags.DEFINE_string(\'config_model\', \'config_model_biminor\', \'The model config\')\n\nFLAGS = flags.FLAGS\n\nconfig_data = importlib.import_module(FLAGS.config_data)\nconfig_model = importlib.import_module(FLAGS.config_model)\n\nencoder_hparams = config_model.encoder_hparams\ndecoder_hparams = config_model.decoder_hparams\nopt_hparams = config_model.opt_hparams\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    # Data\n    train_data = tx.data.MultiAlignedData(config_data.data_hparams[\'train\'])\n    val_data = tx.data.MultiAlignedData(config_data.data_hparams[\'val\'])\n    test_data = tx.data.MultiAlignedData(config_data.data_hparams[\'test\'])\n    iterator = tx.data.TrainTestDataIterator(train=train_data,\n                                             val=val_data,\n                                             test=test_data)\n    data_batch = iterator.get_next()\n\n    # (speaker\'s meta info)\n    spk_src = tf.stack([data_batch[\'spk_{}\'.format(i)]\n                        for i in range(config_data.max_utterance_cnt)], 1)\n    spk_tgt = data_batch[\'spk_tgt\']\n\n    def _add_source_speaker_token(x):\n        return tf.concat([x, tf.reshape(spk_src, (-1, 1))], 1)\n\n    def _add_target_speaker_token(x):\n        return (x, ) + (tf.reshape(spk_tgt, (-1, 1)), )\n\n    # HRED model\n    embedder = tx.modules.WordEmbedder(\n        init_value=train_data.embedding_init_value(0).word_vecs)\n    encoder = tx.modules.HierarchicalRNNEncoder(hparams=encoder_hparams)\n\n    decoder = tx.modules.BasicRNNDecoder(\n        hparams=decoder_hparams, vocab_size=train_data.vocab(0).size)\n\n    connector = tx.modules.connectors.MLPTransformConnector(\n        decoder.cell.state_size)\n\n    context_embed = embedder(data_batch[\'source_text_ids\'])\n    ecdr_states = encoder(\n        context_embed,\n        medium=[\'flatten\', _add_source_speaker_token],\n        sequence_length_minor=data_batch[\'source_length\'],\n        sequence_length_major=data_batch[\'source_utterance_cnt\'])\n    ecdr_states = ecdr_states[1]\n\n    ecdr_states = _add_target_speaker_token(ecdr_states)\n    dcdr_states = connector(ecdr_states)\n\n    # (decoding for training)\n    target_embed = embedder(data_batch[\'target_text_ids\'])\n    outputs, _, lengths = decoder(\n        initial_state=dcdr_states,\n        inputs=target_embed,\n        sequence_length=data_batch[\'target_length\'] - 1)\n\n    # Sentence level lld, for training\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=data_batch[\'target_text_ids\'][:, 1:],\n        logits=outputs.logits,\n        sequence_length=lengths)\n    # Token level lld, for perplexity evaluation\n    avg_mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=data_batch[\'target_text_ids\'][:, 1:],\n        logits=outputs.logits,\n        sequence_length=lengths,\n        sum_over_timesteps=False,\n        average_across_timesteps=True)\n    perplexity = tf.exp(avg_mle_loss)\n\n    global_step = tf.Variable(0, name=\'global_step\', trainable=True)\n    train_op = tx.core.get_train_op(\n        mle_loss, global_step=global_step, hparams=opt_hparams)\n\n    # Decoding\n\n    target_bos_token_id = train_data.vocab(0).bos_token_id\n    target_eos_token_id = train_data.vocab(0).eos_token_id\n    start_tokens = \\\n        tf.ones_like(data_batch[\'target_length\']) * target_bos_token_id\n\n    # Random sample decoding\n    decoding_strategy = \'infer_\' + \'sample\'\n    infer_samples, lengths = [], []\n    for _ in range(config_model.num_samples):\n        infer_outputs_i, _, lengths_i = decoder(\n            decoding_strategy=decoding_strategy,\n            initial_state=dcdr_states,\n            start_tokens=start_tokens,\n            end_token=target_eos_token_id,\n            embedding=embedder,\n            max_decoding_length=50)\n        infer_samples.append(\n            tf.expand_dims(infer_outputs_i.sample_id, axis=2))\n        lengths.append(tf.expand_dims(lengths_i, axis=1))\n\n    infer_samples = tx.utils.pad_and_concat(\n        infer_samples, axis=2, pad_axis=1)\n    rand_sample_text = train_data.vocab(0).map_ids_to_tokens(infer_samples)\n    rand_lengths = tf.concat(lengths, axis=1)\n\n    # Beam search decoding\n    beam_search_samples, beam_states, _ = tx.modules.beam_search_decode(\n        decoder,\n        initial_state=dcdr_states,\n        start_tokens=start_tokens,\n        end_token=target_eos_token_id,\n        embedding=embedder,\n        beam_width=config_model.beam_width,\n        max_decoding_length=50)\n\n    beam_sample_text = train_data.vocab(0).map_ids_to_tokens(\n        beam_search_samples.predicted_ids)\n    beam_lengths = beam_states.lengths\n\n    # Running procedures\n\n    def _train_epoch(sess, epoch, display=1000):\n        iterator.switch_to_train_data(sess)\n\n        while True:\n            try:\n                feed = {tx.global_mode(): tf.estimator.ModeKeys.TRAIN}\n                step, loss, _ = sess.run(\n                    [global_step, mle_loss, train_op], feed_dict=feed)\n\n                if step % display == 0:\n                    print(\'step {} at epoch {}: loss={}\'.format(\n                        step, epoch, loss))\n\n            except tf.errors.OutOfRangeError:\n                break\n\n        print(\'epoch {} train: loss={}\'.format(epoch, loss))\n\n    def _test_epoch_ppl(sess, epoch):\n        iterator.switch_to_test_data(sess)\n\n        pples = []\n        while True:\n            try:\n                feed = {tx.global_mode(): tf.estimator.ModeKeys.EVAL}\n                ppl = sess.run(perplexity, feed_dict=feed)\n                pples.append(ppl)\n\n            except tf.errors.OutOfRangeError:\n                avg_ppl = np.mean(pples)\n                print(\'epoch {} perplexity={}\'.format(epoch, avg_ppl))\n                break\n\n    def _test_epoch_bleu(sess, epoch, sample_text, sample_lengths):\n        iterator.switch_to_test_data(sess)\n\n        bleu_prec = [[] for i in range(1, 5)]\n        bleu_recall = [[] for i in range(1, 5)]\n\n        def _bleus(ref, sample):\n            res = []\n            for weight in [[1, 0, 0, 0],\n                           [1, 0, 0, 0],\n                           [1 / 2., 1 / 2., 0, 0],\n                           [1 / 3., 1 / 3., 1 / 3., 0],\n                           [1 / 4., 1 / 4., 1 / 4., 1 / 4.]]:\n                res.append(sentence_bleu(\n                    [ref],\n                    sample,\n                    smoothing_function=SmoothingFunction().method7,\n                    weights=weight))\n            return res\n\n        while True:\n            try:\n                feed = {tx.global_mode(): tf.estimator.ModeKeys.EVAL}\n\n                samples_, sample_lengths_, references, refs_cnt = \\\n                    sess.run([sample_text,\n                              sample_lengths,\n                              data_batch[\'refs_text\'][:, :, 1:],\n                              data_batch[\'refs_utterance_cnt\']],\n                             feed_dict=feed)\n\n                samples_ = np.transpose(samples_, (0, 2, 1))\n                samples_ = [\n                    [sample[:l] for sample, l in zip(beam, lens)]\n                    for beam, lens in zip(samples_.tolist(), sample_lengths_)\n                ]\n                references = [\n                    [ref[:ref.index(b\'<EOS>\')] for ref in refs[:cnt]]\n                    for refs, cnt in zip(references.tolist(), refs_cnt)\n                ]\n\n                for beam, refs in zip(samples_, references):\n                    bleu_scores = [\n                        [_bleus(ref, sample) for ref in refs]\n                        for sample in beam\n                    ]\n                    bleu_scores = np.transpose(np.array(bleu_scores), (2, 0, 1))\n\n                    for i in range(1, 5):\n                        bleu_i = bleu_scores[i]\n                        bleu_i_precision = bleu_i.max(axis=1).mean()\n                        bleu_i_recall = bleu_i.max(axis=0).mean()\n\n                        bleu_prec[i - 1].append(bleu_i_precision)\n                        bleu_recall[i - 1].append(bleu_i_recall)\n\n            except tf.errors.OutOfRangeError:\n                break\n\n        bleu_prec = [np.mean(x) for x in bleu_prec]\n        bleu_recall = [np.mean(x) for x in bleu_recall]\n\n        print(\'epoch {}:\'.format(epoch))\n        for i in range(1, 5):\n            print(\' -- bleu-{} prec={}, recall={}\'.format(\n                i, bleu_prec[i - 1], bleu_recall[i - 1]))\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        num_epochs = 10\n        for epoch in range(1, num_epochs + 1):\n            _train_epoch(sess, epoch)\n            _test_epoch_ppl(sess, epoch)\n\n            if epoch % 5 == 0:\n                print(\'random sample: \')\n                _test_epoch_bleu(sess, epoch, rand_sample_text, rand_lengths)\n                print(\'beam-search: \')\n                _test_epoch_bleu(sess, epoch, beam_sample_text, beam_lengths)\n\n        if num_epochs % 5 != 0:\n            print(\'random sample: \')\n            _test_epoch_bleu(sess, num_epochs, rand_sample_text, rand_lengths)\n            print(\'beam-search: \')\n            _test_epoch_bleu(sess, num_epochs, beam_sample_text, beam_lengths)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/hierarchical_dialog/sw_loader.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" loader for switch board dataset.\n""""""\nimport os\nimport json\nfrom json_lines import reader\n\nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport texar.tf as tx\n\nfrom config_data import data_root\n\n# pylint: disable=invalid-name, too-many-locals\n\nwnd_sz = 10\n\n\nclass Dataset(object):\n    """"""Data preprocessor.\n    """"""\n\n    def __init__(self, jsonl_path, mode=None):\n        self.mode = mode\n        self.raw = []\n        self.lst = []\n        self.refs = []\n        if mode == \'test\':\n            lst = json.load(open(jsonl_path, \'r\'))\n            for item in lst:\n                context = item[\'context\']\n                dialog = []\n                for utts in context:\n                    p = utts.find(\':\')\n                    dialog.append((\n                        (utts[p - 1] == \'A\') * 2 - 1, utts[p + 2:-1], 0))\n\n                if dialog[0][1][-1] == \'>\':\n                    dialog = dialog[1:]\n\n                if len(dialog) == 0:\n                    continue\n\n                responses = []\n                for resp in item[\'responses\']:\n                    responses.append(resp)\n\n                spk = (item[\'speaker\'] == \'A\') * 2 - 1\n                dialog.append((spk, responses[0], 0))\n                responses = responses[1:]\n                responses = [\' \'.join(WordPunctTokenizer().tokenize(resp))\n                             for resp in responses]\n\n                if len(responses) == 0:\n                    continue\n\n                self.raw.append(dialog)\n                self.lst.append((len(self.raw) - 1, 0, len(dialog)))\n                self.refs.append(responses)\n\n            return\n\n        from collections import Counter\n        self.ct = Counter()\n        self.topics = []\n        with open(jsonl_path, \'r\') as f:\n            for idx, item in enumerate(reader(f)):\n                utts = item[\'utts\']\n                self.topics.append(item[\'topic\'])\n                self.raw.append([(int(speaker == \'A\') * 2 - 1, sentence, _)\n                                 for speaker, sentence, _ in utts])\n\n                lst = [(idx, start, start + wnd_sz)\n                       for start in range(0, len(utts) - wnd_sz)] + \\\n                      [(idx, 0, end)\n                       for end in range(2, min(wnd_sz + 1, len(utts)))]\n\n                self.lst += lst\n\n        self.refs = [[\'none\']] * len(self.lst)\n\n    def __len__(self):\n        return len(self.lst)\n\n    def __getitem__(self, idx):\n        idx, start, end = self.lst[idx]\n        dialog = self.raw[idx][start:end]\n        source, target = dialog[:-1], dialog[-1]\n\n        spks, utts = list(zip(*[(speaker, WordPunctTokenizer().tokenize(uttr)) for speaker, uttr, _ in source]))\n\n        spks = list(spks)\n\n        while len(spks) < 10:\n            spks.append(0)\n\n        source = \'|||\'.join([\' \'.join(uttr) for uttr in utts])\n        target_test = \' \'.join(WordPunctTokenizer().tokenize(target[1]))\n\n        return spks, source, target_test, target[0]\n\n    def get(self, idx):\n        idx, start, end = self.lst[idx]\n        source = self.raw[idx][start:end - 1]\n        target = self.raw[idx][end - 1]\n        source = \' \'.join([b for a, b, c in source])\n        cct = self.raw[idx][end - 2][0] == self.raw[idx][end - 1][0]\n        return self.topics[idx], cct, source, target\n\n\ndef sw1c2r(data_root):\n    dts_train = Dataset(os.path.join(data_root, \'train.jsonl\'))\n    dts_valid = Dataset(os.path.join(data_root, \'valid.jsonl\'))\n    dts_test = Dataset(os.path.join(data_root, \'test_multi_ref.json\'), \'test\')\n    datasets = {\n        \'train\': dts_train,\n        \'val\': dts_valid,\n        \'test\': dts_test\n    }\n    return datasets\n\n\ndef generate_reference_for_test_dialog(dataset, data_root):\n    vocab = {}\n    with open(os.path.join(data_root, \'vocab.txt\'), \'r\') as f:\n        p = f.read().splitlines()\n        for i, x in enumerate(p):\n            vocab[x] = i\n\n    dts_train = dataset[\'train\']\n    dts_val = dataset[\'val\']\n    dts_test = dataset[\'test\']\n\n    vectorizer = TfidfVectorizer(tokenizer=WordPunctTokenizer().tokenize,\n                                 vocabulary=vocab)\n\n    saved = []\n    meta = []\n    data = []\n    tidx = {}\n    for i in range(len(dts_test)):\n        topic, cct, source, target = dts_test.get(i)\n        meta.append((topic, cct, target))\n        data.append(source)\n\n    for i in range(len(dts_train)):\n        topic, cct, source, target = dts_train.get(i)\n        saved.append((topic, cct, target))\n        data.append(source)\n\n        if topic not in tidx:\n            tidx[topic] = []\n        tidx[topic].append(i)\n\n    result = vectorizer.fit_transform(data)\n    x = result[:len(dts_test)]\n    y = result[len(dts_test):]\n\n    from tqdm import tqdm\n    from sklearn.preprocessing import normalize\n\n    y = normalize(y)\n    x = normalize(x)\n\n    dts_test.refs = []\n    for i in tqdm(range(len(dts_test))):\n        c = tidx[meta[i][0]]\n        p = (y * x[i].T).toarray().reshape(-1)[c]\n        d = p.argsort()\n\n        cnt = 0\n        refs = []\n        for a in d[::-1]:\n            if saved[a][1] == meta[i][1]:\n                refs.append(\' \'.join(\n                    WordPunctTokenizer().tokenize(saved[a][2][1])))\n                cnt += 1\n                if cnt == 10:\n                    break\n\n        dts_test.refs.append(refs)\n\n\ndef download_and_process(data_root):\n    if not os.path.isdir(data_root):\n        os.makedirs(data_root)\n        os.makedirs(os.path.join(data_root, \'raw\'))\n\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/1Gytd-SSetUkIY6aVVKNrBOxkHjAlSGeU/view?usp=sharing\',\n            path=\'./\',\n            filenames=os.path.join(data_root, \'sw1c2r.tar.gz\'),\n            extract=True)\n\n        os.system(\'mv {} {}\'.format(os.path.join(data_root, \'sw1c2r.tar.gz\'),\n                                    os.path.join(data_root, \'raw/sw1c2r.tar.gz\')))\n        os.system(\'mv {}/* {}\'.format(\n            os.path.join(data_root, \'switchboard\'), data_root))\n\n        datasets = sw1c2r(os.path.join(data_root, \'json_data\'))\n\n        for stage in [\'train\', \'val\', \'test\']:\n            dts = datasets[stage]\n            spk, src, tgt, meta = list(zip(*[dts[i] for i in range(len(dts))]))\n            src_txt = \'\\n\'.join(src)\n            tgt_txt = \'\\n\'.join(tgt)\n\n            spk = list(zip(*spk))\n\n            for i in range(len(spk)):\n                with open(os.path.join(data_root, \'{}-source-spk-{}.txt\'.format(stage, i)), \'w\') as f:\n                    f.write(\'\\n\'.join([str(a) for a in spk[i]]))\n\n            spk_tgt = meta\n\n            with open(os.path.join(data_root, \'{}-target-spk.txt\'.format(stage)), \'w\') as f:\n                f.write(\'\\n\'.join([str(a) for a in spk_tgt]))\n\n            with open(os.path.join(data_root, \'{}-source.txt\'.format(stage)), \'w\') as f:\n                f.write(src_txt)\n            with open(os.path.join(data_root, \'{}-target.txt\'.format(stage)), \'w\') as f:\n                f.write(tgt_txt)\n\n            with open(os.path.join(data_root, \'{}-target-refs.txt\'.format(stage)), \'w\') as f:\n                f.write(\'\\n\'.join([\'|||\'.join(v) for v in dts.refs]))\n\n\nif __name__ == \'__main__\':\n    download_and_process(data_root)\n'"
examples/language_model_ptb/config_large.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM large size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.04\nnum_epochs = 55\nhidden_size = 1500\nkeep_prob = 0.35\nbatch_size = 20\nnum_steps = 35\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 10.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 1. / 1.15,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 14\n    }\n}\n'"
examples/language_model_ptb/config_medium.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM medium size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.05\nnum_epochs = 39\nhidden_size = 650\nkeep_prob = 0.5\nbatch_size = 20\nnum_steps = 35\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.8,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 5\n    }\n}\n'"
examples/language_model_ptb/config_small.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM small size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.1\nnum_epochs = 13\nhidden_size = 200\nkeep_prob = 1.0\nbatch_size = 20\nnum_steps = 20\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.5,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 3\n    }\n}\n'"
examples/language_model_ptb/lm_ptb.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building the language model.\n\nThis is a reimpmentation of the TensorFlow official PTB example in:\ntensorflow/models/rnn/ptb\n\nModel and training are described in:\n(Zaremba, et. al.) Recurrent Neural Network Regularization\n http://arxiv.org/abs/1409.2329\n\nThere are 3 provided model configurations:\n===========================================\n| config | epochs | train | valid  | test\n===========================================\n| small  | 13     | 37.99 | 121.39 | 115.91\n| medium | 39     | 48.45 |  86.16 |  82.07\n| large  | 55     | 37.87 |  82.62 |  78.29\nThe exact results may vary depending on the random initialization.\n\nThe data required for this example is in the `data/` dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\nIf data is not provided, the program will download from above automatically.\n\nTo run:\n\n$ python lm_ptb.py --data_path=simple-examples/data --config=config_small\n""""""\n\n# pylint: disable=invalid-name, no-member, too-many-locals\n\nimport time\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom ptb_reader import prepare_data, ptb_iterator\n\nflags = tf.flags\n\nflags.DEFINE_string(""data_path"", ""./"",\n                    ""Directory containing PTB raw data (e.g., ptb.train.txt). ""\n                    ""E.g., ./simple-examples/data. If not exists, ""\n                    ""the directory will be created and PTB raw data will ""\n                    ""be downloaded."")\nflags.DEFINE_string(""config"", ""config_small"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    # Data\n    batch_size = config.batch_size\n    num_steps = config.num_steps\n    data = prepare_data(FLAGS.data_path)\n    vocab_size = data[""vocab_size""]\n\n    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n\n    # Model architecture\n    initializer = tf.random_uniform_initializer(\n        -config.init_scale, config.init_scale)\n    with tf.variable_scope(""model"", initializer=initializer):\n        embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams=config.emb)\n        emb_inputs = embedder(inputs)\n        if config.keep_prob < 1:\n            emb_inputs = tf.nn.dropout(\n                emb_inputs, tx.utils.switch_dropout(config.keep_prob))\n\n        decoder = tx.modules.BasicRNNDecoder(\n            vocab_size=vocab_size, hparams={""rnn_cell"": config.cell})\n        initial_state = decoder.zero_state(batch_size, tf.float32)\n        outputs, final_state, seq_lengths = decoder(\n            decoding_strategy=""train_greedy"",\n            impute_finished=True,\n            inputs=emb_inputs,\n            sequence_length=[num_steps] * batch_size,\n            initial_state=initial_state)\n\n    # Losses & train ops\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=targets,\n        logits=outputs.logits,\n        sequence_length=seq_lengths)\n\n    # Use global_step to pass epoch, for lr decay\n    global_step = tf.placeholder(tf.int32)\n    train_op = tx.core.get_train_op(\n        mle_loss, global_step=global_step, increment_global_step=False,\n        hparams=config.opt)\n\n    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):\n        start_time = time.time()\n        loss = 0.\n        iters = 0\n        state = sess.run(initial_state)\n\n        fetches = {\n            ""mle_loss"": mle_loss,\n            ""final_state"": final_state,\n        }\n        if is_train:\n            fetches[""train_op""] = train_op\n            epoch_size = (len(data[""train_text_id""]) // batch_size - 1)\\\n                // num_steps\n\n        mode = (tf.estimator.ModeKeys.TRAIN\n                if is_train\n                else tf.estimator.ModeKeys.EVAL)\n\n        for step, (x, y) in enumerate(data_iter):\n            feed_dict = {\n                inputs: x, targets: y, global_step: epoch,\n                tx.global_mode(): mode,\n            }\n            for i, (c, h) in enumerate(initial_state):\n                feed_dict[c] = state[i].c\n                feed_dict[h] = state[i].h\n\n            rets = sess.run(fetches, feed_dict)\n            loss += rets[""mle_loss""]\n            state = rets[""final_state""]\n            iters += num_steps\n\n            ppl = np.exp(loss / iters)\n            if verbose and is_train and step % (epoch_size // 10) == 10:\n                print(""%.3f perplexity: %.3f speed: %.0f wps"" %\n                      ((step + 1) * 1.0 / epoch_size, ppl,\n                       iters * batch_size / (time.time() - start_time)))\n\n        ppl = np.exp(loss / iters)\n        return ppl\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        for epoch in range(config.num_epochs):\n            # Train\n            train_data_iter = ptb_iterator(\n                data[""train_text_id""], config.batch_size, num_steps)\n            train_ppl = _run_epoch(\n                sess, train_data_iter, epoch, is_train=True, verbose=True)\n            print(""Epoch: %d Train Perplexity: %.3f"" % (epoch, train_ppl))\n            # Valid\n            valid_data_iter = ptb_iterator(\n                data[""valid_text_id""], config.batch_size, num_steps)\n            valid_ppl = _run_epoch(sess, valid_data_iter, epoch)\n            print(""Epoch: %d Valid Perplexity: %.3f"" % (epoch, valid_ppl))\n        # Test\n        test_data_iter = ptb_iterator(\n            data[""test_text_id""], batch_size, num_steps)\n        test_ppl = _run_epoch(sess, test_data_iter, 0)\n        print(""Test Perplexity: %.3f"" % (test_ppl))\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/language_model_ptb/ptb_reader.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for preprocessing and iterating over the PTB data.\n""""""\n\n# pylint: disable=invalid-name, too-many-locals\n\nimport os\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n\ndef ptb_iterator(data, batch_size, num_steps):\n    """"""Iterates through the ptb data.\n    """"""\n    data_length = len(data)\n    batch_length = data_length // batch_size\n\n    data = np.asarray(data[:batch_size * batch_length])\n    data = data.reshape([batch_size, batch_length])\n\n    epoch_size = (batch_length - 1) // num_steps\n    if epoch_size == 0:\n        raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps: (i + 1) * num_steps]\n        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n        yield (x, y)\n\n\ndef prepare_data(data_path):\n    """"""Preprocess PTB data.\n    """"""\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    if not tf.gfile.Exists(train_path):\n        url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\'\n        tx.data.maybe_download(url, data_path, extract=True)\n        data_path = os.path.join(data_path, \'simple-examples\', \'data\')\n\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n    test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n    word_to_id = tx.data.make_vocab(\n        train_path, newline_token=""<EOS>"", return_type=""dict"")\n    assert len(word_to_id) == 10000\n\n    train_text = tx.data.read_words(\n        train_path, newline_token=""<EOS>"")\n    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]\n\n    valid_text = tx.data.read_words(\n        valid_path, newline_token=""<EOS>"")\n    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]\n\n    test_text = tx.data.read_words(\n        test_path, newline_token=""<EOS>"")\n    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]\n\n    data = {\n        ""train_text"": train_text,\n        ""valid_text"": valid_text,\n        ""test_text"": test_text,\n        ""train_text_id"": train_text_id,\n        ""valid_text_id"": valid_text_id,\n        ""test_text_id"": test_text_id,\n        ""vocab"": word_to_id,\n        ""vocab_size"": len(word_to_id)\n    }\n    return data\n'"
examples/memory_network_lm/config.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\nn_hops = 7\ndim = 150\nrelu_dim = dim // 2\nbatch_size = 128\nnum_epochs = 200\nmemory_size = 200\ninitialize_stddev = 0.05\nquery_constant = 0.1\nlearning_rate_anneal_factor = 1.5\nterminating_learning_rate = 1e-5\n\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 0.01}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 50.}\n    },\n}\n\nembed = {\n    ""embedding"": {\n        ""dim"": dim,\n    },\n    ""temporal_embedding"": {\n        ""dim"": dim,\n    }\n}\n\nmemnet = {\n    ""n_hops"": n_hops,\n    ""relu_dim"": relu_dim,\n    ""memory_size"": memory_size,\n    ""A"": embed,\n    ""C"": embed,\n}\n'"
examples/memory_network_lm/lm_ptb_memnet.py,19,"b'#!/usr/bin/env python3\n# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building the PTB language model with Memory Network.\n\nMemory Network model is described in https://arxiv.org/abs/1503.08895v4\n\nThe data required for this example is in the `data/` dir of the\nPTB dataset from Tomas Mikolov\'s webpage:\n\n$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n$ tar xvf simple-examples.tgz\n\nIf data is now provided, the program will download from above automatically.\n\nTo run:\n\n$ python lm_ptb_memnet.py --data_path=simple-examples/data \\\n  --config=config\n\nThis code will automatically save and restore from directory `ckpt/`.\nIf the directory doesn\'t exist, it will be created automatically.\n""""""\n\n# pylint: disable=invalid-name, no-member, too-many-locals\n\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom ptb_reader import prepare_data\nfrom ptb_reader import ptb_iterator_memnet as ptb_iterator\n\nflags = tf.flags\n\nflags.DEFINE_string(""data_path"", ""./"",\n                    ""Directory containing PTB raw data (e.g., ptb.train.txt). ""\n                    ""E.g., ./simple-examples/data. If not exists, ""\n                    ""the directory will be created and PTB raw data will ""\n                    ""be downloaded."")\nflags.DEFINE_string(""config"", ""config"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    # Data\n    batch_size = config.batch_size\n    memory_size = config.memory_size\n    terminating_learning_rate = config.terminating_learning_rate\n    data = prepare_data(FLAGS.data_path)\n    vocab_size = data[""vocab_size""]\n    print(\'vocab_size = {}\'.format(vocab_size))\n\n    inputs = tf.placeholder(tf.int32, [None, memory_size], name=""inputs"")\n    targets = tf.placeholder(tf.int32, [None], name=""targets"")\n\n    # Model architecture\n    initializer = tf.random_normal_initializer(\n        stddev=config.initialize_stddev)\n    with tf.variable_scope(""model"", initializer=initializer):\n        memnet = tx.modules.MemNetRNNLike(raw_memory_dim=vocab_size,\n                                          hparams=config.memnet)\n        queries = tf.fill([tf.shape(inputs)[0], config.dim],\n                          config.query_constant)\n        logits = memnet(inputs, queries)\n\n    # Losses & train ops\n    mle_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=targets, logits=logits)\n    mle_loss = tf.reduce_sum(mle_loss)\n\n    # Use global_step to pass epoch, for lr decay\n    lr = config.opt[""optimizer""][""kwargs""][""learning_rate""]\n    learning_rate = tf.placeholder(tf.float32, [], name=""learning_rate"")\n    global_step = tf.Variable(0, dtype=tf.int32, name=""global_step"")\n    increment_global_step = tf.assign_add(global_step, 1)\n    train_op = tx.core.get_train_op(\n        mle_loss,\n        learning_rate=learning_rate,\n        global_step=global_step,\n        increment_global_step=False,\n        hparams=config.opt)\n\n    def _run_epoch(sess, data_iter, epoch, is_train=False):\n        loss = 0.\n        iters = 0\n\n        fetches = {\n            ""mle_loss"": mle_loss\n        }\n        if is_train:\n            fetches[""train_op""] = train_op\n\n        mode = (tf.estimator.ModeKeys.TRAIN\n                if is_train\n                else tf.estimator.ModeKeys.EVAL)\n\n        for _, (x, y) in enumerate(data_iter):\n            batch_size = x.shape[0]\n            feed_dict = {\n                inputs: x, targets: y, learning_rate: lr,\n                tx.global_mode(): mode,\n            }\n\n            rets = sess.run(fetches, feed_dict)\n            loss += rets[""mle_loss""]\n            iters += batch_size\n\n        ppl = np.exp(loss / iters)\n        return ppl\n\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        try:\n            saver.restore(sess, ""ckpt/model.ckpt"")\n            print(\'restored checkpoint.\')\n        except BaseException:\n            print(\'restore checkpoint failed.\')\n\n        last_valid_ppl = None\n        heuristic_lr_decay = (hasattr(config, \'heuristic_lr_decay\')\n                              and config.heuristic_lr_decay)\n        while True:\n            if lr < terminating_learning_rate:\n                break\n\n            epoch = sess.run(global_step)\n            if epoch >= config.num_epochs:\n                print(\'Too many epochs!\')\n                break\n\n            print(\'epoch: {} learning_rate: {:.6f}\'.format(epoch, lr))\n\n            # Train\n            train_data_iter = ptb_iterator(\n                data[""train_text_id""], batch_size, memory_size)\n            train_ppl = _run_epoch(\n                sess, train_data_iter, epoch, is_train=True)\n            print(""Train Perplexity: {:.3f}"".format(train_ppl))\n            sess.run(increment_global_step)\n\n            # checkpoint\n            if epoch % 5 == 0:\n                try:\n                    saver.save(sess, ""ckpt/model.ckpt"")\n                    print(""saved checkpoint."")\n                except BaseException:\n                    print(""save checkpoint failed."")\n\n            # Valid\n            valid_data_iter = ptb_iterator(\n                data[""valid_text_id""], batch_size, memory_size)\n            valid_ppl = _run_epoch(sess, valid_data_iter, epoch)\n            print(""Valid Perplexity: {:.3f}"".format(valid_ppl))\n\n            # Learning rate decay\n            if last_valid_ppl:\n                if heuristic_lr_decay:\n                    if valid_ppl > last_valid_ppl * config.heuristic_threshold:\n                        lr /= 1. + (valid_ppl / last_valid_ppl\n                                    - config.heuristic_threshold) \\\n                              * config.heuristic_rate\n                    last_valid_ppl = last_valid_ppl \\\n                                     * (1 - config.heuristic_smooth_rate) \\\n                                     + valid_ppl * config.heuristic_smooth_rate\n                else:\n                    if valid_ppl > last_valid_ppl:\n                        lr /= config.learning_rate_anneal_factor\n                    last_valid_ppl = valid_ppl\n            else:\n                last_valid_ppl = valid_ppl\n            print(""last_valid_ppl: {:.6f}"".format(last_valid_ppl))\n\n        epoch = sess.run(global_step)\n        print(\'Terminate after epoch \', epoch)\n\n        # Test\n        test_data_iter = ptb_iterator(data[""test_text_id""], 1, memory_size)\n        test_ppl = _run_epoch(sess, test_data_iter, 0)\n        print(""Test Perplexity: {:.3f}"".format(test_ppl))\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/memory_network_lm/ptb_reader.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for preprocessing and iterating over the PTB data.\n""""""\n\n# pylint: disable=invalid-name, too-many-locals\n\nimport os\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n\ndef ptb_iterator(data, batch_size, num_steps):\n    """"""Iterates through the ptb data.\n    """"""\n    data_length = len(data)\n    batch_length = data_length // batch_size\n\n    data = np.asarray(data[:batch_size * batch_length])\n    data = data.reshape([batch_size, batch_length])\n\n    epoch_size = (batch_length - 1) // num_steps\n    if epoch_size == 0:\n        raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps: (i + 1) * num_steps]\n        y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n        yield (x, y)\n\n\ndef ptb_iterator_memnet(data, batch_size, memory_size):\n    """"""Iterates through the ptb data.\n    """"""\n    data_length = len(data)\n    length = data_length - memory_size\n    order = list(range(length))\n    np.random.shuffle(order)\n\n    data = np.asarray(data)\n\n    for i in range(0, length, batch_size):\n        x, y = [], []\n        for j in range(i, min(i + batch_size, length)):\n            idx = order[j]\n            x.append(data[idx: idx + memory_size])\n            y.append(data[idx + memory_size])\n        x, y = np.asarray(x), np.asarray(y)\n        yield (x, y)\n\n\ndef prepare_data(data_path):\n    """"""Preprocess PTB data.\n    """"""\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    if not tf.gfile.Exists(train_path):\n        url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\'\n        tx.data.maybe_download(url, data_path, extract=True)\n        data_path = os.path.join(data_path, \'simple-examples\', \'data\')\n\n    train_path = os.path.join(data_path, ""ptb.train.txt"")\n    valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n    test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n    word_to_id = tx.data.make_vocab(\n        train_path, newline_token=""<EOS>"", return_type=""dict"")\n    assert len(word_to_id) == 10000\n\n    train_text = tx.data.read_words(\n        train_path, newline_token=""<EOS>"")\n    train_text_id = [word_to_id[w] for w in train_text if w in word_to_id]\n\n    valid_text = tx.data.read_words(\n        valid_path, newline_token=""<EOS>"")\n    valid_text_id = [word_to_id[w] for w in valid_text if w in word_to_id]\n\n    test_text = tx.data.read_words(\n        test_path, newline_token=""<EOS>"")\n    test_text_id = [word_to_id[w] for w in test_text if w in word_to_id]\n\n    data = {\n        ""train_text"": train_text,\n        ""valid_text"": valid_text,\n        ""test_text"": test_text,\n        ""train_text_id"": train_text_id,\n        ""valid_text_id"": valid_text_id,\n        ""test_text_id"": test_text_id,\n        ""vocab"": word_to_id,\n        ""vocab_size"": len(word_to_id)\n    }\n    return data\n'"
examples/rl_gym/ac_cartpole.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPolicy gradient for the CartPole game in OpenAI gym.\n""""""\n\n# pylint: disable=invalid-name\n\nimport importlib\nimport gym\nimport tensorflow as tf\nimport texar.tf as tx\n\nflags = tf.flags\n\nflags.DEFINE_string(""config"", ""config"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\nif __name__ == \'__main__\':\n    env = gym.make(\'CartPole-v0\')\n    env = env.unwrapped\n\n    env_config = tx.agents.get_gym_env_config(env)\n\n    agent = tx.agents.ActorCriticAgent(env_config=env_config)\n    with tf.Session() as sess:\n        agent.sess = sess\n\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.TRAIN}\n\n        for e in range(5000):\n            reward_sum = 0.\n            observ = env.reset()\n            agent.reset()\n            while True:\n                action = agent.get_action(observ, feed_dict=feed_dict)\n\n                next_observ, reward, terminal, _ = env.step(action=action)\n                agent.observe(reward, terminal, feed_dict=feed_dict)\n                observ = next_observ\n\n                reward_sum += reward\n                if terminal:\n                    break\n\n            if (e + 1) % 10 == 0:\n                print(\'episode {}: {}\'.format(e + 1, reward_sum))\n'"
examples/rl_gym/config.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nCartpole config.\n""""""\n\n# pylint: disable=invalid-name\n\npolicy_hparams = None  # Use default hyperparameters\n\npg_agent_hparams = {\n    ""policy_hparams"": policy_hparams,\n    ""normalize_reward"": True\n}\n'"
examples/rl_gym/dqn_cartpole.py,7,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPolicy gradient for the CartPole game in OpenAI gym.\n""""""\n\n# pylint: disable=invalid-name\n\nimport importlib\nimport gym\nimport tensorflow as tf\nimport texar.tf as tx\nfrom texar.tf.agents import PGAgent\n\n\nflags = tf.flags\n\nflags.DEFINE_string(""config"", ""config"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\nif __name__ == \'__main__\':\n    env = gym.make(\'CartPole-v0\')\n    env = env.unwrapped\n\n    env_config = tx.agents.get_gym_env_config(env)\n\n    with tf.Session() as sess:\n        agent = tx.agents.DQNAgent(sess=sess, env_config=env_config)\n\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.TRAIN}\n\n        for e in range(500):\n            reward_sum = 0.\n            observ = env.reset()\n            agent.reset()\n            while True:\n                action = agent.get_action(observ, feed_dict=feed_dict)\n\n                next_observ, reward, terminal, _ = env.step(action=action)\n                agent.observe(reward, terminal, feed_dict=feed_dict)\n                observ = next_observ\n\n                reward_sum += reward\n                if terminal:\n                    break\n\n            if (e + 1) % 10 == 0:\n                print(\'episode {}: {}\'.format(e + 1, reward_sum))\n'"
examples/rl_gym/pg_cartpole.py,8,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPolicy gradient for the CartPole game in OpenAI gym.\n""""""\n\n# pylint: disable=invalid-name\n\nimport importlib\nimport gym\nimport tensorflow as tf\nimport texar.tf as tx\nfrom texar.tf.agents import PGAgent\n\nflags = tf.flags\n\nflags.DEFINE_string(""config"", ""config"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    env = gym.make(\'CartPole-v0\')\n    env = env.unwrapped\n\n    env_config = tx.agents.get_gym_env_config(env)\n    agent = PGAgent(\n        env_config,\n        policy_kwargs={\'action_space\': env_config.action_space},\n        hparams=config.pg_agent_hparams)\n\n    sess = tf.Session()\n    agent.sess = sess\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.tables_initializer())\n    feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.TRAIN}\n\n    for e in range(300):\n        reward_sum = 0.\n        observ = env.reset()\n        agent.reset()\n        while True:\n            action = agent.get_action(observ, feed_dict=feed_dict)\n\n            next_observ, reward, terminal, _ = env.step(action=action)\n            if terminal:\n                reward = 0.\n            agent.observe(reward, terminal, feed_dict=feed_dict)\n            observ = next_observ\n\n            reward_sum += reward\n            if terminal:\n                break\n\n        if (e + 1) % 10 == 0:\n            print(\'episode {}: {}\'.format(e + 1, reward_sum))\n\n    sess.close()\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/sentence_classifier/clas_main.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building a sentence convolutional classifier.\n\nUse `./sst_data_preprocessor.py` to download and clean the SST binary data.\n\nTo run:\n\n$ python clas_main.py --config=config_kim\n""""""\n\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\n\n# pylint: disable=invalid-name, too-many-locals\n\nflags = tf.flags\n\nflags.DEFINE_string(""config"", ""config_kim"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    # Data\n    train_data = tx.data.MultiAlignedData(config.train_data)\n    val_data = tx.data.MultiAlignedData(config.val_data)\n    test_data = tx.data.MultiAlignedData(config.test_data)\n    iterator = tx.data.TrainTestDataIterator(train_data, val_data, test_data)\n    batch = iterator.get_next()\n\n    # Model architecture\n    embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.vocab(\'x\').size, hparams=config.emb)\n    classifier = tx.modules.Conv1DClassifier(config.clas)\n    logits, pred = classifier(embedder(batch[\'x_text_ids\']))\n\n    # Losses & train ops\n    loss = tf.losses.sparse_softmax_cross_entropy(\n        labels=batch[\'y\'], logits=logits)\n    accu = tx.evals.accuracy(batch[\'y\'], pred)\n\n    train_op = tx.core.get_train_op(loss, hparams=config.opt)\n\n    def _run_epoch(sess, mode, epoch=0, verbose=False):\n        is_train = tx.utils.is_train_mode_py(mode)\n\n        fetches = {\n            ""accu"": accu,\n            ""batch_size"": tx.utils.get_batch_size(batch[\'y\'])\n        }\n        if is_train:\n            fetches[""train_op""] = train_op\n        feed_dict = {tx.context.global_mode(): mode}\n\n        cum_accu = 0.\n        nsamples = 0\n        step = 0\n        while True:\n            try:\n                rets = sess.run(fetches, feed_dict)\n                step += 1\n\n                accu_ = rets[\'accu\']\n                cum_accu += accu_ * rets[\'batch_size\']\n                nsamples += rets[\'batch_size\']\n\n                if verbose and (step == 1 or step % 100 == 0):\n                    tf.logging.info(\n                        ""epoch: {0:2} step: {1:4} accu: {2:.4f}""\n                        .format(epoch, step, accu_))\n            except tf.errors.OutOfRangeError:\n                break\n        return cum_accu / nsamples\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        best_val_accu = -1.\n        for epoch in range(config.num_epochs):\n            # Train\n            iterator.switch_to_train_data(sess)\n            train_accu = _run_epoch(sess, tf.estimator.ModeKeys.TRAIN, epoch)\n            # Val\n            iterator.switch_to_val_data(sess)\n            val_accu = _run_epoch(sess, tf.estimator.ModeKeys.EVAL, epoch)\n            tf.logging.info(\'epoch: {0:2} train accu: {1:.4f} val accu: {2:.4f}\'\n                            .format(epoch + 1, train_accu, val_accu))\n            # Test\n            if val_accu > best_val_accu:\n                best_val_accu = val_accu\n\n                iterator.switch_to_test_data(sess)\n                test_accu = _run_epoch(sess, tf.estimator.ModeKeys.EVAL)\n                tf.logging.info(\'test accu: {0:.4f}\'.format(test_accu))\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main=_main)\n'"
examples/sentence_classifier/config_kim.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Sentence convolutional classifier config.\n\nThis is (approximately) the config of the paper:\n(Kim) Convolutional Neural Networks for Sentence Classification\n  https://arxiv.org/pdf/1408.5882.pdf\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\nimport copy\n\nnum_epochs = 15\n\ntrain_data = {\n    ""batch_size"": 50,\n    ""datasets"": [\n        {\n            ""files"": ""./data/sst2.train.sentences.txt"",\n            ""vocab_file"": ""./data/sst2.vocab"",\n            # Discards samples with length > 56\n            ""max_seq_length"": 56,\n            ""length_filter_mode"": ""discard"",\n            # Do not append BOS/EOS tokens to the sentences\n            ""bos_token"": """",\n            ""eos_token"": """",\n            ""data_name"": ""x""\n        },\n        {\n            ""files"": ""./data/sst2.train.labels.txt"",\n            ""data_type"": ""int"",\n            ""data_name"": ""y""\n        }\n    ]\n}\n# The val and test data have the same config with the train data, except\n# for the file names\nval_data = copy.deepcopy(train_data)\nval_data[""datasets""][0][""files""] = ""./data/sst2.dev.sentences.txt""\nval_data[""datasets""][1][""files""] = ""./data/sst2.dev.labels.txt""\ntest_data = copy.deepcopy(train_data)\ntest_data[""datasets""][0][""files""] = ""./data/sst2.test.sentences.txt""\ntest_data[""datasets""][1][""files""] = ""./data/sst2.test.labels.txt""\n\n# Word embedding\nemb = {\n    ""dim"": 300\n}\n\n# Classifier\nclas = {\n    ""num_conv_layers"": 1,\n    ""filters"": 100,\n    ""kernel_size"": [3, 4, 5],\n    ""conv_activation"": ""relu"",\n    ""pooling"": ""MaxPooling1D"",\n    ""num_dense_layers"": 0,\n    ""dropout_conv"": [1],\n    ""dropout_rate"": 0.5,\n    ""num_classes"": 2\n}\n\n# Optimization\n# Just use the default config, e.g., Adam Optimizer\nopt = {}\n'"
examples/sentence_classifier/sst_data_preprocessor.py,5,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Preparing the SST2 dataset.\n""""""\n\nimport os\nimport re\nfrom io import open  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport texar.tf as tx\n\n# pylint: disable=invalid-name, too-many-locals\n\nflags = tf.flags\n\nflags.DEFINE_string(""data_path"", ""./data"",\n                    ""Directory containing SST data. ""\n                    ""E.g., ./data/sst2.train.sentences.txt. If not exists, ""\n                    ""the directory will be created and SST raw data will ""\n                    ""be downloaded."")\n\nFLAGS = flags.FLAGS\n\n\ndef clean_sst_text(text):\n    """"""Cleans tokens in the SST data, which has already been tokenized.\n    """"""\n    text = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", text)\n    text = re.sub(r""\\s{2,}"", "" "", text)\n    return text.strip().lower()\n\n\ndef transform_raw_sst(data_path, raw_fn, new_fn):\n    """"""Transforms the raw data format to a new format.\n    """"""\n    fout_x_name = os.path.join(data_path, new_fn + \'.sentences.txt\')\n    fout_x = open(fout_x_name, \'w\', encoding=\'utf-8\')\n    fout_y_name = os.path.join(data_path, new_fn + \'.labels.txt\')\n    fout_y = open(fout_y_name, \'w\', encoding=\'utf-8\')\n\n    fin_name = os.path.join(data_path, raw_fn)\n    with open(fin_name, \'r\', encoding=\'utf-8\') as fin:\n        for line in fin:\n            parts = line.strip().split()\n            label = parts[0]\n            sent = \' \'.join(parts[1:])\n            sent = clean_sst_text(sent)\n            fout_x.write(sent + \'\\n\')\n            fout_y.write(label + \'\\n\')\n\n    return fout_x_name, fout_y_name\n\n\ndef prepare_data(data_path):\n    """"""Preprocesses SST2 data.\n    """"""\n    train_path = os.path.join(data_path, ""sst.train.sentences.txt"")\n    if not tf.gfile.Exists(train_path):\n        url = (\'https://raw.githubusercontent.com/ZhitingHu/\'\n               \'logicnn/master/data/raw/\')\n        files = [\'stsa.binary.phrases.train\', \'stsa.binary.dev\',\n                 \'stsa.binary.test\']\n        for fn in files:\n            tx.data.maybe_download(url + fn, data_path, extract=True)\n\n    fn_train, _ = transform_raw_sst(\n        data_path, \'stsa.binary.phrases.train\', \'sst2.train\')\n    transform_raw_sst(data_path, \'stsa.binary.dev\', \'sst2.dev\')\n    transform_raw_sst(data_path, \'stsa.binary.test\', \'sst2.test\')\n\n    vocab = tx.data.make_vocab(fn_train)\n    fn_vocab = os.path.join(data_path, \'sst2.vocab\')\n    with open(fn_vocab, \'w\', encoding=\'utf-8\') as f_vocab:\n        for v in vocab:\n            f_vocab.write(v + \'\\n\')\n\n    tf.logging.info(\'Preprocessing done: {}\'.format(data_path))\n\n\ndef _main(_):\n    prepare_data(FLAGS.data_path)\n\n\nif __name__ == \'__main__\':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main=_main)\n'"
examples/seq2seq_attn/config_iwslt14.py,0,"b'\nnum_epochs = 15\ndisplay = 500\n\nsource_vocab_file = \'./data/iwslt14/vocab.de\'\ntarget_vocab_file = \'./data/iwslt14/vocab.en\'\n\ntrain = {\n    \'batch_size\': 32,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/train.de\',\n        \'vocab_file\': source_vocab_file,\n        \'max_seq_length\': 50\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/train.en\',\n        \'vocab_file\': target_vocab_file,\n        \'max_seq_length\': 50\n    }\n}\nval = {\n    \'batch_size\': 32,\n    \'shuffle\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/valid.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/valid.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\ntest = {\n    \'batch_size\': 32,\n    \'shuffle\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/test.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/test.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n'"
examples/seq2seq_attn/config_model.py,0,"b""# Attentional Seq2seq model.\n# Hyperparameters not specified here will take the default values.\n\nnum_units = 256\nbeam_width = 10\n\nembedder = {\n    'dim': num_units\n}\nencoder = {\n    'rnn_cell_fw': {\n        'kwargs': {\n            'num_units': num_units\n        }\n    }\n}\ndecoder = {\n    'rnn_cell': {\n        'kwargs': {\n            'num_units': num_units\n        },\n    },\n    'attention': {\n        'kwargs': {\n            'num_units': num_units,\n        },\n        'attention_layer_size': num_units\n    }\n}\nopt = {\n    'optimizer': {\n        'type':  'AdamOptimizer',\n        'kwargs': {\n            'learning_rate': 0.001,\n        },\n    },\n}\n"""
examples/seq2seq_attn/config_model_full.py,4,"b""# The full possible hyperparameters for the attentional seq2seq model.\n# Most of the hyperparameters take the default values and are not necessary to\n# specify explicitly. The config here results in the same model with the\n# `config_model.py`.\n\nnum_units = 256\nbeam_width = 10\n\n# --------------------- Embedder --------------------- #\nembedder = {\n    'dim': num_units,\n    'initializer': {\n        'type': 'random_uniform_initializer',\n        'kwargs': {\n            'minval': -0.1,\n            'maxval': 0.1,\n            'seed': None\n        },\n    },\n    'regularizer': {\n        'type': 'L1L2',\n        'kwargs': {\n            'l1': 0,\n            'l2': 0\n        }\n    },\n    'dropout_rate': 0,\n    'dropout_strategy': 'element',\n    'trainable': True,\n    'name': 'word_embedder'\n}\n\n# --------------------- Encoder --------------------- #\nencoder = {\n    'rnn_cell_fw': {\n        'type': 'LSTMCell',\n        'kwargs': {\n            'num_units': num_units,\n            'forget_bias': 1.0,\n            'activation': None,\n            # Other arguments go here for tf.nn.rnn_cell.LSTMCell\n            # ...\n        },\n        'num_layers': 1,\n        'dropout': {\n            'input_keep_prob': 1.0,\n            'output_keep_prob': 1.0,\n            'state_keep_prob': 1.0,\n            'variational_recurrent': False,\n            'input_size': [],\n        },\n        'residual': False,\n        'highway': False,\n    },\n    'rnn_cell_bw': {\n        # The same possible hyperparameters as with 'rnn_cell_fw'\n        # ...\n    },\n    'rnn_cell_share_config': True,\n    'output_layer_fw': {\n        'num_layers': 0,\n        'layer_size': 128,\n        'activation': 'identity',\n        'final_layer_activation': None,\n        'other_dense_kwargs': None,\n        'dropout_layer_ids': [],\n        'dropout_rate': 0.5,\n        'variational_dropout': False\n    },\n    'output_layer_bw': {\n        # The same possible hyperparameters as with 'output_layer_fw'\n        # ...\n    },\n    'output_layer_share_config': True,\n    'name': 'bidirectional_rnn_encoder'\n}\n\n# --------------------- Decoder --------------------- #\ndecoder = {\n    'rnn_cell': {\n        'type': 'LSTMCell',\n        'kwargs': {\n            'num_units': num_units,\n            'forget_bias': 1.0,\n            'activation': None,\n            # Other arguments go here for tf.nn.rnn_cell.LSTMCell\n            # ...\n        },\n        'num_layers': 1,\n        'dropout': {\n            'input_keep_prob': 1.0,\n            'output_keep_prob': 1.0,\n            'state_keep_prob': 1.0,\n            'variational_recurrent': False,\n            'input_size': [],\n        },\n        'residual': False,\n        'highway': False,\n    },\n    'attention': {\n        'type': 'LuongAttention',\n        'kwargs': {\n            'num_units': num_units,\n            'scale': False,\n            'probability_fn': None,\n            'score_mask_value': None,\n            # Other arguments go here for tf.contrib.seq2seq.LuongAttention\n            # ...\n        },\n        'attention_layer_size': num_units,\n        'alignment_history': False,\n        'output_attention': True,\n    },\n    'helper_train': {\n        'type': 'TrainingHelper',\n        'kwargs': {\n            # Arguments go here for tf.contrib.seq2seq.TrainingHelper\n        }\n    },\n    'helper_infer': {\n        # The same possible hyperparameters as with 'helper_train'\n        # ...\n    },\n    'max_decoding_length_train': None,\n    'max_decoding_length_infer': None,\n    'name': 'attention_rnn_decoder'\n}\n# --------------------- Optimization --------------------- #\nopt = {\n    'optimizer': {\n        'type':  'AdamOptimizer',\n        'kwargs': {\n            'learning_rate': 0.001,\n            # Other keyword arguments for the optimizer class\n        },\n    },\n    'learning_rate_decay': {\n        # Hyperparameters of learning rate decay\n    },\n    'gradient_clip': {\n        # Hyperparameters of gradient clipping\n    },\n    'gradient_noise_scale': None,\n    'name': None\n}\n"""
examples/seq2seq_attn/config_toy_copy.py,0,"b'\nnum_epochs = 4\ndisplay = 50\n\nsource_vocab_file = \'./data/toy_copy/train/vocab.sources.txt\'\ntarget_vocab_file = \'./data/toy_copy/train/vocab.targets.txt\'\n\ntrain = {\n    \'batch_size\': 32,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/train/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        \'files\': \'./data/toy_copy/train/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\nval = {\n    \'batch_size\': 32,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/dev/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        ""files"": \'./data/toy_copy/dev/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\ntest = {\n    \'batch_size\': 32,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/test/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        ""files"": \'./data/toy_copy/test/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\n'"
examples/seq2seq_attn/prepare_data.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Downloads data.\n""""""\nimport tensorflow as tf\nimport texar.tf as tx\n\n# pylint: disable=invalid-name\n\nflags = tf.flags\n\nflags.DEFINE_string(""data"", ""iwslt14"", ""Data to download [iwslt14|toy_copy]"")\n\nFLAGS = flags.FLAGS\n\n\ndef prepare_data():\n    """"""Downloads data.\n    """"""\n    if FLAGS.data == \'iwslt14\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'iwslt14.zip\',\n            extract=True)\n    elif FLAGS.data == \'toy_copy\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'toy_copy.zip\',\n            extract=True)\n    else:\n        raise ValueError(\'Unknown data: {}\'.format(FLAGS.data))\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    prepare_data()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_attn/seq2seq_attn.py,10,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Attentional Seq2seq.\n""""""\n\n# pylint: disable=invalid-name, too-many-arguments, too-many-locals\n\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""config_iwslt14"", ""The dataset config."")\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\n\ndef build_model(batch, train_data):\n    """"""Assembles the seq2seq model.\n    """"""\n    source_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)\n\n    encoder = tx.modules.BidirectionalRNNEncoder(\n        hparams=config_model.encoder)\n\n    enc_outputs, _ = encoder(source_embedder(batch[\'source_text_ids\']))\n\n    target_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)\n\n    decoder = tx.modules.AttentionRNNDecoder(\n        memory=tf.concat(enc_outputs, axis=2),\n        memory_sequence_length=batch[\'source_length\'],\n        vocab_size=train_data.target_vocab.size,\n        hparams=config_model.decoder)\n\n    training_outputs, _, _ = decoder(\n        decoding_strategy=\'train_greedy\',\n        inputs=target_embedder(batch[\'target_text_ids\'][:, :-1]),\n        sequence_length=batch[\'target_length\'] - 1)\n\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=batch[\'target_text_ids\'][:, 1:],\n        logits=training_outputs.logits,\n        sequence_length=batch[\'target_length\'] - 1)\n\n    train_op = tx.core.get_train_op(mle_loss, hparams=config_model.opt)\n\n    start_tokens = tf.ones_like(batch[\'target_length\']) * \\\n            train_data.target_vocab.bos_token_id\n    beam_search_outputs, _, _ = \\\n        tx.modules.beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=target_embedder,\n            start_tokens=start_tokens,\n            end_token=train_data.target_vocab.eos_token_id,\n            beam_width=config_model.beam_width,\n            max_decoding_length=60)\n\n    return train_op, beam_search_outputs\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    train_data = tx.data.PairedTextData(hparams=config_data.train)\n    val_data = tx.data.PairedTextData(hparams=config_data.val)\n    test_data = tx.data.PairedTextData(hparams=config_data.test)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=train_data, val=val_data, test=test_data)\n\n    batch = data_iterator.get_next()\n\n    train_op, infer_outputs = build_model(batch, train_data)\n\n    def _train_epoch(sess):\n        data_iterator.switch_to_train_data(sess)\n\n        step = 0\n        while True:\n            try:\n                loss = sess.run(train_op)\n                if step % config_data.display == 0:\n                    print(""step={}, loss={:.4f}"".format(step, loss))\n                step += 1\n            except tf.errors.OutOfRangeError:\n                break\n\n    def _eval_epoch(sess, mode):\n        if mode == \'val\':\n            data_iterator.switch_to_val_data(sess)\n        else:\n            data_iterator.switch_to_test_data(sess)\n\n        refs, hypos = [], []\n        while True:\n            try:\n                fetches = [\n                    batch[\'target_text\'][:, 1:],\n                    infer_outputs.predicted_ids[:, :, 0]\n                ]\n                feed_dict = {\n                    tx.global_mode(): tf.estimator.ModeKeys.EVAL\n                }\n                target_texts_ori, output_ids = \\\n                    sess.run(fetches, feed_dict=feed_dict)\n\n                target_texts = tx.utils.strip_special_tokens(\n                    target_texts_ori, is_token_list=True)\n                output_texts = tx.utils.map_ids_to_strs(\n                    ids=output_ids, vocab=val_data.target_vocab)\n\n                for hypo, ref in zip(output_texts, target_texts):\n                    hypos.append(hypo)\n                    refs.append([ref])\n            except tf.errors.OutOfRangeError:\n                break\n\n        return tx.evals.corpus_bleu_moses(list_of_references=refs,\n                                          hypotheses=hypos)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        best_val_bleu = -1.\n        for i in range(config_data.num_epochs):\n            _train_epoch(sess)\n\n            val_bleu = _eval_epoch(sess, \'val\')\n            best_val_bleu = max(best_val_bleu, val_bleu)\n            print(\'val epoch={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n                i, val_bleu, best_val_bleu))\n\n            test_bleu = _eval_epoch(sess, \'test\')\n            print(\'test epoch={}, BLEU={:.4f}\'.format(i, test_bleu))\n\n            print(\'=\' * 50)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_exposure_bias/baseline_seq2seq_attn_main.py,10,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nAttentional Seq2seq.\nsame as examples/seq2seq_attn except that here Rouge is also supported.\n""""""\n\n# pylint: disable=invalid-name, too-many-arguments, too-many-locals\n\nfrom io import open\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\nfrom rouge import Rouge\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""configs.config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""configs.config_iwslt14"",\n                    ""The dataset config."")\n\nflags.DEFINE_string(\'output_dir\', \'.\', \'where to keep training logs\')\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\nif not FLAGS.output_dir.endswith(\'/\'):\n    FLAGS.output_dir += \'/\'\nlog_dir = FLAGS.output_dir + \'training_log_baseline/\'\ntx.utils.maybe_create_dir(log_dir)\n\n\ndef build_model(batch, train_data):\n    """"""Assembles the seq2seq model.\n    """"""\n    source_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)\n\n    encoder = tx.modules.BidirectionalRNNEncoder(\n        hparams=config_model.encoder)\n\n    enc_outputs, _ = encoder(source_embedder(batch[\'source_text_ids\']))\n\n    target_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)\n\n    decoder = tx.modules.AttentionRNNDecoder(\n        memory=tf.concat(enc_outputs, axis=2),\n        memory_sequence_length=batch[\'source_length\'],\n        vocab_size=train_data.target_vocab.size,\n        hparams=config_model.decoder)\n\n    training_outputs, _, _ = decoder(\n        decoding_strategy=\'train_greedy\',\n        inputs=target_embedder(batch[\'target_text_ids\'][:, :-1]),\n        sequence_length=batch[\'target_length\'] - 1)\n\n    train_op = tx.core.get_train_op(\n        tx.losses.sequence_sparse_softmax_cross_entropy(\n            labels=batch[\'target_text_ids\'][:, 1:],\n            logits=training_outputs.logits,\n            sequence_length=batch[\'target_length\'] - 1),\n        hparams=config_model.opt)\n\n    start_tokens = tf.ones_like(batch[\'target_length\']) *\\\n                   train_data.target_vocab.bos_token_id\n    beam_search_outputs, _, _ = \\\n        tx.modules.beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=target_embedder,\n            start_tokens=start_tokens,\n            end_token=train_data.target_vocab.eos_token_id,\n            beam_width=config_model.beam_width,\n            max_decoding_length=60)\n\n    return train_op, beam_search_outputs\n\n\ndef print_stdout_and_file(content, file):\n    print(content)\n    print(content, file=file)\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    train_data = tx.data.PairedTextData(hparams=config_data.train)\n    val_data = tx.data.PairedTextData(hparams=config_data.val)\n    test_data = tx.data.PairedTextData(hparams=config_data.test)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=train_data, val=val_data, test=test_data)\n\n    batch = data_iterator.get_next()\n\n    train_op, infer_outputs = build_model(batch, train_data)\n\n    def _train_epoch(sess, epoch_no):\n        data_iterator.switch_to_train_data(sess)\n        training_log_file = \\\n            open(log_dir + \'training_log\' + str(epoch_no) + \'.txt\', \'w\',\n                 encoding=\'utf-8\')\n\n        step = 0\n        while True:\n            try:\n                loss = sess.run(train_op)\n                print(""step={}, loss={:.4f}"".format(step, loss),\n                      file=training_log_file)\n                if step % config_data.observe_steps == 0:\n                    print(""step={}, loss={:.4f}"".format(step, loss))\n                training_log_file.flush()\n                step += 1\n            except tf.errors.OutOfRangeError:\n                break\n\n    def _eval_epoch(sess, mode, epoch_no):\n        if mode == \'val\':\n            data_iterator.switch_to_val_data(sess)\n        else:\n            data_iterator.switch_to_test_data(sess)\n\n        refs, hypos = [], []\n        while True:\n            try:\n                fetches = [\n                    batch[\'target_text\'][:, 1:],\n                    infer_outputs.predicted_ids[:, :, 0]\n                ]\n                feed_dict = {\n                    tx.global_mode(): tf.estimator.ModeKeys.EVAL\n                }\n                target_texts_ori, output_ids = \\\n                    sess.run(fetches, feed_dict=feed_dict)\n\n                target_texts = tx.utils.strip_special_tokens(\n                    target_texts_ori.tolist(), is_token_list=True)\n                target_texts = tx.utils.str_join(target_texts)\n                output_texts = tx.utils.map_ids_to_strs(\n                    ids=output_ids, vocab=val_data.target_vocab)\n\n                tx.utils.write_paired_text(\n                    target_texts, output_texts,\n                    log_dir + mode + \'_results\' + str(epoch_no) + \'.txt\',\n                    append=True, mode=\'h\', sep=\' ||| \')\n\n                for hypo, ref in zip(output_texts, target_texts):\n                    if config_data.eval_metric == \'bleu\':\n                        hypos.append(hypo)\n                        refs.append([ref])\n                    elif config_data.eval_metric == \'rouge\':\n                        hypos.append(tx.utils.compat_as_text(hypo))\n                        refs.append(tx.utils.compat_as_text(ref))\n            except tf.errors.OutOfRangeError:\n                break\n\n        if config_data.eval_metric == \'bleu\':\n            return tx.evals.corpus_bleu_moses(\n                list_of_references=refs, hypotheses=hypos)\n        elif config_data.eval_metric == \'rouge\':\n            rouge = Rouge()\n            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)\n\n    def _calc_reward(score):\n        """"""\n        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).\n        """"""\n        if config_data.eval_metric == \'bleu\':\n            return score\n        elif config_data.eval_metric == \'rouge\':\n            return sum([value[\'f\'] for key, value in score.items()])\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        best_val_score = -1.\n        scores_file = open(log_dir + \'scores.txt\', \'w\', encoding=\'utf-8\')\n        for i in range(config_data.num_epochs):\n            _train_epoch(sess, i)\n\n            val_score = _eval_epoch(sess, \'val\', i)\n            test_score = _eval_epoch(sess, \'test\', i)\n\n            best_val_score = max(best_val_score, _calc_reward(val_score))\n\n            if config_data.eval_metric == \'bleu\':\n                print_stdout_and_file(\n                    \'val epoch={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n                        i, val_score, best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch={}, BLEU={:.4f}\'.format(i, test_score),\n                    file=scores_file)\n                print_stdout_and_file(\'=\' * 50, file=scores_file)\n\n            elif config_data.eval_metric == \'rouge\':\n                print_stdout_and_file(\n                    \'valid epoch {}:\'.format(i), file=scores_file)\n                for key, value in val_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'fsum: {}; best_val_fsum: {}\'.format(\n                    _calc_reward(val_score), best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch {}:\'.format(i), file=scores_file)\n                for key, value in test_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'=\' * 110, file=scores_file)\n\n            scores_file.flush()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_exposure_bias/interpolation_decoder.py,9,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nInterpolation Decoder is used for interpolation algorithm\nwhich stores one more variable in \'state\' recording the\ndecoded ids(state: [decoded_ids, rnn_state]).\n""""""\n\n# pylint: disable=no-name-in-module, too-many-arguments, too-many-locals\n# pylint: disable=not-context-manager, protected-access, invalid-name\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.rnn_decoders import \\\n    AttentionRNNDecoder, AttentionRNNDecoderOutput\n\n\nclass InterpolationDecoder(AttentionRNNDecoder):\n    """"""\n    Basicly the same as AttentionRNNDecoder except one\n    more variable except rnn_state in \'state\' recording the\n    decoded ids(state: [decoded_ids, rnn_state])\n\n    Args:\n        memory: The memory to query, e.g., the output of an RNN encoder. This\n            tensor should be shaped `[batch_size, max_time, dim]`.\n        memory_sequence_length (optional): A tensor of shape `[batch_size]`\n            containing the sequence lengths for the batch\n            entries in memory. If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n        cell (RNNCell, optional): An instance of `RNNCell`. If `None`, a cell\n            is created as specified in :attr:`hparams`.\n        cell_dropout_mode (optional): A Tensor taking value of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, which\n            toggles dropout in the RNN cell (e.g., activates dropout in\n            TRAIN mode). If `None`, :func:`~texar.tf.global_mode` is used.\n            Ignored if :attr:`cell` is given.\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        output_layer (optional): An instance of\n            :tf_main:`tf.layers.Layer <layers/Layer>`, or\n            :tf_main:`tf.identity <identity>`. Apply to the RNN cell\n            output to get logits. If `None`, a dense layer\n            is used with output dimension set to :attr:`vocab_size`.\n            Set `output_layer=tf.identity` if you do not want to have an\n            output layer after the RNN cell outputs.\n        cell_input_fn (callable, optional): A callable that produces RNN cell\n            inputs. If `None` (default), the default is used:\n            `lambda inputs, attention: tf.concat([inputs, attention], -1)`,\n            which cancats regular RNN cell inputs with attentions.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n    def __init__(self,\n                 memory,\n                 memory_sequence_length=None,\n                 cell=None,\n                 cell_dropout_mode=None,\n                 vocab_size=None,\n                 output_layer=None,\n                 cell_input_fn=None,\n                 hparams=None):\n        AttentionRNNDecoder.__init__(\n            self, memory, memory_sequence_length, cell, cell_dropout_mode,\n            vocab_size, output_layer, cell_input_fn, hparams)\n\n    def initialize(self, name=None):\n        init = AttentionRNNDecoder.initialize(self, name)\n\n        batch_size = tf.shape(init[0])[0]\n\n        # decoded_ids can be initialized as any arbitrary value\n        # because it will be assigned later in decoding\n        initial_decoded_ids = tf.ones((batch_size, 60), dtype=tf.int32)\n\n        initial_rnn_state = init[2]\n        initial_state = [initial_decoded_ids, initial_rnn_state]\n        init[2] = initial_state\n\n        return init\n\n    def step(self, time, inputs, state, name=None):\n        # Basicly the same as in AttentionRNNDecoder except considering\n        # about the different form of \'state\'(decoded_ids, rnn_state)\n\n        wrapper_outputs, wrapper_state = self._cell(inputs, state[1])\n        decoded_ids = state[0]\n\n        logits = self._output_layer(wrapper_outputs)\n\n        sample_ids = self._helper.sample(\n            time=time, outputs=logits, state=[decoded_ids, wrapper_state])\n\n        attention_scores = wrapper_state.alignments\n        attention_context = wrapper_state.attention\n        outputs = AttentionRNNDecoderOutput(\n            logits, sample_ids, wrapper_outputs,\n            attention_scores, attention_context)\n\n        return (outputs, wrapper_state)\n\n    def next_inputs(self, time, outputs, state):\n        (finished, next_inputs, next_state) = self._helper.next_inputs(\n            time=time,\n            outputs=outputs.logits,\n            state=[state[0], state],\n            sample_ids=outputs.sample_id)\n        return (finished, next_inputs, next_state)\n'"
examples/seq2seq_exposure_bias/interpolation_helper.py,17,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper for interpolation algirithm.\nNew token is sample from model, ground_truth or reward according to lambdas\n""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow_probability import distributions as tfpd\nfrom tensorflow.contrib.seq2seq import SampleEmbeddingHelper\nfrom texar.tf.evals.bleu import sentence_bleu\nfrom rouge import Rouge\n\nrouge = Rouge()\n\n\ndef calc_reward(refs, hypo, unk_id, metric):\n    """"""\n    calculate the reward given hypo and refs and will return\n    bleu score if metric is \'bleu\' or return\n    sum of (Rouge-1, Rouge-2, Rouge-L) if metric is \'rouge\'\n    """"""\n    if len(hypo) == 0 or len(refs[0]) == 0:\n        return 0.\n\n    for i in range(len(hypo)):\n        assert isinstance(hypo[i], int)\n        if hypo[i] == unk_id:\n            hypo[i] = -1\n\n    if metric == \'bleu\':\n        return 0.01 * sentence_bleu(\n            references=refs, hypothesis=hypo, smooth=True)\n    else:\n        ref_str = \' \'.join([str(word) for word in refs[0]])\n        hypo_str = \' \'.join([str(word) for word in hypo])\n        rouge_scores = \\\n            rouge.get_scores(hyps=[hypo_str], refs=[ref_str], avg=True)\n        return sum([value[\'f\'] for key, value in rouge_scores.items()])\n\n\nclass InterpolationHelper(SampleEmbeddingHelper):\n    """"""\n    Helper for interpolation algirithm.\n    New token is sample from model, ground_truth or reward according to lambdas\n\n    Args:\n        embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n            or the `params` argument for `embedding_lookup`. The returned tensor\n            will be passed to the decoder input.\n        start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n        end_token: `int32` scalar, the token that marks end of decoding.\n        vocab: texar.Vocab, the vocabularies of training set\n        reward_metric: \'bleu\' or \'rouge\', the metric of reward\n        ground_truth: the ground truth in training set\n        ground_truth_length: the length of ground truth sentences\n        lambdas: \'float32\' vector shapes [3], according to which\n            decide the way of generate the next token in training\n    """"""\n    def __init__(self,\n                 embedding,\n                 start_tokens,\n                 end_token,\n                 vocab,\n                 reward_metric,\n                 ground_truth,\n                 ground_truth_length,\n                 lambdas):\n        SampleEmbeddingHelper.__init__(self, embedding, start_tokens, end_token)\n\n        self._vocab = vocab\n        self._ground_truth = ground_truth\n        self._lambdas = lambdas\n        self._ground_truth_length = ground_truth_length\n        self._metric = reward_metric\n\n    def sample(self, time, outputs, state, name=None):\n        """"""\n        sample tokens for next step, notice the special form\n        of \'state\'([decoded_ids, rnn_state])\n        """"""\n        sample_method_sampler = \\\n            tfpd.Categorical(probs=self._lambdas)\n        sample_method_id = sample_method_sampler.sample()\n\n        truth_feeding = lambda: tf.cond(\n            tf.less(time, tf.shape(self._ground_truth)[1]),\n            lambda: tf.cast(self._ground_truth[:, time], tf.int32),\n            lambda: tf.ones_like(self._ground_truth[:, 0],\n                                 dtype=tf.int32) * self._vocab.eos_token_id)\n\n        self_feeding = lambda: SampleEmbeddingHelper.sample(\n            self, time, outputs, state, name)\n\n        reward_feeding = lambda: self._sample_by_reward(time, state)\n\n        sample_ids = tf.cond(\n            tf.logical_or(tf.equal(time, 0), tf.equal(sample_method_id, 1)),\n            truth_feeding,\n            lambda: tf.cond(\n                tf.equal(sample_method_id, 2),\n                reward_feeding,\n                self_feeding))\n        return sample_ids\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        """"""\n        notice the special form of \'state\'([decoded_ids, rnn_state])\n        """"""\n        finished, next_inputs, next_state = SampleEmbeddingHelper.next_inputs(\n            self, time, outputs, state[1], sample_ids, name)\n\n        next_state = [tf.concat(\n            [state[0][:, :time], tf.expand_dims(sample_ids, 1),\n             state[0][:, time + 1:]], axis=1), next_state]\n        next_state[0] = tf.reshape(next_state[0], (tf.shape(sample_ids)[0], 60))\n\n        return finished, next_inputs, next_state\n\n    def _sample_by_reward(self, time, state):\n        def _get_rewards(time, prefix_ids, target_ids, ground_truth_length):\n            batch_size = np.shape(target_ids)[0]\n            words_in_target = \\\n                [np.unique(target_ids[i]) for i in range(batch_size)]\n            unk_id = self._vocab.unk_token_id\n            eos_id = self._vocab.eos_token_id\n\n            # before append\n            baseline_scores = []\n            baseline_ids = prefix_ids[:, :time]\n            for i in range(batch_size):\n                ref = target_ids[i].tolist()\n                if self._vocab.eos_token_id in ref:\n                    ref = ref[:ref.index(self._vocab.eos_token_id)]\n\n                hypo = baseline_ids[i].tolist()\n                if self._vocab.eos_token_id in hypo:\n                    hypo = hypo[:hypo.index(self._vocab.eos_token_id)]\n\n                baseline_scores.append(calc_reward(\n                    refs=[ref], hypo=hypo, unk_id=unk_id,\n                    metric=self._metric))\n\n            # append UNK\n            syn_ids = np.concatenate([\n                prefix_ids[:, :time],\n                np.ones((batch_size, 1), dtype=np.int32) * unk_id], axis=1)\n\n            reward_unk = []\n            for i in range(batch_size):\n                ref = target_ids[i].tolist()\n                if self._vocab.eos_token_id in ref:\n                    ref = ref[:ref.index(self._vocab.eos_token_id)]\n\n                hypo = syn_ids[i].tolist()\n                if self._vocab.eos_token_id in hypo:\n                    hypo = hypo[:hypo.index(self._vocab.eos_token_id)]\n\n                reward = calc_reward(refs=[ref], hypo=hypo, unk_id=unk_id,\n                                     metric=self._metric)\n                reward_unk.append(\n                    np.ones((1, self._vocab.size), dtype=np.float32) *\n                    reward - baseline_scores[i])\n            result = np.concatenate(reward_unk, axis=0)\n\n            # append tokens\n            for i in range(batch_size):\n                for id in words_in_target[i]:\n                    if id == unk_id:\n                        continue\n\n                    syn_id = np.concatenate(\n                        [prefix_ids[i:i + 1, :time], np.array([[id, ]])],\n                        axis=1)\n                    hypo = syn_id[0].tolist()\n                    if self._vocab.eos_token_id in hypo:\n                        hypo = hypo[:hypo.index(self._vocab.eos_token_id)]\n\n                    ref = target_ids[i].tolist()\n                    if self._vocab.eos_token_id in ref:\n                        ref = ref[:ref.index(self._vocab.eos_token_id)]\n\n                    dup = 1. if prefix_ids[i][time] == id and \\\n                                id != unk_id else 0.\n                    eos = 1. if time < ground_truth_length[i] - 1 and \\\n                                id == eos_id else 0.\n\n                    reward = calc_reward(\n                        refs=[ref], hypo=hypo, unk_id=unk_id,\n                        metric=self._metric)\n                    result[i][id] = reward - baseline_scores[i] - dup - eos\n\n            return result\n\n        sampler = tfpd.Categorical(\n            logits=tf.py_func(_get_rewards, [\n                time, state[0], self._ground_truth,\n                self._ground_truth_length], tf.float32))\n        return tf.reshape(\n            sampler.sample(), (tf.shape(self._ground_truth)[0],))\n'"
examples/seq2seq_exposure_bias/interpolation_main.py,14,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Interpolation Algorithm.\n""""""\n\nimport importlib\nfrom io import open\n\nimport tensorflow as tf\nimport texar.tf as tx\nimport numpy as np\n\nfrom interpolation_decoder import InterpolationDecoder\nfrom interpolation_helper import InterpolationHelper\nfrom rouge import Rouge\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""configs.config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""configs.config_iwslt14"",\n                    ""The dataset config."")\n\nflags.DEFINE_string(\'lambdas_init\', \'[0.04,0.96,0.0]\',\n                    \'initial value of lambdas\')\n\nflags.DEFINE_float(\'delta_lambda_reward\', 0.06,\n                   \'increment of lambda_reward every annealing\')\nflags.DEFINE_float(\'delta_lambda_self\', 0.06,\n                   \'decrement of lambda_self every annealing\')\nflags.DEFINE_integer(\'lambda_reward_steps\', 4,\n                     \'times of increasing lambda_reward \'\n                     \'after incresing lambda_self once\')\n\nflags.DEFINE_string(\'output_dir\', \'.\', \'where to keep training logs\')\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\nFLAGS.lambdas_init = eval(FLAGS.lambdas_init)\n\nif not FLAGS.output_dir.endswith(\'/\'):\n    FLAGS.output_dir += \'/\'\nlog_dir = FLAGS.output_dir + \'training_log_interpolation\' +\\\n          \'_init\' + \'_\' + str(FLAGS.lambdas_init[0]) +\\\n          \'_\' + str(FLAGS.lambdas_init[1]) +\\\n          \'_\' + str(FLAGS.lambdas_init[2]) +\\\n          \'_dr\' + str(FLAGS.delta_lambda_reward) +\\\n          \'_ds\' + str(FLAGS.delta_lambda_self) +\\\n          \'_rstep\' + str(FLAGS.lambda_reward_steps) + \'/\'\ntx.utils.maybe_create_dir(log_dir)\n\n\ndef build_model(batch, train_data, lambdas):\n    """"""\n    This function is basically the same as build_model() in\n    baseline_seq2seq_attn.py, except the\n    InterpolateDecoder and InterpolateHelper.\n    """"""\n    batch_size = tf.shape(batch[\'target_length\'])[0]\n\n    source_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)\n\n    encoder = tx.modules.BidirectionalRNNEncoder(\n        hparams=config_model.encoder)\n\n    enc_outputs, _ = encoder(source_embedder(batch[\'source_text_ids\']))\n\n    target_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)\n\n    decoder = InterpolationDecoder(\n        memory=tf.concat(enc_outputs, axis=2),\n        memory_sequence_length=batch[\'source_length\'],\n        vocab_size=train_data.target_vocab.size,\n        hparams=config_model.decoder)\n\n    start_tokens = tf.ones_like(\n        batch[\'target_length\']) * train_data.target_vocab.bos_token_id\n    helper = InterpolationHelper(\n        embedding=target_embedder,\n        start_tokens=start_tokens,\n        end_token=train_data.target_vocab.eos_token_id,\n        reward_metric=config_data.eval_metric,\n        vocab=train_data.target_vocab,\n        ground_truth=batch[\'target_text_ids\'][:, 1:],\n        ground_truth_length=batch[\'target_length\'] - 1,\n        lambdas=lambdas,)\n\n    training_outputs, _, training_length = decoder(\n        helper=helper,\n        initial_state=decoder.zero_state(\n            batch_size=batch_size, dtype=tf.float32),\n        max_decoding_length=60)\n\n    train_op = tx.core.get_train_op(\n        tx.losses.sequence_sparse_softmax_cross_entropy(\n            labels=training_outputs.sample_id,\n            logits=training_outputs.logits,\n            sequence_length=training_length),\n        hparams=config_model.opt)\n\n    beam_search_outputs, _, _ = \\\n        tx.modules.beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=target_embedder,\n            start_tokens=start_tokens,\n            end_token=train_data.target_vocab.eos_token_id,\n            beam_width=config_model.beam_width,\n            max_decoding_length=60)\n\n    return train_op, beam_search_outputs\n\n\ndef print_stdout_and_file(content, file):\n    print(content)\n    print(content, file=file)\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    training_data = tx.data.PairedTextData(hparams=config_data.train)\n    val_data = tx.data.PairedTextData(hparams=config_data.val)\n    test_data = tx.data.PairedTextData(hparams=config_data.test)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=training_data, val=val_data, test=test_data)\n\n    batch = data_iterator.get_next()\n    lambdas_ts = tf.placeholder(shape=[3], dtype=tf.float32)\n\n    train_op, infer_outputs = build_model(batch, training_data, lambdas_ts)\n\n    def _train_epoch(sess, epoch, lambdas):\n        data_iterator.switch_to_train_data(sess)\n        log_file = open(log_dir + \'training_log\' + str(epoch) + \'.txt\', \'w\',\n                        encoding=\'utf-8\')\n\n        step = 0\n        while True:\n            try:\n                loss = sess.run(train_op, feed_dict={\n                    lambdas_ts: np.array(lambdas)})\n                print(""step={}, loss={:.4f}, lambdas={}"".format(\n                    step, loss, lambdas), file=log_file)\n                if step % config_data.observe_steps == 0:\n                    print(""step={}, loss={:.4f}, lambdas={}"".format(\n                        step, loss, lambdas))\n                log_file.flush()\n                step += 1\n\n            except tf.errors.OutOfRangeError:\n                break\n\n    def _eval_epoch(sess, mode, epoch_no):\n        """"""\n        This function is the same as _eval_epoch() in\n        baseline_seq2seq_attn_main.py.\n        """"""\n        if mode == \'val\':\n            data_iterator.switch_to_val_data(sess)\n        else:\n            data_iterator.switch_to_test_data(sess)\n\n        refs, hypos = [], []\n        while True:\n            try:\n                fetches = [\n                    batch[\'target_text\'][:, 1:],\n                    infer_outputs.predicted_ids[:, :, 0]\n                ]\n                feed_dict = {\n                    tx.global_mode(): tf.estimator.ModeKeys.EVAL\n                }\n                target_texts_ori, output_ids = \\\n                    sess.run(fetches, feed_dict=feed_dict)\n\n                target_texts = tx.utils.strip_special_tokens(\n                    target_texts_ori.tolist(), is_token_list=True)\n                target_texts = tx.utils.str_join(target_texts)\n                output_texts = tx.utils.map_ids_to_strs(\n                    ids=output_ids, vocab=val_data.target_vocab)\n\n                tx.utils.write_paired_text(\n                    target_texts, output_texts,\n                    log_dir + mode + \'_results\' + str(epoch_no) + \'.txt\',\n                    append=True, mode=\'h\', sep=\' ||| \')\n\n                for hypo, ref in zip(output_texts, target_texts):\n                    if config_data.eval_metric == \'bleu\':\n                        hypos.append(hypo)\n                        refs.append([ref])\n                    elif config_data.eval_metric == \'rouge\':\n                        hypos.append(tx.utils.compat_as_text(hypo))\n                        refs.append(tx.utils.compat_as_text(ref))\n            except tf.errors.OutOfRangeError:\n                break\n\n        if config_data.eval_metric == \'bleu\':\n            return tx.evals.corpus_bleu_moses(\n                list_of_references=refs, hypotheses=hypos)\n        elif config_data.eval_metric == \'rouge\':\n            rouge = Rouge()\n            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)\n\n    def _calc_reward(score):\n        """"""\n        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).\n        """"""\n        if config_data.eval_metric == \'bleu\':\n            return score\n        elif config_data.eval_metric == \'rouge\':\n            return sum([value[\'f\'] for key, value in score.items()])\n\n    def _anneal():\n        """"""\n        Operate lambdas when the reward of val set decrease.\n        """"""\n        def _update_self():\n            """"""\n            Decrease lambda_truth and increase lambda_self.\n            """"""\n            lambdas[1] -= FLAGS.delta_lambda_self\n            lambdas[0] += FLAGS.delta_lambda_self\n            updates.append(\'self\')\n\n        def _update_rew():\n            """"""\n            Decrease lambda_truth and increase lambda_reward.\n            """"""\n            lambdas[1] -= FLAGS.delta_lambda_reward\n            lambdas[2] += FLAGS.delta_lambda_reward\n            updates.append(\'rew\')\n\n        if updates[-FLAGS.lambda_reward_steps:] == \\\n                [\'rew\'] * FLAGS.lambda_reward_steps:\n            _update_self()\n        else:\n            _update_rew()\n\n    saver = tf.train.Saver(max_to_keep=2)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        lambdas = FLAGS.lambdas_init\n        updates = [\'rew\'] * FLAGS.lambda_reward_steps\n\n        best_val_score, best_val_score_current_lambdas = -1., -1.\n        scores_file = open(log_dir + \'scores.txt\', \'w\', encoding=\'utf-8\')\n\n        for i in range(config_data.num_epochs):\n            print_stdout_and_file(\n                \'training epoch={}, lambdas={}\'.format(i, lambdas),\n                file=scores_file)\n            _train_epoch(sess, i, lambdas)\n            saver.save(sess, log_dir + \'models/model{}.ckpt\'.format(i))\n\n            val_score = _eval_epoch(sess, \'val\', i)\n            test_score = _eval_epoch(sess, \'test\', i)\n\n            if _calc_reward(val_score) < best_val_score_current_lambdas:\n                _anneal()\n                best_val_score_current_lambdas = -1.\n                saver.restore(\n                    sess, log_dir + \'models/model{}.ckpt\'.format(i - 1))\n            else:\n                best_val_score_current_lambdas = _calc_reward(val_score)\n\n            best_val_score = max(best_val_score, _calc_reward(val_score))\n\n            if config_data.eval_metric == \'bleu\':\n                print_stdout_and_file(\n                    \'val epoch={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n                        i, val_score, best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch={}, BLEU={:.4f}\'.format(i, test_score),\n                    file=scores_file)\n                print_stdout_and_file(\'=\' * 50, file=scores_file)\n\n            elif config_data.eval_metric == \'rouge\':\n                print_stdout_and_file(\n                    \'valid epoch {}:\'.format(i), file=scores_file)\n                for key, value in val_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'fsum: {}; best_val_fsum: {}\'.format(\n                    _calc_reward(val_score), best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch {}:\'.format(i), file=scores_file)\n                for key, value in test_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'=\' * 110, file=scores_file)\n\n            scores_file.flush()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_exposure_bias/raml_main.py,13,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nAttentional Seq2seq with RAML algorithm.\n\nRead a pre-processed file containing the augmented samples and\ncorresponding rewards for every target sentence.\n\nRAML Algorithm is described in https://arxiv.org/pdf/1705.07136.pdf\n\n""""""\n\nfrom io import open\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\nimport numpy as np\nimport random\nfrom rouge import Rouge\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""configs.config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""configs.config_iwslt14"",\n                    ""The dataset config."")\n\nflags.DEFINE_string(\'raml_file\', \'data/iwslt14/samples_iwslt14.txt\',\n                    \'the samples and rewards described in RAML\')\nflags.DEFINE_integer(\'n_samples\', 10,\n                     \'number of samples for every target sentence\')\nflags.DEFINE_float(\'tau\', 0.4, \'the temperature in RAML algorithm\')\n\nflags.DEFINE_string(\'output_dir\', \'.\', \'where to keep training logs\')\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\nif not FLAGS.output_dir.endswith(\'/\'):\n    FLAGS.output_dir += \'/\'\nlog_dir = FLAGS.output_dir + \'training_log_raml\' +\\\n          \'_\' + str(FLAGS.n_samples) + \'samples\' +\\\n          \'_tau\' + str(FLAGS.tau) + \'/\'\ntx.utils.maybe_create_dir(log_dir)\n\n\ndef read_raml_sample_file():\n    raml_file = open(FLAGS.raml_file, encoding=\'utf-8\')\n\n    train_data = []\n    sample_num = -1\n    for line in raml_file.readlines():\n        line = line[:-1]\n        if line.startswith(\'***\'):\n            continue\n        elif line.endswith(\'samples\'):\n            sample_num = eval(line.split()[0])\n            assert sample_num == 1 or sample_num == FLAGS.n_samples\n        elif line.startswith(\'source:\'):\n            train_data.append({\'source\': line[7:], \'targets\': []})\n        else:\n            train_data[-1][\'targets\'].append(line.split(\'|||\'))\n            if sample_num == 1:\n                for i in range(FLAGS.n_samples - 1):\n                    train_data[-1][\'targets\'].append(line.split(\'|||\'))\n    return train_data\n\n\ndef raml_loss(batch, output, training_rewards):\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=batch[\'target_text_ids\'][:, 1:],\n        logits=output.logits,\n        sequence_length=batch[\'target_length\'] - 1,\n        average_across_batch=False)\n    return tf.reduce_sum(mle_loss * training_rewards) /\\\n           tf.reduce_sum(training_rewards)\n\n\ndef build_model(batch, train_data, rewards):\n    """"""\n    Assembles the seq2seq model.\n    Code in this function is basically the same of build_model() in\n    baseline_seq2seq_attn_main.py except the normalization in loss_fn.\n    """"""\n    source_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)\n\n    encoder = tx.modules.BidirectionalRNNEncoder(\n        hparams=config_model.encoder)\n\n    enc_outputs, _ = encoder(source_embedder(batch[\'source_text_ids\']))\n\n    target_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)\n\n    decoder = tx.modules.AttentionRNNDecoder(\n        memory=tf.concat(enc_outputs, axis=2),\n        memory_sequence_length=batch[\'source_length\'],\n        vocab_size=train_data.target_vocab.size,\n        hparams=config_model.decoder)\n\n    training_outputs, _, _ = decoder(\n        decoding_strategy=\'train_greedy\',\n        inputs=target_embedder(batch[\'target_text_ids\'][:, :-1]),\n        sequence_length=batch[\'target_length\'] - 1)\n\n    train_op = tx.core.get_train_op(\n        raml_loss(batch, training_outputs, rewards),\n        hparams=config_model.opt)\n\n    start_tokens = tf.ones_like(batch[\'target_length\']) *\\\n                   train_data.target_vocab.bos_token_id\n    beam_search_outputs, _, _ = \\\n        tx.modules.beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=target_embedder,\n            start_tokens=start_tokens,\n            end_token=train_data.target_vocab.eos_token_id,\n            beam_width=config_model.beam_width,\n            max_decoding_length=60)\n\n    return train_op, beam_search_outputs\n\n\ndef print_stdout_and_file(content, file):\n    print(content)\n    print(content, file=file)\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    config_data.train[\'batch_size\'] *= FLAGS.n_samples\n    config_data.val[\'batch_size\'] *= FLAGS.n_samples\n    config_data.test[\'batch_size\'] *= FLAGS.n_samples\n\n    train_data = tx.data.PairedTextData(hparams=config_data.train)\n    val_data = tx.data.PairedTextData(hparams=config_data.val)\n    test_data = tx.data.PairedTextData(hparams=config_data.test)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=train_data, val=val_data, test=test_data)\n\n    batch = data_iterator.get_next()\n    rewards_ts = tf.placeholder(\n        dtype=tf.float32, shape=[None, ], name=\'training_rewards\')\n\n    train_op, infer_outputs = build_model(batch, train_data, rewards_ts)\n\n    raml_train_data = read_raml_sample_file()\n\n    def _train_epoch(sess, epoch_no):\n        data_iterator.switch_to_train_data(sess)\n        training_log_file = \\\n            open(log_dir + \'training_log\' + str(epoch_no) + \'.txt\', \'w\',\n                 encoding=\'utf-8\')\n\n        step = 0\n        source_buffer, target_buffer = [], []\n        random.shuffle(raml_train_data)\n        for training_pair in raml_train_data:\n            for target in training_pair[\'targets\']:\n                source_buffer.append(training_pair[\'source\'])\n                target_buffer.append(target)\n\n            if len(target_buffer) != train_data.batch_size:\n                continue\n\n            source_ids = []\n            source_length = []\n            target_ids = []\n            target_length = []\n            scores = []\n\n            trunc_len_src = train_data.hparams.source_dataset.max_seq_length\n            trunc_len_tgt = train_data.hparams.target_dataset.max_seq_length\n\n            for sentence in source_buffer:\n                ids = [train_data.source_vocab.token_to_id_map_py[token]\n                       for token in sentence.split()][:trunc_len_src]\n                ids = ids + [train_data.source_vocab.eos_token_id]\n\n                source_ids.append(ids)\n                source_length.append(len(ids))\n\n            for sentence, score_str in target_buffer:\n                ids = [train_data.target_vocab.bos_token_id]\n                ids = ids + [train_data.target_vocab.token_to_id_map_py[token]\n                             for token in sentence.split()][:trunc_len_tgt]\n                ids = ids + [train_data.target_vocab.eos_token_id]\n\n                target_ids.append(ids)\n                scores.append(eval(score_str))\n                target_length.append(len(ids))\n\n            rewards = []\n            for i in range(0, train_data.batch_size, FLAGS.n_samples):\n                tmp = np.array(scores[i:i + FLAGS.n_samples])\n                tmp = np.exp(tmp / FLAGS.tau) / np.sum(np.exp(tmp / FLAGS.tau))\n                for j in range(0, FLAGS.n_samples):\n                    rewards.append(tmp[j])\n\n            for value in source_ids:\n                while len(value) < max(source_length):\n                    value.append(0)\n            for value in target_ids:\n                while len(value) < max(target_length):\n                    value.append(0)\n\n            feed_dict = {\n                batch[\'source_text_ids\']: np.array(source_ids),\n                batch[\'target_text_ids\']: np.array(target_ids),\n                batch[\'source_length\']: np.array(source_length),\n                batch[\'target_length\']: np.array(target_length),\n                rewards_ts: np.array(rewards)\n            }\n            source_buffer = []\n            target_buffer = []\n\n            loss = sess.run(train_op, feed_dict=feed_dict)\n            print(""step={}, loss={:.4f}"".format(step, loss),\n                  file=training_log_file)\n            if step % config_data.observe_steps == 0:\n                print(""step={}, loss={:.4f}"".format(step, loss))\n            training_log_file.flush()\n            step += 1\n\n    # code below this line is exactly the same as baseline_seq2seq_attn_main.py\n\n    def _eval_epoch(sess, mode, epoch_no):\n        if mode == \'val\':\n            data_iterator.switch_to_val_data(sess)\n        else:\n            data_iterator.switch_to_test_data(sess)\n\n        refs, hypos = [], []\n        while True:\n            try:\n                fetches = [\n                    batch[\'target_text\'][:, 1:],\n                    infer_outputs.predicted_ids[:, :, 0]\n                ]\n                feed_dict = {\n                    tx.global_mode(): tf.estimator.ModeKeys.EVAL\n                }\n                target_texts_ori, output_ids = \\\n                    sess.run(fetches, feed_dict=feed_dict)\n\n                target_texts = tx.utils.strip_special_tokens(\n                    target_texts_ori.tolist(), is_token_list=True)\n                target_texts = tx.utils.str_join(target_texts)\n                output_texts = tx.utils.map_ids_to_strs(\n                    ids=output_ids, vocab=val_data.target_vocab)\n\n                tx.utils.write_paired_text(\n                    target_texts, output_texts,\n                    log_dir + mode + \'_results\' + str(epoch_no) + \'.txt\',\n                    append=True, mode=\'h\', sep=\' ||| \')\n\n                for hypo, ref in zip(output_texts, target_texts):\n                    if config_data.eval_metric == \'bleu\':\n                        hypos.append(hypo)\n                        refs.append([ref])\n                    elif config_data.eval_metric == \'rouge\':\n                        hypos.append(tx.utils.compat_as_text(hypo))\n                        refs.append(tx.utils.compat_as_text(ref))\n            except tf.errors.OutOfRangeError:\n                break\n\n        if config_data.eval_metric == \'bleu\':\n            return tx.evals.corpus_bleu_moses(\n                list_of_references=refs, hypotheses=hypos)\n        elif config_data.eval_metric == \'rouge\':\n            rouge = Rouge()\n            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)\n\n    def _calc_reward(score):\n        """"""\n        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).\n        """"""\n        if config_data.eval_metric == \'bleu\':\n            return score\n        elif config_data.eval_metric == \'rouge\':\n            return sum([value[\'f\'] for key, value in score.items()])\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        best_val_score = -1.\n        scores_file = open(log_dir + \'scores.txt\', \'w\', encoding=\'utf-8\')\n        for i in range(config_data.num_epochs):\n            _train_epoch(sess, i)\n\n            val_score = _eval_epoch(sess, \'val\', i)\n            test_score = _eval_epoch(sess, \'test\', i)\n\n            best_val_score = max(best_val_score, _calc_reward(val_score))\n\n            if config_data.eval_metric == \'bleu\':\n                print_stdout_and_file(\n                    \'val epoch={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n                        i, val_score, best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch={}, BLEU={:.4f}\'.format(i, test_score),\n                    file=scores_file)\n                print_stdout_and_file(\'=\' * 50, file=scores_file)\n\n            elif config_data.eval_metric == \'rouge\':\n                print_stdout_and_file(\n                    \'valid epoch {}:\'.format(i), file=scores_file)\n                for key, value in val_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'fsum: {}; best_val_fsum: {}\'.format(\n                    _calc_reward(val_score), best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch {}:\'.format(i), file=scores_file)\n                for key, value in test_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'=\' * 110, file=scores_file)\n\n            scores_file.flush()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_exposure_bias/scheduled_sampling_main.py,12,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nAttentional Seq2seq using Scheduled sampling algorithm.\n\nThis code is basically the same as baseline_seq2seq_attn_main.py,\nexcept using ScheduledEmbeddingTrainingHelper.\n\nScheduled Sampling Algorithm is described in https://arxiv.org/abs/1506.03099\n""""""\n\n# pylint: disable=invalid-name, too-many-arguments, too-many-locals\n\nfrom io import open\nimport math\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\nfrom rouge import Rouge\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""configs.config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""configs.config_iwslt14"",\n                    ""The dataset config."")\n\nflags.DEFINE_float(\'decay_factor\', 500.,\n                   \'The hyperparameter controling the speed of increasing \'\n                   \'the probability of sampling from model\')\n\nflags.DEFINE_string(\'output_dir\', \'.\', \'where to keep training logs\')\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\nif not FLAGS.output_dir.endswith(\'/\'):\n    FLAGS.output_dir += \'/\'\nlog_dir = FLAGS.output_dir + \'training_log_scheduled_sampling\' +\\\n          \'_decayf\' + str(FLAGS.decay_factor) + \'/\'\ntx.utils.maybe_create_dir(log_dir)\n\n\ndef inverse_sigmoid(i):\n    return FLAGS.decay_factor / (\n            FLAGS.decay_factor + math.exp(i / FLAGS.decay_factor))\n\n\ndef build_model(batch, train_data, self_sampling_proba):\n    """"""\n    Assembles the seq2seq model.\n    It is the same as build_model() in baseline_seq2seq_attn.py except\n    using ScheduledEmbeddingTrainingHelper.\n    """"""\n    source_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)\n\n    encoder = tx.modules.BidirectionalRNNEncoder(\n        hparams=config_model.encoder)\n\n    enc_outputs, _ = encoder(source_embedder(batch[\'source_text_ids\']))\n\n    target_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)\n\n    decoder = tx.modules.AttentionRNNDecoder(\n        memory=tf.concat(enc_outputs, axis=2),\n        memory_sequence_length=batch[\'source_length\'],\n        vocab_size=train_data.target_vocab.size,\n        hparams=config_model.decoder)\n\n    helper = tx.modules.get_helper(\n        helper_type=\'ScheduledEmbeddingTrainingHelper\',\n        inputs=target_embedder(batch[\'target_text_ids\'][:, :-1]),\n        sequence_length=batch[\'target_length\'] - 1,\n        embedding=target_embedder,\n        sampling_probability=self_sampling_proba)\n\n    training_outputs, _, _ = decoder(\n        helper=helper, initial_state=decoder.zero_state(\n            batch_size=tf.shape(batch[\'target_length\'])[0], dtype=tf.float32))\n\n    train_op = tx.core.get_train_op(\n        tx.losses.sequence_sparse_softmax_cross_entropy(\n            labels=batch[\'target_text_ids\'][:, 1:],\n            logits=training_outputs.logits,\n            sequence_length=batch[\'target_length\'] - 1),\n        hparams=config_model.opt)\n\n    start_tokens = tf.ones_like(batch[\'target_length\']) *\\\n                   train_data.target_vocab.bos_token_id\n    beam_search_outputs, _, _ = \\\n        tx.modules.beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=target_embedder,\n            start_tokens=start_tokens,\n            end_token=train_data.target_vocab.eos_token_id,\n            beam_width=config_model.beam_width,\n            max_decoding_length=60)\n\n    return train_op, beam_search_outputs\n\n\ndef print_stdout_and_file(content, file):\n    print(content)\n    print(content, file=file)\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    train_data = tx.data.PairedTextData(hparams=config_data.train)\n    val_data = tx.data.PairedTextData(hparams=config_data.val)\n    test_data = tx.data.PairedTextData(hparams=config_data.test)\n    data_iterator = tx.data.TrainTestDataIterator(\n        train=train_data, val=val_data, test=test_data)\n\n    batch = data_iterator.get_next()\n\n    self_sampling_proba = tf.placeholder(shape=[], dtype=tf.float32)\n    train_op, infer_outputs = \\\n        build_model(batch, train_data, self_sampling_proba)\n\n    def _train_epoch(sess, epoch_no, total_step_counter):\n        data_iterator.switch_to_train_data(sess)\n        training_log_file = \\\n            open(log_dir + \'training_log\' + str(epoch_no) + \'.txt\', \'w\',\n                 encoding=\'utf-8\')\n\n        step = 0\n        while True:\n            try:\n                sampling_proba_ = 1. - inverse_sigmoid(total_step_counter)\n                loss = sess.run(train_op, feed_dict={\n                    self_sampling_proba: sampling_proba_})\n                print(""step={}, loss={:.4f}, self_proba={}"".format(\n                    step, loss, sampling_proba_), file=training_log_file)\n                if step % config_data.observe_steps == 0:\n                    print(""step={}, loss={:.4f}, self_proba={}"".format(\n                        step, loss, sampling_proba_))\n                training_log_file.flush()\n                step += 1\n                total_step_counter += 1\n            except tf.errors.OutOfRangeError:\n                break\n\n    # code below this line is exactly the same as baseline_seq2seq_attn_main.py\n\n    def _eval_epoch(sess, mode, epoch_no):\n        if mode == \'val\':\n            data_iterator.switch_to_val_data(sess)\n        else:\n            data_iterator.switch_to_test_data(sess)\n\n        refs, hypos = [], []\n        while True:\n            try:\n                fetches = [\n                    batch[\'target_text\'][:, 1:],\n                    infer_outputs.predicted_ids[:, :, 0]\n                ]\n                feed_dict = {\n                    tx.global_mode(): tf.estimator.ModeKeys.EVAL\n                }\n                target_texts_ori, output_ids = \\\n                    sess.run(fetches, feed_dict=feed_dict)\n\n                target_texts = tx.utils.strip_special_tokens(\n                    target_texts_ori.tolist(), is_token_list=True)\n                target_texts = tx.utils.str_join(target_texts)\n                output_texts = tx.utils.map_ids_to_strs(\n                    ids=output_ids, vocab=val_data.target_vocab)\n\n                tx.utils.write_paired_text(\n                    target_texts, output_texts,\n                    log_dir + mode + \'_results\' + str(epoch_no) + \'.txt\',\n                    append=True, mode=\'h\', sep=\' ||| \')\n\n                for hypo, ref in zip(output_texts, target_texts):\n                    if config_data.eval_metric == \'bleu\':\n                        hypos.append(hypo)\n                        refs.append([ref])\n                    elif config_data.eval_metric == \'rouge\':\n                        hypos.append(tx.utils.compat_as_text(hypo))\n                        refs.append(tx.utils.compat_as_text(ref))\n            except tf.errors.OutOfRangeError:\n                break\n\n        if config_data.eval_metric == \'bleu\':\n            return tx.evals.corpus_bleu_moses(\n                list_of_references=refs, hypotheses=hypos)\n        elif config_data.eval_metric == \'rouge\':\n            rouge = Rouge()\n            return rouge.get_scores(hyps=hypos, refs=refs, avg=True)\n\n    def _calc_reward(score):\n        """"""\n        Return the bleu score or the sum of (Rouge-1, Rouge-2, Rouge-L).\n        """"""\n        if config_data.eval_metric == \'bleu\':\n            return score\n        elif config_data.eval_metric == \'rouge\':\n            return sum([value[\'f\'] for key, value in score.items()])\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        best_val_score = -1.\n        total_step_counter = 1\n        scores_file = open(log_dir + \'scores.txt\', \'w\', encoding=\'utf-8\')\n        for i in range(config_data.num_epochs):\n            _train_epoch(sess, i, total_step_counter)\n\n            val_score = _eval_epoch(sess, \'val\', i)\n            test_score = _eval_epoch(sess, \'test\', i)\n\n            best_val_score = max(best_val_score, _calc_reward(val_score))\n\n            if config_data.eval_metric == \'bleu\':\n                print_stdout_and_file(\n                    \'val epoch={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n                        i, val_score, best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch={}, BLEU={:.4f}\'.format(i, test_score),\n                    file=scores_file)\n                print_stdout_and_file(\'=\' * 50, file=scores_file)\n\n            elif config_data.eval_metric == \'rouge\':\n                print_stdout_and_file(\n                    \'valid epoch {}:\'.format(i), file=scores_file)\n                for key, value in val_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'fsum: {}; best_val_fsum: {}\'.format(\n                    _calc_reward(val_score), best_val_score), file=scores_file)\n\n                print_stdout_and_file(\n                    \'test epoch {}:\'.format(i), file=scores_file)\n                for key, value in test_score.items():\n                    print_stdout_and_file(\n                        \'{}: {}\'.format(key, value), file=scores_file)\n                print_stdout_and_file(\'=\' * 110, file=scores_file)\n\n            scores_file.flush()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_rl/config_iwslt14.py,0,"b'\ndisplay = 100\ndisplay_eval = 5500\n\nsource_vocab_file = \'./data/iwslt14/vocab.de\'\ntarget_vocab_file = \'./data/iwslt14/vocab.en\'\n\ntrain = {\n    \'num_epochs\': 10,\n    \'batch_size\': 32,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/train.de\',\n        \'vocab_file\': source_vocab_file,\n        \'max_seq_length\': 50\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/train.en\',\n        \'vocab_file\': target_vocab_file,\n        \'max_seq_length\': 50\n    }\n}\nval = {\n    \'batch_size\': 32,\n    \'shuffle\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/valid.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/valid.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\ntest = {\n    \'batch_size\': 32,\n    \'shuffle\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/test.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/test.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n'"
examples/seq2seq_rl/config_model.py,0,"b""# Attentional Seq2seq model.\n# Hyperparameters not specified here will take the default values.\n\nnum_units = 256\nbeam_width = 10\n\nembedder = {\n    'dim': num_units\n}\nencoder = {\n    'rnn_cell_fw': {\n        'kwargs': {\n            'num_units': num_units\n        }\n    }\n}\ndecoder = {\n    'rnn_cell': {\n        'kwargs': {\n            'num_units': num_units\n        },\n    },\n    'attention': {\n        'kwargs': {\n            'num_units': num_units,\n        },\n        'attention_layer_size': num_units\n    }\n}\nagent = {\n    'discount_factor': 0.,\n    'entropy_weight': .5\n}\n"""
examples/seq2seq_rl/config_toy_copy.py,0,"b'\ndisplay = 10\ndisplay_eval = 300\n\nsource_vocab_file = \'./data/toy_copy/train/vocab.sources.txt\'\ntarget_vocab_file = \'./data/toy_copy/train/vocab.targets.txt\'\n\ntrain = {\n    \'num_epochs\': 10,\n    \'batch_size\': 32,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/train/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        \'files\': \'./data/toy_copy/train/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\nval = {\n    \'batch_size\': 32,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/dev/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        ""files"": \'./data/toy_copy/dev/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\ntest = {\n    \'batch_size\': 32,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'./data/toy_copy/test/sources.txt\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        ""files"": \'./data/toy_copy/test/targets.txt\',\n        \'vocab_file\': target_vocab_file\n    }\n}\n'"
examples/seq2seq_rl/prepare_data.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Downloads data.\n""""""\nimport tensorflow as tf\nimport texar.tf as tx\n\n# pylint: disable=invalid-name\n\nflags = tf.flags\n\nflags.DEFINE_string(""data"", ""iwslt14"", ""Data to download [iwslt14|toy_copy]"")\n\nFLAGS = flags.FLAGS\n\n\ndef prepare_data():\n    """"""Downloads data.\n    """"""\n    if FLAGS.data == \'iwslt14\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1Vuv3bed10qUxrpldHdYoiWLzPKa4pNXd/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'iwslt14.zip\',\n            extract=True)\n    elif FLAGS.data == \'toy_copy\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'toy_copy.zip\',\n            extract=True)\n    else:\n        raise ValueError(\'Unknown data: {}\'.format(FLAGS.data))\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    prepare_data()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seq2seq_rl/seq2seq_attn_pg.py,13,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Attentional Seq2seq trained with policy gradient.\n""""""\n\n# pylint: disable=invalid-name, too-many-arguments, too-many-locals\n\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""config_iwslt14"", ""The dataset config."")\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\n# A caveats of using `texar.tf.agents.SeqPGAgent`:\n# The training data iterator should not run to raise `OutOfRangeError`,\n# otherwise the iterator cannot be re-initialized and may raise\n# `CancelledError`. This is probably because the iterator is used by\n# `tf.Session.partial_run` in `SeqPGAgent`.\n#\n# A simple workaround is to set `\'num_epochs\'` of training data to a large\n# number so that its iterator will never run into `OutOfRangeError`. Use\n# `texar.tf.data.FeedableDataIterator` to periodically switch to dev/test data\n# for evaluation and switch back to the training data to resume from the\n# breakpoint.\n\n\ndef build_model(batch, train_data):\n    """"""Assembles the seq2seq model.\n    """"""\n    source_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.source_vocab.size, hparams=config_model.embedder)\n\n    encoder = tx.modules.BidirectionalRNNEncoder(\n        hparams=config_model.encoder)\n\n    enc_outputs, _ = encoder(source_embedder(batch[\'source_text_ids\']))\n\n    target_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.target_vocab.size, hparams=config_model.embedder)\n\n    decoder = tx.modules.AttentionRNNDecoder(\n        memory=tf.concat(enc_outputs, axis=2),\n        memory_sequence_length=batch[\'source_length\'],\n        vocab_size=train_data.target_vocab.size,\n        hparams=config_model.decoder)\n\n    start_tokens = tf.ones_like(batch[\'target_length\']) * \\\n            train_data.target_vocab.bos_token_id\n\n    outputs, _, sequence_length = decoder(\n        decoding_strategy=\'infer_sample\',\n        start_tokens=start_tokens,\n        end_token=train_data.target_vocab.eos_token_id,\n        embedding=target_embedder,\n        max_decoding_length=30)\n\n    beam_search_outputs, _, _ = \\\n        tx.modules.beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=target_embedder,\n            start_tokens=start_tokens,\n            end_token=train_data.target_vocab.eos_token_id,\n            beam_width=config_model.beam_width,\n            max_decoding_length=60)\n\n    return outputs, sequence_length, beam_search_outputs\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    train_data = tx.data.PairedTextData(hparams=config_data.train)\n    val_data = tx.data.PairedTextData(hparams=config_data.val)\n    test_data = tx.data.PairedTextData(hparams=config_data.test)\n    iterator = tx.data.FeedableDataIterator(\n        {\'train\': train_data, \'val\': val_data, \'test\': test_data})\n\n    batch = iterator.get_next()\n\n    outputs, sequence_length, infer_outputs = build_model(batch, train_data)\n\n    agent = tx.agents.SeqPGAgent(\n        samples=outputs.sample_id,\n        logits=outputs.logits,\n        sequence_length=sequence_length,\n        hparams=config_model.agent)\n\n    def _train_and_eval(sess, agent):\n        iterator.restart_dataset(sess, \'train\')\n\n        best_val_bleu = -1.\n        step = 0\n        while True:\n            try:\n                # Samples\n                extra_fetches = {\n                    \'truth\': batch[\'target_text_ids\'],\n                }\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'train\')\n                }\n                fetches = agent.get_samples(\n                    extra_fetches=extra_fetches, feed_dict=feed_dict)\n\n                sample_text = tx.utils.map_ids_to_strs(\n                    fetches[\'samples\'], train_data.target_vocab,\n                    strip_eos=False, join=False)\n                truth_text = tx.utils.map_ids_to_strs(\n                    fetches[\'truth\'], train_data.target_vocab,\n                    strip_eos=False, join=False)\n\n                # Computes rewards\n                reward = []\n                for ref, hyp in zip(truth_text, sample_text):\n                    r = tx.evals.sentence_bleu([ref], hyp, smooth=True)\n                    reward.append(r)\n\n                # Updates\n                loss = agent.observe(reward=reward)\n\n                # Displays & evaluates\n                step += 1\n                if step == 1 or step % config_data.display == 0:\n                    print(""step={}, loss={:.4f}, reward={:.4f}"".format(\n                        step, loss, np.mean(reward)))\n\n                if step % config_data.display_eval == 0:\n                    val_bleu = _eval_epoch(sess, \'val\')\n                    best_val_bleu = max(best_val_bleu, val_bleu)\n                    print(\'val step={}, BLEU={:.4f}; best-ever={:.4f}\'.format(\n                        step, val_bleu, best_val_bleu))\n\n                    test_bleu = _eval_epoch(sess, \'test\')\n                    print(\'test step={}, BLEU={:.4f}\'.format(step, test_bleu))\n                    print(\'=\' * 50)\n\n            except tf.errors.OutOfRangeError:\n                break\n\n    def _eval_epoch(sess, mode):\n        """"""`mode` is one of {\'val\', \'test\'}\n        """"""\n        iterator.restart_dataset(sess, mode)\n\n        refs, hypos = [], []\n        while True:\n            try:\n                fetches = [\n                    batch[\'target_text\'][:, 1:],\n                    infer_outputs.predicted_ids[:, :, 0]\n                ]\n                feed_dict = {\n                    tx.global_mode(): tf.estimator.ModeKeys.PREDICT,\n                    iterator.handle: iterator.get_handle(sess, mode)\n                }\n                target_texts, output_ids = \\\n                    sess.run(fetches, feed_dict=feed_dict)\n\n                target_texts = tx.utils.strip_special_tokens(target_texts)\n                output_texts = tx.utils.map_ids_to_strs(\n                    ids=output_ids, vocab=val_data.target_vocab)\n\n                for hypo, ref in zip(output_texts, target_texts):\n                    hypos.append(hypo)\n                    refs.append([ref])\n            except tf.errors.OutOfRangeError:\n                break\n\n        return tx.evals.corpus_bleu_moses(list_of_references=refs,\n                                          hypotheses=hypos)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        agent.sess = sess\n\n        _train_and_eval(sess, agent)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/seqgan/config_coco.py,0,"b'generator_pretrain_epoch = 80\ndiscriminator_pretrain_epoch = 80\nadversial_epoch = 100\n\nhidden_size = 32\nbatch_size = 64\nmax_num_steps = 20\n\nenc_keep_prob_in = 1.0\ndec_keep_prob_out = 1.0\n\nlog_dir = \'./coco_log/\'\nlog_file = log_dir + \'log.txt\'\nbleu_file = log_dir + \'bleu.txt\'\nckpt = \'./checkpoint/ckpt\'\n\ndec_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": dec_keep_prob_out},\n    ""num_layers"": 1\n}\n\nemb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": hidden_size,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': hidden_size**-0.5,\n        },\n    }\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'coco_data/coco.train.txt\',\n        ""vocab_file"": \'coco_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'coco_data/coco.valid.txt\',\n        ""vocab_file"": \'coco_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'coco_data/coco.test.txt\',\n        ""vocab_file"": \'coco_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ng_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.01\n        }\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    }\n}\n\nd_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0001\n        }\n    }\n}\n\nupdate_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0004\n        }\n    }\n}\n'"
examples/seqgan/config_ptb_large.py,0,"b'generator_pretrain_epoch = 55\ndiscriminator_pretrain_epoch = 15\nadversial_epoch = 20\n\nhidden_size = 1500\nbatch_size = 64\nmax_num_steps = 35\n\nenc_keep_prob_in = 1.0\ndec_keep_prob_out = 0.35\n\nlog_dir = \'./ptb_log.large/\'\nlog_file = log_dir + \'log.txt\'\nbleu_file = log_dir + \'bleu.txt\'\nckpt = \'./checkpoint/ckpt\'\n\ndec_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": dec_keep_prob_out},\n    ""num_layers"": 2\n}\n\nemb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": hidden_size,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': hidden_size**-0.5,\n        },\n    }\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.train.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.valid.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.test.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ng_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 10.}\n    }\n}\n\nd_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0001\n        }\n    }\n}\n\nupdate_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0004\n        }\n    }\n}\n'"
examples/seqgan/config_ptb_medium.py,0,"b'generator_pretrain_epoch = 39\ndiscriminator_pretrain_epoch = 15\nadversial_epoch = 20\n\nhidden_size = 650\nbatch_size = 64\nmax_num_steps = 35\n\nenc_keep_prob_in = 1.0\ndec_keep_prob_out = 0.5\n\nlog_dir = \'./ptb_log.medium/\'\nlog_file = log_dir + \'log.txt\'\nbleu_file = log_dir + \'bleu.txt\'\nckpt = \'./checkpoint/ckpt\'\n\ndec_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": dec_keep_prob_out},\n    ""num_layers"": 2\n}\n\nemb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": hidden_size,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': hidden_size**-0.5,\n        },\n    }\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.train.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.valid.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.test.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ng_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    }\n}\n\nd_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0001\n        }\n    }\n}\n\nupdate_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0004\n        }\n    }\n}\n'"
examples/seqgan/config_ptb_small.py,0,"b'generator_pretrain_epoch = 13\ndiscriminator_pretrain_epoch = 15\nadversial_epoch = 10\n\nhidden_size = 200\nbatch_size = 64\nmax_num_steps = 20\n\nenc_keep_prob_in = 1.0\ndec_keep_prob_out = 1.0\n\nlog_dir = \'./ptb_log.small/\'\nlog_file = log_dir + \'log.txt\'\nbleu_file = log_dir + \'bleu.txt\'\nckpt = \'./checkpoint/ckpt\'\n\ndec_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": dec_keep_prob_out},\n    ""num_layers"": 2\n}\n\nemb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": hidden_size,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': hidden_size**-0.5,\n        },\n    }\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.train.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.valid.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'ptb_data/ptb.test.txt\',\n        ""vocab_file"": \'ptb_data/vocab.txt\',\n        ""max_seq_length"": max_num_steps\n    }\n}\n\ng_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    }\n}\n\nd_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0001\n        }\n    }\n}\n\nupdate_opt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.0004\n        }\n    }\n}\n'"
examples/seqgan/data_utils.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""SeqGAN for language modeling\n""""""\nimport os\nimport argparse\nimport importlib\nimport tensorflow as tf\nimport texar.tf as tx\n\nparser = argparse.ArgumentParser(description=\'prepare data\')\nparser.add_argument(\'--dataset\', type=str, default=\'ptb\',\n                    help=\'dataset to prepare\')\nparser.add_argument(\'--data_path\', type=str, default=\'./\',\n                    help=""Directory containing coco. If not exists, ""\n                    ""the directory will be created, and the data ""\n                    ""will be downloaded."")\nparser.add_argument(\'--config\', type=str, default=\'config_ptb_small\',\n                    help=\'The config to use.\')\nargs = parser.parse_args()\n\nconfig = importlib.import_module(args.config)\n\n\ndef prepare_data(args, config, train_path):\n    """"""Downloads the PTB or COCO dataset\n    """"""\n    if not os.path.exists(config.log_dir):\n        os.mkdir(config.log_dir)\n\n    ptb_url = \'https://jxhe.github.io/download/ptb_data.tgz\'\n    coco_url = \'https://VegB.github.io/downloads/coco_data.tgz\'\n\n    data_path = args.data_path\n\n    if not tf.gfile.Exists(train_path):\n        url = ptb_url if args.dataset == \'ptb\' else coco_url\n        tx.data.maybe_download(url, data_path, extract=True)\n        os.remove(\'%s_data.tgz\' % args.dataset)\n\n\nif __name__ == \'__main__\':\n    prepare_data(args, config, config.train_data_hparams[\'dataset\'][\'files\'])\n'"
examples/seqgan/seqgan_train.py,34,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""SeqGAN for language modeling\n""""""\n\n# pylint: disable=invalid-name, no-member, too-many-locals\n\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nflags = tf.flags\nflags.DEFINE_string(""dataset"", ""ptb"",\n                    ""perform training on ptb or coco."")\nflags.DEFINE_string(""data_path"", ""./"",\n                    ""Directory containing coco. If not exists, ""\n                    ""the directory will be created, and the data ""\n                    ""will be downloaded."")\nflags.DEFINE_string(""config"", ""config_ptb_small"", ""The config to use."")\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    log = open(config.log_file, \'w\')\n    bleu_log = open(config.bleu_file, \'w\')\n\n    # Data\n    train_data = tx.data.MonoTextData(config.train_data_hparams)\n    val_data = tx.data.MonoTextData(config.val_data_hparams)\n    test_data = tx.data.MonoTextData(config.test_data_hparams)\n    iterator = tx.data.TrainTestDataIterator(train=train_data,\n                                             val=val_data,\n                                             test=test_data)\n    data_batch = iterator.get_next()\n\n    batch_size = tf.shape(data_batch[""text_ids""])[0]\n    num_steps = tf.shape(data_batch[""text_ids""])[1]\n    vocab_size = train_data.vocab.size\n\n    # Model architecture\n    g_embedder = tx.modules.WordEmbedder(vocab_size=vocab_size,\n                                         hparams=config.emb_hparams)\n    input_embed = g_embedder(data_batch[""text_ids""][:, :-1])\n\n    if config.enc_keep_prob_in < 1:\n        input_embed = tf.nn.dropout(\n            input_embed, tx.utils.switch_dropout(config.enc_keep_prob_in))\n\n    decoder = tx.modules.BasicRNNDecoder(\n        vocab_size=vocab_size,\n        hparams={""rnn_cell"": config.dec_cell_hparams,\n                 ""max_decoding_length_infer"": config.max_num_steps + 2})\n    initial_state = decoder.zero_state(batch_size=batch_size,\n                                       dtype=tf.float32)\n\n    # ------------Pretrain Generator---------------\n    outputs, _, _ = decoder(\n        initial_state=initial_state,\n        decoding_strategy=""train_greedy"",\n        inputs=input_embed,\n        sequence_length=data_batch[""length""] - 1)\n\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=data_batch[""text_ids""][:, 1:],\n        logits=outputs.logits,\n        sequence_length=data_batch[""length""] - 1)\n\n    global_step = tf.Variable(0, trainable=False)\n    g_variables = tx.utils.collect_trainable_variables([g_embedder, decoder])\n    gen_train_op = tx.core.get_train_op(mle_loss,\n                                        variables=g_variables,\n                                        global_step=global_step,\n                                        increment_global_step=True,\n                                        hparams=config.g_opt_hparams)\n\n    # -------------Generator Infer-------------------\n    start_tokens = tf.cast(tf.fill([batch_size],\n                                   train_data.vocab.bos_token_id),\n                           dtype=tf.int32)\n    infer_outputs, _, sequence_length = decoder(\n        decoding_strategy=""infer_sample"",\n        start_tokens=start_tokens,\n        end_token=train_data.vocab.eos_token_id,\n        embedding=g_embedder,\n        initial_state=initial_state,\n        max_decoding_length=config.max_num_steps)\n\n    infer_logits = infer_outputs.logits\n    infer_sample_ids = infer_outputs.sample_id\n\n    # ------------Pretrain Discriminator---------------\n    discriminator = tx.modules.UnidirectionalRNNClassifier(\n        hparams={""clas_strategy"": ""time_wise"", ""num_classes"": 1})\n    d_embedder = tx.modules.WordEmbedder(vocab_size=vocab_size,\n                                         hparams=config.emb_hparams)\n\n    r_logits, _ = discriminator(d_embedder(data_batch[""text_ids""][:, 1:]),\n                                sequence_length=data_batch[""length""] - 1)\n    f_logits, _ = discriminator(d_embedder(infer_sample_ids), sequence_length=sequence_length)\n\n    r_loss = tx.losses.sequence_sigmoid_cross_entropy(\n        labels=tf.ones_like(data_batch[""text_ids""][:, 1:], dtype=tf.float32),\n        logits=tf.squeeze(r_logits),\n        sequence_length=data_batch[""length""] - 1)  # r_preds -> 1.\n    f_loss = tx.losses.sequence_sigmoid_cross_entropy(\n        labels=tf.zeros_like(infer_sample_ids, dtype=tf.float32),\n        logits=tf.squeeze(f_logits),\n        sequence_length=sequence_length)  # infer_logits -> 0.\n    dis_loss = r_loss + f_loss\n    dis_loss.set_shape(())\n\n    d_variables = tx.utils.collect_trainable_variables([discriminator, d_embedder])\n    dis_train_op = tx.core.get_train_op(dis_loss,\n                                        variables=d_variables,\n                                        global_step=global_step,\n                                        increment_global_step=False,\n                                        hparams=config.d_opt_hparams)\n\n    # ------------Adeversarial---------------\n    infer_logits = tf.clip_by_value(\n        tf.nn.softmax(infer_logits) *\n        tf.one_hot(infer_sample_ids, vocab_size), 1e-20, 1)\n\n    expected_reward = tf.Variable(tf.zeros((config.max_num_steps,)))\n    reward = tf.reshape(f_logits, shape=(batch_size, -1)) - \\\n            expected_reward[:tf.shape(f_logits)[1]]\n    mean_reward = tf.reduce_mean(reward)\n    exp_reward_loss = -tf.reduce_mean(tf.abs(reward))\n    exp_reward_loss.set_shape(())\n    exp_op = tx.core.get_train_op(exp_reward_loss,\n                                  variables=[expected_reward],\n                                  global_step=global_step,\n                                  increment_global_step=False,\n                                  hparams=config.update_opt_hparams)\n    reward = tx.losses.discount_reward(\n        reward, sequence_length=tf.squeeze(sequence_length), tensor_rank=2)\n    update_loss = -tf.reduce_mean(tf.log(infer_logits) *\n                                  tf.expand_dims(reward, -1))\n    update_loss.set_shape(())\n    gen_op = tx.core.get_train_op(update_loss,\n                                  variables=g_variables,\n                                  global_step=global_step,\n                                  increment_global_step=True,\n                                  hparams=config.update_opt_hparams)\n    update_op = tf.group(gen_op, exp_op)\n\n    def _g_train_epoch(sess, epoch, mode_string):\n        iterator.switch_to_train_data(sess)\n        while True:\n            try:\n                if mode_string == \'train\':\n                    fetches = {\n                        \'mean_rwd\': mean_reward,\n                        \'exp_rwd_loss\': exp_reward_loss,\n                        \'update_loss\': update_loss,\n                        \'update_op\': update_op,\n                        \'exp_rwd\': expected_reward,\n                        \'step\': global_step\n                    }\n                elif mode_string == \'pretrain\':\n                    fetches = {\n                        \'mle_loss\': mle_loss,\n                        \'num_steps\': num_steps,\n                        \'train_op\': gen_train_op,\n                        \'step\': global_step\n                    }\n                else:\n                    raise ValueError(\n                        ""Expect mode_string to be one of ""\n                        ""[\'pretrain\', \'train\'], got %s"" % mode_string)\n                rtns = sess.run(fetches)\n                step = rtns[\'step\']\n                if step % 200 == 1:\n                    if mode_string == \'pretrain\':\n                        ppl = np.exp(rtns[\'mle_loss\'] / rtns[""num_steps""])\n                        rst = ""G {0:6s} epoch {1:3d}, step {2:3d}:"" \\\n                              "" train_ppl: {3:6f}"".format(mode_string,\n                                                          epoch, step, ppl)\n                    else:\n                        rst = ""G {0:6s} epoch {1:3d}, step {2:3d}: "" \\\n                              ""mean_reward: {3:6f}, "" \\\n                              ""expect_reward_loss:{4:6f}, "" \\\n                              ""update_loss: {5:6f}"".format(\n                                  mode_string, epoch, step, rtns[\'mean_rwd\'],\n                                  rtns[\'exp_rwd_loss\'], rtns[\'update_loss\'])\n                    log.write(rst + \'\\n\')\n                    log.flush()\n                    print(rst)\n                    if mode_string == \'train\':  # a batch per adversarial epoch\n                        break\n            except tf.errors.OutOfRangeError:\n                break\n        return\n\n    def _g_test_epoch(sess, epoch, mode_string):\n        def _id2word_map(id_arrays):\n            return [\' \'.join([train_data.vocab.id_to_token_map_py[i]\n                              for i in sent]) for sent in id_arrays]\n\n        if mode_string == \'valid\':\n            iterator.switch_to_val_data(sess)\n        elif mode_string == \'test\':\n            iterator.switch_to_test_data(sess)\n        else:\n            raise ValueError(""Expect mode_string to be one of ""\n                             ""[\'valid\', \'test\'], got %s"" % mode_string)\n\n        target_list, inference_list = [], []\n        loss, steps = 0., 0\n        while True:\n            try:\n                fetches = {\n                    ""mle_loss"": mle_loss,\n                    ""num_steps"": num_steps\n                }\n                if mode_string == \'test\':\n                    fetches[\'target_sample_id\'] = data_batch[""text_ids""]\n                    fetches[\'infer_sample_id\'] = infer_sample_ids\n\n                feed_dict = {tx.global_mode(): tf.estimator.ModeKeys.EVAL}\n\n                rtns = sess.run(fetches, feed_dict)\n\n                loss += rtns[\'mle_loss\']\n                steps += rtns[\'num_steps\']\n\n                if mode_string == \'test\':\n                    targets = _id2word_map(rtns[\'target_sample_id\'][:, 1:].tolist())  # remove <BOS>\n                    for t in targets:\n                        target_list.extend(t.split(\'<EOS>\')[0].strip().split())\n\n                    inferences = _id2word_map(rtns[\'infer_sample_id\'].tolist())\n                    for inf in inferences:\n                        inference_list.extend(inf.split(\'<EOS>\')[0].strip().split())\n\n            except tf.errors.OutOfRangeError:\n                break\n\n        ppl = np.exp(loss / steps)\n        rst = ""G {0:6s} epoch {1:3d}, step {2:3s}:"" \\\n              "" {3:5s}_ppl: {4:6f}""\\\n            .format(mode_string, epoch, \'-\', mode_string, ppl)\n        log.write(rst + \'\\n\')\n        log.flush()\n        print(rst)\n\n        if mode_string == \'test\':\n            bleu_test = tx.evals.sentence_bleu_moses(\n                references=[target_list],\n                hypothesis=inference_list,\n                lowercase=True, return_all=True)\n            if not isinstance(bleu_test, np.ndarray):  # might return 0.0 if inference_list is null\n                bleu_test = [bleu_test] * 5\n            rst_test = ""epoch %d BLEU1~4 on test dataset:\\n"" \\\n                       ""%f\\n%f\\n%f\\n%f\\n\\n"" % \\\n                       (epoch, bleu_test[1], bleu_test[2],\n                        bleu_test[3], bleu_test[4])\n            print(rst_test)\n            bleu_log.write(rst_test)\n            bleu_log.flush()\n\n        return\n\n    def _d_run_epoch(sess, epoch, mode_string=\'pretrain\'):\n        iterator.switch_to_train_data(sess)\n        step = 0\n        while True:\n            try:\n                fetches = {\n                    ""mle_loss"": dis_loss,\n                    ""r_loss"": r_loss,\n                    ""f_loss"": f_loss,\n                    ""train_op"": dis_train_op\n                }\n                rtns = sess.run(fetches)\n                if step % 200 == 0:\n                    rst = ""D {0:6s} epoch {1:3d}, step {2:3d}: "" \\\n                          ""dis_total_loss: {3:6f}, r_loss: {4:6f}, "" \\\n                          ""f_loss: {5:6f}"".format(\n                              mode_string, epoch, step, rtns[\'mle_loss\'],\n                              rtns[\'r_loss\'], rtns[\'f_loss\'])\n                    log.write(rst + \'\\n\')\n                    log.flush()\n                    print(rst)\n                step += 1\n                if step == 15 and mode_string == \'train\':\n                    break\n            except tf.errors.OutOfRangeError:\n                break\n\n    tf_config = tf.ConfigProto()\n    tf_config.gpu_options.allow_growth = True\n    with tf.Session(config=tf_config) as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        # Generator pre-training\n        for g_epoch in range(config.generator_pretrain_epoch):\n            _g_train_epoch(sess, g_epoch, \'pretrain\')\n            if g_epoch % 10 == 0 or \\\n                    g_epoch == config.generator_pretrain_epoch - 1:\n                _g_test_epoch(sess, g_epoch, \'valid\')\n                _g_test_epoch(sess, g_epoch, \'test\')\n\n        # Discriminator pre-training\n        for d_epoch in range(config.discriminator_pretrain_epoch):\n            _d_run_epoch(sess, d_epoch)\n\n        # Adversarial training\n        for update_epoch in range(config.adversial_epoch):\n            cur_epoch = update_epoch + config.generator_pretrain_epoch\n            _g_train_epoch(sess, cur_epoch, \'train\')\n            _d_run_epoch(sess, cur_epoch, mode_string=\'train\')\n            if update_epoch % 10 == 0 or \\\n                    update_epoch == config.adversial_epoch - 1:\n                _g_test_epoch(sess, cur_epoch, \'valid\')\n                _g_test_epoch(sess, cur_epoch, \'test\')\n\n    log.close()\n    bleu_log.close()\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/sequence_tagging/config.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""NER config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\nnum_epochs = 200\nchar_dim = 30\nembed_dim = 100\nhidden_size = 256\ntag_space = 128\nkeep_prob = 0.5\nbatch_size = 16\nencoder = None\nload_glove = True\n\nemb = {\n    ""name"": ""embedding"",\n    ""dim"": embed_dim,\n    ""dropout_rate"": 0.33,\n    ""dropout_strategy"": \'item\'\n}\n\nchar_emb = {\n    ""name"": ""char_embedding"",\n    ""dim"": char_dim\n}\n\nconv = {\n    ""filters"": 30,\n    ""kernel_size"": [3],\n    ""conv_activation"": ""tanh"",\n    ""num_dense_layers"": 0,\n    ""dropout_rate"": 0.\n}\n\ncell = {\n    ""type"": ""LSTMCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 1.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 1\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""MomentumOptimizer"",\n        ""kwargs"": {""learning_rate"": 0.1,\n                   ""momentum"": 0.9,\n                   ""use_nesterov"": True}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""inverse_time_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.05,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 1\n    }\n}\n'"
examples/sequence_tagging/conll_reader.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for preprocessing and iterating over the CoNLL 2003 data.\n""""""\n\nimport re\nfrom collections import defaultdict\nimport numpy as np\nimport tensorflow as tf\n\n\n# pylint: disable=invalid-name, too-many-locals\n\nMAX_CHAR_LENGTH = 45\nNUM_CHAR_PAD = 2\n\nUNK_WORD, UNK_CHAR, UNK_NER = 0, 0, 0\nPAD_WORD, PAD_CHAR, PAD_NER = 1, 1, 1\n\n# Regular expressions used to normalize digits.\nDIGIT_RE = re.compile(r""\\d"")\n\n\ndef create_vocabs(train_path, dev_path, test_path, normalize_digits=True, min_occur=1, glove_dict=None):\n    word_vocab = defaultdict(lambda: len(word_vocab))\n    word_count = defaultdict(lambda: 0)\n    char_vocab = defaultdict(lambda: len(char_vocab))\n    ner_vocab = defaultdict(lambda: len(ner_vocab))\n\n    UNK_WORD = word_vocab[""<unk>""]\n    PAD_WORD = word_vocab[""<pad>""]\n    UNK_CHAR = char_vocab[""<unk>""]\n    PAD_CHAR = char_vocab[""<pad>""]\n    UNK_NER = ner_vocab[""<unk>""]\n    PAD_NER = ner_vocab[""<pad>""]\n\n    print(""Creating Vocabularies:"")\n\n    for file_path in [train_path, dev_path, test_path]:\n        with open(file_path, \'r\') as file:\n            for line in file:\n                line = line.strip()\n                if len(line) == 0:\n                    continue\n\n                tokens = line.split(\' \')\n                for char in tokens[1]:\n                    cid = char_vocab[char]\n\n                word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n                ner = tokens[4]\n\n                if glove_dict is not None and (word in glove_dict or word.lower() in glove_dict):\n                    word_count[word] += min_occur + 1\n                elif file_path == train_path:\n                    word_count[word] += 1\n\n                nid = ner_vocab[ner]\n\n    print(""Total Vocabulary Size: %d"" % len(word_count))\n    for word in word_count:\n        if word_count[word] > min_occur:\n            wid = word_vocab[word]\n\n    print(""Word Vocabulary Size: %d"" % len(word_vocab))\n    print(""Character Alphabet Size: %d"" % len(char_vocab))\n    print(""NER Alphabet Size: %d"" % len(ner_vocab))\n\n    word_vocab = defaultdict(lambda: UNK_WORD, word_vocab)\n    char_vocab = defaultdict(lambda: UNK_CHAR, char_vocab)\n    ner_vocab = defaultdict(lambda: UNK_NER, ner_vocab)\n\n    i2w = {v: k for k, v in word_vocab.items()}\n    i2n = {v: k for k, v in ner_vocab.items()}\n    return (word_vocab, char_vocab, ner_vocab), (i2w, i2n)\n\n\ndef read_data(source_path, word_vocab, char_vocab, ner_vocab, normalize_digits=True):\n    data = []\n    print(\'Reading data from %s\' % source_path)\n    counter = 0\n    reader = CoNLLReader(source_path, word_vocab, char_vocab, ner_vocab)\n    inst = reader.getNext(normalize_digits)\n    while inst is not None:\n        counter += 1\n        sent = inst.sentence\n        data.append([sent.word_ids, sent.char_id_seqs, inst.ner_ids])\n        inst = reader.getNext(normalize_digits)\n\n    reader.close()\n    print(""Total number of data: %d"" % counter)\n    return data\n\n\ndef iterate_batch(data, batch_size, shuffle=False):\n    if shuffle:\n        np.random.shuffle(data)\n\n    for start_idx in range(0, len(data), batch_size):\n        excerpt = slice(start_idx, start_idx + batch_size)\n        batch = data[excerpt]\n\n        batch_length = max([len(batch[i][0]) for i in range(len(batch))])\n\n        wid_inputs = np.empty([len(batch), batch_length], dtype=np.int64)\n        cid_inputs = np.empty([len(batch), batch_length, MAX_CHAR_LENGTH], dtype=np.int64)\n        nid_inputs = np.empty([len(batch), batch_length], dtype=np.int64)\n        masks = np.zeros([len(batch), batch_length], dtype=np.float32)\n        lengths = np.empty(len(batch), dtype=np.int64)\n\n        for i, inst in enumerate(batch):\n            wids, cid_seqs, nids = inst\n\n            inst_size = len(wids)\n            lengths[i] = inst_size\n            # word ids\n            wid_inputs[i, :inst_size] = wids\n            wid_inputs[i, inst_size:] = PAD_WORD\n            for c, cids in enumerate(cid_seqs):\n                cid_inputs[i, c, :len(cids)] = cids\n                cid_inputs[i, c, len(cids):] = PAD_CHAR\n            cid_inputs[i, inst_size:, :] = PAD_CHAR\n            nid_inputs[i, :inst_size] = nids\n            nid_inputs[i, inst_size:] = PAD_NER\n            masks[i, :inst_size] = 1.0\n\n        yield wid_inputs, cid_inputs, nid_inputs, masks, lengths\n\n\ndef load_glove(filename, emb_dim, normalize_digits=True):\n    """"""Loads embeddings in the glove text format in which each line is\n    \'<word-string> <embedding-vector>\'. Dimensions of the embedding vector\n    are separated with whitespace characters.\n\n    Args:\n        filename (str): Path to the embedding file.\n        vocab (dict): A dictionary that maps token strings to integer index.\n            Tokens not in :attr:`vocab` are not read.\n        word_vecs: A 2D numpy array of shape `[vocab_size, embed_dim]`\n            which is updated as reading from the file.\n\n    Returns:\n        The updated :attr:`word_vecs`.\n    """"""\n    glove_dict = dict()\n    with tf.gfile.Open(filename) as fin:\n        for line in fin:\n            vec = line.strip().split()\n            if len(vec) == 0:\n                continue\n            word, vec = vec[0], vec[1:]\n            word = tf.compat.as_text(word)\n            word = DIGIT_RE.sub(""0"", word) if normalize_digits else word\n            glove_dict[word] = np.array([float(v) for v in vec])\n            if len(vec) != emb_dim:\n                raise ValueError(""Inconsistent word vector sizes: %d vs %d"" %\n                                 (len(vec), emb_dim))\n    return glove_dict\n\n\ndef construct_init_word_vecs(vocab, word_vecs, glove_dict):\n    for word, index in vocab.items():\n        if word in glove_dict:\n            embedding = glove_dict[word]\n        elif word.lower() in glove_dict:\n            embedding = glove_dict[word.lower()]\n        else:\n            embedding = None\n\n        if embedding is not None:\n            word_vecs[index] = embedding\n    return word_vecs\n\n\nclass CoNLLReader(object):\n    def __init__(self, file_path, word_vocab, char_vocab, ner_vocab):\n        self.__source_file = open(file_path, \'r\', encoding=\'utf-8\')\n        self.__word_vocab = word_vocab\n        self.__char_vocab = char_vocab\n        self.__ner_vocab = ner_vocab\n\n    def close(self):\n        self.__source_file.close()\n\n    def getNext(self, normalize_digits=True):\n        line = self.__source_file.readline()\n        # skip multiple blank lines.\n        while len(line) > 0 and len(line.strip()) == 0:\n            line = self.__source_file.readline()\n        if len(line) == 0:\n            return None\n\n        lines = []\n        while len(line.strip()) > 0:\n            line = line.strip()\n            lines.append(line.split(\' \'))\n            line = self.__source_file.readline()\n\n        length = len(lines)\n        if length == 0:\n            return None\n\n        words = []\n        word_ids = []\n        char_seqs = []\n        char_id_seqs = []\n        ner_tags = []\n        ner_ids = []\n\n        for tokens in lines:\n            chars = []\n            char_ids = []\n            for char in tokens[1]:\n                chars.append(char)\n                char_ids.append(self.__char_vocab[char])\n            if len(chars) > MAX_CHAR_LENGTH:\n                chars = chars[:MAX_CHAR_LENGTH]\n                char_ids = char_ids[:MAX_CHAR_LENGTH]\n            char_seqs.append(chars)\n            char_id_seqs.append(char_ids)\n\n            word = DIGIT_RE.sub(""0"", tokens[1]) if normalize_digits else tokens[1]\n            ner = tokens[4]\n\n            words.append(word)\n            word_ids.append(self.__word_vocab[word])\n\n            ner_tags.append(ner)\n            ner_ids.append(self.__ner_vocab[ner])\n\n        return NERInstance(Sentence(words, word_ids, char_seqs, char_id_seqs), ner_tags, ner_ids)\n\n\nclass NERInstance(object):\n    def __init__(self, sentence, ner_tags, ner_ids):\n        self.sentence = sentence\n        self.ner_tags = ner_tags\n        self.ner_ids = ner_ids\n\n    def length(self):\n        return self.sentence.length()\n\n\nclass Sentence(object):\n    def __init__(self, words, word_ids, char_seqs, char_id_seqs):\n        self.words = words\n        self.word_ids = word_ids\n        self.char_seqs = char_seqs\n        self.char_id_seqs = char_id_seqs\n\n    def length(self):\n        return len(self.words)\n'"
examples/sequence_tagging/conll_writer.py,0,"b'__author__ = \'max\'\n\n\nclass CoNLLWriter(object):\n    def __init__(self, i2w, i2n):\n        self.__source_file = None\n        self.__i2w = i2w\n        self.__i2n = i2n\n\n    def start(self, file_path):\n        self.__source_file = open(file_path, \'w\', encoding=\'utf-8\')\n\n    def close(self):\n        self.__source_file.close()\n\n    def write(self, word, predictions, targets, lengths):\n        batch_size, _ = word.shape\n        for i in range(batch_size):\n            for j in range(lengths[i]):\n                w = self.__i2w[word[i, j]]\n                tgt = self.__i2n[targets[i, j]]\n                pred = self.__i2n[predictions[i, j]]\n                self.__source_file.write(\'%d %s %s %s %s %s\\n\' % (j + 1, w, ""_"", ""_"", tgt, pred))\n            self.__source_file.write(\'\\n\')\n'"
examples/sequence_tagging/ner.py,27,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Sequence tagging.\n""""""\n\nimport os\nimport time\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom examples.sequence_tagging.conll_reader import create_vocabs, read_data, iterate_batch, load_glove, construct_init_word_vecs\nfrom examples.sequence_tagging.conll_writer import CoNLLWriter\nfrom examples.sequence_tagging import scores\n\nflags = tf.flags\n\nflags.DEFINE_string(""data_path"", ""./data"",\n                    ""Directory containing NER data (e.g., eng.train.bio.conll)."")\nflags.DEFINE_string(""train"", ""eng.train.bio.conll"",\n                    ""the file name of the training data."")\nflags.DEFINE_string(""dev"", ""eng.dev.bio.conll"",\n                    ""the file name of the dev data."")\nflags.DEFINE_string(""test"", ""eng.test.bio.conll"",\n                    ""the file name of the test data."")\nflags.DEFINE_string(""embedding"", ""glove.6B.100d.txt"",\n                    ""the file name of the GloVe embedding."")\nflags.DEFINE_string(""config"", ""config"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\ntrain_path = os.path.join(FLAGS.data_path, FLAGS.train)\ndev_path = os.path.join(FLAGS.data_path, FLAGS.dev)\ntest_path = os.path.join(FLAGS.data_path, FLAGS.test)\nembedding_path = os.path.join(FLAGS.data_path, FLAGS.embedding)\nEMBEDD_DIM = config.embed_dim\nCHAR_DIM = config.char_dim\n\n# Prepares/loads data\nif config.load_glove:\n    print(\'loading GloVe embedding...\')\n    glove_dict = load_glove(embedding_path, EMBEDD_DIM)\nelse:\n    glove_dict = None\n\n(word_vocab, char_vocab, ner_vocab), (i2w, i2n) = create_vocabs(train_path, dev_path, test_path, glove_dict=glove_dict)\n\ndata_train = read_data(train_path, word_vocab, char_vocab, ner_vocab)\ndata_dev = read_data(dev_path, word_vocab, char_vocab, ner_vocab)\ndata_test = read_data(test_path, word_vocab, char_vocab, ner_vocab)\n\nscale = np.sqrt(3.0 / EMBEDD_DIM)\nword_vecs = np.random.uniform(-scale, scale, [len(word_vocab), EMBEDD_DIM]).astype(np.float32)\nif config.load_glove:\n    word_vecs = construct_init_word_vecs(word_vocab, word_vecs, glove_dict)\n\nscale = np.sqrt(3.0 / CHAR_DIM)\nchar_vecs = np.random.uniform(-scale, scale, [len(char_vocab), CHAR_DIM]).astype(np.float32)\n\n# Builds TF graph\ninputs = tf.placeholder(tf.int64, [None, None])\nchars = tf.placeholder(tf.int64, [None, None, None])\ntargets = tf.placeholder(tf.int64, [None, None])\nmasks = tf.placeholder(tf.float32, [None, None])\nseq_lengths = tf.placeholder(tf.int64, [None])\n\nvocab_size = len(word_vecs)\nembedder = tx.modules.WordEmbedder(vocab_size=vocab_size, init_value=word_vecs, hparams=config.emb)\nemb_inputs = embedder(inputs)\n\nchar_size = len(char_vecs)\nchar_embedder = tx.modules.WordEmbedder(vocab_size=char_size, init_value=char_vecs, hparams=config.char_emb)\nemb_chars = char_embedder(chars)\nchar_shape = tf.shape(emb_chars)  # [batch, length, char_length, char_dim]\nemb_chars = tf.reshape(emb_chars, (-1, char_shape[2], CHAR_DIM))\nchar_encoder = tx.modules.Conv1DEncoder(config.conv)\nchar_outputs = char_encoder(emb_chars)\nchar_outputs = tf.reshape(char_outputs, (char_shape[0], char_shape[1], config.conv[\'filters\']))\n\nemb_inputs = tf.concat([emb_inputs, char_outputs], axis=2)\nemb_inputs = tf.nn.dropout(emb_inputs, keep_prob=0.67)\n\nencoder = tx.modules.BidirectionalRNNEncoder(hparams={""rnn_cell_fw"": config.cell, ""rnn_cell_bw"": config.cell})\noutputs, _ = encoder(emb_inputs, sequence_length=seq_lengths)\noutputs = tf.concat(outputs, axis=2)\n\nrnn_shape = tf.shape(outputs)\noutputs = tf.reshape(outputs, (-1, 2 * config.hidden_size))\n\noutputs = tf.layers.dense(outputs, config.tag_space, activation=tf.nn.elu)\noutputs = tf.nn.dropout(outputs, keep_prob=config.keep_prob)\n\nlogits = tf.layers.dense(outputs, len(ner_vocab))\n\nlogits = tf.reshape(logits, tf.concat([rnn_shape[0:2], [len(ner_vocab)]], axis=0))\n\nmle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n    labels=targets,\n    logits=logits,\n    sequence_length=seq_lengths,\n    average_across_batch=True,\n    average_across_timesteps=True,\n    sum_over_timesteps=False)\n\npredicts = tf.argmax(logits, axis=2)\ncorrects = tf.reduce_sum(tf.cast(tf.equal(targets, predicts), tf.float32) * masks)\n\nglobal_step = tf.placeholder(tf.int32)\ntrain_op = tx.core.get_train_op(\n    mle_loss, global_step=global_step, increment_global_step=False,\n    hparams=config.opt)\n\n# Training/eval processes\n\n\ndef _train_epoch(sess, epoch):\n    start_time = time.time()\n    loss = 0.\n    corr = 0.\n    num_tokens = 0.\n\n    fetches = {\n        ""mle_loss"": mle_loss,\n        ""correct"": corrects,\n    }\n    fetches[""train_op""] = train_op\n\n    mode = tf.estimator.ModeKeys.TRAIN\n    num_inst = 0\n    for batch in iterate_batch(data_train, config.batch_size, shuffle=True):\n        word, char, ner, mask, length = batch\n        feed_dict = {\n            inputs: word, chars: char, targets: ner, masks: mask, seq_lengths: length,\n            global_step: epoch, tx.global_mode(): mode,\n        }\n\n        rets = sess.run(fetches, feed_dict)\n        nums = np.sum(length)\n        num_inst += len(word)\n        loss += rets[""mle_loss""] * nums\n        corr += rets[""correct""]\n        num_tokens += nums\n\n        print(""train: %d (%d/%d) loss: %.4f, acc: %.2f%%"" % (epoch, num_inst, len(data_train), loss / num_tokens, corr / num_tokens * 100))\n    print(""train: %d loss: %.4f, acc: %.2f%%, time: %.2fs"" % (epoch, loss / num_tokens, corr / num_tokens * 100, time.time() - start_time))\n\n\ndef _eval(sess, epoch, data_tag):\n    fetches = {\n        ""predicts"": predicts,\n    }\n    mode = tf.estimator.ModeKeys.EVAL\n    file_name = \'tmp/%s%d\' % (data_tag, epoch)\n    writer = CoNLLWriter(i2w, i2n)\n    writer.start(file_name)\n    data = data_dev if data_tag == \'dev\' else data_test\n    for batch in iterate_batch(data, config.batch_size, shuffle=False):\n        word, char, ner, mask, length = batch\n        feed_dict = {\n            inputs: word, chars: char, targets: ner, masks: mask, seq_lengths: length,\n            global_step: epoch, tx.global_mode(): mode,\n        }\n        rets = sess.run(fetches, feed_dict)\n        predictions = rets[\'predicts\']\n        writer.write(word, predictions, ner, length)\n    writer.close()\n    acc, precision, recall, f1 = scores.scores(file_name)\n    print(\'%s acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%%\' % (data_tag, acc, precision, recall, f1))\n    return acc, precision, recall, f1\n\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.tables_initializer())\n\n    dev_f1 = 0.0\n    dev_acc = 0.0\n    dev_precision = 0.0\n    dev_recall = 0.0\n    best_epoch = 0\n\n    test_f1 = 0.0\n    test_acc = 0.0\n    test_prec = 0.0\n    test_recall = 0.0\n\n    tx.utils.maybe_create_dir(\'./tmp\')\n\n    for epoch in range(config.num_epochs):\n        _train_epoch(sess, epoch)\n        acc, precision, recall, f1 = _eval(sess, epoch, \'dev\')\n        if dev_f1 < f1:\n            dev_f1 = f1\n            dev_acc = acc\n            dev_precision = precision\n            dev_recall = recall\n            best_epoch = epoch\n            test_acc, test_prec, test_recall, test_f1 = _eval(sess, epoch, \'test\')\n        print(\'best acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%%, epoch: %d\' % (dev_acc, dev_precision, dev_recall, dev_f1, best_epoch))\n        print(\'test acc: %.2f%%, precision: %.2f%%, recall: %.2f%%, F1: %.2f%%, epoch: %d\' % (test_acc, test_prec, test_recall, test_f1, best_epoch))\n        print(\'---------------------------------------------------\')\n'"
examples/sequence_tagging/scores.py,0,"b""import subprocess\n\n\ndef scores(path):\n    bashCommand = 'perl conlleval'\n    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE,\n                               stdin=open(path))\n    output, error = process.communicate()\n    output = output.decode().split('\\n')[1].split('%; ')\n    output = [out.split(' ')[-1] for out in output]\n    acc, prec, recall, fb1 = tuple(output)\n    return float(acc), float(prec), float(recall), float(fb1)\n"""
examples/text_style_transfer/config.py,0,"b'""""""Config\n""""""\n\n# pylint: disable=invalid-name\n\nimport copy\n\n# Total number of training epochs (including pre-train and full-train)\nmax_nepochs = 12\npretrain_nepochs = 10  # Number of pre-train epochs (training as autoencoder)\ndisplay = 500  # Display the training results every N training steps.\n# Display the dev results every N training steps (set to a\n# very large value to disable it).\ndisplay_eval = 1e10\n\nsample_path = \'./samples\'\ncheckpoint_path = \'./checkpoints\'\nrestore = \'\'   # Model snapshot to restore from\n\nlambda_g = 0.1  # Weight of the classification loss\ngamma_decay = 0.5  # Gumbel-softmax temperature anneal rate\n\ntrain_data = {\n    \'batch_size\': 64,\n    # \'seed\': 123,\n    \'datasets\': [\n        {\n            \'files\': \'./data/yelp/sentiment.train.text\',\n            \'vocab_file\': \'./data/yelp/vocab\',\n            \'data_name\': \'\'\n        },\n        {\n            \'files\': \'./data/yelp/sentiment.train.labels\',\n            \'data_type\': \'int\',\n            \'data_name\': \'labels\'\n        }\n    ],\n    \'name\': \'train\'\n}\n\nval_data = copy.deepcopy(train_data)\nval_data[\'datasets\'][0][\'files\'] = \'./data/yelp/sentiment.dev.text\'\nval_data[\'datasets\'][1][\'files\'] = \'./data/yelp/sentiment.dev.labels\'\n\ntest_data = copy.deepcopy(train_data)\ntest_data[\'datasets\'][0][\'files\'] = \'./data/yelp/sentiment.test.text\'\ntest_data[\'datasets\'][1][\'files\'] = \'./data/yelp/sentiment.test.labels\'\n\nmodel = {\n    \'dim_c\': 200,\n    \'dim_z\': 500,\n    \'embedder\': {\n        \'dim\': 100,\n    },\n    \'encoder\': {\n        \'rnn_cell\': {\n            \'type\': \'GRUCell\',\n            \'kwargs\': {\n                \'num_units\': 700\n            },\n            \'dropout\': {\n                \'input_keep_prob\': 0.5\n            }\n        }\n    },\n    \'decoder\': {\n        \'rnn_cell\': {\n            \'type\': \'GRUCell\',\n            \'kwargs\': {\n                \'num_units\': 700,\n            },\n            \'dropout\': {\n                \'input_keep_prob\': 0.5,\n                \'output_keep_prob\': 0.5\n            },\n        },\n        \'attention\': {\n            \'type\': \'BahdanauAttention\',\n            \'kwargs\': {\n                \'num_units\': 700,\n            },\n            \'attention_layer_size\': 700,\n        },\n        \'max_decoding_length_train\': 21,\n        \'max_decoding_length_infer\': 20,\n    },\n    \'classifier\': {\n        \'kernel_size\': [3, 4, 5],\n        \'filters\': 128,\n        \'other_conv_kwargs\': {\'padding\': \'same\'},\n        \'dropout_conv\': [1],\n        \'dropout_rate\': 0.5,\n        \'num_dense_layers\': 0,\n        \'num_classes\': 1\n    },\n    \'opt\': {\n        \'optimizer\': {\n            \'type\':  \'AdamOptimizer\',\n            \'kwargs\': {\n                \'learning_rate\': 5e-4,\n            },\n        },\n    },\n}\n'"
examples/text_style_transfer/ctrl_gen_model.py,13,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Text style transfer\n""""""\n\n# pylint: disable=invalid-name, too-many-locals\n\nimport tensorflow as tf\n\nimport texar.tf as tx\nfrom texar.tf.modules import WordEmbedder, UnidirectionalRNNEncoder, \\\n        MLPTransformConnector, AttentionRNNDecoder, \\\n        GumbelSoftmaxEmbeddingHelper, Conv1DClassifier\nfrom texar.tf.core import get_train_op\nfrom texar.tf.utils import collect_trainable_variables, get_batch_size\n\n\nclass CtrlGenModel(object):\n    """"""Control\n    """"""\n\n    def __init__(self, inputs, vocab, gamma, lambda_g, hparams=None):\n        self._hparams = tx.HParams(hparams, None)\n        self._build_model(inputs, vocab, gamma, lambda_g)\n\n    def _build_model(self, inputs, vocab, gamma, lambda_g):\n        """"""Builds the model.\n        """"""\n        embedder = WordEmbedder(\n            vocab_size=vocab.size,\n            hparams=self._hparams.embedder)\n        encoder = UnidirectionalRNNEncoder(hparams=self._hparams.encoder)\n\n        # text_ids for encoder, with BOS token removed\n        enc_text_ids = inputs[\'text_ids\'][:, 1:]\n        enc_outputs, final_state = encoder(embedder(enc_text_ids),\n                                           sequence_length=inputs[\'length\'] - 1)\n        z = final_state[:, self._hparams.dim_c:]\n\n        # Encodes label\n        label_connector = MLPTransformConnector(self._hparams.dim_c)\n\n        # Gets the sentence representation: h = (c, z)\n        labels = tf.cast(tf.reshape(inputs[\'labels\'], [-1, 1]), tf.float32)\n        c = label_connector(labels)\n        c_ = label_connector(1 - labels)\n        h = tf.concat([c, z], 1)\n        h_ = tf.concat([c_, z], 1)\n\n        # Teacher-force decoding and the auto-encoding loss for G\n        decoder = AttentionRNNDecoder(\n            memory=enc_outputs,\n            memory_sequence_length=inputs[\'length\'] - 1,\n            cell_input_fn=lambda inputs, attention: inputs,\n            vocab_size=vocab.size,\n            hparams=self._hparams.decoder)\n\n        connector = MLPTransformConnector(decoder.state_size)\n\n        g_outputs, _, _ = decoder(\n            initial_state=connector(h), inputs=inputs[\'text_ids\'],\n            embedding=embedder, sequence_length=inputs[\'length\'] - 1)\n\n        loss_g_ae = tx.losses.sequence_sparse_softmax_cross_entropy(\n            labels=inputs[\'text_ids\'][:, 1:],\n            logits=g_outputs.logits,\n            sequence_length=inputs[\'length\'] - 1,\n            average_across_timesteps=True,\n            sum_over_timesteps=False)\n\n        # Gumbel-softmax decoding, used in training\n        start_tokens = tf.ones_like(inputs[\'labels\']) * vocab.bos_token_id\n        end_token = vocab.eos_token_id\n        gumbel_helper = GumbelSoftmaxEmbeddingHelper(\n            embedder.embedding, start_tokens, end_token, gamma)\n\n        soft_outputs_, _, soft_length_, = decoder(\n            helper=gumbel_helper, initial_state=connector(h_))\n\n        # Greedy decoding, used in eval\n        outputs_, _, length_ = decoder(\n            decoding_strategy=\'infer_greedy\', initial_state=connector(h_),\n            embedding=embedder, start_tokens=start_tokens, end_token=end_token)\n\n        # Creates classifier\n        classifier = Conv1DClassifier(hparams=self._hparams.classifier)\n        clas_embedder = WordEmbedder(vocab_size=vocab.size,\n                                     hparams=self._hparams.embedder)\n\n        # Classification loss for the classifier\n        clas_logits, clas_preds = classifier(\n            inputs=clas_embedder(ids=inputs[\'text_ids\'][:, 1:]),\n            sequence_length=inputs[\'length\'] - 1)\n        loss_d_clas = tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(inputs[\'labels\'], tf.float32), logits=clas_logits)\n        loss_d_clas = tf.reduce_mean(loss_d_clas)\n        accu_d = tx.evals.accuracy(labels=inputs[\'labels\'], preds=clas_preds)\n\n        # Classification loss for the generator, based on soft samples\n        soft_logits, soft_preds = classifier(\n            inputs=clas_embedder(soft_ids=soft_outputs_.sample_id),\n            sequence_length=soft_length_)\n        loss_g_clas = tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(1 - inputs[\'labels\'], tf.float32),\n            logits=soft_logits)\n        loss_g_clas = tf.reduce_mean(loss_g_clas)\n\n        # Accuracy on soft samples, for training progress monitoring\n        accu_g = tx.evals.accuracy(labels=1 - inputs[\'labels\'],\n                                   preds=soft_preds)\n\n        # Accuracy on greedy-decoded samples, for training progress monitoring\n        _, gdy_preds = classifier(\n            inputs=clas_embedder(ids=outputs_.sample_id),\n            sequence_length=length_)\n        accu_g_gdy = tx.evals.accuracy(\n            labels=1 - inputs[\'labels\'], preds=gdy_preds)\n\n        # Aggregates losses\n        loss_g = loss_g_ae + lambda_g * loss_g_clas\n        loss_d = loss_d_clas\n\n        # Creates optimizers\n        g_vars = collect_trainable_variables(\n            [embedder, encoder, label_connector, connector, decoder])\n        d_vars = collect_trainable_variables([clas_embedder, classifier])\n\n        train_op_g = get_train_op(\n            loss_g, g_vars, hparams=self._hparams.opt)\n        train_op_g_ae = get_train_op(\n            loss_g_ae, g_vars, hparams=self._hparams.opt)\n        train_op_d = get_train_op(\n            loss_d, d_vars, hparams=self._hparams.opt)\n\n        # Interface tensors\n        self.losses = {\n            ""loss_g"": loss_g,\n            ""loss_g_ae"": loss_g_ae,\n            ""loss_g_clas"": loss_g_clas,\n            ""loss_d"": loss_d_clas\n        }\n        self.metrics = {\n            ""accu_d"": accu_d,\n            ""accu_g"": accu_g,\n            ""accu_g_gdy"": accu_g_gdy,\n        }\n        self.train_ops = {\n            ""train_op_g"": train_op_g,\n            ""train_op_g_ae"": train_op_g_ae,\n            ""train_op_d"": train_op_d\n        }\n        self.samples = {\n            ""original"": inputs[\'text_ids\'][:, 1:],\n            ""transferred"": outputs_.sample_id\n        }\n\n        self.fetches_train_g = {\n            ""loss_g"": self.train_ops[""train_op_g""],\n            ""loss_g_ae"": self.losses[""loss_g_ae""],\n            ""loss_g_clas"": self.losses[""loss_g_clas""],\n            ""accu_g"": self.metrics[""accu_g""],\n            ""accu_g_gdy"": self.metrics[""accu_g_gdy""],\n        }\n        self.fetches_train_d = {\n            ""loss_d"": self.train_ops[""train_op_d""],\n            ""accu_d"": self.metrics[""accu_d""]\n        }\n        fetches_eval = {""batch_size"": get_batch_size(inputs[\'text_ids\'])}\n        fetches_eval.update(self.losses)\n        fetches_eval.update(self.metrics)\n        fetches_eval.update(self.samples)\n        self.fetches_eval = fetches_eval\n'"
examples/text_style_transfer/main.py,14,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Text style transfer\n\nThis is a simplified implementation of:\n\nToward Controlled Generation of Text, ICML2017\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing\n\nDownload the data with the cmd:\n\n$ python prepare_data.py\n\nTrain the model with the cmd:\n\n$ python main.py --config config\n""""""\n\n# pylint: disable=invalid-name, too-many-locals, too-many-arguments, no-member\n\nimport os\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom ctrl_gen_model import CtrlGenModel\n\nflags = tf.flags\n\nflags.DEFINE_string(\'config\', \'config\', \'The config to use.\')\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    # Data\n    train_data = tx.data.MultiAlignedData(config.train_data)\n    val_data = tx.data.MultiAlignedData(config.val_data)\n    test_data = tx.data.MultiAlignedData(config.test_data)\n    vocab = train_data.vocab(0)\n\n    # Each training batch is used twice: once for updating the generator and\n    # once for updating the discriminator. Feedable data iterator is used for\n    # such case.\n    iterator = tx.data.FeedableDataIterator(\n        {\'train_g\': train_data, \'train_d\': train_data,\n         \'val\': val_data, \'test\': test_data})\n    batch = iterator.get_next()\n\n    # Model\n    gamma = tf.placeholder(dtype=tf.float32, shape=[], name=\'gamma\')\n    lambda_g = tf.placeholder(dtype=tf.float32, shape=[], name=\'lambda_g\')\n    model = CtrlGenModel(batch, vocab, gamma, lambda_g, config.model)\n\n    def _train_epoch(sess, gamma_, lambda_g_, epoch, verbose=True):\n        avg_meters_d = tx.utils.AverageRecorder(size=10)\n        avg_meters_g = tx.utils.AverageRecorder(size=10)\n\n        step = 0\n        while True:\n            try:\n                step += 1\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'train_d\'),\n                    gamma: gamma_,\n                    lambda_g: lambda_g_\n                }\n\n                vals_d = sess.run(model.fetches_train_d, feed_dict=feed_dict)\n                avg_meters_d.add(vals_d)\n\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, \'train_g\'),\n                    gamma: gamma_,\n                    lambda_g: lambda_g_\n                }\n                vals_g = sess.run(model.fetches_train_g, feed_dict=feed_dict)\n                avg_meters_g.add(vals_g)\n\n                if verbose and (step == 1 or step % config.display == 0):\n                    print(\'step: {}, {}\'.format(step, avg_meters_d.to_str(4)))\n                    print(\'step: {}, {}\'.format(step, avg_meters_g.to_str(4)))\n\n                if verbose and step % config.display_eval == 0:\n                    iterator.restart_dataset(sess, \'val\')\n                    _eval_epoch(sess, gamma_, lambda_g_, epoch)\n\n            except tf.errors.OutOfRangeError:\n                print(\'epoch: {}, {}\'.format(epoch, avg_meters_d.to_str(4)))\n                print(\'epoch: {}, {}\'.format(epoch, avg_meters_g.to_str(4)))\n                break\n\n    def _eval_epoch(sess, gamma_, lambda_g_, epoch, val_or_test=\'val\'):\n        avg_meters = tx.utils.AverageRecorder()\n\n        while True:\n            try:\n                feed_dict = {\n                    iterator.handle: iterator.get_handle(sess, val_or_test),\n                    gamma: gamma_,\n                    lambda_g: lambda_g_,\n                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL\n                }\n\n                vals = sess.run(model.fetches_eval, feed_dict=feed_dict)\n\n                batch_size = vals.pop(\'batch_size\')\n\n                # Computes BLEU\n                samples = tx.utils.dict_pop(vals, list(model.samples.keys()))\n                hyps = tx.utils.map_ids_to_strs(samples[\'transferred\'], vocab)\n\n                refs = tx.utils.map_ids_to_strs(samples[\'original\'], vocab)\n                refs = np.expand_dims(refs, axis=1)\n\n                bleu = tx.evals.corpus_bleu_moses(refs, hyps)\n                vals[\'bleu\'] = bleu\n\n                avg_meters.add(vals, weight=batch_size)\n\n                # Writes samples\n                tx.utils.write_paired_text(\n                    refs.squeeze(), hyps,\n                    os.path.join(config.sample_path, \'val.%d\' % epoch),\n                    append=True, mode=\'v\')\n\n            except tf.errors.OutOfRangeError:\n                print(\'{}: {}\'.format(\n                    val_or_test, avg_meters.to_str(precision=4)))\n                break\n\n        return avg_meters.avg()\n\n    tf.gfile.MakeDirs(config.sample_path)\n    tf.gfile.MakeDirs(config.checkpoint_path)\n\n    # Runs the logics\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        saver = tf.train.Saver(max_to_keep=None)\n        if config.restore:\n            print(\'Restore from: {}\'.format(config.restore))\n            saver.restore(sess, config.restore)\n\n        iterator.initialize_dataset(sess)\n\n        gamma_ = 1.\n        lambda_g_ = 0.\n        for epoch in range(1, config.max_nepochs + 1):\n            if epoch > config.pretrain_nepochs:\n                # Anneals the gumbel-softmax temperature\n                gamma_ = max(0.001, gamma_ * config.gamma_decay)\n                lambda_g_ = config.lambda_g\n            print(\'gamma: {}, lambda_g: {}\'.format(gamma_, lambda_g_))\n\n            # Train\n            iterator.restart_dataset(sess, [\'train_g\', \'train_d\'])\n            _train_epoch(sess, gamma_, lambda_g_, epoch)\n\n            # Val\n            iterator.restart_dataset(sess, \'val\')\n            _eval_epoch(sess, gamma_, lambda_g_, epoch, \'val\')\n\n            saver.save(\n                sess, os.path.join(config.checkpoint_path, \'ckpt\'), epoch)\n\n            # Test\n            iterator.restart_dataset(sess, \'test\')\n            _eval_epoch(sess, gamma_, lambda_g_, epoch, \'test\')\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/text_style_transfer/prepare_data.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Downloads data.\n""""""\nimport texar.tf as tx\n\n# pylint: disable=invalid-name\n\n\ndef prepare_data():\n    """"""Downloads data.\n    """"""\n    tx.data.maybe_download(\n        urls=\'https://drive.google.com/file/d/\'\n             \'1HaUKEYDBEk6GlJGmXwqYteB-4rS9q8Lg/view?usp=sharing\',\n        path=\'./\',\n        filenames=\'yelp.zip\',\n        extract=True)\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    prepare_data()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/torchtext/batchfirst_bptt.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nfrom torchtext.data import BPTTIterator, Dataset, Batch\n\n\nclass BatchFirstBPTTIterator(BPTTIterator):\n    """"""Defines an iterator for language modeling tasks that use BPTT.\n\n    Provides contiguous streams of examples together with targets that are\n    one timestep further forward, for language modeling training with\n    backpropagation through time (BPTT). Expects a Dataset with a single\n    example and a single field called \'text\' and produces Batches with text and\n    target attributes.\n\n    All batches will have sizes [batch_size, bptt_len]\n\n    Attributes:\n        dataset: The Dataset object to load Examples from.\n        batch_size: Batch size.\n        bptt_len: Length of sequences for backpropagation through time.\n        sort_key: A key to use for sorting examples in order to batch together\n            examples with similar lengths and minimize padding. The sort_key\n            provided to the Iterator constructor overrides the sort_key\n            attribute of the Dataset, or defers to it if None.\n        train: Whether the iterator represents a train set.\n        repeat: Whether to repeat the iterator for multiple epochs.\n        shuffle: Whether to shuffle examples between epochs.\n        sort: Whether to sort examples according to self.sort_key.\n            Note that repeat, shuffle, and sort default to train, train, and\n            (not train).\n        device: Device to create batches on. Use -1 for CPU and None for the\n            currently active GPU device.\n    """"""\n\n    def __len__(self):\n        return math.floor(\n            (len(self.dataset[0].text) / self.batch_size - 1) / self.bptt_len)\n\n    def __iter__(self):\n        text = self.dataset[0].text\n        TEXT = self.dataset.fields[\'text\']\n        TEXT.eos_token = None\n        pad_num = int(math.ceil(len(text) / self.batch_size) *\n                      self.batch_size - len(text))\n        text = text + ([TEXT.pad_token] * pad_num)\n        data = TEXT.numericalize([text], device=self.device)\n        data = data.view(self.batch_size, -1).contiguous()\n        dataset = Dataset(examples=self.dataset.examples,\n                          fields=[(\'text\', TEXT), (\'target\', TEXT)])\n        while True:\n            for i in range(0, len(self) * self.bptt_len, self.bptt_len):\n                self.iterations += 1\n                seq_len = self.bptt_len\n                yield Batch.fromvars(\n                    dataset, self.batch_size,\n                    text=data[:, i:i + seq_len],\n                    target=data[:, i + 1:i + 1 + seq_len])\n            if not self.repeat:\n                return\n'"
examples/torchtext/config_small.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PTB LM small size config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ninit_scale = 0.1\nnum_epochs = 13\nhidden_size = 200\nkeep_prob = 1.0\nbatch_size = 20\nnum_steps = 20\n\ncell = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": keep_prob},\n    ""num_layers"": 2\n}\nemb = {\n    ""dim"": hidden_size\n}\nopt = {\n    ""optimizer"": {\n        ""type"": ""GradientDescentOptimizer"",\n        ""kwargs"": {""learning_rate"": 1.0}\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    },\n    ""learning_rate_decay"": {\n        ""type"": ""exponential_decay"",\n        ""kwargs"": {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.5,\n            ""staircase"": True\n        },\n        ""start_decay_step"": 3\n    }\n}\n'"
examples/torchtext/lm_torchtext.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Language Modeling example using torchtext\n""""""\n\nimport time\nimport importlib\nimport numpy as np\nimport tensorflow as tf\nimport texar.tf as tx\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nfrom batchfirst_bptt import BatchFirstBPTTIterator\n\n# pylint: disable=invalid-name, too-many-locals, no-member\n\nflags = tf.flags\n\nflags.DEFINE_string(""data_path"", ""./"",\n                    ""Directory containing PTB raw data (e.g., ptb.train.txt). ""\n                    ""E.g., ./simple-examples/data. If not exists, ""\n                    ""the directory will be created and PTB raw data will ""\n                    ""be downloaded."")\nflags.DEFINE_string(""config"", ""config_small"", ""The config to use."")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef _main(_):\n    # Data\n    batch_size = config.batch_size\n    num_steps = config.num_steps\n\n    # setup vocabulary and data iterators with torchtext\n    TEXT = data.Field()\n    # make splits for data\n    train, valid, test = datasets.PennTreebank.splits(TEXT)\n\n    # build the vocabulary\n    TEXT.build_vocab(train, vectors=None)\n    vocab_size = len(TEXT.vocab)\n\n    # make iterator for splits\n    train_iter, valid_iter, test_iter = BatchFirstBPTTIterator.splits(\n        (train, valid, test), batch_size=batch_size, bptt_len=num_steps,\n        repeat=False)\n\n    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n\n    # Model architecture\n    initializer = tf.random_uniform_initializer(\n        -config.init_scale, config.init_scale)\n    with tf.variable_scope(""model"", initializer=initializer):\n        embedder = tx.modules.WordEmbedder(\n            vocab_size=vocab_size, hparams=config.emb)\n        emb_inputs = embedder(inputs)\n        if config.keep_prob < 1:\n            emb_inputs = tf.nn.dropout(\n                emb_inputs, tx.utils.switch_dropout(config.keep_prob))\n\n        decoder = tx.modules.BasicRNNDecoder(\n            vocab_size=vocab_size, hparams={""rnn_cell"": config.cell})\n        initial_state = decoder.zero_state(batch_size, tf.float32)\n        outputs, final_state, seq_lengths = decoder(\n            decoding_strategy=""train_greedy"",\n            impute_finished=True,\n            inputs=emb_inputs,\n            sequence_length=[num_steps] * batch_size,\n            initial_state=initial_state)\n\n    # Losses & train ops\n    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=targets,\n        logits=outputs.logits,\n        sequence_length=seq_lengths)\n\n    # Use global_step to pass epoch, for lr decay\n    global_step = tf.placeholder(tf.int32)\n    train_op = tx.core.get_train_op(\n        mle_loss, global_step=global_step, increment_global_step=False,\n        hparams=config.opt)\n\n    def _run_epoch(sess, data_iter, epoch, is_train=False, verbose=False):\n        start_time = time.time()\n        loss = 0.\n        iters = 0\n        state = sess.run(initial_state)\n\n        fetches = {\n            ""mle_loss"": mle_loss,\n            ""final_state"": final_state,\n        }\n        if is_train:\n            fetches[""train_op""] = train_op\n\n        mode = (tf.estimator.ModeKeys.TRAIN\n                if is_train\n                else tf.estimator.ModeKeys.EVAL)\n        epoch_size = (len(train) // batch_size - 1) // num_steps\n        for step, data_batch in enumerate(data_iter):\n            feed_dict = {\n                inputs: data_batch.text,\n                targets: data_batch.target,\n                global_step: epoch,\n                tx.global_mode(): mode,\n            }\n            for i, (c, h) in enumerate(initial_state):\n                feed_dict[c] = state[i].c\n                feed_dict[h] = state[i].h\n\n            rets = sess.run(fetches, feed_dict)\n            loss += rets[""mle_loss""]\n            state = rets[""final_state""]\n            iters += num_steps\n\n            ppl = np.exp(loss / iters)\n            if verbose and step % (epoch_size // 10) == 10:\n                print(""%.3f perplexity: %.3f speed: %.0f wps"" %\n                      (step * 1.0 / epoch_size, ppl,\n                       iters * batch_size / (time.time() - start_time)))\n\n        ppl = np.exp(loss / iters)\n        return ppl\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        for epoch in range(config.num_epochs):\n            # Train\n            train_ppl = _run_epoch(\n                sess, train_iter, epoch, is_train=True, verbose=True)\n            print(""Epoch: %d Train Perplexity: %.3f"" % (epoch, train_ppl))\n            # Valid\n            valid_ppl = _run_epoch(sess, valid_iter, epoch)\n            print(""Epoch: %d Valid Perplexity: %.3f"" % (epoch, valid_ppl))\n        # Test\n        test_ppl = _run_epoch(sess, test_iter, 0)\n        print(""Test Perplexity: %.3f"" % (test_ppl))\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
examples/transformer/bleu_tool.py,0,"b'# Copyright 2018 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2018 Texar\n# ==============================================================================\n""""""BLEU metric utililities used for MT eval.\n\nUsage: python bleu_tool.py --translation=my-wmt13.de --reference=wmt13_deen.de\n""""""\n# This also:\n# Put compounds in ATAT format (comparable to papers like GNMT, ConvS2S).\n# See https://nlp.stanford.edu/projects/nmt/ :\n# \'Also, for historical reasons, we split compound words, e.g.,\n#    ""rich-text format"" --> rich ##AT##-##AT## text format.""\'\n# BLEU score will be similar to the one obtained using: mteval-v14.pl\n# Note:compound splitting is not implemented in this module\n\nfrom argparse import ArgumentParser\nfrom io import open\nimport collections\nimport math\nimport re\nimport sys\nimport unicodedata\n\n# Dependency imports\n\nimport numpy as np\nimport six\n# pylint: disable=redefined-builtin\nfrom six.moves import xrange\nfrom six.moves import zip\n\n\n# pylint: enable=redefined-builtin\n\n\ndef _get_ngrams(segment, max_order):\n    """"""Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  """"""\n    ngram_counts = collections.Counter()\n    for order in xrange(1, max_order + 1):\n        for i in xrange(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef compute_bleu(reference_corpus,\n                 translation_corpus,\n                 max_order=4,\n                 use_bp=True):\n    """"""Computes BLEU score of translated segments against references.\n\n    Args:\n        reference_corpus: list of references for each translation. Each\n            reference should be tokenized into a list of tokens.\n        translation_corpus: list of translations to score. Each translation\n            should be tokenized into a list of tokens.\n        max_order: Maximum n-gram order to use when computing BLEU score.\n        use_bp: boolean, whether to apply brevity penalty.\n    Returns:\n        BLEU score.\n    """"""\n\n    reference_length = 0\n    translation_length = 0\n    bp = 1.0\n    geo_mean = 0\n\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    precisions = []\n\n    for (references, translations) in zip(reference_corpus, translation_corpus):\n        reference_length += len(references)\n        translation_length += len(translations)\n        ref_ngram_counts = _get_ngrams(references, max_order)\n        translation_ngram_counts = _get_ngrams(translations, max_order)\n\n        overlap = dict((ngram,\n                        min(count, translation_ngram_counts[ngram]))\n                       for ngram, count in ref_ngram_counts.items())\n\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for ngram in translation_ngram_counts:\n            possible_matches_by_order[len(ngram) - 1] += \\\n                translation_ngram_counts[ngram]\n    precisions = [0] * max_order\n    smooth = 1.0\n    for i in xrange(0, max_order):\n        if possible_matches_by_order[i] > 0:\n            precisions[i] = matches_by_order[i] / possible_matches_by_order[i]\n            if matches_by_order[i] > 0:\n                precisions[i] = matches_by_order[i] / \\\n                    possible_matches_by_order[i]\n            else:\n                smooth *= 2\n                precisions[i] = 1.0 / (smooth * possible_matches_by_order[i])\n        else:\n            precisions[i] = 0.0\n\n    if max(precisions) > 0:\n        p_log_sum = sum(math.log(p) for p in precisions if p)\n        geo_mean = math.exp(p_log_sum / max_order)\n\n    if use_bp:\n        ratio = translation_length / reference_length\n        if ratio <= 0:\n            bp = 0\n        elif ratio < 1.0:\n            bp = math.exp(1 - 1. / ratio)\n        else:\n            bp = 1.0\n    bleu = geo_mean * bp\n    return np.float32(bleu)\n\n\nclass UnicodeRegex(object):\n    """"""Ad-hoc hack to recognize all punctuation and symbols.""""""\n    # pylint:disable=too-few-public-methods\n    def __init__(self):\n        punctuation = self.property_chars(""P"")\n        self.nondigit_punct_re = re.compile(r""([^\\d])(["" + punctuation + r""])"")\n        self.punct_nondigit_re = re.compile(r""(["" + punctuation + r""])([^\\d])"")\n        self.symbol_re = re.compile(""(["" + self.property_chars(""S"") + ""])"")\n\n    def property_chars(self, prefix):\n        # pylint:disable=no-self-use\n        return """".join(six.unichr(x) for x in range(sys.maxunicode)\n                       if unicodedata.category(\n            six.unichr(x)).startswith(prefix))\n\n\nuregex = UnicodeRegex()\n\n\ndef bleu_tokenize(string):\n    r""""""Tokenize a string following the official BLEU implementation.\n\n  See https://github.com/moses-smt/mosesdecoder/""\n           ""blob/master/scripts/generic/mteval-v14.pl#L954-L983\n  In our case, the input string is expected to be just one line\n  and no HTML entities de-escaping is needed.\n  So we just tokenize on punctuation and symbols,\n  except when a punctuation is preceded and followed by a digit\n  (e.g. a comma/dot as a thousand/decimal separator).\n\n  Note that a numer (e.g. a year) followed by a dot at the end of sentence\n  is NOT tokenized,\n  i.e. the dot stays with the number because `s/(\\p{P})(\\P{N})/ $1 $2/g`\n  does not match this case (unless we add a space after each sentence).\n  However, this error is already in the original mteval-v14.pl\n  and we want to be consistent with it.\n\n  Args:\n    string: the input string\n\n  Returns:\n    a list of tokens\n  """"""\n    string = uregex.nondigit_punct_re.sub(r""\\1 \\2 "", string)\n    string = uregex.punct_nondigit_re.sub(r"" \\1 \\2"", string)\n    string = uregex.symbol_re.sub(r"" \\1 "", string)\n    return string.split()\n\n\ndef bleu_wrapper(ref_filename, hyp_filename, case_sensitive=False):\n    """"""Compute BLEU for two files (reference and hypothesis translation).""""""\n    ref_lines = open(ref_filename, encoding=\'utf-8\').read().splitlines()\n    hyp_lines = open(hyp_filename, encoding=\'utf-8\').read().splitlines()\n    assert len(ref_lines) == len(hyp_lines)\n    if not case_sensitive:\n        ref_lines = [x.lower() for x in ref_lines]\n        hyp_lines = [x.lower() for x in hyp_lines]\n    ref_tokens = [bleu_tokenize(x) for x in ref_lines]\n    hyp_tokens = [bleu_tokenize(x) for x in hyp_lines]\n    return compute_bleu(ref_tokens, hyp_tokens)\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(description=\'Compute BLEU score. \\\n        Usage: t2t-bleu --translation=my-wmt13.de --reference=wmt13_deen.de\')\n\n    parser.add_argument(\'--translation\', type=str)\n    parser.add_argument(\'--reference\', type=str)\n    args = parser.parse_args()\n\n    bleu = 100 * bleu_wrapper(args.reference,\n                              args.translation,\n                              case_sensitive=False)\n    print(""BLEU_uncased = %6.2f"" % bleu)\n    bleu = 100 * bleu_wrapper(args.reference,\n                              args.translation,\n                              case_sensitive=True)\n    print(""BLEU_cased = %6.2f"" % bleu)\n'"
examples/transformer/config_iwslt15.py,0,"b'batch_size = 2048\ntest_batch_size = 64\n\nmax_train_epoch = 20\ndisplay_steps = 500\neval_steps = 2000\n\nmax_decoding_length = 256\n\nfilename_prefix = ""processed.""\ninput_dir = \'temp/run_en_vi_spm/data\'\nvocab_file = input_dir + \'/processed.vocab.pickle\'\n'"
examples/transformer/config_model.py,0,"b'""""""Configurations of Transformer model\n""""""\nimport copy\nimport texar.tf as tx\n\nrandom_seed = 1234\nbeam_width = 5\nlength_penalty = 0.6\nhidden_dim = 512\n\nemb = {\n    \'name\': \'lookup_table\',\n    \'dim\': hidden_dim,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': hidden_dim**-0.5,\n        },\n    }\n}\n\nposition_embedder_hparams = {\n    \'dim\': hidden_dim\n}\n\nencoder = {\n    \'dim\': hidden_dim,\n    \'num_blocks\': 6,\n    \'multihead_attention\': {\n        \'num_heads\': 8,\n        \'output_dim\': hidden_dim\n        # See documentation for more optional hyperparameters\n    },\n    \'initializer\': {\n        \'type\': \'variance_scaling_initializer\',\n        \'kwargs\': {\n            \'scale\': 1.0,\n            \'mode\': \'fan_avg\',\n            \'distribution\': \'uniform\',\n        },\n    },\n    \'poswise_feedforward\': tx.modules.default_transformer_poswise_net_hparams(\n        output_dim=hidden_dim)\n}\n\ndecoder = copy.deepcopy(encoder)\n\nloss_label_confidence = 0.9\n\nopt = {\n    \'optimizer\': {\n        \'type\': \'AdamOptimizer\',\n        \'kwargs\': {\n            \'beta1\': 0.9,\n            \'beta2\': 0.997,\n            \'epsilon\': 1e-9\n        }\n    }\n}\n\nlr = {\n    \'learning_rate_schedule\': \'constant.linear_warmup.rsqrt_decay.rsqrt_depth\',\n    \'lr_constant\': 2 * (hidden_dim ** -0.5),\n    \'static_lr\': 1e-3,\n    \'warmup_steps\': 16000,\n}\n'"
examples/transformer/config_wmt14.py,0,"b'batch_size = 3072\ntest_batch_size = 64\n\nmax_train_epoch = 10\ndisplay_steps = 500\neval_steps = 2000\n\nmax_decoding_length = 256\n\nfilename_prefix = ""processed.""\ninput_dir = \'temp/run_en_de_bpe/data\'\nvocab_file = input_dir + \'/processed.vocab.pickle\'\n'"
examples/transformer/transformer_main.py,34,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Transformer model.\n""""""\n\nimport pickle\nimport random\nimport os\nimport importlib\nimport tensorflow as tf\nfrom torchtext import data\nimport texar.tf as tx\nfrom texar.tf.modules import TransformerEncoder, TransformerDecoder\nfrom texar.tf.utils import transformer_utils\n\nfrom utils import data_utils, utils\nfrom utils.preprocess import bos_token_id, eos_token_id\nfrom bleu_tool import bleu_wrapper\n\n# pylint: disable=invalid-name, too-many-locals\n\nflags = tf.flags\n\nflags.DEFINE_string(""config_model"", ""config_model"", ""The model config."")\nflags.DEFINE_string(""config_data"", ""config_iwslt15"", ""The dataset config."")\nflags.DEFINE_string(""run_mode"", ""train_and_evaluate"",\n                    ""Either train_and_evaluate or test."")\nflags.DEFINE_string(""model_dir"", ""./outputs"",\n                    ""Directory to save the trained model and logs."")\n\nFLAGS = flags.FLAGS\n\nconfig_model = importlib.import_module(FLAGS.config_model)\nconfig_data = importlib.import_module(FLAGS.config_data)\n\nutils.set_random_seed(config_model.random_seed)\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    # Load data\n    train_data, dev_data, test_data = data_utils.load_data_numpy(\n        config_data.input_dir, config_data.filename_prefix)\n    with open(config_data.vocab_file, \'rb\') as f:\n        id2w = pickle.load(f)\n    vocab_size = len(id2w)\n\n    beam_width = config_model.beam_width\n\n    # Create logging\n    tx.utils.maybe_create_dir(FLAGS.model_dir)\n    logging_file = os.path.join(FLAGS.model_dir, \'logging.txt\')\n    logger = utils.get_logger(logging_file)\n    print(\'logging file is saved in: %s\', logging_file)\n\n    # Build model graph\n    encoder_input = tf.placeholder(tf.int64, shape=(None, None))\n    decoder_input = tf.placeholder(tf.int64, shape=(None, None))\n    batch_size = tf.shape(encoder_input)[0]\n    # (text sequence length excluding padding)\n    encoder_input_length = tf.reduce_sum(\n        1 - tf.cast(tf.equal(encoder_input, 0), tf.int32), axis=1)\n\n    labels = tf.placeholder(tf.int64, shape=(None, None))\n    is_target = tf.cast(tf.not_equal(labels, 0), tf.float32)\n\n    global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n    learning_rate = tf.placeholder(tf.float64, shape=(), name=\'lr\')\n\n    # Source word embedding\n    src_word_embedder = tx.modules.WordEmbedder(\n        vocab_size=vocab_size, hparams=config_model.emb)\n    src_word_embeds = src_word_embedder(encoder_input)\n    src_word_embeds = src_word_embeds * config_model.hidden_dim ** 0.5\n\n    # Position embedding (shared b/w source and target)\n    pos_embedder = tx.modules.SinusoidsPositionEmbedder(\n        position_size=config_data.max_decoding_length,\n        hparams=config_model.position_embedder_hparams)\n    src_seq_len = tf.ones([batch_size], tf.int32) * tf.shape(encoder_input)[1]\n    src_pos_embeds = pos_embedder(sequence_length=src_seq_len)\n\n    src_input_embedding = src_word_embeds + src_pos_embeds\n\n    encoder = TransformerEncoder(hparams=config_model.encoder)\n    encoder_output = encoder(inputs=src_input_embedding,\n                             sequence_length=encoder_input_length)\n\n    # The decoder ties the input word embedding with the output logit layer.\n    # As the decoder masks out <PAD>\'s embedding, which in effect means\n    # <PAD> has all-zero embedding, so here we explicitly set <PAD>\'s embedding\n    # to all-zero.\n    tgt_embedding = tf.concat(\n        [tf.zeros(shape=[1, src_word_embedder.dim]),\n         src_word_embedder.embedding[1:, :]],\n        axis=0)\n    tgt_embedder = tx.modules.WordEmbedder(tgt_embedding)\n    tgt_word_embeds = tgt_embedder(decoder_input)\n    tgt_word_embeds = tgt_word_embeds * config_model.hidden_dim ** 0.5\n\n    tgt_seq_len = tf.ones([batch_size], tf.int32) * tf.shape(decoder_input)[1]\n    tgt_pos_embeds = pos_embedder(sequence_length=tgt_seq_len)\n\n    tgt_input_embedding = tgt_word_embeds + tgt_pos_embeds\n\n    _output_w = tf.transpose(tgt_embedder.embedding, (1, 0))\n\n    decoder = TransformerDecoder(vocab_size=vocab_size,\n                                 output_layer=_output_w,\n                                 hparams=config_model.decoder)\n    # For training\n    outputs = decoder(\n        memory=encoder_output,\n        memory_sequence_length=encoder_input_length,\n        inputs=tgt_input_embedding,\n        decoding_strategy=\'train_greedy\',\n        mode=tf.estimator.ModeKeys.TRAIN\n    )\n\n    mle_loss = transformer_utils.smoothing_cross_entropy(\n        outputs.logits, labels, vocab_size, config_model.loss_label_confidence)\n    mle_loss = tf.reduce_sum(mle_loss * is_target) / tf.reduce_sum(is_target)\n\n    train_op = tx.core.get_train_op(\n        mle_loss,\n        learning_rate=learning_rate,\n        global_step=global_step,\n        hparams=config_model.opt)\n\n    tf.summary.scalar(\'lr\', learning_rate)\n    tf.summary.scalar(\'mle_loss\', mle_loss)\n    summary_merged = tf.summary.merge_all()\n\n    # For inference (beam-search)\n    start_tokens = tf.fill([batch_size], bos_token_id)\n\n    def _embedding_fn(x, y):\n        x_w_embed = tgt_embedder(x)\n        y_p_embed = pos_embedder(y)\n        return x_w_embed * config_model.hidden_dim ** 0.5 + y_p_embed\n\n    predictions = decoder(\n        memory=encoder_output,\n        memory_sequence_length=encoder_input_length,\n        beam_width=beam_width,\n        length_penalty=config_model.length_penalty,\n        start_tokens=start_tokens,\n        end_token=eos_token_id,\n        embedding=_embedding_fn,\n        max_decoding_length=config_data.max_decoding_length,\n        mode=tf.estimator.ModeKeys.PREDICT)\n    # Uses the best sample by beam search\n    beam_search_ids = predictions[\'sample_id\'][:, :, 0]\n\n    saver = tf.train.Saver(max_to_keep=5)\n    best_results = {\'score\': 0, \'epoch\': -1}\n\n    def _eval_epoch(sess, epoch, mode):\n        if mode == \'eval\':\n            eval_data = dev_data\n        elif mode == \'test\':\n            eval_data = test_data\n        else:\n            raise ValueError(\'`mode` should be either ""eval"" or ""test"".\')\n\n        references, hypotheses = [], []\n        bsize = config_data.test_batch_size\n        for i in range(0, len(eval_data), bsize):\n            sources, targets = zip(*eval_data[i:i + bsize])\n            x_block = data_utils.source_pad_concat_convert(sources)\n            feed_dict = {\n                encoder_input: x_block,\n                tx.global_mode(): tf.estimator.ModeKeys.EVAL,\n            }\n            fetches = {\n                \'beam_search_ids\': beam_search_ids,\n            }\n            fetches_ = sess.run(fetches, feed_dict=feed_dict)\n\n            hypotheses.extend(h.tolist() for h in fetches_[\'beam_search_ids\'])\n            references.extend(r.tolist() for r in targets)\n            hypotheses = utils.list_strip_eos(hypotheses, eos_token_id)\n            references = utils.list_strip_eos(references, eos_token_id)\n\n        if mode == \'eval\':\n            # Writes results to files to evaluate BLEU\n            # For \'eval\' mode, the BLEU is based on token ids (rather than\n            # text tokens) and serves only as a surrogate metric to monitor\n            # the training process\n            fname = os.path.join(FLAGS.model_dir, \'tmp.eval\')\n            hypotheses = tx.utils.str_join(hypotheses)\n            references = tx.utils.str_join(references)\n            hyp_fn, ref_fn = tx.utils.write_paired_text(\n                hypotheses, references, fname, mode=\'s\')\n            eval_bleu = bleu_wrapper(ref_fn, hyp_fn, case_sensitive=True)\n            eval_bleu = 100. * eval_bleu\n            logger.info(\'epoch: %d, eval_bleu %.4f\', epoch, eval_bleu)\n            print(\'epoch: %d, eval_bleu %.4f\' % (epoch, eval_bleu))\n\n            if eval_bleu > best_results[\'score\']:\n                logger.info(\'epoch: %d, best bleu: %.4f\', epoch, eval_bleu)\n                best_results[\'score\'] = eval_bleu\n                best_results[\'epoch\'] = epoch\n                model_path = os.path.join(FLAGS.model_dir, \'best-model.ckpt\')\n                logger.info(\'saving model to %s\', model_path)\n                print(\'saving model to %s\' % model_path)\n                saver.save(sess, model_path)\n\n        elif mode == \'test\':\n            # For \'test\' mode, together with the cmds in README.md, BLEU\n            # is evaluated based on text tokens, which is the standard metric.\n            fname = os.path.join(FLAGS.model_dir, \'test.output\')\n            hwords, rwords = [], []\n            for hyp, ref in zip(hypotheses, references):\n                hwords.append([id2w[y] for y in hyp])\n                rwords.append([id2w[y] for y in ref])\n            hwords = tx.utils.str_join(hwords)\n            rwords = tx.utils.str_join(rwords)\n            hyp_fn, ref_fn = tx.utils.write_paired_text(\n                hwords, rwords, fname, mode=\'s\',\n                src_fname_suffix=\'hyp\', tgt_fname_suffix=\'ref\')\n            logger.info(\'Test output writtn to file: %s\', hyp_fn)\n            print(\'Test output writtn to file: %s\' % hyp_fn)\n\n    def _train_epoch(sess, epoch, step, smry_writer):\n        random.shuffle(train_data)\n        train_iter = data.iterator.pool(\n            train_data,\n            config_data.batch_size,\n            key=lambda x: (len(x[0]), len(x[1])),\n            batch_size_fn=utils.batch_size_fn,\n            random_shuffler=data.iterator.RandomShuffler())\n\n        for _, train_batch in enumerate(train_iter):\n            in_arrays = data_utils.seq2seq_pad_concat_convert(train_batch)\n            feed_dict = {\n                encoder_input: in_arrays[0],\n                decoder_input: in_arrays[1],\n                labels: in_arrays[2],\n                learning_rate: utils.get_lr(step, config_model.lr)\n            }\n            fetches = {\n                \'step\': global_step,\n                \'train_op\': train_op,\n                \'smry\': summary_merged,\n                \'loss\': mle_loss,\n            }\n\n            fetches_ = sess.run(fetches, feed_dict=feed_dict)\n\n            step, loss = fetches_[\'step\'], fetches_[\'loss\']\n            if step and step % config_data.display_steps == 0:\n                logger.info(\'step: %d, loss: %.4f\', step, loss)\n                print(\'step: %d, loss: %.4f\' % (step, loss))\n                smry_writer.add_summary(fetches_[\'smry\'], global_step=step)\n\n            if step and step % config_data.eval_steps == 0:\n                _eval_epoch(sess, epoch, mode=\'eval\')\n        return step\n\n    # Run the graph\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        smry_writer = tf.summary.FileWriter(FLAGS.model_dir, graph=sess.graph)\n\n        if FLAGS.run_mode == \'train_and_evaluate\':\n            logger.info(\'Begin running with train_and_evaluate mode\')\n\n            if tf.train.latest_checkpoint(FLAGS.model_dir) is not None:\n                logger.info(\'Restore latest checkpoint in %s\' % FLAGS.model_dir)\n                saver.restore(sess, tf.train.latest_checkpoint(FLAGS.model_dir))\n\n            step = 0\n            for epoch in range(config_data.max_train_epoch):\n                step = _train_epoch(sess, epoch, step, smry_writer)\n\n        elif FLAGS.run_mode == \'test\':\n            logger.info(\'Begin running with test mode\')\n\n            logger.info(\'Restore latest checkpoint in %s\' % FLAGS.model_dir)\n            saver.restore(sess, tf.train.latest_checkpoint(FLAGS.model_dir))\n\n            _eval_epoch(sess, 0, mode=\'test\')\n\n        else:\n            raise ValueError(\'Unknown mode: {}\'.format(FLAGS.run_mode))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/vae_text/config_lstm_ptb.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""VAE config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ndataset = ""ptb""\nnum_epochs = 100\nhidden_size = 256\ndec_dropout_in = 0.5\ndec_dropout_out = 0.5\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nword_keep_prob = 0.5\nbatch_size = 32\nembed_dim = 256\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    ""init_lr"": 0.001,\n    ""threshold"": 2,\n    ""decay_factor"": 0.5,\n    ""max_decay"": 5\n}\n\n\ndecoder_type = \'lstm\'\n\nenc_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - enc_dropout_out},\n    ""num_layers"": 1\n}\n\ndec_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - dec_dropout_out},\n    ""num_layers"": 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\n# KL annealing\nkl_anneal_hparams = {\n    ""warm_up"": 10,\n    ""start"": 0.1\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./simple-examples/data/ptb.train.txt\',\n        ""vocab_file"": \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./simple-examples/data/ptb.valid.txt\',\n        ""vocab_file"": \'./simple-examples/data/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'./simple-examples/data/ptb.test.txt\',\n        ""vocab_file"": \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.001\n        }\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    }\n}\n'"
examples/vae_text/config_lstm_yahoo.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""VAE config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ndataset = ""yahoo""\nnum_epochs = 100\nhidden_size = 550\ndec_dropout_in = 0.5\ndec_dropout_out = 0.5\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nbatch_size = 32\nembed_dim = 512\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    ""init_lr"": 0.001,\n    ""threshold"": 2,\n    ""decay_factor"": 0.5,\n    ""max_decay"": 5\n}\n\n\nrelu_dropout = 0.2\nembedding_dropout = 0.2\nattention_dropout = 0.2\nresidual_dropout = 0.2\nnum_blocks = 3\n\ndecoder_type = \'lstm\'\n\nenc_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - enc_dropout_out},\n    ""num_layers"": 1\n}\n\ndec_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - dec_dropout_out},\n    ""num_layers"": 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\n\n# KL annealing\n# kl_weight = 1.0 / (1 + np.exp(-k*(step-x0)))\nkl_anneal_hparams = {\n    ""warm_up"": 10,\n    ""start"": 0.1\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.train.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.valid.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.test.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.001\n        }\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    }\n}\n'"
examples/vae_text/config_trans_ptb.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Config file of VAE with Trasnformer decoder, on PTB data.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ndataset = \'ptb\'\nnum_epochs = 100\nhidden_size = 256\ndec_dropout_in = 0.\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nbatch_size = 32\nembed_dim = 256\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    \'init_lr\': 0.001,\n    \'threshold\': 2,\n    \'decay_factor\': 0.5,\n    \'max_decay\': 5\n}\n\n\nrelu_dropout = 0.2\nembedding_dropout = 0.2\nattention_dropout = 0.2\nresidual_dropout = 0.2\nnum_blocks = 3\n\ndecoder_type = \'transformer\'\n\nenc_cell_hparams = {\n    \'type\': \'LSTMBlockCell\',\n    \'kwargs\': {\n        \'num_units\': hidden_size,\n        \'forget_bias\': 0.\n    },\n    \'dropout\': {\'output_keep_prob\': 1. - enc_dropout_out},\n    \'num_layers\': 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    \'dim\': embed_dim,\n    \'dropout_rate\': enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    \'dim\': embed_dim,\n    \'dropout_rate\': dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\nmax_pos = 200  # max sequence length in training data\ndec_pos_emb_hparams = {\n    \'dim\': hidden_size,\n}\n\n# due to the residual connection, the embed_dim should be equal to hidden_size\ntrans_hparams = {\n    \'output_layer_bias\': False,\n    \'embedding_dropout\': embedding_dropout,\n    \'residual_dropout\': residual_dropout,\n    \'num_blocks\': num_blocks,\n    \'dim\': hidden_size,\n    \'initializer\': {\n        \'type\': \'variance_scaling_initializer\',\n        \'kwargs\': {\n            \'scale\': 1.0,\n            \'mode\': \'fan_avg\',\n            \'distribution\': \'uniform\',\n        },\n    },\n    \'multihead_attention\': {\n        \'dropout_rate\': attention_dropout,\n        \'num_heads\': 8,\n        \'num_units\': hidden_size,\n        \'output_dim\': hidden_size\n    },\n    \'poswise_feedforward\': {\n        \'name\': \'fnn\',\n        \'layers\': [\n            {\n                \'type\': \'Dense\',\n                \'kwargs\': {\n                    \'name\': \'conv1\',\n                    \'units\': hidden_size * 4,\n                    \'activation\': \'relu\',\n                    \'use_bias\': True,\n                },\n            },\n            {\n                \'type\': \'Dropout\',\n                \'kwargs\': {\n                    \'rate\': relu_dropout,\n                }\n            },\n            {\n                \'type\': \'Dense\',\n                \'kwargs\': {\n                    \'name\': \'conv2\',\n                    \'units\': hidden_size,\n                    \'use_bias\': True,\n                    }\n            }\n        ],\n    }\n}\n\n# KL annealing\nkl_anneal_hparams = {\n    \'warm_up\': 10,\n    \'start\': 0.1\n}\n\ntrain_data_hparams = {\n    \'num_epochs\': 1,\n    \'batch_size\': batch_size,\n    \'seed\': 123,\n    \'dataset\': {\n        \'files\': \'./simple-examples/data/ptb.train.txt\',\n        \'vocab_file\': \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    \'num_epochs\': 1,\n    \'batch_size\': batch_size,\n    \'seed\': 123,\n    \'dataset\': {\n        \'files\': \'./simple-examples/data/ptb.valid.txt\',\n        \'vocab_file\': \'./simple-examples/data/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    \'num_epochs\': 1,\n    \'batch_size\': batch_size,\n    \'dataset\': {\n        \'files\': \'./simple-examples/data/ptb.test.txt\',\n        \'vocab_file\': \'./simple-examples/data/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    \'optimizer\': {\n        \'type\': \'AdamOptimizer\',\n        \'kwargs\': {\n            \'learning_rate\': 0.001\n        }\n    },\n    \'gradient_clip\': {\n        \'type\': \'clip_by_global_norm\',\n        \'kwargs\': {\'clip_norm\': 5.}\n    }\n}\n'"
examples/vae_text/config_trans_yahoo.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""VAE config.\n""""""\n\n# pylint: disable=invalid-name, too-few-public-methods, missing-docstring\n\ndataset = ""yahoo""\nnum_epochs = 100\nhidden_size = 512\ndec_dropout_in = 0.\nenc_dropout_in = 0.\nenc_dropout_out = 0.\nbatch_size = 32\nembed_dim = 512\n\nlatent_dims = 32\n\nlr_decay_hparams = {\n    ""init_lr"": 0.001,\n    ""threshold"": 2,\n    ""decay_factor"": 0.5,\n    ""max_decay"": 5\n}\n\n\nrelu_dropout = 0.2\nembedding_dropout = 0.2\nattention_dropout = 0.2\nresidual_dropout = 0.2\nnum_blocks = 3\n\ndecoder_type = \'transformer\'\n\nenc_cell_hparams = {\n    ""type"": ""LSTMBlockCell"",\n    ""kwargs"": {\n        ""num_units"": hidden_size,\n        ""forget_bias"": 0.\n    },\n    ""dropout"": {""output_keep_prob"": 1. - enc_dropout_out},\n    ""num_layers"": 1\n}\n\nenc_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": enc_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\ndec_emb_hparams = {\n    \'name\': \'lookup_table\',\n    ""dim"": embed_dim,\n    ""dropout_rate"": dec_dropout_in,\n    \'initializer\': {\n        \'type\': \'random_normal_initializer\',\n        \'kwargs\': {\n            \'mean\': 0.0,\n            \'stddev\': embed_dim**-0.5,\n        },\n    }\n}\n\n\nmax_pos = 200  # max sequence length in training data\ndec_pos_emb_hparams = {\n    \'dim\': hidden_size,\n}\n\n# due to the residual connection, the embed_dim should be equal to hidden_size\ntrans_hparams = {\n    \'output_layer_bias\': False,\n    \'embedding_dropout\': embedding_dropout,\n    \'residual_dropout\': residual_dropout,\n    \'num_blocks\': num_blocks,\n    \'dim\': hidden_size,\n    \'initializer\': {\n        \'type\': \'variance_scaling_initializer\',\n        \'kwargs\': {\n            \'scale\': 1.0,\n            \'mode\': \'fan_avg\',\n            \'distribution\': \'uniform\',\n        },\n    },\n    \'multihead_attention\': {\n        \'dropout_rate\': attention_dropout,\n        \'num_heads\': 8,\n        \'num_units\': hidden_size,\n        \'output_dim\': hidden_size\n    },\n    \'poswise_feedforward\': {\n        \'name\': \'fnn\',\n        \'layers\': [\n            {\n                \'type\': \'Dense\',\n                \'kwargs\': {\n                    \'name\': \'conv1\',\n                    \'units\': hidden_size * 4,\n                    \'activation\': \'relu\',\n                    \'use_bias\': True,\n                },\n            },\n            {\n                \'type\': \'Dropout\',\n                \'kwargs\': {\n                    \'rate\': relu_dropout,\n                }\n            },\n            {\n                \'type\': \'Dense\',\n                \'kwargs\': {\n                    \'name\': \'conv2\',\n                    \'units\': hidden_size,\n                    \'use_bias\': True,\n                    }\n            }\n        ],\n    }\n}\n\n# KL annealing\nkl_anneal_hparams = {\n    ""warm_up"": 10,\n    ""start"": 0.1\n}\n\ntrain_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.train.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nval_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""seed"": 123,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.valid.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\ntest_data_hparams = {\n    ""num_epochs"": 1,\n    ""batch_size"": batch_size,\n    ""dataset"": {\n        ""files"": \'./data/yahoo/yahoo.test.txt\',\n        ""vocab_file"": \'./data/yahoo/vocab.txt\'\n    }\n}\n\nopt_hparams = {\n    ""optimizer"": {\n        ""type"": ""AdamOptimizer"",\n        ""kwargs"": {\n            ""learning_rate"": 0.001\n        }\n    },\n    ""gradient_clip"": {\n        ""type"": ""clip_by_global_norm"",\n        ""kwargs"": {""clip_norm"": 5.}\n    }\n}\n'"
examples/vae_text/prepare_data.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for downloading and preprocessing the PTB and Yohoo data.\n""""""\nimport os\nimport argparse\n\nimport tensorflow as tf\nimport texar.tf as tx\n\n\ndef prepare_data(data_name):\n    """"""Prepare datasets.\n    Args:\n        data_path: the path to save the data\n        data_name: the name of dataset, ""ptb"" and ""yahoo""\n            are currently supported\n    """"""\n    if data_name == ""ptb"":\n        data_path = ""./simple-examples/data""\n        train_path = os.path.join(data_path, ""ptb.train.txt"")\n        if not tf.gfile.Exists(train_path):\n            url = \'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\'\n            tx.data.maybe_download(url, \'./\', extract=True)\n\n        train_path = os.path.join(data_path, ""ptb.train.txt"")\n        vocab_path = os.path.join(data_path, ""vocab.txt"")\n        word_to_id = tx.data.make_vocab(\n            train_path, return_type=""dict"")\n\n        with open(vocab_path, \'w\') as fvocab:\n            for word in word_to_id:\n                fvocab.write(""%s\\n"" % word)\n\n    elif data_name == ""yahoo"":\n        data_path = ""./data/yahoo""\n        train_path = os.path.join(data_path, ""yahoo.train.txt"")\n        if not tf.gfile.Exists(train_path):\n            url = \'https://drive.google.com/file/d/\'\\\n                  \'13IsiffVjcQ-wrrbBGMwiG3sYf-DFxtXH/view?usp=sharing\'\n            tx.data.maybe_download(url, path=\'./\', filenames=\'yahoo.zip\',\n                                   extract=True)\n    else:\n        raise ValueError(\'Unknown data: {}\'.format(data_name))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'prepare data\')\n    parser.add_argument(\'--data\', type=str, help=\'dataset to prepare\')\n    args = parser.parse_args()\n    prepare_data(args.data)\n'"
examples/vae_text/vae_train.py,37,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Example for building the Variational Autoencoder.\n\nThis is an impmentation of Variational Autoencoder for text generation\n\nTo run:\n\n$ python vae_train.py\n\nHyperparameters and data path may be specified in config_trans.py\n\n""""""\n\n# pylint: disable=invalid-name, no-member, too-many-locals\n# pylint: disable=too-many-branches, too-many-statements, redefined-variable-type\n\nimport os\nimport sys\nimport time\nimport importlib\nfrom io import open\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport texar.tf as tx\n\n\ntfd = tfp.distributions\n\nflags = tf.flags\n\nflags.DEFINE_string(""config"", ""config"", ""The config to use."")\nflags.DEFINE_string(""mode"", ""train"", ""train or predict"")\nflags.DEFINE_string(""model"", None, ""model path for generating sentences"")\nflags.DEFINE_string(""out"", None, ""generation output path"")\n\nFLAGS = flags.FLAGS\n\nconfig = importlib.import_module(FLAGS.config)\n\n\ndef kl_dvg(means, logvars):\n    """"""compute the KL divergence between Gaussian distribution\n    """"""\n    kl_cost = -0.5 * (logvars - tf.square(means) -\n                      tf.exp(logvars) + 1.0)\n    kl_cost = tf.reduce_mean(kl_cost, 0)\n\n    return tf.reduce_sum(kl_cost)\n\n\ndef _main(_):\n    # Data\n    train_data = tx.data.MonoTextData(config.train_data_hparams)\n    val_data = tx.data.MonoTextData(config.val_data_hparams)\n    test_data = tx.data.MonoTextData(config.test_data_hparams)\n    iterator = tx.data.TrainTestDataIterator(train=train_data,\n                                             val=val_data,\n                                             test=test_data)\n    data_batch = iterator.get_next()\n\n    opt_vars = {\n        \'learning_rate\': config.lr_decay_hparams[""init_lr""],\n        \'best_valid_nll\': 1e100,\n        \'steps_not_improved\': 0,\n        \'kl_weight\': config.kl_anneal_hparams[""start""]\n    }\n\n    decay_cnt = 0\n    max_decay = config.lr_decay_hparams[""max_decay""]\n    decay_factor = config.lr_decay_hparams[""decay_factor""]\n    decay_ts = config.lr_decay_hparams[""threshold""]\n\n    save_dir = ""./models/%s"" % config.dataset\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    suffix = ""%s_%sDecoder.ckpt"" % \\\n            (config.dataset, config.decoder_type)\n\n    save_path = os.path.join(save_dir, suffix)\n\n    # KL term annealing rate\n    anneal_r = 1.0 / (config.kl_anneal_hparams[""warm_up""] *\n                      (train_data.dataset_size() / config.batch_size))\n\n    # Model architecture\n    encoder_w_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.vocab.size, hparams=config.enc_emb_hparams)\n    input_embed = encoder_w_embedder(data_batch[""text_ids""])\n    encoder = tx.modules.UnidirectionalRNNEncoder(\n        hparams={""rnn_cell"": config.enc_cell_hparams})\n\n    decoder_w_embedder = tx.modules.WordEmbedder(\n        vocab_size=train_data.vocab.size, hparams=config.dec_emb_hparams)\n    output_w_embed = decoder_w_embedder(data_batch[""text_ids""][:, :-1])\n\n    if config.decoder_type == ""lstm"":\n        output_embed = output_w_embed\n\n        decoder = tx.modules.BasicRNNDecoder(\n            vocab_size=train_data.vocab.size,\n            hparams={""rnn_cell"": config.dec_cell_hparams})\n        decoder_initial_state_size = decoder.cell.state_size\n    elif config.decoder_type == \'transformer\':\n        # position embedding\n        decoder_p_embedder = tx.modules.SinusoidsPositionEmbedder(\n            position_size=config.max_pos, hparams=config.dec_pos_emb_hparams)\n        batch_size = tf.shape(data_batch[""text_ids""])[0]\n        max_seq_len = tf.shape(data_batch[""text_ids""])[1] - 1\n        batch_max_seq_len = tf.ones([batch_size], tf.int32) * max_seq_len\n        output_p_embed = decoder_p_embedder(sequence_length=batch_max_seq_len)\n\n        output_w_embed = output_w_embed * config.hidden_size ** 0.5\n        output_embed = output_w_embed + output_p_embed\n\n        # decoder\n        decoder = tx.modules.TransformerDecoder(\n            # tie word embedding with output layer\n            output_layer=tf.transpose(decoder_w_embedder.embedding, (1, 0)),\n            hparams=config.trans_hparams)\n        decoder_initial_state_size = tf.TensorShape(\n            [1, config.dec_emb_hparams[""dim""]])\n    else:\n        raise NotImplementedError\n\n    connector_mlp = tx.modules.MLPTransformConnector(\n        config.latent_dims * 2)\n\n    connector_stoch = tx.modules.ReparameterizedStochasticConnector(\n        decoder_initial_state_size)\n\n    # encoder -> connector -> decoder\n\n    _, ecdr_states = encoder(\n        input_embed,\n        sequence_length=data_batch[""length""])\n\n    mean_logvar = connector_mlp(ecdr_states)\n    mean, logvar = tf.split(mean_logvar, 2, 1)\n    kl_loss = kl_dvg(mean, logvar)\n\n    dst = tfd.MultivariateNormalDiag(\n        loc=mean,\n        scale_diag=tf.exp(0.5 * logvar))\n\n    dcdr_states, latent_z = connector_stoch(dst)\n\n    # decoder\n    if config.decoder_type == ""lstm"":\n        # concat latent variable to input at every time step\n        latent_z = tf.expand_dims(latent_z, axis=1)\n        latent_z = tf.tile(latent_z, [1, tf.shape(output_embed)[1], 1])\n        output_embed = tf.concat([output_embed, latent_z], axis=2)\n\n        outputs, _, _ = decoder(\n            initial_state=dcdr_states,\n            decoding_strategy=""train_greedy"",\n            inputs=output_embed,\n            sequence_length=data_batch[""length""] - 1)\n    else:\n        outputs = decoder(\n            inputs=output_embed,\n            memory=dcdr_states,\n            memory_sequence_length=tf.ones(tf.shape(dcdr_states)[0]))\n\n    logits = outputs.logits\n\n    seq_lengths = data_batch[""length""] - 1\n    # Losses & train ops\n    rc_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n        labels=data_batch[""text_ids""][:, 1:],\n        logits=logits,\n        sequence_length=data_batch[""length""] - 1)\n\n    # KL annealing\n    kl_weight = tf.placeholder(tf.float32, shape=())\n\n    nll = rc_loss + kl_weight * kl_loss\n\n    learning_rate = tf.placeholder(dtype=tf.float32, shape=(),\n                                   name=\'learning_rate\')\n    train_op = tx.core.get_train_op(nll, learning_rate=learning_rate,\n                                    hparams=config.opt_hparams)\n\n    def _run_epoch(sess, epoch, mode_string, display=10):\n        if mode_string == \'train\':\n            iterator.switch_to_train_data(sess)\n        elif mode_string == \'valid\':\n            iterator.switch_to_val_data(sess)\n        elif mode_string == \'test\':\n            iterator.switch_to_test_data(sess)\n\n        step = 0\n        start_time = time.time()\n        num_words = num_sents = 0\n        nll_ = 0.\n        kl_loss_ = rc_loss_ = 0.\n\n        while True:\n            try:\n                fetches = {""nll"": nll,\n                           ""kl_loss"": kl_loss,\n                           ""rc_loss"": rc_loss,\n                           ""lengths"": seq_lengths}\n\n                if mode_string == \'train\':\n                    fetches[""train_op""] = train_op\n                    opt_vars[""kl_weight""] = min(\n                        1.0, opt_vars[""kl_weight""] + anneal_r)\n\n                    kl_weight_ = opt_vars[""kl_weight""]\n                else:\n                    kl_weight_ = 1.0\n\n                mode = (tf.estimator.ModeKeys.TRAIN if mode_string == \'train\'\n                        else tf.estimator.ModeKeys.EVAL)\n\n                feed = {tx.global_mode(): mode,\n                        kl_weight: kl_weight_,\n                        learning_rate: opt_vars[""learning_rate""]}\n\n                fetches_ = sess.run(fetches, feed_dict=feed)\n\n                batch_size_ = len(fetches_[""lengths""])\n                num_sents += batch_size_\n\n                num_words += sum(fetches_[""lengths""])\n                nll_ += fetches_[""nll""] * batch_size_\n                kl_loss_ += fetches_[""kl_loss""] * batch_size_\n                rc_loss_ += fetches_[""rc_loss""] * batch_size_\n\n                if step % display == 0 and mode_string == \'train\':\n                    print(\'%s: epoch %d, step %d, nll %.4f, klw: %.4f, \'\n                          \'KL %.4f,  rc %.4f, log_ppl %.4f, ppl %.4f, \'\n                          \'time elapsed: %.1fs\' %\n                          (mode_string, epoch, step, nll_ / num_sents,\n                           opt_vars[""kl_weight""], kl_loss_ / num_sents,\n                           rc_loss_ / num_sents, nll_ / num_words,\n                           np.exp(nll_ / num_words), time.time() - start_time))\n\n                    sys.stdout.flush()\n\n                step += 1\n\n            except tf.errors.OutOfRangeError:\n                print(\'\\n%s: epoch %d, nll %.4f, KL %.4f, rc %.4f, \'\n                      \'log_ppl %.4f, ppl %.4f\\n\' %\n                      (mode_string, epoch, nll_ / num_sents,\n                       kl_loss_ / num_sents, rc_loss_ / num_sents,\n                       nll_ / num_words, np.exp(nll_ / num_words)))\n                break\n\n        return nll_ / num_sents, np.exp(nll_ / num_words)\n\n    def _generate(sess, saver, fname=None):\n        if tf.train.checkpoint_exists(FLAGS.model):\n            saver.restore(sess, FLAGS.model)\n        else:\n            raise ValueError(""cannot find checkpoint model"")\n\n        batch_size = train_data.batch_size\n\n        dst = tfd.MultivariateNormalDiag(\n            loc=tf.zeros([batch_size, config.latent_dims]),\n            scale_diag=tf.ones([batch_size, config.latent_dims]))\n\n        dcdr_states, latent_z = connector_stoch(dst)\n\n        vocab = train_data.vocab\n        start_tokens = tf.ones(batch_size, tf.int32) * vocab.bos_token_id\n        end_token = vocab.eos_token_id\n\n        if config.decoder_type == ""lstm"":\n            def _cat_embedder(ids):\n                """"""Concatenates latent variable to input word embeddings\n                """"""\n                embedding = decoder_w_embedder(ids)\n                return tf.concat([embedding, latent_z], axis=1)\n\n            outputs, _, _ = decoder(\n                initial_state=dcdr_states,\n                decoding_strategy=""infer_sample"",\n                embedding=_cat_embedder,\n                max_decoding_length=100,\n                start_tokens=start_tokens,\n                end_token=end_token)\n        else:\n            def _embedding_fn(ids, times):\n                w_embed = decoder_w_embedder(ids)\n                p_embed = decoder_p_embedder(times)\n                return w_embed * config.hidden_size ** 0.5 + p_embed\n\n            outputs, _ = decoder(\n                memory=dcdr_states,\n                decoding_strategy=""infer_sample"",\n                memory_sequence_length=tf.ones(tf.shape(dcdr_states)[0]),\n                embedding=_embedding_fn,\n                max_decoding_length=100,\n                start_tokens=start_tokens,\n                end_token=end_token)\n\n        sample_tokens = vocab.map_ids_to_tokens(outputs.sample_id)\n        sess.run(tf.tables_initializer())\n\n        feed = {tx.global_mode(): tf.estimator.ModeKeys.PREDICT}\n        sample_tokens_ = sess.run(sample_tokens, feed_dict=feed)\n\n        if fname is None:\n            fh = sys.stdout\n        else:\n            fh = open(fname, \'w\', encoding=\'utf-8\')\n\n        for sent in sample_tokens_:\n            sent = tx.utils.compat_as_text(list(sent))\n            end_id = len(sent)\n            if vocab.eos_token in sent:\n                end_id = sent.index(vocab.eos_token)\n            fh.write(\' \'.join(sent[:end_id + 1]) + \'\\n\')\n\n        print(\'Output done\')\n        fh.close()\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        # generate samples from prior\n        if FLAGS.mode == ""predict"":\n            _generate(sess, saver, FLAGS.out)\n            return\n\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.tables_initializer())\n\n        # Counts trainable parameters\n        total_parameters = 0\n        for variable in tf.trainable_variables():\n            shape = variable.get_shape()  # shape is an array of tf.Dimension\n            variable_parameters = 1\n            for dim in shape:\n                variable_parameters *= dim.value\n            total_parameters += variable_parameters\n        print(""%d total parameters"" % total_parameters)\n\n        best_nll = best_ppl = 0.\n\n        for epoch in range(config.num_epochs):\n            _, _ = _run_epoch(sess, epoch, \'train\', display=200)\n            val_nll, _ = _run_epoch(sess, epoch, \'valid\')\n            test_nll, test_ppl = _run_epoch(sess, epoch, \'test\')\n\n            if val_nll < opt_vars[\'best_valid_nll\']:\n                opt_vars[\'best_valid_nll\'] = val_nll\n                opt_vars[\'steps_not_improved\'] = 0\n                best_nll = test_nll\n                best_ppl = test_ppl\n                saver.save(sess, save_path)\n            else:\n                opt_vars[\'steps_not_improved\'] += 1\n                if opt_vars[\'steps_not_improved\'] == decay_ts:\n                    old_lr = opt_vars[\'learning_rate\']\n                    opt_vars[\'learning_rate\'] *= decay_factor\n                    opt_vars[\'steps_not_improved\'] = 0\n                    new_lr = opt_vars[\'learning_rate\']\n\n                    print(\'-----\\nchange lr, old lr: %f, new lr: %f\\n-----\' %\n                          (old_lr, new_lr))\n\n                    saver.restore(sess, save_path)\n\n                    decay_cnt += 1\n                    if decay_cnt == max_decay:\n                        break\n\n        print(\'\\nbest testing nll: %.4f, best testing ppl %.4f\\n\' %\n              (best_nll, best_ppl))\n\n\nif __name__ == \'__main__\':\n    tf.app.run(main=_main)\n'"
tests/agents/agent_utils_test.py,3,"b'#\n""""""\nUnit tests for agent utilities.\n""""""\n\n# pylint: disable=no-member, invalid-name, too-many-arguments\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.agents.agent_utils import Space\n\n\nclass SpaceTest(tf.test.TestCase):\n    """"""Tests the Space class.\n    """"""\n\n    def _test_space(self, s, shape, low, high, dtype):\n        self.assertEqual(s.shape, shape)\n        self.assertEqual(s.low, low)\n        self.assertEqual(s.high, high)\n        self.assertEqual(s.dtype, dtype)\n\n    def test_space(self):\n        """"""Tests descrete space.\n        """"""\n        s = Space(shape=(), low=0, high=10, dtype=np.int32)\n        self._test_space(s, (), 0, 10, np.dtype(np.int32))\n        self.assertTrue(s.contains(5))\n        self.assertFalse(s.contains(5.))\n        self.assertFalse(s.contains(15))\n\n        s = Space(low=0, high=10, dtype=np.int32)\n        self._test_space(s, (), 0, 10, np.dtype(np.int32))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/agents/seq_pg_agent_test.py,12,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for sequence prediction policy gradient agents.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.rnn_decoders import BasicRNNDecoder\nfrom texar.tf.agents import SeqPGAgent\nfrom texar.tf import context\n\n\nclass SeqPGAgentTest(tf.test.TestCase):\n    """"""Tests :class:`texar.tf.agents.SeqPGAgent`\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._vocab_size = 4\n        self._max_time = 8\n        self._batch_size = 16\n        self._emb_dim = 20\n        self._inputs = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1., dtype=tf.float32)\n        self._embedding = tf.random_uniform(\n            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)\n\n    def test_seq_pg_agent(self):\n        """"""Tests logits.\n        """"""\n        decoder = BasicRNNDecoder(vocab_size=self._vocab_size)\n        outputs, _, sequence_length = decoder(\n            decoding_strategy=""infer_greedy"",\n            max_decoding_length=10,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2)\n\n        agent = SeqPGAgent(\n            outputs.sample_id, outputs.logits, sequence_length,\n            decoder.trainable_variables)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            agent.sess = sess\n\n            feed_dict = {context.global_mode(): tf.estimator.ModeKeys.TRAIN}\n            for _ in range(2):\n                vals = agent.get_samples(feed_dict=feed_dict)\n                self.assertEqual(vals[\'samples\'].shape[0], self._batch_size)\n\n                loss_1 = agent.observe([1.] * self._batch_size)\n                loss_2 = agent.observe(\n                    [1.] * self._batch_size, train_policy=False)\n                self.assertEqual(loss_1.shape, ())\n                self.assertEqual(loss_2.shape, ())\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/core/layers_test.py,62,"b'#\n""""""\nUnit tests for various layers.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.contrib.rnn as rnn\n\nimport texar.tf as tx\nfrom texar.tf import context\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.core import layers\n\n# pylint: disable=no-member, protected-access, invalid-name\n# pylint: disable=redefined-variable-type\n\n\nclass GetRNNCellTest(tf.test.TestCase):\n    """"""Tests RNN cell creator.\n    """"""\n\n    def test_get_rnn_cell(self):\n        """"""Tests :func:`texar.tf.core.layers.get_rnn_cell`.\n        """"""\n        emb_dim = 4\n        num_units = 64\n\n        # Given instance\n        hparams = {\n            ""type"": rnn.LSTMCell(num_units)\n        }\n        cell = layers.get_rnn_cell(hparams)\n        self.assertTrue(isinstance(cell, rnn.LSTMCell))\n\n        # Given class\n        hparams = {\n            ""type"": rnn.LSTMCell,\n            ""kwargs"": {""num_units"": 10}\n        }\n        cell = layers.get_rnn_cell(hparams)\n        self.assertTrue(isinstance(cell, rnn.LSTMCell))\n\n        # Given string, and complex hyperparameters\n        keep_prob_x = tf.placeholder(\n            name=\'keep_prob\', shape=[], dtype=tf.float32)\n        hparams = {\n            ""type"": ""tensorflow.contrib.rnn.GRUCell"",\n            ""kwargs"": {\n                ""num_units"": num_units\n            },\n            ""num_layers"": 2,\n            ""dropout"": {\n                ""input_keep_prob"": 0.8,\n                ""state_keep_prob"": keep_prob_x,\n                ""variational_recurrent"": True,\n                ""input_size"": [emb_dim, num_units]\n            },\n            ""residual"": True,\n            ""highway"": True\n        }\n\n        hparams_ = HParams(hparams, layers.default_rnn_cell_hparams())\n        cell = layers.get_rnn_cell(hparams_)\n\n        batch_size = 16\n        inputs = tf.zeros([batch_size, emb_dim], dtype=tf.float32)\n        output, state = cell(inputs,\n                             cell.zero_state(batch_size, dtype=tf.float32))\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            feed_dict = {\n                keep_prob_x: 1.0,\n                context.global_mode(): tf.estimator.ModeKeys.TRAIN\n            }\n            output_, state_ = sess.run([output, state], feed_dict=feed_dict)\n\n            self.assertEqual(output_.shape[0], batch_size)\n            if isinstance(state_, (list, tuple)):\n                self.assertEqual(state_[0].shape[0], batch_size)\n                self.assertEqual(state_[0].shape[1],\n                                 hparams_.kwargs.num_units)\n            else:\n                self.assertEqual(state_.shape[0], batch_size)\n                self.assertEqual(state_.shape[1],\n                                 hparams_.kwargs.num_units)\n\n    def test_switch_dropout(self):\n        """"""Tests dropout mode.\n        """"""\n        emb_dim = 4\n        num_units = 64\n        hparams = {\n            ""kwargs"": {\n                ""num_units"": num_units\n            },\n            ""num_layers"": 2,\n            ""dropout"": {\n                ""input_keep_prob"": 0.8,\n            },\n        }\n        mode = tf.placeholder(tf.string)\n        hparams_ = HParams(hparams, layers.default_rnn_cell_hparams())\n        cell = layers.get_rnn_cell(hparams_, mode)\n\n        batch_size = 16\n        inputs = tf.zeros([batch_size, emb_dim], dtype=tf.float32)\n        output, state = cell(inputs,\n                             cell.zero_state(batch_size, dtype=tf.float32))\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_train, _ = sess.run(\n                [output, state],\n                feed_dict={mode: tf.estimator.ModeKeys.TRAIN})\n            self.assertEqual(output_train.shape[0], batch_size)\n            output_test, _ = sess.run(\n                [output, state],\n                feed_dict={mode: tf.estimator.ModeKeys.EVAL})\n            self.assertEqual(output_test.shape[0], batch_size)\n\n\nclass GetActivationFnTest(tf.test.TestCase):\n    """"""Tests :func:`texar.tf.core.layers.get_activation_fn`.\n    """"""\n    def test_get_activation_fn(self):\n        """"""Tests.\n        """"""\n        fn = layers.get_activation_fn()\n        self.assertEqual(fn, tf.identity)\n\n        fn = layers.get_activation_fn(\'relu\')\n        self.assertEqual(fn, tf.nn.relu)\n\n        inputs = tf.random_uniform([64, 100], -5, 20, dtype=tf.int32)\n\n        fn = layers.get_activation_fn(\'leaky_relu\')\n        fn_output = fn(inputs)\n        ref_output = tf.nn.leaky_relu(inputs)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            fn_output_, ref_output_ = sess.run([fn_output, ref_output])\n            np.testing.assert_array_equal(fn_output_, ref_output_)\n\n        fn = layers.get_activation_fn(\'leaky_relu\', kwargs={\'alpha\': 0.1})\n        fn_output = fn(inputs)\n        ref_output = tf.nn.leaky_relu(inputs, alpha=0.1)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            fn_output_, ref_output_ = sess.run([fn_output, ref_output])\n            np.testing.assert_array_equal(fn_output_, ref_output_)\n\n\nclass GetLayerTest(tf.test.TestCase):\n    """"""Tests layer creator.\n    """"""\n    def test_get_layer(self):\n        """"""Tests :func:`texar.tf.core.layers.get_layer`.\n        """"""\n        hparams = {\n            ""type"": ""Conv1D""\n        }\n        layer = layers.get_layer(hparams)\n        self.assertTrue(isinstance(layer, tf.layers.Conv1D))\n\n        hparams = {\n            ""type"": ""MergeLayer"",\n            ""kwargs"": {\n                ""layers"": [\n                    {""type"": ""Conv1D""},\n                    {""type"": ""Conv1D""}\n                ]\n            }\n        }\n        layer = layers.get_layer(hparams)\n        self.assertTrue(isinstance(layer, tx.core.MergeLayer))\n\n        hparams = {\n            ""type"": tf.layers.Conv1D\n        }\n        layer = layers.get_layer(hparams)\n        self.assertTrue(isinstance(layer, tf.layers.Conv1D))\n\n        hparams = {\n            ""type"": tf.layers.Conv1D(filters=10, kernel_size=2)\n        }\n        layer = layers.get_layer(hparams)\n        self.assertTrue(isinstance(layer, tf.layers.Conv1D))\n\n\nclass ReducePoolingLayerTest(tf.test.TestCase):\n    """"""Tests reduce pooling layer.\n    """"""\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        self._batch_size = 64\n        self._seq_length = 16\n        self._emb_dim = 100\n\n    def test_max_reduce_pooling_layer(self):\n        """"""Tests :class:`texar.tf.core.MaxReducePooling1D`.\n        """"""\n        pool_layer = layers.MaxReducePooling1D()\n\n        inputs = tf.random_uniform(\n            [self._batch_size, self._seq_length, self._emb_dim])\n        output_shape = pool_layer.compute_output_shape(inputs.get_shape())\n        output = pool_layer(inputs)\n        output_reduce = tf.reduce_max(inputs, axis=1)\n        self.assertEqual(output.get_shape(), output_shape)\n        self.assertEqual(output.get_shape(), [self._batch_size, self._emb_dim])\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_, output_reduce_ = sess.run([output, output_reduce])\n            np.testing.assert_array_equal(output_, output_reduce_)\n\n    def test_average_reduce_pooling_layer(self):\n        """"""Tests :class:`texar.tf.core.AverageReducePooling1D`.\n        """"""\n        pool_layer = layers.AverageReducePooling1D()\n\n        inputs = tf.random_uniform(\n            [self._batch_size, self._seq_length, self._emb_dim])\n        output_shape = pool_layer.compute_output_shape(inputs.get_shape())\n        output = pool_layer(inputs)\n        output_reduce = tf.reduce_mean(inputs, axis=1)\n        self.assertEqual(output.get_shape(), output_shape)\n        self.assertEqual(output.get_shape(), [self._batch_size, self._emb_dim])\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            output_, output_reduce_ = sess.run([output, output_reduce])\n            np.testing.assert_array_equal(output_, output_reduce_)\n\n\nclass MergeLayerTest(tf.test.TestCase):\n    """"""Tests MergeLayer.\n    """"""\n\n    def test_output_shape(self):\n        """"""Tests MergeLayer.compute_output_shape function.\n        """"""\n        input_shapes = [[None, 1, 2], [64, 2, 2], [None, 3, 2]]\n\n        concat_layer = layers.MergeLayer(mode=\'concat\', axis=1)\n        concat_output_shape = concat_layer.compute_output_shape(input_shapes)\n        self.assertEqual(concat_output_shape, [64, 6, 2])\n\n        sum_layer = layers.MergeLayer(mode=\'sum\', axis=1)\n        sum_output_shape = sum_layer.compute_output_shape(input_shapes)\n        self.assertEqual(sum_output_shape, [64, 2])\n\n        input_shapes = [[None, 5, 2], [64, None, 2], [2]]\n        esum_layer = layers.MergeLayer(mode=\'elemwise_sum\')\n        esum_output_shape = esum_layer.compute_output_shape(input_shapes)\n        self.assertEqual(esum_output_shape, [64, 5, 2])\n\n    def test_layer_logics(self):\n        """"""Test the logic of MergeLayer.\n        """"""\n        layers_ = []\n        layers_.append(tf.layers.Conv1D(filters=200, kernel_size=3))\n        layers_.append(tf.layers.Conv1D(filters=200, kernel_size=4))\n        layers_.append(tf.layers.Conv1D(filters=200, kernel_size=5))\n        layers_.append(tf.layers.Dense(200))\n        layers_.append(tf.layers.Dense(200))\n        m_layer = layers.MergeLayer(layers_)\n\n        inputs = tf.zeros([64, 16, 1024], dtype=tf.float32)\n        outputs = m_layer(inputs)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape[0], 64)\n            self.assertEqual(outputs_.shape[2], 200)\n            self.assertEqual(\n                outputs_.shape,\n                m_layer.compute_output_shape(inputs.shape.as_list()))\n\n    def test_trainable_variables(self):\n        """"""Test the trainable_variables of the layer.\n        """"""\n        layers_ = []\n        layers_.append(tf.layers.Conv1D(filters=200, kernel_size=3))\n        layers_.append(tf.layers.Conv1D(filters=200, kernel_size=4))\n        layers_.append(tf.layers.Conv1D(filters=200, kernel_size=5))\n        layers_.append(tf.layers.Dense(200))\n        layers_.append(tf.layers.Dense(200))\n        m_layer = layers.MergeLayer(layers_)\n\n        inputs = tf.zeros([64, 16, 1024], dtype=tf.float32)\n        _ = m_layer(inputs)\n\n        num_vars = sum([len(layer.trainable_variables) for layer in layers_])\n        self.assertEqual(num_vars, len(m_layer.trainable_variables))\n\n\nclass SequentialLayerTest(tf.test.TestCase):\n    """"""Tests sequential layer.\n    """"""\n\n    def test_seq_layer(self):\n        """"""Test sequential layer.\n        """"""\n        layers_ = []\n        layers_.append(tf.layers.Dense(100))\n        layers_.append(tf.layers.Dense(200))\n        seq_layer = layers.SequentialLayer(layers_)\n\n        output_shape = seq_layer.compute_output_shape([None, 10])\n        self.assertEqual(output_shape[1].value, 200)\n\n        inputs = tf.zeros([10, 20], dtype=tf.float32)\n        outputs = seq_layer(inputs)\n\n        num_vars = sum([len(layer.trainable_variables) for layer in layers_])\n        self.assertEqual(num_vars, len(seq_layer.trainable_variables))\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape[0], 10)\n            self.assertEqual(outputs_.shape[1], 200)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/core/optimization_test.py,21,"b'#\n""""""\nUnit tests for various optimization related utilities.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf.core.optimization as opt\nfrom texar.tf.utils import utils\n\n\nclass OptimizationTest(tf.test.TestCase):\n    """"""Tests optimization.\n    """"""\n\n    def test_get_optimizer(self):\n        """"""Tests get_optimizer.\n        """"""\n        default_optimizer_fn, optimizer_class = opt.get_optimizer_fn(\n            opt.default_optimization_hparams()[""optimizer""])\n        default_optimizer = default_optimizer_fn(1.0)\n        self.assertTrue(optimizer_class, tf.train.Optimizer)\n        self.assertIsInstance(default_optimizer, tf.train.AdamOptimizer)\n\n        hparams = {\n            ""type"": ""MomentumOptimizer"",\n            ""kwargs"": {\n                ""learning_rate"": 0.001,\n                ""momentum"": 0.9,\n                ""use_nesterov"": True\n            }\n        }\n        momentum_optimizer_fn, _ = opt.get_optimizer_fn(hparams)\n        momentum_optimizer = momentum_optimizer_fn()\n        self.assertIsInstance(momentum_optimizer, tf.train.MomentumOptimizer)\n\n        hparams = {\n            ""type"": tf.train.MomentumOptimizer,\n            ""kwargs"": {\n                ""momentum"": 0.9,\n                ""use_nesterov"": True\n            }\n        }\n        momentum_optimizer_fn, _ = opt.get_optimizer_fn(hparams)\n        momentum_optimizer = momentum_optimizer_fn(0.001)\n        self.assertIsInstance(momentum_optimizer, tf.train.MomentumOptimizer)\n\n        hparams = {\n            ""type"": tf.train.MomentumOptimizer(0.001, 0.9)\n        }\n        momentum_optimizer, _ = opt.get_optimizer_fn(hparams)\n        self.assertIsInstance(momentum_optimizer, tf.train.MomentumOptimizer)\n\n    def test_get_learning_rate_decay_fn(self):\n        """"""Tests get_learning_rate_decay_fn.\n        """"""\n        default_lr_decay_fn = opt.get_learning_rate_decay_fn(\n            opt.default_optimization_hparams()[""learning_rate_decay""])\n        self.assertIsNone(default_lr_decay_fn)\n\n        boundaries = [2, 4]\n        values = [0.1, 0.01, 0.001]\n        hparams = {\n            ""type"": ""piecewise_constant"",\n            ""kwargs"": {\n                ""boundaries"": boundaries,\n                ""values"": values\n            },\n            ""min_learning_rate"": 0.05,\n            ""start_decay_step"": 1,\n            ""end_decay_step"": utils.MAX_SEQ_LENGTH,\n        }\n        pc_lr_decay_fn = opt.get_learning_rate_decay_fn(hparams)\n\n        global_step = 1\n        pc_lr = pc_lr_decay_fn(learning_rate=1., global_step=global_step)\n        pc_lr_true = tf.train.piecewise_constant(\n            global_step - hparams[""start_decay_step""], boundaries, values)\n\n        hparams[""type""] = ""natural_exp_decay""\n        hparams[""kwargs""] = {\n            ""decay_steps"": 1,\n            ""decay_rate"": 0.5\n        }\n        ned_lr_decay_fn = opt.get_learning_rate_decay_fn(hparams)\n        ned_lr = ned_lr_decay_fn(learning_rate=1., global_step=global_step)\n        ned_lr_true = tf.train.natural_exp_decay(\n            1., global_step - hparams[""start_decay_step""],\n            hparams[""kwargs""][""decay_steps""], hparams[""kwargs""][""decay_rate""])\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            pc_lr_, pc_lr_true_, ned_lr_, ned_lr_true_ = sess.run(\n                [pc_lr, pc_lr_true, ned_lr, ned_lr_true])\n            self.assertEqual(pc_lr_, pc_lr_true_)\n            self.assertEqual(ned_lr_, ned_lr_true_)\n\n    def test_get_gradient_clip_fn(self):    # pylint: disable=too-many-locals\n        """"""Tests get_gradient_clip_fn.\n        """"""\n        default_grad_clip_fn = opt.get_gradient_clip_fn(\n            opt.default_optimization_hparams()[""gradient_clip""])\n        self.assertIsNone(default_grad_clip_fn)\n\n        grads = [tf.random_uniform([10, 10], -1., 1.) for _ in range(5)]\n        grads_and_vars = list(zip(grads, range(5)))\n\n        hparams = {\n            ""type"": ""clip_by_global_norm"",\n            ""kwargs"": {\n                ""clip_norm"": 0.1\n            }\n        }\n        gn_grad_clip_fn = opt.get_gradient_clip_fn(hparams)\n        gn_grads_and_vars = gn_grad_clip_fn(grads_and_vars)\n        gn_grads, _ = zip(*gn_grads_and_vars)\n        gn_grads_true, _ = tf.clip_by_global_norm(\n            grads, hparams[""kwargs""][""clip_norm""])\n\n        hparams = {\n            ""type"": ""clip_by_value"",\n            ""kwargs"": {\n                ""clip_value_min"": -0.01,\n                ""clip_value_max"": 0.01\n            }\n        }\n        v_grad_clip_fn = opt.get_gradient_clip_fn(hparams)\n        v_grads_and_vars = v_grad_clip_fn(grads_and_vars)\n        v_grads, _ = zip(*v_grads_and_vars)\n        v_grads_true = tf.clip_by_value(grads,\n                                        hparams[""kwargs""][""clip_value_min""],\n                                        hparams[""kwargs""][""clip_value_max""])\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            gn_grads_, gn_grads_true_, v_grads_, v_grads_true_ = sess.run(\n                [gn_grads, gn_grads_true, v_grads, v_grads_true])\n            np.testing.assert_array_equal(gn_grads_, gn_grads_true_)\n            np.testing.assert_array_equal(v_grads_, v_grads_true_)\n\n    def test_get_train_op(self):\n        """"""Tests get_train_op.\n        """"""\n        var = tf.Variable(0.)\n        loss = tf.nn.l2_loss(var)\n        train_op = opt.get_train_op(loss)\n        self.assertTrue(tf.contrib.framework.is_tensor(train_op))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data_utils_test.py,4,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data utils.\n""""""\n\nimport tempfile\n\nimport tensorflow as tf\n\nfrom texar.tf.data import data_utils\n\n\nclass CountFileLinesTest(tf.test.TestCase):\n    """"""Tests :func:`texar.tf.data.data_utils.count_file_lines`.\n    """"""\n\n    def test_load_glove(self):\n        """"""Tests the load_glove function.\n        """"""\n        file_1 = tempfile.NamedTemporaryFile(mode=""w+"")\n        num_lines = data_utils.count_file_lines(file_1.name)\n        self.assertEqual(num_lines, 0)\n\n        file_2 = tempfile.NamedTemporaryFile(mode=""w+"")\n        file_2.write(\'\\n\'.join([\'x\'] * 5))\n        file_2.flush()\n        num_lines = data_utils.count_file_lines(\n            [file_1.name, file_2.name, file_2.name])\n        self.assertEqual(num_lines, 0 + 5 + 5)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/embedding_test.py,6,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUnit tests for embedding related operations.\n""""""\n\nimport sys\nimport tempfile\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.data import embedding\n\nPy3 = sys.version_info[0] == 3  # pylint: disable=invalid-name\n\n\nclass EmbeddingTest(tf.test.TestCase):\n    """"""Tests embedding related operations.\n    """"""\n\n    def test_load_glove(self):\n        """"""Tests the load_glove function.\n        """"""\n        word_vec_lines = [""word 1.2 3.4 5.6"", ""\xe8\xaf\x8d 1. 3. 5.""]\n        glove_file = tempfile.NamedTemporaryFile(mode=""w+"")\n        if Py3:\n            glove_file.write(\'\\n\'.join(word_vec_lines))\n        else:\n            glove_file.write(\'\\n\'.join(word_vec_lines).encode(""utf-8""))\n        glove_file.flush()\n        vocab = {""word"": 0, ""\xe8\xaf\x8d"": 1}\n        word_vecs = np.zeros([2, 3])\n\n        word_vecs = embedding.load_glove(glove_file.name, vocab, word_vecs)\n\n        self.assertEqual(word_vecs.shape[0], 2)\n        self.assertEqual(word_vecs.shape[1], 3)\n        np.testing.assert_array_equal(word_vecs[0], [1.2, 3.4, 5.6])\n        np.testing.assert_array_equal(word_vecs[1], [1., 3., 5.])\n\n    def test_load_word2vec(self):\n        """"""Tests the load_word2vec function.\n        """"""\n        header = ""2 3""\n        words = [""word"", ""\xe8\xaf\x8d""]\n        vec = np.array([1.2, 3.4, 5.6], dtype=\'float32\')\n        w2v_file = tempfile.NamedTemporaryFile()\n        w2v_file.write(tf.compat.as_bytes(header + ""\\n""))\n        for word in words:\n            w2v_file.write(tf.compat.as_bytes(word + "" ""))\n            w2v_file.write(vec.tostring() + b\'\\n\')\n        w2v_file.flush()\n        vocab = {""word"": 0, ""\xe8\xaf\x8d"": 1}\n        word_vecs = np.zeros([2, 3])\n\n        word_vecs = embedding.load_word2vec(w2v_file.name, vocab, word_vecs)\n\n        self.assertEqual(word_vecs.shape[0], 2)\n        self.assertEqual(word_vecs.shape[1], 3)\n        np.testing.assert_array_equal(word_vecs[0], vec)\n        np.testing.assert_array_equal(word_vecs[1], vec)\n\n    def test_embedding(self):\n        """"""Tests :class:`texar.tf.data.embedding.Embedding`.\n        """"""\n        vocab = {""word"": 0, ""\xe8\xaf\x8d"": 1}\n        emb = embedding.Embedding(vocab)\n        self.assertEqual(len(emb.word_vecs), len(vocab))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/vocabulary_test.py,3,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for vocabulary related operations.\n""""""\n\nimport tempfile\nimport tensorflow as tf\n\nfrom texar.tf.data import vocabulary\n\n# pylint: disable=protected-access\n\n\nclass VocabularyTest(tf.test.TestCase):\n    """"""Tests vocabulary related operations.\n    """"""\n\n    def test_make_defaultdict(self):\n        """"""Tests the _make_defaultdict function.\n        """"""\n        keys = [\'word\', \'\xe8\xaf\x8d\']\n        values = [0, 1]\n        default_value = -1\n\n        dict_ = vocabulary._make_defaultdict(keys, values, default_value)\n\n        self.assertEqual(len(dict_), 2)\n        self.assertEqual(dict_[\'word\'], 0)\n        self.assertEqual(dict_[\'\xe8\xaf\x8d\'], 1)\n        self.assertEqual(dict_[\'sth_else\'], -1)\n\n    def test_vocab_construction(self):\n        """"""Test vocabulary construction.\n        """"""\n        vocab_list = [\'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n\n        vocab = vocabulary.Vocab(vocab_file.name)\n\n        self.assertEqual(vocab.size, len(vocab_list) + 4)\n        self.assertEqual(\n            set(vocab.token_to_id_map_py.keys()),\n            set([\'word\', \'\xe8\xaf\x8d\'] + vocab.special_tokens))\n\n        # Tests UNK token\n        unk_token_id = vocab.token_to_id_map_py[\'new\']\n        unk_token_text = vocab.id_to_token_map_py[unk_token_id]\n        self.assertEqual(unk_token_text, vocab.unk_token)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/evals/bleu_test.py,4,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for bleu.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.evals.bleu_moses import sentence_bleu_moses, corpus_bleu_moses\nfrom texar.tf.evals.bleu import sentence_bleu, corpus_bleu\n\n# pylint: disable=too-many-locals, too-many-arguments\n\n\nclass BLEUTest(tf.test.TestCase):\n    """"""Tests the bleu functions.\n    """"""\n\n    def _test_sentence_bleu(self, references, hypothesis, lowercase,\n                            true_bleu):\n        bleu = sentence_bleu_moses(references=references,\n                                   hypothesis=hypothesis,\n                                   lowercase=lowercase)\n        self.assertAlmostEqual(bleu, true_bleu, places=2)\n\n        bleu = sentence_bleu(references=references,\n                             hypothesis=hypothesis,\n                             lowercase=lowercase)\n        self.assertAlmostEqual(bleu, true_bleu, places=0)\n\n    def test_sentence_strings(self):\n        """"""Tests hypothesis as strings.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        references = [""this is a test sentence to evaluate the bleu score .""]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=67.03)\n\n    def test_sentence_list(self):\n        """"""Tests hypothesis as a list of tokens.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        hypothesis = hypothesis.split()\n        references = [""this is a test sentence to evaluate the bleu score .""]\n        references = [references[0].split()]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=67.03)\n\n    def test_sentence_multi_references(self):\n        """"""Tests multiple references.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        references = [""this is a test sentence to evaluate the bleu score ."",\n                      ""this is a test sentence to evaluate the good score .""]\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=76.12)\n\n    def test_sentence_numpy(self):\n        """"""Tests with numpy format.\n        """"""\n        hypothesis = \\\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d""\n        hypothesis = np.array(hypothesis.split())\n        references = [""this is a test sentence to evaluate the bleu score ."",\n                      ""this is a test sentence to evaluate the good score .""]\n        references = np.array([np.array(r.split()) for r in references])\n        self._test_sentence_bleu(\n            references, hypothesis, lowercase=False, true_bleu=76.12)\n\n    def _test_corpus_bleu(self, list_of_references, hypotheses, lowercase,\n                          return_all, true_bleu):\n        bleu = corpus_bleu_moses(list_of_references=list_of_references,\n                                 hypotheses=hypotheses,\n                                 lowercase=lowercase,\n                                 return_all=return_all)\n        if not return_all:\n            self.assertAlmostEqual(bleu, true_bleu, places=2)\n        else:\n            for ret, true in zip(bleu, true_bleu):\n                self.assertAlmostEqual(ret, true, places=2)\n\n        bleu = corpus_bleu(list_of_references=list_of_references,\n                           hypotheses=hypotheses,\n                           lowercase=lowercase,\n                           return_all=return_all)\n        if not return_all:\n            self.assertAlmostEqual(bleu, true_bleu, places=0)\n        else:\n            for ret, true in zip(bleu, true_bleu):\n                self.assertAlmostEqual(ret, true, places=0)\n\n    def test_corpus_strings(self):\n        """"""Tests corpus level BLEU.\n        """"""\n        hypotheses = [\n            ""this is a test sentence to evaluate the good bleu score . \xe8\xaf\x8d"",\n            ""i believe that that the script is \xe8\xaf\x8d perfectly correct .""\n        ]\n        list_of_references = [\n            [""this is a test sentence to evaluate the bleu score ."",\n             ""this is a test sentence to evaluate the good score .""],\n            [""i believe that the script is perfectly correct ."".split()]\n        ]\n        self._test_corpus_bleu(list_of_references, hypotheses,\n                               False, False, 63.02)\n\n        self._test_corpus_bleu(list_of_references, hypotheses,\n                               False, True, [63.02, 87.5, 77.3, 60.0, 38.9])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/losses/adv_losses_test.py,7,"b'#\n""""""\nTests adversarial loss related functions.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.losses.adv_losses import binary_adversarial_losses\n\n\nclass AdvLossesTest(tf.test.TestCase):\n    """"""Tests adversarial losses.\n    """"""\n    def test_binary_adversarial_losses(self):\n        """"""Tests :meth:`~texar.tf.losses.adv_losses.binary_adversarial_losse`.\n        """"""\n        batch_size = 16\n        data_dim = 64\n        real_data = tf.zeros([batch_size, data_dim], dtype=tf.float32)\n        fake_data = tf.ones([batch_size, data_dim], dtype=tf.float32)\n        const_logits = tf.zeros([batch_size], dtype=tf.float32)\n        # Use a dumb discriminator that always outputs logits=0.\n        gen_loss, disc_loss = binary_adversarial_losses(\n            real_data, fake_data, lambda x: const_logits)\n        gen_loss_2, disc_loss_2 = binary_adversarial_losses(\n            real_data, fake_data, lambda x: const_logits, mode=""min_fake"")\n\n        with self.test_session() as sess:\n            gen_loss_, disc_loss_ = sess.run([gen_loss, disc_loss])\n            gen_loss_2_, disc_loss_2_ = sess.run([gen_loss_2, disc_loss_2])\n            self.assertAlmostEqual(gen_loss_, -gen_loss_2_)\n            self.assertAlmostEqual(disc_loss_, disc_loss_2_)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/losses/entropy_test.py,20,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for entropy.\n""""""\n\n# pylint: disable=invalid-name\n\nimport tensorflow as tf\nimport texar.tf as tx\n\n\nclass EntropyTest(tf.test.TestCase):\n    """"""Tests entropy.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._batch_size = 64\n        self._max_time = 128\n        self._d = 16\n        self._distribution_dim = 32\n        self._logits = tf.random_uniform([self._batch_size, self._d,\n                                          self._distribution_dim])\n        self._sequence_logits = tf.random_uniform([self._batch_size,\n                                                   self._max_time,\n                                                   self._d,\n                                                   self._distribution_dim])\n        self._sequence_length = tf.random_uniform(\n            [self._batch_size], maxval=self._max_time, dtype=tf.int32)\n\n    def _test_entropy(self, entropy_fn, logits, sequence_length=None):\n        with self.test_session() as sess:\n            if sequence_length is None:\n                entropy = entropy_fn(logits)\n                rank = sess.run(tf.rank(entropy))\n                self.assertEqual(rank, 0)\n\n                entropy = entropy_fn(logits, average_across_batch=False)\n                rank = sess.run(tf.rank(entropy))\n                self.assertEqual(rank, 1)\n                self.assertEqual(entropy.shape,\n                                 tf.TensorShape([self._batch_size]))\n            else:\n                entropy = entropy_fn(logits, sequence_length=sequence_length)\n                rank = sess.run(tf.rank(entropy))\n                self.assertEqual(rank, 0)\n\n                entropy = entropy_fn(logits, sequence_length=sequence_length,\n                                     sum_over_timesteps=False)\n                rank = sess.run(tf.rank(entropy))\n                self.assertEqual(rank, 1)\n                self.assertEqual(entropy.shape,\n                                 tf.TensorShape([self._max_time]))\n\n                entropy = entropy_fn(logits, sequence_length=sequence_length,\n                                     sum_over_timesteps=False,\n                                     average_across_timesteps=True,\n                                     average_across_batch=False)\n                rank = sess.run(tf.rank(entropy))\n                self.assertEqual(rank, 1)\n                self.assertEqual(entropy.shape,\n                                 tf.TensorShape([self._batch_size]))\n\n                entropy = entropy_fn(logits, sequence_length=sequence_length,\n                                     sum_over_timesteps=False,\n                                     average_across_batch=False)\n                rank = sess.run(tf.rank(entropy))\n                self.assertEqual(rank, 2)\n                self.assertEqual(entropy.shape,\n                                 tf.TensorShape([self._batch_size,\n                                                 self._max_time]))\n\n                sequence_length_time = tf.random_uniform(\n                    [self._max_time], maxval=self._batch_size, dtype=tf.int32)\n                entropy = entropy_fn(logits,\n                                     sequence_length=sequence_length_time,\n                                     sum_over_timesteps=False,\n                                     average_across_batch=False,\n                                     time_major=True)\n                self.assertEqual(entropy.shape, tf.TensorShape(\n                    [self._batch_size, self._max_time]))\n\n    def test_entropy_with_logits(self):\n        """"""Tests `entropy_with_logits`\n        """"""\n        self._test_entropy(\n            tx.losses.entropy_with_logits, self._logits)\n\n    def test_sequence_entropy_with_logits(self):\n        """"""Tests `sequence_entropy_with_logits`\n        """"""\n        self._test_entropy(\n            tx.losses.sequence_entropy_with_logits, self._sequence_logits,\n            sequence_length=self._sequence_length)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/losses/mle_losses_test.py,24,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for mle losses.\n""""""\n\n# pylint: disable=invalid-name\n\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n\nclass MLELossesTest(tf.test.TestCase):\n    """"""Tests mle losses.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._batch_size = 64\n        self._max_time = 16\n        self._num_classes = 100\n        self._labels = tf.ones([self._batch_size, self._max_time],\n                               dtype=tf.int32)\n        one_hot_labels = tf.one_hot(\n            self._labels, self._num_classes, dtype=tf.float32)\n        self._one_hot_labels = tf.reshape(\n            one_hot_labels, [self._batch_size, self._max_time, -1])\n        self._logits = tf.random_uniform(\n            [self._batch_size, self._max_time, self._num_classes])\n        self._sequence_length = tf.random_uniform(\n            [self._batch_size], maxval=self._max_time, dtype=tf.int32)\n\n    def _test_sequence_loss(self, loss_fn, labels, logits, sequence_length):\n        with self.test_session() as sess:\n            loss = loss_fn(labels, logits, sequence_length)\n            rank = sess.run(tf.rank(loss))\n            self.assertEqual(rank, 0)\n\n            loss = loss_fn(\n                labels, logits, sequence_length, sum_over_timesteps=False)\n            rank = sess.run(tf.rank(loss))\n            self.assertEqual(rank, 1)\n            self.assertEqual(loss.shape, tf.TensorShape([self._max_time]))\n\n            loss = loss_fn(\n                labels, logits, sequence_length, sum_over_timesteps=False,\n                average_across_timesteps=True, average_across_batch=False)\n            rank = sess.run(tf.rank(loss))\n            self.assertEqual(rank, 1)\n            self.assertEqual(loss.shape, tf.TensorShape([self._batch_size]))\n\n            loss = loss_fn(\n                labels, logits, sequence_length, sum_over_timesteps=False,\n                average_across_batch=False)\n            rank = sess.run(tf.rank(loss))\n            self.assertEqual(rank, 2)\n            self.assertEqual(loss.shape,\n                             tf.TensorShape([self._batch_size, self._max_time]))\n\n            sequence_length_time = tf.random_uniform(\n                [self._max_time], maxval=self._batch_size, dtype=tf.int32)\n            loss = loss_fn(\n                labels, logits, sequence_length_time, sum_over_timesteps=False,\n                average_across_batch=False, time_major=True)\n            self.assertEqual(loss.shape,\n                             tf.TensorShape([self._batch_size, self._max_time]))\n\n    def test_sequence_softmax_cross_entropy(self):\n        """"""Tests `sequence_softmax_cross_entropy`\n        """"""\n        self._test_sequence_loss(\n            tx.losses.sequence_softmax_cross_entropy,\n            self._one_hot_labels, self._logits, self._sequence_length)\n\n    def test_sequence_sparse_softmax_cross_entropy(self):\n        """"""Tests `sequence_sparse_softmax_cross_entropy`\n        """"""\n        self._test_sequence_loss(\n            tx.losses.sequence_sparse_softmax_cross_entropy,\n            self._labels, self._logits, self._sequence_length)\n\n    def test_sequence_sigmoid_cross_entropy(self):\n        """"""Tests `sequence_sigmoid_cross_entropy`.\n        """"""\n        self._test_sequence_loss(\n            tx.losses.sequence_sigmoid_cross_entropy,\n            self._one_hot_labels, self._logits, self._sequence_length)\n\n        self._test_sequence_loss(\n            tx.losses.sequence_sigmoid_cross_entropy,\n            self._one_hot_labels[:, :, 0],\n            self._logits[:, :, 0],\n            self._sequence_length)\n\n        labels = tf.placeholder(dtype=tf.int32, shape=None)\n        loss = tx.losses.sequence_sigmoid_cross_entropy(\n            logits=self._logits[:, :, 0],\n            labels=tf.cast(labels, tf.float32),\n            sequence_length=self._sequence_length)\n        with self.test_session() as sess:\n            rank = sess.run(\n                tf.rank(loss),\n                feed_dict={labels: np.ones([self._batch_size, self._max_time])})\n            self.assertEqual(rank, 0)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/losses/pg_losses_test.py,25,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for pg losses.\n""""""\n\n# pylint: disable=invalid-name\n\nimport tensorflow as tf\nimport texar.tf as tx\n\n\nclass PGLossesTest(tf.test.TestCase):\n    """"""Tests pg losses.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._batch_size = 64\n        self._max_time = 16\n        self._d1 = 32\n        self._d2 = 32\n        self._d3 = 32\n        self._num_classes = 10\n        self._actions_batch = tf.ones([self._batch_size, self._max_time,\n                                      self._d1, self._d2, self._d3],\n                                      dtype=tf.int32)\n        self._logits_batch = tf.random_uniform([self._batch_size,\n                                                self._max_time,\n                                                self._d1, self._d2, self._d3,\n                                                self._num_classes])\n        self._advantages_batch = tf.random_uniform([self._batch_size,\n                                                    self._max_time,\n                                                    self._d1, self._d2,\n                                                    self._d3])\n        self._actions_no_batch = tf.ones([self._max_time,\n                                          self._d1, self._d2, self._d3],\n                                         dtype=tf.int32)\n        self._logits_no_batch = tf.random_uniform([self._max_time,\n                                                   self._d1, self._d2, self._d3,\n                                                   self._num_classes])\n        self._advantages_no_batch = tf.random_uniform([self._max_time,\n                                                       self._d1, self._d2,\n                                                       self._d3])\n        self._sequence_length = tf.random_uniform(\n            [self._batch_size], maxval=self._max_time, dtype=tf.int32)\n\n    def _test_sequence_loss(self, loss_fn, actions, logits, advantages,\n                            batched, sequence_length):\n        with self.test_session() as sess:\n            loss = loss_fn(actions, logits, advantages, batched=batched,\n                           sequence_length=sequence_length)\n            rank = sess.run(tf.rank(loss))\n            self.assertEqual(rank, 0)\n\n            loss = loss_fn(actions, logits, advantages, batched=batched,\n                           sequence_length=sequence_length,\n                           sum_over_timesteps=False)\n            rank = sess.run(tf.rank(loss))\n            self.assertEqual(rank, 1)\n            self.assertEqual(loss.shape, tf.TensorShape([self._max_time]))\n\n            loss = loss_fn(actions, logits, advantages, batched=batched,\n                           sequence_length=sequence_length,\n                           sum_over_timesteps=False,\n                           average_across_timesteps=True,\n                           average_across_batch=False)\n            rank = sess.run(tf.rank(loss))\n            if batched:\n                self.assertEqual(rank, 1)\n                self.assertEqual(loss.shape, tf.TensorShape([self._batch_size]))\n            else:\n                self.assertEqual(rank, 0)\n\n            loss = loss_fn(actions, logits, advantages, batched=batched,\n                           sequence_length=sequence_length,\n                           sum_over_timesteps=False,\n                           average_across_batch=False)\n            rank = sess.run(tf.rank(loss))\n            if batched:\n                self.assertEqual(rank, 2)\n                self.assertEqual(loss.shape,\n                                 tf.TensorShape([self._batch_size,\n                                                 self._max_time]))\n            else:\n                self.assertEqual(rank, 1)\n                self.assertEqual(loss.shape,\n                                 tf.TensorShape([self._max_time]))\n\n            sequence_length_time = tf.random_uniform(\n                [self._max_time], maxval=self._max_time, dtype=tf.int32)\n            loss = loss_fn(actions, logits, advantages, batched=batched,\n                           sequence_length=sequence_length_time,\n                           sum_over_timesteps=False,\n                           average_across_batch=False,\n                           time_major=True)\n            if batched:\n                self.assertEqual(loss.shape,\n                                 tf.TensorShape([self._batch_size,\n                                                 self._max_time]))\n            else:\n                self.assertEqual(loss.shape,\n                                 tf.TensorShape([self._max_time]))\n\n    def test_pg_losses_with_logits(self):\n        """"""Tests `pg_losses_with_logits`.\n        """"""\n        self._test_sequence_loss(tx.losses.pg_loss_with_logits,\n                                 self._actions_batch,\n                                 self._logits_batch,\n                                 self._advantages_batch,\n                                 True,\n                                 self._sequence_length)\n\n        self._test_sequence_loss(tx.losses.pg_loss_with_logits,\n                                 self._actions_no_batch,\n                                 self._logits_no_batch,\n                                 self._advantages_no_batch,\n                                 False,\n                                 self._sequence_length)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/losses/rewards_test.py,18,"b'""""""\nUnit tests for RL rewards.\n""""""\n\n# pylint: disable=invalid-name, no-member\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.losses.rewards import \\\n        _discount_reward_tensor_2d, _discount_reward_tensor_1d, \\\n        _discount_reward_py_1d, _discount_reward_py_2d, \\\n        discount_reward\n\n\nclass RewardTest(tf.test.TestCase):\n    """"""Tests reward related functions.\n    """"""\n\n    def test_discount_reward(self):\n        """"""Tests :func:`texar.tf.losses.rewards.discount_reward`\n        """"""\n        # 1D\n        reward = np.ones([2], dtype=np.float64)\n        sequence_length = [3, 5]\n\n        discounted_reward = discount_reward(\n            reward, sequence_length, discount=1.)\n        discounted_reward_n = discount_reward(\n            reward, sequence_length, discount=.1, normalize=True)\n\n        discounted_reward_ = discount_reward(\n            tf.constant(reward, dtype=tf.float64),\n            sequence_length, discount=1.)\n        discounted_reward_n_ = discount_reward(\n            tf.constant(reward, dtype=tf.float64),\n            sequence_length, discount=.1, normalize=True)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            r, r_n = sess.run([discounted_reward_, discounted_reward_n_])\n\n            np.testing.assert_array_almost_equal(\n                discounted_reward, r, decimal=6)\n            np.testing.assert_array_almost_equal(\n                discounted_reward_n, r_n, decimal=6)\n\n        # 2D\n        reward = np.ones([2, 10], dtype=np.float64)\n        sequence_length = [5, 10]\n\n        discounted_reward = discount_reward(\n            reward, sequence_length, discount=1.)\n        discounted_reward_n = discount_reward(\n            reward, sequence_length, discount=.1, normalize=True)\n\n        discounted_reward_ = discount_reward(\n            tf.constant(reward, dtype=tf.float64), sequence_length,\n            discount=1., tensor_rank=2)\n        discounted_reward_n_ = discount_reward(\n            tf.constant(reward, dtype=tf.float64), sequence_length,\n            discount=.1, tensor_rank=2, normalize=True)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            r, r_n = sess.run([discounted_reward_, discounted_reward_n_])\n\n            np.testing.assert_array_almost_equal(\n                discounted_reward, r, decimal=6)\n            np.testing.assert_array_almost_equal(\n                discounted_reward_n, r_n, decimal=6)\n\n    def test_discount_reward_py_1d(self):\n        """"""Tests :func:`texar.tf.losses.rewards._discount_reward_py_1d`\n        """"""\n        reward = np.ones([2], dtype=np.float64)\n        sequence_length = [3, 5]\n\n        discounted_reward_1 = _discount_reward_py_1d(\n            reward, sequence_length, discount=1.)\n\n        discounted_reward_2 = _discount_reward_py_1d(\n            reward, sequence_length, discount=.1)\n\n        r = discounted_reward_1\n        for i in range(5):\n            if i < 3:\n                self.assertEqual(r[0, i], 1)\n            else:\n                self.assertEqual(r[0, i], 0)\n            self.assertEqual(r[1, i], 1)\n\n        r = discounted_reward_2\n        for i in range(5):\n            if i < 3:\n                self.assertAlmostEqual(r[0, i], 0.1**(2 - i))\n            else:\n                self.assertAlmostEqual(r[0, i], 0)\n            self.assertAlmostEqual(r[1, i], 0.1**(4 - i))\n\n    def test_discount_reward_tensor_1d(self):\n        """"""Tests :func:`texar.tf.losses.rewards._discount_reward_tensor_1d`\n        """"""\n        reward = tf.ones([2], dtype=tf.float64)\n        sequence_length = [3, 5]\n\n        discounted_reward_1 = _discount_reward_tensor_1d(\n            reward, sequence_length, discount=1.)\n\n        discounted_reward_2 = _discount_reward_tensor_1d(\n            reward, sequence_length, discount=.1)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            r = sess.run(discounted_reward_1)\n            for i in range(5):\n                if i < 3:\n                    self.assertEqual(r[0, i], 1)\n                else:\n                    self.assertEqual(r[0, i], 0)\n                self.assertEqual(r[1, i], 1)\n\n            r = sess.run(discounted_reward_2)\n            for i in range(5):\n                if i < 3:\n                    self.assertAlmostEqual(r[0, i], 0.1**(2 - i))\n                else:\n                    self.assertAlmostEqual(r[0, i], 0)\n                self.assertAlmostEqual(r[1, i], 0.1**(4 - i))\n\n    def test_discount_reward_py_2d(self):\n        """"""Tests :func:`texar.tf.losses.rewards._discount_reward_py_2d`\n        """"""\n        reward = np.ones([2, 10], dtype=np.float64)\n        sequence_length = [5, 10]\n\n        discounted_reward_1 = _discount_reward_py_2d(\n            reward, sequence_length, discount=1.)\n\n        discounted_reward_2 = _discount_reward_py_2d(\n            reward, sequence_length, discount=.1)\n\n        r = discounted_reward_1\n        for i in range(10):\n            if i < 5:\n                self.assertEqual(r[0, i], 5 - i)\n            else:\n                self.assertEqual(r[0, i], 0)\n            self.assertEqual(r[1, i], 10 - i)\n\n        r = discounted_reward_2\n        for i in range(10):\n            if i < 5:\n                self.assertEqual(r[0, i], int(11111. / 10**i) / 10**(4 - i))\n            else:\n                self.assertEqual(r[0, i], 0)\n            self.assertEqual(r[1, i], int(1111111111. / 10**i) / 10**(9 - i))\n\n    def test_discount_reward_tensor_2d(self):\n        """"""Tests :func:`texar.tf.losses.rewards._discount_reward_tensor_2d`\n        """"""\n        reward = tf.ones([2, 10], dtype=tf.float64)\n        sequence_length = [5, 10]\n\n        discounted_reward_1 = _discount_reward_tensor_2d(\n            reward, sequence_length, discount=1.)\n\n        discounted_reward_2 = _discount_reward_tensor_2d(\n            reward, sequence_length, discount=.1)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            r = sess.run(discounted_reward_1)\n            for i in range(10):\n                if i < 5:\n                    self.assertEqual(r[0, i], 5 - i)\n                else:\n                    self.assertEqual(r[0, i], 0)\n                self.assertEqual(r[1, i], 10 - i)\n\n            r = sess.run(discounted_reward_2)\n            for i in range(10):\n                if i < 5:\n                    self.assertEqual(r[0, i],\n                                     int(11111. / 10**i) / 10**(4 - i))\n                else:\n                    self.assertEqual(r[0, i], 0)\n                self.assertEqual(r[1, i],\n                                 int(1111111111. / 10**i) / 10**(9 - i))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/run/executor_test.py,7,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for executor.\n""""""\n\nimport tempfile\nimport shutil\n\nimport tensorflow as tf\n\nfrom texar.tf.run.executor import Executor\nfrom texar.tf.models.seq2seq.basic_seq2seq import BasicSeq2seq\n\n\nclass ExecutorTest(tf.test.TestCase):\n    """"""Tests :class:`texar.tf.run.executor.Executor`\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create data\n        vocab_list = [\'This\', \'is\', \'a\', \'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        src_text = [\'This is a sentence from source .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 source\']\n        src_text_file = tempfile.NamedTemporaryFile()\n        src_text_file.write(\'\\n\'.join(src_text).encode(""utf-8""))\n        src_text_file.flush()\n        self._src_text_file = src_text_file\n\n        tgt_text = [\'This is a sentence from target .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 target\']\n        tgt_text_file = tempfile.NamedTemporaryFile()\n        tgt_text_file.write(\'\\n\'.join(tgt_text).encode(""utf-8""))\n        tgt_text_file.flush()\n        self._tgt_text_file = tgt_text_file\n\n        self._data_hparams = {\n            ""num_epochs"": 20,\n            ""batch_size"": 2,\n            ""source_dataset"": {\n                ""files"": [self._src_text_file.name],\n                ""vocab_file"": self._vocab_file.name,\n            },\n            ""target_dataset"": {\n                ""files"": self._tgt_text_file.name,\n                ""vocab_share"": True,\n            }\n        }\n\n    def test_execute_seq2seq(self):\n        """"""Tests running seq2seq with Executor.\n        """"""\n        seq2seq = BasicSeq2seq(self._data_hparams)\n        data_hparams = {\'train\': self._data_hparams, \'eval\': self._data_hparams}\n\n        model_dir = tempfile.mkdtemp()\n        config = tf.estimator.RunConfig(\n            model_dir=model_dir,\n            save_summary_steps=10,\n            save_checkpoints_steps=10,\n            save_checkpoints_secs=None)\n\n        exor = Executor(model=seq2seq, data_hparams=data_hparams, config=config)\n\n        exor.train_and_evaluate(max_train_steps=20, eval_steps=5)\n\n        exor.train(max_steps=20)\n        exor.evaluate(steps=5)\n\n        shutil.rmtree(model_dir)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/utils/average_recorder_test.py,5,"b'""""""\nUnit tests for average recoder.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.average_recorder import _SingleAverageRecorder, AverageRecorder\n\n\nclass AverageRecorderTest(tf.test.TestCase):\n    """"""Tests average recoder.\n    """"""\n\n    def test_single_average_recoder(self):\n        """"""Tests :class:`~texar.tf.utils._SingleAverageRecorder`\n        """"""\n        recoder = _SingleAverageRecorder(5)\n        for i in range(100):\n            self.assertEqual(recoder.add(1), 1.)\n            self.assertEqual(recoder.avg(), 1.)\n\n        recoder = _SingleAverageRecorder()\n        for i in range(100):\n            self.assertEqual(recoder.add(1), 1.)\n            self.assertEqual(recoder.avg(), 1.)\n\n        def _cal_ground_truth(n):\n            """"""Calculates ((n-4)^2 + ... + n^5) / (n-4 + ... + n)\n            """"""\n            lb = max(n - 4, 0)\n            _sum = 0\n            _w = 0\n            for i in range(lb, n + 1):\n                _sum += i * i\n                _w += i\n            if _w == 0:\n                return 0\n            return _sum / _w\n\n        recoder = _SingleAverageRecorder(5)\n        for i in range(100):\n            self.assertEqual(recoder.add(i, i), _cal_ground_truth(i))\n            self.assertEqual(recoder.avg(), _cal_ground_truth(i))\n\n    def test_average_recorder(self):\n        """"""Tests :class:`~texar.tf.utils.AverageRecorder`\n        """"""\n        recorder = AverageRecorder(5)\n        for i in range(100):\n            self.assertEqual(recorder.add([1., 2.]), [1., 2.])\n            self.assertEqual(recorder.add([1.]), [1., 2.])\n            self.assertEqual(recorder.avg(), [1., 2.])\n            self.assertEqual(recorder.avg(0), 1.)\n            self.assertEqual(recorder.avg(1), 2.)\n            self.assertEqual(recorder.avg([0, 1]), [1., 2.])\n\n        recorder = AverageRecorder()\n        for i in range(100):\n            self.assertEqual(recorder.add({\'1\': 1, \'2\': 2}), {\'1\': 1., \'2\': 2.})\n            self.assertEqual(recorder.add({\'1\': 1}), {\'1\': 1., \'2\': 2.})\n            self.assertEqual(recorder.avg(), {\'1\': 1., \'2\': 2.})\n            self.assertEqual(recorder.avg(\'1\'), 1.)\n            self.assertEqual(recorder.avg(\'2\'), 2.)\n            self.assertEqual(recorder.avg([\'1\', \'2\']), {\'1\': 1., \'2\': 2.})\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/utils/mode_test.py,8,"b'\n""""""\nUnit tests for mode-related utility functions.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils import mode\nfrom texar.tf import context\n\n\nclass UtilsTest(tf.test.TestCase):\n    """"""Tests utility functions.\n    """"""\n\n    def test_mode(self):\n        """""" Tests mode related utilities.\n        """"""\n        training = mode.is_train_mode(None)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            training_ = sess.run(training)\n            self.assertTrue(training_)\n\n            training_ = sess.run(\n                training,\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN})\n            self.assertTrue(training_)\n\n            training_ = sess.run(\n                training,\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.EVAL})\n            self.assertFalse(training_)\n\n        training = mode.is_train_mode(tf.estimator.ModeKeys.TRAIN)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            training_ = sess.run(training)\n            self.assertTrue(training_)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/utils/shapes_test.py,13,"b'""""""\nUnit tests for shape-related utility functions.\n""""""\n\n# pylint: disable=no-member\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.utils import shapes\n\n\nclass ShapesTest(tf.test.TestCase):\n    """"""Tests shape-related utility functions.\n    """"""\n\n    def test_mask_sequences(self):\n        """"""Tests :func:`texar.tf.utils.shapes.mask_sequences`.\n        """"""\n        seq = np.ones([3, 4, 3], dtype=np.int32)\n        seq_length = np.array([3, 2, 1], dtype=np.int32)\n\n        masked_seq = shapes.mask_sequences(seq, seq_length)\n        self.assertEqual(masked_seq.shape, seq.shape)\n        seq_sum = np.sum(masked_seq, axis=(1, 2))\n        np.testing.assert_array_equal(seq_sum, seq_length * 3)\n\n    def test_reduce_with_weights(self):\n        """"""Tests :func:`texar.tf.utils.shapes.reduce_with_weights`\n        """"""\n        x = np.asarray([[10, 10, 2, 2],\n                        [20, 2, 2, 2]])\n        x = tf.constant(x)\n        w = np.asarray([[1, 1, 0, 0],\n                        [1, 0, 0, 0]])\n\n        z = shapes.reduce_with_weights(x, weights=w)\n\n        with self.test_session() as sess:\n            z_ = sess.run(z)\n            np.testing.assert_array_equal(z_, 20)\n\n    def test_pad_and_concat(self):\n        """"""Test :func:`texar.tf.utils.shapes.pad_and_concat`.\n        """"""\n        a = tf.ones([3, 10, 2])\n        b = tf.ones([4, 20, 3])\n        c = tf.ones([5, 1, 4])\n\n        t = shapes.pad_and_concat([a, b, c], 0)\n        self.assertEqual(t.shape, [3 + 4 + 5, 20, 4])\n        t = shapes.pad_and_concat([a, b, c], 1)\n        self.assertEqual(t.shape, [5, 10 + 20 + 1, 4])\n        t = shapes.pad_and_concat([a, b, c], 2)\n        self.assertEqual(t.shape, [5, 20, 2 + 3 + 4])\n\n        d = tf.placeholder(dtype=tf.float32, shape=[6, None, 1])\n        t = shapes.pad_and_concat([a, b, c, d], 0)\n        with self.test_session() as sess:\n            t_ = sess.run(t, feed_dict={d: np.ones([6, 2, 1])})\n            self.assertEqual(list(t_.shape), [3 + 4 + 5 + 6, 20, 4])\n\n    def test_varlength_concat(self):\n        """"""\n        Tests :func:`texar.tf.utils.shapes.varlength_concat`.\n        """"""\n        # 2D\n        x = np.asarray(\n            [[1, 1, 0, 0],\n             [1, 0, 0, 0],\n             [1, 1, 1, 1]], dtype=np.int32)\n        x_length = np.asarray([2, 1, 4], dtype=np.int32)\n        y = np.asarray(\n            [[2, 2, 2, 0],\n             [2, 2, 2, 2],\n             [2, 2, 0, 0]], dtype=np.int32)\n\n        z_true = np.asarray(\n            [[1, 1, 2, 2, 2, 0, 0, 0],\n             [1, 2, 2, 2, 2, 0, 0, 0],\n             [1, 1, 1, 1, 2, 2, 0, 0]], dtype=np.int32)\n\n        # py\n        z = shapes.varlength_concat_py(x, y, x_length)\n        np.testing.assert_array_equal(z, z_true)\n\n        # tf\n        z = shapes.varlength_concat(x, y, x_length)\n        with self.test_session() as sess:\n            z_ = sess.run(z)\n            np.testing.assert_array_equal(z_, z_true)\n\n        # 3D\n        x = np.asarray(\n            [[[1], [1], [0], [0]],\n             [[1], [0], [0], [0]],\n             [[1], [1], [1], [1]]], dtype=np.int32)\n        x_length = [2, 1, 4]\n        y = np.asarray(\n            [[[2], [2], [2], [0]],\n             [[2], [2], [2], [2]],\n             [[2], [2], [0], [0]]], dtype=np.int32)\n        z_true = np.asarray(\n            [[[1], [1], [2], [2], [2], [0], [0], [0]],\n             [[1], [2], [2], [2], [2], [0], [0], [0]],\n             [[1], [1], [1], [1], [2], [2], [0], [0]]], dtype=np.int32)\n\n        # py\n        z = shapes.varlength_concat_py(x, y, x_length)\n        np.testing.assert_array_equal(z, z_true)\n\n        # tf\n        z = shapes.varlength_concat(x, y, x_length)\n        with self.test_session() as sess:\n            z_ = sess.run(z)\n            np.testing.assert_array_equal(z_, z_true)\n\n    def test_varlength_roll(self):\n        """"""\n        Tests :func:`texar.tf.utils.shapes.varlength_roll`.\n        """"""\n        # 2D\n        x = np.asarray(\n            [[1, 1, 0, 0],\n             [1, 0, 0, 0],\n             [1, 1, 1, 1]], dtype=np.int32)\n        x_length = [-2, -1, -4]\n        z = shapes.varlength_roll(x, x_length)\n\n        with self.test_session() as sess:\n            z_ = sess.run(z)\n\n            z_true = np.asarray(\n                [[0, 0, 1, 1],\n                 [0, 0, 0, 1],\n                 [1, 1, 1, 1]], dtype=np.int32)\n\n            np.testing.assert_array_equal(z_, z_true)\n\n        # 3D\n        x = np.asarray(\n            [[[1], [1], [0], [0]],\n             [[1], [0], [0], [0]],\n             [[1], [1], [1], [1]]], dtype=np.int32)\n        x_length = [-2, -1, -4]\n        z = shapes.varlength_roll(x, x_length)\n\n        with self.test_session() as sess:\n            z_ = sess.run(z)\n\n            z_true = np.asarray(\n                [[[0], [0], [1], [1]],\n                 [[0], [0], [0], [1]],\n                 [[1], [1], [1], [1]]], dtype=np.int32)\n\n            np.testing.assert_array_equal(z_, z_true)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/utils/utils_test.py,12,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for utility functions.\n""""""\n\nimport tempfile\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.utils import utils\nfrom texar.tf.data.vocabulary import Vocab\n\n\nclass UtilsTest(tf.test.TestCase):\n    """"""Tests utility functions.\n    """"""\n\n    def test_dict_patch(self):\n        """"""Tests :meth:`texar.tf.utils.dict_patch`.\n        """"""\n        src_dict = {\n            ""k1"": ""k1"",\n            ""k_dict_1"": {\n                ""kd1_k1"": ""kd1_k1"",\n                ""kd1_k2"": ""kd1_k2""\n            },\n            ""k_dict_2"": {\n                ""kd2_k1"": ""kd2_k1""\n            }\n        }\n        tgt_dict = {\n            ""k1"": ""k1_tgt"",\n            ""k_dict_1"": {\n                ""kd1_k1"": ""kd1_k1""\n            },\n            ""k_dict_2"": ""kd2_not_dict""\n        }\n\n        patched_dict = utils.dict_patch(tgt_dict, src_dict)\n        self.assertEqual(patched_dict[""k1""], tgt_dict[""k1""])\n        self.assertEqual(patched_dict[""k_dict_1""], src_dict[""k_dict_1""])\n        self.assertEqual(patched_dict[""k_dict_2""], tgt_dict[""k_dict_2""])\n\n    def test_strip_token(self):\n        """"""Tests :func:`texar.tf.utils.strip_token`\n        """"""\n        str_ = "" <PAD>  <PAD>\\t  i am <PAD> \\t <PAD>  \\t""\n        self.assertEqual(utils.strip_token(str_, ""<PAD>""), ""i am"")\n        self.assertEqual(utils.strip_token(str_, """"),\n                         ""<PAD> <PAD> i am <PAD> <PAD>"")\n        self.assertEqual(utils.strip_token([str_], ""<PAD>""), [""i am""])\n        self.assertEqual(\n            utils.strip_token(np.asarray([str_]), ""<PAD>""),\n            [""i am""])\n        self.assertEqual(type(utils.strip_token(np.asarray([str_]), ""<PAD>"")),\n                         np.ndarray)\n        self.assertEqual(\n            utils.strip_token([[[str_]], [\'\']], ""<PAD>""),\n            [[[""i am""]], [\'\']])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_token(str_, ""<PAD>"", is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_token([str_], ""<PAD>"", is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_strip_bos(self):\n        """"""Tests :func:`texar.tf.utils.strip_bos`\n        """"""\n        str_ = ""<BOS> i am""\n        self.assertEqual(utils.strip_bos(str_, ""<BOS>""), ""i am"")\n        self.assertEqual(utils.strip_bos(str_, """"), ""<BOS> i am"")\n        self.assertEqual(utils.strip_bos([str_], ""<BOS>""), [""i am""])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_bos(str_, ""<BOS>"", is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_bos([str_], ""<BOS>"", is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_strip_eos(self):\n        """"""Tests :func:`texar.tf.utils.strip_eos`\n        """"""\n        str_ = ""i am <EOS>""\n        self.assertEqual(utils.strip_eos(str_, ""<EOS>""), ""i am"")\n        self.assertEqual(utils.strip_eos([str_], ""<EOS>""), [""i am""])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_eos(str_, ""<EOS>"", is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_eos([str_], ""<EOS>"", is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_strip_special_tokens(self):\n        """"""Test :func:`texar.tf.utils.strip_special_tokens`\n        """"""\n        str_ = ""<BOS> i am <EOS> <PAD> <PAD>""\n        self.assertEqual(utils.strip_special_tokens(str_), ""i am"")\n        self.assertEqual(utils.strip_special_tokens([str_]), [""i am""])\n\n        str_ = str_.split()\n        self.assertEqual(utils.strip_special_tokens(str_, is_token_list=True),\n                         [""i"", ""am""])\n        self.assertEqual(utils.strip_special_tokens([str_], is_token_list=True),\n                         [[""i"", ""am""]])\n\n    def test_str_join(self):\n        """"""Tests :func:`texar.tf.utils.str_join`\n        """"""\n        tokens = np.ones([2, 2, 3], dtype=\'str\')\n\n        str_ = utils.str_join(tokens)\n        np.testing.assert_array_equal(\n            str_, np.asarray([[\'1 1 1\', \'1 1 1\'], [\'1 1 1\', \'1 1 1\']]))\n        self.assertIsInstance(str_, np.ndarray)\n\n        str_ = utils.str_join(tokens.tolist())\n        np.testing.assert_array_equal(\n            str_, [[\'1 1 1\', \'1 1 1\'], [\'1 1 1\', \'1 1 1\']])\n        self.assertIsInstance(str_, list)\n\n        tokens = [[], [\'1\', \'1\']]\n        str_ = utils.str_join(tokens)\n        np.testing.assert_array_equal(str_, [\'\', \'1 1\'])\n\n    def test_uniquify_str(self):\n        """"""Tests :func:`texar.tf.utils.uniquify_str`.\n        """"""\n        str_set = [\'str\']\n        unique_str = utils.uniquify_str(\'str\', str_set)\n        self.assertEqual(unique_str, \'str_1\')\n\n        str_set.append(\'str_1\')\n        str_set.append(\'str_2\')\n        unique_str = utils.uniquify_str(\'str\', str_set)\n        self.assertEqual(unique_str, \'str_3\')\n\n    def test_map_ids_to_strs(self):\n        """"""Tests :func:`texar.tf.utils.map_ids_to_strs`.\n        """"""\n        vocab_list = [\'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        vocab = Vocab(vocab_file.name)\n\n        text = [[\'<BOS>\', \'word\', \'\xe8\xaf\x8d\', \'<EOS>\', \'<PAD>\'],\n                [\'word\', \'\xe8\xaf\x8d\', \'word\', \'\xe8\xaf\x8d\', \'<PAD>\']]\n        text = np.asarray(text)\n        ids = vocab.map_tokens_to_ids_py(text)\n\n        ids = ids.tolist()\n        text_ = utils.map_ids_to_strs(ids, vocab)\n\n        self.assertEqual(text_[0], \'word \xe8\xaf\x8d\')\n        self.assertEqual(text_[1], \'word \xe8\xaf\x8d word \xe8\xaf\x8d\')\n\n    def test_truncate_seq_pair(self):\n\n        tokens_a = [1, 2, 3]\n        tokens_b = [4, 5, 6]\n        utils.truncate_seq_pair(tokens_a, tokens_b, 4)\n        self.assertListEqual(tokens_a, [1, 2])\n        self.assertListEqual(tokens_b, [4, 5])\n\n        tokens_a = [1]\n        tokens_b = [2, 3, 4, 5]\n        utils.truncate_seq_pair(tokens_a, tokens_b, 3)\n        self.assertListEqual(tokens_a, [1])\n        self.assertListEqual(tokens_b, [2, 3])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
texar/tf/__init__.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library.\n""""""\n\n# pylint: disable=wildcard-import\n\nimport pkg_resources\nimport tensorflow as tf\n\nVERSION_WARNING = ""1.13.2""\n\n\nif (pkg_resources.parse_version(tf.__version__) <=\n        pkg_resources.parse_version(VERSION_WARNING)):\n    tf.logging.set_verbosity(tf.logging.ERROR)\nelse:\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\nfrom texar.tf.version import VERSION as __version__\n\nfrom texar.tf import agents\nfrom texar.tf import core\nfrom texar.tf import data\nfrom texar.tf import evals\nfrom texar.tf import losses\nfrom texar.tf import models\nfrom texar.tf import modules\nfrom texar.tf import run\nfrom texar.tf import utils\nfrom texar.tf.module_base import *\nfrom texar.tf.hyperparams import *\nfrom texar.tf.context import *\n'"
texar/tf/context.py,18,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGlobal context manager that handles train/infer mode, etc\n""""""\n\nimport tensorflow as tf\n\n__all__ = [\n    ""global_mode"",\n    ""global_mode_train"",\n    ""global_mode_eval"",\n    ""global_mode_predict"",\n    ""valid_modes""\n]\n\n_GLOBAL_MODE_KEY = ""GLOBAL_MODE""\n\n\ndef global_mode():\n    """"""Returns the Tensor of global mode.\n\n    This is a placeholder with default value of\n    :tf_main:`tf.estimator.ModeKeys.TRAIN <estimator/ModeKeys>`.\n\n    Example:\n\n        .. code-block:: python\n\n            mode = session.run(global_mode())\n            # mode == tf.estimator.ModeKeys.TRAIN\n\n            mode = session.run(\n                global_mode(),\n                feed_dict={tf.global_mode(): tf.estimator.ModeKeys.PREDICT})\n            # mode == tf.estimator.ModeKeys.PREDICT\n    """"""\n    mode = tf.get_collection_ref(_GLOBAL_MODE_KEY)\n    if len(mode) < 1:\n        # mode_tensor = tf.placeholder(tf.string, name=""global_mode"")\n        mode_tensor = tf.placeholder_with_default(\n            input=tf.estimator.ModeKeys.TRAIN,\n            shape=(),\n            name=""global_mode"")\n        # mode_tensor = tf.constant(\n        #    value=tf.estimator.ModeKeys.TRAIN,\n        #    dtype=tf.string,\n        #    name=""global_mode"")\n        mode.append(mode_tensor)\n    return mode[0]\n\n\ndef global_mode_train():\n    """"""Returns a bool Tensor indicating whether the global mode is TRAIN.\n\n    Example:\n\n        .. code-block:: python\n\n            is_train = session.run(global_mode_train())\n            # is_train == True\n\n            is_train = session.run(\n                global_mode_train()\n                feed_dict={tf.global_mode(): tf.estimator.ModeKeys.PREDICT})\n            # is_train == False\n    """"""\n    mode = global_mode()\n    return tf.equal(mode, tf.estimator.ModeKeys.TRAIN)\n\n\ndef global_mode_eval():\n    """"""Returns a bool Tensor indicating whether the global mode is EVAL.\n    """"""\n    mode = global_mode()\n    return tf.equal(mode, tf.estimator.ModeKeys.EVAL)\n\n\ndef global_mode_predict():\n    """"""Returns a bool Tensor indicating whether the global mode is PREDICT.\n    """"""\n    mode = global_mode()\n    return tf.equal(mode, tf.estimator.ModeKeys.PREDICT)\n\n\ndef valid_modes():\n    """"""Returns a set of possible values of mode.\n    """"""\n    return {tf.estimator.ModeKeys.TRAIN,\n            tf.estimator.ModeKeys.EVAL,\n            tf.estimator.ModeKeys.PREDICT}\n'"
texar/tf/hyperparams.py,5,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHyperparameter manager\n""""""\n\nimport copy\nimport json\n\n\n__all__ = [\n    ""HParams""\n]\n\n\ndef _type_name(value):\n    return type(value).__name__\n\n\nclass HParams(object):\n    r""""""A class that maintains hyperparameters for configuring Texar modules.\n    The class has several useful features:\n\n    - **Auto-completion of missing values.** Users can specify only a subset of\n      hyperparameters they care about. Other hyperparameters will automatically\n      take the default values. The auto-completion performs **recursively** so\n      that hyperparameters taking `dict` values will also be auto-completed\n      **All Texar modules** provide a :meth:`default_hparams` containing\n      allowed hyperparameters and their default values. For example\n\n        .. code-block:: python\n\n            ## Recursive auto-completion\n            default_hparams = {""a"": 1, ""b"": {""c"": 2, ""d"": 3}}\n            hparams = {""b"": {""c"": 22}}\n            hparams_ = HParams(hparams, default_hparams)\n            hparams_.todict() == {""a"": 1, ""b"": {""c"": 22, ""d"": 3}}\n                # ""a"" and ""d"" are auto-completed\n\n            ## All Texar modules have built-in `default_hparams`\n            hparams = {""dropout_rate"": 0.1}\n            emb = tx.modules.WordEmbedder(hparams=hparams, ...)\n            emb.hparams.todict() == {\n                ""dropout_rate"": 0.1,  # provided value\n                ""dim"": 100,           # default value\n                ...\n            }\n\n    - **Automatic typecheck.** For most hyperparameters, provided value must\n      have the same or compatible dtype with the default value. :class:`HParams`\n      does necessary type-check, and raises Error if improper dtype is provided.\n      Also, hyperparameters not listed in `default_hparams` are not allowed,\n      except for `""kwargs""` as detailed below.\n\n    - **Flexible dtype for specified hyperparameters.**  Some hyperparameters\n      may allow different dtypes of values.\n\n        - Hyperparameters named `""type""` are not type-checked.\n          For example, in :func:`~texar.tf.core.get_rnn_cell`, hyperparameter\n          `""type""` can take value of an RNNCell class, its string name of module\n          path, or an RNNCell class instance. (String name or module path is\n          allowed so that users can specify the value in YAML configuration\n          files.)\n\n        - For other hyperparameters, list them in the ""@no_typecheck"" field\n          in :meth:`default_hparams` to skip type-check. For example, in\n          :func:`~texar.tf.core.get_rnn_cell`, hyperparameter\n          `""\\*_keep_prob""` can be set to either a `float` or a `tf.placeholder`.\n\n    - **Special flexibility of keyword argument hyparameters.**\n      Hyperparameters named ``""kwargs""`` are used as keyword arguments for a\n      class constructor or a function call. Such hyperparameters take a `dict`,\n      and users can add arbitrary valid keyword arguments to the dict.\n      For example:\n\n        .. code-block:: python\n\n            default_rnn_cell_hparams = {\n                ""type"": ""LSTMCell"",\n                ""kwargs"": {""num_units"": 256}\n                # Other hyperparameters\n                ...\n            }\n            my_hparams = {\n                ""kwargs"" {\n                    ""num_units"": 123,\n                    # Other valid keyword arguments for LSTMCell constructor\n                    ""forget_bias"": 0.0\n                    ""activation"": ""tf.nn.relu""\n                }\n            }\n            _ = HParams(my_hparams, default_rnn_cell_hparams)\n\n    - **Rich interfaces.** An :class:`HParams` instance provides rich interfaces\n      for accessing, updating, or adding hyperparameters.\n\n        .. code-block:: python\n\n            hparams = HParams(my_hparams, default_hparams)\n            # Access\n            hparams.type == hparams[""type""]\n            # Update\n            hparams.type = ""GRUCell""\n            hparams.kwargs = { ""num_units"": 100 }\n            hparams.kwargs.num_units == 100\n            # Add new\n            hparams.add_hparam(""index"", 1)\n            hparams.index == 1\n\n            # Convert to `dict` (recursively)\n            type(hparams.todic()) == dict\n\n            # I/O\n            pickle.dump(hparams, ""hparams.dump"")\n            with open(""hparams.dump"", \'rb\') as f:\n                hparams_loaded = pickle.load(f)\n\n\n    Args:\n        hparams: A `dict` or an :class:`HParams` instance containing\n            hyperparameters. If `None`, all hyperparameters are set to default\n            values.\n        default_hparams (dict): Hyperparameters with default values. If `None`,\n            Hyperparameters are fully defined by :attr:`hparams`.\n        allow_new_hparam (bool): If `False` (default), :attr:`hparams` cannot\n            contain hyperparameters that are not included in\n            :attr:`default_hparams`, except for the case of :attr:`""kwargs""` as\n            above.\n    """"""\n    # - The default hyperparameters in :attr:`""kwargs""` are used (for type-check\n    #   and complementing missing hyperparameters) only when :attr:`""type""`\n    #   takes default value (i.e., missing in :attr:`hparams` or set to\n    #   the same value with the default). In this case :attr:`kwargs` allows to\n    #   contain new keys not included in :attr:`default_hparams[""kwargs""]`.\n    #\n    # - If :attr:`""type""` is set to an other value and :attr:`""kwargs""` is\n    #   missing in :attr:`hparams`, :attr:`""kwargs""` is set to an empty\n    #   dictionary.\n\n    def __init__(self, hparams, default_hparams, allow_new_hparam=False):\n        if isinstance(hparams, HParams):\n            hparams = hparams.todict()\n        if default_hparams is not None:\n            parsed_hparams = self._parse(\n                hparams, default_hparams, allow_new_hparam)\n        else:\n            parsed_hparams = self._parse(hparams, hparams)\n        super(HParams, self).__setattr__(\'_hparams\', parsed_hparams)\n\n    @staticmethod\n    def _parse(hparams,\n               default_hparams,\n               allow_new_hparam=False):\n        """"""Parses hyperparameters.\n\n        Args:\n            hparams (dict): Hyperparameters. If `None`, all hyperparameters are\n                set to default values.\n            default_hparams (dict): Hyperparameters with default values.\n                If `None`,Hyperparameters are fully defined by :attr:`hparams`.\n            allow_new_hparam (bool): If `False` (default), :attr:`hparams`\n                cannot contain hyperparameters that are not included in\n                :attr:`default_hparams`, except the case of :attr:`""kwargs""`.\n\n        Return:\n            A dictionary of parsed hyperparameters. Returns `None` if both\n            :attr:`hparams` and :attr:`default_hparams` are `None`.\n\n        Raises:\n            ValueError: If :attr:`hparams` is not `None` and\n                :attr:`default_hparams` is `None`.\n            ValueError: If :attr:`default_hparams` contains ""kwargs"" not does\n                not contains ""type"".\n        """"""\n        if hparams is None and default_hparams is None:\n            return None\n\n        if hparams is None:\n            return HParams._parse(default_hparams, default_hparams)\n\n        if default_hparams is None:\n            raise ValueError(""`default_hparams` cannot be `None` if `hparams` ""\n                             ""is not `None`."")\n        no_typecheck_names = default_hparams.get(""@no_typecheck"", [])\n\n        if ""kwargs"" in default_hparams and ""type"" not in default_hparams:\n            raise ValueError(""Ill-defined hyperparameter structure: \'kwargs\' ""\n                             ""must accompany with \'type\'."")\n\n        parsed_hparams = copy.deepcopy(default_hparams)\n\n        # Parse recursively for params of type dictionary that are missing\n        # in `hparams`.\n        for name, value in default_hparams.items():\n            if name not in hparams and isinstance(value, dict):\n                if name == ""kwargs"" and ""type"" in hparams and \\\n                        hparams[""type""] != default_hparams[""type""]:\n                    # Set params named ""kwargs"" to empty dictionary if ""type""\n                    # takes value other than default.\n                    parsed_hparams[name] = HParams({}, {})\n                else:\n                    parsed_hparams[name] = HParams(value, value)\n\n        from texar.tf.utils.dtypes import is_callable\n\n        # Parse hparams\n        for name, value in hparams.items():\n            if name not in default_hparams:\n                if allow_new_hparam:\n                    parsed_hparams[name] = HParams._parse_value(value, name)\n                    continue\n                else:\n                    raise ValueError(\n                        ""Unknown hyperparameter: %s. Only hyperparameters ""\n                        ""named \'kwargs\' hyperparameters can contain new ""\n                        ""entries undefined in default hyperparameters."" % name)\n\n            if value is None:\n                parsed_hparams[name] = \\\n                    HParams._parse_value(parsed_hparams[name])\n\n            default_value = default_hparams[name]\n            if default_value is None:\n                parsed_hparams[name] = HParams._parse_value(value)\n                continue\n\n            # Parse recursively for params of type dictionary.\n            if isinstance(value, dict):\n                if name not in no_typecheck_names \\\n                        and not isinstance(default_value, dict):\n                    raise ValueError(\n                        ""Hyperparameter \'%s\' must have type %s, got %s"" %\n                        (name, _type_name(default_value), _type_name(value)))\n                if name == ""kwargs"":\n                    if ""type"" in hparams and \\\n                            hparams[""type""] != default_hparams[""type""]:\n                        # Leave ""kwargs"" as-is if ""type"" takes value\n                        # other than default.\n                        parsed_hparams[name] = HParams(value, value)\n                    else:\n                        # Allow new hyperparameters if ""type"" takes default\n                        # value\n                        parsed_hparams[name] = HParams(\n                            value, default_value, allow_new_hparam=True)\n                elif name in no_typecheck_names:\n                    parsed_hparams[name] = HParams(value, value)\n                else:\n                    parsed_hparams[name] = HParams(\n                        value, default_value, allow_new_hparam)\n                continue\n\n            # Do not type-check hyperparameter named ""type"" and accompanied\n            # with ""kwargs""\n            if name == ""type"" and ""kwargs"" in default_hparams:\n                parsed_hparams[name] = value\n                continue\n\n            if name in no_typecheck_names:\n                parsed_hparams[name] = value\n            elif isinstance(value, type(default_value)):\n                parsed_hparams[name] = value\n            elif is_callable(value) and is_callable(default_value):\n                parsed_hparams[name] = value\n            else:\n                try:\n                    parsed_hparams[name] = type(default_value)(value)\n                except TypeError:\n                    raise ValueError(\n                        ""Hyperparameter \'%s\' must have type %s, got %s"" %\n                        (name, _type_name(default_value), _type_name(value)))\n\n        return parsed_hparams\n\n    @staticmethod\n    def _parse_value(value, name=None):\n        if isinstance(value, dict) and (name is None or name != ""kwargs""):\n            return HParams(value, None)\n        else:\n            return value\n\n    def __getattr__(self, name):\n        """"""Retrieves the value of the hyperparameter.\n        """"""\n        if name == \'_hparams\':\n            return super(HParams, self).__getattribute__(\'_hparams\')\n        if name not in self._hparams:\n            # Raise AttributeError to allow copy.deepcopy, etc\n            raise AttributeError(""Unknown hyperparameter: %s"" % name)\n        return self._hparams[name]\n\n    def __getitem__(self, name):\n        """"""Retrieves the value of the hyperparameter.\n        """"""\n        return self.__getattr__(name)\n\n    def __setattr__(self, name, value):\n        """"""Sets the value of the hyperparameter.\n        """"""\n        if name not in self._hparams:\n            raise ValueError(\n                ""Unknown hyperparameter: %s. Only the `kwargs` ""\n                ""hyperparameters can contain new entries undefined ""\n                ""in default hyperparameters."" % name)\n        self._hparams[name] = self._parse_value(value, name)\n\n    def items(self):\n        """"""Returns the list of hyperparam `(name, value)` pairs\n        """"""\n        return iter(self)\n\n    def keys(self):\n        """"""Returns the list of hyperparam names\n        """"""\n        return self._hparams.keys()\n\n    def __iter__(self):\n        for name, value in self._hparams.items():\n            yield name, value\n\n    def __len__(self):\n        return len(self._hparams)\n\n    def __contains__(self, name):\n        return name in self._hparams\n\n    def __str__(self):\n        """"""Return a string of the hparams.\n        """"""\n        hparams_dict = self.todict()\n        return json.dumps(hparams_dict, sort_keys=True, indent=2)\n\n    def get(self, name, default=None):\n        """"""Returns the hyperparameter value for the given name. If name is not\n        available then returns :attr:`default`.\n\n        Args:\n            name (str): the name of hyperparameter.\n            default: the value to be returned in case name does not exist.\n        """"""\n        try:\n            return self.__getattr__(name)\n        except AttributeError:\n            return default\n\n    def add_hparam(self, name, value):\n        """"""Adds a new hyperparameter.\n        """"""\n        if (name in self._hparams) or hasattr(self, name):\n            raise ValueError(""Hyperparameter name already exists: %s"" % name)\n        self._hparams[name] = self._parse_value(value, name)\n\n    def todict(self):\n        """"""Returns a copy of hyperparameters as a dictionary.\n        """"""\n        dict_ = copy.deepcopy(self._hparams)\n        for name, value in self._hparams.items():\n            if isinstance(value, HParams):\n                dict_[name] = value.todict()\n        return dict_\n'"
texar/tf/module_base.py,7,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for modules.\n""""""\n\nimport re\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.exceptions import TexarError\nfrom texar.tf.hyperparams import HParams\n\n__all__ = [\n    ""ModuleBase""\n]\n\n\nclass ModuleBase(object):\n    """"""Base class inherited by modules that create Variables and are\n    configurable through hyperparameters.\n\n    A Texar module inheriting :class:`~texar.tf.ModuleBase` has following key\n    features:\n\n        - **Convenient variable re-use**: A module instance creates \\\n        its own sets of variables, and automatically re-uses its variables on \\\n        subsequent calls. Hence TF variable/name scope is \\\n        transparent to users. For example:\n\n            .. code-block:: python\n\n                encoder = UnidirectionalRNNEncoder(hparams) # create instance\n                output_1 = encoder(inputs_1) # variables are created\n                output_2 = encoder(inputs_2) # variables are re-used\n\n                print(encoder.trainable_variables) # access trainable variables\n                # [ ... ]\n\n        - **Configurable through hyperparameters**: Each module defines \\\n        allowed hyperparameters and default values. Hyperparameters not \\\n        specified by users will take default values.\n\n        - **Callable**: As the above example, a module instance is ""called"" \\\n        with input tensors and returns output tensors. Every call of a module \\\n        will add ops to the Graph to perform the module\'s logic.\n\n    Args:\n        hparams (dict, optional): Hyperparameters of the module. See\n            :meth:`default_hparams` for the structure and default values.\n\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, hparams=None):\n        if not hasattr(self, \'_hparams\'):\n            self._hparams = HParams(hparams, self.default_hparams())\n        else:\n            # Probably already parsed by subclasses. We rely on subclass\n            # implementations to get this right.\n            # As a sanity check, we require `hparams` to be `None` in this case.\n            if hparams is not None:\n                raise ValueError(\n                    ""`self._hparams` already exists. Argument `hparams` ""\n                    ""must be set to `None` in this case."")\n        self._template = tf.make_template(self._hparams.name, self._build,\n                                          create_scope_now_=True)\n        self._unique_name = self.variable_scope.name.split(""/"")[-1]\n        self._trainable_variables = []\n        self._built = False\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a `dict` of hyperparameters of the module with default\n        values. Used to replace the missing values of input `hparams`\n        during module construction.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""module""\n            }\n        """"""\n        return {\n            ""name"": ""module""\n        }\n\n    def _build(self, *args, **kwargs):\n        """"""Subclass must implement this method to build the logic.\n\n        Args:\n            *args: Arguments.\n            **kwargs: Keyword arguments.\n\n        Returns:\n            Output Tensor(s).\n        """"""\n        raise NotImplementedError\n\n    def __call__(self, *args, **kwargs):\n        """"""Executes the module logic defined in _build method\n\n        Args:\n            *args: Arguments of _build method.\n            **kwargs: Keyword arguments of _build method.\n\n        Returns:\n            The output of _build method.\n        """"""\n        return self._template(*args, **kwargs)\n\n    def _add_internal_trainable_variables(self):  # pylint: disable=invalid-name\n        """"""Collects trainable variables constructured internally in this module.\n\n        This is typically called at the end of `_build()` where all necessary\n        trainable variables have been constructed.\n        """"""\n        scope_name = self.variable_scope.name\n        # Escape to handle possible ""."" characters in the name.\n        # Append a slash to the end to avoid searching scopes that have this\n        # scope name as a prefix.\n        scope_name = re.escape(scope_name) + ""/""\n        internal_trainable_variables = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name)\n        self._add_trainable_variable(internal_trainable_variables)\n\n    def _add_trainable_variable(self, variable):\n        """"""Adds a trainable variable to the trainable variable list of the\n        module.\n\n        Args:\n            variable: a (list of) trainable variable(s) constructed either\n                internally in the module or constructured outside but used\n                inside the module.\n        """"""\n        if isinstance(variable, (list, tuple)):\n            for var in variable:\n                self._add_trainable_variable(var)\n        else:\n            if variable not in self._trainable_variables:\n                self._trainable_variables.append(variable)\n\n    @property\n    def variable_scope(self):\n        """"""The variable scope of the module.\n        """"""\n        return self._template.variable_scope\n\n    @property\n    def name(self):\n        """"""The uniquified name of the module.\n        """"""\n        return self._unique_name\n\n    @property\n    def trainable_variables(self):\n        """"""The list of trainable variables of the module.\n        """"""\n        if not self._built:\n            raise TexarError(\n                ""Attempting to access trainable_variables before module %s ""\n                ""was fully built. The module is built once it is called, ""\n                ""e.g., with `%s(...)`"" % (self.name, self.name))\n        return self._trainable_variables\n\n    @property\n    def hparams(self):\n        """"""An :class:`~texar.tf.HParams` instance. The hyperparameters\n        of the module.\n        """"""\n        return self._hparams\n'"
texar/tf/version.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n_MAJOR = ""0""\n_MINOR = ""2""\n_REVISION = ""4""\n\nVERSION_SHORT = ""{0}.{1}"".format(_MAJOR, _MINOR)\nVERSION = ""{0}.{1}.{2}"".format(_MAJOR, _MINOR, _REVISION)\n'"
examples/bert/data/download_glue_data.py,0,"b'""""""Script for downloading all GLUE data.\n\nAdapted from https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\n\n""""""\nimport argparse\nimport os\nimport sys\nimport urllib.request\nimport zipfile\n\nTASKS = [""CoLA"", ""SST"", ""MRPC"", ""QQP"", ""STS"", ""MNLI"", ""SNLI"", ""QNLI"",\n         ""RTE"", ""WNLI"", ""diagnostic""]\n\n# pylint: disable=line-too-long\n\nTASK2PATH = {\n    ""CoLA"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\',\n    ""SST"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\',\n    ""MRPC"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\',\n    ""QQP"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\',\n    ""STS"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\',\n    ""MNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\',\n    ""SNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\',\n    ""QNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\',\n    ""RTE"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\',\n    ""WNLI"": \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\',\n    ""diagnostic"": \'https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\'}\n\n# pylint: enable=line-too-long\n\n\ndef download_and_extract(task, data_dir):\n    print(""Downloading and extracting %s..."" % task)\n    data_file = ""%s.zip"" % task\n    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n    with zipfile.ZipFile(data_file) as zip_ref:\n        zip_ref.extractall(data_dir)\n    os.remove(data_file)\n    print(""\\tCompleted!"")\n\n\ndef format_mrpc(data_dir, path_to_data):\n    print(""Processing MRPC..."")\n    mrpc_dir = os.path.join(data_dir, ""MRPC"")\n    if not os.path.isdir(mrpc_dir):\n        os.mkdir(mrpc_dir)\n    if path_to_data:\n        mrpc_train_file = os.path.join(path_to_data, ""msr_paraphrase_train.txt"")\n        mrpc_test_file = os.path.join(path_to_data, ""msr_paraphrase_test.txt"")\n    else:\n        mrpc_train_file = os.path.join(mrpc_dir, ""msr_paraphrase_train.txt"")\n        mrpc_test_file = os.path.join(mrpc_dir, ""msr_paraphrase_test.txt"")\n    assert os.path.isfile(mrpc_train_file), \\\n        ""Train data not found at %s"" % mrpc_train_file\n    assert os.path.isfile(mrpc_test_file), \\\n        ""Test data not found at %s"" % mrpc_test_file\n    urllib.request.urlretrieve(TASK2PATH[""MRPC""],\n                               os.path.join(mrpc_dir, ""dev_ids.tsv""))\n\n    dev_ids = []\n    with open(os.path.join(mrpc_dir, ""dev_ids.tsv"")) as ids_fh:\n        for row in ids_fh:\n            dev_ids.append(row.strip().split(\'\\t\'))\n\n    with open(mrpc_train_file) as data_fh, \\\n            open(os.path.join(mrpc_dir, ""train.tsv""), \'w\') as train_fh, \\\n            open(os.path.join(mrpc_dir, ""dev.tsv""), \'w\') as dev_fh:\n        header = data_fh.readline()\n        train_fh.write(header)\n        dev_fh.write(header)\n        for row in data_fh:\n            label, id1, id2, s1, s2 = row.strip().split(\'\\t\')\n            if [id1, id2] in dev_ids:\n                dev_fh.write(""%s\\t%s\\t%s\\t%s\\t%s\\n"" % (label, id1, id2, s1, s2))\n            else:\n                train_fh.write(\n                    ""%s\\t%s\\t%s\\t%s\\t%s\\n"" % (label, id1, id2, s1, s2))\n    with open(mrpc_test_file) as data_fh, \\\n            open(os.path.join(mrpc_dir, ""test.tsv""), \'w\') as test_fh:\n        _ = data_fh.readline()\n        test_fh.write(""index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n"")\n        for idx, row in enumerate(data_fh):\n            label, id1, id2, s1, s2 = row.strip().split(\'\\t\')\n            test_fh.write(""%d\\t%s\\t%s\\t%s\\t%s\\n"" % (idx, id1, id2, s1, s2))\n    print(""\\tCompleted!"")\n\n\ndef download_diagnostic(data_dir):\n    print(""Downloading and extracting diagnostic..."")\n    if not os.path.isdir(os.path.join(data_dir, ""diagnostic"")):\n        os.mkdir(os.path.join(data_dir, ""diagnostic""))\n    data_file = os.path.join(data_dir, ""diagnostic"", ""diagnostic.tsv"")\n    urllib.request.urlretrieve(TASK2PATH[""diagnostic""], data_file)\n    print(""\\tCompleted!"")\n    return\n\n\ndef get_tasks(task_names):\n    task_names = task_names.split(\',\')\n    if ""all"" in task_names:\n        tasks = TASKS\n    else:\n        tasks = []\n        for task_name in task_names:\n            assert task_name in TASKS, ""Task %s not found!"" % task_name\n            tasks.append(task_name)\n    return tasks\n\n\ndef main(arguments):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--data_dir\', help=\'directory to save data to\',\n        type=str, default=\'data\')\n    parser.add_argument(\n        \'--tasks\',\n        help=\'tasks to download data for as a comma separated string\',\n        type=str, default=\'all\')\n    parser.add_argument(\n        \'--path_to_mrpc\',\n        help=\'path to directory containing extracted MRPC data, \'\n             \'msr_paraphrase_train.txt and msr_paraphrase_text.txt\',\n        type=str, default=\'\')\n    args = parser.parse_args(arguments)\n\n    if not os.path.isdir(args.data_dir):\n        os.mkdir(args.data_dir)\n    tasks = get_tasks(args.tasks)\n\n    for task in tasks:\n        if task == \'MRPC\':\n            import subprocess\n            if not os.path.exists(""data/MRPC""):\n                subprocess.run(""mkdir data/MRPC"", shell=True)\n            # pylint: disable=line-too-long\n            subprocess.run(\n                \'wget -P data/MRPC/ https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\',\n                shell=True)\n            subprocess.run(\n                \'wget -P data/MRPC/ https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\',\n                shell=True)\n            # pylint: enable=line-too-long\n            format_mrpc(args.data_dir, args.path_to_mrpc)\n            subprocess.run(\'rm data/MRPC/msr_paraphrase_train.txt\', shell=True)\n            subprocess.run(\'rm data/MRPC/msr_paraphrase_test.txt\', shell=True)\n        elif task == \'diagnostic\':\n            download_diagnostic(args.data_dir)\n        else:\n            download_and_extract(task, args.data_dir)\n\n\nif __name__ == \'__main__\':\n    sys.exit(main(sys.argv[1:]))\n'"
examples/bert/utils/__init__.py,0,b''
examples/bert/utils/data_utils.py,13,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThis is the Data Loading Pipeline for Sentence Classifier Task from:\n    `https://github.com/google-research/bert/blob/master/run_classifier.py`\n""""""\n\nimport os\nimport csv\nimport collections\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n\nclass InputExample():\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        """"""Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence.\n                For single sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second\n                sequence. Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n                specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures:\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for prediction.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""Reads a tab separated value file.""""""\n        with tf.gfile.Open(input_file, ""r"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                lines.append(line)\n        return lines\n\n\nclass SSTProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        if set_type == \'train\' or set_type == \'dev\':\n            for (i, line) in enumerate(lines):\n                if i == 0:\n                    continue\n                guid = ""%s-%s"" % (set_type, i)\n                text_a = tx.utils.compat_as_text(line[0])\n                # Single sentence classification, text_b doesn\'t exist\n                text_b = None\n                label = tx.utils.compat_as_text(line[1])\n                examples.append(InputExample(guid=guid, text_a=text_a,\n                                             text_b=text_b, label=label))\n        if set_type == \'test\':\n            for (i, line) in enumerate(lines):\n                if i == 0:\n                    continue\n                guid = ""%s-%s"" % (set_type, i)\n                text_a = tx.utils.compat_as_text(line[1])\n                # Single sentence classification, text_b doesn\'t exist\n                text_b = None\n                label = \'0\'  # arbitrary set as 0\n                examples.append(InputExample(guid=guid, text_a=text_a,\n                                             text_b=text_b, label=label))\n        return examples\n\n\nclass XnliProcessor(DataProcessor):\n    """"""Processor for the XNLI data set.""""""\n\n    def __init__(self):\n        self.language = ""zh""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        lines = self._read_tsv(\n            os.path.join(data_dir, ""multinli"",\n                         ""multinli.train.%s.tsv"" % self.language))\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""train-%d"" % (i)\n            text_a = tx.utils.compat_as_text(line[0])\n            text_b = tx.utils.compat_as_text(line[1])\n            label = tx.utils.compat_as_text(line[2])\n            if label == tx.utils.compat_as_text(""contradictory""):\n                label = tx.utils.compat_as_text(""contradiction"")\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        lines = self._read_tsv(os.path.join(data_dir, ""xnli.dev.tsv""))\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""dev-%d"" % (i)\n            language = tx.utils.compat_as_text(line[0])\n            if language != tx.utils.compat_as_text(self.language):\n                continue\n            text_a = tx.utils.compat_as_text(line[6])\n            text_b = tx.utils.compat_as_text(line[7])\n            label = tx.utils.compat_as_text(line[1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n            ""dev_matched"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")),\n            ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type,\n                              tx.utils.compat_as_text(line[0]))\n            text_a = tx.utils.compat_as_text(line[8])\n            text_b = tx.utils.compat_as_text(line[9])\n            if set_type == ""test"":\n                label = ""contradiction""\n            else:\n                label = tx.utils.compat_as_text(line[-1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")),\n            ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")),\n            ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = tx.utils.compat_as_text(line[3])\n            text_b = tx.utils.compat_as_text(line[4])\n            if set_type == ""test"":\n                label = ""0""\n            else:\n                label = tx.utils.compat_as_text(line[0])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples\n\n\nclass ColaProcessor(DataProcessor):\n    """"""Processor for the CoLA data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")),\n            ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")),\n            ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            # Only the test set has a header\n            if set_type == ""test"" and i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            if set_type == ""test"":\n                text_a = tx.utils.compat_as_text(line[1])\n                label = ""0""\n            else:\n                text_a = tx.utils.compat_as_text(line[3])\n                label = tx.utils.compat_as_text(line[1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=None, label=label))\n        return examples\n\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n    """"""Converts a single `InputExample` into a single `InputFeatures`.""""""\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    input_ids, segment_ids, input_mask = \\\n        tokenizer.encode_text(text_a=example.text_a,\n                              text_b=example.text_b,\n                              max_seq_length=max_seq_length)\n\n    label_id = label_map[example.label]\n\n    # here we disable the verbose printing of the data\n    if ex_index < 0:\n        tf.logging.info(""*** Example ***"")\n        tf.logging.info(""guid: %s"" % example.guid)\n        tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n        tf.logging.info(""input_ids length: %d"" % len(input_ids))\n        tf.logging.info(""input_mask: %s"" %\n                        "" "".join([str(x) for x in input_mask]))\n        tf.logging.info(""segment_ids: %s"" %\n                        "" "".join([str(x) for x in segment_ids]))\n        tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n    feature = InputFeatures(input_ids=input_ids,\n                            input_mask=input_mask,\n                            segment_ids=segment_ids,\n                            label_id=label_id)\n    return feature\n\n\ndef convert_examples_to_features_and_output_to_files(\n        examples, label_list, max_seq_length, tokenizer, output_file):\n    """"""Convert a set of `InputExample`s to a TFRecord file.""""""\n\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    for (ex_index, example) in enumerate(examples):\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        def create_int_feature(values):\n            return tf.train.Feature(\n                int64_list=tf.train.Int64List(value=list(values)))\n\n        features = collections.OrderedDict()\n        features[""input_ids""] = create_int_feature(feature.input_ids)\n        features[""input_mask""] = create_int_feature(feature.input_mask)\n        features[""segment_ids""] = create_int_feature(feature.segment_ids)\n        features[""label_ids""] = create_int_feature([feature.label_id])\n\n        tf_example = tf.train.Example(\n            features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n\n\ndef prepare_TFRecord_data(processor, tokenizer,\n                          data_dir, max_seq_length, output_dir):\n    """"""\n    Args:\n        processor: Data Preprocessor, which must have get_lables,\n            get_train/dev/test/examples methods defined.\n        tokenizer: The Sentence Tokenizer. Generally should be\n            SentencePiece Model.\n        data_dir: The input data directory.\n        max_seq_length: Max sequence length.\n        output_dir: The directory to save the TFRecord in.\n    """"""\n    label_list = processor.get_labels()\n\n    train_examples = processor.get_train_examples(data_dir)\n    train_file = os.path.join(output_dir, ""train.tf_record"")\n    convert_examples_to_features_and_output_to_files(\n        train_examples, label_list, max_seq_length,\n        tokenizer, train_file)\n\n    eval_examples = processor.get_dev_examples(data_dir)\n    eval_file = os.path.join(output_dir, ""eval.tf_record"")\n    convert_examples_to_features_and_output_to_files(\n        eval_examples, label_list,\n        max_seq_length, tokenizer, eval_file)\n\n    test_examples = processor.get_test_examples(data_dir)\n    test_file = os.path.join(output_dir, ""predict.tf_record"")\n    convert_examples_to_features_and_output_to_files(\n        test_examples, label_list,\n        max_seq_length, tokenizer, test_file)\n'"
examples/bert/utils/model_utils.py,8,"b'""""""\nModel utility functions\n""""""\n\nimport tensorflow as tf\n\n\ndef get_lr(global_step, num_train_steps, num_warmup_steps, static_lr):\n    """"""\n    Calculate the learinng rate given global step and warmup steps.\n    The learinng rate is following a linear warmup and linear decay.\n    """"""\n    learning_rate = tf.constant(value=static_lr,\n                                shape=[], dtype=tf.float32)\n\n    learning_rate = tf.train.polynomial_decay(\n        learning_rate,\n        global_step,\n        num_train_steps,\n        end_learning_rate=0.0,\n        power=1.0,\n        cycle=False)\n\n    if num_warmup_steps:\n        global_steps_int = tf.cast(global_step, tf.int32)\n        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n        global_steps_float = tf.cast(global_steps_int, tf.float32)\n        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n        warmup_percent_done = global_steps_float / warmup_steps_float\n        warmup_learning_rate = static_lr * warmup_percent_done\n\n        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n        learning_rate = ((1.0 - is_warmup) * learning_rate\n                         + is_warmup * warmup_learning_rate)\n\n    return learning_rate\n'"
examples/gpt-2/configs/__init__.py,0,b''
examples/gpt-2/configs/config_model_117M.py,0,"b'""""""Texar config file of the GPT-2 model_117M model.\n""""""\n\nvocab_size = 50257\ndim = 768\n\nembed = {\n    ""dim"": dim,\n}\n\npos_embed = {\n    ""dim"": dim\n}\nposition_size = 1024\n\ndecoder = {\n    ""dim"": dim,\n    ""num_blocks"": 12,\n    ""multihead_attention"": {\n        ""use_bias"": True,\n        ""num_units"": dim,\n        ""num_heads"": 12,\n        ""output_dim"": dim,\n    },\n    ""initializer"": {\n        ""type"": ""variance_scaling_initializer"",\n        ""kwargs"": {\n            ""scale"": 1.0,\n            ""mode"": ""fan_avg"",\n            ""distribution"": ""uniform"",\n        },\n    },\n    ""poswise_feedforward"": {\n        ""layers"": [\n            {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""name"": ""conv1"",\n                    ""units"": dim * 4,\n                    ""activation"": ""gelu"",\n                    ""use_bias"": True,\n                }\n            },\n            {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""name"": ""conv2"",\n                    ""units"": dim,\n                    ""use_bias"": True,\n                }\n            }\n        ],\n        ""name"": ""ffn"",\n    },\n}\n'"
examples/gpt-2/configs/config_model_345M.py,0,"b'""""""Texar config file of the GPT-2 model_345M model.\n""""""\n\nvocab_size = 50257\ndim = 1024\n\nembed = {\n    ""dim"": dim,\n}\n\npos_embed = {\n    ""dim"": dim\n}\nposition_size = 1024\n\ndecoder = {\n    ""dim"": dim,\n    ""num_blocks"": 24,\n    ""multihead_attention"": {\n        ""use_bias"": True,\n        ""num_units"": dim,\n        ""num_heads"": 16,\n        ""output_dim"": dim,\n    },\n    ""initializer"": {\n        ""type"": ""variance_scaling_initializer"",\n        ""kwargs"": {\n            ""scale"": 1.0,\n            ""mode"": ""fan_avg"",\n            ""distribution"": ""uniform"",\n        },\n    },\n    ""poswise_feedforward"": {\n        ""layers"": [\n            {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""name"": ""conv1"",\n                    ""units"": dim * 4,\n                    ""activation"": ""gelu"",\n                    ""use_bias"": True,\n                }\n            },\n            {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""name"": ""conv2"",\n                    ""units"": dim,\n                    ""use_bias"": True,\n                }\n            }\n        ],\n        ""name"": ""ffn"",\n    },\n}\n'"
examples/gpt-2/configs/config_train.py,7,"b'""""""Config file for GPT2 training.\n""""""\n# pylint: disable=invalid-name\n\ntfrecord_data_dir = ""data/toy""\nmax_seq_length = 128\nmax_decoding_length = max_seq_length\n\ntrain_batch_size = 32\nmax_train_epoch = 100\ndisplay_steps = 10  # Print training loss every display_steps; -1 to disable\neval_steps = -1    # Eval on the dev set every eval_steps; -1 to disable\n# Checkpoint model parameters every checkpoint_steps; -1 to disable\ncheckpoint_steps = -1\n\neval_batch_size = 8\ntest_batch_size = 8\n\n# Optimization configs\n\nopt = {\n    \'optimizer\': {\n        \'type\': \'AdamOptimizer\',\n        \'kwargs\': {\n            \'learning_rate\': 0.001\n        }\n    }\n}\n\n# Data configs\n\nfeature_original_types = {\n    # Reading features from TFRecord data file.\n    # E.g., Reading feature ""text_ids"" as dtype `tf.int64`;\n    # ""FixedLenFeature"" indicates its length is fixed for all data instances;\n    # and the sequence length is limited by `max_seq_length`.\n    ""text_ids"": [""tf.int64"", ""FixedLenFeature"", max_seq_length],\n    ""length"": [""tf.int64"", ""FixedLenFeature""]\n}\nfeature_convert_types = {\n    # Converting feature dtype after reading. E.g.,\n    # Converting the dtype of feature ""text_ids"" from `tf.int64` (as above)\n    # to `tf.int32`\n    ""text_ids"": ""tf.int32"",\n    ""length"": ""tf.int32""\n}\n\ntrain_hparam = {\n    ""allow_smaller_final_batch"": False,\n    ""batch_size"": train_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_original_types"": feature_original_types,\n        ""feature_convert_types"": feature_convert_types,\n        ""files"": ""{}/train.tf_record"".format(tfrecord_data_dir)\n    },\n    ""shuffle"": True,\n    ""shuffle_buffer_size"": 1000\n}\n\ndev_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": eval_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_original_types"": feature_original_types,\n        ""feature_convert_types"": feature_convert_types,\n        ""files"": ""{}/dev.tf_record"".format(tfrecord_data_dir)\n    },\n    ""shuffle"": False\n}\n\n# Set to `test_hparam` to `None` if generating from scratch\n# (instead of generating continuation) at test time\ntest_hparam = {\n    ""allow_smaller_final_batch"": True,\n    ""batch_size"": test_batch_size,\n    ""dataset"": {\n        ""data_name"": ""data"",\n        ""feature_original_types"": feature_original_types,\n        ""feature_convert_types"": feature_convert_types,\n        ""files"": ""{}/test.tf_record"".format(tfrecord_data_dir)\n    },\n    ""shuffle"": False\n}\n'"
examples/gpt-2/utils/__init__.py,0,b''
examples/gpt-2/utils/data_utils.py,8,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of data preprocessing for GPT2 training.\n""""""\n\nimport os\nimport collections\nimport tensorflow as tf\n\n# pylint: disable=invalid-name, too-many-arguments\n\n\ndef process_single_text(raw_text, max_seq_length, encoder,\n                        BOS_token, EOS_token, PAD_token):\n    """"""Processes a single piece of text. Performs BPE encoding,\n    converting to indexes, truncation, and padding, etc.\n    """"""\n    # BPE\n    tokens = encoder.encode(raw_text)\n\n    # Truncate\n    max_len = max_seq_length\n    if BOS_token is not None and len(BOS_token) > 0:\n        max_len -= 1\n    if EOS_token is not None and len(EOS_token) > 0:\n        max_len -= 1\n    tokens = tokens[:max_len]\n\n    # Append special tokens\n    if BOS_token is not None and len(BOS_token) > 0:\n        tokens = [encoder.encoder[BOS_token]] + tokens\n    if EOS_token is not None and len(EOS_token) > 0:\n        tokens = tokens + [encoder.encoder[EOS_token]]\n\n    token_length = len(tokens)\n\n    # Pad\n    PAD_token_id = encoder.encoder[PAD_token]\n    while len(tokens) < max_seq_length:\n        tokens.append(PAD_token_id)\n\n    assert len(tokens) == max_seq_length\n\n    return tokens, token_length\n\n\ndef read_raw_data(data_fn):\n    """"""\n    Reads raw data from a file. Each line contains one example.\n    """"""\n    examples = []\n    with open(data_fn, ""r"") as fin:\n        for line in fin:\n            examples.append(line.strip())\n    return examples\n\n\ndef file_based_convert_examples_to_features(\n        examples, max_seq_length, encoder, output_file,\n        BOS_token=""<|endoftext|>"", EOS_token=""<|endoftext|>"",\n        PAD_token=""<|endoftext|>""):\n    """"""Converts a set of examples to a TFRecord file.""""""\n\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    for (_, example) in enumerate(examples):\n\n        text_ids, length = process_single_text(\n            example, max_seq_length, encoder, BOS_token, EOS_token, PAD_token)\n\n        def _create_int_feature(values):\n            return tf.train.Feature(\n                int64_list=tf.train.Int64List(value=list(values)))\n\n        features = collections.OrderedDict()\n        features[""text_ids""] = _create_int_feature(text_ids)\n        features[""length""] = _create_int_feature([length])\n\n        tf_example = tf.train.Example(\n            features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n\n\ndef prepare_TFRecord_data(data_dir, max_seq_length, encoder, output_dir):\n    """"""\n    Args:\n        data_dir: The input data directory.\n        max_seq_length: Max sequence length.\n        output_dir: The directory to save the TFRecord files in.\n    """"""\n    train_fn = os.path.join(data_dir, ""train.txt"")\n    if os.path.isfile(train_fn):\n        tf.logging.info(""Processing %s"" % train_fn)\n        train_examples = read_raw_data(train_fn)\n        train_file = os.path.join(output_dir, ""train.tf_record"")\n        file_based_convert_examples_to_features(\n            train_examples, max_seq_length, encoder, train_file)\n\n    dev_fn = os.path.join(data_dir, ""dev.txt"")\n    if os.path.isfile(dev_fn):\n        tf.logging.info(""Processing %s"" % dev_fn)\n        eval_examples = read_raw_data(dev_fn)\n        eval_file = os.path.join(output_dir, ""dev.tf_record"")\n        file_based_convert_examples_to_features(\n            eval_examples, max_seq_length, encoder, eval_file)\n\n    test_fn = os.path.join(data_dir, ""test.txt"")\n    if os.path.isfile(test_fn):\n        tf.logging.info(""Processing %s"" % test_fn)\n        test_examples = read_raw_data(test_fn)\n        test_file = os.path.join(output_dir, ""test.tf_record"")\n        file_based_convert_examples_to_features(\n            test_examples, max_seq_length, encoder, test_file, EOS_token=None)\n'"
examples/gpt-2/utils/model_utils.py,4,"b'""""""\nModel utility functions\n""""""\nimport sys\nimport json\nimport tensorflow as tf\nimport numpy as np\nfrom texar.tf import HParams\n\n\ndef transform_gpt2_to_texar_config(input_json_path):\n    """"""\n    Remap the config file\n    """"""\n    config_gpt = json.loads(open(input_json_path).read())\n    configs = dict()\n    configs[""vocab_size""] = config_gpt[""n_vocab""]\n    configs[""context_size""] = config_gpt[""n_ctx""]\n    configs[""embedding_size""] = config_gpt[""n_embd""]\n    hidden_dim = config_gpt[""n_embd""]\n    configs[""embed""] = {\n        ""dim"": hidden_dim,\n    }\n    configs[""position_size""] = config_gpt[""n_ctx""]\n    configs[""pos_embed""] = {\n        ""dim"": hidden_dim\n    }\n    configs[""decoder""] = {\n        ""dim"": hidden_dim,\n        ""num_blocks"": config_gpt[""n_layer""],\n        ""multihead_attention"": {\n            ""use_bias"": True,\n            ""num_units"": hidden_dim,\n            ""num_heads"": config_gpt[""n_head""],\n            ""output_dim"": hidden_dim,\n        },\n        ""initializer"": {\n            ""type"": ""variance_scaling_initializer"",\n            ""kwargs"": {\n                ""scale"": 1.0,\n                ""mode"": ""fan_avg"",\n                ""distribution"": ""uniform"",\n            },\n        },\n        ""poswise_feedforward"": {\n            ""layers"": [\n                {\n                    ""type"": ""Dense"",\n                    ""kwargs"": {\n                        ""name"": ""conv1"",\n                        ""units"": hidden_dim * 4,\n                        ""activation"": ""gelu"",\n                        ""use_bias"": True,\n                    }\n                },\n                {\n                    ""type"": ""Dense"",\n                    ""kwargs"": {\n                        ""name"": ""conv2"",\n                        ""units"": hidden_dim,\n                        ""use_bias"": True,\n                    }\n                }\n            ],\n            ""name"": ""ffn"",\n        },\n    }\n    return HParams(configs, default_hparams=None)\n\n\ndef _map_tensor_names(original_tensor_name):\n    """"""\n    Tensor name mapping\n    """"""\n    global_tensor_map = {\n        ""model/wte"": ""word_embedder/w"",\n        ""model/wpe"": ""position_embedder/w"",\n        ""model/ln_f/b"": ""transformer_decoder/beta"",\n        ""model/ln_f/g"": ""transformer_decoder/gamma"",\n    }\n    if original_tensor_name in global_tensor_map:\n        return global_tensor_map[original_tensor_name]\n    original_tensor_name_split = original_tensor_name.split(""/"")\n    layer_tensor_map = {\n        ""ln_1/b"": ""beta"",\n        ""ln_1/g"": ""gamma"",\n        ""ln_2/b"": ""past_poswise_ln/beta"",\n        ""ln_2/g"": ""past_poswise_ln/gamma"",\n        ""mlp/c_fc/b"": ""ffn/conv1/bias"",\n        ""mlp/c_fc/w"": ""ffn/conv1/kernel"",\n        ""mlp/c_proj/b"": ""ffn/conv2/bias"",\n        ""mlp/c_proj/w"": ""ffn/conv2/kernel"",\n        ""attn/c_proj/b"": ""self_attention/multihead_attention/output/bias"",\n        ""attn/c_proj/w"": ""self_attention/multihead_attention/output/kernel"",\n    }\n    layer_num = int(original_tensor_name_split[1][1:])\n    layer_feature = ""/"".join(original_tensor_name.split(""/"")[2:])\n    # pylint: disable=no-else-return\n    if layer_feature in layer_tensor_map:\n        layer_feature_ = layer_tensor_map[layer_feature]\n        tensor_name_ = ""/"".join(\n            [\n                ""transformer_decoder"",\n                ""layer_{}"".format(layer_num),\n                layer_feature_\n            ])\n        return tensor_name_\n    else:\n        return original_tensor_name\n\n\n# pylint: disable=too-many-locals\ndef _get_assignment_map_from_checkpoint(sess, all_variables, init_checkpoint):\n    """"""\n    Load pretrained parameters to texar model\n    """"""\n\n    assignment_map = {}\n\n    reader = tf.train.NewCheckpointReader(init_checkpoint)\n    var_names_list = reader.get_variable_to_shape_map().keys()\n    ckpt_names_vs_vals = {}\n    for var_name in var_names_list:\n        ckpt_names_vs_vals[var_name] = reader.get_tensor(var_name)\n\n    def _assign_by_name(sess, tensor_name, data):\n        local_tensor = [var for var in all_variables\n                        if tensor_name in var.name][0]\n        sess.run(tf.assign(local_tensor, data))\n\n    def _get_tensor_by_name(tensor_name):\n        local_tensor = [var for var in all_variables\n                        if tensor_name in var.name][0]\n        return local_tensor\n\n    for idx, ckpt_tensor_name in enumerate(ckpt_names_vs_vals):\n        processing = (idx + 1.0) / len(ckpt_names_vs_vals.keys())\n        sys.stdout.write(""\\rLoading checkpoint: {:.1%}"".format(processing))\n        sys.stdout.flush()\n\n        ckpt_tensor_name_feature = """"\n        if len(ckpt_tensor_name.split(""/"")) > 2:\n            ckpt_tensor_name_feature = ""/"".join(\n                ckpt_tensor_name.split(""/"")[2:])\n        if ckpt_tensor_name_feature == ""attn/c_attn/w"":\n            layer_num = int(ckpt_tensor_name.split(""/"")[1][1:])\n            template = (""transformer_decoder/layer_{}/self_attention/""\n                        ""multihead_attention/{}/kernel"")\n            local_tensor_name_q_w = template.format(layer_num, ""query"")\n            local_tensor_name_k_w = template.format(layer_num, ""key"")\n            local_tensor_name_v_w = template.format(layer_num, ""value"")\n\n            data = ckpt_names_vs_vals[ckpt_tensor_name]\n            assert data.shape[2] % 3 == 0, (""tensor \'attn/c_attn/w\' ""\n                                            ""shape is not dividable"")\n            index_w = data.shape[2] // 3\n            q_w = data[:, :, :index_w]\n            k_w = data[:, :, index_w: 2 * index_w]\n            v_w = data[:, :, 2 * index_w:]\n            _assign_by_name(sess, local_tensor_name_q_w, np.squeeze(q_w))\n            _assign_by_name(sess, local_tensor_name_k_w, np.squeeze(k_w))\n            _assign_by_name(sess, local_tensor_name_v_w, np.squeeze(v_w))\n\n        elif ckpt_tensor_name_feature == ""attn/c_attn/b"":\n            layer_num = int(ckpt_tensor_name.split(""/"")[1][1:])\n            template = (""transformer_decoder/layer_{}/self_attention/""\n                        ""multihead_attention/{}/bias"")\n            local_tensor_name_q_b = template.format(layer_num, ""query"")\n            local_tensor_name_k_b = template.format(layer_num, ""key"")\n            local_tensor_name_v_b = template.format(layer_num, ""value"")\n\n            data = ckpt_names_vs_vals[ckpt_tensor_name]\n            assert data.shape[0] % 3 == 0, (""tensor \'attn/c_attn/b\'""\n                                            "" shape is not dividable"")\n            index_b = data.shape[0] // 3\n            q_b = data[:index_b]\n            k_b = data[index_b: 2 * index_b]\n            v_b = data[2 * index_b:]\n            _assign_by_name(sess, local_tensor_name_q_b, q_b)\n            _assign_by_name(sess, local_tensor_name_k_b, k_b)\n            _assign_by_name(sess, local_tensor_name_v_b, v_b)\n\n        else:\n            local_tensor_name = _map_tensor_names(ckpt_tensor_name)\n            local_tensor = _get_tensor_by_name(local_tensor_name)\n            assignment_map[ckpt_tensor_name] = local_tensor\n\n    return assignment_map\n\n\ndef init_gpt2_checkpoint(sess, init_checkpoint):\n    """"""\n    Initializes GPT-2 model parameters from a checkpoint\n\n    Args:\n        init_checkpoint (str): Path to the checkpoint.\n    """"""\n    tvars = tf.trainable_variables()\n    if init_checkpoint:\n        assignment_map = _get_assignment_map_from_checkpoint(\n            sess,\n            tvars,\n            init_checkpoint)\n        init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n            init_checkpoint, assignment_map, reshape_variables=True)\n        init_fn(sess)\n'"
examples/gpt-2/utils/processor.py,0,"b'# -*- coding: utf-8 -*-\n#\n""""""\nByte pair encoding utilities\n\nAdapted from https://github.com/openai/gpt-2/blob/master/src/encoder.py\n""""""\n\nimport os\nimport json\nimport regex as re\nfrom functools import lru_cache\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"") + 1)) + list(\n        range(ord(""\xc2\xa1""), ord(""\xc2\xac"") + 1)) + list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=""replace""):\n        self.encoder = encoder\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = errors  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(\n                pair, float(""inf"")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except BaseException:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 \\\n                        and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = "" "".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = """".join(self.byte_encoder[b] for b in token.encode(""utf-8""))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split("" ""))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = """".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(""utf-8"", errors=self.errors)\n        return text\n\n\ndef get_encoder(gpt2_pretrained_path):\n    with open(os.path.join(gpt2_pretrained_path, ""encoder.json""), ""r"") as f:\n        encoder = json.load(f)\n    with open(os.path.join(gpt2_pretrained_path, ""vocab.bpe""), ""r"", encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(""\\n"")[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n'"
examples/seq2seq_exposure_bias/configs/__init__.py,0,b''
examples/seq2seq_exposure_bias/configs/config_giga.py,0,"b'num_epochs = 30\nobserve_steps = 500\n\neval_metric = \'rouge\'\n\nbatch_size = 64\nsource_vocab_file = \'./data/giga/vocab.article\'\ntarget_vocab_file = \'./data/giga/vocab.title\'\n\ntrain = {\n    \'batch_size\': batch_size,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'data/giga/train.article\',\n        \'vocab_file\': source_vocab_file\n    },\n    \'target_dataset\': {\n        \'files\': \'data/giga/train.title\',\n        \'vocab_file\': target_vocab_file\n    }\n}\nval = {\n    \'batch_size\': batch_size,\n    \'shuffle\': False,\n    \'allow_smaller_final_batch\': True,\n    \'source_dataset\': {\n        ""files"": \'data/giga/valid.article\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/giga/valid.title\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\ntest = {\n    \'batch_size\': batch_size,\n    \'shuffle\': False,\n    \'allow_smaller_final_batch\': True,\n    \'source_dataset\': {\n        ""files"": \'data/giga/test.article\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/giga/test.title\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n'"
examples/seq2seq_exposure_bias/configs/config_iwslt14.py,0,"b'num_epochs = 50  # the best epoch occurs within 10 epochs in most cases\nobserve_steps = 500\n\neval_metric = \'bleu\'\n\nbatch_size = 64\nsource_vocab_file = \'./data/iwslt14/vocab.de\'\ntarget_vocab_file = \'./data/iwslt14/vocab.en\'\n\ntrain = {\n    \'batch_size\': batch_size,\n    \'shuffle\': True,\n    \'allow_smaller_final_batch\': False,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/train.de\',\n        \'vocab_file\': source_vocab_file,\n        \'max_seq_length\': 50\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/train.en\',\n        \'vocab_file\': target_vocab_file,\n        \'max_seq_length\': 50\n    }\n}\nval = {\n    \'batch_size\': batch_size,\n    \'shuffle\': False,\n    \'allow_smaller_final_batch\': True,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/valid.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/valid.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\ntest = {\n    \'batch_size\': batch_size,\n    \'shuffle\': False,\n    \'allow_smaller_final_batch\': True,\n    \'source_dataset\': {\n        ""files"": \'data/iwslt14/test.de\',\n        \'vocab_file\': source_vocab_file,\n    },\n    \'target_dataset\': {\n        \'files\': \'data/iwslt14/test.en\',\n        \'vocab_file\': target_vocab_file,\n    }\n}\n'"
examples/seq2seq_exposure_bias/configs/config_model.py,0,"b""num_units = 256\nbeam_width = 5\ndecoder_layers = 1\ndropout = 0.2\n\nembedder = {\n    'dim': num_units\n}\nencoder = {\n    'rnn_cell_fw': {\n        'kwargs': {\n            'num_units': num_units\n        },\n        'dropout': {\n            'input_keep_prob': 1. - dropout\n        }\n    }\n}\ndecoder = {\n    'rnn_cell': {\n        'kwargs': {\n            'num_units': num_units\n        },\n        'dropout': {\n            'input_keep_prob': 1. - dropout\n        },\n        'num_layers': decoder_layers\n    },\n    'attention': {\n        'kwargs': {\n            'num_units': num_units,\n        },\n        'attention_layer_size': num_units\n    }\n}\nopt = {\n    'optimizer': {\n        'type':  'AdamOptimizer',\n        'kwargs': {\n            'learning_rate': 0.001,\n        },\n    },\n}\n"""
examples/seq2seq_exposure_bias/utils/prepare_data.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Downloads data.\n""""""\nimport tensorflow as tf\nimport texar.tf as tx\n\n# pylint: disable=invalid-name\n\nflags = tf.flags\n\nflags.DEFINE_string(""data"", ""iwslt14"", ""Data to download [iwslt14|toy_copy]"")\n\nFLAGS = flags.FLAGS\n\n\ndef prepare_data():\n    """"""Downloads data.\n    """"""\n    if FLAGS.data == \'giga\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'12RZs7QFwjj6dfuYNQ_0Ah-ccH1xFDMD5/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'giga.zip\',\n            extract=True)\n    elif FLAGS.data == \'iwslt14\':\n        tx.data.maybe_download(\n            urls=\'https://drive.google.com/file/d/\'\n                 \'1y4mUWXRS2KstgHopCS9koZ42ENOh6Yb9/view?usp=sharing\',\n            path=\'./\',\n            filenames=\'iwslt14.zip\',\n            extract=True)\n    else:\n        raise ValueError(\'Unknown data: {}\'.format(FLAGS.data))\n\n\ndef main():\n    """"""Entrypoint.\n    """"""\n    prepare_data()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/transformer/utils/__init__.py,0,b''
examples/transformer/utils/data_utils.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Data read/write utilities for Transformer.\n""""""\nimport os\nimport codecs\nimport six\nimport numpy as np\n\n# pylint: disable=no-member\n\n\ndef load_data_numpy(input_dir, prefix):\n    train_data = np.load(\n        os.path.join(input_dir, prefix + ""train.npy""),\n        encoding=""latin1"",\n        allow_pickle=True,\n    ).tolist()\n    dev_data = np.load(\n        os.path.join(input_dir, prefix + ""valid.npy""),\n        encoding=""latin1"",\n        allow_pickle=True,\n    ).tolist()\n    test_data = np.load(\n        os.path.join(input_dir, prefix + ""test.npy""),\n        encoding=""latin1"",\n        allow_pickle=True,\n    ).tolist()\n    print(""train data size:{}"".format(len(train_data)))\n    return train_data, dev_data, test_data\n\n\ndef seq2seq_pad_concat_convert(xy_batch, eos_id=2, bos_id=1):\n    """"""\n    Args:\n        xy_batch (list of tuple of two numpy.ndarray-s or cupy.ndarray-s):\n            xy_batch[i][0] is an array\n            of token ids of i-th input sentence in a minibatch.\n            xy_batch[i][1] is an array\n            of token ids of i-th target sentence in a minibatch.\n            The shape of each array is `(sentence length, )`.\n        eos_id: The index of end-of-sentence special token in the\n            dictionary.\n\n    Returns:\n        Tuple of Converted array.\n            (input_sent_batch_array, target_sent_batch_input_array,\n            target_sent_batch_output_array).\n            The shape of each array is `(batchsize, max_sentence_length)`.\n            All sentences are padded with 0 to reach max_sentence_length.\n    """"""\n\n    x_seqs, y_seqs = zip(*xy_batch)\n    x_block = _concat_examples(x_seqs, padding=0)\n    y_block = _concat_examples(y_seqs, padding=0)\n\n    # Add EOS\n    x_block = np.pad(x_block, ((0, 0), (0, 1)), ""constant"", constant_values=0)\n    for i_batch, seq in enumerate(x_seqs):\n        x_block[i_batch, len(seq)] = eos_id\n\n    y_out_block = np.pad(\n        y_block, ((0, 0), (0, 1)), ""constant"", constant_values=0\n    )\n    for i_batch, seq in enumerate(y_seqs):\n        y_out_block[i_batch, len(seq)] = eos_id\n\n    # Add BOS in target language\n    y_in_block = np.pad(\n        y_block, ((0, 0), (1, 0)), ""constant"", constant_values=bos_id\n    )\n    return x_block, y_in_block, y_out_block\n\n\ndef source_pad_concat_convert(x_seqs, eos_id=2, bos_id=1):\n    """"""\n    This function is used when testing the model without target input.\n    """"""\n    x_block = _concat_examples(x_seqs, padding=0)\n\n    # add EOS\n    x_block = np.pad(x_block, ((0, 0), (0, 1)), ""constant"", constant_values=0)\n    for i_batch, seq in enumerate(x_seqs):\n        x_block[i_batch, len(seq)] = eos_id\n    return x_block\n\n\ndef _concat_examples(arrays, padding=0):\n    if len(arrays) == 0:\n        raise ValueError(""batch is empty"")\n\n    first_elem = arrays[0]\n    assert isinstance(first_elem, np.ndarray)\n\n    shape = np.array(arrays[0].shape, dtype=int)\n    for array in arrays[1:]:\n        if np.any(shape != array.shape):\n            np.maximum(shape, array.shape, shape)\n    shape = tuple(np.insert(shape, 0, len(arrays)))\n\n    result = np.full(shape, padding, dtype=arrays[0].dtype)\n    for i in six.moves.range(len(arrays)):\n        src = arrays[i]\n        slices = tuple(slice(dim) for dim in src.shape)\n        result[(i,) + slices] = src\n    return result\n\n\ndef write_words(words_list, filename):\n    with codecs.open(filename, ""w+"", ""utf-8"") as myfile:\n        for words in words_list:\n            myfile.write("" "".join(words) + ""\\n"")\n'"
examples/transformer/utils/preprocess.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\npreprocessing text data. Generally it\'s to generate plain text vocab file,\ntruncate sequence by length, generate the preprocessed dataset.\n""""""\nimport collections\nimport re\nimport json\nimport os\nimport numpy as np\nimport pickle\nimport argparse\nfrom io import open\n# pylint:disable=invalid-name\n\nsplit_pattern = re.compile(r\'([.,!?""\\\':;)(])\')\ndigit_pattern = re.compile(r\'\\d\')\n\n# Refer to https://texar.readthedocs.io/en/latest/_modules/texar/data/vocabulary.html#SpecialTokens\n# these tokens will by default have token ids 0, 1, 2, 3 respectively\npad_token_id, bos_token_id, eos_token_id, unk_token_id = 0, 1, 2, 3\n\n\ndef split_sentence(s, tok=False):\n    """"""split sentence with some segmentation rules.""""""\n    if tok:\n        s = s.lower()\n        s = s.replace(\'\\u2019\', ""\'"")\n        s = digit_pattern.sub(\'0\', s)\n    words = []\n    for word in s.split():\n        if tok:\n            words.extend(split_pattern.split(word))\n        else:\n            words.append(word)\n    words = [w for w in words if w]\n    return words\n\n\ndef open_file(path):\n    """"""more robust open function""""""\n    return open(path, encoding=\'utf-8\')\n\n\ndef read_file(path, tok=False):\n    """"""a generator to generate each line of file.""""""\n    with open_file(path) as f:\n        for line in f.readlines():\n            words = split_sentence(line.strip(), tok)\n            yield words\n\n\ndef count_words(path, max_vocab_size=40000, tok=False):\n    """"""count all words in the corpus and output a counter""""""\n    counts = collections.Counter()\n    for words in read_file(path, tok):\n        for word in words:\n            counts[word] += 1\n\n    vocab = [word for (word, _) in counts.most_common(max_vocab_size)]\n    return vocab\n\n\ndef make_array(word_id, words):\n    """"""generate id numpy array from plain text words.""""""\n    ids = [word_id.get(word, unk_token_id) for word in words]\n    return np.array(ids, \'i\')\n\n\ndef make_dataset(path, w2id, tok=False):\n    """"""generate dataset.""""""\n    dataset, npy_dataset = [], []\n    token_count, unknown_count = 0, 0\n    for words in read_file(path, tok):\n        array = make_array(w2id, words)\n        npy_dataset.append(array)\n        dataset.append(words)\n        token_count += array.size\n        unknown_count += (array == unk_token_id).sum()\n    print(\'# of tokens:{}\'.format(token_count))\n    print(\'# of unknown {} {:.2}\'.format(unknown_count,\n                                         100. * unknown_count / token_count))\n    return dataset, npy_dataset\n\n\ndef get_preprocess_args():\n    """"""Data preprocessing options.""""""\n    class Config():\n        pass\n    config = Config()\n    parser = argparse.ArgumentParser(description=\'Preprocessing Options\')\n    parser.add_argument(\'--source_vocab\', type=int, default=40000,\n                        help=\'Vocabulary size of source language\')\n    parser.add_argument(\'--target_vocab\', type=int, default=40000,\n                        help=\'Vocabulary size of target language\')\n    parser.add_argument(\'--tok\', dest=\'tok\', action=\'store_true\',\n                        help=\'tokenized and lowercased\')\n    parser.set_defaults(tok=False)\n    parser.add_argument(\'--max_seq_length\', type=int, default=70)\n    parser.add_argument(\'--pre_encoding\', type=str, default=\'spm\')\n    parser.add_argument(\'--src\', type=str, default=\'en\')\n    parser.add_argument(\'--tgt\', type=str, default=\'vi\')\n    parser.add_argument(\'--input_dir\', \'-i\', type=str,\n                        default=\'./data/en_vi/data/\', help=\'Input directory\')\n    parser.add_argument(\'--save_data\', type=str, default=\'preprocess\',\n                        help=\'Output file for the prepared data\')\n    parser.parse_args(namespace=config)\n\n    # keep consistent with original implementation\n    # pylint:disable=attribute-defined-outside-init\n    config.input = config.input_dir\n    config.source_train = \'train.\' + config.src\n    config.target_train = \'train.\' + config.tgt\n    config.source_valid = \'valid.\' + config.src\n    config.target_valid = \'valid.\' + config.tgt\n    config.source_test = \'test.\' + config.src\n    config.target_test = \'test.\' + config.tgt\n    return config\n\n\nif __name__ == ""__main__"":\n    args = get_preprocess_args()\n\n    print(json.dumps(args.__dict__, indent=4))\n\n    # pylint:disable=no-member\n    # Vocab Construction\n    source_path = os.path.join(args.input_dir, args.source_train)\n    target_path = os.path.join(args.input_dir, args.target_train)\n\n    src_cntr = count_words(source_path, args.source_vocab, args.tok)\n    trg_cntr = count_words(target_path, args.target_vocab, args.tok)\n    all_words = sorted(list(set(src_cntr + trg_cntr)))\n\n    vocab = [\'<pad>\', \'<bos>\', \'<eos>\', \'<unk>\'] + all_words\n\n    w2id = {word: index for index, word in enumerate(vocab)}\n\n    # Train Dataset\n    source_data, source_npy = make_dataset(source_path, w2id, args.tok)\n    target_data, target_npy = make_dataset(target_path, w2id, args.tok)\n    assert len(source_data) == len(target_data)\n\n    train_data = [(s, t) for s, t in zip(source_data, target_data)\n                  if s and len(s) < args.max_seq_length\n                  and t and len(t) < args.max_seq_length]\n    train_npy = [(s, t) for s, t in zip(source_npy, target_npy)\n                 if len(s) > 0 and len(s) < args.max_seq_length\n                 and len(t) > 0 and len(t) < args.max_seq_length]\n    assert len(train_data) == len(train_npy)\n\n    # Display corpus statistics\n    print(""Vocab: {} with special tokens"".format(len(vocab)))\n    print(\'Original training data size: %d\' % len(source_data))\n    print(\'Filtered training data size: %d\' % len(train_data))\n\n    # Valid Dataset\n    source_path = os.path.join(args.input_dir, args.source_valid)\n    source_data, source_npy = make_dataset(source_path, w2id, args.tok)\n    target_path = os.path.join(args.input_dir, args.target_valid)\n    target_data, target_npy = make_dataset(target_path, w2id, args.tok)\n    assert len(source_data) == len(target_data)\n\n    valid_data = [(s, t) for s, t in zip(source_data, target_data)\n                  if s and t]\n    valid_npy = [(s, t) for s, t in zip(source_npy, target_npy)\n                 if len(s) > 0 and len(t) > 0]\n    assert len(valid_data) == len(valid_npy)\n    print(\'Original dev data size: %d\' % len(source_data))\n    print(\'Filtered dev data size: %d\' % len(valid_data))\n\n    # Test Dataset\n    source_path = os.path.join(args.input_dir, args.source_test)\n    source_data, source_npy = make_dataset(source_path, w2id, args.tok)\n    target_path = os.path.realpath(\n        os.path.join(args.input_dir, args.target_test))\n    target_data, target_npy = make_dataset(target_path, w2id, args.tok)\n    assert len(source_data) == len(target_data)\n    test_data = [(s, t) for s, t in zip(source_data, target_data)\n                 if s and t]\n    test_npy = [(s, t) for s, t in zip(source_npy, target_npy)\n                if len(s) > 0 and len(t) > 0]\n    print(\'Original test data size: %d\' % len(source_data))\n    print(\'Filtered test data size: %d\' % len(test_data))\n    id2w = {i: w for w, i in w2id.items()}\n    # Save the dataset to numpy files\n    train_src_output = os.path.join(\n        args.input_dir, args.save_data + \'train.\' + args.src + \'.txt\')\n    train_tgt_output = os.path.join(\n        args.input_dir, args.save_data + \'train.\' + args.tgt + \'.txt\')\n    dev_src_output = os.path.join(args.input_dir,\n                                  args.save_data + \'dev.\' + args.src + \'.txt\')\n    dev_tgt_output = os.path.join(args.input_dir,\n                                  args.save_data + \'dev.\' + args.tgt + \'.txt\')\n    test_src_output = os.path.join(args.input_dir,\n                                   args.save_data + \'test.\' + args.src + \'.txt\')\n    test_tgt_output = os.path.join(args.input_dir,\n                                   args.save_data + \'test.\' + args.tgt + \'.txt\')\n\n    np.save(os.path.join(args.input, args.save_data + \'train.npy\'),\n            train_npy)\n    np.save(os.path.join(args.input, args.save_data + \'valid.npy\'),\n            valid_npy)\n    np.save(os.path.join(args.input, args.save_data + \'test.npy\'),\n            test_npy)\n    with open(os.path.join(args.input, args.save_data + \'vocab.pickle\'), \'wb\')\\\n        as f:\n        pickle.dump(id2w, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n    with open(train_src_output, \'w+\', encoding=\'utf-8\') as fsrc, \\\n        open(train_tgt_output, \'w+\', encoding=\'utf-8\') as ftgt:\n        for words in train_data:\n            fsrc.write(\'{}\\n\'.format(\' \'.join(words[0])))\n            ftgt.write(\'{}\\n\'.format(\' \'.join(words[1])))\n    with open(dev_src_output, \'w+\', encoding=\'utf-8\') as fsrc, \\\n        open(dev_tgt_output, \'w+\', encoding=\'utf-8\') as ftgt:\n        for words in valid_data:\n            fsrc.write(\'{}\\n\'.format(\' \'.join(words[0])))\n            ftgt.write(\'{}\\n\'.format(\' \'.join(words[1])))\n    with open(test_src_output, \'w+\', encoding=\'utf-8\') as fsrc, \\\n        open(test_tgt_output, \'w+\', encoding=\'utf-8\') as ftgt:\n        for words in test_data:\n            fsrc.write(\'{}\\n\'.format(\' \'.join(words[0])))\n            ftgt.write(\'{}\\n\'.format(\' \'.join(words[1])))\n    with open(os.path.join(args.input_dir,\n                           args.save_data + args.pre_encoding + \'.vocab.text\'),\n              \'w+\', encoding=\'utf-8\') as f:\n        max_size = len(id2w)\n        for idx in range(4, max_size):\n            f.write(\'{}\\n\'.format(id2w[idx]))\n'"
examples/transformer/utils/utils.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions for model training.\n""""""\n\nimport random\nimport math\nimport logging\nimport numpy as np\nimport tensorflow as tf\n\n\ndef set_random_seed(myseed):\n    tf.set_random_seed(myseed)\n    np.random.seed(myseed)\n    random.seed(myseed)\n\n\ndef batch_size_fn(new, count, size_so_far):\n    max_src_in_batch, max_tgt_in_batch = 0, 0\n    max_src_in_batch = max(max_src_in_batch, len(new[0] + 1))\n    max_tgt_in_batch = max(max_tgt_in_batch, len(new[1] + 1))\n    src_elements = count * max_src_in_batch\n    tgt_elements = count * max_tgt_in_batch\n    return max(src_elements, tgt_elements)\n\n\ndef get_lr(fstep, opt_config):\n    if opt_config[\'learning_rate_schedule\'] == \'static\':\n        lr = opt_config[\'static_lr\']\n    else:\n        lr = opt_config[\'lr_constant\'] \\\n            * min(1.0, (fstep / opt_config[\'warmup_steps\'])) \\\n            * (1 / math.sqrt(max(fstep, opt_config[\'warmup_steps\'])))\n    return lr\n\n\ndef get_logger(log_path):\n    """"""Returns a logger.\n\n    Args:\n        log_path (str): Path to the log file.\n    """"""\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    fh = logging.FileHandler(log_path)\n    fh.setLevel(logging.DEBUG)\n    fh.setFormatter(\n        logging.Formatter(\'%(asctime)s:%(levelname)s:%(message)s\'))\n    logger.addHandler(fh)\n    return logger\n\n\ndef list_strip_eos(list_, eos_token):\n    """"""Strips EOS token from a list of lists of tokens.\n    """"""\n    list_strip = []\n    for elem in list_:\n        if eos_token in elem:\n            elem = elem[:elem.index(eos_token)]\n        list_strip.append(elem)\n    return list_strip\n'"
tests/data/data/data_iterators_test.py,38,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data iterator related operations.\n""""""\n\n# pylint: disable=no-member, invalid-name\n\nimport tempfile\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n\nclass DataIteratorTest(tf.test.TestCase):\n    """"""Tests data iterators.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create data\n        train_text = list(np.linspace(1, 1000, num=1000, dtype=np.int64))\n        train_text = [str(x) for x in train_text]\n        train_text_file = tempfile.NamedTemporaryFile()\n        train_text_file.write(\'\\n\'.join(train_text).encode(""utf-8""))\n        train_text_file.flush()\n        self._train_text_file = train_text_file\n\n        test_text = list(np.linspace(1001, 2000, num=1000, dtype=np.int64))\n        test_text = [str(x) for x in test_text]\n        test_text_file = tempfile.NamedTemporaryFile()\n        test_text_file.write(\'\\n\'.join(test_text).encode(""utf-8""))\n        test_text_file.flush()\n        self._test_text_file = test_text_file\n\n        vocab_list = train_text + test_text\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        self._train_hparams = {\n            ""num_epochs"": 2,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._train_text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n                ""bos_token"": \'\',\n                ""eos_token"": \'\'\n            },\n            ""name"": ""train""\n        }\n\n        self._test_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._test_text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n                ""bos_token"": \'\',\n                ""eos_token"": \'\'\n            },\n            ""name"": ""test""\n        }\n\n    def test_iterator_single_dataset(self):\n        """"""Tests iterating over a single dataset.\n        """"""\n        data = tx.data.MonoTextData(self._test_hparams)\n\n        iterator = tx.data.DataIterator(data)\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n\n            for _ in range(2):\n                iterator.switch_to_dataset(sess)\n                i = 1001\n                while True:\n                    try:\n                        data_batch_ = sess.run(data_batch)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i))\n                        i += 1\n                    except tf.errors.OutOfRangeError:\n                        print(\'Done -- epoch limit reached\')\n                        self.assertEqual(i, 2001)\n                        break\n\n    def test_iterator_multi_datasets(self):\n        """"""Tests iterating over multiple datasets.\n        """"""\n        train_data = tx.data.MonoTextData(self._train_hparams)\n        test_data = tx.data.MonoTextData(self._test_hparams)\n\n        iterator = tx.data.DataIterator([train_data, test_data])\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n\n            for _ in range(2):\n                # Iterates over train data\n                iterator.switch_to_dataset(sess, train_data.name)\n                i = 0\n                while True:\n                    try:\n                        data_batch_ = sess.run(data_batch)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i + 1))\n                        i = (i + 1) % 1000\n                    except tf.errors.OutOfRangeError:\n                        print(\'Train data limit reached\')\n                        self.assertEqual(i, 0)\n                        break\n\n                # Iterates over test data\n                iterator.switch_to_dataset(sess, test_data.name)\n                i = 1001\n                while True:\n                    try:\n                        data_batch_ = sess.run(data_batch)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i))\n                        i += 1\n                    except tf.errors.OutOfRangeError:\n                        print(\'Test data limit reached\')\n                        self.assertEqual(i, 2001)\n                        break\n\n    def test_train_test_data_iterator(self):\n        """"""Tests :class:`texar.tf.data.TrainTestDataIterator`\n        """"""\n        train_data = tx.data.MonoTextData(self._train_hparams)\n        test_data = tx.data.MonoTextData(self._test_hparams)\n\n        iterator = tx.data.TrainTestDataIterator(train=train_data,\n                                                 test=test_data)\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n\n            for _ in range(2):\n                iterator.switch_to_train_data(sess)\n                i = 0\n                while True:\n                    try:\n                        data_batch_ = sess.run(data_batch)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i + 1))\n                        i = (i + 1) % 1000\n                    except tf.errors.OutOfRangeError:\n                        print(\'Train data limit reached\')\n                        self.assertEqual(i, 0)\n                        break\n\n                iterator.switch_to_test_data(sess)\n                i = 1001\n                while True:\n                    try:\n                        data_batch_ = sess.run(data_batch)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i))\n                        i += 1\n                    except tf.errors.OutOfRangeError:\n                        print(\'Test data limit reached\')\n                        self.assertEqual(i, 2001)\n                        break\n\n    def test_feedable_iterator_multi_datasets(self):\n        """"""Tests iterating over multiple datasets with the\n        :class:`FeedableDataIterator`.\n        """"""\n        train_data = tx.data.MonoTextData(self._train_hparams)\n        test_data = tx.data.MonoTextData(self._test_hparams)\n\n        iterator = tx.data.FeedableDataIterator([train_data, test_data])\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n\n            iterator.initialize_dataset(sess)\n\n            for _ in range(2):\n                # Iterates over train data\n                iterator.restart_dataset(sess, train_data.name)\n                data_handle = iterator.get_handle(sess, train_data.name)\n                i = 0\n                while True:\n                    try:\n                        feed_dict = {iterator.handle: data_handle}\n                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i + 1))\n                        i = (i + 1) % 1000\n                    except tf.errors.OutOfRangeError:\n                        print(\'Train data limit reached\')\n                        self.assertEqual(i, 0)\n                        break\n\n                # Iterates over test data\n                iterator.restart_dataset(sess, test_data.name)\n                data_handle = iterator.get_handle(sess, test_data.name)\n                i = 1001\n                while True:\n                    try:\n                        feed_dict = {iterator.handle: data_handle}\n                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i))\n                        i += 1\n                    except tf.errors.OutOfRangeError:\n                        print(\'Test data limit reached\')\n                        self.assertEqual(i, 2001)\n                        break\n\n    def test_train_test_feedable_data_iterator(self):\n        """"""Tests :class:`texar.tf.data.TrainTestFeedableDataIterator`\n        """"""\n        train_data = tx.data.MonoTextData(self._train_hparams)\n        test_data = tx.data.MonoTextData(self._test_hparams)\n\n        iterator = tx.data.TrainTestFeedableDataIterator(train=train_data,\n                                                         test=test_data)\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n\n            for _ in range(2):\n                iterator.restart_train_dataset(sess)\n                i = 0\n                while True:\n                    try:\n                        feed_dict = {\n                            iterator.handle: iterator.get_train_handle(sess)\n                        }\n                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i + 1))\n                        i = (i + 1) % 1000\n                    except tf.errors.OutOfRangeError:\n                        print(\'Train data limit reached\')\n                        self.assertEqual(i, 0)\n                        break\n\n                iterator.restart_test_dataset(sess)\n                i = 1001\n                while True:\n                    try:\n                        feed_dict = {\n                            iterator.handle: iterator.get_test_handle(sess)\n                        }\n                        data_batch_ = sess.run(data_batch, feed_dict=feed_dict)\n                        self.assertEqual(\n                            tf.compat.as_text(data_batch_[\'text\'][0][0]),\n                            str(i))\n                        i += 1\n                    except tf.errors.OutOfRangeError:\n                        print(\'Test data limit reached\')\n                        self.assertEqual(i, 2001)\n                        break\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data/dataset_utils_test.py,6,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data utils.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.data.data import dataset_utils as dsutils\n\n\n# pylint: disable=invalid-name\n\nclass TransformationTest(tf.test.TestCase):\n    """"""Tests various transformation utilities.\n    """"""\n\n    def test_make_chained_transformation(self):\n        """"""Tests :func:`texar.tf.data.make_chained_transformation`\n        """"""\n        original_data = np.arange(0, 10)\n        dataset = tf.data.Dataset.from_tensor_slices(original_data)\n\n        def _tran_a(data):\n            return data + 100\n\n        def _tran_b(data):\n            return data + 1000\n\n        def _tran_c(data):\n            return data + 10000\n\n        chained_tran = dsutils.make_chained_transformation(\n            [_tran_a, _tran_b, _tran_c])\n        dataset = dataset.map(chained_tran)\n\n        iterator = dataset.make_one_shot_iterator()\n        elem = iterator.get_next()\n        with self.test_session() as sess:\n            data_ = []\n            while True:\n                try:\n                    data_.append(sess.run(elem))\n                except tf.errors.OutOfRangeError:\n                    break\n            self.assertEqual(len(data_), len(original_data))\n            data_ = [elem_ - 11100 for elem_ in data_]\n            self.assertEqual(data_, original_data.tolist())\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data/mono_text_data_test.py,17,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data related operations.\n""""""\n\nimport tempfile\nimport copy\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n# pylint: disable=too-many-locals, protected-access, too-many-branches\n# pylint: disable=invalid-name\n\n\nclass MonoTextDataTest(tf.test.TestCase):\n    """"""Tests text data class.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create test data\n        vocab_list = [\'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        text = [\'This is a test sentence .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82\']\n        text_file = tempfile.NamedTemporaryFile()\n        text_file.write(\'\\n\'.join(text).encode(""utf-8""))\n        text_file.flush()\n        self._text_file = text_file\n\n        self._hparams = {\n            ""num_epochs"": 50,\n            ""batch_size"": 3,\n            ""dataset"": {\n                ""files"": self._text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n            }\n        }\n\n    def _run_and_test(self,\n                      hparams,\n                      test_batch_size=False,\n                      length_inc=None):\n        # Construct database\n        text_data = tx.data.MonoTextData(hparams)\n        self.assertEqual(text_data.vocab.size,\n                         self._vocab_size + len(text_data.vocab.special_tokens))\n\n        iterator = text_data.dataset.make_initializable_iterator()\n        text_data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n\n            while True:\n                try:\n                    data_batch_ = sess.run(text_data_batch)\n\n                    self.assertEqual(set(data_batch_.keys()),\n                                     set(text_data.list_items()))\n\n                    if test_batch_size:\n                        self.assertEqual(len(data_batch_[\'text\']),\n                                         hparams[\'batch_size\'])\n\n                    if length_inc:\n                        for i in range(len(data_batch_[\'text\'])):\n                            text_ = data_batch_[\'text\'][i].tolist()\n                            self.assertEqual(\n                                text_.index(b\'<EOS>\') + 1,\n                                data_batch_[\'length\'][i] - length_inc)\n\n                    max_seq_length = text_data.hparams.dataset.max_seq_length\n                    mode = text_data.hparams.dataset.length_filter_mode\n                    if max_seq_length == 6:\n                        max_l = max_seq_length\n                        max_l += text_data._decoder.added_length\n                        for length in data_batch_[\'length\']:\n                            self.assertLessEqual(length, max_l)\n                        if mode == ""discard"":\n                            for length in data_batch_[\'length\']:\n                                self.assertEqual(length, 5)\n                        elif mode == ""truncate"":\n                            num_length_6 = 0\n                            for length in data_batch_[\'length\']:\n                                num_length_6 += int(length == 6)\n                            self.assertGreater(num_length_6, 0)\n                        else:\n                            raise ValueError(""Unknown mode: %s"" % mode)\n\n                    if text_data.hparams.dataset.pad_to_max_seq_length:\n                        max_l = max_seq_length + text_data._decoder.added_length\n                        for x in data_batch_[\'text\']:\n                            self.assertEqual(len(x), max_l)\n                        for x in data_batch_[\'text_ids\']:\n                            self.assertEqual(len(x), max_l)\n\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_default_setting(self):\n        """"""Tests the logics of MonoTextData.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_batching(self):\n        """"""Tests different batching.\n        """"""\n        # dis-allow smaller final batch\n        hparams = copy.copy(self._hparams)\n        hparams.update({""allow_smaller_final_batch"": False})\n        self._run_and_test(hparams, test_batch_size=True)\n\n    def test_bucketing(self):\n        """"""Tests bucketing.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams.update({\n            ""bucket_boundaries"": [7],\n            ""bucket_batch_sizes"": [6, 4]})\n\n        text_data = tx.data.MonoTextData(hparams)\n        iterator = text_data.dataset.make_initializable_iterator()\n        text_data_batch = iterator.get_next()\n\n        hparams.update({\n            ""bucket_boundaries"": [7],\n            ""bucket_batch_sizes"": [7, 7],\n            ""allow_smaller_final_batch"": False})\n\n        text_data_1 = tx.data.MonoTextData(hparams)\n        iterator_1 = text_data_1.dataset.make_initializable_iterator()\n        text_data_batch_1 = iterator_1.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n            sess.run(iterator_1.initializer)\n\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_, data_batch_1_ = sess.run(\n                        [text_data_batch, text_data_batch_1])\n\n                    length_ = data_batch_[\'length\'][0]\n                    if length_ < 7:\n                        last_batch_size = hparams[\'num_epochs\'] % 6\n                        self.assertTrue(\n                            len(data_batch_[\'text\']) == 6 or\n                            len(data_batch_[\'text\']) == last_batch_size)\n                    else:\n                        last_batch_size = hparams[\'num_epochs\'] % 4\n                        self.assertTrue(\n                            len(data_batch_[\'text\']) == 4 or\n                            len(data_batch_[\'text\']) == last_batch_size)\n\n                    self.assertEqual(len(data_batch_1_[\'text\']), 7)\n\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_shuffle(self):\n        """"""Tests different shuffle strategies.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams.update({\n            ""shard_and_shuffle"": True,\n            ""shuffle_buffer_size"": 1})\n        self._run_and_test(hparams)\n\n    def test_prefetch(self):\n        """"""Tests prefetching.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams.update({""prefetch_buffer_size"": 2})\n        self._run_and_test(hparams)\n\n    def test_other_transformations(self):\n        """"""Tests use of other transformations\n        """"""\n        def _transform(x, data_specs):  # pylint: disable=invalid-name\n            x[data_specs.decoder.length_tensor_name] += 1\n            return x\n\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""].update(\n            {""other_transformations"": [_transform, _transform]})\n        self._run_and_test(hparams, length_inc=2)\n\n    def test_list_items(self):\n        """"""Tests the item names of the output data.\n        """"""\n        text_data = tx.data.MonoTextData(self._hparams)\n        self.assertSetEqual(set(text_data.list_items()),\n                            {""text"", ""text_ids"", ""length""})\n\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""][""data_name""] = ""data""\n        text_data = tx.data.MonoTextData(hparams)\n        self.assertSetEqual(set(text_data.list_items()),\n                            {""data_text"", ""data_text_ids"", ""data_length""})\n\n    def test_length_discard(self):\n        """"""Tests discard lenghy seq.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 4,\n                                   ""length_filter_mode"": ""discard""})\n        self._run_and_test(hparams)\n\n    def test_length_truncate(self):\n        """"""Tests truncation.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 4,\n                                   ""length_filter_mode"": ""truncate""})\n        hparams[""shuffle""] = False\n        hparams[""allow_smaller_final_batch""] = False\n        self._run_and_test(hparams)\n\n    def test_pad_to_max_length(self):\n        """"""Tests padding.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 10,\n                                   ""length_filter_mode"": ""truncate"",\n                                   ""pad_to_max_seq_length"": True})\n        self._run_and_test(hparams)\n\n\nclass VarUttMonoTextDataTest(tf.test.TestCase):\n    """"""Tests variable utterance text data class.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create test data\n        vocab_list = [\'word\', \'sentence\', \'\xe8\xaf\x8d\', \'response\', \'dialog\', \'1\', \'2\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        text = [\n            \'This is a dialog 1 sentence . ||| This is a dialog 1 sentence . \'\n            \'||| This is yet another dialog 1 sentence .\',  # //\n            \'This is a dialog 2 sentence . ||| \'\n            \'This is also a dialog 2 sentence . \',  # //\n            \'\xe8\xaf\x8d \xe8\xaf\x8d \xe8\xaf\x8d ||| word\',  # //\n            \'This This\',  # //\n            \'1 1 1 ||| 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ||| 1 1 1 ||| 2\'\n        ]\n        text_file = tempfile.NamedTemporaryFile()\n        text_file.write(\'\\n\'.join(text).encode(""utf-8""))\n        text_file.flush()\n        self._text_file = text_file\n\n        self._hparams = {\n            ""num_epochs"": 50,\n            ""batch_size"": 3,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._text_file.name,\n                ""vocab_file"": self._vocab_file.name,\n                ""variable_utterance"": True,\n                ""max_utterance_cnt"": 3,\n                ""max_seq_length"": 10\n            }\n        }\n\n    def _run_and_test(self, hparams):\n        # Construct database\n        text_data = tx.data.MonoTextData(hparams)\n        self.assertEqual(text_data.vocab.size,\n                         self._vocab_size + len(text_data.vocab.special_tokens))\n\n        iterator = text_data.dataset.make_initializable_iterator()\n        text_data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_ = sess.run(text_data_batch)\n\n                    self.assertEqual(set(data_batch_.keys()),\n                                     set(text_data.list_items()))\n\n                    # Test utterance count\n                    utt_ind = np.sum(data_batch_[""text_ids""], 2) != 0\n                    utt_cnt = np.sum(utt_ind, 1)\n                    self.assertListEqual(\n                        data_batch_[text_data.utterance_cnt_name].tolist(),\n                        utt_cnt.tolist())\n\n                    if text_data.hparams.dataset.pad_to_max_seq_length:\n                        max_l = text_data.hparams.dataset.max_seq_length\n                        max_l += text_data._decoder.added_length\n                        for x in data_batch_[\'text\']:\n                            for xx in x:\n                                self.assertEqual(len(xx), max_l)\n                        for x in data_batch_[\'text_ids\']:\n                            for xx in x:\n                                self.assertEqual(len(xx), max_l)\n\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_default_setting(self):\n        """"""Tests the logics of the text data.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_pad_to_max_length(self):\n        """"""Tests padding.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""dataset""].update({""max_seq_length"": 20,\n                                   ""length_filter_mode"": ""truncate"",\n                                   ""pad_to_max_seq_length"": True})\n        self._run_and_test(hparams)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data/multi_aligned_data_test.py,19,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data related operations.\n""""""\n\nimport sys\nimport tempfile\nimport copy\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n# pylint: disable=too-many-locals, too-many-branches, protected-access\n\n\nclass MultiAlignedDataTest(tf.test.TestCase):\n    """"""Tests multi aligned text data class.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create test data\n        vocab_list = [\'This\', \'is\', \'a\', \'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        text_0 = [\'This is a sentence from source .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 source\']\n        text_0_file = tempfile.NamedTemporaryFile()\n        text_0_file.write(\'\\n\'.join(text_0).encode(""utf-8""))\n        text_0_file.flush()\n        self._text_0_file = text_0_file\n\n        text_1 = [\'This is a sentence from target .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 target\']\n        text_1_file = tempfile.NamedTemporaryFile()\n        text_1_file.write(\'\\n\'.join(text_1).encode(""utf-8""))\n        text_1_file.flush()\n        self._text_1_file = text_1_file\n\n        text_2 = [\n            \'This is a sentence from dialog . ||| dialog \',\n            \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 ||| \xe8\xaf\x8d dialog\']\n        text_2_file = tempfile.NamedTemporaryFile()\n        text_2_file.write(\'\\n\'.join(text_2).encode(""utf-8""))\n        text_2_file.flush()\n        self._text_2_file = text_2_file\n\n        int_3 = [0, 1]\n        int_3_file = tempfile.NamedTemporaryFile()\n        int_3_file.write((\'\\n\'.join([str(_) for _ in int_3])).encode(""utf-8""))\n        int_3_file.flush()\n        self._int_3_file = int_3_file\n\n        def _bytes_feature(value):\n            """"""Returns a bytes_list from a string / byte.\n            """"""\n            value = tf.compat.as_bytes(\n                value,\n                encoding=\'utf-8\'\n            )\n            return tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[value]))\n\n        def _int64_feature(value):\n            """"""Returns an int64_list from a bool / enum / int / uint.\n            """"""\n            return tf.train.Feature(\n                int64_list=tf.train.Int64List(value=[value]))\n\n        feature = {\n            ""number1"": _int64_feature(128),\n            ""number2"": _int64_feature(512),\n            ""text"": _bytes_feature(""This is a sentence for TFRecord \xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82"")\n        }\n        data_example = tf.train.Example(\n            features=tf.train.Features(feature=feature))\n        tfrecord_file = tempfile.NamedTemporaryFile(suffix="".tfrecord"")\n        with tf.python_io.TFRecordWriter(tfrecord_file.name) as writer:\n            writer.write(data_example.SerializeToString())\n        tfrecord_file.flush()\n        self._tfrecord_file = tfrecord_file\n\n        # Construct database\n        self._hparams = {\n            ""num_epochs"": 123,\n            ""batch_size"": 23,\n            ""datasets"": [\n                {  # dataset 0\n                    ""files"": [self._text_0_file.name],\n                    ""vocab_file"": self._vocab_file.name,\n                    ""bos_token"": """",\n                    ""data_name"": ""0""\n                },\n                {  # dataset 1\n                    ""files"": [self._text_1_file.name],\n                    ""vocab_share_with"": 0,\n                    ""eos_token"": ""<TARGET_EOS>"",\n                    ""data_name"": ""1""\n                },\n                {  # dataset 2\n                    ""files"": [self._text_2_file.name],\n                    ""vocab_file"": self._vocab_file.name,\n                    ""processing_share_with"": 0,\n                    ""variable_utterance"": True,\n                    ""data_name"": ""2""\n                },\n                {  # dataset 3\n                    ""files"": self._int_3_file.name,\n                    ""data_type"": ""int"",\n                    ""data_name"": ""label""\n                },\n                {  # dataset 4\n                    ""files"": self._tfrecord_file.name,\n                    ""feature_original_types"": {\n                        \'number1\': [\'tf.int64\', \'FixedLenFeature\'],\n                        \'number2\': [\'tf.int64\', \'FixedLenFeature\'],\n                        \'text\': [\'tf.string\', \'FixedLenFeature\'],\n                    },\n                    ""feature_convert_types"": {\n                        \'number2\': \'tf.float32\',\n                    },\n                    ""num_shards"": 2,\n                    ""shard_id"": 1,\n                    ""data_type"": ""tf_record"",\n                    ""data_name"": ""4""\n                }\n            ]\n        }\n\n    def _run_and_test(self, hparams, discard_did=None):\n        # Construct database\n        text_data = tx.data.MultiAlignedData(hparams)\n        self.assertEqual(\n            text_data.vocab(0).size,\n            self._vocab_size + len(text_data.vocab(0).special_tokens))\n\n        iterator = text_data.dataset.make_initializable_iterator()\n        text_data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_ = sess.run(text_data_batch)\n\n                    self.assertEqual(set(data_batch_.keys()),\n                                     set(text_data.list_items()))\n                    self.assertEqual(text_data.utterance_cnt_name(\'2\'),\n                                     \'2_utterance_cnt\')\n                    text_0 = data_batch_[\'0_text\']\n                    text_1 = data_batch_[\'1_text\']\n                    text_2 = data_batch_[\'2_text\']\n                    int_3 = data_batch_[\'label\']\n                    number_1 = data_batch_[\'4_number1\']\n                    number_2 = data_batch_[\'4_number2\']\n                    text_3 = data_batch_[\'4_text\']\n\n                    # pylint: disable=invalid-name\n                    for t0, t1, t2, i3, n1, n2, t4 in zip(\n                        text_0, text_1, text_2, int_3,\n                            number_1, number_2, text_3):\n\n                        np.testing.assert_array_equal(\n                            t0[:2], t1[1:3])\n                        np.testing.assert_array_equal(\n                            t0[:3], t2[0][:3])\n                        if t0[0].startswith(b\'This\'):\n                            self.assertEqual(i3, 0)\n                        else:\n                            self.assertEqual(i3, 1)\n                        self.assertEqual(n1, 128)\n                        self.assertEqual(n2, 512)\n                        self.assertTrue(isinstance(n1, np.int64))\n                        self.assertTrue(isinstance(n2, np.float32))\n                        self.assertTrue(isinstance(t4, bytes))\n\n                    if discard_did is not None:\n                        hpms = text_data._hparams.datasets[discard_did]\n                        max_l = hpms.max_seq_length\n                        max_l += text_data._decoder[discard_did].added_length\n                        for i in range(2):\n                            for length in data_batch_[text_data.length_name(i)]:\n                                self.assertLessEqual(length, max_l)\n                        for lengths in data_batch_[text_data.length_name(2)]:\n                            for length in lengths:\n                                self.assertLessEqual(length, max_l)\n                    for i, hpms in enumerate(text_data._hparams.datasets):\n                        if hpms.data_type != ""text"":\n                            continue\n                        max_l = hpms.max_seq_length\n                        mode = hpms.length_filter_mode\n                        if max_l is not None and mode == ""truncate"":\n                            max_l += text_data._decoder[i].added_length\n                            for length in data_batch_[text_data.length_name(i)]:\n                                self.assertLessEqual(length, max_l)\n\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_default_setting(self):\n        """"""Tests the logics of the text data.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_length_filter(self):\n        """"""Tests filtering by length.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""datasets""][0].update(\n            {""max_seq_length"": 4,\n             ""length_filter_mode"": ""discard""})\n        hparams[""datasets""][1].update(\n            {""max_seq_length"": 2,\n             ""length_filter_mode"": ""truncate""})\n        self._run_and_test(hparams, discard_did=0)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data/paired_text_data_test.py,12,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data related operations.\n""""""\n\nimport tempfile\nimport copy\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\nfrom texar.tf.data import SpecialTokens\n\n# pylint: disable=too-many-locals, too-many-branches, protected-access\n# pylint: disable=invalid-name\n\n\nclass PairedTextDataTest(tf.test.TestCase):\n    """"""Tests paired text data class.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create test data\n        vocab_list = [\'This\', \'is\', \'a\', \'word\', \'\xe8\xaf\x8d\']\n        vocab_file = tempfile.NamedTemporaryFile()\n        vocab_file.write(\'\\n\'.join(vocab_list).encode(""utf-8""))\n        vocab_file.flush()\n        self._vocab_file = vocab_file\n        self._vocab_size = len(vocab_list)\n\n        src_text = [\'This is a sentence from source .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 source\']\n        src_text_file = tempfile.NamedTemporaryFile()\n        src_text_file.write(\'\\n\'.join(src_text).encode(""utf-8""))\n        src_text_file.flush()\n        self._src_text_file = src_text_file\n\n        tgt_text = [\'This is a sentence from target .\', \'\xe8\xaf\x8d \xe8\xaf\x8d \xe3\x80\x82 target\']\n        tgt_text_file = tempfile.NamedTemporaryFile()\n        tgt_text_file.write(\'\\n\'.join(tgt_text).encode(""utf-8""))\n        tgt_text_file.flush()\n        self._tgt_text_file = tgt_text_file\n\n        self._hparams = {\n            ""num_epochs"": 50,\n            ""batch_size"": 3,\n            ""source_dataset"": {\n                ""files"": [self._src_text_file.name],\n                ""vocab_file"": self._vocab_file.name,\n            },\n            ""target_dataset"": {\n                ""files"": self._tgt_text_file.name,\n                ""vocab_share"": True,\n                ""eos_token"": ""<TARGET_EOS>""\n            }\n        }\n\n    def _run_and_test(self, hparams, proc_shr=False, length_inc=None,\n                      discard_src=False):\n        # Construct database\n        text_data = tx.data.PairedTextData(hparams)\n        self.assertEqual(\n            text_data.source_vocab.size,\n            self._vocab_size + len(text_data.source_vocab.special_tokens))\n\n        iterator = text_data.dataset.make_initializable_iterator()\n        text_data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n\n            if proc_shr:\n                tgt_eos = b\'<EOS>\'\n            else:\n                tgt_eos = b\'<TARGET_EOS>\'\n\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_ = sess.run(text_data_batch)\n                    self.assertEqual(set(data_batch_.keys()),\n                                     set(text_data.list_items()))\n                    # Test matching\n                    src_text = data_batch_[\'source_text\']\n                    tgt_text = data_batch_[\'target_text\']\n                    if proc_shr:\n                        for src, tgt in zip(src_text, tgt_text):\n                            np.testing.assert_array_equal(src[:3], tgt[:3])\n                    else:\n                        for src, tgt in zip(src_text, tgt_text):\n                            np.testing.assert_array_equal(src[:3], tgt[1:4])\n                    self.assertTrue(\n                        tgt_eos in data_batch_[\'target_text\'][0])\n\n                    if length_inc:\n                        for i in range(len(data_batch_[\'source_text\'])):\n                            text_ = data_batch_[\'source_text\'][i].tolist()\n                            self.assertEqual(\n                                text_.index(b\'<EOS>\') + 1,\n                                data_batch_[\'source_length\'][i] - length_inc[0])\n                        for i in range(len(data_batch_[\'target_text\'])):\n                            text_ = data_batch_[\'target_text\'][i].tolist()\n                            self.assertEqual(\n                                text_.index(tgt_eos) + 1,\n                                data_batch_[\'target_length\'][i] - length_inc[1])\n\n                    if discard_src:\n                        src_hparams = text_data.hparams.source_dataset\n                        max_l = src_hparams.max_seq_length\n                        max_l += text_data._decoder[0].added_length\n                        for l in data_batch_[text_data.source_length_name]:\n                            self.assertLessEqual(l, max_l)\n\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_default_setting(self):\n        """"""Tests the logics of the text data.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_shuffle(self):\n        """"""Tests toggling shuffle.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""shuffle""] = False\n        self._run_and_test(hparams)\n\n    def test_processing_share(self):\n        """"""Tests sharing processing.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""target_dataset""][""processing_share""] = True\n        self._run_and_test(hparams, proc_shr=True)\n\n    def test_other_transformations(self):\n        """"""Tests use of other transformations\n        """"""\n        def _transform(x, data_specs):  # pylint: disable=invalid-name\n            x[data_specs.decoder.length_tensor_name] += 1\n            return x\n\n        hparams = copy.copy(self._hparams)\n        hparams[""source_dataset""].update(\n            {""other_transformations"": [_transform, _transform]})\n        hparams[""target_dataset""].update(\n            {""other_transformations"": [_transform]})\n        self._run_and_test(hparams, length_inc=(2, 1))\n\n    def test_length_filter(self):\n        """"""Tests filtering by length.\n        """"""\n        hparams = copy.copy(self._hparams)\n        hparams[""source_dataset""].update(\n            {""max_seq_length"": 4,\n             ""length_filter_mode"": ""discard""})\n        self._run_and_test(hparams, discard_src=True)\n\n    # def test_sequence_length(self):\n    #    hparams = {\n    #        ""batch_size"": 64,\n    #        ""num_epochs"": 1,\n    #        ""shuffle"": False,\n    #        ""allow_smaller_final_batch"": False,\n    #        ""source_dataset"": {\n    #            ""files"": ""../../../data/yelp/sentiment.dev.sort.0"",\n    #            ""vocab_file"": ""../../../data/yelp/vocab"",\n    #            ""bos_token"": SpecialTokens.BOS,\n    #            ""eos_token"": SpecialTokens.EOS,\n    #        },\n    #        ""target_dataset"": {\n    #            ""files"": ""../../../data/yelp/sentiment.dev.sort.1"",\n    #            ""vocab_share"": True,\n    #        },\n    #    }\n    #    data = tx.data.PairedTextData(hparams)\n\n    #    iterator = tx.data.TrainTestDataIterator(val=data)\n    #    text_data_batch = iterator.get_next()\n\n    #    with self.test_session() as sess:\n    #        sess.run(tf.global_variables_initializer())\n    #        sess.run(tf.local_variables_initializer())\n    #        sess.run(tf.tables_initializer())\n    #        iterator.switch_to_val_data(sess)\n\n    #        while True:\n    #            try:\n    #                data_batch_ = sess.run(text_data_batch)\n    #                src = data_batch_[""source_text_ids""]\n    #                src_len = data_batch_[""source_length""]\n    #                self.assertEqual(src.shape[1], np.max(src_len))\n    #                tgt = data_batch_[""target_text_ids""]\n    #                tgt_len = data_batch_[""target_length""]\n    #                self.assertEqual(tgt.shape[1], np.max(tgt_len))\n    #            except tf.errors.OutOfRangeError:\n    #                break\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data/scalar_data_test.py,11,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data related operations.\n""""""\n\nimport copy\nimport tempfile\nimport numpy as np\n\nimport tensorflow as tf\n\nimport texar.tf as tx\n\n\nclass ScalarDataTest(tf.test.TestCase):\n    """"""Tests scalar data class.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create test data\n        # pylint: disable=no-member\n        int_data = np.linspace(0, 100, num=101, dtype=np.int32).tolist()\n        int_data = [str(i) for i in int_data]\n        int_file = tempfile.NamedTemporaryFile()\n        int_file.write(\'\\n\'.join(int_data).encode(""utf-8""))\n        int_file.flush()\n        self._int_file = int_file\n\n        self._int_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._int_file.name,\n                ""data_type"": ""int"",\n                ""data_name"": ""label""\n            }\n        }\n\n        self._float_hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": self._int_file.name,\n                ""data_type"": ""float"",\n                ""data_name"": ""feat""\n            }\n        }\n\n    def _run_and_test(self, hparams):\n        # Construct database\n        scalar_data = tx.data.ScalarData(hparams)\n\n        self.assertEqual(scalar_data.list_items()[0],\n                         hparams[""dataset""][""data_name""])\n\n        iterator = scalar_data.dataset.make_initializable_iterator()\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n\n            i = 0\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_ = sess.run(data_batch)\n                    self.assertEqual(set(data_batch_.keys()),\n                                     set(scalar_data.list_items()))\n                    value = data_batch_[scalar_data.data_name][0]\n                    self.assertEqual(i, value)\n                    i += 1\n                    # pylint: disable=no-member\n                    if hparams[""dataset""][""data_type""] == ""int"":\n                        self.assertTrue(isinstance(value, np.int32))\n                    else:\n                        self.assertTrue(isinstance(value, np.float32))\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_default_setting(self):\n        """"""Tests the logics of ScalarData.\n        """"""\n        self._run_and_test(self._int_hparams)\n        self._run_and_test(self._float_hparams)\n\n    def test_shuffle(self):\n        """"""Tests results of toggling shuffle.\n        """"""\n        hparams = copy.copy(self._int_hparams)\n        hparams[""batch_size""] = 10\n        scalar_data = tx.data.ScalarData(hparams)\n        iterator = scalar_data.dataset.make_initializable_iterator()\n        data_batch = iterator.get_next()\n\n        hparams_sfl = copy.copy(hparams)\n        hparams_sfl[""shuffle""] = True\n        scalar_data_sfl = tx.data.ScalarData(hparams_sfl)\n        iterator_sfl = scalar_data_sfl.dataset.make_initializable_iterator()\n        data_batch_sfl = iterator_sfl.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n            sess.run(iterator_sfl.initializer)\n\n            vals = []\n            vals_sfl = []\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_, data_batch_sfl_ = sess.run([data_batch,\n                                                             data_batch_sfl])\n                    vals += data_batch_[scalar_data.data_name].tolist()\n                    vals_sfl += data_batch_sfl_[scalar_data.data_name].tolist()\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n            self.assertEqual(len(vals), len(vals_sfl))\n            self.assertSetEqual(set(vals), set(vals_sfl))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/data/tfrecord_data_test.py,29,"b'# -*- coding: utf-8 -*-\n#\n""""""\nUnit tests for data related operations.\n""""""\n\nimport os\nimport sys\nimport copy\nimport shutil\nimport tempfile\nimport ssl\nimport tensorflow as tf\nimport texar.tf as tx\n\n\nssl._create_default_https_context = ssl._create_unverified_context\n\n\nclass TFRecordDataTest(tf.test.TestCase):\n    """"""Tests tfrecord data class.\n    """"""\n    # pylint: disable=too-many-locals\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n\n        # Create test data\n        # pylint: disable=no-member\n        self._test_dir = tempfile.mkdtemp()\n\n        cat_in_snow = tf.keras.utils.get_file(\n            os.path.join(self._test_dir, \'cat_0.jpg\'),\n            \'https://storage.googleapis.com/download.tensorflow.org/\'\n            \'example_images/320px-Felis_catus-cat_on_snow.jpg\')\n        williamsburg_bridge = tf.keras.utils.get_file(\n            os.path.join(self._test_dir, \'bridge_0.jpg\'),\n            \'https://storage.googleapis.com/download.tensorflow.org/\'\n            \'example_images/194px-New_East_River_Bridge_from_Brooklyn_\'\n            \'det.4a09796u.jpg\')\n\n        def _bytes_feature(value=None):\n            """"""Returns a bytes_list from a string / byte.\n            """"""\n            # pylint: disable=undefined-loop-variable\n            value = tf.compat.as_bytes(\n                value,\n                encoding=\'utf-8\'\n            )\n            return tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[value]))\n\n        def _int64_feature(value=None):\n            """"""Returns an int64_list from a bool / enum / int / uint.\n            """"""\n            return tf.train.Feature(\n                int64_list=tf.train.Int64List(value=[value]))\n\n        _feature_original_types = {\n            \'height\': [\'tf.int64\', \'FixedLenFeature\'],\n            \'width\': [\'tf.int64\', \'FixedLenFeature\'],\n            \'label\': [\'tf.int64\', \'FixedLenFeature\'],\n            \'shape\': [tf.int64, \'VarLenFeature\'],\n            \'image_raw\': [\'tf.string\', \'FixedLenFeature\'],\n            \'variable1\': [tf.string, \'FixedLenFeature\'],\n            \'variable2\': [\'tf.int64\', \'FixedLenFeature\'],\n        }\n        self._feature_convert_types = {\n            \'variable1\': \'tf.float32\',\n            \'variable2\': \'tf.string\',\n        }\n        _image_options = {}\n        self._unconvert_features = [\'height\', \'width\', \'label\']\n\n        def _image_example(image_string, image_shape, label):\n            """"""Create data example with image\n            """"""\n            feature = {\n                \'height\': _int64_feature(image_shape[0]),\n                \'width\': _int64_feature(image_shape[1]),\n                \'shape\': tf.train.Feature(\n                    int64_list=tf.train.Int64List(value=list(image_shape))),\n                \'label\': _int64_feature(label),\n                \'image_raw\': _bytes_feature(image_string),\n                \'variable1\': _bytes_feature(\'1234567890\'),\n                \'variable2\': _int64_feature(9876543210),\n            }\n            return tf.train.Example(\n                features=tf.train.Features(feature=feature))\n\n        self._dataset_valid = {\n            \'height\': [],\n            \'width\': [],\n            \'shape\': [],\n            \'label\': [],\n            \'image_raw\': [],\n            \'variable1\': [],\n            \'variable2\': [],\n        }\n        _toy_image_labels_valid = {\n            cat_in_snow: 0,\n            williamsburg_bridge: 1,\n        }\n        _toy_image_shapes = {\n            cat_in_snow: (213, 320, 3),\n            williamsburg_bridge: (239, 194),\n        }\n        _tfrecord_filepath = os.path.join(\n            self._test_dir,\n            \'test.tfrecord\')\n        # Prepare Validation data\n        with tf.python_io.TFRecordWriter(_tfrecord_filepath) as writer:\n            for image_path, label in _toy_image_labels_valid.items():\n\n                with open(image_path, \'rb\') as fid:\n                    image_data = fid.read()\n                image_shape = _toy_image_shapes[image_path]\n\n                tf_example = _image_example(image_data, image_shape, label)\n                writer.write(tf_example.SerializeToString())\n\n                # _construct_dataset_valid("""", shape, label)\n                single_data = {\n                    \'height\': image_shape[0],\n                    \'width\': image_shape[1],\n                    \'shape\': image_shape,\n                    \'label\': label,\n                    \'image_raw\': image_data,\n                    \'variable1\': ""1234567890"",\n                    \'variable2\': int(9876543210),\n                }\n                for key, value in single_data.items():\n                    self._dataset_valid[key].append(value)\n\n        self._hparams = {\n            ""num_epochs"": 1,\n            ""batch_size"": 1,\n            ""shuffle"": False,\n            ""dataset"": {\n                ""files"": _tfrecord_filepath,\n                ""feature_original_types"": _feature_original_types,\n                ""feature_convert_types"": self._feature_convert_types,\n                ""image_options"": [_image_options],\n            }\n        }\n\n    def tearDown(self):\n        """"""Remove the downloaded files after the test\n        """"""\n        shutil.rmtree(self._test_dir)\n\n    def _run_and_test(self, hparams):\n        # Construct database\n        tfrecord_data = tx.data.TFRecordData(hparams)\n        iterator = tfrecord_data.dataset.make_initializable_iterator()\n        data_batch = iterator.get_next()\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n            sess.run(tf.tables_initializer())\n            sess.run(iterator.initializer)\n            i = 0\n\n            def _prod(lst):\n                res = 1\n                for i in lst:\n                    res *= i\n                return res\n            while True:\n                try:\n                    # Run the logics\n                    data_batch_ = sess.run(data_batch)\n                    self.assertEqual(\n                        set(data_batch_.keys()),\n                        set(tfrecord_data.list_items()))\n\n                    # Check data consistency\n                    for key in self._unconvert_features:\n                        value = data_batch_[key][0]\n                        self.assertEqual(value, self._dataset_valid[key][i])\n                    self.assertEqual(\n                        list(data_batch_[\'shape\'].values),\n                        list(self._dataset_valid[\'shape\'][i]))\n\n                    # Check data type conversion\n                    for key, item in self._feature_convert_types.items():\n                        value = data_batch_[key][0]\n                        if item == \'tf.string\' or item is tf.string:\n                            self.assertTrue(isinstance(value, bytes))\n                        else:\n                            dtype_matched = (\n                                tx.utils.dtypes.get_tf_dtype(str(value.dtype))\n                                is tx.utils.dtypes.get_tf_dtype(item))\n                            self.assertTrue(dtype_matched)\n\n                    # Check image decoding and resize\n                    if hparams[""dataset""].get(""image_options""):\n                        image_options = hparams[""dataset""].get(""image_options"")\n                        if isinstance(image_options, dict):\n                            image_options = [image_options]\n                        for image_option_feature in image_options:\n                            image_key = image_option_feature.get(\n                                ""image_feature_name"")\n                            if image_key is None:\n                                continue\n                            image_gen = data_batch_[image_key][0]\n                            image_valid_shape = self._dataset_valid[""shape""][i]\n                            resize_height = image_option_feature.get(\n                                ""resize_height"")\n                            resize_width = image_option_feature.get(\n                                ""resize_width"")\n                            if resize_height and resize_width:\n                                self.assertEqual(\n                                    image_gen.shape[0] * image_gen.shape[1],\n                                    resize_height * resize_width)\n                            else:\n                                self.assertEqual(\n                                    _prod(image_gen.shape),\n                                    _prod(image_valid_shape))\n                    i += 1\n                except tf.errors.OutOfRangeError:\n                    print(\'Done -- epoch limit reached\')\n                    break\n\n    def test_default_setting(self):\n        """"""Tests the logics of TFRecordData.\n        """"""\n        self._run_and_test(self._hparams)\n\n    def test_image_resize(self):\n        """"""Tests the image resize function\n        """"""\n        hparams = copy.copy(self._hparams)\n        _image_options = {\n            \'image_feature_name\': \'image_raw\',\n            \'resize_height\': 512,\n            \'resize_width\': 512,\n        }\n        hparams[""dataset""].update({""image_options"": _image_options})\n        self._run_and_test(hparams)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/tokenizers/bert_tokenizer_test.py,4,"b'""""""\nUnit tests for pre-trained BERT tokenizer.\n""""""\n\nimport os\nimport pickle\nimport tempfile\n\nimport tensorflow as tf\n\nfrom texar.tf.data.tokenizers.bert_tokenizer import \\\n    BERTTokenizer\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass BERTTokenizerTest(tf.test.TestCase):\n\n    def setUp(self):\n        vocab_tokens = [\n            ""[UNK]"", ""[CLS]"", ""[SEP]"", ""want"", ""##want"", ""##ed"", ""wa"", ""un"",\n            ""runn"",\n            ""##ing"", "","", ""low"", ""lowest"",\n        ]\n\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.vocab_file = os.path.join(self.tmp_dir.name, \'vocab.txt\')\n        with open(self.vocab_file, ""w"", encoding=\'utf-8\') as vocab_writer:\n            vocab_writer.write("""".join([x + ""\\n"" for x in vocab_tokens]))\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in BERTTokenizer.available_checkpoints():\n            tokenizer = BERTTokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(u""UNwant\\u00E9d,running"")\n\n    def test_tokenize(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        tokens = tokenizer.map_text_to_token(u""UNwant\\u00E9d,running"")\n        self.assertListEqual(tokens,\n                             [""un"", ""##want"", ""##ed"", "","", ""runn"", ""##ing""])\n\n        ids = tokenizer.map_token_to_id(tokens)\n        self.assertListEqual(ids, [7, 4, 5, 10, 8, 9])\n\n    def test_pickle(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(BERTTokenizer._MODEL2URL.keys())\n        model_list_2 = list(BERTTokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        input_text = u""UNwant\\u00E9d,running""\n        output_text = u""unwanted, running""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_encode_text(self):\n        tokenizer = BERTTokenizer.load(self.vocab_file)\n\n        text_1 = u""He is very happy""\n        text_2 = u""unwanted, running""\n\n        text_1_ids = tokenizer.map_text_to_id(text_1)\n        text_2_ids = tokenizer.map_text_to_id(text_2)\n\n        cls_token_id = tokenizer.map_token_to_id(tokenizer.cls_token)\n        sep_token_id = tokenizer.map_token_to_id(tokenizer.sep_token)\n\n        input_ids, segment_ids, input_mask = \\\n            tokenizer.encode_text(text_1, None, 4)\n\n        self.assertListEqual(input_ids,\n                             [cls_token_id] + text_1_ids[:2] + [sep_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 0])\n        self.assertListEqual(input_mask, [1, 1, 1, 1])\n\n        input_ids, segment_ids, input_mask = \\\n            tokenizer.encode_text(text_1, text_2, 7)\n\n        self.assertListEqual(input_ids, [cls_token_id] + text_1_ids[:2] +\n                             [sep_token_id] + text_2_ids[:2] + [sep_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 0, 1, 1, 1])\n        self.assertListEqual(input_mask, [1, 1, 1, 1, 1, 1, 1])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/tokenizers/bert_tokenizer_utils_test.py,3,"b'""""""\nUnit tests for pre-trained BERT tokenizer utils.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.data.tokenizers.bert_tokenizer_utils import (\n    BasicTokenizer, WordpieceTokenizer, _is_control,\n    _is_punctuation, _is_whitespace)\n\n\nclass BERTTokenizerUtilsTest(tf.test.TestCase):\n\n    def test_chinese(self):\n\n        tokenizer = BasicTokenizer()\n\n        self.assertListEqual(\n            tokenizer.tokenize(u""ah\\u535A\\u63A8zz""),\n            [u""ah"", u""\\u535A"", u""\\u63A8"", u""zz""])\n\n    def test_basic_tokenizer_lower(self):\n\n        tokenizer = BasicTokenizer(do_lower_case=True)\n\n        self.assertListEqual(\n            tokenizer.tokenize(u"" \\tHeLLo!how  \\n Are yoU?  ""),\n            [""hello"", ""!"", ""how"", ""are"", ""you"", ""?""])\n        self.assertListEqual(tokenizer.tokenize(u""H\\u00E9llo""), [""hello""])\n\n    def test_basic_tokenizer_no_lower(self):\n\n        tokenizer = BasicTokenizer(do_lower_case=False)\n\n        self.assertListEqual(\n            tokenizer.tokenize(u"" \\tHeLLo!how  \\n Are yoU?  ""),\n            [""HeLLo"", ""!"", ""how"", ""Are"", ""yoU"", ""?""])\n\n    def test_wordpiece_tokenizer(self):\n\n        vocab_tokens = [\n            ""[UNK]"", ""[CLS]"", ""[SEP]"", ""want"", ""##want"", ""##ed"", ""wa"", ""un"", ""runn"",\n            ""##ing""\n        ]\n\n        vocab = {}\n        for (i, token) in enumerate(vocab_tokens):\n            vocab[token] = i\n        tokenizer = WordpieceTokenizer(vocab=vocab, unk_token=""[UNK]"")\n\n        self.assertListEqual(tokenizer.tokenize(""""), [])\n\n        self.assertListEqual(\n            tokenizer.tokenize(""unwanted running""),\n            [""un"", ""##want"", ""##ed"", ""runn"", ""##ing""])\n\n        self.assertListEqual(\n            tokenizer.tokenize(""unwantedX running""), [""[UNK]"", ""runn"", ""##ing""])\n\n    def test_is_whitespace(self):\n\n        self.assertTrue(_is_whitespace(u"" ""))\n        self.assertTrue(_is_whitespace(u""\\t""))\n        self.assertTrue(_is_whitespace(u""\\r""))\n        self.assertTrue(_is_whitespace(u""\\n""))\n        self.assertTrue(_is_whitespace(u""\\u00A0""))\n\n        self.assertFalse(_is_whitespace(u""A""))\n        self.assertFalse(_is_whitespace(u""-""))\n\n    def test_is_control(self):\n\n        self.assertTrue(_is_control(u""\\u0005""))\n\n        self.assertFalse(_is_control(u""A""))\n        self.assertFalse(_is_control(u"" ""))\n        self.assertFalse(_is_control(u""\\t""))\n        self.assertFalse(_is_control(u""\\r""))\n\n    def test_is_punctuation(self):\n\n        self.assertTrue(_is_punctuation(u""-""))\n        self.assertTrue(_is_punctuation(u""$""))\n        self.assertTrue(_is_punctuation(u""`""))\n        self.assertTrue(_is_punctuation(u"".""))\n\n        self.assertFalse(_is_punctuation(u""A""))\n        self.assertFalse(_is_punctuation(u"" ""))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/tokenizers/gpt2_tokenizer_test.py,4,"b'""""""\nUnit tests for pre-trained GPT2 tokenizer.\n""""""\n\nimport json\nimport os\nimport pickle\nimport tempfile\n\nimport tensorflow as tf\n\nfrom texar.tf.data.tokenizers.gpt2_tokenizer import \\\n    GPT2Tokenizer\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass GPT2TokenizerTest(tf.test.TestCase):\n\n    def setUp(self):\n        vocab = [""l"", ""o"", ""w"", ""e"", ""r"", ""s"", ""t"", ""i"", ""d"", ""n"",\n                 ""lo"", ""low"", ""er"",\n                 ""low"", ""lowest"", ""newer"", ""wider"", ""<unk>""]\n        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n        merges = [""#version: 0.2"", ""l o"", ""lo w"", ""e r"", """"]\n        self.special_tokens_map = {""unk_token"": ""<unk>""}\n\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        self.vocab_file = os.path.join(self.tmp_dir.name, \'encoder.json\')\n        self.merges_file = os.path.join(self.tmp_dir.name, \'vocab.bpe\')\n\n        with open(self.vocab_file, ""w"") as fp:\n            fp.write(json.dumps(vocab_tokens))\n        with open(self.merges_file, ""w"") as fp:\n            fp.write(""\\n"".join(merges))\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in \\\n                GPT2Tokenizer.available_checkpoints():\n            tokenizer = GPT2Tokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(\n                u""Munich and Berlin are nice cities"")\n\n    def test_tokenize(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        text = ""lower""\n        bpe_tokens = [""low"", ""er""]\n        tokens = tokenizer.map_text_to_token(text)\n        self.assertListEqual(tokens, bpe_tokens)\n\n        input_tokens = tokens + [tokenizer.unk_token]\n        input_bpe_tokens = [13, 12, 17]\n        self.assertListEqual(\n            tokenizer.map_token_to_id(input_tokens),\n            input_bpe_tokens)\n\n    def test_pickle(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(GPT2Tokenizer._MODEL2URL.keys())\n        model_list_2 = list(GPT2Tokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        input_text = u""lower newer""\n        output_text = u""lower<unk>newer""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_encode_text(self):\n        tokenizer = GPT2Tokenizer.load(self.tmp_dir.name,\n                                       self.special_tokens_map)\n\n        text_1 = u""lower newer""\n\n        text_1_ids = tokenizer.map_text_to_id(text_1)\n\n        input_ids, seq_len = \\\n            tokenizer.encode_text(text=text_1, max_seq_length=10)\n\n        bos_token_id = tokenizer.map_token_to_id(tokenizer.bos_token)\n        eos_token_id = tokenizer.map_token_to_id(tokenizer.eos_token)\n        pad_token_id = tokenizer.map_token_to_id(tokenizer.pad_token)\n\n        self.assertListEqual(input_ids,\n                             [bos_token_id] + text_1_ids + [eos_token_id] +\n                             [pad_token_id])\n        self.assertEqual(seq_len, 9)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/data/tokenizers/xlnet_tokenizer_test.py,5,"b'""""""\nUnit tests for pre-trained XLNet tokenizer.\n""""""\n\nimport os\nimport pickle\nimport tempfile\n\nimport tensorflow as tf\n\nfrom texar.tf.data.data_utils import maybe_download\nfrom texar.tf.data.tokenizers.xlnet_tokenizer import \\\n    XLNetTokenizer, SPIECE_UNDERLINE\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass XLNetTokenizerTest(tf.test.TestCase):\n\n    def setUp(self):\n        self.tmp_dir = tempfile.TemporaryDirectory()\n        # Use the test sentencepiece model downloaded from huggingface\n        # transformers\n        self.SAMPLE_VOCAB = maybe_download(\n            \'https://github.com/huggingface/transformers/blob/master/\'\n            \'tests/fixtures/test_sentencepiece.model?raw=true\',\n            self.tmp_dir.name)\n\n        self.tokenizer = XLNetTokenizer.load(\n            self.SAMPLE_VOCAB[0], configs={\'keep_accents\': True})\n        self.tokenizer.save(self.tmp_dir.name)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    @pretrained_test\n    def test_model_loading(self):\n        for pretrained_model_name in \\\n                XLNetTokenizer.available_checkpoints():\n            tokenizer = XLNetTokenizer(\n                pretrained_model_name=pretrained_model_name)\n            _ = tokenizer.map_text_to_token(u""This is a test"")\n\n    def test_tokenize(self):\n        tokens = self.tokenizer.map_text_to_token(u\'This is a test\')\n        self.assertListEqual(tokens, [u\'\xe2\x96\x81This\', u\'\xe2\x96\x81is\', u\'\xe2\x96\x81a\', u\'\xe2\x96\x81t\', u\'est\'])\n\n        self.assertListEqual(\n            self.tokenizer.map_token_to_id(tokens),\n            [285, 46, 10, 170, 382])\n\n        tokens = self.tokenizer.map_text_to_token(\n            u""I was born in 92000, and this is fals\xc3\xa9."")\n        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u\'I\',\n                                      SPIECE_UNDERLINE + u\'was\',\n                                      SPIECE_UNDERLINE + u\'b\',\n                                      u\'or\', u\'n\', SPIECE_UNDERLINE + u\'in\',\n                                      SPIECE_UNDERLINE + u\'\',\n                                      u\'9\', u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                      SPIECE_UNDERLINE + u\'and\',\n                                      SPIECE_UNDERLINE + u\'this\',\n                                      SPIECE_UNDERLINE + u\'is\',\n                                      SPIECE_UNDERLINE + u\'f\', u\'al\', u\'s\',\n                                      u\'\xc3\xa9\', u\'.\'])\n        ids = self.tokenizer.map_token_to_id(tokens)\n        self.assertListEqual(\n            ids, [8, 21, 84, 55, 24, 19, 7, 0,\n                  602, 347, 347, 347, 3, 12, 66,\n                  46, 72, 80, 6, 0, 4])\n\n        back_tokens = self.tokenizer.map_id_to_token(ids)\n        self.assertListEqual(back_tokens, [SPIECE_UNDERLINE + u\'I\',\n                                           SPIECE_UNDERLINE + u\'was\',\n                                           SPIECE_UNDERLINE + u\'b\',\n                                           u\'or\', u\'n\',\n                                           SPIECE_UNDERLINE + u\'in\',\n                                           SPIECE_UNDERLINE + u\'\', u\'<unk>\',\n                                           u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                           SPIECE_UNDERLINE + u\'and\',\n                                           SPIECE_UNDERLINE + u\'this\',\n                                           SPIECE_UNDERLINE + u\'is\',\n                                           SPIECE_UNDERLINE + u\'f\', u\'al\', u\'s\',\n                                           u\'<unk>\', u\'.\'])\n\n    def test_pickle(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n        self.assertIsNotNone(tokenizer)\n\n        text = u""Munich and Berlin are nice cities""\n        subwords = tokenizer.map_text_to_token(text)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            filename = os.path.join(tmpdirname, u""tokenizer.bin"")\n            with open(filename, ""wb"") as f:\n                pickle.dump(tokenizer, f)\n            with open(filename, ""rb"") as f:\n                tokenizer_new = pickle.load(f)\n\n        subwords_loaded = tokenizer_new.map_text_to_token(text)\n\n        self.assertListEqual(subwords, subwords_loaded)\n\n    def test_save_load(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n\n        before_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tokenizer.save(tmpdirname)\n            tokenizer = tokenizer.load(tmpdirname)\n\n        after_tokens = tokenizer.map_text_to_id(\n            u""He is very happy, UNwant\\u00E9d,running"")\n        self.assertListEqual(before_tokens, after_tokens)\n\n    def test_pretrained_model_list(self):\n        model_list_1 = list(XLNetTokenizer._MODEL2URL.keys())\n        model_list_2 = list(XLNetTokenizer._MAX_INPUT_SIZE.keys())\n\n        self.assertListEqual(model_list_1, model_list_2)\n\n    def test_encode_decode(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n\n        input_text = u""This is a test""\n        output_text = u""This is a test""\n\n        tokens = tokenizer.map_text_to_token(input_text)\n        ids = tokenizer.map_token_to_id(tokens)\n        ids_2 = tokenizer.map_text_to_id(input_text)\n        self.assertListEqual(ids, ids_2)\n\n        tokens_2 = tokenizer.map_id_to_token(ids)\n        text_2 = tokenizer.map_id_to_text(ids)\n\n        self.assertEqual(text_2, output_text)\n\n        self.assertNotEqual(len(tokens_2), 0)\n        self.assertIsInstance(text_2, str)\n\n    def test_add_tokens(self):\n        tokenizer = XLNetTokenizer.load(self.tmp_dir.name)\n\n        vocab_size = tokenizer.vocab_size\n        all_size = len(tokenizer)\n\n        self.assertNotEqual(vocab_size, 0)\n        self.assertEqual(vocab_size, all_size)\n\n        new_toks = [""aaaaabbbbbb"", ""cccccccccdddddddd""]\n        added_toks = tokenizer.add_tokens(new_toks)\n        vocab_size_2 = tokenizer.vocab_size\n        all_size_2 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_2, 0)\n        self.assertEqual(vocab_size, vocab_size_2)\n        self.assertEqual(added_toks, len(new_toks))\n        self.assertEqual(all_size_2, all_size + len(new_toks))\n\n        tokens = tokenizer.map_text_to_id(""aaaaabbbbbb low cccccccccdddddddd l"")\n        self.assertGreaterEqual(len(tokens), 4)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n\n        new_toks_2 = {\'eos_token\': "">>>>|||<||<<|<<"",\n                      \'pad_token\': ""<<<<<|||>|>>>>|>""}\n        added_toks_2 = tokenizer.add_special_tokens(new_toks_2)\n        vocab_size_3 = tokenizer.vocab_size\n        all_size_3 = len(tokenizer)\n\n        self.assertNotEqual(vocab_size_3, 0)\n        self.assertEqual(vocab_size, vocab_size_3)\n        self.assertEqual(added_toks_2, len(new_toks_2))\n        self.assertEqual(all_size_3, all_size_2 + len(new_toks_2))\n\n        tokens = tokenizer.map_text_to_id(\n            "">>>>|||<||<<|<< aaaaabbbbbb low cccccccccdddddddd ""\n            ""<<<<<|||>|>>>>|> l"")\n\n        self.assertGreaterEqual(len(tokens), 6)\n        self.assertGreater(tokens[0], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[0], tokens[1])\n        self.assertGreater(tokens[-2], tokenizer.vocab_size - 1)\n        self.assertGreater(tokens[-2], tokens[-3])\n        self.assertEqual(tokens[0],\n                         tokenizer.map_token_to_id(tokenizer.eos_token))\n        self.assertEqual(tokens[-2],\n                         tokenizer.map_token_to_id(tokenizer.pad_token))\n\n    def test_tokenizer_lower(self):\n        tokenizer = XLNetTokenizer.load(\n            self.SAMPLE_VOCAB[0], configs={\'do_lower_case\': True,\n                                           \'keep_accents\': False})\n        tokens = tokenizer.map_text_to_token(\n            u""I was born in 92000, and this is fals\xc3\xa9."")\n        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u\'\', u\'i\',\n                                      SPIECE_UNDERLINE + u\'was\',\n                                      SPIECE_UNDERLINE + u\'b\',\n                                      u\'or\', u\'n\', SPIECE_UNDERLINE + u\'in\',\n                                      SPIECE_UNDERLINE + u\'\',\n                                      u\'9\', u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                      SPIECE_UNDERLINE + u\'and\',\n                                      SPIECE_UNDERLINE + u\'this\',\n                                      SPIECE_UNDERLINE + u\'is\',\n                                      SPIECE_UNDERLINE + u\'f\', u\'al\', u\'se\',\n                                      u\'.\'])\n        self.assertListEqual(tokenizer.map_text_to_token(u""H\\u00E9llo""),\n                             [u""\xe2\x96\x81he"", u""ll"", u""o""])\n\n    def test_tokenizer_no_lower(self):\n        tokenizer = XLNetTokenizer.load(\n            self.SAMPLE_VOCAB[0], configs={\'do_lower_case\': False,\n                                           \'keep_accents\': False})\n        tokens = tokenizer.map_text_to_token(\n            u""I was born in 92000, and this is fals\xc3\xa9."")\n        self.assertListEqual(tokens, [SPIECE_UNDERLINE + u\'I\',\n                                      SPIECE_UNDERLINE + u\'was\',\n                                      SPIECE_UNDERLINE + u\'b\', u\'or\',\n                                      u\'n\', SPIECE_UNDERLINE + u\'in\',\n                                      SPIECE_UNDERLINE + u\'\',\n                                      u\'9\', u\'2\', u\'0\', u\'0\', u\'0\', u\',\',\n                                      SPIECE_UNDERLINE + u\'and\',\n                                      SPIECE_UNDERLINE + u\'this\',\n                                      SPIECE_UNDERLINE + u\'is\',\n                                      SPIECE_UNDERLINE + u\'f\', u\'al\', u\'se\',\n                                      u\'.\'])\n\n    def test_encode_text(self):\n        text_1 = u""He is very happy""\n        text_2 = u""unwanted, running""\n\n        text_1_ids = self.tokenizer.map_text_to_id(text_1)\n        text_2_ids = self.tokenizer.map_text_to_id(text_2)\n\n        cls_token_id = self.tokenizer.map_token_to_id(self.tokenizer.cls_token)\n        sep_token_id = self.tokenizer.map_token_to_id(self.tokenizer.sep_token)\n\n        input_ids, segment_ids, input_mask = \\\n            self.tokenizer.encode_text(text_1, None, 4)\n\n        self.assertListEqual(input_ids,\n                             text_1_ids[:2] + [sep_token_id] + [cls_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 2])\n        self.assertListEqual(input_mask, [0, 0, 0, 0])\n\n        input_ids, segment_ids, input_mask = \\\n            self.tokenizer.encode_text(text_1, text_2, 7)\n\n        self.assertListEqual(input_ids, text_1_ids[:2] +\n                             [sep_token_id] + text_2_ids[:2] + [sep_token_id] +\n                             [cls_token_id])\n        self.assertListEqual(segment_ids, [0, 0, 0, 1, 1, 1, 2])\n        self.assertListEqual(input_mask, [0, 0, 0, 0, 0, 0, 0])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/classifiers/bert_classifier_test.py,21,"b'""""""\nUnit tests for BERT classifiers.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom texar.tf.modules.classifiers.bert_classifier import BERTClassifier\nfrom texar.tf.utils.test import pretrained_test\n\n# pylint: disable=too-many-locals, no-member\n\n\nclass BERTClassifierTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.BERTClassifier` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in BERTClassifier.available_checkpoints():\n            classifier = BERTClassifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(inputs)\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        clas = BERTClassifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 199 + 2)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        clas = BERTClassifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 199 + 2)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise"",\n        }\n        clas = BERTClassifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 199 + 2)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size,\n                                             clas.hparams.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time, clas.hparams.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time, clas.hparams.encoder.dim))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": max_time\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size,\n                                             clas.hparams.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n    def test_binary(self):\n        """"""Tests binary classification.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size, max_time))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": max_time\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size, ))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": max_time\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        clas = BERTClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size, ))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/classifiers/conv_classifiers_test.py,8,"b'#\n""""""\nUnit tests for conv encoders.\n""""""\n\nimport tensorflow as tf\n\nimport texar.tf as tx\nfrom texar.tf.modules.classifiers.conv_classifiers import Conv1DClassifier\n\n\nclass Conv1DClassifierTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.Conv1DClassifier` class.\n    """"""\n\n    def test_classifier(self):\n        """"""Tests classification.\n        """"""\n        # case 1: default hparams\n        classifier = Conv1DClassifier()\n        self.assertEqual(len(classifier.layers), 5)\n        self.assertTrue(isinstance(classifier.layers[-1],\n                                   tf.layers.Dense))\n        inputs = tf.ones([64, 16, 300], tf.float32)\n        logits, pred = classifier(inputs)\n        self.assertEqual(logits.shape, [64, 2])\n        self.assertEqual(pred.shape, [64])\n\n        inputs = tf.placeholder(tf.float32, [64, None, 300])\n        logits, pred = classifier(inputs)\n        self.assertEqual(logits.shape, [64, 2])\n        self.assertEqual(pred.shape, [64])\n\n        # case 1\n        hparams = {\n            ""num_classes"": 10,\n            ""logit_layer_kwargs"": {""use_bias"": False}\n        }\n        classifier = Conv1DClassifier(hparams=hparams)\n        inputs = tf.ones([64, 16, 300], tf.float32)\n        logits, pred = classifier(inputs)\n        self.assertEqual(logits.shape, [64, 10])\n        self.assertEqual(pred.shape, [64])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/classifiers/gpt2_classifier_test.py,18,"b'""""""\nUnit tests for BERT classifiers.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom texar.tf.modules.classifiers.gpt2_classifier import GPT2Classifier\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass GPT2ClassifierTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.GPT2Classifier` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in GPT2Classifier.available_checkpoints():\n            classifier = GPT2Classifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(inputs)\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        clas = GPT2Classifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 198)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": 8,\n        }\n        clas = GPT2Classifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 198)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise"",\n        }\n        clas = GPT2Classifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 198)\n\n    def test_classification(self):\n        r""""""Tests classificaiton.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size, 2))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size, max_time, 10))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size, max_time, 768))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": max_time,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size, 10))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n    def test_binary(self):\n        r""""""Tests binary classification.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise"",\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size, max_time))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": max_time,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size,))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": max_time,\n        }\n        classifier = GPT2Classifier(hparams=hparams)\n        logits, preds = classifier(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, preds])\n            self.assertEqual(logits_.shape, (batch_size,))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/classifiers/rnn_classifiers_test.py,18,"b'#\n""""""\nUnit tests for RNN classifiers.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.classifiers.rnn_classifiers import \\\n        UnidirectionalRNNClassifier\n\n# pylint: disable=too-many-locals, no-member\n\n\nclass UnidirectionalRNNClassifierTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.UnidirectionalRNNClassifierTest` class.\n    """"""\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 100])\n\n        # case 1\n        clas = UnidirectionalRNNClassifier()\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 2 + 2)\n\n        # case 2\n        hparams = {\n            ""output_layer"": {""num_layers"": 2},\n            ""logit_layer_kwargs"": {""use_bias"": False}\n        }\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 2 + 2 + 2 + 1)\n        _, _ = clas(inputs)\n        self.assertEqual(len(clas.trainable_variables), 2 + 2 + 2 + 1)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        max_time = 8\n        batch_size = 16\n        emb_dim = 100\n        inputs = tf.random_uniform([batch_size, max_time, emb_dim],\n                                   maxval=1., dtype=tf.float32)\n\n        # case 1\n        clas = UnidirectionalRNNClassifier()\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size, clas.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n        # case 2\n        hparams = {\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time, clas.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 3\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 1,\n                ""layer_size"": 10\n            },\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time, 10))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 4\n        hparams = {\n            ""num_classes"": 10,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": max_time\n        }\n        inputs = tf.placeholder(tf.float32, shape=[batch_size, 6, emb_dim])\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randn(batch_size, 6, emb_dim)})\n            self.assertEqual(logits_.shape, (batch_size, clas.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n    def test_binary(self):\n        """"""Tests binary classification.\n        """"""\n        max_time = 8\n        batch_size = 16\n        emb_dim = 100\n        inputs = tf.random_uniform([batch_size, max_time, emb_dim],\n                                   maxval=1., dtype=tf.float32)\n\n        # case 1 omittd\n\n        # case 2\n        hparams = {\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size, max_time))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 3\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 1,\n                ""layer_size"": 10\n            },\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size, max_time))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 4\n        hparams = {\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_length"": max_time\n        }\n        inputs = tf.placeholder(tf.float32, shape=[batch_size, 6, emb_dim])\n        clas = UnidirectionalRNNClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randn(batch_size, 6, emb_dim)})\n            self.assertEqual(logits_.shape, (batch_size, ))\n            self.assertEqual(pred_.shape, (batch_size, ))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/classifiers/xlnet_classifier_test.py,21,"b'#\n""""""\nUnit tests for XLNet classifier.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom texar.tf.modules.classifiers.xlnet_classifier import XLNetClassifier\nfrom texar.tf.utils.test import pretrained_test\n\n# pylint: disable=too-many-locals, no-member\n\n\nclass XLNetClassifierTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.XLNetClassifier` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in XLNetClassifier.available_checkpoints():\n            classifier = XLNetClassifier(\n                pretrained_model_name=pretrained_model_name)\n            _, _ = classifier(inputs)\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        clas(inputs)\n        n_xlnet_vars = 162\n        n_projection_vars = 2\n        n_logits_vars = 2\n        self.assertEqual(len(clas.trainable_variables),\n                         n_xlnet_vars + n_logits_vars + n_projection_vars)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        clas(inputs)\n        self.assertEqual(len(clas.trainable_variables),\n                         n_xlnet_vars + n_logits_vars + n_projection_vars)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""clas_strategy"": ""all_time""\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        clas(inputs)\n        self.assertEqual(len(clas.trainable_variables),\n                         n_xlnet_vars + n_logits_vars + n_projection_vars)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size,\n                                             clas.hparams.num_classes))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 10,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time, clas.hparams.num_classes))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 0,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time, clas.hparams.hidden_dim))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 3,\n            ""clas_strategy"": ""all_time"",\n            ""use_projection"": False,\n            ""vocab_size"": 40000\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size,\n                                             clas.hparams.num_classes))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n    def test_binary(self):\n        """"""Tests binary classification.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""time_wise""\n        }\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run([logits, pred])\n            self.assertEqual(logits_.shape, (batch_size, max_time))\n            self.assertEqual(pred_.shape, (batch_size, max_time))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_len"": max_time\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size,))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_classes"": 1,\n            ""clas_strategy"": ""all_time"",\n            ""max_seq_len"": max_time\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        clas = XLNetClassifier(hparams=hparams)\n        logits, pred = clas(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_, pred_ = sess.run(\n                [logits, pred],\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size,))\n            self.assertEqual(pred_.shape, (batch_size,))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/connectors/connectors_test.py,36,"b'#\n""""""\nUnit tests for connectors.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow_probability import distributions as tfpd\nfrom tensorflow.python.util import nest    # pylint: disable=E0611\n\nfrom texar.tf.core import layers\nfrom texar.tf.modules import ConstantConnector\nfrom texar.tf.modules import MLPTransformConnector\nfrom texar.tf.modules import (ReparameterizedStochasticConnector,\n                           StochasticConnector)\nfrom texar.tf.modules.connectors.connectors import _assert_same_size\n\n# pylint: disable=too-many-locals, invalid-name\n\n\nclass TestConnectors(tf.test.TestCase):\n    """"""Tests various connectors.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._batch_size = 100\n\n        self._decoder_cell = layers.get_rnn_cell(\n            layers.default_rnn_cell_hparams())\n\n    def test_constant_connector(self):\n        """"""Tests the logic of\n        :class:`~texar.tf.modules.connectors.ConstantConnector`.\n        """"""\n        connector = ConstantConnector(self._decoder_cell.state_size)\n\n        decoder_initial_state_0 = connector(self._batch_size)\n        decoder_initial_state_1 = connector(self._batch_size, value=1.)\n        nest.assert_same_structure(decoder_initial_state_0,\n                                   self._decoder_cell.state_size)\n        nest.assert_same_structure(decoder_initial_state_1,\n                                   self._decoder_cell.state_size)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            s_0, s_1 = sess.run(\n                [decoder_initial_state_0, decoder_initial_state_1])\n            self.assertEqual(nest.flatten(s_0)[0][0, 0], 0.)\n            self.assertEqual(nest.flatten(s_1)[0][0, 0], 1.)\n\n    def test_forward_connector(self):\n        """"""Tests the logic of\n        :class:`~texar.tf.modules.connectors.ForwardConnector`.\n        """"""\n        # TODO(zhiting)\n        pass\n\n    def test_mlp_transform_connector(self):\n        """"""Tests the logic of\n        :class:`~texar.tf.modules.connectors.MLPTransformConnector`.\n        """"""\n        connector = MLPTransformConnector(self._decoder_cell.state_size)\n        output = connector(tf.zeros([5, 10]))\n        nest.assert_same_structure(output, self._decoder_cell.state_size)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            output_ = sess.run(output)\n            nest.assert_same_structure(output_, self._decoder_cell.state_size)\n\n    def test_reparameterized_stochastic_connector(self):\n        """"""Tests the logic of\n        :class:`~texar.tf.modules.ReparameterizedStochasticConnector`.\n        """"""\n        state_size = (10, 10)\n        variable_size = 100\n        state_size_ts = (tf.TensorShape([10, 10]), tf.TensorShape([2, 3, 4]))\n        sample_num = 10\n\n        mu = tf.zeros([self._batch_size, variable_size])\n        var = tf.ones([self._batch_size, variable_size])\n        mu_vec = tf.zeros([variable_size])\n        var_vec = tf.ones([variable_size])\n        gauss_ds = tfpd.MultivariateNormalDiag(loc=mu, scale_diag=var)\n        gauss_ds_vec = tfpd.MultivariateNormalDiag(loc=mu_vec,\n                                                   scale_diag=var_vec)\n        gauss_connector = ReparameterizedStochasticConnector(state_size)\n        gauss_connector_ts = ReparameterizedStochasticConnector(state_size_ts)\n\n        output_1, _ = gauss_connector(gauss_ds)\n        output_2, _ = gauss_connector(\n            distribution=""MultivariateNormalDiag"",\n            distribution_kwargs={""loc"": mu, ""scale_diag"": var})\n        sample_ts, _ = gauss_connector_ts(gauss_ds)\n\n        # specify sample num\n        sample_test_num, _ = gauss_connector(\n            gauss_ds_vec, num_samples=sample_num)\n\n        # test when :attr:`transform` is False\n        # sample_test_no_transform = gauss_connector(gauss_ds, transform=False)\n\n        test_list = [output_1, output_2, sample_ts, sample_test_num]\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_list = sess.run(test_list)\n            out1 = out_list[0]\n            out2 = out_list[1]\n            out_ts = out_list[2]\n            out_test_num = out_list[3]\n\n            # check the same size\n            self.assertEqual(out_test_num[0].shape,\n                             tf.TensorShape([sample_num, state_size[0]]))\n            self.assertEqual(out1[0].shape,\n                             tf.TensorShape([self._batch_size, state_size[0]]))\n            self.assertEqual(out2[0].shape,\n                             tf.TensorShape([self._batch_size, state_size[0]]))\n            _assert_same_size(out_ts, state_size_ts)\n\n            # sample_mu = np.mean(sample_outputs, axis=0)\n            # # pylint: disable=no-member\n            # sample_var = np.var(sample_outputs, axis=0)\n\n            # check if the value is approximated N(0, 1)\n            # for i in range(variable_size):\n            #     self.assertAlmostEqual(0, sample_mu[i], delta=0.2)\n            #     self.assertAlmostEqual(1, sample_var[i], delta=0.2)\n\n    def test_stochastic_connector(self):\n        """"""Tests the logic of\n        :class:`~texar.tf.modules.StochasticConnector`.\n        """"""\n        state_size = (10, 10)\n        variable_size = 100\n        state_size_ts = tf.TensorShape([self._batch_size, variable_size])\n        gauss_connector = StochasticConnector(state_size)\n        mu = tf.zeros([self._batch_size, variable_size])\n        var = tf.ones([self._batch_size, variable_size])\n        gauss_ds = tfpd.MultivariateNormalDiag(loc=mu, scale_diag=var)\n        output_1, _ = gauss_connector(gauss_ds)\n\n        gauss_connector_2 = StochasticConnector(state_size_ts)\n        output_2, sample2 = gauss_connector_2(\n            distribution=""MultivariateNormalDiag"",\n            distribution_kwargs={""loc"": mu, ""scale_diag"": var}, transform=False)\n        test_list = [output_1, output_2, sample2]\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            out_list = sess.run(test_list)\n            out1 = out_list[0]\n            out2 = out_list[1]\n            sample2 = out_list[2]\n            self.assertEqual(out1[0].shape,\n                             tf.TensorShape([self._batch_size, state_size[0]]))\n            self.assertEqual(out2.shape, state_size_ts)\n            self.assertEqual(out2.shape, sample2.shape)\n\n    # def test_concat_connector(self): # pylint: disable=too-many-locals\n    #    """"""Tests the logic of\n    #    :class:`~texar.tf.modules.connectors.ConcatConnector`.\n    #    """"""\n    #    gauss_size = 5\n    #    constant_size = 7\n    #    variable_size = 13\n\n    #    decoder_size1 = 16\n    #    decoder_size2 = (16, 32)\n\n    #    gauss_connector = StochasticConnector(gauss_size)\n    #    categorical_connector = StochasticConnector(1)\n    #    constant_connector = ConstantConnector(constant_size)\n    #    concat_connector1 = ConcatConnector(decoder_size1)\n    #    concat_connector2 = ConcatConnector(decoder_size2)\n\n    #    # pylint: disable=invalid-name\n    #    mu = tf.zeros([self._batch_size, gauss_size])\n    #    var = tf.ones([self._batch_size, gauss_size])\n    #    categorical_prob = tf.constant(\n    #       [[0.1, 0.2, 0.7] for _ in xrange(self._batch_size)])\n    #    categorical_ds = tfds.Categorical(probs = categorical_prob)\n    #    gauss_ds = tfds.MultivariateNormalDiag(loc = mu, scale_diag = var)\n\n    #    gauss_state = gauss_connector(gauss_ds)\n    #    categorical_state = categorical_connector(categorical_ds)\n    #    constant_state = constant_connector(self._batch_size, value=1.)\n    #    with tf.Session() as debug_sess:\n    #        debug_cater = debug_sess.run(categorical_state)\n\n    #    state1 = concat_connector1(\n    #       [gauss_state, categorical_state, constant_state])\n    #    state2 = concat_connector2(\n    #       [gauss_state, categorical_state, constant_state])\n\n    #    with self.test_session() as sess:\n    #        sess.run(tf.global_variables_initializer())\n    #        [output1, output2] = sess.run([state1, state2])\n\n    #        # check the same size\n    #        self.assertEqual(output1.shape[1], decoder_size1)\n    #        self.assertEqual(output2[1].shape[1], decoder_size2[1])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/decoders/beam_search_decode_test.py,22,"b'""""""\nUnit tests for beam search decoding.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import dynamic_decode\nfrom tensorflow.contrib.seq2seq import BeamSearchDecoder, tile_batch\n\nimport texar.tf as tx\nfrom texar.tf.modules.decoders.beam_search_decode import beam_search_decode\nfrom texar.tf import context\n\n# pylint: disable=no-member, too-many-instance-attributes, invalid-name\n# pylint: disable=too-many-locals, too-many-arguments\n\n\nclass BeamSearchDecodeTest(tf.test.TestCase):\n    """"""Tests\n    :func:`texar.tf.modules.decoders.beam_search_decode.beam_search_decode`.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._vocab_size = 10\n        self._max_time = 16\n        self._batch_size = 8\n        self._emb_dim = 20\n        self._cell_dim = 256\n        self._attention_dim = self._cell_dim\n        self._beam_width = 11\n        self._inputs = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1., dtype=tf.float32)\n        self._embedding = tf.random_uniform(\n            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)\n        self._encoder_output = tf.random_uniform(\n            [self._batch_size, self._max_time, 64])\n\n    def _test_beam_search(\n            self, decoder, initial_state=None, tiled_initial_state=None,\n            tf_initial_state=None, beam_width_1=1, initiated=False):\n        # Compare with tf built-in BeamSearchDecoder\n        outputs, final_state, _ = beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            beam_width=beam_width_1,\n            max_decoding_length=20)\n\n        self.assertIsInstance(\n            outputs, tf.contrib.seq2seq.FinalBeamSearchDecoderOutput)\n        self.assertIsInstance(\n            final_state, tf.contrib.seq2seq.BeamSearchDecoderState)\n\n        num_trainable_variables = len(tf.trainable_variables())\n        _ = decoder(\n            decoding_strategy=\'infer_greedy\',\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            max_decoding_length=20)\n        self.assertEqual(num_trainable_variables, len(tf.trainable_variables()))\n\n        if tf_initial_state is None:\n            tf_initial_state = decoder.cell.zero_state(\n                self._batch_size * beam_width_1, tf.float32)\n        beam_decoder = BeamSearchDecoder(\n            cell=decoder.cell,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            initial_state=tf_initial_state,\n            beam_width=beam_width_1,\n            output_layer=decoder.output_layer)\n\n        outputs_1, final_state_1, _ = dynamic_decode(\n            decoder=beam_decoder, maximum_iterations=20)\n\n        # Tests time major\n        outputs_2, _, _ = beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            beam_width=self._beam_width,\n            initial_state=initial_state,\n            tiled_initial_state=tiled_initial_state,\n            max_decoding_length=21)\n        outputs_3, _, _ = beam_search_decode(\n            decoder_or_cell=decoder,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            beam_width=self._beam_width,\n            initial_state=initial_state,\n            tiled_initial_state=tiled_initial_state,\n            max_decoding_length=21,\n            output_time_major=True)\n\n        with self.test_session() as sess:\n            if not initiated:\n                sess.run(tf.global_variables_initializer())\n\n            outputs_, final_state_, outputs_1_, final_state_1_ = sess.run(\n                [outputs, final_state, outputs_1, final_state_1],\n                feed_dict={context.global_mode():\n                           tf.estimator.ModeKeys.PREDICT})\n\n            np.testing.assert_array_equal(\n                outputs_.predicted_ids, outputs_1_.predicted_ids)\n            np.testing.assert_array_equal(\n                outputs_.beam_search_decoder_output.scores,\n                outputs_1_.beam_search_decoder_output.scores)\n            np.testing.assert_array_equal(\n                outputs_.beam_search_decoder_output.predicted_ids,\n                outputs_1_.beam_search_decoder_output.predicted_ids)\n            np.testing.assert_array_equal(\n                outputs_.beam_search_decoder_output.parent_ids,\n                outputs_1_.beam_search_decoder_output.parent_ids)\n            np.testing.assert_array_equal(\n                final_state_.log_probs, final_state_1_.log_probs)\n            np.testing.assert_array_equal(\n                final_state_.lengths, final_state_1_.lengths)\n\n            outputs_2_, outputs_3_ = sess.run(\n                [outputs_2, outputs_3],\n                feed_dict={context.global_mode():\n                           tf.estimator.ModeKeys.PREDICT})\n            self.assertEqual(outputs_2_.predicted_ids.shape,\n                             tuple([self._batch_size, 21, 11]))\n            self.assertEqual(outputs_3_.predicted_ids.shape,\n                             tuple([21, self._batch_size, 11]))\n\n    def test_basic_rnn_decoder_beam_search(self):\n        """"""Tests beam search with BasicRNNDecoder.\n        """"""\n        hparams = {\n            ""rnn_cell"": {\n                ""kwargs"": {""num_units"": self._cell_dim}\n            }\n        }\n        decoder = tx.modules.BasicRNNDecoder(\n            vocab_size=self._vocab_size,\n            hparams=hparams)\n\n        self._test_beam_search(decoder)\n\n        self._test_beam_search(\n            decoder, beam_width_1=self._beam_width, initiated=True)\n\n    def test_basic_rnn_decoder_given_initial_state(self):\n        """"""Tests beam search with BasicRNNDecoder given initial state.\n        """"""\n        hparams = {\n            ""rnn_cell"": {\n                ""kwargs"": {""num_units"": self._cell_dim}\n            }\n        }\n        decoder = tx.modules.BasicRNNDecoder(\n            vocab_size=self._vocab_size,\n            hparams=hparams)\n\n        # (zhiting): The beam search decoder does not generate max-length\n        # samples if only one cell_state is created. Perhaps due to\n        # random seed or bugs?\n        cell_state = decoder.cell.zero_state(self._batch_size, tf.float32)\n\n        self._test_beam_search(decoder, initial_state=cell_state)\n\n        tiled_cell_state = tile_batch(cell_state, multiplier=self._beam_width)\n        self._test_beam_search(\n            decoder, tiled_initial_state=tiled_cell_state, initiated=True)\n\n    def test_attention_decoder_beam_search(self):\n        """"""Tests beam search with RNNAttentionDecoder.\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = tf.constant(seq_length)\n        hparams = {\n            ""attention"": {\n                ""kwargs"": {""num_units"": self._attention_dim}\n            },\n            ""rnn_cell"": {\n                ""kwargs"": {""num_units"": self._cell_dim}\n            }\n        }\n        decoder = tx.modules.AttentionRNNDecoder(\n            vocab_size=self._vocab_size,\n            memory=self._encoder_output,\n            memory_sequence_length=encoder_values_length,\n            hparams=hparams)\n\n        self._test_beam_search(decoder)\n\n    def test_attention_decoder_given_initial_state(self):\n        """"""Tests beam search with RNNAttentionDecoder given initial state.\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = tf.constant(seq_length)\n        hparams = {\n            ""attention"": {\n                ""kwargs"": {""num_units"": self._attention_dim}\n            },\n            ""rnn_cell"": {\n                ""kwargs"": {""num_units"": self._cell_dim}\n            }\n        }\n        decoder = tx.modules.AttentionRNNDecoder(\n            vocab_size=self._vocab_size,\n            memory=self._encoder_output,\n            memory_sequence_length=encoder_values_length,\n            hparams=hparams)\n\n        state = decoder.cell.zero_state(self._batch_size, tf.float32)\n\n        cell_state = state.cell_state\n        self._test_beam_search(decoder, initial_state=cell_state)\n\n        tiled_cell_state = tile_batch(cell_state, multiplier=self._beam_width)\n        self._test_beam_search(\n            decoder, tiled_initial_state=tiled_cell_state, initiated=True)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/decoders/gpt2_decoder_test.py,12,"b'""""""\nUnit tests for GPT2 decoder.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.gpt2_decoder import GPT2Decoder\nfrom texar.tf.modules.decoders.transformer_decoders import \\\n    TransformerDecoderOutput\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass GPT2DecoderTest(tf.test.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.GPT2Decoder`\n    """"""\n\n    @pretrained_test\n    def test_hparams(self):\n        r""""""Tests the priority of the decoder arch parameters.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[2, 3])\n\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        decoder = GPT2Decoder(pretrained_model_name=""gpt2-small"",\n                              hparams=hparams)\n        _ = decoder(inputs=inputs)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 12)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-small"",\n            ""decoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        _ = decoder(inputs=inputs)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 12)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""decoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        _ = decoder(inputs=inputs)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 6)\n\n        # case 4: using default hparams\n        decoder = GPT2Decoder()\n        _ = decoder(inputs=inputs)\n        self.assertEqual(decoder.hparams.decoder.num_blocks, 12)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[2, 3])\n\n        def get_variable_num(n_layers: int) -> int:\n            return 1 + 1 + n_layers * 16 + 2\n\n        # case 1: GPT2 small\n        decoder = GPT2Decoder()\n        _ = decoder(inputs=inputs)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(12))\n\n        # case 2: GPT2 medium\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        _ = decoder(inputs=inputs)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(24))\n\n        # case 2: GPT2 large\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-large"",\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        _ = decoder(inputs=inputs)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(36))\n\n        # case 3: self-designed GPT2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""decoder"": {\n                ""num_blocks"": 6,\n            }\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n        _ = decoder(inputs=inputs)\n        self.assertEqual(len(decoder.trainable_variables), get_variable_num(6))\n\n    def test_decode_train(self):\n        r""""""Tests train_greedy.\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=50257, dtype=tf.int32)\n        outputs = decoder(inputs=inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.logits.shape, (batch_size,\n                                                     max_time, 50257))\n            self.assertEqual(outputs_.sample_id.shape, (batch_size, max_time))\n\n    def test_decode_infer_greedy(self):\n        r""""""Tests infer_greedy\n        """"""\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        decoder = GPT2Decoder(hparams=hparams)\n\n        start_tokens = tf.fill([16], 1)\n        end_token = 2\n        outputs, length = decoder(max_decoding_length=4,\n                                  start_tokens=start_tokens,\n                                  end_token=end_token,\n                                  decoding_strategy=""infer_greedy"")\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertIsInstance(outputs_, TransformerDecoderOutput)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/decoders/rnn_decoder_helpers_test.py,25,"b'""""""\nUnit tests for decoder helpers.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.rnn_decoder_helpers import \\\n        SoftmaxEmbeddingHelper, GumbelSoftmaxEmbeddingHelper\nfrom texar.tf.modules.decoders.tf_helpers import GreedyEmbeddingHelper\nfrom texar.tf.modules.decoders.rnn_decoders import BasicRNNDecoder\nfrom texar.tf.modules.embedders.embedders import WordEmbedder\nfrom texar.tf.modules.embedders.position_embedders import PositionEmbedder\n\n# pylint: disable=no-member, too-many-locals, too-many-instance-attributes\n# pylint: disable=too-many-arguments, protected-access, redefined-variable-type\n\n\nclass HelpersTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.decoders.rnn_decoders.BasicRNNDecoder`.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._batch_size = 16\n        self._vocab_size = 4\n        self._start_tokens = [self._vocab_size - 2] * self._batch_size\n        self._end_token = self._vocab_size - 1\n        self._max_time = 8\n        self._emb_dim = 100\n        self._inputs = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1., dtype=tf.float32)\n        self._embedding = tf.random_uniform(\n            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)\n        self._max_seq_length = 10\n\n    def test_softmax_embedding_helpers(self):\n        """"""Tests softmax helpers.\n        """"""\n\n        def _test_fn(helper):\n            _, next_inputs, _ = helper.next_inputs(\n                time=1,\n                outputs=tf.ones([self._batch_size, self._vocab_size]),\n                state=None,\n                sample_ids=tf.ones([self._batch_size, self._vocab_size]))\n\n            self.assertEqual(helper.sample_ids_shape,\n                             tf.TensorShape(self._vocab_size))\n            self.assertEqual(next_inputs.get_shape(),\n                             tf.TensorShape([self._batch_size, self._emb_dim]))\n\n            # Test in an RNN decoder\n            output_layer = tf.layers.Dense(self._vocab_size)\n            decoder = BasicRNNDecoder(vocab_size=self._vocab_size,\n                                      output_layer=output_layer)\n            outputs, final_state, sequence_lengths = decoder(\n                helper=helper, max_decoding_length=self._max_seq_length)\n\n            cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n            with self.test_session() as sess:\n                sess.run(tf.global_variables_initializer())\n                outputs_, final_state_, sequence_lengths_ = sess.run(\n                    [outputs, final_state, sequence_lengths])\n                max_length = max(sequence_lengths_)\n                self.assertEqual(\n                    outputs_.logits.shape,\n                    (self._batch_size, max_length, self._vocab_size))\n                self.assertEqual(\n                    outputs_.sample_id.shape,\n                    (self._batch_size, max_length, self._vocab_size))\n                self.assertEqual(final_state_[0].shape,\n                                 (self._batch_size, cell_dim))\n\n        # SoftmaxEmbeddingHelper\n\n        # case-(1)\n        helper = SoftmaxEmbeddingHelper(\n            self._embedding, self._start_tokens, self._end_token, 0.7)\n        _test_fn(helper)\n\n        # case-(2)\n        embedder = WordEmbedder(self._embedding)\n        helper = SoftmaxEmbeddingHelper(\n            embedder, self._start_tokens, self._end_token, 0.7,\n            embedding_size=self._vocab_size)\n        _test_fn(helper)\n\n        # case-(3)\n        word_embedder = WordEmbedder(self._embedding)\n        pos_embedder = PositionEmbedder(position_size=self._max_seq_length)\n\n        def _emb_fn(soft_ids, times):\n            return word_embedder(soft_ids=soft_ids) + pos_embedder(times)\n        helper = SoftmaxEmbeddingHelper(\n            _emb_fn, self._start_tokens, self._end_token, 0.7,\n            embedding_size=self._vocab_size)\n        _test_fn(helper)\n\n        # GumbelSoftmaxEmbeddingHelper\n\n        # case-(1)\n        helper = GumbelSoftmaxEmbeddingHelper(\n            self._embedding, self._start_tokens, self._end_token, 0.7)\n        _test_fn(helper)\n\n    def test_infer_helpers(self):\n        """"""Tests inference helpers.\n        """"""\n\n        def _test_fn(helper):\n            _, next_inputs, _ = helper.next_inputs(\n                time=1,\n                outputs=tf.ones([self._batch_size, self._vocab_size]),\n                state=None,\n                sample_ids=tf.ones([self._batch_size], dtype=tf.int32))\n\n            self.assertEqual(helper.sample_ids_shape,\n                             tf.TensorShape([]))\n            self.assertEqual(next_inputs.get_shape(),\n                             tf.TensorShape([self._batch_size, self._emb_dim]))\n\n            # Test in an RNN decoder\n            output_layer = tf.layers.Dense(self._vocab_size)\n            decoder = BasicRNNDecoder(vocab_size=self._vocab_size,\n                                      output_layer=output_layer)\n            outputs, final_state, sequence_lengths = decoder(\n                helper=helper, max_decoding_length=self._max_seq_length)\n\n            cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n            with self.test_session() as sess:\n                sess.run(tf.global_variables_initializer())\n                outputs_, final_state_, sequence_lengths_ = sess.run(\n                    [outputs, final_state, sequence_lengths])\n                max_length = max(sequence_lengths_)\n                self.assertEqual(\n                    outputs_.logits.shape,\n                    (self._batch_size, max_length, self._vocab_size))\n                self.assertEqual(\n                    outputs_.sample_id.shape, (self._batch_size, max_length))\n                self.assertEqual(final_state_[0].shape,\n                                 (self._batch_size, cell_dim))\n\n        # case-(1)\n        helper = GreedyEmbeddingHelper(\n            self._embedding, self._start_tokens, self._end_token)\n        _test_fn(helper)\n\n        # case-(2)\n        embedder = WordEmbedder(self._embedding)\n        helper = GreedyEmbeddingHelper(\n            embedder, self._start_tokens, self._end_token)\n        _test_fn(helper)\n\n        # case-(3)\n        word_embedder = WordEmbedder(self._embedding)\n        pos_embedder = PositionEmbedder(position_size=self._max_seq_length)\n\n        def _emb_fn(ids, times):\n            return word_embedder(ids) + pos_embedder(times)\n        helper = GreedyEmbeddingHelper(\n            _emb_fn, self._start_tokens, self._end_token)\n        _test_fn(helper)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/decoders/rnn_decoders_test.py,54,"b'""""""\nUnit tests for RNN decoders.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.rnn_decoders import BasicRNNDecoderOutput\nfrom texar.tf.modules.decoders.rnn_decoders import BasicRNNDecoder\nfrom texar.tf.modules.decoders.rnn_decoders import AttentionRNNDecoderOutput\nfrom texar.tf.modules.decoders.rnn_decoders import AttentionRNNDecoder\nfrom texar.tf.modules.decoders.rnn_decoder_helpers import get_helper\nfrom texar.tf import context\n\n# pylint: disable=no-member, too-many-locals, too-many-instance-attributes\n# pylint: disable=too-many-arguments, protected-access\n\n\nclass BasicRNNDecoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.decoders.rnn_decoders.BasicRNNDecoder`.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._vocab_size = 4\n        self._max_time = 8\n        self._batch_size = 16\n        self._emb_dim = 20\n        self._inputs = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1., dtype=tf.float32)\n        self._embedding = tf.random_uniform(\n            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)\n\n    def _test_outputs(self, decoder, outputs, final_state, sequence_lengths,\n                      test_mode=False):\n        # 4 trainable variables: cell-kernel, cell-bias,\n        # fc-layer-weights, fc-layer-bias\n        self.assertEqual(len(decoder.trainable_variables), 4)\n\n        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            outputs_, final_state_, sequence_lengths_ = sess.run(\n                [outputs, final_state, sequence_lengths],\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN})\n            self.assertIsInstance(outputs_, BasicRNNDecoderOutput)\n            if not test_mode:\n                self.assertEqual(\n                    outputs_.logits.shape,\n                    (self._batch_size, self._max_time, self._vocab_size))\n                self.assertEqual(\n                    outputs_.sample_id.shape,\n                    (self._batch_size, self._max_time))\n                np.testing.assert_array_equal(\n                    sequence_lengths_, [self._max_time] * self._batch_size)\n            self.assertEqual(final_state_[0].shape,\n                             (self._batch_size, cell_dim))\n\n    def test_output_layer(self):\n        decoder = BasicRNNDecoder(vocab_size=self._vocab_size,\n                                  output_layer=None)\n        self.assertIsInstance(decoder, BasicRNNDecoder)\n\n        decoder = BasicRNNDecoder(output_layer=tf.identity)\n        self.assertIsInstance(decoder, BasicRNNDecoder)\n\n        tensor = tf.random_uniform(\n            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32\n        )\n        decoder = BasicRNNDecoder(output_layer=tensor)\n        self.assertIsInstance(decoder, BasicRNNDecoder)\n        self.assertEqual(decoder.vocab_size, self._vocab_size)\n\n    def test_decode_train(self):\n        """"""Tests decoding in training mode.\n        """"""\n        output_layer = tf.layers.Dense(self._vocab_size)\n        decoder = BasicRNNDecoder(vocab_size=self._vocab_size,\n                                  output_layer=output_layer)\n\n        helper_train = get_helper(\n            decoder.hparams.helper_train.type,\n            inputs=self._inputs,\n            sequence_length=[self._max_time] * self._batch_size,\n            **decoder.hparams.helper_train.kwargs.todict())\n        outputs, final_state, sequence_lengths = decoder(helper=helper_train)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n        outputs, final_state, sequence_lengths = decoder(\n            inputs=self._inputs,\n            sequence_length=[self._max_time] * self._batch_size)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n        outputs, final_state, sequence_lengths = decoder(\n            decoding_strategy=None,\n            inputs=self._inputs,\n            sequence_length=[self._max_time] * self._batch_size)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths)\n\n        outputs, final_state, sequence_lengths = decoder(\n            decoding_strategy=None,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            mode=tf.estimator.ModeKeys.EVAL)\n        self._test_outputs(decoder, outputs, final_state, sequence_lengths,\n                           test_mode=True)\n\n    def test_decode_train_with_tf(self):\n        """"""Compares decoding results with TF built-in decoder.\n        """"""\n        _inputs_placeholder = tf.placeholder(\n            tf.int32, [self._batch_size, self._max_time], name=""inputs"")\n        _embedding_placeholder = tf.placeholder(\n            tf.float32, [self._vocab_size, self._emb_dim], name=""emb"")\n        inputs = tf.nn.embedding_lookup(_embedding_placeholder,\n                                        _inputs_placeholder)\n\n        output_layer = tf.layers.Dense(self._vocab_size)\n        decoder = BasicRNNDecoder(vocab_size=self._vocab_size,\n                                  output_layer=output_layer)\n\n        helper_train = get_helper(\n            decoder.hparams.helper_train.type,\n            inputs=inputs,\n            sequence_length=[self._max_time] * self._batch_size,\n            **decoder.hparams.helper_train.kwargs.todict())\n\n        outputs, final_state, sequence_lengths = decoder(helper=helper_train)\n\n        tf_helper = tf.contrib.seq2seq.TrainingHelper(\n            inputs, [self._max_time] * self._batch_size)\n\n        tf_decoder = tf.contrib.seq2seq.BasicDecoder(\n            decoder.cell,\n            tf_helper,\n            decoder.cell.zero_state(self._batch_size, tf.float32),\n            output_layer=output_layer)\n\n        tf_outputs, tf_final_state, tf_sequence_lengths = \\\n            tf.contrib.seq2seq.dynamic_decode(tf_decoder)\n\n        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            inputs_ = np.random.randint(\n                self._vocab_size, size=(self._batch_size, self._max_time),\n                dtype=np.int32)\n            embedding_ = np.random.randn(self._vocab_size, self._emb_dim)\n\n            outputs_, final_state_, sequence_lengths_ = sess.run(\n                [outputs, final_state, sequence_lengths],\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN,\n                           _inputs_placeholder: inputs_,\n                           _embedding_placeholder: embedding_})\n            self.assertEqual(final_state_[0].shape,\n                             (self._batch_size, cell_dim))\n\n            tf_outputs_, tf_final_state_, tf_sequence_lengths_ = sess.run(\n                [tf_outputs, tf_final_state, tf_sequence_lengths],\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN,\n                           _inputs_placeholder: inputs_,\n                           _embedding_placeholder: embedding_})\n\n            np.testing.assert_array_equal(outputs_.logits,\n                                          tf_outputs_.rnn_output)\n            np.testing.assert_array_equal(outputs_.sample_id,\n                                          tf_outputs_.sample_id)\n            np.testing.assert_array_equal(final_state_.c, tf_final_state_.c)\n            np.testing.assert_array_equal(final_state_.h, tf_final_state_.h)\n            np.testing.assert_array_equal(sequence_lengths_,\n                                          tf_sequence_lengths_)\n\n    def test_decode_infer(self):\n        """"""Tests decoding in inferencee mode.\n        """"""\n        output_layer = tf.layers.Dense(self._vocab_size)\n        decoder = BasicRNNDecoder(vocab_size=self._vocab_size,\n                                  output_layer=output_layer)\n\n        helper_infer = get_helper(\n            decoder.hparams.helper_infer.type,\n            embedding=self._embedding,\n            start_tokens=[self._vocab_size - 2] * self._batch_size,\n            end_token=self._vocab_size - 1,\n            **decoder.hparams.helper_train.kwargs.todict())\n\n        outputs, final_state, sequence_lengths = decoder(helper=helper_infer)\n\n        # 4 trainable variables: embedding, cell-kernel, cell-bias,\n        # fc-layer-weights, fc-layer-bias\n        self.assertEqual(len(decoder.trainable_variables), 4)\n\n        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, final_state_, sequence_lengths_ = sess.run(\n                [outputs, final_state, sequence_lengths],\n                feed_dict={context.global_mode():\n                           tf.estimator.ModeKeys.PREDICT})\n            self.assertIsInstance(outputs_, BasicRNNDecoderOutput)\n            max_length = max(sequence_lengths_)\n            self.assertEqual(\n                outputs_.logits.shape,\n                (self._batch_size, max_length, self._vocab_size))\n            self.assertEqual(\n                outputs_.sample_id.shape, (self._batch_size, max_length))\n            self.assertEqual(final_state_[0].shape,\n                             (self._batch_size, cell_dim))\n\n\nclass AttentionRNNDecoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.decoders.rnn_decoders.AttentionRNNDecoder`.\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._vocab_size = 10\n        self._max_time = 16\n        self._batch_size = 8\n        self._emb_dim = 20\n        self._attention_dim = 256\n        self._inputs = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1., dtype=tf.float32)\n        self._embedding = tf.random_uniform(\n            [self._vocab_size, self._emb_dim], maxval=1., dtype=tf.float32)\n        self._encoder_output = tf.random_uniform(\n            [self._batch_size, self._max_time, 64])\n\n    def test_decode_train(self):\n        """"""Tests decoding in training mode.\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = tf.constant(seq_length)\n        hparams = {\n            ""attention"": {\n                ""kwargs"": {\n                    ""num_units"": self._attention_dim,\n                    # Note: to use sparsemax in TF-CPU, it looks\n                    # `memory_sequence_length` must equal max_time.\n                    # ""probability_fn"": ""sparsemax""\n                }\n            }\n        }\n        decoder = AttentionRNNDecoder(\n            memory=self._encoder_output,\n            memory_sequence_length=encoder_values_length,\n            vocab_size=self._vocab_size,\n            hparams=hparams)\n\n        helper_train = get_helper(\n            decoder.hparams.helper_train.type,\n            inputs=self._inputs,\n            sequence_length=[self._max_time] * self._batch_size,\n            **decoder.hparams.helper_train.kwargs.todict())\n\n        outputs, final_state, sequence_lengths = decoder(helper=helper_train)\n        # 4+1 trainable variables: cell-kernel, cell-bias,\n        # fc-weight, fc-bias, and\n        # memory_layer: For LuongAttention, we only transform the memory layer;\n        # thus num_units *must* match the expected query depth.\n        self.assertEqual(len(decoder.trainable_variables), 5)\n\n        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, final_state_, sequence_lengths_ = sess.run(\n                [outputs, final_state, sequence_lengths],\n                feed_dict={context.global_mode(): tf.estimator.ModeKeys.TRAIN})\n            self.assertIsInstance(outputs_, AttentionRNNDecoderOutput)\n            self.assertEqual(\n                outputs_.logits.shape,\n                (self._batch_size, self._max_time, self._vocab_size))\n            self.assertEqual(\n                outputs_.sample_id.shape, (self._batch_size, self._max_time))\n            self.assertEqual(final_state_.cell_state[0].shape,\n                             (self._batch_size, cell_dim))\n            np.testing.assert_array_equal(\n                sequence_lengths_, [self._max_time] * self._batch_size)\n\n    def test_decode_infer(self):\n        """"""Tests decoding in inference mode.\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = tf.constant(seq_length)\n        hparams = {\n            ""attention"": {\n                ""kwargs"": {\n                    ""num_units"": 256,\n                }\n            }\n        }\n        decoder = AttentionRNNDecoder(\n            vocab_size=self._vocab_size,\n            memory=self._encoder_output,\n            memory_sequence_length=encoder_values_length,\n            hparams=hparams)\n\n        helper_infer = get_helper(\n            decoder.hparams.helper_infer.type,\n            embedding=self._embedding,\n            start_tokens=[1] * self._batch_size,\n            end_token=2,\n            **decoder.hparams.helper_train.kwargs.todict())\n\n        outputs, final_state, sequence_lengths = decoder(helper=helper_infer)\n\n        # 4+1 trainable variables: cell-kernel, cell-bias,\n        # fc-weight, fc-bias, and\n        # memory_layer: For LuongAttention, we only transform the memory layer;\n        # thus num_units *must* match the expected query depth.\n        self.assertEqual(len(decoder.trainable_variables), 5)\n        cell_dim = decoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, final_state_, sequence_lengths_ = sess.run(\n                [outputs, final_state, sequence_lengths],\n                feed_dict={context.global_mode():\n                           tf.estimator.ModeKeys.PREDICT})\n            self.assertIsInstance(outputs_, AttentionRNNDecoderOutput)\n            max_length = max(sequence_lengths_)\n            self.assertEqual(\n                outputs_.logits.shape,\n                (self._batch_size, max_length, self._vocab_size))\n            self.assertEqual(\n                outputs_.sample_id.shape, (self._batch_size, max_length))\n            self.assertEqual(final_state_.cell_state[0].shape,\n                             (self._batch_size, cell_dim))\n\n    def test_beam_search_cell(self):\n        """"""Tests :meth:`texar.tf.modules.AttentionRNNDecoder._get_beam_search_cell`\n        """"""\n        seq_length = np.random.randint(\n            self._max_time, size=[self._batch_size]) + 1\n        encoder_values_length = tf.constant(seq_length)\n        hparams = {\n            ""attention"": {\n                ""kwargs"": {\n                    ""num_units"": self._attention_dim,\n                    ""probability_fn"": ""sparsemax""\n                }\n            }\n        }\n        decoder = AttentionRNNDecoder(\n            memory=self._encoder_output,\n            memory_sequence_length=encoder_values_length,\n            vocab_size=self._vocab_size,\n            hparams=hparams)\n\n        helper_train = get_helper(\n            decoder.hparams.helper_train.type,\n            inputs=self._inputs,\n            sequence_length=[self._max_time] * self._batch_size,\n            **decoder.hparams.helper_train.kwargs.todict())\n\n        _, _, _ = decoder(helper=helper_train)\n\n        # 4+1 trainable variables: cell-kernel, cell-bias,\n        # fc-weight, fc-bias, and\n        # memory_layer: For LuongAttention, we only transform the memory layer;\n        # thus num_units *must* match the expected query depth.\n        self.assertEqual(len(decoder.trainable_variables), 5)\n\n        beam_width = 3\n        beam_cell = decoder._get_beam_search_cell(beam_width)\n        cell_input = tf.random_uniform([self._batch_size * beam_width,\n                                        self._emb_dim])\n        cell_state = beam_cell.zero_state(self._batch_size * beam_width,\n                                          tf.float32)\n        _ = beam_cell(cell_input, cell_state)\n        # Test if beam_cell is sharing variables with decoder cell.\n        for tvar in beam_cell.trainable_variables:\n            self.assertTrue(tvar in decoder.trainable_variables)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/decoders/transformer_decoders_test.py,40,"b'#\n""""""\nUnit tests for Transformer decodre.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.transformer_decoders import TransformerDecoder\nfrom texar.tf.modules.decoders.transformer_decoders import TransformerDecoderOutput\nfrom texar.tf.modules.decoders import tf_helpers as tx_helper\n\n# pylint: disable=too-many-instance-attributes\n\n\nclass TransformerDecoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.TransformerDecoder`\n    """"""\n\n    def setUp(self):\n        tf.test.TestCase.setUp(self)\n        self._vocab_size = 15\n        self._batch_size = 2\n        self._max_time = 10\n        self._emb_dim = 512\n        self._max_decode_len = 16\n        self._inputs = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1, dtype=tf.float32)\n\n        self._memory = tf.random_uniform(\n            [self._batch_size, self._max_time, self._emb_dim],\n            maxval=1, dtype=tf.float32)\n        self._memory_sequence_length = tf.random_uniform(\n            [self._batch_size], maxval=self._max_time, dtype=tf.int32)\n\n        self._embedding = tf.random_uniform(\n            [self._vocab_size, self._emb_dim], maxval=1, dtype=tf.float32)\n        self._pos_embedding = tf.random_uniform(\n            [self._max_decode_len, self._emb_dim], maxval=1, dtype=tf.float32)\n\n        def _embedding_fn(x, y):\n            x_emb = tf.nn.embedding_lookup(self._embedding, x)\n            y_emb = tf.nn.embedding_lookup(self._pos_embedding, y)\n            return x_emb * self._emb_dim ** 0.5 + y_emb\n        self._embedding_fn = _embedding_fn\n\n        self._output_layer = tf.random_uniform(\n            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32)\n\n        self._start_tokens = tf.fill([self._batch_size], 1)\n        self._end_token = 2\n        self.max_decoding_length = self._max_time\n\n        _context = [[3, 4, 5, 2, 0], [4, 3, 5, 7, 2]]\n        _context_length = [4, 5]\n        self._context = tf.Variable(_context)\n        self._context_length = tf.Variable(_context_length)\n\n    def test_output_layer(self):\n        decoder = TransformerDecoder(vocab_size=self._vocab_size,\n                                     output_layer=None)\n        self.assertIsInstance(decoder, TransformerDecoder)\n\n        decoder = TransformerDecoder(output_layer=tf.identity)\n        self.assertIsInstance(decoder, TransformerDecoder)\n\n        tensor = tf.random_uniform(\n            [self._emb_dim, self._vocab_size], maxval=1, dtype=tf.float32\n        )\n        decoder = TransformerDecoder(output_layer=tensor)\n        self.assertIsInstance(decoder, TransformerDecoder)\n        self.assertEqual(decoder.vocab_size, self._vocab_size)\n\n    def test_decode_train(self):\n        """"""Tests train_greedy\n        """"""\n        decoder = TransformerDecoder(\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer\n        )\n        # 6 blocks\n        # -self multihead_attention: 4 dense without bias + 2 layer norm vars\n        # -encdec multihead_attention: 4 dense without bias + 2 layer norm vars\n        # -poswise_network: Dense with bias, Dense with bias + 2 layer norm vars\n        # 2 layer norm vars\n        outputs = decoder(memory=self._memory,\n                          memory_sequence_length=self._memory_sequence_length,\n                          memory_attention_bias=None,\n                          inputs=self._inputs,\n                          decoding_strategy=\'train_greedy\',\n                          mode=tf.estimator.ModeKeys.TRAIN)\n        self.assertEqual(len(decoder.trainable_variables), 110)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n\n            self.assertIsInstance(outputs_, TransformerDecoderOutput)\n\n    def test_decode_infer_greedy(self):\n        """"""Tests train_greedy\n        """"""\n        decoder = TransformerDecoder(\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer\n        )\n        helper = tx_helper.GreedyEmbeddingHelper(\n            self._embedding_fn, self._start_tokens, self._end_token)\n\n        outputs, length = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            inputs=None,\n            helper=helper,\n            max_decoding_length=self._max_decode_len,\n            mode=tf.estimator.ModeKeys.PREDICT)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertIsInstance(outputs_, TransformerDecoderOutput)\n\n    def test_infer_greedy_with_context_without_memory(self):\n        """"""Tests train_greedy with context\n        """"""\n        decoder = TransformerDecoder(\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer\n        )\n        helper = tx_helper.GreedyEmbeddingHelper(\n            self._embedding_fn, self._start_tokens, self._end_token)\n\n        outputs, length = decoder(\n            memory=None,\n            memory_sequence_length=None,\n            memory_attention_bias=None,\n            inputs=None,\n            decoding_strategy=\'infer_greedy\',\n            helper=helper,\n            context=self._context,\n            context_sequence_length=self._context_length,\n            end_token=self._end_token,\n            max_decoding_length=self._max_decode_len,\n            mode=tf.estimator.ModeKeys.PREDICT)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertIsInstance(outputs_, TransformerDecoderOutput)\n\n    def test_decode_infer_sample(self):\n        """"""Tests infer_sample\n        """"""\n        decoder = TransformerDecoder(\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer\n        )\n        helper = tx_helper.SampleEmbeddingHelper(\n            self._embedding_fn, self._start_tokens, self._end_token)\n\n        outputs, length = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            inputs=None,\n            helper=helper,\n            max_decoding_length=self._max_decode_len,\n            mode=tf.estimator.ModeKeys.PREDICT)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertIsInstance(outputs_, TransformerDecoderOutput)\n\n    def test_beam_search(self):\n        """"""Tests beam_search\n        """"""\n        decoder = TransformerDecoder(\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer\n        )\n\n        outputs = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            inputs=None,\n            embedding=self._embedding_fn,\n            beam_width=5,\n            start_tokens=self._start_tokens,\n            end_token=self._end_token,\n            max_decoding_length=self._max_decode_len,\n            mode=tf.estimator.ModeKeys.PREDICT\n        )\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_[\'log_prob\'].shape,\n                             (self._batch_size, 5))\n            self.assertEqual(outputs_[\'sample_id\'].shape,\n                             (self._batch_size, self._max_decode_len, 5))\n\n    def test_greedy_embedding_helper(self):\n        """"""Tests with tf.contrib.seq2seq.GreedyEmbeddingHelper\n        """"""\n        decoder = TransformerDecoder(\n            vocab_size=self._vocab_size,\n            output_layer=self._output_layer\n        )\n        helper = tx_helper.GreedyEmbeddingHelper(\n            self._embedding, self._start_tokens, self._end_token)\n        outputs, length = decoder(\n            memory=self._memory,\n            memory_sequence_length=self._memory_sequence_length,\n            memory_attention_bias=None,\n            helper=helper,\n            max_decoding_length=self._max_decode_len,\n            mode=tf.estimator.ModeKeys.PREDICT)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertIsInstance(outputs_, TransformerDecoderOutput)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/embedders/embedder_utils_test.py,6,"b'""""""\nUnit tests for embedder utils.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.embedders import embedder_utils\n\n\nclass GetEmbeddingTest(tf.test.TestCase):\n    """"""Tests embedding creator.\n    """"""\n    def test_get_embedding(self):\n        """"""Tests :func:`~texar.tf.modules.embedder.embedder_utils.get_embedding`.\n        """"""\n        vocab_size = 100\n        emb = embedder_utils.get_embedding(num_embeds=vocab_size)\n        self.assertEqual(emb.shape[0].value, vocab_size)\n        self.assertEqual(emb.shape[1].value,\n                         embedder_utils.default_embedding_hparams()[""dim""])\n\n        hparams = {\n            ""initializer"": {\n                ""type"": tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n            },\n            ""regularizer"": {\n                ""type"": tf.keras.regularizers.L1L2(0.1, 0.1)\n            }\n        }\n        emb = embedder_utils.get_embedding(\n            hparams=hparams, num_embeds=vocab_size,\n            variable_scope=\'embedding_2\')\n        self.assertEqual(emb.shape[0].value, vocab_size)\n        self.assertEqual(emb.shape[1].value,\n                         embedder_utils.default_embedding_hparams()[""dim""])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/embedders/embedders_test.py,21,"b'""""""\nUnit tests for embedders.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.embedders.embedders import WordEmbedder\nfrom texar.tf.modules.embedders.position_embedders import PositionEmbedder\nfrom texar.tf.context import global_mode\n\n\nclass EmbedderTest(tf.test.TestCase):\n    """"""Tests parameterized embedder.\n    """"""\n\n    def _test_word_embedder(self, hparams):\n        """"""Tests :class:`texar.tf.modules.WordEmbedder`.\n        """"""\n        embedder = WordEmbedder(\n            vocab_size=100, hparams=hparams)\n\n        inputs = tf.ones([64, 16], dtype=tf.int32)\n        outputs = embedder(inputs)\n\n        inputs_soft = tf.ones([64, 16, embedder.vocab_size], dtype=tf.float32)\n        outputs_soft = embedder(soft_ids=inputs_soft)\n\n        emb_dim = embedder.dim\n        if not isinstance(emb_dim, (list, tuple)):\n            emb_dim = [emb_dim]\n\n        hparams_dim = hparams[""dim""]\n        if not isinstance(hparams[""dim""], (list, tuple)):\n            hparams_dim = [hparams[""dim""]]\n\n        self.assertEqual(outputs.shape, [64, 16] + emb_dim)\n        self.assertEqual(outputs_soft.shape, [64, 16] + emb_dim)\n        self.assertEqual(emb_dim, hparams_dim)\n        self.assertEqual(embedder.vocab_size, 100)\n        self.assertEqual(len(embedder.trainable_variables), 1)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, outputs_soft_ = sess.run(\n                [outputs, outputs_soft],\n                feed_dict={global_mode(): tf.estimator.ModeKeys.TRAIN})\n            self.assertEqual(outputs_.shape, (64, 16) + tuple(emb_dim))\n            self.assertEqual(outputs_soft_.shape, (64, 16) + tuple(emb_dim))\n\n        # Tests unknown input shapes\n        inputs = tf.placeholder(dtype=tf.int64, shape=[None, None])\n        outputs = embedder(inputs)\n        self.assertEqual(len(outputs.get_shape()), 2 + len(hparams_dim))\n\n        inputs_soft = tf.placeholder(dtype=tf.int64, shape=[None, None, None])\n        outputs_soft = embedder(soft_ids=inputs_soft)\n        self.assertEqual(len(outputs_soft.get_shape()), 2 + len(hparams_dim))\n\n    def _test_position_embedder(self, hparams):\n        """"""Tests :class:`texar.tf.modules.PositionEmbedder`.\n        """"""\n        pos_size = 100\n        embedder = PositionEmbedder(\n            position_size=pos_size, hparams=hparams)\n        inputs = tf.ones([64, 16], dtype=tf.int32)\n        outputs = embedder(inputs)\n\n        emb_dim = embedder.dim\n        if not isinstance(emb_dim, (list, tuple)):\n            emb_dim = [emb_dim]\n\n        hparams_dim = hparams[""dim""]\n        if not isinstance(hparams[""dim""], (list, tuple)):\n            hparams_dim = [hparams[""dim""]]\n\n        self.assertEqual(outputs.shape, [64, 16] + emb_dim)\n        self.assertEqual(emb_dim, hparams_dim)\n        self.assertEqual(embedder.position_size, 100)\n        self.assertEqual(len(embedder.trainable_variables), 1)\n\n        seq_length = tf.random_uniform([64], maxval=pos_size, dtype=tf.int32)\n        outputs = embedder(sequence_length=seq_length)\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, max_seq_length = sess.run(\n                [outputs, tf.reduce_max(seq_length)],\n                feed_dict={global_mode(): tf.estimator.ModeKeys.TRAIN})\n            self.assertEqual(outputs_.shape,\n                             (64, max_seq_length) + tuple(emb_dim))\n\n    def test_embedder(self):\n        """"""Tests various embedders.\n        """"""\n        # no dropout\n        hparams = {""dim"": 1024, ""dropout_rate"": 0}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024], ""dropout_rate"": 0}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024, 10], ""dropout_rate"": 0}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        # dropout with default strategy\n        hparams = {""dim"": 1024, ""dropout_rate"": 0.3}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024], ""dropout_rate"": 0.3}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024, 10], ""dropout_rate"": 0.3}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        # dropout with different strategies\n        hparams = {""dim"": 1024, ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item""}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024], ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item""}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024, 10], ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item""}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": 1024, ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item_type""}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024], ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item_type""}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n        hparams = {""dim"": [1024, 10], ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item_type""}\n        self._test_word_embedder(hparams)\n        self._test_position_embedder(hparams)\n\n    def test_embedder_multi_calls(self):\n        """"""Tests embedders called by multiple times.\n        """"""\n        hparams = {""dim"": 1024, ""dropout_rate"": 0.3,\n                   ""dropout_strategy"": ""item""}\n        embedder = WordEmbedder(\n            vocab_size=100, hparams=hparams)\n        inputs = tf.ones([64, 16], dtype=tf.int32)\n        outputs = embedder(inputs)\n\n        emb_dim = embedder.dim\n        if not isinstance(emb_dim, (list, tuple)):\n            emb_dim = [emb_dim]\n        self.assertEqual(outputs.shape, [64, 16] + emb_dim)\n\n        # Call with inputs in a different shape\n        inputs = tf.ones([64, 10, 20], dtype=tf.int32)\n        outputs = embedder(inputs)\n\n        emb_dim = embedder.dim\n        if not isinstance(emb_dim, (list, tuple)):\n            emb_dim = [emb_dim]\n        self.assertEqual(outputs.shape, [64, 10, 20] + emb_dim)\n\n    def test_word_embedder_soft_ids(self):\n        """"""Tests the correctness of using soft ids.\n        """"""\n        init_value = np.expand_dims(np.arange(5), 1)\n        embedder = WordEmbedder(init_value=init_value)\n\n        ids = np.array([3])\n        soft_ids = np.array([[0, 0, 0, 1, 0]])\n\n        outputs = embedder(ids=ids)\n        soft_outputs = embedder(soft_ids=soft_ids)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, soft_outputs_ = sess.run([outputs, soft_outputs])\n            self.assertEqual(outputs_, soft_outputs_)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/encoders/bert_encoder_test.py,14,"b'""""""\nUnit tests for BERT encoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.encoders.bert_encoder import BERTEncoder\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass BERTEncoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.BERTEncoder` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in BERTEncoder.available_checkpoints():\n            encoder = BERTEncoder(pretrained_model_name=pretrained_model_name)\n            _, _ = encoder(inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        """"""Tests the priority of the encoder arch parameter.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""bert-large-uncased"",\n        }\n        encoder = BERTEncoder(pretrained_model_name=""bert-base-uncased"",\n                              hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""bert-large-uncased"",\n            ""encoder"": {\n                ""num_blocks"": 6\n            }\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 24)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6\n            },\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)\n\n        # case 4: using default hparams\n        encoder = BERTEncoder()\n        _, _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1: bert base\n        encoder = BERTEncoder()\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 12 * 16 + 2)\n\n        # case 2: bert large\n        hparams = {\n            ""pretrained_model_name"": ""bert-large-uncased""\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 24 * 16 + 2)\n\n        # case 3: self-designed bert\n        hparams = {\n            ""encoder"": {\n                ""num_blocks"": 6,\n            },\n            ""pretrained_model_name"": None\n        }\n        encoder = BERTEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 3 + 2 + 6 * 16 + 2)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        # case 1: bert base\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        encoder = BERTEncoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        pooled_output_dim = encoder.hparams.hidden_size\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, pooled_output_ = sess.run([outputs, pooled_output])\n            self.assertEqual(outputs_.shape, (batch_size,\n                                              max_time, outputs_dim))\n            self.assertEqual(pooled_output_.shape, (batch_size,\n                                                    pooled_output_dim))\n\n        # case 2: self-designed bert\n        hparams = {\n            ""hidden_size"": 100,\n            ""pretrained_model_name"": None\n        }\n        encoder = BERTEncoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n        outputs, pooled_output = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        pooled_output_dim = encoder.hparams.hidden_size\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, pooled_output_ = sess.run([outputs, pooled_output])\n            self.assertEqual(outputs_.shape, (batch_size,\n                                              max_time, outputs_dim))\n            self.assertEqual(pooled_output_.shape,\n                             (batch_size, pooled_output_dim))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/encoders/conv_encoders_test.py,9,"b'#\n""""""\nUnit tests for conv encoders.\n""""""\n\nimport tensorflow as tf\n\nimport texar.tf as tx\nfrom texar.tf.modules.encoders.conv_encoders import Conv1DEncoder\n\n\nclass Conv1DEncoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.Conv1DEncoder` class.\n    """"""\n\n    def test_encode(self):\n        """"""Tests encode.\n        """"""\n        encoder_1 = Conv1DEncoder()\n        self.assertEqual(len(encoder_1.layers), 4)\n        self.assertTrue(isinstance(encoder_1.layer_by_name(""conv_pool_1""),\n                                   tx.core.MergeLayer))\n        for layer in encoder_1.layers[0].layers:\n            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))\n\n        inputs_1 = tf.ones([64, 16, 300], tf.float32)\n        outputs_1 = encoder_1(inputs_1)\n        self.assertEqual(outputs_1.shape, [64, 128])\n\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""filters"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": {""padding"": ""same""},\n            # Pooling layers\n            ""pooling"": ""AveragePooling"",\n            ""pool_size"": 2,\n            ""pool_strides"": 1,\n            # Dense layers\n            ""num_dense_layers"": 3,\n            ""dense_size"": [128, 128, 10],\n            ""dense_activation"": ""relu"",\n            ""other_dense_kwargs"": {""use_bias"": False},\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n        encoder_2 = Conv1DEncoder(hparams)\n        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten\n        self.assertEqual(len(encoder_2.layers), 1 + 1 + 1 + 3 + 4 + 1)\n        self.assertTrue(isinstance(encoder_2.layer_by_name(""conv_pool_1""),\n                                   tx.core.MergeLayer))\n        for layer in encoder_2.layers[1].layers:\n            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))\n\n        inputs_2 = tf.ones([64, 16, 300], tf.float32)\n        outputs_2 = encoder_2(inputs_2)\n        self.assertEqual(outputs_2.shape, [64, 10])\n\n    def test_unknown_seq_length(self):\n        """"""Tests use of pooling layer when the seq_length dimension of inputs\n        is `None`.\n        """"""\n        encoder_1 = Conv1DEncoder()\n        inputs_1 = tf.placeholder(tf.float32, [64, None, 300])\n        outputs_1 = encoder_1(inputs_1)\n        self.assertEqual(outputs_1.shape, [64, 128])\n\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""filters"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            # Pooling layers\n            ""pooling"": ""AveragePooling"",\n            ""pool_size"": [2, None],\n            # Dense layers\n            ""num_dense_layers"": 1,\n            ""dense_size"": 10,\n        }\n        encoder = Conv1DEncoder(hparams)\n        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten\n        self.assertEqual(len(encoder.layers), 1 + 1 + 1 + 1 + 1 + 1)\n        self.assertTrue(isinstance(encoder.layer_by_name(\'pool_2\'),\n                                   tx.core.AverageReducePooling1D))\n\n        inputs = tf.placeholder(tf.float32, [64, None, 300])\n        outputs = encoder(inputs)\n        self.assertEqual(outputs.shape, [64, 10])\n\n        hparams_2 = {\n            # Conv layers\n            ""num_conv_layers"": 1,\n            ""filters"": 128,\n            ""kernel_size"": 4,\n            ""other_conv_kwargs"": {\'data_format\': \'channels_first\'},\n            # Pooling layers\n            ""pooling"": ""MaxPooling"",\n            ""other_pool_kwargs"": {\'data_format\': \'channels_first\'},\n            # Dense layers\n            ""num_dense_layers"": 1,\n            ""dense_size"": 10,\n        }\n        encoder_2 = Conv1DEncoder(hparams_2)\n        inputs_2 = tf.placeholder(tf.float32, [64, 300, None])\n        outputs_2 = encoder_2(inputs_2)\n        self.assertEqual(outputs_2.shape, [64, 10])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/encoders/gpt2_encoder_test.py,10,"b'""""""\nUnit tests for GPT2 encoder.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.encoders.gpt2_encoder import GPT2Encoder\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass GPT2EncoderTest(tf.test.TestCase):\n    r""""""Tests :class:`~texar.torch.modules.GPT2Encoder` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in GPT2Encoder.available_checkpoints():\n            encoder = GPT2Encoder(pretrained_model_name=pretrained_model_name)\n            _ = encoder(inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        """"""Tests the priority of the encoder arch parameter.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        encoder = GPT2Encoder(pretrained_model_name=""gpt2-small"",\n                              hparams=hparams)\n        _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-small"",\n            ""encoder"": {\n                ""num_blocks"": 6\n            }\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n\n        # case 3: set to None in both hparams and constructor argument\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6\n            },\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 6)\n\n        # case 4: using default hparams\n        encoder = GPT2Encoder()\n        _ = encoder(inputs)\n        self.assertEqual(encoder.hparams.encoder.num_blocks, 12)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        r""""""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        def get_variable_num(n_layers: int) -> int:\n            return 1 + 1 + n_layers * 16 + 2\n\n        # case 1: GPT2 small\n        encoder = GPT2Encoder()\n        _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(12))\n\n        # case 2: GPT2 medium\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-medium"",\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(24))\n\n        # case 3: GPT2 large\n        hparams = {\n            ""pretrained_model_name"": ""gpt2-large"",\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(36))\n\n        # case 4: self-designed GPT2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""encoder"": {\n                ""num_blocks"": 6\n            },\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n        _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), get_variable_num(6))\n\n    def test_encode(self):\n        r""""""Tests encoding.\n        """"""\n        # case 1: GPT2 small\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        encoder = GPT2Encoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n        outputs = encoder(inputs)\n\n        outputs_dim = encoder.hparams.encoder.dim\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape, (batch_size,\n                                              max_time, outputs_dim))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/encoders/hierarchical_encoders_test.py,15,"b'#\n""""""\nUnit tests for RNN encoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.encoders.hierarchical_encoders import HierarchicalRNNEncoder\n\n# pylint: disable=too-many-locals\n\n\nclass HierarchicalRNNEncoderTest(tf.test.TestCase):\n    """"""Tests HierarchicalRNNEncoder\n    """"""\n\n    def test_trainable_variables(self):\n        encoder = HierarchicalRNNEncoder()\n\n        inputs = tf.random_uniform(\n            [3, 2, 3, 4],\n            maxval=1,\n            minval=-1,\n            dtype=tf.float32)\n        _, _ = encoder(inputs)\n\n        self.assertEqual(\n            len(encoder.trainable_variables),\n            len(encoder.encoder_major.trainable_variables) +\n            len(encoder.encoder_minor.trainable_variables))\n\n    def test_encode(self):\n        encoder = HierarchicalRNNEncoder()\n\n        batch_size = 16\n        max_major_time = 8\n        max_minor_time = 6\n        dim = 10\n        inputs = tf.random_uniform(\n            [batch_size, max_major_time, max_minor_time, dim],\n            maxval=1,\n            minval=-1,\n            dtype=tf.float32)\n        outputs, state = encoder(inputs)\n\n        cell_dim = encoder.encoder_major.hparams.rnn_cell.kwargs.num_units\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, state_ = sess.run([outputs, state])\n            self.assertEqual(state_[0].shape, (batch_size, cell_dim))\n\n    def test_order(self):\n        encoder = HierarchicalRNNEncoder()\n\n        batch_size = 16\n        max_major_time = 8\n        max_minor_time = 6\n        dim = 10\n        inputs = tf.random_uniform(\n            [batch_size, max_major_time, max_minor_time, dim],\n            maxval=1,\n            minval=-1,\n            dtype=tf.float32)\n\n        outputs_1, state_1 = encoder(inputs, order=\'btu\')\n        outputs_2, state_2 = encoder(inputs, order=\'utb\')\n        outputs_3, state_3 = encoder(inputs, order=\'tbu\')\n        outputs_4, state_4 = encoder(inputs, order=\'ubt\')\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            sess.run([outputs_1, state_1, outputs_2, state_2,\n                      outputs_3, state_3, outputs_4, state_4])\n\n    def test_depack(self):\n        hparams = {\n            ""encoder_major_type"": ""BidirectionalRNNEncoder"",\n            ""encoder_major_hparams"": {\n                ""rnn_cell_fw"": {\n                    ""type"": ""LSTMCell"",\n                    ""kwargs"": {\n                        ""num_units"": 100\n                    }\n                }\n            }\n        }\n        encoder = HierarchicalRNNEncoder(hparams=hparams)\n\n        batch_size = 16\n        max_major_time = 8\n        max_minor_time = 6\n        dim = 10\n        inputs = tf.random_uniform(\n            [batch_size, max_major_time, max_minor_time, dim],\n            maxval=1,\n            minval=-1,\n            dtype=tf.float32)\n\n        _, _ = encoder(inputs)\n\n        self.assertEqual(\n            encoder.states_minor_before_medium.h.shape[1],\n            encoder.states_minor_after_medium.shape[1])\n\n    def test_encoder_minor_as_birnn(self):\n        """"""Tests encoder_minor as a BidirectionalRNNEncoder\n        """"""\n        hparams = {\n            ""encoder_minor_type"": ""BidirectionalRNNEncoder"",\n            ""encoder_minor_hparams"": {\n                ""rnn_cell_fw"": {\n                    ""type"": ""LSTMCell"",\n                    ""kwargs"": {\n                        ""num_units"": 100\n                    }\n                }\n            },\n            ""encoder_major_hparams"": {\n                ""rnn_cell"": {\n                    ""type"": ""LSTMCell"",\n                    ""kwargs"": {\n                        ""num_units"": 200\n                    }\n                }\n            }\n        }\n        encoder = HierarchicalRNNEncoder(hparams=hparams)\n\n        batch_size = 16\n        max_major_time = 8\n        max_minor_time = 6\n        dim = 10\n        inputs = tf.random_uniform(\n            [batch_size, max_major_time, max_minor_time, dim],\n            maxval=1,\n            minval=-1,\n            dtype=tf.float32)\n\n        outputs, _ = encoder(inputs)\n        self.assertEqual(list(outputs.shape), [16, 8, 200])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/encoders/rnn_encoders_test.py,29,"b'#\n""""""\nUnit tests for RNN encoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.encoders.rnn_encoders import UnidirectionalRNNEncoder\nfrom texar.tf.modules.encoders.rnn_encoders import BidirectionalRNNEncoder\nfrom texar.tf.modules.embedders.embedders import WordEmbedder\n\n# pylint: disable=too-many-locals\n\n\nclass UnidirectionalRNNEncoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.UnidirectionalRNNEncoder` class.\n    """"""\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 100])\n\n        # case 1\n        encoder = UnidirectionalRNNEncoder()\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 2)\n\n        # case 2\n        hparams = {\n            ""rnn_cell"": {\n                ""dropout"": {\n                    ""input_keep_prob"": 0.5\n                }\n            }\n        }\n        encoder = UnidirectionalRNNEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 2)\n\n        # case 3\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""activation"": ""relu"",\n                ""final_layer_activation"": ""identity"",\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": False\n            }\n        }\n        encoder = UnidirectionalRNNEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 2)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 2 + 2 + 2)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        # case 1\n        encoder = UnidirectionalRNNEncoder()\n\n        max_time = 8\n        batch_size = 16\n        emb_dim = 100\n        inputs = tf.random_uniform([batch_size, max_time, emb_dim],\n                                   maxval=1., dtype=tf.float32)\n        outputs, state = encoder(inputs)\n\n        cell_dim = encoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, state_ = sess.run([outputs, state])\n            self.assertEqual(outputs_.shape, (batch_size, max_time, cell_dim))\n            self.assertEqual(state_[0].shape, (batch_size, cell_dim))\n\n        # case 2: with output layers\n        hparams = {\n            ""output_layer"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": True\n            }\n        }\n        encoder = UnidirectionalRNNEncoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 16\n        emb_dim = 100\n        inputs = tf.random_uniform([batch_size, max_time, emb_dim],\n                                   maxval=1., dtype=tf.float32)\n        outputs, state, cell_outputs, output_size = encoder(\n            inputs, return_cell_output=True, return_output_size=True)\n\n        self.assertEqual(output_size[0], 6)\n        self.assertEqual(cell_outputs.shape[-1], encoder.cell.output_size)\n\n        out_dim = encoder.hparams.output_layer.layer_size[-1]\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape, (batch_size, max_time, out_dim))\n\n    def test_encode_with_embedder(self):\n        """"""Tests encoding companioned with :mod:`texar.tf.modules.embedders`.\n        """"""\n        embedder = WordEmbedder(vocab_size=20, hparams={""dim"": 100})\n        inputs = tf.ones([64, 16], dtype=tf.int32)\n\n        encoder = UnidirectionalRNNEncoder()\n        outputs, state = encoder(embedder(inputs))\n\n        cell_dim = encoder.hparams.rnn_cell.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, state_ = sess.run([outputs, state])\n            self.assertEqual(outputs_.shape, (64, 16, cell_dim))\n            self.assertEqual(state_[0].shape, (64, cell_dim))\n\n\nclass BidirectionalRNNEncoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.BidirectionalRNNEncoder` class.\n    """"""\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 100])\n\n        # case 1\n        encoder = BidirectionalRNNEncoder()\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 4)\n\n        # case 2\n        hparams = {\n            ""rnn_cell_fw"": {\n                ""dropout"": {\n                    ""input_keep_prob"": 0.5\n                }\n            }\n        }\n        encoder = BidirectionalRNNEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 4)\n\n        # case 3\n        hparams = {\n            ""output_layer_fw"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""activation"": ""relu"",\n                ""final_layer_activation"": ""identity"",\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": False\n            },\n            ""output_layer_bw"": {\n                ""num_layers"": 3,\n                ""other_dense_kwargs"": {""use_bias"": False}\n            },\n            ""output_layer_share_config"": False\n        }\n        encoder = BidirectionalRNNEncoder(hparams=hparams)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 4 + 4 + 3)\n        _, _ = encoder(inputs)\n        self.assertEqual(len(encoder.trainable_variables), 4 + 4 + 3)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        # case 1\n        encoder = BidirectionalRNNEncoder()\n\n        max_time = 8\n        batch_size = 16\n        emb_dim = 100\n        inputs = tf.random_uniform([batch_size, max_time, emb_dim],\n                                   maxval=1., dtype=tf.float32)\n        outputs, state = encoder(inputs)\n\n        cell_dim = encoder.hparams.rnn_cell_fw.kwargs.num_units\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_, state_ = sess.run([outputs, state])\n            self.assertEqual(outputs_[0].shape,\n                             (batch_size, max_time, cell_dim))\n            self.assertEqual(state_[0][0].shape, (batch_size, cell_dim))\n\n        # case 2: with output layers\n        hparams = {\n            ""output_layer_fw"": {\n                ""num_layers"": 2,\n                ""layer_size"": [100, 6],\n                ""dropout_layer_ids"": [0, 1, 2],\n                ""variational_dropout"": True\n            }\n        }\n        encoder = BidirectionalRNNEncoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 16\n        emb_dim = 100\n        inputs = tf.random_uniform([batch_size, max_time, emb_dim],\n                                   maxval=1., dtype=tf.float32)\n        outputs, state, cell_outputs, output_size = encoder(\n            inputs, return_cell_output=True, return_output_size=True)\n\n        self.assertEqual(output_size[0][0], 6)\n        self.assertEqual(output_size[1][0], 6)\n        self.assertEqual(cell_outputs[0].shape[-1], encoder.cell_fw.output_size)\n        self.assertEqual(cell_outputs[1].shape[-1], encoder.cell_bw.output_size)\n\n        out_dim = encoder.hparams.output_layer_fw.layer_size[-1]\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_[0].shape, (batch_size, max_time, out_dim))\n            self.assertEqual(outputs_[1].shape, (batch_size, max_time, out_dim))\n\n# TODO(zhiting): not completed yet\n# class HierarchicalForwardRNNEncoderTest(tf.test.TestCase):\n#    """"""Tests HierarchicalForwardRNNEncoder class.\n#    """"""\n#\n#    def test_trainable_variables(self):\n#        """"""Tests the functionality of automatically collecting trainable\n#        variables.\n#        """"""\n#        encoder = HierarchicalForwardRNNEncoder(vocab_size=2)\n#        inputs = [[[1, 0], [0, 1], [0, 1]]]\n#        _, _ = encoder(inputs)\n#        self.assertEqual(len(encoder.trainable_variables), 5)\n#\n#    def test_encode(self):\n#        """"""Tests encoding.\n#        """"""\n#        vocab_size = 4\n#        encoder = HierarchicalForwardRNNEncoder(vocab_size=vocab_size)\n#\n#        max_major_time = 8\n#        max_minor_time = 6\n#        batch_size = 16\n#        inputs = tf.random_uniform([batch_size, max_major_time, max_minor_time],\n#                                   maxval=vocab_size,\n#                                   dtype=tf.int32)\n#        outputs, state = encoder(inputs)\n#\n#        cell_dim = encoder.hparams.rnn_cell.kwargs.num_units\n#        with self.test_session() as sess:\n#            sess.run(tf.global_variables_initializer())\n#            outputs_, state_ = sess.run([outputs, state])\n#            self.assertEqual(outputs_.shape, (batch_size, max_major_time, cell_dim))\n#            self.assertEqual(state_[0].shape, (batch_size, cell_dim))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/encoders/xlnet_encoder_test.py,13,"b'#\n""""""\nUnit tests for XLNet encoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass XLNetEncoderTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.XLNetEncoder` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in XLNetEncoder.available_checkpoints():\n            encoder = XLNetEncoder(pretrained_model_name=pretrained_model_name)\n            _ = encoder(inputs)\n\n    @pretrained_test\n    def test_hparams(self):\n        """"""Tests the priority of the encoder architecture parameter.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1: set ""pretrained_mode_name"" by constructor argument\n        encoder = XLNetEncoder(pretrained_model_name=""xlnet-large-cased"",\n                               hparams={})\n        encoder(inputs)\n        self.assertEqual(len(encoder.attn_layers), 24)\n        self.assertEqual(len(encoder.ff_layers), 24)\n\n        # case 2: set ""pretrained_mode_name"" by hparams\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-base-cased""\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        encoder(inputs)\n        self.assertEqual(len(encoder.attn_layers), 12)\n        self.assertEqual(len(encoder.ff_layers), 12)\n\n        # case 3: set to None in both hparams and constructor argument\n        # load no pre-trained model\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""num_layers"": 16\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        encoder(inputs)\n        self.assertEqual(len(encoder.attn_layers), 16)\n        self.assertEqual(len(encoder.ff_layers), 16)\n\n        # case 4: using default hparams\n        encoder = XLNetEncoder()\n        encoder(inputs)\n        self.assertEqual(len(encoder.attn_layers), 12)\n        self.assertEqual(len(encoder.ff_layers), 12)\n\n    @pretrained_test\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1: XLNet with no pre-trained model\n        encoder = XLNetEncoder(hparams={\n                                   ""pretrained_model_name"": None,\n                                   ""untie_r"": False\n                               })\n        encoder(inputs)\n\n        n_word_embed_vars = 1\n        n_mask_embed_vars = 1\n        n_bias_vars = 3  # r_r_bias, r_w_bias, r_s_bias\n        n_pos_wise_ff_vars = 6  # 2 kernels + 2 bias + beta + gamma\n        n_rel_multi_head_vars = 7  # q,k,v,r,o + beta + gamma\n        n_segment_embed_vars = 1\n        n_layers = encoder.hparams.num_layers\n        n_trainable_variables = \\\n            n_word_embed_vars + n_segment_embed_vars + n_mask_embed_vars + \\\n            n_layers * (n_rel_multi_head_vars + n_pos_wise_ff_vars) + \\\n            n_bias_vars\n        self.assertEqual(len(encoder.trainable_variables),\n                         n_trainable_variables)\n\n        # case 2: XLNet with pre-trained model\n        hparams = {\n            ""pretrained_model_name"": ""xlnet-large-cased""\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        encoder(inputs)\n        n_segment_embed_vars = 1\n        n_layers = encoder.hparams.num_layers\n        n_trainable_variables = \\\n            n_word_embed_vars + n_segment_embed_vars + n_mask_embed_vars + \\\n            n_layers * (n_rel_multi_head_vars + n_pos_wise_ff_vars) \\\n            + n_bias_vars\n        self.assertEqual(len(encoder.trainable_variables),\n                         n_trainable_variables)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        # case 1: XLNet pre-trained\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""untie_r"": False\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n\n        max_time = 8\n        batch_size = 128\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n        outputs, _ = encoder(inputs)\n\n        outputs_dim = encoder.hparams.hidden_dim\n        with self.session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape,\n                             (batch_size, max_time, outputs_dim))\n\n        # case 2: XLNet pre-trained, untie_r=True\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""untie_r"": True\n        }\n\n        encoder = XLNetEncoder(hparams=hparams)\n        outputs, _ = encoder(inputs)\n        with self.session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape,\n                             (batch_size, max_time, outputs_dim))\n\n        # case 3: XLNet with no pre-trained model\n        hparams = {\n            ""pretrained_model_name"": None\n        }\n        encoder = XLNetEncoder(hparams=hparams)\n        outputs_dim = encoder.hparams.hidden_dim\n        outputs, _ = encoder(inputs)\n        with self.session() as sess:\n            sess.run(tf.global_variables_initializer())\n            outputs_ = sess.run(outputs)\n            self.assertEqual(outputs_.shape,\n                             (batch_size, max_time, outputs_dim))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/memory/memory_network_test.py,11,"b'""""""\nUnit tests for memory networks.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.memory.memory_network import MemNetRNNLike\nfrom texar.tf import context\n\n# pylint: disable=no-member, too-many-locals, too-many-instance-attributes\n# pylint: disable=too-many-arguments, protected-access\n\n\nclass MemNetRNNLikeTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.memory.memory_network.MemNetRNNLike`.\n    """"""\n\n    def _test_memory_dim(self, combine_mode=\'add\', soft_memory=False,\n                         soft_query=False, use_B=False):\n        """"""Tests :attr:`memory_dim` in the :attr:`combine_mode` and soft\n        options.\n        """"""\n        print(\'testing: combine_mode={}, soft_memory={}, soft_query={}, \'\n              \'use_B={}\'.format(combine_mode, soft_memory, soft_query, use_B))\n\n        n_hops = 3\n        if combine_mode == \'add\' or combine_mode is None:\n            memory_dim = 19\n            embedding_dim = memory_dim\n            temporal_embedding_dim = memory_dim\n        elif combine_mode == \'concat\':\n            embedding_dim = 19\n            temporal_embedding_dim = 17\n            memory_dim = embedding_dim + temporal_embedding_dim\n        else:\n            raise ValueError(\n                ""combine_mode = {} is not recognized"".format(combine_mode))\n        relu_dim = 13\n        memory_size = 7\n        raw_memory_dim = 11\n        batch_size = 2\n        embed_hparams = {\n            ""embedding"": {\n                ""dim"": embedding_dim,\n            },\n            ""temporal_embedding"": {\n                ""dim"": temporal_embedding_dim,\n            },\n            ""combine_mode"": combine_mode,\n        }\n        memnet_hparams = {\n            ""n_hops"": n_hops,\n            ""relu_dim"": relu_dim,\n            ""memory_size"": memory_size,\n            ""A"": embed_hparams,\n            ""C"": embed_hparams,\n            ""B"": embed_hparams,\n            ""use_B"": use_B,\n        }\n        memnet = MemNetRNNLike(raw_memory_dim=raw_memory_dim,\n                               hparams=memnet_hparams)\n        kwargs = {}\n        if soft_memory:\n            kwargs[\'soft_memory\'] = tf.random_uniform(\n                [batch_size, memory_size, raw_memory_dim])\n        else:\n            kwargs[\'memory\'] = tf.tile(tf.expand_dims(\n                tf.range(memory_size, dtype=tf.int32), 0), [batch_size, 1])\n        if use_B:\n            if soft_query:\n                kwargs[\'soft_query\'] = tf.random_uniform(\n                    [batch_size, raw_memory_dim])\n            else:\n                kwargs[\'query\'] = tf.random_uniform(\n                    [batch_size], maxval=raw_memory_dim, dtype=tf.int32)\n        else:\n            kwargs[\'query\'] = tf.random_uniform([batch_size, memory_dim])\n        logits = memnet(**kwargs)\n        self.assertEqual(memnet.memory_dim, memory_dim)\n        self.assertEqual(logits.shape[0], batch_size)\n        self.assertEqual(logits.shape[1], raw_memory_dim)\n\n    def test_memory_dim(self):\n        """"""Tests :attr:`memory_dim` in different :attr:`combine_mode` and\n        different soft options.\n        """"""\n        for combine_mode in [\'add\', \'concat\']:\n            for soft_memory in [False, True]:\n                for use_B in [False, True]:\n                    for soft_query in ([False, True] if use_B else [False]):\n                        self._test_memory_dim(combine_mode, soft_memory,\n                                              soft_query, use_B)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/networks/conv_networks_test.py,10,"b'#\n""""""\nUnit tests for conv networks.\n""""""\n\nimport tensorflow as tf\n\nimport texar.tf as tx\nfrom texar.tf.modules.networks.conv_networks import Conv1DNetwork\n\n\nclass Conv1DNetworkTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.Conv1DNetwork` class.\n    """"""\n\n    def test_feedforward(self):\n        """"""Tests feed forward.\n        """"""\n        network_1 = Conv1DNetwork()\n        self.assertEqual(len(network_1.layers), 4)\n        self.assertTrue(isinstance(network_1.layer_by_name(""conv_pool_1""),\n                                   tx.core.MergeLayer))\n        for layer in network_1.layers[0].layers:\n            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))\n\n        inputs_1 = tf.ones([64, 16, 300], tf.float32)\n        outputs_1 = network_1(inputs_1)\n        self.assertEqual(outputs_1.shape, [64, 128])\n\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""filters"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            ""other_conv_kwargs"": {""padding"": ""same""},\n            # Pooling layers\n            ""pooling"": ""AveragePooling"",\n            ""pool_size"": 2,\n            ""pool_strides"": 1,\n            # Dense layers\n            ""num_dense_layers"": 3,\n            ""dense_size"": [128, 128, 10],\n            ""dense_activation"": ""relu"",\n            ""other_dense_kwargs"": {""use_bias"": False},\n            # Dropout\n            ""dropout_conv"": [0, 1, 2],\n            ""dropout_dense"": 2\n        }\n        network_2 = Conv1DNetwork(hparams)\n        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten\n        self.assertEqual(len(network_2.layers), 1 + 1 + 1 + 3 + 4 + 1)\n        self.assertTrue(isinstance(network_2.layer_by_name(""conv_pool_1""),\n                                   tx.core.MergeLayer))\n        for layer in network_2.layers[1].layers:\n            self.assertTrue(isinstance(layer, tx.core.SequentialLayer))\n\n        inputs_2 = tf.ones([64, 16, 300], tf.float32)\n        outputs_2 = network_2(inputs_2)\n        self.assertEqual(outputs_2.shape, [64, 10])\n\n    def test_unknown_seq_length(self):\n        """"""Tests use of pooling layer when the seq_length dimension of inputs\n        is `None`.\n        """"""\n        network_1 = Conv1DNetwork()\n        inputs_1 = tf.placeholder(tf.float32, [64, None, 300])\n        outputs_1 = network_1(inputs_1)\n        self.assertEqual(outputs_1.shape, [64, 128])\n\n        hparams = {\n            # Conv layers\n            ""num_conv_layers"": 2,\n            ""filters"": 128,\n            ""kernel_size"": [[3, 4, 5], 4],\n            # Pooling layers\n            ""pooling"": ""AveragePooling"",\n            ""pool_size"": [2, None],\n            # Dense layers\n            ""num_dense_layers"": 1,\n            ""dense_size"": 10,\n        }\n        network = Conv1DNetwork(hparams)\n        # nlayers = nconv-pool + nconv + npool + ndense + ndropout + flatten\n        self.assertEqual(len(network.layers), 1 + 1 + 1 + 1 + 1 + 1)\n        self.assertTrue(isinstance(network.layer_by_name(\'pool_2\'),\n                                   tx.core.AverageReducePooling1D))\n\n        inputs = tf.placeholder(tf.float32, [64, None, 300])\n        outputs = network(inputs)\n        self.assertEqual(outputs.shape, [64, 10])\n\n        hparams_2 = {\n            # Conv layers\n            ""num_conv_layers"": 1,\n            ""filters"": 128,\n            ""kernel_size"": 4,\n            ""other_conv_kwargs"": {\'data_format\': \'channels_first\'},\n            # Pooling layers\n            ""pooling"": ""MaxPooling"",\n            ""other_pool_kwargs"": {\'data_format\': \'channels_first\'},\n            # Dense layers\n            ""num_dense_layers"": 1,\n            ""dense_size"": 10,\n        }\n        network_2 = Conv1DNetwork(hparams_2)\n        inputs_2 = tf.placeholder(tf.float32, [64, 300, None])\n        outputs_2 = network_2(inputs_2)\n        self.assertEqual(outputs_2.shape, [64, 10])\n\n    def test_mask_input(self):\n        """"""Tests masked inputs.\n        """"""\n        network_1 = Conv1DNetwork()\n        inputs_1 = tf.ones([3, 16, 300], tf.float32)\n        seq_length = [10, 15, 1]\n        outputs_1 = network_1(inputs_1, sequence_length=seq_length)\n        self.assertEqual(outputs_1.shape, [3, 128])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/networks/networks_test.py,5,"b'""""""\nUnit tests for feed forward neural networks.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.networks.networks import FeedForwardNetwork\n\n# pylint: disable=no-member, invalid-name\n\n\nclass FeedForwardNetworkTest(tf.test.TestCase):\n    """"""Tests the class\n    :class:`~texar.tf.modules.networks.networks.FeedForwardNetwork`.\n    """"""\n\n    def test_feedforward(self):\n        """"""Tests feed-forward.\n        """"""\n        hparams = {\n            ""layers"": [\n                {\n                    ""type"": ""Dense"",\n                },\n                {\n                    ""type"": ""Dense"",\n                }\n            ]\n        }\n\n        nn = FeedForwardNetwork(hparams=hparams)\n\n        self.assertEqual(len(nn.layers), len(hparams[""layers""]))\n        _ = nn(tf.ones([64, 16, 16]))\n        self.assertEqual(len(nn.trainable_variables),\n                         len(hparams[""layers""]) * 2)\n        self.assertEqual(len(nn.layer_outputs), len(hparams[""layers""]))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/policies/policy_nets_test.py,6,"b'#\n""""""\nTests policy nets.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow_probability import distributions as tfpd\n\nfrom texar.tf.modules.policies.policy_nets import CategoricalPolicyNet\n\n\nclass CategoricalPolicyNetTest(tf.test.TestCase):\n    """"""Tests :class:`texar.tf.modules.CategoricalPolicyNet`.\n    """"""\n\n    def test_categorical_policy(self):\n        """"""Tests logics.\n        """"""\n        policy = CategoricalPolicyNet()\n\n        inputs = tf.random_uniform(shape=[1, 4])\n        outputs = policy(inputs=inputs)\n        self.assertEqual(list(outputs[\'action\'].shape[1:]),\n                         list(policy.action_space.shape))\n        self.assertIsInstance(outputs[\'dist\'],\n                              tfpd.Categorical)\n\n        inputs = tf.random_uniform(shape=[64, 4])\n        outputs = policy(inputs=inputs)\n        self.assertEqual(list(outputs[\'action\'].shape[1:]),\n                         list(policy.action_space.shape))\n        self.assertEqual(int(outputs[\'action\'].shape[0]), 64)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/pretrained/bert_test.py,4,"b'""""""\nUnit tests for BERT utils.\n""""""\n\nimport os\nimport tensorflow as tf\n\nfrom texar.tf.modules.pretrained.bert import *\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass BERTUtilsTest(tf.test.TestCase):\n    r""""""Tests BERT utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_bert_AND_transform_bert_to_texar_config(self):\n\n        pretrained_model_dir = PretrainedBERTMixin.download_checkpoint(\n            pretrained_model_name=""bert-base-uncased"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'bert_model.ckpt.meta\', files)\n        self.assertIn(\'bert_model.ckpt.data-00000-of-00001\', files)\n        self.assertIn(\'bert_model.ckpt.index\', files)\n        self.assertIn(\'bert_config.json\', files)\n\n        model_config = PretrainedBERTMixin._transform_config(\n            pretrained_model_name=""bert-base-uncased"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'hidden_size\': 768,\n            \'embed\': {\n                \'name\': \'word_embeddings\',\n                \'dim\': 768\n            },\n            \'vocab_size\': 30522,\n            \'segment_embed\': {\n                \'name\': \'token_type_embeddings\',\n                \'dim\': 768\n            },\n            \'type_vocab_size\': 2,\n            \'position_embed\': {\n                \'name\': \'position_embeddings\',\n                \'dim\': 768\n            },\n            \'position_size\': 512,\n            \'encoder\': {\n                  \'name\': \'encoder\',\n                  \'embedding_dropout\': 0.1,\n                  \'num_blocks\': 12,\n                  \'multihead_attention\': {\n                      \'use_bias\': True,\n                      \'num_units\': 768,\n                      \'num_heads\': 12,\n                      \'output_dim\': 768,\n                      \'dropout_rate\': 0.1,\n                      \'name\': \'self\'\n                  },\n                  \'residual_dropout\': 0.1,\n                  \'dim\': 768,\n                  \'use_bert_config\': True,\n                  \'poswise_feedforward\': {\n                      \'layers\': [\n                          {\n                              \'type\': \'Dense\',\n                              \'kwargs\': {\n                                  \'name\': \'intermediate\',\n                                  \'units\': 3072,\n                                  \'activation\': \'gelu\',\n                                  \'use_bias\': True\n                              }\n                          },\n                          {\n                              \'type\': \'Dense\',\n                              \'kwargs\': {\n                                  \'name\': \'output\',\n                                  \'units\': 768,\n                                  \'activation\': None,\n                                  \'use_bias\': True\n                              }\n                          }\n                      ]\n                  }\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/pretrained/gpt2_test.py,4,"b'""""""\nUnit tests for GPT2 utils.\n""""""\n\nimport os\nimport tensorflow as tf\n\nfrom texar.tf.modules.pretrained.gpt2 import *\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass GPT2UtilsTest(tf.test.TestCase):\n    r""""""Tests GPT2 utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_gpt2_AND_transform_gpt2_to_texar_config(self):\n        pretrained_model_dir = PretrainedGPT2Mixin.download_checkpoint(\n            pretrained_model_name=""gpt2-small"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'checkpoint\', files)\n        self.assertIn(\'encoder.json\', files)\n        self.assertIn(\'hparams.json\', files)\n        self.assertIn(\'model.ckpt.data-00000-of-00001\', files)\n        self.assertIn(\'model.ckpt.index\', files)\n        self.assertIn(\'model.ckpt.meta\', files)\n        self.assertIn(\'vocab.bpe\', files)\n\n        model_config = PretrainedGPT2Mixin._transform_config(\n            pretrained_model_name=""gpt2-small"",\n            cache_dir=pretrained_model_dir)\n\n        exp_config = {\n            \'vocab_size\': 50257,\n            \'context_size\': 1024,\n            \'embedding_size\': 768,\n            \'embed\': {\n                \'dim\': 768\n            },\n            \'position_size\': 1024,\n            \'position_embed\': {\n                \'dim\': 768\n            },\n\n            \'encoder\': {\n                \'dim\': 768,\n                \'num_blocks\': 12,\n                \'embedding_dropout\': 0,\n                \'residual_dropout\': 0,\n                \'multihead_attention\': {\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768\n                },\n                \'initializer\': {\n                    \'type\': \'variance_scaling_initializer\',\n                    \'kwargs\': {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                    }\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            ""type"": ""Dense"",\n                            ""kwargs"": {\n                                \'name\': \'intermediate\',\n                                \'activation\': \'gelu\',\n                                ""units"": 3072,\n                                ""use_bias"": True,\n                            }\n                        },\n                        {\n                            ""type"": ""Dense"",\n                            ""kwargs"": {\n                                \'activation\': None,\n                                \'name\': \'output\',\n                                ""units"": 768,\n                                ""use_bias"": True,\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n\n        self.assertDictEqual(model_config, exp_config)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/pretrained/xlnet_test.py,4,"b'""""""\nUnit tests for xlnet utils.\n""""""\n\nimport os\nimport tensorflow as tf\n\nfrom texar.tf.modules.pretrained.xlnet import *\nfrom texar.tf.utils.test import pretrained_test\n\n\nclass XLNetUtilsTest(tf.test.TestCase):\n    r""""""Tests XLNet utils.\n    """"""\n\n    @pretrained_test\n    def test_load_pretrained_model_AND_transform_xlnet_to_texar_config(self):\n\n        pretrained_model_dir = PretrainedXLNetMixin.download_checkpoint(\n            pretrained_model_name=""xlnet-base-cased"")\n\n        info = list(os.walk(pretrained_model_dir))\n        _, _, files = info[0]\n        self.assertIn(\'spiece.model\', files)\n        self.assertIn(\'xlnet_model.ckpt.meta\', files)\n        self.assertIn(\'xlnet_model.ckpt.data-00000-of-00001\', files)\n        self.assertIn(\'xlnet_model.ckpt.index\', files)\n        self.assertIn(\'xlnet_config.json\', files)\n\n        model_config = PretrainedXLNetMixin._transform_config(\n            pretrained_model_name=""xlnet-base-cased"",\n            cache_dir=pretrained_model_dir)\n\n        expected_config = {\n            \'head_dim\': 64,\n            \'ffn_inner_dim\': 3072,\n            \'hidden_dim\': 768,\n            \'activation\': \'gelu\',\n            \'num_heads\': 12,\n            \'num_layers\': 12,\n            \'vocab_size\': 32000,\n            \'untie_r\': True\n        }\n\n        self.assertDictEqual(model_config, expected_config)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/pretrained/xlnet_utils_test.py,10,"b'""""""\nUnit tests for xlnet model utils.\n""""""\nimport tensorflow as tf\n\nfrom texar.tf.modules.pretrained.xlnet_utils import \\\n    PositionWiseFF, RelativePositionalEncoding, RelativeMutiheadAttention\n\n\nclass XLNetModelUtilsTest(tf.test.TestCase):\n    r""""""Tests xlnet model utils.\n    """"""\n\n    def test_PositionWiseFF(self):\n\n        # Case 1\n        model = PositionWiseFF()\n        inputs = tf.random_uniform(shape=(32, model.hparams.hidden_dim))\n        outputs = model(inputs)\n        self.assertEqual(outputs.shape, [32, model._hparams.hidden_dim])\n\n        # Case 2\n        hparams = {\n            ""hidden_dim"": 16,\n            ""ffn_inner_dim"": 32,\n            ""dropout"": 0.1,\n            ""activation"": \'relu\',\n        }\n        model = PositionWiseFF(hparams=hparams)\n        inputs = tf.random_uniform(shape=(32, 16))\n        outputs = model(inputs)\n        self.assertEqual(outputs.shape, [32, 16])\n\n        # Case 3\n        hparams = {\n            ""hidden_dim"": 16,\n            ""ffn_inner_dim"": 32,\n            ""dropout"": 0.1,\n            ""activation"": \'gelu\',\n        }\n        model = PositionWiseFF(hparams=hparams)\n        inputs = tf.random_uniform(shape=(32, 16))\n        outputs = model(inputs)\n        self.assertEqual(outputs.shape, [32, 16])\n\n    def test_RelativeMultiheadAttention(self):\n        num_heads = 12\n        head_dim = 64\n\n        r_r_bias = tf.random_normal(shape=(num_heads, head_dim))\n        r_w_bias = tf.random_normal(shape=(num_heads, head_dim))\n\n        model = RelativeMutiheadAttention(r_r_bias=r_r_bias, r_w_bias=r_w_bias)\n\n        states_h = tf.random_uniform(shape=(16, 32, model._hparams.hidden_dim))\n        pos_embed = tf.random_uniform(shape=(24, 32, model._hparams.hidden_dim))\n\n        output_h, output_g = model(states_h=states_h, pos_embed=pos_embed)\n\n        self.assertEqual(output_h.shape,\n                         [16, 32, model._hparams.hidden_dim])\n        self.assertEqual(output_g, None)\n\n    def test_RelativePositionalEncoding(self):\n\n        batch_size = 16\n        max_time = 8\n        total_len = 32\n\n        # Case 1\n        model = RelativePositionalEncoding()\n        pos_embed = model(batch_size=batch_size,\n                          max_time=max_time,\n                          total_len=total_len)\n        self.assertEqual(pos_embed.shape,\n                         [40, 16, model._hparams.dim])\n\n        # Case 2\n        model = RelativePositionalEncoding()\n        pos_embed = model(batch_size=batch_size,\n                          max_time=max_time,\n                          total_len=total_len,\n                          attn_type=\'uni\')\n        self.assertEqual(pos_embed.shape,\n                         [33, 16, model._hparams.dim])\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/modules/regressors/xlnet_regressor_test.py,16,"b'#\n""""""\nUnit tests for XLNet regressor.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom texar.tf.modules.regressors.xlnet_regressor import XLNetRegressor\nfrom texar.tf.utils.test import pretrained_test\n\n# pylint: disable=too-many-locals, no-member\n\n\nclass XLNetRegressorTest(tf.test.TestCase):\n    """"""Tests :class:`~texar.tf.modules.XLNetRegressor` class.\n    """"""\n\n    @pretrained_test\n    def test_model_loading(self):\n        r""""""Tests model loading functionality.""""""\n\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        for pretrained_model_name in XLNetRegressor.available_checkpoints():\n            regressor = XLNetRegressor(\n                pretrained_model_name=pretrained_model_name)\n            _ = regressor(inputs)\n\n    def test_trainable_variables(self):\n        """"""Tests the functionality of automatically collecting trainable\n        variables.\n        """"""\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None])\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        regressor(inputs)\n        n_xlnet_vars = 162\n        n_projection_vars = 2\n        n_logits_vars = 2\n        self.assertEqual(len(regressor.trainable_variables),\n                         n_xlnet_vars + n_logits_vars + n_projection_vars)\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""all_time""\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        regressor(inputs)\n        self.assertEqual(len(regressor.trainable_variables),\n                         n_xlnet_vars + n_logits_vars + n_projection_vars)\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""time_wise""\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        regressor(inputs)\n        self.assertEqual(len(regressor.trainable_variables),\n                         n_xlnet_vars + n_logits_vars + n_projection_vars)\n\n    def test_encode(self):\n        """"""Tests encoding.\n        """"""\n        max_time = 8\n        batch_size = 16\n        inputs = tf.random_uniform([batch_size, max_time],\n                                   maxval=30521, dtype=tf.int32)\n\n        # case 1\n        hparams = {\n            ""pretrained_model_name"": None,\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        logits = regressor(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_ = sess.run(logits)\n            self.assertEqual(logits_.shape, (batch_size,))\n\n        # case 2\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""cls_time""\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        logits = regressor(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_ = sess.run(logits)\n            self.assertEqual(logits_.shape, (batch_size,))\n\n        # case 3\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""time_wise""\n        }\n        regressor = XLNetRegressor(hparams=hparams)\n        logits = regressor(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_ = sess.run(logits)\n            self.assertEqual(logits_.shape,\n                             (batch_size, max_time))\n\n        # case 4\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""all_time"",\n            ""max_seq_len"": max_time\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        regressor = XLNetRegressor(hparams=hparams)\n        logits = regressor(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_ = sess.run(\n                logits,\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.shape, (batch_size,))\n\n    def test_regression(self):\n        """"""Test the type of regression output.""""""\n        batch_size = 8\n\n        hparams = {\n            ""pretrained_model_name"": None,\n            ""regr_strategy"": ""cls_time""\n        }\n        inputs = tf.placeholder(tf.int32, shape=[batch_size, 6])\n        regressor = XLNetRegressor(hparams=hparams)\n        logits = regressor(inputs)\n\n        with self.test_session() as sess:\n            sess.run(tf.global_variables_initializer())\n            logits_ = sess.run(\n                logits,\n                feed_dict={inputs: np.random.randint(30521,\n                                                     size=(batch_size, 6))})\n            self.assertEqual(logits_.dtype, np.float32)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
texar/tf/agents/__init__.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RL Agents\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.agents.pg_agent import *\nfrom texar.tf.agents.seq_pg_agent import *\nfrom texar.tf.agents.dqn_agent import *\nfrom texar.tf.agents.ac_agent import *\nfrom texar.tf.agents.agent_utils import *\ntry:\n    from texar.tf.agents.agent_gym_utils import *\nexcept ImportError:\n    pass\n'"
texar/tf/agents/ac_agent.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Actor-critic agent.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom texar.tf.agents.episodic_agent_base import EpisodicAgentBase\nfrom texar.tf.utils import utils\n\n# pylint: disable=too-many-instance-attributes, protected-access\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""ActorCriticAgent""\n]\n\n\nclass ActorCriticAgent(EpisodicAgentBase):\n    """"""Actor-critic agent for episodic setting.\n\n    An actor-critic algorithm consists of several components:\n\n        - **Actor** is the policy to optimize. As a temporary implementation,\\\n        here by default we use a :class:`~texar.tf.agents.PGAgent` instance \\\n        that wraps a `policy net` and provides proper interfaces to perform \\\n        the role of an actor.\n        - **Critic** that provides learning signals to the actor. Again, as \\\n        a temporary implemetation, here by default we use a \\\n        :class:`~texar.tf.agents.DQNAgent` instance that wraps a `Q net` and \\\n        provides proper interfaces to perform the role of a critic.\n\n    Args:\n        env_config: An instance of :class:`~texar.tf.agents.EnvConfig`\n            specifying action space, observation space, and reward range, etc.\n            Use :func:`~texar.tf.agents.get_gym_env_config` to create an\n            EnvConfig from a gym environment.\n        sess (optional): A tf session.\n            Can be `None` here and set later with `agent.sess = session`.\n        actor (optional): An instance of :class:`~texar.tf.agents.PGAgent` that\n            performs as actor in the algorithm.\n            If not provided, an actor is created based on :attr:`hparams`.\n        actor_kwargs (dict, optional): Keyword arguments for actor\n            constructor. Note that the `hparams` argument for actor\n            constructor is specified in the ""actor_hparams"" field of\n            :attr:`hparams` and should not be included in `actor_kwargs`.\n            Ignored if :attr:`actor` is given.\n        critic (optional): An instance of :class:`~texar.tf.agents.DQNAgent`\n            that performs as critic in the algorithm.\n            If not provided, a critic is created based on :attr:`hparams`.\n        critic_kwargs (dict, optional): Keyword arguments for critic\n            constructor. Note that the `hparams` argument for critic\n            constructor is specified in the ""critic_hparams"" field of\n            :attr:`hparams` and should not be included in `critic_kwargs`.\n            Ignored if :attr:`critic` is given.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n\n    def __init__(self,\n                 env_config,\n                 sess=None,\n                 actor=None,\n                 actor_kwargs=None,\n                 critic=None,\n                 critic_kwargs=None,\n                 hparams=None):\n        EpisodicAgentBase.__init__(self, env_config=env_config, hparams=hparams)\n\n        self._sess = sess\n        self._num_actions = self._env_config.action_space.high - \\\n                            self._env_config.action_space.low\n\n        with tf.variable_scope(self.variable_scope):\n            if actor is None:\n                kwargs = utils.get_instance_kwargs(\n                    actor_kwargs, self._hparams.actor_hparams)\n                kwargs.update(dict(env_config=env_config, sess=sess))\n                actor = utils.get_instance(\n                    class_or_name=self._hparams.actor_type,\n                    kwargs=kwargs,\n                    module_paths=[\'texar.tf.agents\', \'texar.tf.custom\'])\n            self._actor = actor\n\n            if critic is None:\n                kwargs = utils.get_instance_kwargs(\n                    critic_kwargs, self._hparams.critic_hparams)\n                kwargs.update(dict(env_config=env_config, sess=sess))\n                critic = utils.get_instance(\n                    class_or_name=self._hparams.critic_type,\n                    kwargs=kwargs,\n                    module_paths=[\'texar.tf.agents\', \'texar.tf.custom\'])\n            self._critic = critic\n\n            if self._actor._discount_factor != self._critic._discount_factor:\n                raise ValueError(\'discount_factor of the actor and the critic \'\n                                 \'must be the same.\')\n            self._discount_factor = self._actor._discount_factor\n\n            self._observs = []\n            self._actions = []\n            self._rewards = []\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values:\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                \'actor_type\': \'PGAgent\',\n                \'actor_hparams\': None,\n                \'critic_type\': \'DQNAgent\',\n                \'critic_hparams\': None,\n                \'name\': \'actor_critic_agent\'\n            }\n\n        Here:\n\n        ""actor_type"": str or class or instance\n            Actor. Can be class, its\n            name or module path, or a class instance. If class name is given,\n            the class must be from module :mod:`texar.tf.agents` or\n            :mod:`texar.tf.custom`. Ignored if a `actor` is given to\n            the agent constructor.\n\n        ""actor_kwargs"": dict, optional\n            Hyperparameters for the actor class. With the :attr:`actor_kwargs`\n            argument to the constructor, an actor is created with\n            :python:`actor_class(**actor_kwargs, hparams=actor_hparams)`.\n\n        ""critic_type"": str or class or instance\n            Critic. Can be class, its\n            name or module path, or a class instance. If class name is given,\n            the class must be from module :mod:`texar.tf.agents` or\n            :mod:`texar.tf.custom`. Ignored if a `critic` is given to\n            the agent constructor.\n\n        ""critic_kwargs"": dict, optional\n            Hyperparameters for the critic class. With the :attr:`critic_kwargs`\n            argument to the constructor, an critic is created with\n            :python:`critic_class(**critic_kwargs, hparams=critic_hparams)`.\n\n        ""name"": str\n            Name of the agent.\n        """"""\n        return {\n            \'actor_type\': \'PGAgent\',\n            \'actor_hparams\': None,\n            \'critic_type\': \'DQNAgent\',\n            \'critic_hparams\': None,\n            \'name\': \'actor_critic_agent\'\n        }\n\n    def _reset(self):\n        self._actor._reset()\n        self._critic._reset()\n\n    def _observe(self, reward, terminal, train_policy, feed_dict):\n        self._train_actor(\n            observ=self._observ,\n            action=self._action,\n            feed_dict=feed_dict)\n        self._critic._observe(reward, terminal, train_policy, feed_dict)\n\n    def _train_actor(self, observ, action, feed_dict):\n        qvalues = self._critic._qvalues_from_target(observ=observ)\n        advantage = qvalues[0][action] - np.mean(qvalues)\n        # TODO (bowen): should be a funciton to customize?\n\n        feed_dict_ = {\n            self._actor._observ_inputs: [observ],\n            self._actor._action_inputs: [action],\n            self._actor._advantage_inputs: [advantage]\n        }\n        feed_dict_.update(feed_dict)\n\n        self._actor._train_policy(feed_dict=feed_dict_)\n\n    def get_action(self, observ, feed_dict=None):\n        self._observ = observ\n        self._action = self._actor.get_action(observ, feed_dict=feed_dict)\n\n        self._critic._update_observ_action(self._observ, self._action)\n\n        return self._action\n\n    @property\n    def sess(self):\n        """"""The tf session.\n        """"""\n        return self._sess\n\n    @sess.setter\n    def sess(self, session):\n        self._sess = session\n        self._actor._sess = session\n        self._critic._sess = session\n'"
texar/tf/agents/agent_base.py,3,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for reinforcement learning agents.\n""""""\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.utils.variables import get_unique_named_variable_scope\n\n# pylint: disable=too-many-instance-attributes\n\n__all__ = [\n    ""AgentBase""\n]\n\n\nclass AgentBase(object):\n    """"""\n    Base class inherited by RL agents.\n\n    Args:\n        TODO\n    """"""\n    def __init__(self, hparams=None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n        name = self._hparams.name\n        self._variable_scope = get_unique_named_variable_scope(name)\n        self._unique_name = self._variable_scope.name.split(""/"")[-1]\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        TODO\n        """"""\n        return {\n            \'name\': \'agent\'\n        }\n\n    @property\n    def variable_scope(self):\n        """"""The variable scope of the agent.\n        """"""\n        return self._variable_scope\n\n    @property\n    def name(self):\n        """"""The name of the module (not uniquified).\n        """"""\n        return self._unique_name\n\n    @property\n    def hparams(self):\n        """"""A :class:`~texar.tf.hyperparams.HParams` instance. The hyperparameters\n        of the module.\n        """"""\n        return self._hparams\n'"
texar/tf/agents/agent_gym_utils.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious agent utilities based on OpenAI Gym.\n""""""\n\nimport gym\n\n__all__ = [\n    ""convert_gym_space"",\n    ""get_gym_env_config""\n]\n\n\ndef convert_gym_space(spc):\n    """"""Converts a :gym:`gym.Space <#spaces>` instance to a\n    :class:`~texar.tf.agents.Space` instance.\n\n    Args:\n        spc: An instance of `gym.Space` or\n            :class:`~texar.tf.agents.Space`.\n    """"""\n    from texar.tf.agents.agent_utils import Space\n    if isinstance(spc, Space):\n        return spc\n    if isinstance(spc, gym.spaces.Discrete):\n        return Space(shape=(), low=0, high=spc.n, dtype=spc.dtype)\n    elif isinstance(spc, gym.spaces.Box):\n        return Space(\n            shape=spc.shape, low=spc.low, high=spc.high, dtype=spc.dtype)\n\n\ndef get_gym_env_config(env):\n    """"""Creates an instance of :class:`~texar.tf.agents.EnvConfig`\n    from a :gym:`gym env <#environments>`.\n\n    Args:\n        env: An instance of OpenAI gym Environment.\n\n    Returns:\n        An instance of :class:`~texar.tf.agents.EnvConfig`.\n    """"""\n    from texar.tf.agents.agent_utils import EnvConfig\n    return EnvConfig(\n        action_space=env.action_space,\n        observ_space=env.observation_space,\n        reward_range=env.reward_range)\n'"
texar/tf/agents/agent_utils.py,3,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious agent utilities.\n""""""\n\n# pylint: disable=too-many-arguments, too-few-public-methods, no-member\n# pylint: disable=invalid-name, wrong-import-position\n\nimport numpy as np\n\ngym_utils = None\ntry:\n    from texar.tf.agents import agent_gym_utils as gym_utils\nexcept ImportError:\n    pass\n\n__all__ = [\n    ""Space"",\n    ""EnvConfig""\n]\n\n\nclass Space(object):\n    """"""Observation and action spaces. Describes valid actions and observations.\n    Similar to :gym:`gym.Space <#spaces>`.\n\n    Args:\n        shape (optional): Shape of the space, a tuple. If not\n            given, infers from :attr:`low` and :attr:`high`.\n        low (optional): Lower bound (inclusive) of each dimension of the\n            space. Must have\n            shape as specified by :attr:`shape`, and of the same shape with\n            with :attr:`high` (if given). If `None`, set to `-inf` for each\n            dimension.\n        high (optional): Upper bound (inclusive) of each dimension of the\n            space. Must have\n            shape as specified by :attr:`shape`, and of the same shape with\n            with :attr:`low` (if given). If `None`, set to `inf` for each\n            dimension.\n        dtype (optional): Data type of elements in the space. If not given,\n            infers from :attr:`low` (if given) or set to `float`.\n\n    Example:\n\n        .. code-block:: python\n\n            s = Space(low=0, high=10, dtype=np.int32)\n            #s.contains(2) == True\n            #s.contains(10) == True\n            #s.contains(11) == False\n            #s.shape == ()\n\n            s2 = Space(shape=(2,2), high=np.ones([2,2]), dtype=np.float)\n            #s2.low == [[-inf, -inf], [-inf, -inf]]\n            #s2.high == [[1., 1.], [1., 1.]]\n    """"""\n    def __init__(self, shape=None, low=None, high=None, dtype=None):\n        if low is None:\n            low = -float(\'inf\')\n        if high is None:\n            high = float(\'inf\')\n\n        if shape is None:\n            low = np.asarray(low)\n            high = np.asarray(high)\n            if low.shape != high.shape:\n                raise ValueError(\'`low` and `high` must have the same shape.\')\n            shape = low.shape\n        else:\n            shape = tuple(shape)\n\n        if np.isscalar(low):\n            low = low + np.zeros(shape, dtype=dtype)\n        if np.isscalar(high):\n            high = high + np.zeros(shape, dtype=dtype)\n        if shape != low.shape or shape != high.shape:\n            raise ValueError(\n                \'Shape inconsistent: shape={}, low.shape={}, high.shape={}\'\n                .format(shape, low.shape, high.shape))\n        if dtype is None:\n            dtype = low.dtype\n        dtype = np.dtype(dtype)\n        low = low.astype(dtype)\n        high = high.astype(dtype)\n        self._shape = shape\n        self._low = low\n        self._high = high\n        self._dtype = dtype\n\n    def contains(self, x):\n        """"""Checks if x is contained in the space. Returns a `bool`.\n        """"""\n        x = np.asarray(x)\n        dtype_match = True\n        if self._dtype.kind in np.typecodes[\'AllInteger\']:\n            if x.dtype.kind not in np.typecodes[\'AllInteger\']:\n                dtype_match = False\n        shape_match = x.shape == self._shape\n        low_match = (x >= self._low).all()\n        high_match = (x <= self._high).all()\n        return dtype_match and shape_match and low_match and high_match\n\n    @property\n    def shape(self):\n        """"""Shape of the space.\n        """"""\n        return self._shape\n\n    @property\n    def low(self):\n        """"""Lower bound of the space.\n        """"""\n        return self._low\n\n    @property\n    def high(self):\n        """"""Upper bound of the space.\n        """"""\n        return self._high\n\n    @property\n    def dtype(self):\n        """"""Data type of the element.\n        """"""\n        return self._dtype\n\n\nclass EnvConfig(object):\n    """"""Configurations of an environment.\n\n    Args:\n        action_space: An instance of :class:`~texar.tf.agents.Space` or\n            :gym:`gym.Space <#spaces>`, the action space.\n        observ_space: An instance of :class:`~texar.tf.agents.Space` or\n            :gym:`gym.Space <#spaces>`, the observation space.\n        reward_range: A tuple corresponding to the min and max possible\n            rewards, e.g., `reward_range=(-1.0, 1.0)`.\n    """"""\n\n    def __init__(self,\n                 action_space,\n                 observ_space,\n                 reward_range):\n        if gym_utils:\n            action_space = gym_utils.convert_gym_space(action_space)\n            observ_space = gym_utils.convert_gym_space(observ_space)\n\n        self.action_space = action_space\n        self.action_dtype = action_space.dtype\n        self.action_shape = action_space.shape\n\n        self.observ_space = observ_space\n        self.observ_dtype = observ_space.dtype\n        self.observ_shape = observ_space.shape\n\n        self.reward_range = reward_range\n'"
texar/tf/agents/dqn_agent.py,31,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Deep Q learning Agent.\n""""""\n\nimport random\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf import context\nfrom texar.tf.agents.episodic_agent_base import EpisodicAgentBase\nfrom texar.tf.utils import utils\nfrom texar.tf.core import optimization as opt\n\n# pylint: disable=too-many-instance-attributes, too-many-arguments\n# pylint: disable=invalid-name\n\n__all__ = [\n    ""DQNAgent""\n]\n\n\nclass DQNAgent(EpisodicAgentBase):\n    """"""Deep Q learning agent for episodic setting.\n\n    A Q learning algorithm consists of several components:\n\n        - A **Q-net** takes in a state and returns Q-value for action sampling.\n          See :class:`~texar.tf.modules.CategoricalQNet` for an example Q-net\n          class and required interface.\n        - A **replay memory** manages past experience for Q-net updates. See\\\n        :class:`~texar.tf.core.DequeReplayMemory` for an example replay memory\\\n        class and required interface.\n        - An **exploration** that specifies the exploration strategy used\\\n        to train the Q-net. See\\\n        :class:`~texar.tf.core.EpsilonLinearDecayExploration` for an example\\\n        class and required interface.\n\n    Args:\n        env_config: An instance of :class:`~texar.tf.agents.EnvConfig`\n            specifying action space, observation space, and reward range, etc.\n            Use :func:`~texar.tf.agents.get_gym_env_config` to create an\n            EnvConfig from a gym environment.\n        sess (optional): A tf session.\n            Can be `None` here and set later with `agent.sess = session`.\n        qnet (optional): A Q network that predicts Q values given states.\n            If not given, a Q network is created based on :attr:`hparams`.\n        target (optional): A target network to compute target Q values.\n        qnet_kwargs (dict, optional): Keyword arguments for qnet\n            constructor. Note that the `hparams` argument for network\n            constructor is specified in the ""policy_hparams"" field of\n            :attr:`hparams` and should not be included in `policy_kwargs`.\n            Ignored if :attr:`qnet` is given.\n        qnet_caller_kwargs (dict, optional): Keyword arguments for\n            calling `qnet` to get Q values. The `qnet` is called with\n            :python:`outputs=qnet(inputs=observation, **qnet_caller_kwargs)`\n        replay_memory (optional): A replay memory instance.\n            If not given, a replay memory is created based on :attr:`hparams`.\n        replay_memory_kwargs (dict, optional): Keyword arguments for\n            replay_memory constructor.\n            Ignored if :attr:`replay_memory` is given.\n        exploration (optional): An exploration instance used in the algorithm.\n            If not given, an exploration instance is created based on\n            :attr:`hparams`.\n        exploration_kwargs (dict, optional): Keyword arguments for exploration\n            class constructor. Ignored if :attr:`exploration` is given.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n    def __init__(self,\n                 env_config,\n                 sess=None,\n                 qnet=None,\n                 target=None,\n                 qnet_kwargs=None,\n                 qnet_caller_kwargs=None,\n                 replay_memory=None,\n                 replay_memory_kwargs=None,\n                 exploration=None,\n                 exploration_kwargs=None,\n                 hparams=None):\n        EpisodicAgentBase.__init__(self, env_config, hparams)\n\n        self._sess = sess\n        self._cold_start_steps = self._hparams.cold_start_steps\n        self._sample_batch_size = self._hparams.sample_batch_size\n        self._update_period = self._hparams.update_period\n        self._discount_factor = self._hparams.discount_factor\n        self._target_update_strategy = self._hparams.target_update_strategy\n        self._num_actions = self._env_config.action_space.high - \\\n                            self._env_config.action_space.low\n\n        with tf.variable_scope(self.variable_scope):\n            if qnet is None:\n                kwargs = utils.get_instance_kwargs(\n                    qnet_kwargs, self._hparams.qnet_hparams)\n                qnet = utils.check_or_get_instance(\n                    ins_or_class_or_name=self._hparams.qnet_type,\n                    kwargs=kwargs,\n                    module_paths=[\'texar.tf.modules\', \'texar.tf.custom\'])\n                target = utils.check_or_get_instance(\n                    ins_or_class_or_name=self._hparams.qnet_type,\n                    kwargs=kwargs,\n                    module_paths=[\'texar.tf.modules\', \'texar.tf.custom\'])\n            self._qnet = qnet\n            self._target = target\n            self._qnet_caller_kwargs = qnet_caller_kwargs or {}\n\n            if replay_memory is None:\n                kwargs = utils.get_instance_kwargs(\n                    replay_memory_kwargs, self._hparams.replay_memory_hparams)\n                replay_memory = utils.check_or_get_instance(\n                    ins_or_class_or_name=self._hparams.replay_memory_type,\n                    kwargs=kwargs,\n                    module_paths=[\'texar.tf.core\', \'texar.tf.custom\'])\n            self._replay_memory = replay_memory\n\n            if exploration is None:\n                kwargs = utils.get_instance_kwargs(\n                    exploration_kwargs, self._hparams.exploration_hparams)\n                exploration = utils.check_or_get_instance(\n                    ins_or_class_or_name=self._hparams.exploration_type,\n                    kwargs=kwargs,\n                    module_paths=[\'texar.tf.core\', \'texar.tf.custom\'])\n            self._exploration = exploration\n\n        self._build_graph()\n\n        self._observ = None\n        self._action = None\n        self._timestep = 0\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values:\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                \'qnet_type\': \'CategoricalQNet\',\n                \'qnet_hparams\': None,\n                \'replay_memory_type\': \'DequeReplayMemory\',\n                \'replay_memory_hparams\': None,\n                \'exploration_type\': \'EpsilonLinearDecayExploration\',\n                \'exploration_hparams\': None,\n                \'optimization\': opt.default_optimization_hparams(),\n                \'target_update_strategy\': \'copy\',\n                \'cold_start_steps\': 100,\n                \'sample_batch_size\': 32,\n                \'update_period\': 100,\n                \'discount_factor\': 0.95,\n                \'name\': \'dqn_agent\'\n            }\n\n        Here:\n\n        ""qnet_type"": str or class or instance\n            Q-value net. Can be class, its\n            name or module path, or a class instance. If class name is given,\n            the class must be from module :mod:`texar.tf.modules` or\n            :mod:`texar.tf.custom`. Ignored if a `qnet` is given to\n            the agent constructor.\n\n        ""qnet_hparams"": dict, optional\n            Hyperparameters for the Q net. With the :attr:`qnet_kwargs`\n            argument to the constructor, a network is created with\n            :python:`qnet_class(**qnet_kwargs, hparams=qnet_hparams)`.\n\n        ""replay_memory_type"": str or class or instance\n            Replay memory class. Can be class, its name or module path,\n            or a class instance.\n            If class name is given, the class must be from module\n            :mod:`texar.tf.core` or :mod:`texar.tf.custom`.\n            Ignored if a `replay_memory` is given to the agent constructor.\n\n        ""replay_memory_hparams"": dict, optional\n            Hyperparameters for the replay memory. With the\n            :attr:`replay_memory_kwargs` argument to the constructor,\n            a network is created with\n            :python:`replay_memory_class(\n            **replay_memory_kwargs, hparams=replay_memory_hparams)`.\n\n        ""exploration_type"": str or class or instance\n            Exploration class. Can be class,\n            its name or module path, or a class instance. If class name is\n            given, the class must be from module :mod:`texar.tf.core` or\n            :mod:`texar.tf.custom`. Ignored if a `exploration` is given to\n            the agent constructor.\n\n        ""exploration_hparams"": dict, optional\n            Hyperparameters for the exploration class.\n            With the :attr:`exploration_kwargs` argument to the constructor,\n            a network is created with :python:`exploration_class(\n            **exploration_kwargs, hparams=exploration_hparams)`.\n\n        ""optimization"": dict\n            Hyperparameters of optimization for updating the Q-net.\n            See :func:`~texar.tf.core.default_optimization_hparams` for details.\n\n        ""cold_start_steps"": int\n            In the beginning, Q-net is not trained in the first few steps.\n\n        ""sample_batch_size"": int\n            The number of samples taken in replay memory when training.\n\n        ""target_update_strategy"": string\n\n            - If **""copy""**, the target network is assigned with the parameter \\\n            of Q-net every :attr:`""update_period""` steps.\n            - If **""tau""**, target will be updated by assigning as \\\n            ``` (1 - 1/update_period) * target + 1/update_period * qnet ```\n\n        ""update_period"": int\n            Frequecy of updating the target network, i.e., updating\n            the target once for every ""update_period"" steps.\n\n        ""discount_factor"": float\n            The discount factor of reward.\n\n        ""name"": str\n            Name of the agent.\n        """"""\n        return {\n            \'qnet_type\': \'CategoricalQNet\',\n            \'qnet_hparams\': None,\n            \'replay_memory_type\': \'DequeReplayMemory\',\n            \'replay_memory_hparams\': None,\n            \'exploration_type\': \'EpsilonLinearDecayExploration\',\n            \'exploration_hparams\': None,\n            \'optimization\': opt.default_optimization_hparams(),\n            \'target_update_strategy\': \'copy\',\n            \'cold_start_steps\': 100,\n            \'sample_batch_size\': 32,\n            \'update_period\': 100,\n            \'discount_factor\': 0.95,\n            \'name\': \'dqn_agent\'\n        }\n\n    def _build_graph(self):\n        with tf.variable_scope(self.variable_scope):\n            self._observ_inputs = tf.placeholder(\n                dtype=self._env_config.observ_dtype,\n                shape=[None, ] + list(self._env_config.observ_shape),\n                name=\'observ_inputs\')\n            self._action_inputs = tf.placeholder(\n                dtype=self._env_config.action_dtype,\n                shape=[None, self._num_actions],\n                name=\'action_inputs\')\n            self._y_inputs = tf.placeholder(\n                dtype=tf.float32,\n                shape=[None, ],\n                name=\'y_inputs\')\n\n            self._qnet_outputs = self._get_qnet_outputs(self._observ_inputs)\n            self._target_outputs = self._get_target_outputs(self._observ_inputs)\n            self._td_error = self._get_td_error(\n                qnet_qvalues=self._qnet_outputs[\'qvalues\'],\n                actions=self._action_inputs,\n                y=self._y_inputs)\n            self._train_op = self._get_train_op()\n\n            if self._target_update_strategy == \'copy\':\n                self._update_op = self._get_copy_update_op()\n            elif self._target_update_strategy == \'tau\':\n                self._update_op = self._get_tau_update_op()\n\n    def _get_qnet_outputs(self, state_inputs):\n        return self._qnet(inputs=state_inputs, **self._qnet_caller_kwargs)\n\n    def _get_target_outputs(self, state_inputs):\n        return self._target(inputs=state_inputs, **self._qnet_caller_kwargs)\n\n    def _get_td_error(self, qnet_qvalues, actions, y):\n        return y - tf.reduce_sum(qnet_qvalues * tf.cast(actions, tf.float), axis=1)\n\n    def _get_train_op(self):\n        train_op = opt.get_train_op(\n            loss=tf.reduce_sum(self._td_error ** 2),\n            variables=self._qnet.trainable_variables,\n            hparams=self._hparams.optimization.todict())\n        return train_op\n\n    def _get_copy_update_op(self):\n        op = []\n        for i in range(len(self._qnet.trainable_variables)):\n            op.append(tf.assign(ref=self._target.trainable_variables[i],\n                                value=self._qnet.trainable_variables[i]))\n        return op\n\n    def _get_tau_update_op(self):\n        tau = 1. / self._update_period\n        op = []\n        for i in range(len(self._qnet.trainable_variables)):\n            value_ = (1. - tau) * self._target.trainable_variables[i] + \\\n                    tau * self._qnet.trainable_variables[i]\n            op.append(tf.assign(\n                ref=self._target.trainable_variables[i], value=value_))\n        return op\n\n    def _observe(self, reward, terminal, train_policy, feed_dict):\n        if self._timestep > self._cold_start_steps and train_policy:\n            self._train_qnet(feed_dict)\n\n        action_one_hot = [0.] * self._num_actions\n        action_one_hot[self._action] = 1.\n\n        self._replay_memory.add(dict(\n            observ=self._observ,\n            action=action_one_hot,\n            reward=reward,\n            terminal=terminal,\n            next_observ=None))\n        self._timestep += 1\n\n    def _train_qnet(self, feed_dict):\n        minibatch = self._replay_memory.get(self._sample_batch_size)\n        observ_batch = np.array([data[\'observ\'] for data in minibatch])\n        action_batch = np.array([data[\'action\'] for data in minibatch])\n        reward_batch = np.array([data[\'reward\'] for data in minibatch])\n        terminal_batch = np.array([data[\'terminal\'] for data in minibatch])\n        next_observ_batch = \\\n            np.array([data[\'next_observ\'] for data in minibatch])\n\n        target_qvalue = self._sess.run(\n            self._target_outputs[\'qvalues\'], feed_dict={\n                self._observ_inputs: next_observ_batch,\n                context.global_mode(): tf.estimator.ModeKeys.PREDICT})\n\n        y_batch = reward_batch\n        for i in range(self._sample_batch_size):\n            if not terminal_batch[i]:\n                y_batch[i] += self._discount_factor * np.max(target_qvalue[i])\n\n        feed_dict_ = {\n            self._observ_inputs: observ_batch,\n            self._y_inputs: y_batch,\n            self._action_inputs: action_batch\n        }\n        feed_dict_.update(feed_dict or {})\n\n        self._sess.run(self._train_op, feed_dict=feed_dict_)\n\n        self._update_target(feed_dict)\n\n    def _update_target(self, feed_dict):\n        if self._target_update_strategy == \'tau\' or (\n                self._target_update_strategy == \'copy\' and\n                self._timestep % self._update_period == 0):\n            self._sess.run(self._update_op, feed_dict=feed_dict)\n\n    def _qvalues_from_qnet(self, observ):\n        return self._sess.run(\n            self._qnet_outputs[\'qvalues\'],\n            feed_dict={self._observ_inputs: np.array([observ]),\n                       context.global_mode(): tf.estimator.ModeKeys.PREDICT})\n\n    def _qvalues_from_target(self, observ):\n        return self._sess.run(\n            self._target_outputs[\'qvalues\'],\n            feed_dict={self._observ_inputs: np.array([observ]),\n                       context.global_mode(): tf.estimator.ModeKeys.PREDICT})\n\n    def _update_observ_action(self, observ, action):\n        self._observ = observ\n        self._action = action\n        if self._replay_memory.size() > 0:\n            self._replay_memory.last()[\'next_observ\'] = self._observ\n\n    def _get_action(self, observ, feed_dict=None):\n        qvalue = self._qvalues_from_qnet(observ)\n\n        if random.random() < self._exploration.get_epsilon(self._timestep):\n            action = random.randrange(self._num_actions)\n        else:\n            action = np.argmax(qvalue)\n\n        self._update_observ_action(observ, action)\n\n        return action\n\n    def _reset(self):\n        self._observ = None\n        self._action = None\n\n    @property\n    def sess(self):\n        """"""The tf session.\n        """"""\n        return self._sess\n\n    @sess.setter\n    def sess(self, session):\n        self._sess = session\n'"
texar/tf/agents/episodic_agent_base.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for episodic reinforcement learning agents.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.agents.agent_base import AgentBase\n\n# pylint: disable=too-many-instance-attributes\n\n\nclass EpisodicAgentBase(AgentBase):\n    """"""Base class inherited by episodic RL agents.\n\n    An agent is a wrapper of the **training process** that trains a model\n    with RL algorithms. Agent itself does not create new trainable variables.\n\n    An episodic RL agent typically provides 3 interfaces, namely, :meth:`reset`,\n    :meth:`get_action` and :meth:`observe`, and is used as the following\n    example.\n\n    Example:\n\n        .. code-block:: python\n\n            env = SomeEnvironment(...)\n            agent = PGAgent(...)\n\n            while True:\n                # Starts one episode\n                agent.reset()\n                observ = env.reset()\n                while True:\n                    action = agent.get_action(observ)\n                    next_observ, reward, terminal = env.step(action)\n                    agent.observe(reward, terminal)\n                    observ = next_observ\n                    if terminal:\n                        break\n\n    Args:\n        env_config: An instance of :class:`~texar.tf.agents.EnvConfig`\n            specifying action space, observation space, and reward range, etc.\n            Use :func:`~texar.tf.agents.get_gym_env_config` to create an\n            EnvConfig from a gym environment.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n    def __init__(self, env_config, hparams=None):\n        AgentBase.__init__(self, hparams)\n\n        self._env_config = env_config\n\n        self._reset_tmplt_fn = tf.make_template(\n            ""{}_reset"".format(self.name), self._reset)\n        self._observe_tmplt_fn = tf.make_template(\n            ""{}_observe"".format(self.name), self._observe)\n        self._get_action_tmplt_fn = tf.make_template(\n            ""{}_get_action"".format(self.name), self._get_action)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""agent""\n            }\n        """"""\n        return {\n            \'name\': \'agent\'\n        }\n\n    def reset(self):\n        """"""Resets the states to begin new episode.\n        """"""\n        self._reset_tmplt_fn()\n\n    def _reset(self):\n        raise NotImplementedError\n\n    def observe(self, reward, terminal, train_policy=True, feed_dict=None):\n        """"""Observes experience from environment.\n\n        Args:\n            reward: Reward of the action. The configuration (e.g., shape) of\n                the reward is defined in :attr:`env_config`.\n            terminal (bool): Whether the episode is terminated.\n            train_policy (bool): Wether to update the policy for this step.\n            feed_dict (dict, optional): Any stuffs fed to running the training\n                operator.\n        """"""\n        return self._observe_tmplt_fn(reward, terminal, train_policy, feed_dict)\n\n    def _observe(self, reward, terminal, train_policy, feed_dict):\n        raise NotImplementedError\n\n    def get_action(self, observ, feed_dict=None):\n        """"""Gets action according to observation.\n\n        Args:\n            observ: Observation from the environment.\n\n        Returns:\n            action from the policy.\n        """"""\n        return self._get_action_tmplt_fn(observ, feed_dict)\n\n    def _get_action(self, observ, feed_dict):\n        raise NotImplementedError\n\n    @property\n    def env_config(self):\n        """"""Environment configuration.\n        """"""\n        return self._env_config\n'"
texar/tf/agents/pg_agent.py,18,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Policy Gradient agent.\n""""""\n\n# pylint: disable=too-many-instance-attributes, too-many-arguments\n\nimport tensorflow as tf\n\nfrom texar.tf.agents.episodic_agent_base import EpisodicAgentBase\nfrom texar.tf.utils import utils\nfrom texar.tf.core import optimization as opt\nfrom texar.tf.losses import pg_losses as losses\nfrom texar.tf.losses.rewards import discount_reward\n\n\nclass PGAgent(EpisodicAgentBase):\n    """"""Policy gradient agent for episodic setting. This agent here supports\n    **un-batched** training, i.e., each time generates one action, takes one\n    observation, and updates the policy.\n\n    The policy must take in an observation of shape `[1] + observation_shape`,\n    where the first dimension 1 stands for batch dimension, and output a `dict`\n    containing:\n\n    - Key **""action""** whose value is a Tensor of shape \\\n    `[1] + action_shape` containing a single action.\n    - One of keys ""log_prob"" or ""dist"":\n\n        - **""log_prob""**: A Tensor of shape `[1]`, the log probability of the \\\n        ""action"".\n        - **""dist""**: A \\\n        tf_main:`tf.distributions.Distribution <distributions/Distribution>`\\\n        with the `log_prob` interface and \\\n        `log_prob = dist.log_prob(outputs[""action""])`.\n\n    .. role:: python(code)\n       :language: python\n\n    Args:\n        env_config: An instance of :class:`~texar.tf.agents.EnvConfig`\n            specifying action space, observation space, and reward range, etc.\n            Use :func:`~texar.tf.agents.get_gym_env_config` to create an\n            EnvConfig from a gym environment.\n        sess (optional): A tf session.\n            Can be `None` here and set later with `agent.sess = session`.\n        policy (optional): A policy net that takes in observation and outputs\n            actions and probabilities.\n            If not given, a policy network is created based on :attr:`hparams`.\n        policy_kwargs (dict, optional): Keyword arguments for policy\n            constructor. Note that the `hparams` argument for network\n            constructor is specified in the ""policy_hparams"" field of\n            :attr:`hparams` and should not be included in `policy_kwargs`.\n            Ignored if :attr:`policy` is given.\n        policy_caller_kwargs (dict, optional): Keyword arguments for\n            calling the policy to get actions. The policy is called with\n            :python:`outputs=policy(inputs=observation, **policy_caller_kwargs)`\n        learning_rate (optional): Learning rate for policy optimization. If\n            not given, determine the learning rate from :attr:`hparams`.\n            See :func:`~texar.tf.core.get_train_op` for more details.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n    def __init__(self,\n                 env_config,\n                 sess=None,\n                 policy=None,\n                 policy_kwargs=None,\n                 policy_caller_kwargs=None,\n                 learning_rate=None,\n                 hparams=None):\n        EpisodicAgentBase.__init__(self, env_config, hparams)\n\n        self._sess = sess\n        self._lr = learning_rate\n        self._discount_factor = self._hparams.discount_factor\n\n        with tf.variable_scope(self.variable_scope):\n            if policy is None:\n                kwargs = utils.get_instance_kwargs(\n                    policy_kwargs, self._hparams.policy_hparams)\n                policy = utils.check_or_get_instance(\n                    self._hparams.policy_type,\n                    kwargs,\n                    module_paths=[\'texar.tf.modules\', \'texar.tf.custom\'])\n            self._policy = policy\n            self._policy_caller_kwargs = policy_caller_kwargs or {}\n\n        self._observs = []\n        self._actions = []\n        self._rewards = []\n\n        self._train_outputs = None\n\n        self._build_graph()\n\n    def _build_graph(self):\n        with tf.variable_scope(self.variable_scope):\n            self._observ_inputs = tf.placeholder(\n                dtype=self._env_config.observ_dtype,\n                shape=[None, ] + list(self._env_config.observ_shape),\n                name=\'observ_inputs\')\n            self._action_inputs = tf.placeholder(\n                dtype=self._env_config.action_dtype,\n                shape=[None, ] + list(self._env_config.action_shape),\n                name=\'action_inputs\')\n            self._advantage_inputs = tf.placeholder(\n                dtype=tf.float32,\n                shape=[None, ],\n                name=\'advantages_inputs\')\n\n            self._outputs = self._get_policy_outputs()\n\n            self._pg_loss = self._get_pg_loss()\n\n            self._train_op = self._get_train_op()\n\n    def _get_policy_outputs(self):\n        outputs = self._policy(\n            inputs=self._observ_inputs, **self._policy_caller_kwargs)\n        return outputs\n\n    def _get_pg_loss(self):\n        if \'log_prob\' in self._outputs:\n            log_probs = self._outputs[\'log_prob\']\n        elif \'dist\' in self._outputs:\n            log_probs = self._outputs[\'dist\'].log_prob(self._action_inputs)\n        else:\n            raise ValueError(\'Outputs of the policy must have one of \'\n                             \'""log_prob"" or ""dist"".\')\n        pg_loss = losses.pg_loss_with_log_probs(\n            log_probs=log_probs,\n            advantages=self._advantage_inputs,\n            average_across_timesteps=True,\n            sum_over_timesteps=False)\n        return pg_loss\n\n    def _get_train_op(self):\n        train_op = opt.get_train_op(\n            loss=self._pg_loss,\n            variables=self._policy.trainable_variables,\n            learning_rate=self._lr,\n            hparams=self._hparams.optimization.todict())\n        return train_op\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values:\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                \'policy_type\': \'CategoricalPolicyNet\',\n                \'policy_hparams\': None,\n                \'discount_factor\': 0.95,\n                \'normalize_reward\': False,\n                \'optimization\': default_optimization_hparams(),\n                \'name\': \'pg_agent\',\n            }\n\n        Here:\n\n        ""policy_type"": str or class or instance\n            Policy net. Can be class, its name or module path, or a class\n            instance. If class name is given, the class must be from module\n            :mod:`texar.tf.modules` or :mod:`texar.tf.custom`. Ignored if a\n            `policy` is given to the agent constructor.\n\n        ""policy_hparams"": dict, optional\n            Hyperparameters for the policy net. With the :attr:`policy_kwargs`\n            argument to the constructor, a network is created with\n            :python:`policy_class(**policy_kwargs, hparams=policy_hparams)`.\n\n        ""discount_factor"": float\n            The discount factor of reward.\n\n        ""normalize_reward"": bool\n            Whether to normalize the discounted reward, by\n            `(discounted_reward - mean) / std`.\n\n        ""optimization"": dict\n            Hyperparameters of optimization for updating the policy net.\n            See :func:`~texar.tf.core.default_optimization_hparams` for details.\n\n        ""name"": str\n            Name of the agent.\n        """"""\n        return {\n            \'policy_type\': \'CategoricalPolicyNet\',\n            \'policy_hparams\': None,\n            \'discount_factor\': 0.95,\n            \'normalize_reward\': False,\n            \'optimization\': opt.default_optimization_hparams(),\n            \'name\': \'pg_agent\',\n        }\n\n    def _reset(self):\n        self._observs = []\n        self._actions = []\n        self._rewards = []\n\n    def _get_action(self, observ, feed_dict):\n        fetches = {\n            ""action"": self._outputs[\'action\']\n        }\n\n        feed_dict_ = {self._observ_inputs: [observ, ]}\n        feed_dict_.update(feed_dict or {})\n\n        vals = self._sess.run(fetches, feed_dict=feed_dict_)\n        action = vals[\'action\']\n        action = action[0]  # Removes the batch dimension\n\n        self._observs.append(observ)\n        self._actions.append(action)\n\n        return action\n\n    def _observe(self, reward, terminal, train_policy, feed_dict):\n        self._rewards.append(reward)\n\n        if terminal and train_policy:\n            self._train_policy(feed_dict=feed_dict)\n\n    def _train_policy(self, feed_dict=None):\n        """"""Updates the policy.\n\n        Args:\n            TODO\n        """"""\n        qvalues = discount_reward(\n            [self._rewards], discount=self._hparams.discount_factor,\n            normalize=self._hparams.normalize_reward)\n        qvalues = qvalues[0, :]\n\n        fetches = dict(loss=self._train_op)\n        feed_dict_ = {\n            self._observ_inputs: self._observs,\n            self._action_inputs: self._actions,\n            self._advantage_inputs: qvalues}\n        feed_dict_.update(feed_dict or {})\n\n        self._train_outputs = self._sess.run(fetches, feed_dict=feed_dict_)\n\n    @property\n    def sess(self):\n        """"""The tf session.\n        """"""\n        return self._sess\n\n    @sess.setter\n    def sess(self, session):\n        self._sess = session\n\n    @property\n    def policy(self):\n        """"""The policy model.\n        """"""\n        return self._policy\n'"
texar/tf/agents/seq_agent_base.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for reinforcement learning agents for sequence prediction.\n""""""\n\nfrom texar.tf.agents.agent_base import AgentBase\n\n# pylint: disable=too-many-instance-attributes\n\n\nclass SeqAgentBase(AgentBase):\n    """"""\n    Base class inherited by sequence prediction RL agents.\n\n    Args:\n        TODO\n    """"""\n    def __init__(self, hparams=None):\n        AgentBase.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        TODO\n        """"""\n        return {\n            \'name\': \'agent\'\n        }\n'"
texar/tf/agents/seq_pg_agent.py,13,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Policy Gradient agent for sequence prediction.\n""""""\n\n# pylint: disable=too-many-instance-attributes, too-many-arguments, no-member\n\nimport tensorflow as tf\n\nfrom texar.tf.agents.seq_agent_base import SeqAgentBase\nfrom texar.tf.core import optimization as opt\nfrom texar.tf.losses.pg_losses import pg_loss_with_logits\nfrom texar.tf.losses.rewards import discount_reward\nfrom texar.tf.losses.entropy import sequence_entropy_with_logits\n\n__all__ = [\n    ""SeqPGAgent""\n]\n\n\nclass SeqPGAgent(SeqAgentBase):\n    """"""Policy Gradient agent for sequence prediction.\n\n    This is a wrapper of the **training process** that trains a model\n    with policy gradient. Agent itself does not create new trainable variables.\n\n    Args:\n        samples: An `int` Tensor of shape `[batch_size, max_time]` containing\n            sampled sequences from the model.\n        logits: A float Tenosr of shape `[batch_size, max_time, vocab_size]`\n            containing the logits of samples from the model.\n        sequence_length: A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths are masked out.\n        trainable_variables (optional): Trainable variables of the model to\n            update during training. If `None`, all trainable variables in the\n            graph are used.\n        learning_rate (optional): Learning rate for policy optimization. If\n            not given, determine the learning rate from :attr:`hparams`.\n            See :func:`~texar.tf.core.get_train_op` for more details.\n        sess (optional): A tf session.\n            Can be `None` here and set later with `agent.sess = session`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    Example:\n\n        .. code-block:: python\n\n            ## Train a decoder with policy gradient\n            decoder = BasicRNNDecoder(...)\n            outputs, _, sequence_length = decoder(\n                decoding_strategy=\'infer_sample\', ...)\n\n            sess = tf.Session()\n            agent = SeqPGAgent(\n                samples=outputs.sample_id,\n                logits=outputs.logits,\n                sequence_length=sequence_length,\n                sess=sess)\n            while training:\n                # Generate samples\n                vals = agent.get_samples()\n                # Evaluate reward\n                sample_text = tx.utils.map_ids_to_strs(vals[\'samples\'], vocab)\n                reward_bleu = []\n                for y, y_ in zip(ground_truth, sample_text)\n                    reward_bleu.append(tx.evals.sentence_bleu(y, y_)\n                # Update\n                agent.observe(reward=reward_bleu)\n    """"""\n    def __init__(self,\n                 samples,\n                 logits,\n                 sequence_length,\n                 trainable_variables=None,\n                 learning_rate=None,\n                 sess=None,\n                 hparams=None):\n        SeqAgentBase.__init__(self, hparams)\n\n        self._lr = learning_rate\n\n        # Tensors\n        self._samples = samples\n        self._logits = logits\n        self._sequence_length = sequence_length\n        self._trainable_variables = trainable_variables\n\n        # Python values\n        self._samples_py = None\n        self._sequence_length_py = None\n        self._rewards = None\n\n        self._sess = sess\n\n        # For session partial run\n        self._partial_run_handle = None\n        self._qvalue_inputs_fed = False\n\n        self._build_graph()\n\n    def _build_graph(self):\n        with tf.variable_scope(self.variable_scope):\n            self._qvalue_inputs = tf.placeholder(\n                dtype=tf.float32,\n                shape=[None, None],\n                name=\'qvalue_inputs\')\n            self._pg_loss = self._get_pg_loss()\n            self._train_op = self._get_train_op()\n\n    def _get_pg_loss(self):\n        loss_hparams = self._hparams.loss\n        pg_loss = pg_loss_with_logits(\n            actions=self._samples,\n            logits=self._logits,\n            sequence_length=self._sequence_length,\n            advantages=self._qvalue_inputs,\n            batched=True,\n            average_across_batch=loss_hparams.average_across_batch,\n            average_across_timesteps=loss_hparams.average_across_timesteps,\n            sum_over_batch=loss_hparams.sum_over_batch,\n            sum_over_timesteps=loss_hparams.sum_over_timesteps,\n            time_major=loss_hparams.time_major)\n\n        if self._hparams.entropy_weight > 0:\n            entropy = self._get_entropy()\n            pg_loss -= self._hparams.entropy_weight * entropy\n\n        return pg_loss\n\n    def _get_entropy(self):\n        loss_hparams = self._hparams.loss\n        return sequence_entropy_with_logits(\n            self._logits,\n            sequence_length=self._sequence_length,\n            average_across_batch=loss_hparams.average_across_batch,\n            average_across_timesteps=loss_hparams.average_across_timesteps,\n            sum_over_batch=loss_hparams.sum_over_batch,\n            sum_over_timesteps=loss_hparams.sum_over_timesteps,\n            time_major=loss_hparams.time_major)\n\n    def _get_train_op(self):\n        train_op = opt.get_train_op(\n            loss=self._pg_loss,\n            variables=self._trainable_variables,\n            learning_rate=self._lr,\n            hparams=self._hparams.optimization.todict())\n        return train_op\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values:\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                \'discount_factor\': 0.95,\n                \'normalize_reward\': False,\n                \'entropy_weight\': 0.,\n                \'loss\': {\n                    \'average_across_batch\': True,\n                    \'average_across_timesteps\': False,\n                    \'sum_over_batch\': False,\n                    \'sum_over_timesteps\': True,\n                    \'time_major\': False\n                },\n                \'optimization\': default_optimization_hparams(),\n                \'name\': \'pg_agent\',\n            }\n\n        Here:\n\n        ""discount_factor"": float\n            The discount factor of reward.\n\n        ""normalize_reward"": bool\n            Whether to normalize the discounted reward, by\n            `(discounted_reward - mean) / std`. Here `mean` and `std` are\n            over all time steps and all samples in the batch.\n\n        ""entropy_weight"": float\n            The weight of entropy loss of the sample distribution, to encourage\n            maximizing the Shannon entropy. Set to 0 to disable the loss.\n\n        ""loss"": dict\n            Extra keyword arguments for\n            :func:`~texar.tf.losses.pg_loss_with_logits`, including the\n            reduce arguments (e.g., `average_across_batch`) and `time_major`\n\n        ""optimization"": dict\n            Hyperparameters of optimization for updating the policy net.\n            See :func:`~texar.tf.core.default_optimization_hparams` for details.\n\n        ""name"": str\n            Name of the agent.\n        """"""\n        return {\n            \'discount_factor\': 0.95,\n            \'normalize_reward\': False,\n            \'entropy_weight\': 0.,\n            \'loss\': {\n                \'average_across_batch\': True,\n                \'average_across_timesteps\': False,\n                \'sum_over_batch\': False,\n                \'sum_over_timesteps\': True,\n                \'time_major\': False\n            },\n            \'optimization\': opt.default_optimization_hparams(),\n            \'name\': \'pg_agent\',\n        }\n\n    def _get_partial_run_feeds(self, feeds=None):\n        if feeds is None:\n            feeds = []\n        feeds += [self._qvalue_inputs]\n        return feeds\n\n    def _setup_partial_run(self, fetches=None, feeds=None):\n        fetches_ = [self._samples, self._sequence_length, self._pg_loss,\n                    self._train_op]\n        if fetches is not None:\n            for fet in fetches:\n                if fet not in fetches_:\n                    fetches_.append(fet)\n\n        feeds = self._get_partial_run_feeds(feeds)\n\n        self._partial_run_handle = self._sess.partial_run_setup(\n            fetches_, feeds=feeds)\n\n        self._qvalue_inputs_fed = False\n\n    def _check_extra_fetches(self, extra_fetches):\n        fetch_values = None\n        if extra_fetches is not None:\n            fetch_values = list(extra_fetches.values())\n        if fetch_values is not None:\n            if self._samples in fetch_values:\n                raise ValueError(\n                    ""`samples` must not be included in `extra_fetches`. ""\n                    ""It is added automatically."")\n            if self._sequence_length in fetch_values:\n                raise ValueError(\n                    ""`sequence_length` must not be included in `extra_fetches`.""\n                    "" It is added automatically."")\n            if ""samples"" in extra_fetches:\n                raise ValueError(\n                    ""Key \'samples\' is preserved and must not be used ""\n                    ""in `extra_fetches`."")\n            if ""sequence_length"" in extra_fetches:\n                raise ValueError(\n                    ""Key \'sequence_length\' is preserved and must not be used ""\n                    ""in `extra_fetches`."")\n\n    def get_samples(self, extra_fetches=None, feed_dict=None):\n        """"""Returns sequence samples and extra results.\n\n        Args:\n            extra_fetches (dict, optional): Extra tensors to fetch values,\n                besides `samples` and `sequence_length`. Same as the\n                `fetches` argument of\n                :tf_main:`tf.Session.run <Session#run>` and\n                tf_main:`partial_run <Session#partial_run>`.\n            feed_dict (dict, optional): A `dict` that maps tensor to\n                values. Note that all placeholder values used in\n                :meth:`get_samples` and subsequent :meth:`observe` calls\n                should be fed here.\n\n        Returns:\n            A `dict` with keys **""samples""** and **""sequence_length""**\n            containing the fetched values of :attr:`samples` and\n            :attr:`sequence_length`, as well as other fetched values\n            as specified in :attr:`extra_fetches`.\n\n        Example:\n\n            .. code-block:: python\n\n                extra_fetches = {\'truth_ids\': data_batch[\'text_ids\']}\n                vals = agent.get_samples()\n                sample_text = tx.utils.map_ids_to_strs(vals[\'samples\'], vocab)\n                truth_text = tx.utils.map_ids_to_strs(vals[\'truth_ids\'], vocab)\n                reward = reward_fn_in_python(truth_text, sample_text)\n        """"""\n        if self._sess is None:\n            raise ValueError(""`sess` must be specified before sampling."")\n\n        self._check_extra_fetches(extra_fetches)\n\n        # Sets up partial_run\n        fetch_values = None\n        if extra_fetches is not None:\n            fetch_values = list(extra_fetches.values())\n        feeds = None\n        if feed_dict is not None:\n            feeds = list(feed_dict.keys())\n        self._setup_partial_run(fetches=fetch_values, feeds=feeds)\n\n        # Runs the sampling\n        fetches = {\n            ""samples"": self._samples,\n            ""sequence_length"": self._sequence_length\n        }\n        if extra_fetches is not None:\n            fetches.update(extra_fetches)\n\n        feed_dict_ = feed_dict\n\n        vals = self._sess.partial_run(\n            self._partial_run_handle, fetches, feed_dict=feed_dict_)\n\n        self._samples_py = vals[\'samples\']\n        self._sequence_length_py = vals[\'sequence_length\']\n\n        return vals\n\n    def observe(self, reward, train_policy=True, compute_loss=True):\n        """"""Observes the reward, and updates the policy or computes loss\n        accordingly.\n\n        Args:\n            reward: A Python array/list of shape `[batch_size]` containing\n                the reward for the samples generated in last call of\n                :meth:`get_samples`.\n            train_policy (bool): Whether to update the policy model according\n                to the reward.\n            compute_loss (bool): If `train_policy` is False, whether to\n                compute the policy gradient loss (but does not update the\n                policy).\n\n        Returns:\n            If `train_policy` or `compute_loss` is True, returns the loss\n            (a python float scalar). Otherwise returns `None`.\n        """"""\n        self._rewards = reward\n\n        if train_policy:\n            return self._train_policy()\n        elif compute_loss:\n            return self._evaluate_pg_loss()\n        else:\n            return None\n\n    def _get_qvalues(self):\n        qvalues = discount_reward(\n            self._rewards,\n            self._sequence_length_py,\n            discount=self._hparams.discount_factor,\n            normalize=self._hparams.normalize_reward)\n        return qvalues\n\n    def _evaluate_pg_loss(self):\n        fetches = {\n            ""loss"": self._pg_loss\n        }\n\n        feed_dict_ = None\n        if not self._qvalue_inputs_fed:\n            qvalues = self._get_qvalues()\n            feed_dict_ = {self._qvalue_inputs: qvalues}\n\n        vals = self._sess.partial_run(\n            self._partial_run_handle, fetches, feed_dict=feed_dict_)\n\n        self._qvalue_inputs_fed = True\n\n        return vals[\'loss\']\n\n    def _train_policy(self):\n        """"""Updates the policy.\n        """"""\n        fetches = {\n            ""loss"": self._train_op,\n        }\n\n        feed_dict_ = None\n        if not self._qvalue_inputs_fed:\n            qvalues = self._get_qvalues()\n            feed_dict_ = {self._qvalue_inputs: qvalues}\n\n        vals = self._sess.partial_run(\n            self._partial_run_handle, fetches, feed_dict=feed_dict_)\n\n        self._qvalue_inputs_fed = True\n\n        return vals[\'loss\']\n\n    @property\n    def sess(self):\n        """"""The tf session.\n        """"""\n        return self._sess\n\n    @sess.setter\n    def sess(self, sess):\n        self._sess = sess\n\n    @property\n    def pg_loss(self):\n        """"""The scalar tensor of policy gradient loss.\n        """"""\n        return self._pg_loss\n\n    @property\n    def sequence_length(self):\n        """"""The tensor of sample sequence length, of shape `[batch_size]`.\n        """"""\n        return self._sequence_length\n\n    @property\n    def samples(self):\n        """"""The tensor of sequence samples.\n        """"""\n        return self._samples\n\n    @property\n    def logits(self):\n        """"""The tensor of sequence logits.\n        """"""\n        return self._logits\n'"
texar/tf/core/__init__.py,4,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar core.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.core.layers import *\nfrom texar.tf.core.replay_memories import *\nfrom texar.tf.core.explorations import *\nfrom texar.tf.core.optimization import *\n'"
texar/tf/core/explorations.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nClasses and utilities for exploration in RL.\n""""""\n\nfrom texar.tf.hyperparams import HParams\n\n# pylint: disable=invalid-name\n\n__all__ = [\n    ""ExplorationBase"",\n    ""EpsilonLinearDecayExploration""\n]\n\n\nclass ExplorationBase(object):\n    """"""Base class inherited by all exploration classes.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters are set to default values. See\n            :meth:`default_hparams` for the defaults.\n    """"""\n    def __init__(self, hparams=None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a `dict` of hyperparameters and their default values.\n\n        .. code-block:: python\n\n            {\n                \'name\': \'exploration_base\'\n            }\n        """"""\n        return {\n            \'name\': \'exploration_base\'\n        }\n\n    def get_epsilon(self, timestep):\n        """"""Returns the epsilon value.\n\n        Args:\n            timestep (int): The time step.\n\n        Returns:\n            float: the epsilon value.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def hparams(self):\n        """"""The hyperparameter.\n        """"""\n        return self._hparams\n\n\nclass EpsilonLinearDecayExploration(ExplorationBase):\n    """"""Decays epsilon linearly.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters are set to default values. See\n            :meth:`default_hparams` for the defaults.\n    """"""\n    def __init__(self, hparams=None):\n        ExplorationBase.__init__(self, hparams=hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a `dict` of hyperparameters and their default values.\n\n        .. code-block:: python\n\n            {\n                \'initial_epsilon\': 0.1,\n                \'final_epsilon\': 0.0,\n                \'decay_timesteps\': 20000,\n                \'start_timestep\': 0,\n                \'name\': \'epsilon_linear_decay_exploration\',\n            }\n\n        This specifies the decay process that starts at\n        ""start_timestep"" with the value ""initial_epsilon"", and decays for\n        steps ""decay_timesteps"" to reach the final epsilon value\n        ""final_epsilon"".\n        """"""\n        return {\n            \'name\': \'epsilon_linear_decay_exploration\',\n            \'initial_epsilon\': 0.1,\n            \'final_epsilon\': 0.0,\n            \'decay_timesteps\': 20000,\n            \'start_timestep\': 0\n        }\n\n    def get_epsilon(self, timestep):\n        nsteps = self._hparams.decay_timesteps\n        st = self._hparams.start_timestep\n        et = st + nsteps\n\n        if timestep <= st:\n            return self._hparams.initial_epsilon\n        if timestep > et:\n            return self._hparams.final_epsilon\n        r = (timestep - st) * 1.0 / nsteps\n        epsilon = (1 - r) * self._hparams.initial_epsilon + \\\n                r * self._hparams.final_epsilon\n\n        return epsilon\n'"
texar/tf/core/layers.py,108,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious neural network layers\n""""""\n\nimport copy\n\nimport tensorflow as tf\nimport tensorflow.contrib.rnn as rnn\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.dtypes import is_str\nfrom texar.tf.utils.variables import add_variable\nfrom texar.tf.utils.mode import is_train_mode, switch_dropout\n\n# pylint: disable=redefined-variable-type, invalid-name\n# pylint: disable=too-many-branches, too-many-arguments, too-many-lines\n# pylint: disable=protected-access\n\n__all__ = [\n    ""default_rnn_cell_hparams"",\n    ""get_rnn_cell"",\n    ""get_rnn_cell_trainable_variables"",\n    ""default_regularizer_hparams"",\n    ""get_regularizer"",\n    ""get_initializer"",\n    ""get_activation_fn"",\n    ""get_constraint_fn"",\n    ""get_layer"",\n    ""_ReducePooling1D"",\n    ""MaxReducePooling1D"",\n    ""AverageReducePooling1D"",\n    ""get_pooling_layer_hparams"",\n    ""MergeLayer"",\n    ""SequentialLayer"",\n    ""default_conv1d_kwargs"",\n    ""default_conv2d_kwargs"",\n    ""default_conv3d_kwargs"",\n    ""default_conv2d_transpose_kwargs"",\n    ""default_conv3d_transpose_kwargs"",\n    ""default_dense_kwargs"",\n    ""default_dropout_kwargs"",\n    ""default_flatten_kwargs"",\n    ""default_max_pooling1d_kwargs"",\n    ""default_max_pooling2d_kwargs"",\n    ""default_max_pooling3d_kwargs"",\n    ""default_separable_conv2d_kwargs"",\n    ""default_batch_normalization_kwargs"",\n    ""default_average_pooling1d_kwargs"",\n    ""default_average_pooling2d_kwargs"",\n    ""default_average_pooling3d_kwargs"",\n    ""layer_normalize"",\n]\n\n\ndef default_rnn_cell_hparams():\n    r""""""Returns a `dict` of RNN cell hyperparameters and their default values.\n\n    .. code-block:: python\n\n        {\n            ""type"": ""LSTMCell"",\n            ""kwargs"": {\n                ""num_units"": 256\n            },\n            ""num_layers"": 1,\n            ""dropout"": {\n                ""input_keep_prob"": 1.0,\n                ""output_keep_prob"": 1.0,\n                ""state_keep_prob"": 1.0,\n                ""variational_recurrent"": False,\n                ""input_size"": []\n            },\n            ""residual"": False,\n            ""highway"": False,\n        }\n\n    Here:\n\n    `""type""`: str or cell class or cell instance\n        The RNN cell type. This can be\n\n        - The string name or full module path of a cell class. If class name is\n          provided, the class must be in module\n          :tf_main:`tf.nn.rnn_cell <nn/rnn_cell/LSTMCell>`,\n          :tf_main:`tf.contrib.rnn <contrib/rnn>`, or :mod:`texar.tf.custom`.\n        - A cell class.\n        - An instance of a cell class. This is not valid if `""num_layers""` > 1.\n\n        For example\n\n        .. code-block:: python\n\n            ""type"": ""LSTMCell"" # class name\n            ""type"": ""tensorflow.contrib.rnn.Conv1DLSTMCell"" # module path\n            ""type"": ""my_module.MyCell"" # module path\n            ""type"": tf.nn.rnn_cell.GRUCell # class\n            ""type"": BasicRNNCell(num_units=100) # cell instance\n            ""type"": MyCell(...) # cell instance\n\n    `""kwargs""`: dict\n        Keyword arguments for the constructor of the cell class.\n        A cell is created by :python:`cell_class(**kwargs)`, where\n        `cell_class` is specified in ""type"" above.\n\n        Ignored if ""type"" is a cell instance.\n\n    `""num_layers""`: int\n        Number of cell layers. Each layer is a cell created as above, with\n        the same hyperparameters specified in `""kwargs""`.\n\n    `""dropout""`: dict\n        Dropout applied to the cell in **each** layer. See\n        :tf_main:`DropoutWrapper <contrib/rnn/DropoutWrapper>` for details of\n        the hyperparameters. If all `""\\*_keep_prob""` = 1, no dropout is applied.\n\n        Specifically, if `""variational_recurrent""` = `True`,\n        the same dropout mask is applied across all time steps per run call.\n        If `True`, `""input_size""` is required, which is a list of input\n        size of each cell layer. The input size of a cell layer is the last\n        dimension size of its input tensor. For example, the\n        input size of the first layer is usually the dimension of\n        word embeddings, while the input size of subsequent layers\n        are usually the `num_units` of the preceding-layer cell. E.g.,\n\n        .. code-block:: python\n\n            # Assume embedding_dim = 100\n            ""type"": ""LSTMCell"",\n            ""kwargs"": { ""num_units"": 123 },\n            ""num_layers"": 3,\n            ""dropout"": {\n                ""output_keep_prob"": 0.5,\n                ""variational_recurrent"": True,\n                ""input_size"": [100, 123, 123]\n            }\n\n    `""residual""`: bool\n        If `True`, apply residual connection on the inputs and\n        outputs of cell in **each** layer except the first layer. Ignored\n        if `""num_layers""` = 1.\n\n    `""highway""`: bool\n        If True, apply highway connection on the inputs and\n        outputs of cell in each layer except the first layer. Ignored if\n        `""num_layers""` = 1.\n    """"""\n    return {\n        ""type"": ""LSTMCell"",\n        ""kwargs"": {\n            ""num_units"": 256,\n        },\n        ""num_layers"": 1,\n        ""dropout"": {\n            ""input_keep_prob"": 1.0,\n            ""output_keep_prob"": 1.0,\n            ""state_keep_prob"": 1.0,\n            ""variational_recurrent"": False,\n            ""input_size"": [],\n            ""@no_typecheck"": [\n                ""input_keep_prob"", ""output_keep_prob"", ""state_keep_prob""\n            ]\n        },\n        ""residual"": False,\n        ""highway"": False,\n        ""@no_typecheck"": [""type""]\n    }\n\n\ndef get_rnn_cell(hparams=None, mode=None):\n    """"""Creates an RNN cell.\n\n    See :func:`~texar.tf.core.default_rnn_cell_hparams` for all\n    hyperparameters and default values.\n\n    Args:\n        hparams (dict or HParams, optional): Cell hyperparameters. Missing\n            hyperparameters are set to default values.\n        mode (optional): A Tensor taking value in\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n            `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout will be\n            controlled by :func:`texar.tf.global_mode`.\n\n    Returns:\n        A cell instance.\n\n    Raises:\n        ValueError: If hparams[""num_layers""]>1 and hparams[""type""] is a class\n            instance.\n        ValueError: The cell is not an\n            :tf_main:`RNNCell <contrib/rnn/RNNCell>` instance.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(hparams, default_rnn_cell_hparams())\n\n    d_hp = hparams[""dropout""]\n    if d_hp[""variational_recurrent""] and \\\n            len(d_hp[""input_size""]) != hparams[""num_layers""]:\n        raise ValueError(\n            ""If variational_recurrent=True, input_size must be a list of ""\n            ""num_layers(%d) integers. Got len(input_size)=%d."" %\n            (hparams[""num_layers""], len(d_hp[""input_size""])))\n\n    cells = []\n    cell_kwargs = hparams[""kwargs""].todict()\n    num_layers = hparams[""num_layers""]\n    for layer_i in range(num_layers):\n        # Create the basic cell\n        cell_type = hparams[""type""]\n        if not is_str(cell_type) and not isinstance(cell_type, type):\n            if num_layers > 1:\n                raise ValueError(\n                    ""If \'num_layers\'>1, then \'type\' must be a cell class or ""\n                    ""its name/module path, rather than a cell instance."")\n        cell_modules = [\'tensorflow.nn.rnn_cell\', \'tensorflow.contrib.rnn\',\n                        \'texar.tf.custom\']\n        cell = utils.check_or_get_instance(\n            cell_type, cell_kwargs, cell_modules, rnn.RNNCell)\n\n        # Optionally add dropout\n        if d_hp[""input_keep_prob""] < 1.0 or \\\n                d_hp[""output_keep_prob""] < 1.0 or \\\n                d_hp[""state_keep_prob""] < 1.0:\n            vr_kwargs = {}\n            if d_hp[""variational_recurrent""]:\n                vr_kwargs = {\n                    ""variational_recurrent"": True,\n                    ""input_size"": d_hp[""input_size""][layer_i],\n                    ""dtype"": tf.float32\n                }\n            input_keep_prob = switch_dropout(d_hp[""input_keep_prob""],\n                                             mode)\n            output_keep_prob = switch_dropout(d_hp[""output_keep_prob""],\n                                              mode)\n            state_keep_prob = switch_dropout(d_hp[""state_keep_prob""],\n                                             mode)\n            cell = rnn.DropoutWrapper(\n                cell=cell,\n                input_keep_prob=input_keep_prob,\n                output_keep_prob=output_keep_prob,\n                state_keep_prob=state_keep_prob,\n                **vr_kwargs)\n\n        # Optionally add residual and highway connections\n        if layer_i > 0:\n            if hparams[""residual""]:\n                cell = rnn.ResidualWrapper(cell)\n            if hparams[""highway""]:\n                cell = rnn.HighwayWrapper(cell)\n\n        cells.append(cell)\n\n    if hparams[""num_layers""] > 1:\n        cell = rnn.MultiRNNCell(cells)\n    else:\n        cell = cells[0]\n\n    return cell\n\n\ndef get_rnn_cell_trainable_variables(cell):\n    """"""Returns the list of trainable variables of an RNN cell.\n\n    Args:\n        cell: an instance of :tf_main:`RNNCell <nn/rnn_cell/RNNCell>`.\n\n    Returns:\n        list: trainable variables of the cell.\n    """"""\n    cell_ = cell\n    while True:\n        try:\n            return cell_.trainable_variables\n        except AttributeError:\n            # Cell wrappers (e.g., `DropoutWrapper`) cannot directly access to\n            # `trainable_variables` as they don\'t initialize superclass\n            # (tf==v1.3). So try to access through the cell in the wrapper.\n            cell_ = cell._cell  # pylint: disable=protected-access\n\n\ndef default_regularizer_hparams():\n    """"""Returns the hyperparameters and their default values of a variable\n    regularizer:\n\n    .. code-block:: python\n\n        {\n            ""type"": ""L1L2"",\n            ""kwargs"": {\n                ""l1"": 0.,\n                ""l2"": 0.\n            }\n        }\n\n    The default value corresponds to :tf_main:`L1L2 <keras/regularizers/L1L2>`\n    and, with `(l1=0, l2=0)`, disables regularization.\n    """"""\n    return {\n        ""type"": ""L1L2"",\n        ""kwargs"": {\n            ""l1"": 0.,\n            ""l2"": 0.\n        }\n    }\n\n\ndef get_regularizer(hparams=None):\n    """"""Returns a variable regularizer instance.\n\n    See :func:`~texar.tf.core.default_regularizer_hparams` for all\n    hyperparameters and default values.\n\n    The ""type"" field can be a subclass\n    of :tf_main:`Regularizer <keras/regularizers/Regularizer>`, its string name\n    or module path, or a class instance.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters are set to default values.\n\n    Returns:\n        A :tf_main:`Regularizer <keras/regularizers/Regularizer>` instance.\n        `None` if :attr:`hparams` is `None` or taking the default\n        hyperparameter value.\n\n    Raises:\n        ValueError: The resulting regularizer is not an instance of\n            :tf_main:`Regularizer <keras/regularizers/Regularizer>`.\n    """"""\n    if hparams is None:\n        return None\n\n    if isinstance(hparams, dict):\n        hparams = HParams(hparams, default_regularizer_hparams())\n\n    rgl = utils.check_or_get_instance(\n        hparams.type, hparams.kwargs.todict(),\n        [""tensorflow.keras.regularizers"", ""texar.tf.custom""])\n\n    if not isinstance(rgl, tf.keras.regularizers.Regularizer):\n        raise ValueError(""The regularizer must be an instance of ""\n                         ""tf.keras.regularizers.Regularizer."")\n\n    if isinstance(rgl, tf.keras.regularizers.L1L2) and \\\n            rgl.l1 == 0. and rgl.l2 == 0.:\n        return None\n\n    return rgl\n\n\ndef get_initializer(hparams=None):\n    """"""Returns an initializer instance.\n\n    .. role:: python(code)\n       :language: python\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters with the structure\n\n            .. code-block:: python\n\n                {\n                    ""type"": ""initializer_class_or_function"",\n                    ""kwargs"": {\n                        #...\n                    }\n                }\n\n            The ""type"" field can be a initializer class, its name or module\n            path, or class instance. If class name is provided, the class must\n            be from one the following modules:\n            :tf_main:`tf.initializers <initializers>`,\n            :tf_main:`tf.keras.initializers <keras/initializers>`,\n            :tf_main:`tf < >`, and :mod:`texar.tf.custom`. The class is created\n            by :python:`initializer_class(**kwargs)`. If a class instance\n            is given, ""kwargs"" is ignored and can be omitted.\n\n            Besides, the ""type"" field can also be an initialization function\n            called with :python:`initialization_fn(**kwargs)`. In this case\n            ""type"" can be the function, or its name or module path. If\n            function name is provided, the function must be from one of the\n            above modules or module `tf.contrib.layers`. If no\n            keyword argument is required, ""kwargs"" can be omitted.\n\n    Returns:\n        An initializer instance. `None` if :attr:`hparams` is `None`.\n    """"""\n    if hparams is None:\n        return None\n\n    kwargs = hparams.get(""kwargs"", {})\n    if isinstance(kwargs, HParams):\n        kwargs = kwargs.todict()\n    modules = [""tensorflow.initializers"", ""tensorflow.keras.initializers"",\n               ""tensorflow"", ""texar.tf.custom""]\n    try:\n        initializer = utils.check_or_get_instance(hparams[""type""], kwargs,\n                                                  modules)\n    except (TypeError, ValueError):\n        modules = [\'tensorflow.contrib.layers\'] + modules\n        initializer_fn = utils.get_function(hparams[""type""], modules)\n        initializer = initializer_fn(**kwargs)\n\n    return initializer\n\n\ndef get_activation_fn(fn_name=""identity"", kwargs=None):\n    """"""Returns an activation function `fn` with the signature\n    `output = fn(input)`.\n\n    If the function specified by :attr:`fn_name` has more than one arguments\n    without default values, then all these arguments except the input feature\n    argument must be specified in :attr:`kwargs`. Arguments with default values\n    can also be specified in :attr:`kwargs` to take values other than the\n    defaults. In this case a partial function is returned with the above\n    signature.\n\n    Args:\n        fn_name (str or callable): An activation function, or its name or\n            module path. The function can be:\n\n            - Built-in function defined in :tf_main:`tf < >` or\n              :tf_main:`tf.nn <nn>`, e.g., :tf_main:`tf.identity <identity>`.\n            - User-defined activation functions in module\n              :mod:`texar.tf.custom`.\n            - External activation functions. Must provide the full module path,\n              e.g., ""my_module.my_activation_fn"".\n\n        kwargs (optional): A `dict` or instance of :class:`~texar.tf.HParams`\n            containing the keyword arguments of the activation function.\n\n    Returns:\n        An activation function. `None` if :attr:`fn_name` is `None`.\n    """"""\n    if fn_name is None:\n        return None\n\n    fn_modules = [\'tensorflow\', \'tensorflow.nn\', \'texar.tf.custom\',\n                  \'texar.tf.core.layers\']\n    activation_fn_ = utils.get_function(fn_name, fn_modules)\n    activation_fn = activation_fn_\n\n    # Make a partial function if necessary\n    if kwargs is not None:\n        if isinstance(kwargs, HParams):\n            kwargs = kwargs.todict()\n\n        def _partial_fn(features):\n            return activation_fn_(features, **kwargs)\n        activation_fn = _partial_fn\n\n    return activation_fn\n\n\ndef get_constraint_fn(fn_name=""NonNeg""):\n    """"""Returns a constraint function.\n\n    .. role:: python(code)\n       :language: python\n\n    The function must follow the signature:\n    :python:`w_ = constraint_fn(w)`.\n\n    Args:\n        fn_name (str or callable): The name or full path to a\n            constraint function, or the function itself.\n\n            The function can be:\n\n            - Built-in constraint functions defined in modules \\\n            :tf_main:`tf.keras.constraints <keras/constraints>` \\\n            (e.g., :tf_main:`NonNeg <keras/constraints/NonNeg>`) \\\n            or :tf_main:`tf < >` or :tf_main:`tf.nn <nn>` \\\n            (e.g., activation functions).\n            - User-defined function in :mod:`texar.tf.custom`.\n            - Externally defined function. Must provide the full path, \\\n            e.g., `""my_module.my_constraint_fn""`.\n\n            If a callable is provided, then it is returned directly.\n\n    Returns:\n        The constraint function. `None` if :attr:`fn_name` is `None`.\n    """"""\n    if fn_name is None:\n        return None\n\n    fn_modules = [\'tensorflow.keras.constraints\', \'tensorflow\',\n                  \'tensorflow.nn\', \'texar.tf.custom\']\n    constraint_fn = utils.get_function(fn_name, fn_modules)\n    return constraint_fn\n\n\ndef get_layer(hparams):\n    r""""""Makes a layer instance.\n\n    The layer must be an instance of :tf_main:`tf.layers.Layer <layers/Layer>`.\n\n    Args:\n        hparams (dict or HParams): Hyperparameters of the layer, with\n            structure:\n\n            .. code-block:: python\n\n                {\n                    ""type"": ""LayerClass"",\n                    ""kwargs"": {\n                        # Keyword arguments of the layer class\n                        # ...\n                    }\n                }\n\n            Here:\n\n            `""type""`: str or layer class or layer instance\n                The layer type. This can be\n\n                - The string name or full module path of a layer class. If\n                  the class name is provided, the class must be in module\n                  :tf_main:`tf.layers <layers>`, :mod:`texar.tf.core`,\n                  or :mod:`texar.tf.custom`.\n                - A layer class.\n                - An instance of a layer class.\n\n                For example\n\n                .. code-block:: python\n\n                    ""type"": ""Conv1D"" # class name\n                    ""type"": ""texar.tf.core.MaxReducePooling1D"" # module path\n                    ""type"": ""my_module.MyLayer"" # module path\n                    ""type"": tf.layers.Conv2D # class\n                    ""type"": Conv1D(filters=10, kernel_size=2) # cell instance\n                    ""type"": MyLayer(...) # cell instance\n\n            `""kwargs""`: dict\n                A dictionary of keyword arguments for constructor of the\n                layer class. Ignored if :attr:`""type""` is a layer instance.\n\n                - Arguments named ""activation"" can be a callable,\n                  or a `str` of the name or module path to the activation\n                  function.\n                - Arguments named ""\\*_regularizer"" and ""\\*_initializer""\n                  can be a class instance, or a `dict` of hyperparameters of\n                  respective regularizers and initializers. See\n                - Arguments named ""\\*_constraint"" can be a callable, or a\n                  `str` of the name or full path to the constraint function.\n\n    Returns:\n        A layer instance. If ``hparams[""type""]`` is a layer instance, returns it\n        directly.\n\n    Raises:\n        ValueError: If :attr:`hparams` is `None`.\n        ValueError: If the resulting layer is not an instance of\n            :tf_main:`tf.layers.Layer <layers/Layer>`.\n    """"""\n    if hparams is None:\n        raise ValueError(""`hparams` must not be `None`."")\n\n    layer_type = hparams[""type""]\n    if not is_str(layer_type) and not isinstance(layer_type, type):\n        layer = layer_type\n    else:\n        layer_modules = [""tensorflow.layers"", ""texar.tf.core"",\n                         ""texar.tf.custom""]\n        layer_class = utils.check_or_get_class(layer_type, layer_modules)\n        if isinstance(hparams, dict):\n            default_kwargs = _layer_class_to_default_kwargs_map.get(layer_class,\n                                                                    {})\n            default_hparams = {""type"": layer_type, ""kwargs"": default_kwargs}\n            hparams = HParams(hparams, default_hparams)\n\n        kwargs = {}\n        for k, v in hparams.kwargs.items():\n            if k.endswith(\'_regularizer\'):\n                kwargs[k] = get_regularizer(v)\n            elif k.endswith(\'_initializer\'):\n                kwargs[k] = get_initializer(v)\n            elif k.endswith(\'activation\'):\n                kwargs[k] = get_activation_fn(v)\n            elif k.endswith(\'_constraint\'):\n                kwargs[k] = get_constraint_fn(v)\n            else:\n                kwargs[k] = v\n        layer = utils.get_instance(layer_type, kwargs, layer_modules)\n\n    if not isinstance(layer, tf.layers.Layer):\n        raise ValueError(""layer must be an instance of `tf.layers.Layer`."")\n\n    return layer\n\n\ndef _compute_concat_output_shape(input_shape, axis):\n    """"""Infers the output shape of concat given the input shape.\n\n    The code is adapted from the ConcatLayer of lasagne\n    (https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/merge.py)\n\n    Args:\n        input_shape (list): A list of shapes, each of which is in turn a\n            list or TensorShape.\n        axis (int): Axis of the concat operation.\n\n    Returns:\n        list: Output shape of concat.\n    """"""\n    # The size of each axis of the output shape equals the first\n    # input size of respective axis that is not `None`\n    input_shape = [tf.TensorShape(s).as_list() for s in input_shape]\n    output_shape = [next((s for s in sizes if s is not None), None)\n                    for sizes in zip(*input_shape)]\n    axis_sizes = [s[axis] for s in input_shape]\n    concat_axis_size = None if any(s is None for s in axis_sizes) \\\n            else sum(axis_sizes)\n    output_shape[axis] = concat_axis_size\n    return output_shape\n\n\nclass _ReducePooling1D(tf.layers.Layer):\n    """"""Pooling layer for arbitrary reduce functions for 1D inputs.\n\n    The same as `tf.python.layers.pooling._Pooling1D` except that the pooling\n    dimension is entirely reduced (i.e., `pool_size=length`).\n\n    This class is for code reuse, rather than an exposed API.\n    """"""\n    def __init__(self, reduce_function, data_format=\'channels_last\',\n                 name=None, **kwargs):\n        super(_ReducePooling1D, self).__init__(name=name, **kwargs)\n        self._reduce_function = reduce_function\n        if data_format not in {\'channels_last\', \'channels_first\'}:\n            raise ValueError(""`data_format must be either \'channels_last\' or` ""\n                             ""\'channels_first\'. Got: {}"".format(data_format))\n        self._data_format = data_format\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n        if self._data_format == \'channels_last\':\n            return tf.TensorShape([input_shape[0], input_shape[2]])\n        else:\n            return tf.TensorShape([input_shape[0], input_shape[1]])\n\n    def call(self, inputs):\n        if self._data_format == \'channels_last\':\n            return self._reduce_function(inputs, axis=1)\n        else:\n            return self._reduce_function(inputs, axis=2)\n\n\nclass MaxReducePooling1D(_ReducePooling1D):\n    """"""A subclass of :tf_main:`tf.layers.Layer <layers/Layer>`.\n    Max Pooling layer for 1D inputs. The same as\n    :tf_main:`MaxPooling1D <layers/MaxPooling1D>` except that the pooling\n    dimension is entirely reduced (i.e., `pool_size=input_length`).\n    """"""\n    def __init__(self, data_format=\'channels_last\', name=None, **kwargs):\n        super(MaxReducePooling1D, self).__init__(\n            tf.reduce_max, data_format=data_format, name=name, **kwargs)\n\n\nclass AverageReducePooling1D(_ReducePooling1D):\n    """"""A subclass of :tf_main:`tf.layers.Layer <layers/Layer>`.\n    Average Pooling layer for 1D inputs. The same as\n    :tf_main:`AveragePooling1D <layers/AveragePooling1D>` except that the\n    pooling dimension is entirely reduced (i.e., `pool_size=input_length`).\n    """"""\n    def __init__(self, data_format=\'channels_last\', name=None, **kwargs):\n        super(AverageReducePooling1D, self).__init__(\n            tf.reduce_mean, data_format=data_format, name=name, **kwargs)\n\n\n_POOLING_TO_REDUCE = {\n    ""MaxPooling1D"": ""MaxReducePooling1D"",\n    ""AveragePooling1D"": ""AverageReducePooling1D"",\n    tf.layers.MaxPooling1D: MaxReducePooling1D,\n    tf.layers.AveragePooling1D: AverageReducePooling1D\n}\n\n\ndef get_pooling_layer_hparams(hparams):\n    """"""Creates pooling layer hparams `dict` usable for :func:`get_layer`.\n\n    If the :attr:`hparams` sets `\'pool_size\'` to `None`, the layer will be\n    changed to the respective reduce-pooling layer. For example,\n    :class:`tf.layers.MaxPooling1D <layers/MaxPooling1D>` is replaced with\n    :class:`~texar.tf.core.MaxReducePooling1D`.\n    """"""\n    if isinstance(hparams, HParams):\n        hparams = hparams.todict()\n\n    new_hparams = copy.copy(hparams)\n    kwargs = new_hparams.get(\'kwargs\', None)\n\n    if kwargs and kwargs.get(\'pool_size\', None) is None:\n        pool_type = hparams[\'type\']\n        new_hparams[\'type\'] = _POOLING_TO_REDUCE.get(pool_type, pool_type)\n        kwargs.pop(\'pool_size\', None)\n        kwargs.pop(\'strides\', None)\n        kwargs.pop(\'padding\', None)\n\n    return new_hparams\n\n\nclass MergeLayer(tf.layers.Layer):\n    """"""A subclass of :tf_main:`tf.layers.Layer <layers/Layer>`.\n    A layer that consists of multiple layers in parallel. Input is fed to\n    each of the parallel layers, and the outputs are merged with a\n    specified mode.\n\n    Args:\n        layers (list, optional): A list of :tf_main:`tf.layers.Layer\n            <layers/layer>` instances, or a list of hyperparameter dicts\n            each of which specifies type and kwargs of each layer (see\n            the `hparams` argument of :func:`get_layer`).\n\n            If `None`, this layer degenerates to a merging operator that merges\n            inputs directly.\n        mode (str): Mode of the merge op. This can be:\n\n            - :attr:`\'concat\'`: Concatenates layer outputs along one axis. \\\n              Tensors must have the same shape except for the dimension \\\n              specified in `axis`, which can have different sizes.\n            - :attr:`\'elemwise_sum\'`: Outputs element-wise sum.\n            - :attr:`\'elemwise_mul\'`: Outputs element-wise product.\n            - :attr:`\'sum\'`: Computes the sum of layer outputs along the \\\n              dimension given by `axis`. E.g., given `axis=1`, \\\n              two tensors of shape `[a, b]` and `[a, c]` respectively \\\n              will result in a merged tensor of shape `[a]`.\n            - :attr:`\'mean\'`: Computes the mean of layer outputs along the \\\n              dimension given in `axis`.\n            - :attr:`\'prod\'`: Computes the product of layer outputs along the \\\n              dimension given in `axis`.\n            - :attr:`\'max\'`: Computes the maximum of layer outputs along the \\\n              dimension given in `axis`.\n            - :attr:`\'min\'`: Computes the minimum of layer outputs along the \\\n              dimension given in `axis`.\n            - :attr:`\'and\'`: Computes the `logical and` of layer outputs along \\\n              the dimension given in `axis`.\n            - :attr:`\'or\'`: Computes the `logical or` of layer outputs along \\\n              the dimension given in `axis`.\n            - :attr:`\'logsumexp\'`: Computes \\\n              log(sum(exp(elements across the dimension of layer outputs)))\n        axis (int): The axis to use in merging. Ignored in modes\n            :attr:`\'elemwise_sum\'` and :attr:`\'elemwise_mul\'`.\n        trainable (bool): Whether the layer should be trained.\n        name (str, optional): Name of the layer.\n    """"""\n\n    def __init__(self,\n                 layers=None,\n                 mode=\'concat\',\n                 axis=1,\n                 trainable=True,\n                 name=None,\n                 **kwargs):\n        super(MergeLayer, self).__init__(\n            trainable=trainable, name=name, **kwargs)\n        self._mode = mode\n        self._axis = axis\n\n        self._layers = None\n        if layers is not None:\n            if len(layers) == 0:\n                raise ValueError(\n                    ""\'layers\' must be either None or a non-empty list."")\n            self._layers = []\n            for layer in layers:\n                if isinstance(layer, tf.layers.Layer):\n                    self._layers.append(layer)\n                else:\n                    self._layers.append(get_layer(hparams=layer))\n\n        # Keep tracks of whether trainable variables have been created\n        self._vars_built = False\n\n    def compute_output_shape(self, input_shape):\n        if self._layers is None:\n            _shapes = input_shape\n            if not isinstance(_shapes, (list, tuple)):\n                _shapes = [_shapes]\n        else:\n            _shapes = []\n            for layer in self._layers:\n                layer_output_shape = layer.compute_output_shape(input_shape)\n                _shapes.append(layer_output_shape)\n        _shapes = [tf.TensorShape(s) for s in _shapes]\n\n        if self._mode == \'concat\':\n            output_shape = _compute_concat_output_shape(_shapes, self._axis)\n        elif self._mode in [\'sum\', \'mean\', \'prod\', \'max\', \'min\',\n                            \'and\', \'or\', \'logsumexp\']:\n            output_shape = _compute_concat_output_shape(_shapes, self._axis)\n            output_shape.pop(self._axis)\n        elif self._mode in [\'elemwise_sum\', \'elemwise_mul\']:\n            # Simply infer the output shape as the input shape of highest rank\n            _ranks = [s.ndims for s in _shapes]\n            max_rank = max(_ranks)\n            max_ranked_shapes = []\n            for i, s in enumerate(_shapes):\n                if _ranks[i] == max_rank:\n                    max_ranked_shapes.append(s.as_list())\n            # Grab the first size of each axis that is not `None`\n            output_shape = [next((s for s in sizes if s is not None), None)\n                            for sizes in zip(*max_ranked_shapes)]\n        else:\n            raise ValueError(""Unknown merge mode: \'%s\'"" % self._mode)\n\n        return tf.TensorShape(output_shape)\n\n    def _collect_weights(self):\n        """"""Collects (non-)trainable weights of each of the parallel layers.\n        """"""\n        if self._layers is None:\n            pass\n        for layer in self._layers:\n            if self.trainable:\n                add_variable(\n                    layer._trainable_weights, self._trainable_weights)\n            else:\n                add_variable(\n                    layer._trainable_weights, self._non_trainable_weights)\n            add_variable(\n                layer._non_trainable_weights, self._non_trainable_weights)\n\n    @property\n    def trainable_weights(self):\n        return self._trainable_weights\n\n    @property\n    def non_trainable_weights(self):\n        return self._non_trainable_weights\n\n    def call(self, inputs):\n        if self._layers is None:\n            layer_outputs = inputs\n            if not isinstance(layer_outputs, (list, tuple)):\n                layer_outputs = [layer_outputs]\n        else:\n            layer_outputs = []\n            for layer in self._layers:\n                layer_output = layer(inputs)\n                layer_outputs.append(layer_output)\n\n        if self._mode == \'concat\':\n            outputs = tf.concat(values=layer_outputs, axis=self._axis)\n        elif self._mode == \'elemwise_sum\':\n            outputs = layer_outputs[0]\n            for i in range(1, len(layer_outputs)):\n                outputs = tf.add(outputs, layer_outputs[i])\n        elif self._mode == \'elemwise_mul\':\n            outputs = layer_outputs[0]\n            for i in range(1, len(layer_outputs)):\n                outputs = tf.multiply(outputs, layer_outputs[i])\n        elif self._mode == \'sum\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_sum(_concat, axis=self._axis)\n        elif self._mode == \'mean\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_mean(_concat, axis=self._axis)\n        elif self._mode == \'prod\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_prod(_concat, axis=self._axis)\n        elif self._mode == \'max\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_max(_concat, axis=self._axis)\n        elif self._mode == \'min\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_min(_concat, axis=self._axis)\n        elif self._mode == \'and\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_all(_concat, axis=self._axis)\n        elif self._mode == \'or\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_any(_concat, axis=self._axis)\n        elif self._mode == \'logsumexp\':\n            _concat = tf.concat(values=layer_outputs, axis=self._axis)\n            outputs = tf.reduce_logsumexp(_concat, axis=self._axis)\n        else:\n            raise ValueError(""Unknown merge mode: \'%s\'"" % self._mode)\n\n        if not self.built or not self._vars_built:\n            self._collect_weights()\n            self._vars_built = True\n\n        return outputs\n\n    @property\n    def layers(self):\n        """"""The list of parallel layers.\n        """"""\n        return self._layers\n\n\nclass SequentialLayer(tf.layers.Layer):\n    """"""A subclass of :tf_main:`tf.layers.Layer <layers/Layer>`.\n    A layer that consists of multiple layers connected sequentially.\n\n    Args:\n        layers (list): A list of :tf_main:`tf.layers.Layer\n            <layers/layer>` instances, or a list of hyperparameter dicts\n            each of which specifying type and kwargs of each layer (see\n            the `hparams` argument of :func:`get_layer`). The layers are\n            connected sequentially.\n    """"""\n    def __init__(self,\n                 layers,\n                 trainable=True,\n                 name=None,\n                 **kwargs):\n        super(SequentialLayer, self).__init__(\n            trainable=trainable, name=name, **kwargs)\n\n        if len(layers) == 0:\n            raise ValueError(""\'layers\' must be a non-empty list."")\n        self._layers = []\n        for layer in layers:\n            if isinstance(layer, tf.layers.Layer):\n                self._layers.append(layer)\n            else:\n                self._layers.append(get_layer(hparams=layer))\n\n        # Keep tracks of whether trainable variables have been created\n        self._vars_built = False\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        for layer in self._layers:\n            output_shape = layer.compute_output_shape(input_shape)\n            input_shape = output_shape\n        return output_shape\n\n    def _collect_weights(self):\n        """"""Collects (non-)trainable weights of each of the layers.\n        """"""\n        for layer in self._layers:\n            if self.trainable:\n                add_variable(\n                    layer._trainable_weights, self._trainable_weights)\n            else:\n                add_variable(\n                    layer._trainable_weights, self._non_trainable_weights)\n            add_variable(\n                layer._non_trainable_weights, self._non_trainable_weights)\n\n    @property\n    def trainable_weights(self):\n        return self._trainable_weights\n\n    @property\n    def non_trainable_weights(self):\n        return self._non_trainable_weights\n\n    def call(self, inputs, mode=None):  # pylint: disable=arguments-differ\n        training = is_train_mode(mode)\n\n        outputs = inputs\n        for layer in self._layers:\n            if isinstance(layer, tf.layers.Dropout) or \\\n                    isinstance(layer, tf.layers.BatchNormalization):\n                outputs = layer(outputs, training=training)\n            else:\n                outputs = layer(inputs)\n            inputs = outputs\n\n        if not self.built or not self._vars_built:\n            self._collect_weights()\n            self._vars_built = True\n\n        return outputs\n\n    @property\n    def layers(self):\n        """"""The list of layers connected sequentially.\n        """"""\n        return self._layers\n\n\ndef _common_default_conv_dense_kwargs():\n    """"""Returns the default keyword argument values that are common to\n    convolution layers.\n    """"""\n    return {\n        ""activation"": None,\n        ""use_bias"": True,\n        ""kernel_initializer"": {\n            ""type"": ""glorot_uniform_initializer"",\n            ""kwargs"": {}\n        },\n        ""bias_initializer"": {\n            ""type"": ""zeros_initializer"",\n            ""kwargs"": {}\n        },\n        ""kernel_regularizer"": default_regularizer_hparams(),\n        ""bias_regularizer"": default_regularizer_hparams(),\n        ""activity_regularizer"": default_regularizer_hparams(),\n        ""kernel_constraint"": None,\n        ""bias_constraint"": None,\n        ""trainable"": True,\n        ""name"": None\n    }\n\n\ndef default_conv1d_kwargs():\n    """"""Returns the default keyword argument values of the constructor\n    of 1D-convolution layer class\n    :tf_main:`tf.layers.Conv1D <layers/Conv1D>`.\n\n    .. code-block:: python\n\n        {\n            ""filters"": 100,\n            ""kernel_size"": 3,\n            ""strides"": 1,\n            ""padding"": \'valid\',\n            ""data_format"": \'channels_last\',\n            ""dilation_rate"": 1\n            ""activation"": ""identity"",\n            ""use_bias"": True,\n            ""kernel_initializer"": {\n                ""type"": ""glorot_uniform_initializer"",\n                ""kwargs"": {}\n            },\n            ""bias_initializer"": {\n                ""type"": ""zeros_initializer"",\n                ""kwargs"": {}\n            },\n            ""kernel_regularizer"": {\n                ""type"": ""L1L2"",\n                ""kwargs"": {\n                    ""l1"": 0.,\n                    ""l2"": 0.\n                }\n            },\n            ""bias_regularizer"": {\n                # same as in ""kernel_regularizer""\n                # ...\n            },\n            ""activity_regularizer"": {\n                # same as in ""kernel_regularizer""\n                # ...\n            },\n            ""kernel_constraint"": None,\n            ""bias_constraint"": None,\n            ""trainable"": True,\n            ""name"": None\n        }\n    """"""\n    kwargs = _common_default_conv_dense_kwargs()\n    kwargs.update({\n        ""kernel_size"": 3,\n        ""filters"": 100,\n        ""strides"": 1,\n        ""dilation_rate"": 1,\n        ""data_format"": ""channels_last""\n    })\n    return kwargs\n\n\ndef default_conv2d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n\n\ndef default_conv3d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n\n\ndef default_conv2d_transpose_kwargs():\n    """"""TODO\n    """"""\n    return {}\n\n\ndef default_conv3d_transpose_kwargs():\n    """"""TODO\n    """"""\n    return {}\n\n\ndef default_dense_kwargs():\n    """"""Returns the default keyword argument values of the constructor\n    of the dense layer class :tf_main:`tf.layers.Dense <layers/Dense>`.\n\n    .. code-block:: python\n\n        {\n            ""units"": 256,\n            ""activation"": ""identity"",\n            ""use_bias"": True,\n            ""kernel_initializer"": {\n                ""type"": ""glorot_uniform_initializer"",\n                ""kwargs"": {}\n            },\n            ""bias_initializer"": {\n                ""type"": ""zeros_initializer"",\n                ""kwargs"": {}\n            },\n            ""kernel_regularizer"": {\n                ""type"": ""L1L2"",\n                ""kwargs"": {\n                    ""l1"": 0.,\n                    ""l2"": 0.\n                }\n            },\n            ""bias_regularizer"": {\n                # same as in ""kernel_regularizer""\n                # ...\n            },\n            ""activity_regularizer"": {\n                # same as in ""kernel_regularizer""\n                # ...\n            },\n            ""kernel_constraint"": None,\n            ""bias_constraint"": None,\n            ""trainable"": True,\n            ""name"": None\n        }\n    """"""\n    kwargs = _common_default_conv_dense_kwargs()\n    kwargs.update({\n        ""units"": 256\n    })\n    return kwargs\n\n\ndef default_dropout_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_flatten_kwargs():\n    """"""TODO\n    """"""\n    return {}\n\n\ndef default_max_pooling1d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_max_pooling2d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_max_pooling3d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_separable_conv2d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_batch_normalization_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_average_pooling1d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_average_pooling2d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\ndef default_average_pooling3d_kwargs():\n    """"""TODO\n    """"""\n    return {}\n    # raise NotImplementedError\n\n\n_layer_class_to_default_kwargs_map = {\n    tf.layers.Conv1D: default_conv1d_kwargs(),\n    tf.layers.Conv2D: default_conv2d_kwargs(),\n    tf.layers.Conv3D: default_conv3d_kwargs(),\n    tf.layers.Conv2DTranspose: default_conv2d_transpose_kwargs(),\n    tf.layers.Conv3DTranspose: default_conv3d_transpose_kwargs(),\n    tf.layers.Dense: default_dense_kwargs(),\n    tf.layers.Dropout: default_dropout_kwargs(),\n    tf.layers.Flatten: default_flatten_kwargs(),\n    tf.layers.MaxPooling1D: default_max_pooling1d_kwargs(),\n    tf.layers.MaxPooling2D: default_max_pooling2d_kwargs(),\n    tf.layers.MaxPooling3D: default_max_pooling3d_kwargs(),\n    tf.layers.SeparableConv2D: default_separable_conv2d_kwargs(),\n    tf.layers.BatchNormalization: default_batch_normalization_kwargs(),\n    tf.layers.AveragePooling1D: default_average_pooling1d_kwargs(),\n    tf.layers.AveragePooling2D: default_average_pooling2d_kwargs(),\n    tf.layers.AveragePooling3D: default_average_pooling3d_kwargs(),\n}\n\n\ndef layer_normalize(inputs,\n                    scope=None,\n                    **kwargs):\n    """"""Applies layer normalization. Normalizes over the last dimension.\n\n    Args:\n        inputs: A tensor with 2 or more dimensions, where the first\n            dimension must be `batch_size`.\n        scope (optional): variable scope.\n\n    Returns:\n        A tensor with the same shape and data dtype as `inputs`.\n    """"""\n    return tf.contrib.layers.layer_norm(\n        inputs=inputs, begin_norm_axis=-1, begin_params_axis=-1, scope=scope,\n        **kwargs\n    )\n\n\ndef gelu(input_tensor):\n    """"""Gaussian Error Linear Unit.\n\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Args:\n      input_tensor: float Tensor to perform activation.\n\n    Returns:\n      `input_tensor` with the GELU activation applied.\n    """"""\n    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n    return input_tensor * cdf\n'"
texar/tf/core/optimization.py,68,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious optimization related utilities.\n""""""\n\nimport re\nimport tensorflow as tf\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.utils import utils\n\n# pylint: disable=too-many-arguments, no-member\n\n__all__ = [\n    ""default_optimization_hparams"",\n    ""get_optimizer_fn"",\n    ""get_learning_rate_decay_fn"",\n    ""get_gradient_clip_fn"",\n    ""get_optimizer"",\n    ""get_train_op"",\n    ""AdamWeightDecayOptimizer"",\n]\n\n\ndef default_optimization_hparams():\n    """"""Returns a `dict` of default hyperparameters of training op\n    and their default values\n\n    .. role:: python(code)\n       :language: python\n\n    .. code-block:: python\n\n        {\n            ""optimizer"": {\n                ""type"": ""AdamOptimizer"",\n                ""kwargs"": {\n                    ""learning_rate"": 0.001\n                }\n            },\n            ""learning_rate_decay"": {\n                ""type"": """",\n                ""kwargs"": {},\n                ""min_learning_rate"": 0.,\n                ""start_decay_step"": 0,\n                ""end_decay_step"": inf\n            },\n            ""gradient_clip"": {\n                ""type"": """",\n                ""kwargs"": {}\n            },\n            ""gradient_noise_scale"": None,\n            ""name"": None\n        }\n\n    Here:\n\n    ""optimizer"": dict\n        Hyperparameters of a :tf_main:`tf.train.Optimizer <train/Optimizer>`.\n\n        - **""type""** specifies the optimizer class. This can be\n\n            - The string name or full module path of an optimizer class. \\\n            If the class name is provided, the class must be in module \\\n            :tf_main:`tf.train <train>`, \\\n            :tf_main:`tf.contrib.opt <contrib/opt>` or :mod:`texar.tf.custom` \\\n            , :mod:`texar.tf.core.optimization`\n            - An optimizer class.\n            - An instance of an optimizer class.\n\n            For example\n\n            .. code-block:: python\n\n                ""type"": ""AdamOptimizer"" # class name\n                ""type"": ""my_module.MyOptimizer"" # module path\n                ""type"": tf.contrib.opt.AdamWOptimizer # class\n                ""type"": my_module.MyOptimizer # class\n                ""type"": GradientDescentOptimizer(learning_rate=0.1) # instance\n                ""type"": MyOptimizer(...) # instance\n\n        - **""kwargs""** is a `dict` specifying keyword arguments for creating \\\n        the optimizer class instance, with :python:`opt_class(**kwargs)`. \\\n        Ignored if ""type"" is a class instance.\n\n    ""learning_rate_decay"": dict\n        Hyperparameters of learning rate decay function. The learning rate\n        starts decay from :attr:`""start_decay_step""` and keeps unchanged after\n        :attr:`""end_decay_step""` or reaching :attr:`""min_learning_rate""`.\n\n        The decay function is specified in ""type"" and ""kwargs"".\n\n            - ""type"" can be a decay function or its name or module path. If \\\n            function name is provided, it must be from module \\\n            :tf_main:`tf.train <train>` or :mod:`texar.tf.custom`, \\\n            :mod:`texar.tf.core.optimization`.\n\n            - ""kwargs"" is a `dict` of keyword arguments for the function \\\n            excluding arguments named ""global_step"" and ""learning_rate"".\n\n        The function is called with\n        :python:`lr = decay_fn(learning_rate=lr, global_step=offset_step,\n        **kwargs)`, where `offset_step` is the global step offset as above.\n        The only exception is :tf_main:`tf.train.piecewise_constant\n        <train/piecewise_constant>` which is called with\n        :python:`lr = piecewise_constant(x=offset_step, **kwargs)`.\n\n    ""gradient_clip"": dict\n        Hyperparameters of gradient clipping. The gradient clipping function\n        takes a list of `(gradients, variables)` tuples and returns a list\n        of `(clipped_gradients, variables)` tuples. Typical examples include\n        :tf_main:`tf.clip_by_global_norm <clip_by_global_norm>`,\n        :tf_main:`tf.clip_by_value <clip_by_value>`,\n        :tf_main:`tf.clip_by_norm <clip_by_norm>`,\n        :tf_main:`tf.clip_by_average_norm <clip_by_average_norm>`, etc.\n\n        ""type"" specifies the gradient clip function, and can be a function,\n        or its name or mudule path. If function name is provided, the\n        function must be from module :tf_main:`tf < >`\n        or :mod:`texar.tf.custom`, :mod:`texar.tf.core.optimization`.\n\n\n        ""kwargs"" specifies keyword arguments to the function, except arguments\n        named ""t"" or ""t_list"".\n\n        The function is called with\n        :python:`clipped_grads(, _) = clip_fn(t_list=grads, **kwargs)`\n        (e.g., for :tf_main:`tf.clip_by_global_norm <clip_by_global_norm>`) or\n        :python:`clipped_grads = [clip_fn(t=grad, **kwargs) for grad in grads]`\n        (e.g., for :tf_main:`tf.clip_by_value <clip_by_value>`).\n\n    ""gradient_noise_scale"": float, optional\n        Adds 0-mean normal noise scaled by this value to gradient.\n    """"""\n    return {\n        ""optimizer"": {\n            ""type"": ""AdamOptimizer"",\n            ""kwargs"": {\n                ""learning_rate"": 0.001\n            }\n        },\n        ""learning_rate_decay"": {\n            ""type"": """",\n            ""kwargs"": {},\n            ""min_learning_rate"": 0.,\n            ""start_decay_step"": 0,\n            ""end_decay_step"": utils.MAX_SEQ_LENGTH,\n        },\n        ""gradient_clip"": {\n            ""type"": """",\n            ""kwargs"": {}\n        },\n        ""gradient_noise_scale"": None,\n        # TODO(zhiting): allow module-level control of gradient_multipliers\n        ""name"": None\n    }\n\n\ndef get_optimizer_fn(hparams=None):\n    """"""Returns a function `optimizer_fn` of making optimizer instance, along\n    with the optimizer class.\n\n    .. role:: python(code)\n       :language: python\n\n    The function has the signiture\n    :python:`optimizer_fn(learning_rate=None) -> optimizer class instance`\n\n    See the :attr:`""optimizer""` field of\n    :meth:`~texar.tf.core.default_optimization_hparams` for all\n    hyperparameters and default values.\n\n    The optimizer class must be a subclass of\n    :tf_main:`tf.train.Optimizer <train/Optimizer>`.\n\n    Args:\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically.\n\n    Returns:\n        - If hparams[""type""] is a string or optimizer class, returns\\\n        `(optimizer_fn, optimizer class)`,\n\n        - If hparams[""type""] is an optimizer instance, returns \\\n        `(the optimizer instance, optimizer class)`\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(\n            hparams, default_optimization_hparams()[""optimizer""])\n\n    opt = hparams[""type""]\n    if isinstance(opt, tf.train.Optimizer):\n        return opt, type(opt)\n    opt_modules = [\'tensorflow.train\',\n                   \'tensorflow.contrib.opt\',\n                   \'texar.tf.core.optimization\',\n                   \'texar.tf.custom\']\n    try:\n        opt_class = utils.check_or_get_class(opt, opt_modules,\n                                             tf.train.Optimizer)\n    except TypeError:\n        raise ValueError(\n            ""Unrecognized optimizer. Must be string name of the ""\n            ""optimizer class, or the class which is a subclass of ""\n            ""tf.train.Optimizer, or an instance of the subclass of ""\n            ""Optimizer."")\n\n    def _get_opt(learning_rate=None):\n        opt_kwargs = hparams[""kwargs""].todict()\n        fn_args = set(utils.get_args(opt_class.__init__))\n        if \'learning_rate\' in fn_args and learning_rate is not None:\n            opt_kwargs[""learning_rate""] = learning_rate\n        return opt_class(**opt_kwargs)\n\n    return _get_opt, opt_class\n\n\ndef get_learning_rate_decay_fn(hparams=None):\n    """"""Creates learning rate decay function based on the hyperparameters.\n\n    See the :attr:`learning_rate_decay` field in\n    :meth:`~texar.tf.core.default_optimization_hparams` for all\n    hyperparameters and default values.\n\n    Args:\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically.\n\n    Returns:\n        function or None: If hparams[""type""] is specified, returns a\n        function that takes `(learning_rate, step, **kwargs)` and\n        returns a decayed learning rate. If\n        hparams[""type""] is empty, returns `None`.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(\n            hparams, default_optimization_hparams()[""learning_rate_decay""])\n\n    fn_type = hparams[""type""]\n    if fn_type is None or fn_type == """":\n        return None\n\n    fn_modules = [""tensorflow.train"", ""texar.tf.custom""]\n    decay_fn = utils.get_function(fn_type, fn_modules)\n    fn_kwargs = hparams[""kwargs""]\n    if fn_kwargs is HParams:\n        fn_kwargs = fn_kwargs.todict()\n\n    start_step = tf.cast(hparams[""start_decay_step""], tf.int32)\n    end_step = tf.cast(hparams[""end_decay_step""], tf.int32)\n\n    def lr_decay_fn(learning_rate, global_step):\n        """"""Learning rate decay function.\n\n        Args:\n            learning_rate (float or Tensor): The original learning rate.\n            global_step (int or scalar int Tensor): optimization step counter.\n\n        Returns:\n            scalar float Tensor: decayed learning rate.\n        """"""\n        offset_global_step = tf.maximum(\n            tf.minimum(tf.cast(global_step, tf.int32), end_step) - start_step,\n            0)\n        if decay_fn == tf.train.piecewise_constant:\n            decayed_lr = decay_fn(x=offset_global_step, **fn_kwargs)\n        else:\n            fn_kwargs_ = {\n                ""learning_rate"": learning_rate,\n                ""global_step"": offset_global_step}\n            fn_kwargs_.update(fn_kwargs)\n            decayed_lr = utils.call_function_with_redundant_kwargs(\n                decay_fn, fn_kwargs_)\n\n            decayed_lr = tf.maximum(decayed_lr, hparams[""min_learning_rate""])\n\n        return decayed_lr\n\n    return lr_decay_fn\n\n\ndef get_gradient_clip_fn(hparams=None):\n    """"""Creates a gradient clipping function based on the hyperparameters.\n\n    See the :attr:`gradient_clip` field in\n    :meth:`~texar.tf.core.default_optimization_hparams` for all\n    hyperparameters and default values.\n\n    The gradient clipping function takes a list of `(gradients, variables)`\n    tuples and returns a list of `(clipped_gradients, variables)` tuples.\n    Typical examples include\n    :tf_main:`tf.clip_by_global_norm <clip_by_global_norm>`,\n    :tf_main:`tf.clip_by_value <clip_by_value>`,\n    :tf_main:`tf.clip_by_norm <clip_by_norm>`,\n    :tf_main:`tf.clip_by_average_norm <clip_by_average_norm>`, etc.\n\n    Args:\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically.\n\n    Returns:\n        function or `None`: If hparams[""type""] is specified, returns\n        the respective function. If hparams[""type""] is empty,\n        returns `None`.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        hparams = HParams(\n            hparams, default_optimization_hparams()[""gradient_clip""])\n    fn_type = hparams[""type""]\n    if fn_type is None or fn_type == """":\n        return None\n\n    fn_modules = [""tensorflow"", ""texar.tf.custom""]\n    clip_fn = utils.get_function(fn_type, fn_modules)\n    clip_fn_args = utils.get_args(clip_fn)\n    fn_kwargs = hparams[""kwargs""]\n    if isinstance(fn_kwargs, HParams):\n        fn_kwargs = fn_kwargs.todict()\n\n    def grad_clip_fn(grads_and_vars):\n        """"""Gradient clipping function.\n\n        Args:\n            grads_and_vars (list): A list of `(gradients, variables)` tuples.\n\n        Returns:\n            list: A list of `(clipped_gradients, variables)` tuples.\n        """"""\n        grads, vars_ = zip(*grads_and_vars)\n        if clip_fn == tf.clip_by_global_norm:\n            clipped_grads, _ = clip_fn(t_list=grads, **fn_kwargs)\n        elif \'t_list\' in clip_fn_args:\n            clipped_grads = clip_fn(t_list=grads, **fn_kwargs)\n        elif \'t\' in clip_fn_args:     # e.g., tf.clip_by_value\n            clipped_grads = [clip_fn(t=grad, **fn_kwargs) for grad in grads]\n\n        return list(zip(clipped_grads, vars_))\n\n    return grad_clip_fn\n\n\ndef _get_static_lr(learning_rate=None, optimizer_class=None, hparams=None):\n    """"""Return the base static learning_rate.\n        A helper function for creating the optimization function.\n    """"""\n    hparams = HParams(hparams, default_optimization_hparams())\n    opt_hparams = hparams[\'optimizer\']\n    if learning_rate is None:\n        learning_rate = opt_hparams[""kwargs""].get(""learning_rate"", None)\n    if learning_rate is None:\n        # Try to get learning_rate from the default value of the\n        # optimizer\'s argument\n        opt_argspec = utils.get_default_arg_values(optimizer_class.__init__)\n        learning_rate = opt_argspec.get(""learning_rate"", None)\n    return learning_rate\n\n\ndef get_optimizer(learning_rate=None, global_step=None, hparams=None):\n\n    """"""Creates a optimizer instance.\n\n    Args:\n        learning_rate (float or Tensor, optional): If `None`, learning rate\n            specified in :attr:`hparams`, or the default learning rate\n            of the optimizer (if exists) is used.\n        global_step (optional): A scalar int Tensor. Step counter to update on\n            each step unless :attr:`increment_global_step` is `False`.\n            Learning rate decay uses :attr:`global_step`.\n            If `None`, it will be fetched from the default graph (see\n            :tf_main:`tf.train.get_global_step <train/get_global_step>` for\n            more details). If it has not been created, no step will be\n            incremented with each weight update.\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically. See\n            :func:`~texar.tf.core.default_optimization_hparams` for\n            all hyperparameters and default values.\n\n    Returns:\n        optimizer: the tf.train.Optimizer instance specified in hparams.\n    """"""\n    hparams = HParams(hparams, default_optimization_hparams())\n\n    opt_hparams = hparams[""optimizer""]\n    optimizer_fn, optimizer_class = get_optimizer_fn(opt_hparams)\n\n    static_lr = _get_static_lr(learning_rate, optimizer_class, hparams)\n\n    lr_decay_fn = get_learning_rate_decay_fn(hparams[""learning_rate_decay""])\n    if lr_decay_fn is not None:\n        learning_rate = lr_decay_fn(learning_rate=static_lr,\n                                    global_step=global_step)\n    else:\n        learning_rate = static_lr\n\n    tf.summary.scalar(""learning_rate"", learning_rate)\n\n    optimizer = optimizer_fn(learning_rate=learning_rate)\n\n    return optimizer\n\n\ndef get_train_op(loss, variables=None,\n                 optimizer=None, learning_rate=None,\n                 global_step=None, increment_global_step=True, hparams=None):\n    """"""Creates a training op.\n\n    This is a wrapper of :tf_main:`tf.contrib.layers.optimize_loss\n    <contrib/layers/optimize_loss>`.\n\n    Args:\n        loss: A scalar Tensor representing the loss to minimize.\n        variables (optional): A list of Variables to optimize. If\n            `None`, all trainable variables are used.\n        optimizer (optional): An tf.train.Optimizer instance. If `None`,\n            use the setting in `hparams` to create the optimizer.\n        learning_rate (float or Tensor, optional): If `None`, learning rate\n            specified in :attr:`hparams`, or the default learning rate\n            of the optimizer will be used (if exists).\n        global_step (optional): A scalar int Tensor. Step counter to update on\n            each step unless :attr:`increment_global_step` is `False`.\n            Learning rate decay uses :attr:`global_step`.\n            If `None`, it will be fetched from the default graph (see\n            :tf_main:`tf.train.get_global_step <train/get_global_step>` for\n            more details). If it has not been created, no step will be\n            incremented with each weight update.\n        increment_global_step (bool): Whether to increment\n            :attr:`global_step`. This is useful if the :attr:`global_step` is\n            used in multiple training ops per training step (e.g. to optimize\n            different parts of the model) to avoid incrementing\n            :attr:`global_step` more times than necessary.\n        hparams (dict or HParams, optional): hyperparameters. Missing\n            hyperparameters are set to default values automatically. See\n            :func:`~texar.tf.core.default_optimization_hparams` for\n            all hyperparameters and default values.\n\n    Returns:\n        train_op: the operator used for variables optimization.\n    """"""\n    hparams = HParams(hparams, default_optimization_hparams())\n    grad_clip_fn = get_gradient_clip_fn(hparams[""gradient_clip""])\n\n    if not isinstance(optimizer, tf.train.Optimizer):\n        opt_hparams = hparams[""optimizer""]\n        optimizer_fn, optimizer_class = get_optimizer_fn(opt_hparams)\n        learning_rate = _get_static_lr(learning_rate, optimizer_class, hparams)\n        lr_decay_fn = get_learning_rate_decay_fn(\n            hparams[""learning_rate_decay""])\n        train_op = tf.contrib.layers.optimize_loss(\n            loss=loss,\n            global_step=global_step,\n            learning_rate=learning_rate,\n            optimizer=optimizer_fn,\n            gradient_noise_scale=hparams[""gradient_noise_scale""],\n            clip_gradients=grad_clip_fn,\n            learning_rate_decay_fn=lr_decay_fn,\n            variables=variables,\n            name=hparams[""name""],\n            increment_global_step=increment_global_step)\n\n    else:\n        train_op = tf.contrib.layers.optimize_loss(\n            loss=loss,\n            global_step=global_step,\n            learning_rate=None,\n            optimizer=optimizer,\n            gradient_noise_scale=hparams[""gradient_noise_scale""],\n            clip_gradients=grad_clip_fn,\n            variables=variables,\n            name=hparams[""name""],\n            increment_global_step=increment_global_step)\n\n    return train_op\n\n\nclass AdamWeightDecayOptimizer(tf.train.Optimizer):\n    """"""\n    A basic Adam optimizer that includes ""correct"" L2 weight decay.\n    Copied from the google BERT repo.\n    Except that in `apply_gradient` function, we add the support to increment\n    the passed global step parameter, to make it more compatible to\n    tf.train.Optimizer implementation.\n    """"""\n\n    def __init__(self,\n                 learning_rate,\n                 weight_decay_rate=0.0,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-6,\n                 exclude_from_weight_decay=None,\n                 name=""AdamWeightDecayOptimizer""):\n        """"""Constructs a AdamWeightDecayOptimizer.""""""\n        super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n        self.learning_rate = learning_rate\n        self.weight_decay_rate = weight_decay_rate\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.exclude_from_weight_decay = exclude_from_weight_decay\n\n    # pylint: disable=too-many-locals\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n        """"""See base class.""""""\n        with tf.name_scope(name, self._name) as name:\n            assignments = []\n            for (grad, param) in grads_and_vars:\n                if grad is None or param is None:\n                    continue\n\n                param_name = self._get_variable_name(param.name)\n\n                m = tf.get_variable(\n                    name=param_name + ""/adam_m"",\n                    shape=param.shape.as_list(),\n                    dtype=tf.float32,\n                    trainable=False,\n                    initializer=tf.zeros_initializer())\n                v = tf.get_variable(\n                    name=param_name + ""/adam_v"",\n                    shape=param.shape.as_list(),\n                    dtype=tf.float32,\n                    trainable=False,\n                    initializer=tf.zeros_initializer())\n\n                # Standard Adam update.\n                next_m = (tf.multiply(self.beta_1, m)\n                          + tf.multiply(1.0 - self.beta_1,\n                                        grad))\n                next_v = (tf.multiply(self.beta_2, v)\n                          + tf.multiply(1.0 - self.beta_2, tf.square(grad)))\n\n                update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n                # Just adding the square of the weights to the loss function is\n                # *not* the correct way of using L2 regularization/weight decay\n                # with Adam, since that will interact with the m and v\n                # parameters in strange ways.\n                # Instead we want ot decay the weights in a manner that doesn\'t\n                # interact with the m/v parameters.\n                # This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if self._do_use_weight_decay(param_name):\n                    update += self.weight_decay_rate * param\n\n                update_with_lr = self.learning_rate * update\n\n                next_param = param - update_with_lr\n\n                assignments.extend(\n                    [param.assign(next_param),\n                     m.assign(next_m),\n                     v.assign(next_v)])\n\n            update_ops = assignments\n            if global_step is None:\n                apply_updates = self._finish(update_ops, name)\n            else:\n                with tf.control_dependencies([self._finish(update_ops,\n                                                           ""update"")]):\n                    with tf.colocate_with(global_step):\n                        apply_updates = tf.assign_add(global_step, 1, name=name)\n\n        return apply_updates\n\n    def _do_use_weight_decay(self, param_name):\n        """"""Whether to use L2 weight decay for `param_name`.""""""\n        if not self.weight_decay_rate:\n            return False\n        if self.exclude_from_weight_decay:\n            for r in self.exclude_from_weight_decay:\n                if re.search(r, param_name) is not None:\n                    return False\n        return True\n\n    def _get_variable_name(self, param_name):\n        """"""Get the variable name from the tensor name.""""""\n        m = re.match(""^(.*):\\\\d+$"", param_name)\n        if m is not None:\n            param_name = m.group(1)\n        return param_name\n'"
texar/tf/core/replay_memories.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nClasses and utilities for replay memory in RL.\n""""""\n\nfrom collections import deque\nimport random\n\nfrom texar.tf.hyperparams import HParams\n\n__all__ = [\n    ""ReplayMemoryBase"",\n    ""DequeReplayMemory""\n]\n\n\nclass ReplayMemoryBase(object):\n    """"""Base class of replay memory inheritted by all replay memory classes.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters are set to default values. See\n            :meth:`default_hparams` for the defaults.\n    """"""\n    def __init__(self, hparams=None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a `dict` of hyperparameters and their default values.\n\n        .. code-block:: python\n\n            {\n                \'name\': \'replay_memory\'\n            }\n        """"""\n        return {\n            \'name\': \'replay_memory\'\n        }\n\n    def add(self, element):\n        """"""Inserts a memory entry\n        """"""\n        raise NotImplementedError\n\n    def get(self, size):\n        """"""Pops a memory entry.\n        """"""\n        raise NotImplementedError\n\n    def last(self):\n        """"""Returns the latest element in the memeory.\n        """"""\n        raise NotImplementedError\n\n    def size(self):\n        """"""Returns the current size of the memory.\n        """"""\n        raise NotImplementedError\n\n\nclass DequeReplayMemory(ReplayMemoryBase):\n    """"""A deque based replay memory that accepts new memory entry and deletes\n    oldest memory entry if exceeding the capacity. Memory entries are\n    accessed in random order.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters are set to default values. See\n            :meth:`default_hparams` for the defaults.\n    """"""\n    def __init__(self, hparams=None):\n        ReplayMemoryBase.__init__(self, hparams)\n        self.deque = deque()\n        self.capacity = self._hparams.capacity\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a `dict` of hyperparameters and their default values.\n\n        .. code-block:: python\n\n            {\n                \'capacity\': 80000,\n                \'name\': \'deque_replay_memory\',\n            }\n\n        Here:\n\n        ""capacity"": int\n            Maximum size of memory kept. Deletes oldest memories if exceeds\n            the capacity.\n        """"""\n        return {\n            \'name\': \'deque_replay_memory\',\n            \'capacity\': 80000\n        }\n\n    def add(self, element):\n        """"""Appends element to the memory and deletes old memory if exceeds\n        the capacity.\n        """"""\n        self.deque.append(element)\n        if len(self.deque) > self.capacity:\n            self.deque.popleft()\n\n    # TODO(zhiting): is it okay to have stand alone random generator ?\n    def get(self, size):\n        """"""Randomly samples :attr:`size` entries from the memory. Returns\n        a list.\n        """"""\n        return random.sample(self.deque, size)\n\n    def last(self):\n        """"""Returns the latest element in the memeory.\n        """"""\n        return self.deque[-1]\n\n    def size(self):\n        """"""Returns the current size of the memory.\n        """"""\n        return len(self.deque)\n'"
texar/tf/custom/__init__.py,0,b''
texar/tf/data/__init__.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library data.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.data.data import *\nfrom texar.tf.data.tokenizers import *\nfrom texar.tf.data.data_utils import *\nfrom texar.tf.data.data_decoders import *\nfrom texar.tf.data.vocabulary import *\nfrom texar.tf.data.embedding import *\n'"
texar/tf/data/data_decoders.py,56,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions and classes for decoding text data which are used after\nreading raw text data.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.data import data_decoder\n\nfrom texar.tf.data.vocabulary import SpecialTokens\nfrom texar.tf.utils import dtypes\nfrom texar.tf.hyperparams import HParams\n\n\n# pylint: disable=too-many-instance-attributes, too-many-arguments,\n# pylint: disable=no-member, invalid-name\n\n__all__ = [\n    ""ScalarDataDecoder"",\n    ""TextDataDecoder"",\n    ""VarUttTextDataDecoder"",\n    ""TFRecordDataDecoder"",\n]\n\n\ndef _append_token(token):\n    return token is not None and token != """"\n\n\nclass ScalarDataDecoder(data_decoder.DataDecoder):\n    """"""A data decoder that decodes a scalar, e.g., int label or float number.\n\n    The only operation is to cast the data into a specified data type.\n\n    Args:\n        dtype: A :tf_main:`tf DType <DType>` that data is cast into. Can be\n            `tf.int32` or `tf.float32`.\n        data_name (str): Name of the decoded data.\n    """"""\n\n    def __init__(self, dtype=tf.int32, data_name=""data""):\n        self._dtype = dtype\n        self._data_name = data_name\n        if self._data_name is None:\n            self._data_name = ""data""\n\n    def __call__(self, data):\n        outputs = self.decode(data, self.list_items())\n        return dict(zip(self.list_items(), outputs))\n\n    def decode(self, data, items):\n        """"""Decodes the data to return the tensors specified by the list of\n        items.\n\n        Args:\n            data: The scalar data to decode.\n            items: A list of strings, each of which is the name of the resulting\n                tensors to retrieve.\n\n        Returns:\n            A list of tensors, each of which corresponds to each item.\n        """"""\n        data = tf.reshape(data, shape=[])\n        if data.dtype is tf.string:\n            decoded_data = tf.string_to_number(data, out_type=self._dtype)\n        else:\n            decoded_data = tf.cast(data, self._dtype)\n        outputs = {\n            self._data_name: decoded_data\n        }\n        return [outputs[item] for item in items]\n\n    def list_items(self):\n        """"""Returns the list of item names that the decoder can produce.\n\n        Returns:\n            A list of strings can be passed to :meth:`decode()`.\n        """"""\n        return [self._data_name]\n\n    @property\n    def data_tensor_name(self):\n        """"""The name of the data tensor.\n        """"""\n        return self._data_name\n\n\nclass TextDataDecoder(data_decoder.DataDecoder):\n    """"""A text data decoder that decodes raw text data.\n\n    Operations include splitting on word or character level, truncation,\n    inserting special tokens, mapping text units to indexes, etc.\n\n    Args:\n        split_level (str): The name of split level on which text sequence is\n            split. Either ""word"" or ""char"".\n        delimiter (str): The delimiter character used when splitting on word\n            level.\n        bos_token (str, optional): Special token added to the beginning of\n            sequences. If it is `None` (default) or an empty string, no\n            BOS token is added.\n        eos_token (str, optional): Special tokan added to the end of\n            sequences. If it is `None` (default) or an empty string, no EOS\n            token is added.\n        max_seq_length (int, optional): Maximum length of output sequences.\n            Tokens exceeding the maximum length will be truncated. The length\n            does not include any added bos_token and eos_token. If not\n            given, no truncation is performed.\n        token_to_id_map (optional): A\n            :class:`~tensorflow.contrib.lookup.HashTable` instance that maps\n            token strings to integer indexes. If not given, the decoder will\n            not decode text into indexes. :attr:`bos_token` and\n            :attr:`eos_token` (if given) should have entries in the\n            :attr:`token_to_id_map` (if given).\n        text_tensor_name (str): Name of the text tensor results. Used as a\n            key to retrieve the text tensor.\n        length_tensor_name (str): Name of the text length tensor results.\n        text_id_tensor_name (str): Name of the text index tensor results.\n    """"""\n\n    def __init__(self,\n                 split_level=""word"",\n                 delimiter="" "",\n                 bos_token=None,\n                 eos_token=None,\n                 max_seq_length=None,\n                 token_to_id_map=None,\n                 text_tensor_name=""text"",\n                 length_tensor_name=""length"",\n                 text_id_tensor_name=""text_ids""):\n        self._split_level = split_level\n        self._delimiter = delimiter\n        self._bos_token = bos_token\n        self._eos_token = eos_token\n        self._max_seq_length = max_seq_length\n        self._token_to_id_map = token_to_id_map\n        self._text_tensor_name = text_tensor_name\n        self._text_id_tensor_name = text_id_tensor_name\n        self._length_tensor_name = length_tensor_name\n        self._added_length = 0\n\n    def __call__(self, data):\n        outputs = self.decode(data, self.list_items())\n        return dict(zip(self.list_items(), outputs))\n\n    def decode(self, data, items):\n        """"""Decodes the data to return the tensors specified by the list of\n        items.\n\n        Args:\n            data: The text data to decode.\n            items: A list of strings, each of which is the name of the resulting\n                tensors to retrieve.\n\n        Returns:\n            A list of tensors, each of which corresponds to each item. If\n            `token_to_id_map` is not given when constructing the decoder,\n            returns `None` for the token index item.\n        """"""\n        # Split\n        if self._split_level == ""word"":\n            tokens = tf.string_split([data], delimiter=self._delimiter).values\n        elif self._split_level == ""char"":\n            raise NotImplementedError\n        else:\n            raise ValueError(""Unknown split level: %s"" % self._split_level)\n\n        # Truncate\n        if self._max_seq_length is not None:\n            tokens = tokens[:self._max_seq_length]\n\n        # Add BOS/EOS tokens\n        if _append_token(self._bos_token):\n            tokens = tf.concat([[self._bos_token], tokens], axis=0)\n            self._added_length += 1\n        if _append_token(self._eos_token):\n            tokens = tf.concat([tokens, [self._eos_token]], axis=0)\n            self._added_length += 1\n\n        # Map to index\n        token_ids = None\n        if self._token_to_id_map is not None:\n            token_ids = self._token_to_id_map.lookup(tokens)\n\n        outputs = {\n            self._text_tensor_name: tokens,\n            self._length_tensor_name: tf.size(tokens),\n            self._text_id_tensor_name: token_ids\n        }\n        return [outputs[item] for item in items]\n\n    def list_items(self):\n        """"""Returns the list of item names that the decoder can produce.\n\n        Returns:\n            A list of strings can be passed to :meth:`decode()`.\n        """"""\n        return [self._text_tensor_name,\n                self._length_tensor_name,\n                self._text_id_tensor_name]\n\n    @property\n    def text_tensor_name(self):\n        """"""The name of text tensor.\n        """"""\n        return self._text_tensor_name\n\n    @text_tensor_name.setter\n    def text_tensor_name(self, name):\n        self._text_tensor_name = name\n\n    @property\n    def length_tensor_name(self):\n        """"""The name of length tensor.\n        """"""\n        return self._length_tensor_name\n\n    @length_tensor_name.setter\n    def length_tensor_name(self, name):\n        self._length_tensor_name = name\n\n    @property\n    def text_id_tensor_name(self):\n        """"""The name of text index tensor.\n        """"""\n        return self._text_id_tensor_name\n\n    @text_id_tensor_name.setter\n    def text_id_tensor_name(self, name):\n        self._text_id_tensor_name = name\n\n    @property\n    def added_length(self):\n        """"""The added text length due to appended bos and eos tokens.\n        """"""\n        return self._added_length\n\n\nclass VarUttTextDataDecoder(data_decoder.DataDecoder):\n    """"""A text data decoder that decodes raw text data. Each data is considered\n    to be multiple sentences concatenated by a delimiter.\n\n    Operations include splitting on word or character level, truncation,\n    inserting special tokens, mapping text units to indexes, etc.\n\n    Args:\n        split_level (str): The name of split level on which text sequence is\n            split. Either ""word"" or ""char"".\n        delimiter (str): The delimiter character used when splitting on word\n            level.\n        bos_token (str, optional): Special token added to the beginning of\n            sequences. If it is `None` (default) or an empty string, no\n            BOS token is added.\n        eos_token (str, optional): Special tokan added to the end of\n            sequences. If it is `None` (default) or an empty string, no EOS\n            token is added.\n        max_seq_length (int): Maximum length of each sequence.\n            Tokens exceed the maximum length will be truncated. Additional\n            padding will be done to ensure output sequence all reach this\n            number. The length does not include any added bos_token and eos_\n            token.\n        max_utterance_cnt (int): Maximum number of sequences.\n            Additional empty sentences will be added to\n            ensure the respective dimension of the output tensor has size\n            :attr:`max_utterance_cnt`. The output item named by\n            :meth:`utterance_cnt_tensor_name` contains the actual number of\n            utterance in the data.\n        token_to_id_map (optional): A\n            :class:`~tensorflow.contrib.lookup.HashTable` instance that maps\n            token strings to integer indexes. If not given, the decoder will\n            not decode text into indexes. :attr:`bos_token` and\n            :attr:`eos_token` (if given) should have entries in the\n            :attr:`token_to_id_map` (if given).\n        text_tensor_name (str): Name of the text tensor results. Used as a\n            key to retrieve the text tensor.\n        length_tensor_name (str): Name of the text length tensor results.\n        text_id_tensor_name (str): Name of the text index tensor results.\n    """"""\n\n    def __init__(self,\n                 split_level=""word"",\n                 delimiter="" "",\n                 sentence_delimiter=""|||"",\n                 bos_token=None,\n                 eos_token=None,\n                 max_seq_length=None,\n                 max_utterance_cnt=None,\n                 token_to_id_map=None,\n                 text_tensor_name=""text"",\n                 length_tensor_name=""length"",\n                 text_id_tensor_name=""text_ids"",\n                 utterance_cnt_tensor_name=""utterance_cnt""):\n        self._split_level = split_level\n        self._delimiter = delimiter\n        self._bos_token = bos_token\n        self._eos_token = eos_token\n        self._max_seq_length = max_seq_length\n        self._token_to_id_map = token_to_id_map\n        self._text_tensor_name = text_tensor_name\n        self._text_id_tensor_name = text_id_tensor_name\n        self._length_tensor_name = length_tensor_name\n        self._utterance_cnt_tensor_name = utterance_cnt_tensor_name\n        self._sentence_delimiter = sentence_delimiter\n        self._max_utterance_cnt = max_utterance_cnt\n        self._added_length = 0\n\n    def __call__(self, data):\n        outputs = self.decode(data, self.list_items())\n        return dict(zip(self.list_items(), outputs))\n\n    def decode(self, data, items):  # pylint: disable=too-many-locals\n        """"""Decodes the data to return the tensors specified by the list of\n        items.\n\n        Args:\n            data: The text data to decode.\n            items: A list of strings, each of which is the name of the resulting\n                tensors to retrieve.\n\n        Returns:\n            A list of tensors, each of which corresponds to each item. If\n            `token_to_id_map` is not given when constructing the decoder,\n            returns `None` for the token index item.\n        """"""\n\n        sentences = tf.string_split([data],\n                                    delimiter=self._sentence_delimiter).values\n\n        # Truncate utterances\n        if self._max_utterance_cnt:\n            sentences = sentences[:self._max_utterance_cnt]\n        utterance_cnt = tf.shape(sentences)[0]\n\n        # Get (max) sentence length\n        def _get_sent_length(s):\n            raw_length = tf.size(\n                tf.string_split([s], delimiter=self._delimiter).values)\n            if self._max_seq_length:\n                return tf.minimum(raw_length, self._max_seq_length)\n            else:\n                return raw_length\n\n        raw_sent_length = tf.map_fn(\n            _get_sent_length, sentences, dtype=tf.int32)\n        sent_length = self._max_seq_length\n        if not sent_length:\n            sent_length = tf.reduce_max(raw_sent_length)\n        if _append_token(self._eos_token):\n            raw_sent_length += 1\n            sent_length += 1\n            self._added_length += 1\n        if _append_token(self._bos_token):\n            raw_sent_length += 1\n            sent_length += 1\n            self._added_length += 1\n\n        def _trunc_and_pad(s, pad_token, max_length):\n            if self._max_seq_length:\n                s = s[:self._max_seq_length]\n            if _append_token(self._bos_token):\n                s = np.append([self._bos_token], s)\n            if _append_token(self._eos_token):\n                s = np.append(s, [self._eos_token])\n            s = np.append(s, [pad_token] * (max_length - s.size))\n            return s\n\n        # Split each sentence to tokens, and pad them to a same length.\n        # This is necessary to treat all sentences as a single tensor.\n        split_sentences = tf.map_fn(\n            lambda s: tf.py_func(\n                _trunc_and_pad,\n                [\n                    tf.string_split([s], delimiter=self._delimiter).values,\n                    SpecialTokens.PAD,\n                    sent_length\n                ],\n                tf.string),\n            sentences, dtype=tf.string\n        )\n\n        split_sentences = tf.reshape(split_sentences,\n                                     [utterance_cnt, sent_length])\n\n        # Map to index\n        token_ids = None\n        if self._token_to_id_map is not None:\n            token_ids = self._token_to_id_map.lookup(split_sentences)\n\n        outputs = {\n            self._text_tensor_name: split_sentences,\n            self._length_tensor_name: raw_sent_length,\n            self._utterance_cnt_tensor_name: tf.shape(sentences)[0],\n            self._text_id_tensor_name: token_ids\n        }\n        return [outputs[item] for item in items]\n\n    def list_items(self):\n        """"""Returns the list of item names that the decoder can produce.\n\n        Returns:\n            A list of strings can be passed to :meth:`decode()`.\n        """"""\n        return [\n            self._text_tensor_name,\n            self._length_tensor_name,\n            self._text_id_tensor_name,\n            self._utterance_cnt_tensor_name\n        ]\n\n    @property\n    def text_tensor_name(self):\n        """"""The name of text tensor.\n        """"""\n        return self._text_tensor_name\n\n    @text_tensor_name.setter\n    def text_tensor_name(self, name):\n        self._text_tensor_name = name\n\n    @property\n    def utterance_cnt_tensor_name(self):\n        """"""The name of the utterance count tensor.\n        """"""\n        return self._utterance_cnt_tensor_name\n\n    @property\n    def length_tensor_name(self):\n        """"""The name of length tensor.\n        """"""\n        return self._length_tensor_name\n\n    @length_tensor_name.setter\n    def length_tensor_name(self, name):\n        self._length_tensor_name = name\n\n    @property\n    def text_id_tensor_name(self):\n        """"""The name of text index tensor.\n        """"""\n        return self._text_id_tensor_name\n\n    @text_id_tensor_name.setter\n    def text_id_tensor_name(self, name):\n        self._text_id_tensor_name = name\n\n    @property\n    def added_length(self):\n        """"""The added text length due to appended bos and eos tokens.\n        """"""\n        return self._added_length\n\n\nclass TFRecordDataDecoder(data_decoder.DataDecoder):\n    """"""A data decoder that decodes a TFRecord file, e.g., the\n    TFRecord file.\n\n    The only operation is to parse the TFRecord data into a\n    specified data type that can be accessed by features.\n\n    Args:\n        ""feature_original_types"" (dict): The feature names (str) with their\n            data types and length types, key and value in pair\n            `<feature_name: [dtype, feature_len_type, len]>`, type of\n            `dtype` can be `tf DType <DType>` or `str`, e.g., \'tf.int32\',\n            \'tf.float32\', etc.\n\n            - `feature_len_type` is of type `str` and can be \\\n            \'FixedLenFeature\' or \'VarLenFeature\' for fixed length \\\n            features and non-fixed length features respectively.\n\n            - `len` is optional, it is the length for the \\\n                \'FixedLenFeature\', can be a `int`.\n\n        ""feature_convert_types"" (dict, optional): The feature names (str)\n            with data types they are converted into, key and value in pair\n            `<feature_name: dtype>`, `dtype` can be a `tf DType <DType>` or\n            `str`, e.g., \'tf.int32\', \'tf.float32\', etc. If not set, data type\n            conversion will not be performed.\n\n            Be noticed that this converting process is after all the data\n            are restored, `feature_original_types` has to be set firstly.\n\n        ""image_options"" (dict, optional): Specifies the image feature name\n            and performs image resizing, includes three fields:\n\n            - ""image_feature_name"":\n                A `str`, the name of the feature which contains\n                the image data. If set, the image data\n                will be restored in format `numpy.ndarray`.\n            - ""resize_height"":\n                A `int`, the height of the image after resizing.\n            - ""resize_width"":\n                A `int`, the width of the image after resizing\n\n            If either `resize_height` or `resize_width` is not set,\n            image data will be restored with original shape.\n    """"""\n\n    def __init__(self,\n                 feature_original_types,\n                 feature_convert_types,\n                 image_options):\n        self._feature_original_types = feature_original_types\n        self._feature_convert_types = feature_convert_types\n        self._image_options = image_options\n\n    def __call__(self, data):\n        outputs = self.decode(data, self.list_items())\n        return dict(zip(self.list_items(), outputs))\n\n    def _decode_image_str_byte(self,\n                               image_option_feature,\n                               decoded_data):\n        # pylint: disable=no-self-use\n        # Get image\n        output_type = tf.uint8\n        image_key = image_option_feature.get(\'image_feature_name\')\n        resize_height = image_option_feature.get(""resize_height"")\n        resize_width = image_option_feature.get(""resize_width"")\n        resize_method = image_option_feature.get(""resize_method"")\n        if image_key is None:\n            return\n        image_byte = decoded_data.get(image_key)\n        if image_byte is None:\n            return\n\n        def _find_resize_method(resize_method):\n            if resize_method in {""AREA"",\n                                 ""ResizeMethod.AREA"",\n                                 ""tf.image.ResizeMethod.AREA"",\n                                 tf.image.ResizeMethod.AREA}:\n                resize_method = tf.image.ResizeMethod.AREA\n            elif resize_method in {""BICUBIC"",\n                                   ""ResizeMethod.BICUBIC"",\n                                   ""tf.image.ResizeMethod.BICUBIC"",\n                                   tf.image.ResizeMethod.BICUBIC}:\n                resize_method = tf.image.ResizeMethod.BICUBIC\n            elif resize_method in {""NEAREST_NEIGHBOR"",\n                                   ""ResizeMethod.NEAREST_NEIGHBOR"",\n                                   ""tf.image.ResizeMethod.NEAREST_NEIGHBOR"",\n                                   tf.image.ResizeMethod.NEAREST_NEIGHBOR}:\n                resize_method = tf.image.ResizeMethod.AREA\n            else:\n                resize_method = tf.image.ResizeMethod.BILINEAR\n            return resize_method\n\n        # Decode image\n        image_decoded = tf.cond(\n            tf.image.is_jpeg(image_byte),\n            lambda: tf.image.decode_jpeg(image_byte),\n            lambda: tf.image.decode_png(image_byte))\n        decoded_data[image_key] = image_decoded\n\n        # Resize the image\n        if resize_height and resize_width:\n            resize_method = _find_resize_method(resize_method)\n            image_resized = tf.image.resize_images(\n                image_decoded,\n                (resize_height, resize_width),\n                method=resize_method)\n            image_resized_converted = tf.cast(image_resized, output_type)\n            decoded_data[image_key] = image_resized_converted\n        return\n\n    def decode(self, data, items):\n        """"""Decodes the data to return the tensors specified by the list of\n        items.\n\n        Args:\n            data: The TFRecord data(serialized example) to decode.\n            items: A list of strings, each of which is the name of the resulting\n                tensors to retrieve.\n\n        Returns:\n            A list of tensors, each of which corresponds to each item.\n        """"""\n        # pylint: disable=too-many-branches\n        feature_description = dict()\n        for key, value in self._feature_original_types.items():\n            shape = []\n            if len(value) == 3:\n                if isinstance(value[-1], int):\n                    shape = [value[-1]]\n                elif isinstance(value[-1], list):\n                    shape = value\n            if len(value) < 2 or value[1] == \'FixedLenFeature\':\n                feature_description.update(\n                    {key: tf.FixedLenFeature(\n                        shape,\n                        dtypes.get_tf_dtype(value[0]))})\n            elif value[1] == \'VarLenFeature\':\n                feature_description.update(\n                    {key: tf.VarLenFeature(\n                        dtypes.get_tf_dtype(value[0]))})\n        decoded_data = tf.parse_single_example(data, feature_description)\n\n        # Handle TFRecord containing images\n        if isinstance(self._image_options, dict):\n            self._decode_image_str_byte(\n                self._image_options,\n                decoded_data)\n        elif isinstance(self._image_options, HParams):\n            self._decode_image_str_byte(\n                self._image_options.todict(),\n                decoded_data)\n        elif isinstance(self._image_options, list):\n            _ = list(map(\n                lambda x: self._decode_image_str_byte(x, decoded_data),\n                self._image_options))\n\n        # Convert Dtypes\n        for key, value in self._feature_convert_types.items():\n            from_type = decoded_data[key].dtype\n            to_type = dtypes.get_tf_dtype(value)\n            if from_type is to_type:\n                continue\n            elif to_type is tf.string:\n                decoded_data[key] = tf.as_string(decoded_data[key])\n            elif from_type is tf.string:\n                decoded_data[key] = tf.string_to_number(\n                    decoded_data[key], to_type)\n            else:\n                decoded_data[key] = tf.cast(\n                    decoded_data[key], to_type)\n        outputs = decoded_data\n        return [outputs[item] for item in items]\n\n    def list_items(self):\n        """"""Returns the list of item names that the decoder can produce.\n\n        Returns:\n            A list of strings can be passed to :meth:`decode()`.\n        """"""\n        return sorted(list(self._feature_original_types.keys()))\n'"
texar/tf/data/data_utils.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious utilities specific to data processing.\n""""""\n\nimport os\nimport sys\nimport tarfile\nimport zipfile\nimport collections\nimport numpy as np\nfrom six.moves import urllib\nimport requests\n\nimport tensorflow as tf\n\nfrom texar.tf.utils import utils_io\n\n# pylint: disable=invalid-name, too-many-branches\n\n__all__ = [\n    ""maybe_download"",\n    ""read_words"",\n    ""make_vocab"",\n    ""count_file_lines""\n]\n\nPy3 = sys.version_info[0] == 3\n\n\ndef maybe_download(urls, path, filenames=None, extract=False):\n    """"""Downloads a set of files.\n\n    Args:\n        urls: A (list of) urls to download files.\n        path (str): The destination path to save the files.\n        filenames: A (list of) strings of the file names. If given,\n            must have the same length with :attr:`urls`. If `None`,\n            filenames are extracted from :attr:`urls`.\n        extract (bool): Whether to extract compressed files.\n\n    Returns:\n        A list of paths to the downloaded files.\n    """"""\n    utils_io.maybe_create_dir(path)\n\n    if not isinstance(urls, (list, tuple)):\n        urls = [urls]\n    if filenames is not None:\n        if not isinstance(filenames, (list, tuple)):\n            filenames = [filenames]\n        if len(urls) != len(filenames):\n            raise ValueError(\n                \'`filenames` must have the same number of elements as `urls`.\')\n\n    result = []\n    for i, url in enumerate(urls):\n        if filenames is not None:\n            filename = filenames[i]\n        elif \'drive.google.com\' in url:\n            filename = _extract_google_drive_file_id(url)\n        else:\n            filename = url.split(\'/\')[-1]\n            # If downloading from GitHub, remove suffix ?raw=True\n            # from local filename\n            if filename.endswith(""?raw=true""):\n                filename = filename[:-9]\n\n        filepath = os.path.join(path, filename)\n        result.append(filepath)\n\n        if not tf.gfile.Exists(filepath):\n            if \'drive.google.com\' in url:\n                filepath = _download_from_google_drive(url, filename, path)\n            else:\n                filepath = _download(url, filename, path)\n\n            if extract:\n                tf.logging.info(\'Extract %s\', filepath)\n                if tarfile.is_tarfile(filepath):\n                    tarfile.open(filepath, \'r\').extractall(path)\n                elif zipfile.is_zipfile(filepath):\n                    with zipfile.ZipFile(filepath) as zfile:\n                        zfile.extractall(path)\n                else:\n                    tf.logging.info(""Unknown compression type. Only .tar.gz, ""\n                                    "".tar.bz2, .tar, and .zip are supported"")\n\n    return result\n\n\ndef _download(url, filename, path):\n    def _progress(count, block_size, total_size):\n        percent = float(count * block_size) / float(total_size) * 100.\n        # pylint: disable=cell-var-from-loop\n        sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' %\n                         (filename, percent))\n        sys.stdout.flush()\n\n    filepath = os.path.join(path, filename)\n    filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded {} {} bytes.\'.format(\n        filename, statinfo.st_size))\n\n    return filepath\n\n\ndef _extract_google_drive_file_id(url):\n    # id is between `/d/` and \'/\'\n    url_suffix = url[url.find(\'/d/\') + 3:]\n    file_id = url_suffix[:url_suffix.find(\'/\')]\n    return file_id\n\n\ndef _download_from_google_drive(url, filename, path):\n    """"""Adapted from `https://github.com/saurabhshri/gdrive-downloader`\n    """"""\n    def _get_confirm_token(response):\n        for key, value in response.cookies.items():\n            if key.startswith(\'download_warning\'):\n                return value\n        return None\n\n    file_id = _extract_google_drive_file_id(url)\n\n    gurl = ""https://docs.google.com/uc?export=download""\n    sess = requests.Session()\n    response = sess.get(gurl, params={\'id\': file_id}, stream=True)\n    token = _get_confirm_token(response)\n\n    if token:\n        params = {\'id\': file_id, \'confirm\': token}\n        response = sess.get(gurl, params=params, stream=True)\n\n    filepath = os.path.join(path, filename)\n    CHUNK_SIZE = 32768\n    with tf.gfile.GFile(filepath, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk:\n                f.write(chunk)\n\n    print(\'Successfully downloaded {}.\'.format(filename))\n\n    return filepath\n\n\ndef read_words(filename, newline_token=None):\n    """"""Reads word from a file.\n\n    Args:\n        filename (str): Path to the file.\n        newline_token (str, optional): The token to replace the original newline\n            token ""\\\\\\\\n"". For example,\n            `newline_token=tx.data.SpecialTokens.EOS`.\n            If `None`, no replacement is performed.\n\n    Returns:\n        A list of words.\n    """"""\n    with tf.gfile.GFile(filename, ""r"") as f:\n        if Py3:\n            if newline_token is None:\n                return f.read().split()\n            else:\n                return f.read().replace(""\\n"", newline_token).split()\n        else:\n            if newline_token is None:\n                return f.read().decode(""utf-8"").split()\n            else:\n                return (f.read().decode(""utf-8"")\n                        .replace(""\\n"", newline_token).split())\n\n\ndef make_vocab(filenames, max_vocab_size=-1, newline_token=None,\n               return_type=""list"", return_count=False):\n    """"""Builds vocab of the files.\n\n    Args:\n        filenames (str): A (list of) files.\n        max_vocab_size (int): Maximum size of the vocabulary. Low frequency\n            words that exceeding the limit will be discarded.\n            Set to `-1` (default) if no truncation is wanted.\n        newline_token (str, optional): The token to replace the original newline\n            token ""\\\\\\\\n"". For example,\n            `newline_token=tx.data.SpecialTokens.EOS`.\n            If `None`, no replacement is performed.\n        return_type (str): Either ""list"" or ""dict"". If ""list"" (default), this\n            function returns a list of words sorted by frequency. If ""dict"",\n            this function returns a dict mapping words to their index sorted\n            by frequency.\n        return_count (bool): Whether to return word counts. If `True` and\n            :attr:`return_type` is ""dict"", then a count dict is returned, which\n            is a mapping from words to their frequency.\n\n    Returns:\n        - If :attr:`return_count` is False, returns a list or dict containing \\\n        the vocabulary words.\n\n        - If :attr:`return_count` if True, returns a pair of list or dict \\\n        `(a, b)`, where `a` is a list or dict containing the vocabulary \\\n        words, `b` is a list of dict containing the word counts.\n    """"""\n    if not isinstance(filenames, (list, tuple)):\n        filenames = [filenames]\n\n    words = []\n    for fn in filenames:\n        words += read_words(fn, newline_token=newline_token)\n\n    counter = collections.Counter(words)\n    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n    words, counts = list(zip(*count_pairs))\n    if max_vocab_size >= 0:\n        words = words[:max_vocab_size]\n        counts = counts[:max_vocab_size]\n\n    if return_type == ""list"":\n        if not return_count:\n            return words\n        else:\n            return words, counts\n    elif return_type == ""dict"":\n        word_to_id = dict(zip(words, range(len(words))))\n        if not return_count:\n            return word_to_id\n        else:\n            word_to_count = dict(zip(words, counts))\n            return word_to_id, word_to_count\n    else:\n        raise ValueError(""Unknown return_type: {}"".format(return_type))\n\n\ndef count_file_lines(filenames):\n    """"""Counts the number of lines in the file(s).\n    """"""\n    def _count_lines(fn):\n        with open(fn, ""rb"") as f:\n            i = -1\n            for i, _ in enumerate(f):\n                pass\n            return i + 1\n\n    if not isinstance(filenames, (list, tuple)):\n        filenames = [filenames]\n    num_lines = np.sum([_count_lines(fn) for fn in filenames])\n    return num_lines\n'"
texar/tf/data/embedding.py,11,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions and classes for embedding processing.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow import gfile\nimport numpy as np\n\nfrom texar.tf.utils import utils\nfrom texar.tf.hyperparams import HParams\n\n__all__ = [\n    ""load_word2vec"",\n    ""load_glove"",\n    ""Embedding""\n]\n\n\ndef load_word2vec(filename, vocab, word_vecs):\n    """"""Loads embeddings in the word2vec binary format which has a header line\n    containing the number of vectors and their dimensionality (two integers),\n    followed with number-of-vectors lines each of which is formatted as\n    \'<word-string> <embedding-vector>\'.\n\n    Args:\n        filename (str): Path to the embedding file.\n        vocab (dict): A dictionary that maps token strings to integer index.\n            Tokens not in :attr:`vocab` are not read.\n        word_vecs: A 2D numpy array of shape `[vocab_size, embed_dim]`\n            which is updated as reading from the file.\n\n    Returns:\n        The updated :attr:`word_vecs`.\n    """"""\n    with gfile.GFile(filename, ""rb"") as fin:\n        header = fin.readline()\n        vocab_size, vector_size = [int(s) for s in header.split()]\n        if vector_size != word_vecs.shape[1]:\n            raise ValueError(""Inconsistent word vector sizes: %d vs %d"" %\n                             (vector_size, word_vecs.shape[1]))\n        binary_len = np.dtype(\'float32\').itemsize * vector_size\n        for _ in np.arange(vocab_size):\n            chars = []\n            while True:\n                char = fin.read(1)\n                if char == b\' \':\n                    break\n                if char != b\'\\n\':\n                    chars.append(char)\n            word = b\'\'.join(chars)\n            word = tf.compat.as_text(word)\n            if word in vocab:\n                word_vecs[vocab[word]] = np.fromstring(\n                    fin.read(binary_len), dtype=\'float32\')\n            else:\n                fin.read(binary_len)\n    return word_vecs\n\n\ndef load_glove(filename, vocab, word_vecs):\n    """"""Loads embeddings in the glove text format in which each line is\n    \'<word-string> <embedding-vector>\'. Dimensions of the embedding vector\n    are separated with whitespace characters.\n\n    Args:\n        filename (str): Path to the embedding file.\n        vocab (dict): A dictionary that maps token strings to integer index.\n            Tokens not in :attr:`vocab` are not read.\n        word_vecs: A 2D numpy array of shape `[vocab_size, embed_dim]`\n            which is updated as reading from the file.\n\n    Returns:\n        The updated :attr:`word_vecs`.\n    """"""\n    with gfile.GFile(filename) as fin:\n        for line in fin:\n            vec = line.strip().split()\n            if len(vec) == 0:\n                continue\n            word, vec = vec[0], vec[1:]\n            word = tf.compat.as_text(word)\n            if word not in vocab:\n                continue\n            if len(vec) != word_vecs.shape[1]:\n                raise ValueError(""Inconsistent word vector sizes: %d vs %d"" %\n                                 (len(vec), word_vecs.shape[1]))\n            word_vecs[vocab[word]] = np.array([float(v) for v in vec])\n    return word_vecs\n\n\nclass Embedding(object):\n    """"""Embedding class that loads token embedding vectors from file. Token\n    embeddings not in the embedding file are initialized as specified in\n    :attr:`hparams`.\n\n    Args:\n        vocab (dict): A dictionary that maps token strings to integer index.\n        read_fn: Callable that takes `(filename, vocab, word_vecs)` and\n            returns the updated `word_vecs`. E.g.,\n            :func:`~texar.tf.data.embedding.load_word2vec` and\n            :func:`~texar.tf.data.embedding.load_glove`.\n    """"""\n    def __init__(self, vocab, hparams=None):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n        # Initialize embeddings\n        init_fn_kwargs = self._hparams.init_fn.kwargs.todict()\n        if ""shape"" in init_fn_kwargs or ""size"" in init_fn_kwargs:\n            raise ValueError(""Argument \'shape\' or \'size\' must not be ""\n                             ""specified. They are inferred automatically."")\n        init_fn = utils.get_function(\n            self._hparams.init_fn.type,\n            [""numpy.random"", ""numpy"", ""texar.tf.custom""])\n\n        try:\n            self._word_vecs = init_fn(size=[len(vocab), self._hparams.dim],\n                                      **init_fn_kwargs)\n        except TypeError:\n            self._word_vecs = init_fn(shape=[len(vocab), self._hparams.dim],\n                                      **init_fn_kwargs)\n\n        # Optionally read embeddings from file\n        if self._hparams.file is not None and self._hparams.file != """":\n            read_fn = utils.get_function(\n                self._hparams.read_fn,\n                [""texar.tf.data.embedding"", ""texar.tf.data"", ""texar.tf.custom""])\n\n            self._word_vecs = \\\n                read_fn(self._hparams.file, vocab, self._word_vecs)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values:\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                ""file"": """",\n                ""dim"": 50,\n                ""read_fn"": ""load_word2vec"",\n                ""init_fn"": {\n                    ""type"": ""numpy.random.uniform"",\n                    ""kwargs"": {\n                        ""low"": -0.1,\n                        ""high"": 0.1,\n                    }\n                },\n            }\n\n        Here:\n\n        ""file"": str\n            Path to the embedding file. If not provided, all embeddings are\n            initialized with the initialization function.\n\n        ""dim"": int\n            Dimension size of each embedding vector\n\n        ""read_fn"": str or callable\n            Function to read the embedding file. This can be the function,\n            or its string name or full module path. E.g.,\n\n            .. code-block:: python\n\n                ""read_fn"": texar.tf.data.load_word2vec\n                ""read_fn"": ""load_word2vec""\n                ""read_fn"": ""texar.tf.data.load_word2vec""\n                ""read_fn"": ""my_module.my_read_fn""\n\n            If function string name is used, the function must be in\n            one of the modules: :mod:`texar.tf.data` or :mod:`texar.tf.custom`.\n\n            The function must have the same signature as with\n            :func:`load_word2vec`.\n\n        ""init_fn"": dict\n            Hyperparameters of the initialization function used to initialize\n            embedding of tokens missing in the embedding\n            file.\n\n            The function must accept argument named `size` or `shape` to\n            specify the output shape, and return a numpy array of the shape.\n\n            The `dict` has the following fields:\n\n                ""type"": str or callable\n                    The initialization function. Can be either the function,\n                    or its string name or full module path.\n\n                ""kwargs"": dict\n                    Keyword arguments for calling the function. The function\n                    is called with :python:`init_fn(size=[.., ..], **kwargs)`.\n        """"""\n        return {\n            ""file"": """",\n            ""dim"": 50,\n            ""read_fn"": ""load_word2vec"",\n            ""init_fn"": {\n                ""type"": ""numpy.random.uniform"",\n                ""kwargs"": {\n                    ""low"": -0.1,\n                    ""high"": 0.1,\n                },\n            },\n            ""@no_typecheck"": [""read_fn"", ""init_fn""]\n        }\n\n    @property\n    def word_vecs(self):\n        """"""2D numpy array of shape `[vocab_size, embedding_dim]`.\n        """"""\n        return self._word_vecs\n\n    @property\n    def vector_size(self):\n        """"""The embedding dimention size.\n        """"""\n        return self._hparams.dim\n'"
texar/tf/data/vocabulary.py,9,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nHelper functions and classes for vocabulary processing.\n""""""\n\nimport warnings\nfrom collections import defaultdict\n\nimport tensorflow as tf\nfrom tensorflow import gfile\nimport numpy as np\n\nfrom texar.tf.utils.utils import dict_lookup\n\n# pylint: disable=too-few-public-methods, invalid-name\n# pylint: disable=too-many-instance-attributes, too-many-arguments\n\n__all__ = [\n    ""SpecialTokens"",\n    ""Vocab""\n]\n\n\nclass SpecialTokens(object):\n    """"""Special tokens, including :attr:`PAD`, :attr:`BOS`, :attr:`EOS`,\n    :attr:`UNK`. These tokens will by default have token ids 0, 1, 2, 3,\n    respectively.\n    """"""\n    PAD = ""<PAD>""\n    BOS = ""<BOS>""\n    EOS = ""<EOS>""\n    UNK = ""<UNK>""\n\n\ndef _make_defaultdict(keys, values, default_value):\n    """"""Creates a python defaultdict.\n\n    Args:\n        keys (list): Keys of the dictionary.\n        values (list): Values correspond to keys. The two lists :attr:`keys` and\n            :attr:`values` must be of the same length.\n        default_value: default value returned when key is missing.\n\n    Returns:\n        defaultdict: A python `defaultdict` instance that maps keys to values.\n    """"""\n    dict_ = defaultdict(lambda: default_value)\n    for k, v in zip(keys, values):\n        dict_[k] = v\n\n    return dict_\n\n\nclass Vocab(object):\n    """"""Vocabulary class that loads vocabulary from file, and maintains mapping\n    tables between token strings and indexes.\n\n    Each line of the vocab file should contains one vocabulary token, e.g.,::\n\n        vocab_token_1\n        vocab token 2\n        vocab       token | 3 .\n        ...\n\n    Args:\n        filename (str): Path to the vocabulary file where each line contains\n            one token.\n        bos_token (str): A special token that will be added to the beginning of\n            sequences.\n        eos_token (str): A special token that will be added to the end of\n            sequences.\n        unk_token (str): A special token that will replace all unknown tokens\n            (tokens not included in the vocabulary).\n        pad_token (str): A special token that is used to do padding.\n    """"""\n\n    def __init__(self,\n                 filename,\n                 pad_token=SpecialTokens.PAD,\n                 bos_token=SpecialTokens.BOS,\n                 eos_token=SpecialTokens.EOS,\n                 unk_token=SpecialTokens.UNK):\n        self._filename = filename\n        self._pad_token = pad_token\n        self._bos_token = bos_token\n        self._eos_token = eos_token\n        self._unk_token = unk_token\n\n        self._id_to_token_map, self._token_to_id_map, \\\n        self._id_to_token_map_py, self._token_to_id_map_py = \\\n            self.load(self._filename)\n\n    def load(self, filename):\n        """"""Loads the vocabulary from the file.\n\n        Args:\n            filename (str): Path to the vocabulary file.\n\n        Returns:\n            A tuple of TF and python mapping tables between word string and\n            index, (:attr:`id_to_token_map`, :attr:`token_to_id_map`,\n            :attr:`id_to_token_map_py`, :attr:`token_to_id_map_py`), where\n            :attr:`id_to_token_map` and :attr:`token_to_id_map` are\n            TF :tf_main:`HashTable <contrib/lookup/HashTable>` instances,\n            and :attr:`id_to_token_map_py` and\n            :attr:`token_to_id_map_py` are python `defaultdict` instances.\n        """"""\n        with gfile.GFile(filename) as vocab_file:\n            # Converts to \'unicode\' (Python 2) or \'str\' (Python 3)\n            vocab = list(tf.compat.as_text(line.strip()) for line in vocab_file)\n\n        warnings.simplefilter(""ignore"", UnicodeWarning)\n\n        if self._bos_token in vocab:\n            raise ValueError(""Special begin-of-seq token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._bos_token)\n        if self._eos_token in vocab:\n            raise ValueError(""Special end-of-seq token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._eos_token)\n        if self._unk_token in vocab:\n            raise ValueError(""Special UNK token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._unk_token)\n        if self._pad_token in vocab:\n            raise ValueError(""Special padding token already exists in the ""\n                             ""vocabulary: \'%s\'"" % self._pad_token)\n\n        warnings.simplefilter(""default"", UnicodeWarning)\n\n        # Places _pad_token at the beginning to make sure it take index 0.\n        vocab = [self._pad_token, self._bos_token, self._eos_token,\n                 self._unk_token] + vocab\n        # Must make sure this is consistent with the above line\n        unk_token_idx = 3\n        vocab_size = len(vocab)\n        vocab_idx = np.arange(vocab_size)\n\n        # Creates TF maps\n        id_to_token_map = tf.contrib.lookup.HashTable(\n            tf.contrib.lookup.KeyValueTensorInitializer(\n                vocab_idx, vocab, key_dtype=tf.int64, value_dtype=tf.string),\n            self._unk_token)\n\n        token_to_id_map = tf.contrib.lookup.HashTable(\n            tf.contrib.lookup.KeyValueTensorInitializer(\n                vocab, vocab_idx, key_dtype=tf.string, value_dtype=tf.int64),\n            unk_token_idx)\n\n        # Creates python maps to interface with python code\n        id_to_token_map_py = _make_defaultdict(\n            vocab_idx, vocab, self._unk_token)\n        token_to_id_map_py = _make_defaultdict(\n            vocab, vocab_idx, unk_token_idx)\n\n        return id_to_token_map, token_to_id_map, \\\n               id_to_token_map_py, token_to_id_map_py\n\n    def map_ids_to_tokens(self, ids):\n        """"""Maps ids into text tokens.\n\n        The returned tokens are a Tensor.\n\n        Args:\n            ids: An `int` tensor of token ids.\n\n        Returns:\n            A tensor of text tokens of the same shape.\n        """"""\n        return self.id_to_token_map.lookup(tf.cast(ids, tf.int64))\n\n    def map_tokens_to_ids(self, tokens):\n        """"""Maps text tokens into ids.\n\n        The returned ids are a Tensor.\n\n        Args:\n            tokens: An tensor of text tokens.\n\n        Returns:\n            A tensor of token ids of the same shape.\n        """"""\n        return self.token_to_id_map.lookup(tokens)\n\n    def map_ids_to_tokens_py(self, ids):\n        """"""Maps ids into text tokens.\n\n        The input :attr:`ids` and returned tokens are both python\n        arrays or list.\n\n        Args:\n            ids: An `int` numpy arry or (possibly nested) list of token ids.\n\n        Returns:\n            A numpy array of text tokens of the same shape as :attr:`ids`.\n        """"""\n        return dict_lookup(self.id_to_token_map_py, ids, self.unk_token)\n\n    def map_tokens_to_ids_py(self, tokens):\n        """"""Maps text tokens into ids.\n\n        The input :attr:`tokens` and returned ids are both python\n        arrays or list.\n\n        Args:\n            tokens: A numpy array or (possibly nested) list of text tokens.\n\n        Returns:\n            A numpy array of token ids of the same shape as :attr:`tokens`.\n        """"""\n        return dict_lookup(self.token_to_id_map_py, tokens, self.unk_token_id)\n\n    @property\n    def id_to_token_map(self):\n        """"""The :tf_main:`HashTable <contrib/lookup/HashTable>` instance that\n        maps from token index to the string form.\n        """"""\n        return self._id_to_token_map\n\n    @property\n    def token_to_id_map(self):\n        """"""The :tf_main:`HashTable <contrib/lookup/HashTable>` instance\n        that maps from token string to the index.\n        """"""\n        return self._token_to_id_map\n\n    @property\n    def id_to_token_map_py(self):\n        """"""The python `defaultdict` instance that maps from token index to the\n        string form.\n        """"""\n        return self._id_to_token_map_py\n\n    @property\n    def token_to_id_map_py(self):\n        """"""The python `defaultdict` instance that maps from token string to the\n        index.\n        """"""\n        return self._token_to_id_map_py\n\n    @property\n    def size(self):\n        """"""The vocabulary size.\n        """"""\n        return len(self.token_to_id_map_py)\n\n    @property\n    def bos_token(self):\n        """"""A string of the special token indicating the beginning of sequence.\n        """"""\n        return self._bos_token\n\n    @property\n    def bos_token_id(self):\n        """"""The `int` index of the special token indicating the beginning\n        of sequence.\n        """"""\n        return self.token_to_id_map_py[self._bos_token]\n\n    @property\n    def eos_token(self):\n        """"""A string of the special token indicating the end of sequence.\n        """"""\n        return self._eos_token\n\n    @property\n    def eos_token_id(self):\n        """"""The `int` index of the special token indicating the end\n        of sequence.\n        """"""\n        return self.token_to_id_map_py[self._eos_token]\n\n    @property\n    def unk_token(self):\n        """"""A string of the special token indicating unknown token.\n        """"""\n        return self._unk_token\n\n    @property\n    def unk_token_id(self):\n        """"""The `int` index of the special token indicating unknown token.\n        """"""\n        return self.token_to_id_map_py[self._unk_token]\n\n    @property\n    def pad_token(self):\n        """"""A string of the special token indicating padding token. The\n        default padding token is an empty string.\n        """"""\n        return self._pad_token\n\n    @property\n    def pad_token_id(self):\n        """"""The `int` index of the special token indicating padding token.\n        """"""\n        return self.token_to_id_map_py[self._pad_token]\n\n    @property\n    def special_tokens(self):\n        """"""The list of special tokens\n        [:attr:`pad_token`, :attr:`bos_token`, :attr:`eos_token`,\n        :attr:`unk_token`].\n        """"""\n        return [self._pad_token, self._bos_token, self._eos_token,\n                self._unk_token]\n'"
texar/tf/evals/__init__.py,3,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library evals.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.evals.bleu_moses import *\nfrom texar.tf.evals.bleu import *\nfrom texar.tf.evals.metrics import *\n'"
texar/tf/evals/bleu.py,1,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2018 Texar\n# ==============================================================================\n""""""\nPython implementation of BLEU and smoothed BLEU adapted from:\n    `https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py`\n\nThis module provides a Python implementation of BLEU and smoothed BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\n\n    (Lin et al. 2004) ORANGE: a method for evaluating automatic evaluation\n    metrics for maching translation.\n    Chin-Yew Lin, Franz Josef Och. COLING 2004.\n""""""\n\nimport collections\nimport math\n\nfrom texar.tf.utils.dtypes import compat_as_text, is_str\n\n# pylint: disable=invalid-name, too-many-branches, too-many-locals\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""sentence_bleu"",\n    ""corpus_bleu""\n]\n\n\ndef _get_ngrams(segment, max_order):\n    """"""Extracts all n-grams up to a given maximum order from an input segment.\n\n    Args:\n        segment: text segment from which n-grams will be extracted.\n        max_order: maximum length in tokens of the n-grams returned by this\n            methods.\n\n    Returns:\n        The Counter containing all n-grams upto max_order in segment\n        with a count of how many times each n-gram occurred.\n    """"""\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef _maybe_str_to_list(list_or_str):\n    if is_str(list_or_str):\n        return list_or_str.split()\n    return list_or_str\n\n\ndef _lowercase(str_list):\n    return [str_.lower() for str_ in str_list]\n\n\ndef sentence_bleu(references, hypothesis, max_order=4, lowercase=False,\n                  smooth=False, return_all=False):\n    """"""Calculates BLEU score of a hypothesis sentence.\n\n    Args:\n        references: A list of reference for the hypothesis.\n            Each reference can be either a list of string tokens, or a string\n            containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        hypotheses: A hypothesis sentence.\n            Each hypothesis can be either a list of string tokens, or a\n            string containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        lowercase (bool): If `True`, lowercase reference and hypothesis tokens.\n        max_order (int): Maximum n-gram order to use when computing BLEU score.\n        smooth (bool): Whether or not to apply (Lin et al. 2004) smoothing.\n        return_all (bool): If `True`, returns BLEU and all n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a float32\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of float32 scores:\n        `[BLEU] + n-gram precisions`, which is of length :attr:`max_order` + 1.\n    """"""\n    return corpus_bleu(\n        [references], [hypothesis], max_order=max_order, lowercase=lowercase,\n        smooth=smooth, return_all=return_all)\n\n\ndef corpus_bleu(list_of_references, hypotheses, max_order=4, lowercase=False,\n                smooth=False, return_all=True):\n    """"""Computes corpus-level BLEU score.\n\n    Args:\n        list_of_references: A list of lists of references for each hypothesis.\n            Each reference can be either a list of string tokens, or a string\n            containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        hypotheses: A list of hypothesis sentences.\n            Each hypothesis can be either a list of string tokens, or a\n            string containing tokenized tokens separated with whitespaces.\n            List can also be numpy array.\n        lowercase (bool): If `True`, lowercase reference and hypothesis tokens.\n        max_order (int): Maximum n-gram order to use when computing BLEU score.\n        smooth (bool): Whether or not to apply (Lin et al. 2004) smoothing.\n        return_all (bool): If `True`, returns BLEU and all n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a float32\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of float32 scores:\n        `[BLEU] + n-gram precisions`, which is of length :attr:`max_order` + 1.\n    """"""\n    list_of_references = compat_as_text(list_of_references)\n    hypotheses = compat_as_text(hypotheses)\n\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    hyperthsis_length = 0\n    for (references, hyperthsis) in zip(list_of_references, hypotheses):\n        reference_length += min(len(r) for r in references)\n        hyperthsis_length += len(hyperthsis)\n\n        merged_ref_ngram_counts = collections.Counter()\n        for reference in references:\n            reference = _maybe_str_to_list(reference)\n            if lowercase:\n                reference = _lowercase(reference)\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n\n        hyperthsis = _maybe_str_to_list(hyperthsis)\n        if lowercase:\n            hyperthsis = _lowercase(hyperthsis)\n        hyperthsis_ngram_counts = _get_ngrams(hyperthsis, max_order)\n\n        overlap = hyperthsis_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for order in range(1, max_order + 1):\n            possible_matches = len(hyperthsis) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order - 1] += possible_matches\n\n    precisions = [0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = ((matches_by_order[i] + 1.) /\n                             (possible_matches_by_order[i] + 1.))\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = (float(matches_by_order[i]) /\n                                 possible_matches_by_order[i])\n            else:\n                precisions[i] = 0.0\n\n    if min(precisions) > 0:\n        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    ratio = float(hyperthsis_length) / reference_length\n\n    if ratio > 1.0:\n        bp = 1.\n    else:\n        try:\n            bp = math.exp(1 - 1. / ratio)\n        except ZeroDivisionError:\n            bp = math.exp(1 - 1. / (ratio + 1e-8))\n\n    bleu = geo_mean * bp\n\n    if return_all:\n        return [bleu * 100] + [p * 100 for p in precisions]\n    else:\n        return bleu * 100\n'"
texar/tf/evals/bleu_moses.py,3,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe BLEU metric.\n""""""\n\nimport os\nfrom io import open  # pylint: disable=redefined-builtin\nimport shutil\nimport re\nimport subprocess\nimport tempfile\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.dtypes import compat_as_text\n\n# pylint: disable=too-many-locals, no-member, redefined-variable-type\n\n__all__ = [\n    ""sentence_bleu_moses"",\n    ""corpus_bleu_moses""\n]\n\n\ndef _maybe_list_to_str(list_or_str):\n    if isinstance(list_or_str, (tuple, list, np.ndarray)):\n        return \' \'.join(list_or_str)\n    return list_or_str\n\n\ndef _parse_multi_bleu_ret(bleu_str, return_all=False):\n    bleu_score = re.search(r""BLEU = (.+?),"", bleu_str).group(1)\n    bleu_score = np.float32(bleu_score)\n\n    if return_all:\n        bleus = re.search(r"", (.+?)/(.+?)/(.+?)/(.+?) "", bleu_str)\n        bleus = [bleus.group(group_idx) for group_idx in range(1, 5)]\n        bleus = [np.float32(b) for b in bleus]\n        bleu_score = [bleu_score] + bleus\n\n    return bleu_score\n\n\ndef sentence_bleu_moses(references, hypothesis, lowercase=False,\n                        return_all=False):\n    """"""Calculates BLEU score of a hypothesis sentence using the\n    **MOSES multi-bleu.perl** script.\n\n    Args:\n        references: A list of reference for the hypothesis.\n            Each reference can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        hypotheses: A hypothesis sentence.\n            The hypothesis can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        lowercase (bool): If `True`, pass the ""-lc"" flag to the multi-bleu\n            script.\n        return_all (bool): If `True`, returns BLEU and all n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a float32\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of 5 float32 scores:\n        `[BLEU, 1-gram precision, ..., 4-gram precision]`.\n    """"""\n    return corpus_bleu_moses(\n        [references], [hypothesis], lowercase=lowercase, return_all=return_all)\n\n\ndef corpus_bleu_moses(list_of_references, hypotheses, lowercase=False,\n                      return_all=False):\n    """"""Calculates corpus-level BLEU score using the\n    **MOSES multi-bleu.perl** script.\n\n    Args:\n        list_of_references: A list of lists of references for each hypothesis.\n            Each reference can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        hypotheses: A list of hypothesis sentences.\n            Each hyperthsis can be either a string, or a list of string tokens.\n            List can also be numpy array.\n        lowercase (bool): If `True`, pass the ""-lc"" flag to the multi-bleu\n            script.\n        return_all (bool): If `True`, returns BLEU and all n-gram precisions.\n\n    Returns:\n        If :attr:`return_all` is `False` (default), returns a float32\n        BLEU score.\n\n        If :attr:`return_all` is `True`, returns a list of 5 float32 scores:\n        `[BLEU, 1-gram precision, ..., 4-gram precision]`.\n    """"""\n    list_of_references = compat_as_text(list_of_references)\n    hypotheses = compat_as_text(hypotheses)\n\n    if np.size(hypotheses) == 0:\n        return np.float32(0.)   # pylint: disable=no-member\n\n    # Get multi-bleu.perl\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    multi_bleu_path = os.path.abspath(\n        os.path.join(cur_dir, "".."", "".."", "".."", ""bin"", ""utils"",\n                     ""multi-bleu.perl""))\n\n    # Create a temporary folder containing hyperthesis and reference files\n    result_path = tempfile.mkdtemp()\n    # Create hyperthesis file\n    hfile_path = os.path.join(result_path, \'hyp\')\n    hyps = [_maybe_list_to_str(h) for h in hypotheses]\n    with open(hfile_path, \'w\', encoding=\'utf-8\') as hfile:\n        text = ""\\n"".join(hyps)\n        hfile.write(text)\n        hfile.write(""\\n"")\n    # Create reference files\n    max_nrefs = max([len(refs) for refs in list_of_references])\n    rfile_path = os.path.join(result_path, \'ref\')\n    for rid in range(max_nrefs):\n        with open(rfile_path + \'%d\' % rid, \'w\', encoding=\'utf-8\') as rfile:\n            for refs in list_of_references:\n                if rid < len(refs):\n                    ref = _maybe_list_to_str(refs[rid])\n                    rfile.write(ref + ""\\n"")\n                else:\n                    rfile.write(""\\n"")\n\n    # Calculate BLEU\n    multi_bleu_cmd = [multi_bleu_path]\n    if lowercase:\n        multi_bleu_cmd += [""-lc""]\n    multi_bleu_cmd += [rfile_path]\n    with open(hfile_path, ""r"") as hyp_input:\n        try:\n            multi_bleu_ret = subprocess.check_output(\n                multi_bleu_cmd, stdin=hyp_input, stderr=subprocess.STDOUT)\n            multi_bleu_ret = multi_bleu_ret.decode(""utf-8"")\n            bleu_score = _parse_multi_bleu_ret(multi_bleu_ret, return_all)\n        except subprocess.CalledProcessError as error:\n            if error.output is not None:\n                tf.logging.warning(\n                    ""multi-bleu.perl returned non-zero exit code"")\n                tf.logging.warning(error.output)\n            if return_all:\n                bleu_score = [np.float32(0.0)] * 5\n            else:\n                bleu_score = np.float32(0.0)\n\n    shutil.rmtree(result_path)\n\n    return np.float32(bleu_score)\n'"
texar/tf/evals/metrics.py,6,"b'""""""\nVarious metrics.\n""""""\n\nimport tensorflow as tf\n\n__all__ = [\n    ""accuracy"",\n    ""binary_clas_accuracy""\n]\n\n\ndef accuracy(labels, preds):\n    """"""Calculates the accuracy of predictions.\n\n    Args:\n        labels: The ground truth values. A Tensor of the same shape of\n            :attr:`preds`.\n        preds: A Tensor of any shape containing the predicted values.\n\n    Returns:\n        A float scalar Tensor containing the accuracy.\n    """"""\n    labels = tf.cast(labels, preds.dtype)\n    return tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n\n\ndef binary_clas_accuracy(pos_preds=None, neg_preds=None):\n    """"""Calculates the accuracy of binary predictions.\n\n    Args:\n        pos_preds (optional): A Tensor of any shape containing the\n            predicted values on positive data (i.e., ground truth labels are\n            `1`).\n        neg_preds (optional): A Tensor of any shape containing the\n            predicted values on negative data (i.e., ground truth labels are\n            `0`).\n\n    Returns:\n        A float scalar Tensor containing the accuracy.\n    """"""\n    pos_accu = accuracy(tf.ones_like(pos_preds), pos_preds)\n    neg_accu = accuracy(tf.zeros_like(neg_preds), neg_preds)\n    psize = tf.cast(tf.size(pos_preds), tf.float32)\n    nsize = tf.cast(tf.size(neg_preds), tf.float32)\n    accu = (pos_accu * psize + neg_accu * nsize) / (psize + nsize)\n    return accu\n'"
texar/tf/losses/__init__.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar losses.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.losses.losses_utils import *\nfrom texar.tf.losses.mle_losses import *\nfrom texar.tf.losses.pg_losses import *\nfrom texar.tf.losses.adv_losses import *\nfrom texar.tf.losses.rewards import *\nfrom texar.tf.losses.entropy import *\n'"
texar/tf/losses/adv_losses.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nAdversarial losses.\n""""""\n\nimport tensorflow as tf\n\n\ndef binary_adversarial_losses(real_data,\n                              fake_data,\n                              discriminator_fn,\n                              mode=""max_real""):\n    """"""Computes adversarial losses of real/fake binary discrimination game.\n\n    .. role:: python(code)\n       :language: python\n\n    Args:\n        real_data (Tensor or array): Real data of shape\n            `[num_real_examples, ...]`.\n        fake_data (Tensor or array): Fake data of shape\n            `[num_fake_examples, ...]`. `num_real_examples` does not\n            necessarily equal `num_fake_examples`.\n        discriminator_fn: A callable takes data (e.g., :attr:`real_data` and\n            :attr:`fake_data`) and returns the logits of being real. The\n            signature of `discriminator_fn` must be:\n            :python:`logits, ... = discriminator_fn(data)`.\n            The return value of `discriminator_fn` can be the logits, or\n            a tuple where the logits are the first element.\n\n        mode (str): Mode of the generator loss. Either ""max_real"" or ""min_fake"".\n\n            - **""max_real""** (default): minimizing the generator loss is to\\\n            maximize the probability of fake data being classified as real.\n\n            - **""min_fake""**: minimizing the generator loss is to minimize the\\\n            probability of fake data being classified as fake.\n\n    Returns:\n        A tuple `(generator_loss, discriminator_loss)` each of which is\n        a scalar Tensor, loss to be minimized.\n    """"""\n    real_logits = discriminator_fn(real_data)\n    if isinstance(real_logits, (list, tuple)):\n        real_logits = real_logits[0]\n    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        logits=real_logits, labels=tf.ones_like(real_logits)))\n\n    fake_logits = discriminator_fn(fake_data)\n    if isinstance(fake_logits, (list, tuple)):\n        fake_logits = fake_logits[0]\n    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n        logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n\n    d_loss = real_loss + fake_loss\n\n    if mode == ""min_fake"":\n        g_loss = - fake_loss\n    elif mode == ""max_real"":\n        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=fake_logits, labels=tf.ones_like(fake_logits)))\n    else:\n        raise ValueError(""Unknown mode: %s. Only \'min_fake\' and \'max_real\' ""\n                         ""are allowed."")\n\n    return g_loss, d_loss\n'"
texar/tf/losses/entropy.py,5,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious entropies.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.losses.losses_utils import mask_and_reduce, reduce_dimensions\nfrom texar.tf.utils.shapes import get_rank\n\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""entropy_with_logits"",\n    ""sequence_entropy_with_logits""\n]\n\n\ndef _get_entropy(logits):\n    probs = tf.nn.softmax(logits) + 1e-8\n    entropy = - probs * tf.log(probs)\n    entropy = tf.reduce_sum(entropy, -1)\n    return entropy\n\n\ndef entropy_with_logits(logits,\n                        rank=None,\n                        average_across_batch=True,\n                        average_across_remaining=False,\n                        sum_over_batch=False,\n                        sum_over_remaining=True):\n    """"""Shannon entropy given logits.\n\n    Args:\n        logits: Unscaled log probabilities of shape\n            `[batch_size, d_2, ..., d_{rank-1}, distribution_dim]`\n            and of dtype `float32` or `float64`.\n\n            The rank of the tensor is optionally specified by the argument\n            :attr:`rank`.\n\n            The tensor is considered as having `[batch_size, .., d_{rank-1}]`\n            elements, each of which has a distribution of length `d_rank`\n            (i.e., `distribution_dim`). So the last dimension is always\n            summed out to compute the entropy.\n        rank (int, optional): The rank of :attr:`logits`.\n            If `None` (default), `rank` is inferred automatically from\n            `logits`. If the inference fails, `rank` is\n            set to 2, i.e., assuming :attr:`logits` is of shape\n            `[batch_size, distribution_dim]`\n        average_across_batch (bool): If set, average the entropy across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the entropy across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 3.\n        sum_over_batch (bool): If set, sum the entropy across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_remaining (bool): If set, sum the entropy across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 3.\n\n    Returns:\n        A Tensor containing the shannon entropy. The dimensionality of the\n        Tensor depends on the configuration of reduction arguments. For\n        example, if both batch and remaining dimensions are reduced (by\n        either sum or average), the returned Tensor is a scalar Tensor.\n    """"""\n    entropy = _get_entropy(logits)\n\n    if rank is None:\n        rank = get_rank(logits)\n    if rank is None:\n        rank = 2\n    rank -= 1  # reduced last dimension\n\n    # Reduces\n    if average_across_batch and sum_over_batch:\n        raise ValueError(""Only one of `average_across_batch` and ""\n                         ""`sum_over_batch` can be set."")\n    if average_across_remaining and sum_over_remaining:\n        raise ValueError(""Only one of `average_across_remaining` and ""\n                         ""`sum_over_remaining` can be set."")\n    sum_axes, average_axes = [], []\n    if sum_over_batch:\n        sum_axes.append(0)\n    if average_across_batch:\n        average_axes.append(0)\n    if sum_over_remaining and rank >= 2:\n        sum_axes += list(range(1, rank))\n    if average_across_remaining and rank >= 2:\n        average_axes += list(range(1, rank))\n\n    entropy = reduce_dimensions(\n        entropy, average_axes=average_axes, sum_axes=sum_axes)\n\n    return entropy\n\n\ndef sequence_entropy_with_logits(logits,\n                                 rank=None,\n                                 sequence_length=None,\n                                 average_across_batch=True,\n                                 average_across_timesteps=False,\n                                 average_across_remaining=False,\n                                 sum_over_batch=False,\n                                 sum_over_timesteps=True,\n                                 sum_over_remaining=True,\n                                 time_major=False):\n    """"""Shannon entropy given logits.\n\n    Args:\n        logits: Unscaled log probabilities of shape\n            `[batch_size, max_time, d_3, ..., d_{rank-1}, distribution_dim]`\n            and of dtype `float32` or `float64`.\n\n            The rank of the tensor is optionally specified by the argument\n            :attr:`rank`.\n\n            The tensor is considered as having `[batch_size, .., d_{rank-1}]`\n            elements, each of which has a distribution of length `d_rank`\n            (i.e., `distribution_dim`). So the last dimension is always\n            summed out to compute the entropy.\n\n            The batch and time dimensions are exchanged if :attr:`time_major`\n            is `True`.\n        rank (int, optional): The rank of :attr:`logits`.\n            If `None` (default), `rank` is inferred automatically from\n            `logits`. If the inference fails, `rank` is\n            set to 3, i.e., assuming `logits` is of shape\n            `[batch_size, max_time, distribution_dim]`\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths are\n            counted into the entropy.\n        average_across_timesteps (bool): If set, average the entropy across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the entropy across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the entropy across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 4.\n        sum_over_timesteps (bool): If set, sum the entropy across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the entropy across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_remaining (bool): If set, sum the entropy across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time.\n            Used only when :attr:`logits` has rank >= 4.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`logits` must have shape `[max_time, batch_size, ...]`.\n            If `False` (default), it must have shape\n            `[batch_size, max_time, ...]`.\n\n    Returns:\n        A Tensor containing the shannon entropy. The dimensionality of the\n        Tensor depends on the configuration of reduction arguments. For\n        example, if batch, time, and remaining dimensions are all reduced (by\n        either sum or average), the returned Tensor is a scalar Tensor.\n    """"""\n    entropy = _get_entropy(logits)\n\n    if rank is None:\n        rank = get_rank(logits)\n    if rank is None:\n        rank = 3\n    rank -= 1  # reduced last dimension\n\n    entropy = mask_and_reduce(\n        entropy,\n        sequence_length,\n        rank=rank,\n        average_across_batch=average_across_batch,\n        average_across_timesteps=average_across_timesteps,\n        average_across_remaining=average_across_remaining,\n        sum_over_batch=sum_over_batch,\n        sum_over_timesteps=sum_over_timesteps,\n        sum_over_remaining=sum_over_remaining,\n        time_major=time_major)\n\n    return entropy\n'"
texar/tf/losses/losses_utils.py,14,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious utilities for losses.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn          # pylint: disable=E0611\n\nfrom texar.tf.utils.shapes import mask_sequences\n\n# pylint: disable=invalid-name, not-context-manager, protected-access,\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""mask_and_reduce"",\n    ""reduce_batch_time"",\n    ""reduce_dimensions""\n]\n\n\ndef mask_and_reduce(sequence,\n                    sequence_length,\n                    rank=2,\n                    average_across_batch=True,\n                    average_across_timesteps=False,\n                    average_across_remaining=False,\n                    sum_over_batch=False,\n                    sum_over_timesteps=True,\n                    sum_over_remaining=True,\n                    dtype=None,\n                    time_major=False):\n    """"""Masks out sequence entries that are beyond the respective sequence\n    lengths, and reduces (average or sum) away dimensions.\n\n    This is a combination of :func:`~texar.tf.utils.shapes.mask_sequences`\n    and :func:`~texar.tf.losses.losses_utils.reduce_batch_time`.\n\n    Args:\n        sequence: A Tensor of sequence values.\n            If `time_major=False` (default), this must be a Tensor of shape\n            `[batch_size, max_time, d_2, ..., d_rank]`, where the rank of\n            the Tensor is specified with :attr:`rank`.\n            The batch and time dimensions are exchanged if `time_major` is True.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will be made zero. If `None`,\n            not masking is performed.\n        rank (int): The rank of :attr:`sequence`. Must be >= 2. Default is 2,\n            i.e., `sequence` is a 2D Tensor consisting of batch and time\n            dimensions.\n        average_across_timesteps (bool): If set, average the sequence across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the sequence across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the sequence across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_remaining (bool): If set, sum the loss across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`sequence` must have shape `[max_time, batch_size, ...]`.\n            If `False` (default), `sequence` must have\n            shape `[batch_size, max_time, ...]`.\n        dtype (dtype): Type of :attr:`sequence`. If `None`, infer from\n            :attr:`sequence` automatically.\n\n    Returns\n        A Tensor containing the masked and reduced sequence.\n    """"""\n    if rank < 2:\n        raise ValueError(\'`rank` must be >= 2.\')\n\n    if time_major:\n        sequence = rnn._transpose_batch_time(sequence)\n\n    if sequence_length is not None:\n        sequence = mask_sequences(sequence, sequence_length, dtype=dtype,\n                                  time_major=False, tensor_rank=rank)\n\n    if rank > 2:\n        if average_across_remaining and sum_over_remaining:\n            raise ValueError(""Only one of `average_across_remaining` and ""\n                             ""`sum_over_remaining` can be set."")\n        if average_across_remaining:\n            sequence = tf.reduce_mean(sequence, axis=np.arange(2, rank))\n        elif sum_over_remaining:\n            sequence = tf.reduce_sum(sequence, axis=np.arange(2, rank))\n\n    sequence = reduce_batch_time(sequence,\n                                 sequence_length,\n                                 average_across_batch,\n                                 average_across_timesteps,\n                                 sum_over_batch,\n                                 sum_over_timesteps)\n\n    reduce_time = average_across_timesteps or sum_over_timesteps\n    reduce_batch = average_across_batch or sum_over_batch\n    if not reduce_time and not reduce_batch and time_major:\n        sequence = rnn._transpose_batch_time(sequence)\n\n    return sequence\n\n\ndef reduce_batch_time(sequence,\n                      sequence_length,\n                      average_across_batch=True,\n                      average_across_timesteps=False,\n                      sum_over_batch=False,\n                      sum_over_timesteps=True):\n    """"""Average or sum over the respective dimensions of :attr:`sequence`, which\n    is of shape `[batch_size, max_time, ...]`.\n\n    Assumes :attr:`sequence` has been properly masked according to\n    :attr:`sequence_length`.\n    """"""\n    if average_across_timesteps and sum_over_timesteps:\n        raise ValueError(""Only one of `average_across_timesteps` and ""\n                         ""`sum_over_timesteps` can be set."")\n    if average_across_batch and sum_over_batch:\n        raise ValueError(""Only one of `average_across_batch` and ""\n                         ""`sum_over_batch` can be set."")\n\n    if sum_over_timesteps:\n        sequence = tf.reduce_sum(sequence, axis=[1])\n    elif average_across_timesteps:\n        if sequence_length is None:\n            sequence = tf.reduce_mean(sequence, axis=[1])\n        else:\n            sequence = tf.reduce_sum(sequence, axis=[1]) / \\\n                       tf.cast(sequence_length, sequence.dtype)\n\n    if sum_over_batch:\n        sequence = tf.reduce_sum(sequence, axis=[0])\n    elif average_across_batch:\n        sequence = tf.reduce_mean(sequence, axis=[0])\n\n    return sequence\n\n\ndef reduce_dimensions(tensor, average_axes=None, sum_axes=None, keepdims=None):\n    """"""Average or sum over dimensions of :attr:`tensor`.\n\n    :attr:`average_axes` and :attr:`sum_axes` must be mutually exclusive. That\n    is, elements in `average_axes` must not be contained in\n    `sum_axes`, and vice versa.\n\n    Args:\n        tensor: A tensor to reduce.\n        average_axes (optional): A (list of) `int` that indicates the\n            dimensions to reduce by taking average.\n        sum_axes (optional): A (list of) `int` that indicates the\n            dimensions to reduce by taking sum.\n        keepdims (optional): If `True`, retains reduced dimensions with\n            length 1.\n    """"""\n    reduced_axes = set()\n    if average_axes is not None:\n        if not isinstance(average_axes, (list, tuple)):\n            average_axes = [average_axes]\n        if len(average_axes) > 0:\n            tensor = tf.reduce_mean(tensor, axis=average_axes, keepdims=True)\n            reduced_axes.update(average_axes)\n\n    if sum_axes is not None:\n        if not isinstance(sum_axes, (list, tuple)):\n            sum_axes = [sum_axes]\n        if len(sum_axes) > 0:\n            tensor = tf.reduce_sum(tensor, axis=sum_axes, keepdims=True)\n            reduced_axes.update(sum_axes)\n\n            if average_axes is not None:\n                if len(reduced_axes) != len(average_axes) + len(sum_axes):\n                    raise ValueError(\'`average_axes` and `sum_axes` must not \'\n                                     \'have overlapped elements.\')\n    if not keepdims:\n        tensor = tf.squeeze(tensor, axis=list(reduced_axes))\n\n    return tensor\n'"
texar/tf/losses/mle_losses.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious losses\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.losses.losses_utils import mask_and_reduce, reduce_dimensions\nfrom texar.tf.utils import shapes\n\n# pylint: disable=invalid-name, not-context-manager, protected-access,\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""sequence_softmax_cross_entropy"",\n    ""sequence_sparse_softmax_cross_entropy"",\n    ""sequence_sigmoid_cross_entropy"",\n    ""binary_sigmoid_cross_entropy"",\n    ""binary_sigmoid_cross_entropy_with_clas""\n]\n\n\ndef sequence_softmax_cross_entropy(labels,\n                                   logits,\n                                   sequence_length,\n                                   average_across_batch=True,\n                                   average_across_timesteps=False,\n                                   sum_over_batch=False,\n                                   sum_over_timesteps=True,\n                                   time_major=False,\n                                   stop_gradient_to_label=False,\n                                   name=None):\n    """"""Computes softmax cross entropy for each time step of sequence\n    predictions.\n\n    Args:\n        labels: Target class distributions.\n\n            - If :attr:`time_major` is `False` (default), this must be a\\\n            Tensor of shape `[batch_size, max_time, num_classes]`.\n\n            - If `time_major` is `True`, this must be a Tensor of shape\\\n            `[max_time, batch_size, num_classes]`.\n\n            Each row of `labels` should be a valid probability\n            distribution, otherwise, the computation of the gradient will be\n            incorrect.\n        logits: Unscaled log probabilities. This must have the shape of\n            `[max_time, batch_size, num_classes]` or\n            `[batch_size, max_time, num_classes]` according to\n            the value of `time_major`.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will have zero losses.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`labels` and :attr:`logits` must have shape\n            `[max_time, batch_size, ...]`. If `False`\n            (default), they must have shape `[batch_size, max_time, ...]`.\n        stop_gradient_to_label (bool): If set, gradient propagation to\n            :attr:`labels` will be disabled.\n        name (str, optional): A name for the operation.\n\n    Returns:\n        A Tensor containing the loss, of rank 0, 1, or 2 depending on the\n        arguments :attr:`{average_across}/{sum_over}_{timesteps}/{batch}`.\n        For example:\n\n        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`  \\\n        are `True` (default), the return Tensor is of rank 0.\n\n        - If :attr:`average_across_batch` is `True` and other arguments are \\\n        `False`, the return Tensor is of shape `[max_time]`.\n    """"""\n    with tf.name_scope(name, ""sequence_softmax_cross_entropy""):\n        if stop_gradient_to_label:\n            labels = tf.stop_gradient(labels)\n\n        losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n            labels=labels, logits=logits)\n\n        losses = mask_and_reduce(\n            losses,\n            sequence_length,\n            rank=2,\n            average_across_batch=average_across_batch,\n            average_across_timesteps=average_across_timesteps,\n            sum_over_batch=sum_over_batch,\n            sum_over_timesteps=sum_over_timesteps,\n            time_major=time_major)\n\n        return losses\n\n\ndef sequence_sparse_softmax_cross_entropy(labels,\n                                          logits,\n                                          sequence_length,\n                                          average_across_batch=True,\n                                          average_across_timesteps=False,\n                                          sum_over_batch=False,\n                                          sum_over_timesteps=True,\n                                          time_major=False,\n                                          name=None):\n    """"""Computes sparse softmax cross entropy for each time step of sequence\n    predictions.\n\n    Args:\n        labels: Target class indexes. I.e., classes are mutually exclusive\n            (each entry is in exactly one class).\n\n            - If :attr:`time_major` is `False` (default), this must be\\\n            a Tensor of shape `[batch_size, max_time]`.\n\n            - If `time_major` is `True`, this must be a Tensor of shape\\\n            `[max_time, batch_size].`\n        logits: Unscaled log probabilities. This must have the shape of\n            `[max_time, batch_size, num_classes]` or\n            `[batch_size, max_time, num_classes]` according to\n            the value of `time_major`.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will have zero losses.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`labels` and :attr:`logits` must have shape\n            `[max_time, batch_size, ...]`. If `False`\n            (default), they must have shape `[batch_size, max_time, ...]`.\n        name (str, optional): A name for the operation.\n\n    Returns:\n        A Tensor containing the loss, of rank 0, 1, or 2 depending on the\n        arguments :attr:`{average_across}/{sum_over}_{timesteps}/{batch}`.\n        For example:\n\n        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`  \\\n        are `True` (default), the return Tensor is of rank 0.\n\n        - If :attr:`average_across_batch` is `True` and other arguments are \\\n        `False`, the return Tensor is of shape `[max_time]`.\n\n    Example:\n\n        .. code-block:: python\n\n            embedder = WordEmbedder(vocab_size=data.vocab.size)\n            decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n            outputs, _, _ = decoder(\n                decoding_strategy=\'train_greedy\',\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\']-1)\n\n            loss = sequence_sparse_softmax_cross_entropy(\n                labels=data_batch[\'text_ids\'][:, 1:],\n                logits=outputs.logits,\n                sequence_length=data_batch[\'length\']-1)\n\n    """"""\n    with tf.name_scope(name, ""sequence_sparse_softmax_cross_entropy""):\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=labels, logits=logits)\n\n        losses = mask_and_reduce(\n            losses,\n            sequence_length,\n            rank=2,\n            average_across_batch=average_across_batch,\n            average_across_timesteps=average_across_timesteps,\n            sum_over_batch=sum_over_batch,\n            sum_over_timesteps=sum_over_timesteps,\n            time_major=time_major)\n\n        return losses\n\n\ndef sequence_sigmoid_cross_entropy(labels,\n                                   logits,\n                                   sequence_length,\n                                   average_across_batch=True,\n                                   average_across_timesteps=False,\n                                   average_across_classes=True,\n                                   sum_over_batch=False,\n                                   sum_over_timesteps=True,\n                                   sum_over_classes=False,\n                                   time_major=False,\n                                   stop_gradient_to_label=False,\n                                   name=None):\n    """"""Computes sigmoid cross entropy for each time step of sequence\n    predictions.\n\n    Args:\n        labels: Target class distributions.\n\n            - If :attr:`time_major` is `False` (default), this must be a\\\n            Tensor of shape `[batch_size, max_time(, num_classes)]`.\n\n            - If `time_major` is `True`, this must be a Tensor of shape\\\n            `[max_time, batch_size(, num_classes)]`.\n\n            Each row of `labels` should be a valid probability\n            distribution, otherwise, the computation of the gradient will be\n            incorrect.\n        logits: Unscaled log probabilities having the same shape as with\n            :attr:`labels`.\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will have zero losses.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_classes (bool): If set, average the loss across the\n            class dimension (if exists). Must not set\n            `average_across_classes`\' and `sum_over_classes` at\n            the same time. Ignored if :attr:`logits` is a 2D Tensor.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_classes (bool): If set, sum the loss across the\n            class dimension. Must not set `average_across_classes`\n            and `sum_over_classes` at the same time. Ignored if\n            :attr:`logits` is a 2D Tensor.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`labels` and :attr:`logits` must have shape\n            `[max_time, batch_size, ...]`. If `False`\n            (default), they must have shape `[batch_size, max_time, ...]`.\n        stop_gradient_to_label (bool): If set, gradient propagation to\n            :attr:`labels` will be disabled.\n        name (str, optional): A name for the operation.\n\n    Returns:\n        A Tensor containing the loss, of rank 0, 1, or 2 depending on the\n        arguments\n        :attr:`{average_across}/{sum_over}_{timesteps}/{batch}/{classes}`.\n        For example, if the class dimension does not exist, and\n\n        - If :attr:`sum_over_timesteps` and :attr:`average_across_batch`  \\\n        are `True` (default), the return Tensor is of rank 0.\n\n        - If :attr:`average_across_batch` is `True` and other arguments are \\\n        `False`, the return Tensor is of shape `[max_time]`.\n    """"""\n\n    with tf.name_scope(name, ""sequence_sigmoid_cross_entropy""):\n        if stop_gradient_to_label:\n            labels = tf.stop_gradient(labels)\n\n        losses = tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=labels, logits=logits)\n\n        rank = shapes.get_rank(logits) or shapes.get_rank(labels)\n        if rank is None:\n            raise ValueError(\n                \'Cannot determine the rank of `logits` or `labels`.\')\n\n        losses = mask_and_reduce(\n            losses,\n            sequence_length,\n            rank=rank,\n            average_across_batch=average_across_batch,\n            average_across_timesteps=average_across_timesteps,\n            average_across_remaining=average_across_classes,\n            sum_over_batch=sum_over_batch,\n            sum_over_timesteps=sum_over_timesteps,\n            sum_over_remaining=sum_over_classes,\n            time_major=time_major)\n\n        return losses\n\n\ndef binary_sigmoid_cross_entropy(pos_logits=None,\n                                 neg_logits=None,\n                                 average_across_batch=True,\n                                 average_across_classes=True,\n                                 sum_over_batch=False,\n                                 sum_over_classes=False,\n                                 return_pos_neg_losses=False,\n                                 name=None):\n    """"""Computes sigmoid cross entropy of binary predictions.\n\n    Args:\n        pos_logits: The logits of predicting positive on positive data. A\n            tensor of shape `[batch_size(, num_classes)]`.\n        neg_logits: The logits of predicting positive on negative data. A\n            tensor of shape `[batch_size(, num_classes)]`.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_classes (bool): If set, average the loss across the\n            class dimension (if exists). Must not set\n            `average_across_classes`\' and `sum_over_classes` at\n            the same time. Ignored if :attr:`logits` is a 1D Tensor.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_classes (bool): If set, sum the loss across the\n            class dimension. Must not set `average_across_classes`\n            and `sum_over_classes` at the same time. Ignored if\n            :attr:`logits` is a 2D Tensor.\n        return_pos_neg_losses (bool): If set, additionally returns the losses\n            on :attr:`pos_logits` and :attr:`neg_logits`, respectively.\n        name (str, optional): A name for the operation.\n\n    Returns:\n        By default, a Tensor containing the loss, of rank 0, 1, or 2 depending\n        on the arguments :attr:`{average_across}/{sum_over}_{batch}/{classes}`.\n        For example:\n\n            - If :attr:`sum_over_batch` and :attr:`average_across_classes`  \\\n            are `True` (default), the return Tensor is of rank 0.\n\n            - If  arguments are `False`, the return Tensor is of shape \\\n            `[batch_size(, num_classes)]`.\n\n        If :attr:`return_pos_neg_losses` is `True`, returns a tuple\n        `(loss, pos_loss, neg_loss)`, where `loss` is the loss above;\n        `pos_loss` is the loss on `pos_logits` only; and\n        `neg_loss` is the loss on `neg_logits` only. They have\n        `loss = pos_loss + neg_loss`.\n    """"""\n    with tf.name_scope(name, ""binary_sigmoid_cross_entropy""):\n        average_axes, sum_axes = [], []\n        average_axes += [0] if average_across_batch else []\n        average_axes += [1] if average_across_classes else []\n        sum_axes += [0] if sum_over_batch else []\n        sum_axes += [1] if sum_over_classes else []\n\n        pos_loss = 0\n        if pos_logits is not None:\n            pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n                logits=pos_logits, labels=tf.ones_like(pos_logits))\n\n            pos_loss = reduce_dimensions(pos_loss, average_axes, sum_axes)\n\n        neg_loss = 0\n        if neg_logits is not None:\n            neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n                logits=neg_logits, labels=tf.zeros_like(neg_logits))\n\n            neg_loss = reduce_dimensions(neg_loss, average_axes, sum_axes)\n\n    loss = pos_loss + neg_loss\n\n    if return_pos_neg_losses:\n        return loss, pos_loss, neg_loss\n    else:\n        return loss\n\n\ndef binary_sigmoid_cross_entropy_with_clas(clas_fn,\n                                           pos_inputs=None,\n                                           neg_inputs=None,\n                                           average_across_batch=True,\n                                           average_across_classes=True,\n                                           sum_over_batch=False,\n                                           sum_over_classes=False,\n                                           return_pos_neg_losses=False,\n                                           name=None):\n    """"""Computes sigmoid cross entropy of binary classifier.\n\n    .. role:: python(code)\n       :language: python\n\n    Args:\n        clas_fn: A callable takes data (e.g., :attr:`pos_inputs` and\n            :attr:`fake_inputs`) and returns the logits of being positive. The\n            signature of `clas_fn` must be:\n            :python:`logits (, ...) = clas_fn(inputs)`.\n            The return value of `clas_fn` can be the logits, or\n            a tuple where the logits are the first element.\n        pos_inputs: The positive data fed into `clas_fn`.\n        neg_inputs: The negative data fed into `clas_fn`.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_classes (bool): If set, average the loss across the\n            class dimension (if exists). Must not set\n            `average_across_classes`\' and `sum_over_classes` at\n            the same time. Ignored if :attr:`logits` is a 1D Tensor.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_classes (bool): If set, sum the loss across the\n            class dimension. Must not set `average_across_classes`\n            and `sum_over_classes` at the same time. Ignored if\n            :attr:`logits` is a 2D Tensor.\n        return_pos_neg_losses (bool): If set, additionally returns the losses\n            on :attr:`pos_logits` and :attr:`neg_logits`, respectively.\n        name (str, optional): A name for the operation.\n\n    Returns:\n        By default, a Tensor containing the loss, of rank 0, 1, or 2 depending\n        on the arguments :attr:`{average_across}/{sum_over}_{batch}/{classes}`.\n        For example:\n\n            - If :attr:`sum_over_batch` and :attr:`average_across_classes`  \\\n            are `True` (default), the return Tensor is of rank 0.\n\n            - If  arguments are `False`, the return Tensor is of shape \\\n            `[batch_size(, num_classes)]`.\n\n        If :attr:`return_pos_neg_losses`=`True`, returns a tuple\n        `(loss, pos_loss, neg_loss)`, where `loss` is the loss above;\n        `pos_loss` is the loss on `pos_logits` only; and\n        `neg_loss` is the loss on `neg_logits` only. They have\n        `loss = pos_loss + neg_loss`.\n    """"""\n    pos_logits = None\n    if pos_inputs is not None:\n        pos_logits = clas_fn(pos_inputs)\n        if isinstance(pos_logits, (list, tuple)):\n            pos_logits = pos_logits[0]\n\n    neg_logits = None\n    if neg_inputs is not None:\n        neg_logits = clas_fn(neg_inputs)\n        if isinstance(neg_logits, (list, tuple)):\n            neg_logits = neg_logits[0]\n\n    return binary_sigmoid_cross_entropy(\n        pos_logits=pos_logits,\n        neg_logits=neg_logits,\n        average_across_batch=average_across_batch,\n        average_across_classes=average_across_classes,\n        sum_over_batch=sum_over_batch,\n        sum_over_classes=sum_over_classes,\n        return_pos_neg_losses=return_pos_neg_losses,\n        name=name)\n'"
texar/tf/losses/pg_losses.py,9,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious loss functions for policy gradients.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.losses.losses_utils import mask_and_reduce\nfrom texar.tf.utils.shapes import get_rank\n\n# pylint: disable=too-many-arguments, protected-access\n\n__all__ = [\n    ""pg_loss_with_logits"",\n    ""pg_loss_with_log_probs""\n]\n\n\ndef pg_loss_with_logits(actions,\n                        logits,\n                        advantages,\n                        rank=None,\n                        batched=False,\n                        sequence_length=None,\n                        average_across_batch=True,\n                        average_across_timesteps=False,\n                        average_across_remaining=False,\n                        sum_over_batch=False,\n                        sum_over_timesteps=True,\n                        sum_over_remaining=True,\n                        time_major=False):\n    """"""Policy gradient loss with logits. Used for discrete actions.\n\n    `pg_loss = reduce( advantages * -log_prob( actions )  )`,\n    where `advantages` and `actions` do not back-propagate gradients.\n\n    All arguments except :attr:`logits` and :attr:`actions` are the same with\n    :func:`pg_loss_with_log_probs`.\n\n    Args:\n        actions: Tensor of shape\n            `[(batch_size,) max_time, d_3, ..., d_rank]` and of dtype\n            `int32` or `int64`.\n            The rank of the Tensor is specified with :attr:`rank`.\n\n            The batch dimension exists only if :attr:`batched` is `True`.\n\n            The batch and time dimensions\n            are exchanged, i.e., `[max_time, batch_size, ...]` if\n            :attr:`time_major` is `True`.\n        logits: Unscaled log probabilities of shape\n            `[(batch_size,) max_time, d_3, ..., d_{rank+1}]`\n            and dtype `float32` or `float64`.\n            The batch and time dimensions are exchanged if `time_major`\n            is `True`.\n        advantages: Tensor of shape\n            `[(batch_size,) max_time, d_3, ..., d_rank]` and\n            dtype `float32` or `float64`.\n            The batch and time dimensions are exchanged if `time_major`\n            is `True`.\n        rank (int, optional): The rank of :attr:`actions`.\n            If `None` (default), rank is automatically inferred from\n            `actions` or `advantages`. If the inference fails,\n            `rank` is set to 1 if :attr:`batched` is `False`,\n            and set to 2 if :attr:`batched` is `True`.\n        batched (bool): `True` if the inputs are batched.\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will have zero\n            losses. Used if :attr:`batched` is `True`.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        average_across_remaining (bool): If set, average the sequence across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        sum_over_remaining (bool): If set, sum the loss across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`logits`, :attr:`actions` and :attr:`advantages` must\n            have shape `[max_time, batch_size, ...]`. If `False` (default),\n            they must have shape `[batch_size, max_time, ...]`.\n            Ignored if `batched` is `False`.\n\n    Returns:\n        A Tensor containing the loss to minimize, whose rank depends on the\n        reduce arguments. For example, the batch dimension is reduced if\n        either :attr:`average_across_batch` or :attr:`sum_over_batch` is\n        `True`, which decreases the rank of output tensor by 1.\n    """"""\n    actions = tf.stop_gradient(actions)\n    neg_log_probs = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=actions)\n    return pg_loss_with_log_probs(\n        log_probs=-neg_log_probs,\n        advantages=advantages,\n        rank=rank,\n        batched=batched,\n        sequence_length=sequence_length,\n        average_across_batch=average_across_batch,\n        average_across_timesteps=average_across_timesteps,\n        average_across_remaining=average_across_remaining,\n        sum_over_batch=sum_over_batch,\n        sum_over_timesteps=sum_over_timesteps,\n        sum_over_remaining=sum_over_remaining,\n        time_major=time_major)\n\n\ndef pg_loss_with_log_probs(log_probs,\n                           advantages,\n                           rank=None,\n                           batched=False,\n                           sequence_length=None,\n                           average_across_batch=True,\n                           average_across_timesteps=False,\n                           average_across_remaining=False,\n                           sum_over_batch=False,\n                           sum_over_timesteps=True,\n                           sum_over_remaining=True,\n                           time_major=False):\n    """"""Policy gradient loss with log probs of actions.\n\n    `pg_loss = reduce( advantages * -log_probs )`,\n    where `advantages` does not back-propagate gradients.\n\n    All arguments except :attr:`log_probs` are the same as\n    :func:`pg_loss_with_logits`.\n\n    Args:\n        log_probs: Log probabilities of shape\n            `[(batch_size,) max_time, ..., d_rank]` and dtype `float32`\n            or `float64`. The rank of the Tensor is specified\n            with :attr:`rank`.\n\n            The batch dimension exists only if :attr:`batched` is `True`.\n\n            The batch and time dimensions are exchanged, i.e.,\n            `[max_time, batch_size, ...]` if :attr:`time_major` is `True`.\n        advantages: Tensor of shape\n            `[(batch_size,) max_time, d_3, ..., d_rank]` and\n            dtype `float32` or `float64`.\n            The batch dimension exists only if `batched` is `True`.\n            The batch and time dimensions\n            are exchanged if `time_major` is `True`.\n        rank (int, optional): The rank of :attr:`log_probs`.\n            If `None` (default), rank is automatically inferred from\n            `log_probs` or `advantages`. If the inference fails,\n            `rank` is set to 1 if `batched``==False`,\n            and set to 2 if `batched``==True`.\n        batched (bool): `True` if the inputs are batched.\n        sequence_length (optional): A Tensor of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will have zero\n            losses. Used if :attr:`batched` is `True`.\n        average_across_timesteps (bool): If set, average the loss across\n            the time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        average_across_batch (bool): If set, average the loss across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        average_across_remaining (bool): If set, average the sequence across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        sum_over_timesteps (bool): If set, sum the loss across the\n            time dimension. Must not set `average_across_timesteps`\n            and `sum_over_timesteps` at the same time.\n        sum_over_batch (bool): If set, sum the loss across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n            Ignored if `batched` is `False`.\n        sum_over_remaining (bool): If set, sum the loss across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time. Ignored if\n            no more dimensions other than the batch and time dimensions.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`log_probs` and :attr:`advantages` must have shape\n            `[max_time, batch_size, ...]`. If `False` (default),\n            they must have shape `[batch_size, max_time, ...]`.\n            Ignored if :attr:`batched` is `False`.\n\n    Returns:\n        A Tensor containing the loss to minimize, whose rank depends on the\n        reduce arguments. For example, the batch dimension is reduced if\n        either :attr:`average_across_batch` or :attr:`sum_over_batch` is\n        `True`, which decreases the rank of output tensor by 1.\n    """"""\n    advantages = tf.stop_gradient(advantages)\n\n    losses = -log_probs * advantages\n\n    if rank is None:\n        rank = get_rank(log_probs) or get_rank(advantages)\n    if rank is None:\n        rank = 2 if batched else 1\n\n    if batched:\n        losses = mask_and_reduce(\n            losses,\n            sequence_length,\n            rank=rank,\n            average_across_batch=average_across_batch,\n            average_across_timesteps=average_across_timesteps,\n            average_across_remaining=average_across_remaining,\n            sum_over_batch=sum_over_batch,\n            sum_over_timesteps=sum_over_timesteps,\n            sum_over_remaining=sum_over_remaining,\n            time_major=time_major)\n    elif rank > 1:\n        if average_across_remaining and sum_over_remaining:\n            raise ValueError(""Only one of `average_across_remaining` and ""\n                             ""`sum_over_remaining` can be set."")\n        if average_across_remaining:\n            losses = tf.reduce_mean(losses, axis=list(range(1, rank)))\n        elif sum_over_remaining:\n            losses = tf.reduce_sum(losses, axis=list(range(1, rank)))\n\n    if not batched:\n        if average_across_timesteps and sum_over_timesteps:\n            raise ValueError(""Only one of `average_across_timesteps` and ""\n                             ""`sum_over_timesteps` can be set."")\n        if average_across_timesteps:\n            losses = tf.reduce_mean(losses, axis=0)\n        elif sum_over_timesteps:\n            losses = tf.reduce_sum(losses, axis=0)\n\n    return losses\n'"
texar/tf/losses/rewards.py,18,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious reward related functions.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.shapes import mask_sequences\n\n# pylint: disable=invalid-name, too-many-arguments, no-member\n\n__all__ = [\n    ""discount_reward"",\n    ""_discount_reward_py_1d"",\n    ""_discount_reward_tensor_1d"",\n    ""_discount_reward_py_2d"",\n    ""_discount_reward_tensor_2d""\n]\n\n\ndef discount_reward(reward,\n                    sequence_length=None,\n                    discount=1.,\n                    normalize=False,\n                    dtype=None,\n                    tensor_rank=1):\n    """"""Computes discounted reward.\n\n    :attr:`reward` and :attr:`sequence_length` can be either Tensors or python\n    arrays. If both are python array (or `None`), the return will be a python\n    array as well. Otherwise tf Tensors are returned.\n\n    Args:\n        reward: A Tensor or python array. Can be 1D with shape `[batch_size]`,\n            or 2D with shape `[batch_size, max_time]`.\n        sequence_length (optional): A Tensor or python array of shape\n            `[batch_size]`. Time steps beyond the respective sequence lengths\n            will be masked. Required if :attr:`reward` is 1D.\n        discount (float): A scalar. The discount factor.\n        normalize (bool): Whether to normalize the discounted reward, by\n            `(discounted_reward - mean) / std`. Here `mean` and `std` are\n            over all time steps and all samples in the batch.\n        dtype (dtype): Type of :attr:`reward`. If `None`, infer from\n            `reward` automatically.\n        tensor_rank (int): The number of dimensions of :attr:`reward`.\n            Default is 1, i.e., :attr:`reward` is a 1D Tensor consisting\n            of a batch dimension. Ignored if :attr:`reward`\n            and :attr:`sequence_length` are python arrays (or `None`).\n\n    Returns:\n        A 2D Tensor or python array of the discounted reward.\n\n        If :attr:`reward` and :attr:`sequence_length` are python\n        arrays (or `None`), the returned value is a python array as well.\n\n\n    Example:\n\n        .. code-block:: python\n\n            r = [2., 1.]\n            seq_length = [3, 2]\n            discounted_r = discount_reward(r, seq_length, discount=0.1)\n            # discounted_r == [[2. * 0.1^2, 2. * 0.1, 2.],\n            #                  [1. * 0.1,   1.,       0.]]\n\n            r = [[3., 4., 5.], [6., 7., 0.]]\n            seq_length = [3, 2]\n            discounted_r = discount_reward(r, seq_length, discount=0.1)\n            # discounted_r == [[3. + 4.*0.1 + 5.*0.1^2, 4. + 5.*0.1, 5.],\n            #                  [6. + 7.*0.1,            7.,          0.]]\n    """"""\n    is_tensor = tf.contrib.framework.is_tensor\n    if is_tensor(reward) or is_tensor(sequence_length):\n        if tensor_rank == 1:\n            disc_reward = _discount_reward_tensor_1d(\n                reward, sequence_length, discount, dtype)\n        elif tensor_rank == 2:\n            disc_reward = _discount_reward_tensor_2d(\n                reward, sequence_length, discount, dtype)\n        else:\n            raise ValueError(""`tensor_rank` can only be 1 or 2."")\n\n        if normalize:\n            mu, var = tf.nn.moments(disc_reward, axes=[0, 1], keep_dims=True)\n            disc_reward = (disc_reward - mu) / (tf.sqrt(var) + 1e-8)\n    else:\n        reward = np.array(reward)\n        tensor_rank = reward.ndim\n        if tensor_rank == 1:\n            disc_reward = _discount_reward_py_1d(\n                reward, sequence_length, discount, dtype)\n        elif tensor_rank == 2:\n            disc_reward = _discount_reward_py_2d(\n                reward, sequence_length, discount, dtype)\n        else:\n            raise ValueError(""`reward` can only be 1D or 2D."")\n\n        if normalize:\n            mu = np.mean(disc_reward)\n            std = np.std(disc_reward)\n            disc_reward = (disc_reward - mu) / (std + 1e-8)\n\n    return disc_reward\n\n\ndef _discount_reward_py_1d(reward, sequence_length, discount=1., dtype=None):\n    if sequence_length is None:\n        raise ValueError(\'sequence_length must not be `None` for 1D reward.\')\n\n    reward = np.array(reward)\n    sequence_length = np.array(sequence_length)\n\n    batch_size = reward.shape[0]\n    max_seq_length = np.max(sequence_length)\n    dtype = dtype or reward.dtype\n\n    if discount == 1.:\n        dmat = np.ones([batch_size, max_seq_length], dtype=dtype)\n    else:\n        steps = np.tile(np.arange(max_seq_length), [batch_size, 1])\n        mask = np.asarray(steps < (sequence_length - 1)[:, None], dtype=dtype)\n        # Make each row = [discount, ..., discount, 1, ..., 1]\n        dmat = mask * discount + (1 - mask)\n        dmat = np.cumprod(dmat[:, ::-1], axis=1)[:, ::-1]\n\n    disc_reward = dmat * reward[:, None]\n    disc_reward = mask_sequences(disc_reward, sequence_length, dtype=dtype)\n    # mask = np.asarray(steps < sequence_length[:, None], dtype=dtype)\n    # disc_reward = mask * disc_reward\n\n    return disc_reward\n\n\ndef _discount_reward_tensor_1d(reward, sequence_length,\n                               discount=1., dtype=None):\n    if sequence_length is None:\n        raise ValueError(\'sequence_length must not be `None` for 1D reward.\')\n\n    batch_size = tf.shape(reward)[0]\n    max_seq_length = tf.reduce_max(sequence_length)\n    dtype = dtype or reward.dtype\n\n    if discount == 1.:\n        dmat = tf.ones(\n            tf.concat([[batch_size], [max_seq_length]], 0), dtype=dtype)\n    else:\n        mask = tf.sequence_mask(sequence_length, dtype=dtype)\n        mask = tf.concat([mask[:, 1:], tf.zeros_like(mask[:, -1:])], axis=1)\n        # Make each row = [discount, ..., discount, 1, ..., 1]\n        dmat = mask * discount + (1 - mask)\n        dmat = tf.cumprod(dmat, axis=1, reverse=True)\n\n    disc_reward = dmat * tf.expand_dims(reward, -1)\n    disc_reward = mask_sequences(\n        disc_reward, sequence_length, dtype=dtype, tensor_rank=2)\n\n    return disc_reward\n\n\ndef _discount_reward_py_2d(reward, sequence_length=None,\n                           discount=1., dtype=None):\n    if sequence_length is not None:\n        reward = mask_sequences(reward, sequence_length, dtype=dtype)\n\n    dtype = dtype or reward.dtype\n\n    if discount == 1.:\n        disc_reward = np.cumsum(\n            reward[:, ::-1], axis=1, dtype=dtype)[:, ::-1]\n    else:\n        disc_reward = np.copy(reward)\n        for i in range(reward.shape[1] - 2, -1, -1):\n            disc_reward[:, i] += disc_reward[:, i + 1] * discount\n\n    return disc_reward\n\n\ndef _discount_reward_tensor_2d(reward, sequence_length=None,\n                               discount=1., dtype=None):\n    if sequence_length is not None:\n        reward = mask_sequences(\n            reward, sequence_length, dtype=dtype, tensor_rank=2)\n\n    if discount == 1.:\n        disc_reward = tf.cumsum(reward, axis=1, reverse=True)\n    else:\n        # [max_time, batch_size]\n        rev_reward_T = tf.transpose(tf.reverse(reward, [1]), [1, 0])\n        rev_reward_T_cum = tf.scan(\n            fn=lambda acc, cur: cur + discount * acc,\n            elems=rev_reward_T,\n            initializer=tf.zeros_like(reward[:, 1]),\n            back_prop=False)\n        disc_reward = tf.reverse(\n            tf.transpose(rev_reward_T_cum, [1, 0]), [1])\n\n    return disc_reward\n'"
texar/tf/losses/rl_losses.py,7,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RL losses\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.losses.mle_losses import _mask_sequences\n\n\ndef reinforce_loss(sample_fn,\n                   global_reward_fn,\n                   local_reward_fn=None,\n                   num_samples=1):\n    """"""Computes REINFORCE loss with global and local rewards.\n\n    Args:\n        sample_fn: A callable that takes :attr:`num_samples` and returns\n            `(samples, probabilities, sequence_lengths)`, where:\n\n            `samples` is a Tensor of shape `[num_samples, max_sequence_length]`\n            containing the generated samples;\n\n            `probabilities` is a Tensor of shape\n            `[num_samples, max_sequence_length]` containing the probabilities of\n            generating each position of the samples. Probabilities beyond the\n            respective sequence lengths are ignored.\n\n            `sequence_lengths` is a Tensor of shape `[num_samples]` containing\n            the length of each samples.\n        global_reward_fn: A callable that takes `(samples, sequence_lengths)`\n            and returns a Tensor of shape `[num_samples]` containing the reward\n            of each of the samples.\n        local_reward_fn (optional): A callable that takes\n            `(samples, sequence_lengths)` and returns a Tensor of shape\n            `[num_samples, max_sequence_length]` containing the local reward\n            at each time step of samples.\n        num_samples (int scalar Tensor): the number of sequences to sample.\n\n    Returns:\n        A scalar Tensor of the REINFORCE loss.\n    """"""\n\n    # shape = [batch, length]\n    sequences, probs, seq_lens = sample_fn(num_samples)\n    batch, _ = tf.shape(sequences)\n    rewards_local = tf.constant(0., dtype=probs.dtype, shape=probs.shape)\n    if local_reward_fn is not None:\n        rewards_local = local_reward_fn(sequences, seq_lens)\n\n    # shape = [batch, ]\n    rewards_global = global_reward_fn(sequences, seq_lens)\n    # add broadcast to rewards_global to match the shape of rewards_local\n    rewards = rewards_local + tf.reshape(rewards_global, [batch, 1])\n\n    eps = 1e-12\n    log_probs = _mask_sequences(tf.log(probs + eps), seq_lens)\n    loss = - tf.reduce_mean(\n        tf.reduce_sum(log_probs * rewards, axis=1) / seq_lens)\n    return loss\n\n\ndef reinforce_loss_with_MCtree(sample_fn,   # pylint: disable=invalid-name\n                               global_reward_fn,\n                               local_reward_fn=None,\n                               num_samples=1):\n    """"""Computes REINFORCE loss with Monte Carlo tree search.\n\n    Args:\n        sample_fn: A callable that takes :attr:`num_samples`, \'given_actions\'\n            and returns `(samples, probabilities, sequence_lengths)`, where:\n\n            `samples` is a Tensor of shape `[num_samples, max_sequence_length]`\n            containing the generated samples;\n\n            `probabilities` is a Tensor of shape\n            `[num_samples, max_sequence_length]` containing the probabilities of\n            generating each position of the samples. Probabilities beyond the\n            respective sequence lengths are ignored.\n\n            `sequence_lengths` is a Tensor of shape `[num_samples]` containing\n            the length of each samples.\n        global_reward_fn: A callable that takes `(samples, sequence_lengths)`\n            and returns a Tensor of shape `[num_samples]` containing the reward\n            of each of the samples.\n        local_reward_fn (optional): A callable that takes\n            `(samples, sequence_lengths)` and returns a Tensor of shape\n            `[num_samples, max_sequence_length]` containing the local reward\n            at each time step of samples.\n        num_samples (int scalar Tensor): the number of sequences to sample.\n\n    Returns:\n        A scalar Tensor of the REINFORCE loss.\n    """"""\n    raise NotImplementedError\n'"
texar/tf/models/__init__.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library models.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.models.model_base import *\nfrom texar.tf.models.seq2seq import *\n'"
texar/tf/models/model_base.py,5,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for models.\n""""""\n\nfrom texar.tf.hyperparams import HParams\n\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""ModelBase""\n]\n\n\nclass ModelBase(object):\n    """"""Base class inherited by all model classes.\n\n    A model class implements interfaces that are compatible with\n    :tf_main:`TF Estimator <estimator/Estimator>`. In particular,\n    :meth:`_build` implements the\n    :tf_main:`model_fn <estimator/Estimator#__init__>` interface; and\n    :meth:`get_input_fn` is for the :attr:`input_fn` interface.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, hparams=None):\n        self._hparams = HParams(hparams, self.default_hparams(),\n                                allow_new_hparam=True)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n        """"""\n        hparams = {\n            ""name"": ""model""\n        }\n        return hparams\n\n    def __call__(self, features, labels, params, mode, config=None):\n        """"""Used for the :tf_main:`model_fn <estimator/Estimator#__init__>`\n        argument when constructing\n        :tf_main:`tf.estimator.Estimator <estimator/Estimator>`.\n        """"""\n        return self._build(features, labels, params, mode, config=config)\n\n    def _build(self, features, labels, params, mode, config=None):\n        """"""Used for the :tf_main:`model_fn <estimator/Estimator#__init__>`\n        argument when constructing\n        :tf_main:`tf.estimator.Estimator <estimator/Estimator>`.\n        """"""\n        raise NotImplementedError\n\n    def get_input_fn(self, *args, **kwargs):\n        """"""Returns the :attr:`input_fn` function that constructs the input\n        data, used in :tf_main:`tf.estimator.Estimator <estimator/Estimator>`.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def hparams(self):\n        """"""A :class:`~texar.tf.HParams` instance. The hyperparameters\n        of the module.\n        """"""\n        return self._hparams\n'"
texar/tf/modules/__init__.py,11,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library module.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.classifiers import *\nfrom texar.tf.modules.connectors import *\nfrom texar.tf.modules.decoders import *\nfrom texar.tf.modules.embedders import *\nfrom texar.tf.modules.encoders import *\nfrom texar.tf.modules.memory import *\nfrom texar.tf.modules.networks import *\nfrom texar.tf.modules.policies import *\nfrom texar.tf.modules.pretrained import *\nfrom texar.tf.modules.qnets import *\nfrom texar.tf.modules.regressors import *\n'"
texar/tf/run/__init__.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library run.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.run.executor import *\n'"
texar/tf/run/executor.py,17,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nA class that executes training, evaluation, prediction, export of estimators.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.dtypes import maybe_hparams_to_dict\n\n# pylint: disable=too-many-instance-attributes, too-many-arguments\n\n__all__ = [\n    ""Executor""\n]\n\n\nclass Executor(object):\n    """"""Class that executes training, evaluation, prediction, export, and other\n    actions of :tf_main:`Estimator <estimator/Estimator>`.\n\n    Args:\n        model: An instance of a subclass of\n            :class:`~texar.tf.models.model_base.ModelBase`.\n        data_hparams: A `dict` or an instance of :class:`~texar.tf.hparams.HParams`\n            containing the hyperparameters of data. It must contain `train`\n            and/or `eval` fields for relevant processes. For example, for\n            :meth:`train_and_evaluate`, both fields are required.\n        config: An instance of\n            :tf_main:`tf.estimator.RunConfig <estimator/RunConfig>`, used as\n            the :attr:`config` argument of\n            :tf_main:`Estimator <estimator/Estimator#__init__>`.\n        model_hparams (optional): A `dict` or an instance of\n            :class:`~texar.tf.hparams.HParams` containing the hyperparameters of\n            the model. If `None`, uses :attr:`model.hparams`. Used as\n            the :attr:`params` argument of\n            :tf_main:`Estimator <estimator/Estimator#__init__>`.\n        train_hooks (optional): Iterable of :tf_main:`tf.train.SessionRunHook\n            <train/SessionRunHook>` objects to run during training.\n        eval_hooks (optional): Iterable of :tf_main:`tf.train.SessionRunHook\n            <train/SessionRunHook>` objects to run during evaluation.\n        session_config (optional): An instance of\n            :tf_main:`tf.ConfigProto <ConfigProto>`, used as the :attr:`config`\n            argument of :tf_main:`tf session <Session>`.\n\n    Example:\n\n        .. code-block:: python\n\n            model = BasicSeq2seq(data_hparams, model_hparams)\n            exor = Executor(\n                model=model,\n                data_hparams=data_hparams,\n                config=run_config)\n            exor.train_and_evaluate(\n                max_train_steps=10000,\n                eval_steps=100)\n\n    See `bin/train.py` for the usage in detail.\n    """"""\n\n    def __init__(self,\n                 model,\n                 data_hparams,\n                 config,\n                 model_hparams=None,\n                 train_hooks=None,\n                 eval_hooks=None,\n                 session_config=None):\n        self._model = model\n        self._data_hparams = maybe_hparams_to_dict(data_hparams)\n        self._config = config\n        self._train_hooks = train_hooks\n        self._eval_hooks = eval_hooks\n        self._session_config = session_config\n\n        if model_hparams is None:\n            model_hparams = model.hparams\n        self._model_hparams = maybe_hparams_to_dict(model_hparams)\n\n        self._estimator = tf.estimator.Estimator(\n            model_fn=self._model, config=config, params=self._model_hparams)\n\n    def _get_train_spec(self, max_steps=None):\n        if \'train\' not in self._data_hparams:\n            raise ValueError(\'`data_hparams` must contain field `train` for \'\n                             \'training data config.\')\n        input_fn = self._model.get_input_fn(\n            mode=tf.estimator.ModeKeys.TRAIN,\n            hparams=self._data_hparams[\'train\'])\n        return tf.estimator.TrainSpec(\n            input_fn=input_fn,\n            max_steps=max_steps,\n            hooks=self._train_hooks)\n\n    def _get_eval_spec(self, steps):\n        if \'eval\' not in self._data_hparams:\n            raise ValueError(\'`data_hparams` must contain field `eval` for \'\n                             \'evaluation data config.\')\n        input_fn = self._model.get_input_fn(\n            mode=tf.estimator.ModeKeys.EVAL,\n            hparams=self._data_hparams[\'eval\'])\n        return tf.estimator.EvalSpec(\n            input_fn=input_fn,\n            steps=steps,\n            hooks=self._eval_hooks)\n\n    def train(self, max_steps=None):\n        """"""Trains the model. See :tf_main:`tf.estimator.Estimator.train\n        <estimator/Estimator#train>` for more details.\n\n        Args:\n            max_steps (int, optional): Total number of steps for which\n                to train model. If `None`, train forever or until the train\n                data generates the OutOfRange exception. If OutOfRange occurs\n                in the middle, training stops before :attr:`max_steps` steps.\n        """"""\n        train_spec = self._get_train_spec(max_steps=max_steps)\n        self._estimator.train(\n            input_fn=train_spec.input_fn,\n            hooks=train_spec.hooks,\n            max_steps=train_spec.max_steps)\n\n    def evaluate(self, steps=None, checkpoint_path=None):\n        """"""Evaluates the model. See :tf_main:`tf.estimator.Estimator.evaluate\n        <estimator/Estimator#evaluate>` for more details.\n\n        Args:\n            steps (int, optional): Number of steps for which to evaluate\n                model. If `None`, evaluates until the eval data raises an\n                OutOfRange exception.\n            checkpoint_path (str, optional): Path of a specific checkpoint to\n                evaluate. If `None`, the the latest checkpoint in\n                :attr:`config.model_dir` is used. If there are no checkpoints\n                in :attr:`model_dir`, evaluation is run with newly initialized\n                variables instead of restored from checkpoint.\n        """"""\n        eval_spec = self._get_eval_spec(steps=steps)\n        self._estimator.evaluate(\n            input_fn=eval_spec.input_fn,\n            steps=eval_spec.steps,\n            hooks=eval_spec.hooks,\n            checkpoint_path=checkpoint_path)\n\n    def train_and_evaluate(self, max_train_steps=None, eval_steps=None):\n        """"""Trains and evaluates the model. See\n        :tf_main:`tf.estimator.train_and_evaluate\n        <estimator/train_and_evaluate>` for more details.\n\n        Args:\n            max_train_steps (int, optional): Total number of steps for which\n                to train model. If `None`, train forever or until the train\n                data generates the OutOfRange exception. If OutOfRange occurs\n                in the middle, training stops before :attr:`max_steps` steps.\n            eval_steps (int, optional): Number of steps for which to evaluate\n                model. If `None`, evaluates until the eval data raises an\n                OutOfRange exception.\n        """"""\n        train_spec = self._get_train_spec(max_steps=max_train_steps)\n        eval_spec = self._get_eval_spec(steps=eval_steps)\n        tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)\n'"
texar/tf/utils/__init__.py,8,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library utils.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.utils.utils import *\nfrom texar.tf.utils.exceptions import *\nfrom texar.tf.utils.shapes import *\nfrom texar.tf.utils.dtypes import *\nfrom texar.tf.utils.variables import *\nfrom texar.tf.utils.mode import *\nfrom texar.tf.utils.average_recorder import *\nfrom texar.tf.utils.utils_io import *\n'"
texar/tf/utils/average_recorder.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtilities for maintaining moving average.\n""""""\n\nfrom collections import deque\n\n# pylint: disable=invalid-name\n\n__all__ = [\n    ""_SingleAverageRecorder"",\n    ""AverageRecorder""\n]\n\n\nclass _SingleAverageRecorder(object):\n    """"""Maintains the moving average (i.e., the average of the latest N records)\n    of a single metric.\n\n    Args:\n        size (int, optional): The window size of moving average. If `None`,\n            the average of all added records is maintained.\n        name (str, optional): name of the recorder. Used when printing.\n    """"""\n\n    def __init__(self, size=None, name=None):\n        if size is not None and size <= 0:\n            raise ValueError(""`size` must be > 0 or `None`."")\n        self._size = size\n        self._q = deque([])\n        self._w = deque([])\n        self._sum = 0.\n        self._w_sum = 0\n        self._name = name\n\n    def add(self, record, weight=None):\n        """"""Appends a new record.\n\n        Args:\n            record: A scalar; the new record to append.\n            weight (optional): A scalar, weight of the new record for\n                calculating a weighted average. If `None`, weight is set to `1`.\n                For example, :attr:`weight` can be set to batch size and\n                :attr:`record` the average value of certain metric on the batch\n                in order to calculate the average metric value on a whole\n                dataset.\n\n        Returns:\n            The (moving) average after appending the record.\n        """"""\n        w = weight if weight is not None else 1\n        self._w_sum += w\n        self._sum += record * w\n\n        if self._size is not None:\n            if len(self._q) == self._size:\n                w_pop = self._w.popleft()\n                self._sum -= self._q.popleft() * w_pop\n                self._w_sum -= w_pop\n            self._q.append(record)\n            self._w.append(w)\n\n        return self.avg()\n\n    def avg(self):\n        """"""Returns the (moving) average.\n        """"""\n        if self._w_sum == 0:\n            return 0.\n        return self._sum / self._w_sum\n\n    def reset(self):\n        """"""Cleans all records.\n        """"""\n        self._q.clear()\n        self._w.clear()\n        self._sum = 0.\n        self._w_sum = 0\n\n    def to_str(self, precision=None):\n        """"""Returns a string of the average value.\n\n        Args:\n            precision (int, optional): The number of decimal places to keep in\n                the returned string. E.g., for an average value of `0.1234`,\n                :attr:`precision = 2` leads to `\'0.12\'`.\n\n        Returns:\n            A string of the average value. If :meth:`name` is given, the\n            string is of the format like `\'name: 0.1234\'`, otherwise\n            the string is of the format like `\'0.1234\'`.\n        """"""\n        prec_str = ""{}""\n        if precision is not None:\n            prec_str = ""{:.%df}"" % precision\n\n        avg_str = prec_str.format(self.avg())\n        if self._name is not None:\n            avg_str = ""{}: {}"".format(self._name, avg_str)\n\n        return avg_str\n\n    @property\n    def name(self):\n        """"""The name of the recorder.\n        """"""\n        return self.name\n\n\nclass AverageRecorder(object):\n    """"""Maintains the moving averages (i.e., the average of the latest N\n    records) of (possibly multiple) fields.\n\n    Fields are determined by the first call of :meth:`add`.\n\n    Args:\n        size (int, optional): The window size of moving average. If `None`,\n            the average of all added records is maintained.\n\n    Example:\n\n        .. code-block:: python\n\n            ## Use to maintain moving average of training loss\n            avg_rec = AverageRecorder(size=10) # average over latest 10 records\n            while training:\n                loss_0, loss_1  = ...\n                avg_rec.add([loss_0, loss_1])\n                # avg_rec.avg() == [0.12343452, 0.567800323]\n                # avg_rec.avg(0) == 0.12343452\n                # avg_rec.to_str(precision=2, ) == \'0.12 0.57\'\n\n            ## Use to maintain average of test metrics on the whole test set\n            avg_rec = AverageRecorder() # average over ALL records\n            while test:\n                metric_0, metric_1  = ...\n                avg_rec.add({\'m0\': metric_0, \'m1\': metric_1}) # dict is allowed\n            print(avg_rec.to_str(precision=4, delimiter=\' , \'))\n            # \'m0: 0.1234 , m1: 0.5678\'\n            #\n            # avg_rec.avg() == {\'m0\': 0.12343452, \'m1\': 0.567800323}\n            # avg_rec.avg(0) == 0.12343452\n\n    """"""\n\n    def __init__(self, size=None):\n        if size is not None and size <= 0:\n            raise ValueError(""`size` must be > 0 or `None`."")\n        self._size = size\n        self._recorders = None\n        self._default_metric_name = ""metric""\n        self._record_type = None\n\n    def _to_dict(self, record):\n        if isinstance(record, dict):\n            record_dict = record\n        elif isinstance(record, (list, tuple)):\n            record_dict = {i: vi for i, vi in enumerate(record)}\n        else:\n            record_dict = {self._default_metric_name: record}\n        return record_dict\n\n    def add(self, record, weight=None):\n        """"""Appends a new record.\n\n        :attr:`record` can be a `list`, `dict`, or a single scalar. The\n        record type is determined at the first time :meth:`add` is called.\n        All subsequent calls to :meth:`add` must have the same type of\n        :attr:`record`.\n\n        :attr:`record` in subsequent calls to :meth:`add` can contain only\n        a subset of fields than the first call to :meth:`add`.\n\n        Example:\n\n            .. code-block:: python\n\n                recorder.add({\'1\': 0.2, \'2\': 0.2}) # 1st call to `add`\n                x = recorder.add({\'1\': 0.4}) # 2nd call to `add`\n                # x == {\'1\': 0.3, \'2\': 0.2}\n\n        Args:\n            record: A single scalar, a list of scalars, or a dict of scalars.\n            weight (optional): A scalar, weight of the new record for\n                calculating a weighted average. If `None`, weight is set to `1`.\n                For example, :attr:`weight` can be set to batch size and\n                :attr:`record` the average value of certain metrics on the batch\n                in order to calculate the average metric values on a whole\n                dataset.\n\n        Returns:\n            The (moving) average after appending the record, with the same\n            type as :attr:`record`.\n        """"""\n        if self._record_type is None:\n            self._record_type = type(record)\n        elif self._record_type != type(record):\n            raise ValueError(\'The type of `record` is not consistent. \'\n                             \'Expect type `{}`\'.format(self._record_type))\n\n        record_dict = self._to_dict(record)\n        if self._recorders is None:\n            self._recorders = {\n                name: _SingleAverageRecorder(\n                    self._size, name if self._record_type == dict else None)\n                for name in record_dict.keys()\n            }\n\n        for name, val in record_dict.items():\n            self._recorders[name].add(val, weight=weight)\n\n        return self.avg()\n\n    def avg(self, id_or_name=None):\n        """"""Returns the (moving) average.\n\n        Args:\n            id_or_name (optional): A list of or a single element.\n                Each element is the index (if the record type is `list`) or\n                name (if the record type is `dict`) of the field for which\n                the average is calculated. If not given, the average of all\n                fields are returned.\n\n        Returns:\n            The average value(s). If :attr:`id_or_name` is a single element\n            (not a list), then returns the average value of the corresponding\n            field. Otherwise, if :attr:`id_or_name` is a list of element(s),\n            then returns average value(s) in the same type as :attr:`record`\n            of :meth:`add`.\n        """"""\n        if self._recorders is None:\n            return 0.\n\n        keys = id_or_name\n        if keys is None:\n            keys = list(self._recorders.keys())\n\n        if not isinstance(keys, (list, tuple)):\n            return self._recorders[keys].avg()\n\n        avg = {key: self._recorders[key].avg() for key in keys}\n        if self._record_type in {list, tuple}:\n            ret_avg = []\n            for k, v in avg.items():\n                if k in keys:\n                    ret_avg.append(v)\n            return self._record_type(ret_avg)\n        elif self._record_type == dict:\n            return avg\n        else:\n            return avg[self._default_metric_name]\n\n    def reset(self, id_or_name=None):\n        """"""Resets the record.\n\n        Args:\n            id_or_name (optional): A list or a single element. Each element is\n                the index (if the record type is `list`) or name (if the\n                record type is `dict`) of the field to reset.\n                If `None`, all fields are reset.\n        """"""\n        keys = id_or_name\n        if keys is None:\n            keys = list(self._recorders.keys())\n        elif not isinstance(keys, (list, tuple)):\n            keys = [keys]\n\n        for key in keys:\n            self._recorders[key].reset()\n\n    def to_str(self, precision=None, delimiter=\' \'):\n        """"""Returns a string of the average values of the records.\n\n        Args:\n            precision (int, optional): The number of decimal places to keep in\n                the returned string. E.g., for an average value of `0.1234`,\n                :attr:`precision = 2` leads to `\'0.12\'`.\n            delimiter (str): The delimiter string that separates between\n                fields.\n\n        Returns:\n            A string of the average values.\n\n            If record is of type `dict`, the string is a concatenation of\n            \'field_name: average_value\', delimited with :attr:`delimiter`.\n            E.g., `\'field_name_1: 0.1234 field_name_2: 0.5678 ...\'`.\n\n            Otherwise, the string is of a concatenation of \'average_value\'.\n            E.g., `\'0.1234 0.5678 ...\'`\n        """"""\n        strs = {name: rec.to_str(precision=precision)\n                for name, rec in self._recorders.items()}\n        str_list = []\n        if self._record_type in {list, tuple}:\n            for i in range(len(strs)):\n                # Enumerates the keys in order, which are the indexes\n                str_list.append(strs[i])\n        elif self._record_type == dict:\n            str_list = list(strs.values())\n        else:\n            str_list = [strs[self._default_metric_name]]\n\n        avg_str = delimiter.join(str_list)\n\n        return avg_str\n'"
texar/tf/utils/beam_search.py,63,"b'# coding=utf-8\n# Copyright 2018 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2019 Texar\n# ==============================================================================\n""""""\nImplementation of beam search with penalties.\nAdapted from tensor2tensor repository.\n""""""\n\nimport tensorflow as tf\n\nfrom tensorflow.python.util import nest\nfrom texar.tf.utils.shapes import shape_list\n\n# Default value for INF\nINF = 1. * 1e7\n\n\ndef _merge_beam_dim(tensor):\n    """"""Reshapes first two dimensions in to single dimension.\n\n    Args:\n        tensor: Tensor to reshape of shape [A, B, ...]\n\n    Returns:\n        Reshaped tensor of shape [A*B, ...]\n    """"""\n    if not isinstance(tensor, tf.Tensor):\n        return tensor\n    shape = shape_list(tensor)\n    shape[0] *= shape[1]    # batch -> batch * beam_size\n    shape.pop(1)    # Remove beam dim\n    return tf.reshape(tensor, shape)\n\n\ndef _unmerge_beam_dim(tensor, batch_size, beam_size):\n    """"""Reshapes first dimension back to [batch_size, beam_size].\n\n    Args:\n        tensor: Tensor to reshape of shape [batch_size*beam_size, ...]\n        batch_size: Tensor, original batch size.\n        beam_size: int, original beam size.\n\n    Returns:\n        Reshaped tensor of shape [batch_size, beam_size, ...]\n    """"""\n    if not isinstance(tensor, tf.Tensor):\n        return tensor\n    shape = shape_list(tensor)\n    new_shape = [batch_size] + [beam_size] + shape[1:]\n    return tf.reshape(tensor, new_shape)\n\n\ndef _expand_to_beam_size(tensor, beam_size):\n    """"""Tiles a given tensor by beam_size.\n\n    Args:\n        tensor: tensor to tile [batch_size, ...]\n        beam_size: How much to tile the tensor by.\n\n    Returns:\n        Tiled tensor [batch_size, beam_size, ...]\n    """"""\n    if not isinstance(tensor, tf.Tensor):\n        return tensor\n    tensor = tf.expand_dims(tensor, axis=1)\n    tile_dims = [1] * tensor.shape.ndims\n    tile_dims[1] = beam_size\n\n    return tf.tile(tensor, tile_dims)\n\n\ndef get_state_shape_invariants(tensor):\n    """"""Returns the shape of the tensor but sets middle dims to None.""""""\n    shape = tensor.shape.as_list()\n    for i in range(1, len(shape) - 1):\n        shape[i] = None\n    return tf.TensorShape(shape)\n\n\ndef log_prob_from_logits(logits):\n    return logits - tf.reduce_logsumexp(logits, axis=-1, keepdims=True)\n\n\ndef compute_batch_indices(batch_size, beam_size):\n    """"""Computes the i\'th coodinate that contains the batch index for\n    gathers.\n\n    Batch pos is a tensor like [[0,0,0,0,],[1,1,1,1],..]. It says which\n    batch the beam item is in. This will create the i of the i,j coordinate\n    needed for the gather.\n\n    Args:\n        batch_size: Batch size\n        beam_size: Size of the beam.\n\n    Returns:\n        batch_pos: [batch_size, beam_size] tensor of ids\n    """"""\n    batch_pos = tf.range(batch_size * beam_size) // beam_size\n    batch_pos = tf.reshape(batch_pos, [batch_size, beam_size])\n    return batch_pos\n\n\ndef compute_topk_scores_and_seq(sequences, scores, scores_to_gather, flags,\n                                beam_size, batch_size, prefix=""default"",\n                                states_to_gather=None):\n    """"""Given sequences and scores, will gather the top k=beam size\n    sequences.\n\n    This function is used to grow alive, and finished. It takes sequences,\n    scores, and flags, and returns the top k from sequence\n    scores_to_gather, and flags based on the values in scores.\n\n    This method permits easy introspection using tfdbg. It adds three\n    named ops that are prefixed by `prefix`:\n        - _topk_seq: the tensor for topk_seq returned by this method.\n        - _topk_flags: the tensor for topk_finished_flags returned by this\n            method.\n        - _topk_scores: the tensor for tokp_gathered_scores returned by\n            this method.\n\n    Args:\n        sequences: Tensor of sequences that we need to gather from.\n            [batch_size, beam_size, seq_length]\n        scores: Tensor of scores for each sequence in sequences.\n            [batch_size, beam_size]. We will use these to compute the topk.\n        scores_to_gather: Tensor of scores for each sequence in sequences.\n            [batch_size, beam_size]. We will return the gathered scores\n            from here.\n            Scores to gather is different from scores because for\n            grow_alive, we will need to return log_probs, while for\n            grow_finished, we will need to return the length penalized\n            scors.\n        flags: Tensor of bools for sequences that say whether a sequence\n            has reached EOS or not\n        beam_size: int\n        batch_size: int\n        prefix: string that will prefix unique names for the ops run.\n        states_to_gather: dict (possibly nested) of decoding states.\n\n    Returns:\n        Tuple of\n        (topk_seq [batch_size, beam_size, decode_length],\n         topk_gathered_scores [batch_size, beam_size],\n         topk_finished_flags[batch_size, beam_size])\n    """"""\n    _, topk_indexes = tf.nn.top_k(scores, k=beam_size)\n    # The next three steps are to create coordinates for tf.gather_nd to\n    # pull out the topk sequences from sequences based on scores.\n    # batch pos is a tensor like [[0,0,0,0,],[1,1,1,1],..]. It says which\n    # batch the beam item is in. This will create the i of the i,j\n    # coordinate needed for the gather\n    batch_pos = compute_batch_indices(batch_size, beam_size)\n\n    # top coordinates will give us the actual coordinates to do the gather.\n    # stacking will create a tensor of dimension batch * beam * 2, where\n    # the last dimension contains the i,j gathering coordinates.\n    top_coordinates = tf.stack([batch_pos, topk_indexes], axis=2)\n\n    # Gather up the highest scoring sequences.    For each operation\n    # added, give it a concrete name to simplify observing these\n    # operations with tfdbg. Clients can capture these tensors by watching\n    # these node names.\n    def gather(tensor, name):\n        if not isinstance(tensor, tf.Tensor):\n            return tensor\n        return tf.gather_nd(tensor, top_coordinates, name=(prefix + name))\n    topk_seq = gather(sequences, ""_topk_seq"")\n    topk_flags = gather(flags, ""_topk_flags"")\n    topk_gathered_scores = gather(scores_to_gather, ""_topk_scores"")\n    if states_to_gather:\n        topk_gathered_states = nest.map_structure(\n            lambda state: gather(state, ""_topk_states""), states_to_gather)\n    else:\n        topk_gathered_states = states_to_gather\n    return topk_seq, topk_gathered_scores, topk_flags, topk_gathered_states\n\n\ndef beam_search(symbols_to_logits_fn,\n                initial_ids,\n                beam_size,\n                decode_length,\n                vocab_size,\n                alpha,\n                eos_id,\n                states=None,\n                stop_early=True):\n    """"""Beam search with length penalties.\n\n    Requires a function that can take the currently decoded symbols and\n    return the logits for the next symbol. The implementation is inspired\n    by https://arxiv.org/abs/1609.08144.\n\n    When running, the beam search steps can be visualized by using tfdbg to\n    watch the operations generating the output ids for each beam step.\n    These operations have the pattern:\n        (alive|finished)_topk_(seq,scores)\n\n    Operations marked `alive` represent the new beam sequences that will be\n    processed in the next step.    Operations marked `finished` represent\n    the completed beam sequences, which may be padded with 0s if no beams\n    finished.\n\n    Operations marked `seq` store the full beam sequence for the time step.\n    Operations marked `scores` store the sequence\'s final log scores.\n\n    The beam search steps will be processed sequentially in order, so when\n    capturing observed from these operations, tensors, clients can make\n    assumptions about which step is being recorded.\n\n    WARNING: Assumes 2nd dimension of tensors in `states` and not\n    invariant, this means that the shape of the 2nd dimension of these\n    tensors will not be available (i.e. set to None) inside\n    symbols_to_logits_fn.\n\n    Args:\n        symbols_to_logits_fn: Interface to the model, to provide logits.\n            Should take [batch_size, decoded_ids] and return\n            [batch_size, vocab_size]\n        initial_ids: Ids to start off the decoding, this will be the first\n            thing handed to symbols_to_logits_fn (after expanding to beam size)\n            [batch_size]\n        beam_size: Size of the beam.\n        decode_length: Number of steps to decode for.\n        vocab_size: Size of the vocab, must equal the size of the logits\n            returned by symbols_to_logits_fn\n        alpha: alpha for length penalty.\n        states: dict (possibly nested) of decoding states.\n        eos_id: ID for end of sentence.\n        stop_early: a boolean - stop once best sequence is provably\n            determined.\n\n    Returns:\n        Tuple of\n        (decoded beams [batch_size, beam_size, decode_length]\n         decoding probablities [batch_size, beam_size])\n    """"""\n    batch_size = shape_list(initial_ids)[0]\n\n    # Assume initial_ids are prob 1.0\n    initial_log_probs = tf.constant([[0.] + [-float(""inf"")] * (\n        beam_size - 1)])\n    # Expand to beam_size (batch_size, beam_size)\n    alive_log_probs = tf.tile(initial_log_probs, [batch_size, 1])\n\n    # Expand each batch and state to beam_size\n    alive_seq = _expand_to_beam_size(initial_ids, beam_size)\n    alive_seq = tf.expand_dims(alive_seq, axis=2)\n\n    # (batch_size, beam_size, 1)\n    if states:\n        states = nest.map_structure(\n            lambda state: _expand_to_beam_size(state, beam_size), states)\n    else:\n        states = {}\n\n    # Finished will keep track of all the sequences that have finished so\n    # far\n    # Finished log probs will be negative infinity in the beginning\n    # finished_flags will keep track of booleans\n    finished_seq = tf.zeros(shape_list(alive_seq), tf.int32)\n    # Setting the scores of the initial to negative infinity.\n    finished_scores = tf.ones([batch_size, beam_size]) * -INF\n    finished_flags = tf.zeros([batch_size, beam_size], tf.bool)\n\n    def grow_finished(finished_seq, finished_scores, finished_flags,\n        curr_seq, curr_scores, curr_finished):\n        """"""Given sequences and scores, will gather the top k=beam size\n        sequences.\n\n        Args:\n            finished_seq: Current finished sequences.\n                [batch_size, beam_size, current_decoded_length]\n            finished_scores: scores for each of these sequences.\n                [batch_size, beam_size]\n            finished_flags: finished bools for each of these sequences.\n                [batch_size, beam_size]\n            curr_seq: current topk sequence that has been grown by one\n                position.\n                [batch_size, beam_size, current_decoded_length]\n            curr_scores: scores for each of these sequences. [batch_size,\n                beam_size]\n            curr_finished: Finished flags for each of these sequences.\n                [batch_size, beam_size]\n\n        Returns:\n            Tuple of\n                (Topk sequences based on scores,\n                 log probs of these sequences,\n                 Finished flags of these sequences)\n        """"""\n        # First append a column of 0\'ids to finished to make the same\n        # length with finished scores\n        finished_seq = tf.concat(\n                [finished_seq,\n                 tf.zeros([batch_size, beam_size, 1], tf.int32)], axis=2)\n\n        # Set the scores of the unfinished seq in curr_seq to large\n        # negative values\n        curr_scores += (1. - tf.cast(curr_finished, tf.float32)) * -INF\n        # concatenating the sequences and scores along beam axis\n        curr_finished_seq = tf.concat([finished_seq, curr_seq], axis=1)\n        curr_finished_scores = tf.concat([finished_scores, curr_scores],\n            axis=1)\n        curr_finished_flags = tf.concat([finished_flags, curr_finished],\n            axis=1)\n        return compute_topk_scores_and_seq(\n            curr_finished_seq, curr_finished_scores, curr_finished_scores,\n                curr_finished_flags, beam_size, batch_size,\n                ""grow_finished"")\n\n    def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished,\n        states):\n        """"""Given sequences and scores, will gather the top k=beam size\n        sequences.\n\n        Args:\n            curr_seq: current topk sequence that has been grown by one\n                position.\n                [batch_size, beam_size, i+1]\n            curr_scores: scores for each of these sequences. [batch_size,\n                beam_size]\n            curr_log_probs: log probs for each of these sequences.\n                [batch_size, beam_size]\n            curr_finished: Finished flags for each of these sequences.\n                [batch_size, beam_size]\n            states: dict (possibly nested) of decoding states.\n\n        Returns:\n            Tuple of\n                (Topk sequences based on scores,\n                 log probs of these sequences,\n                 Finished flags of these sequences)\n        """"""\n        # Set the scores of the finished seq in curr_seq to large negative\n        # values\n        curr_scores += tf.cast(curr_finished, tf.float32) * -INF\n        return compute_topk_scores_and_seq(curr_seq, curr_scores,\n            curr_log_probs, curr_finished, beam_size, batch_size,\n            ""grow_alive"", states)\n\n    def grow_topk(i, alive_seq, alive_log_probs, states):\n        r""""""Inner beam seach loop.\n\n        This function takes the current alive sequences, and grows them to\n        topk sequences where k = 2*beam. We use 2*beam because, we could\n        have beam_size number of sequences that might hit <EOS> and there\n        will be no alive sequences to continue. With 2*beam_size, this\n        will not happen. This relies on the assumption the vocab size is >\n        beam size. If this is true, we\'ll have at least beam_size non\n        <EOS> extensions if we extract the next top 2*beam words.\n        Length penalty is given by = (5+len(decode)/6) ^ -\\alpha.\n        Pls refer to https://arxiv.org/abs/1609.08144.\n\n        Args:\n            i: loop index\n            alive_seq: Topk sequences decoded so far [batch_size,\n                beam_size, i+1]\n            alive_log_probs: probabilities of these sequences.\n                [batch_size, beam_size]\n            states: dict (possibly nested) of decoding states.\n\n        Returns:\n            Tuple of\n                (Topk sequences extended by the next word,\n                 The log probs of these sequences,\n                 The scores with length penalty of these sequences,\n                 Flags indicating which of these sequences have finished\n                 decoding, dict of transformed decoding states)\n        """"""\n        # Get the logits for all the possible next symbols\n        flat_ids = tf.reshape(alive_seq, [batch_size * beam_size, -1])\n\n        # (batch_size * beam_size, decoded_length)\n        if states:\n            flat_states = nest.map_structure(_merge_beam_dim, states)\n            flat_logits, flat_states = symbols_to_logits_fn(flat_ids, i,\n                                                            flat_states)\n            states = nest.map_structure(\n                lambda t: _unmerge_beam_dim(t, batch_size, beam_size),\n                flat_states)\n        else:\n            flat_logits = symbols_to_logits_fn(flat_ids)\n        logits = tf.reshape(flat_logits, [batch_size, beam_size, -1])\n\n        # Convert logits to normalized log probs\n        candidate_log_probs = log_prob_from_logits(logits)\n\n        # Multiply the probabilites by the current probabilites of the\n        # beam.\n        # (batch_size, beam_size, vocab_size) + (batch_size, beam_size, 1)\n        log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs,\n            axis=2)\n        i_p = tf.cast(i + 1, tf.float32)\n        length_penalty = tf.pow(((5. + i_p) / 6.), alpha)\n\n        curr_scores = log_probs / length_penalty\n        # Flatten out (beam_size, vocab_size) probs in to a list of\n        # possibilites\n        flat_curr_scores = tf.reshape(curr_scores,\n            [-1, beam_size * vocab_size])\n\n        topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores,\n            k=beam_size * 2)\n\n        # Recovering the log probs because we will need to send them back\n        topk_log_probs = topk_scores * length_penalty\n\n        # Work out what beam the top probs are in.\n        topk_beam_index = topk_ids // vocab_size\n        topk_ids %= vocab_size    # Unflatten the ids\n\n        # The next three steps are to create coordinates for tf.gather_nd\n        # to pull out the correct seqences from id\'s that we need to grow.\n        # We will also use the coordinates to gather the booleans of the\n        # beam items that survived.\n        batch_pos = compute_batch_indices(batch_size, beam_size * 2)\n\n        # top beams will give us the actual coordinates to do the gather.\n        # stacking will create a tensor of dimension batch * beam * 2,\n        # where the last dimension contains the i,j gathering coordinates.\n        topk_coordinates = tf.stack([batch_pos, topk_beam_index], axis=2)\n\n        # Gather up the most probable 2*beams both for the ids and\n        # finished_in_alive bools\n        topk_seq = tf.gather_nd(alive_seq, topk_coordinates)\n        if states:\n            states = nest.map_structure(\n                lambda state: tf.gather_nd(state, topk_coordinates), states)\n\n        # Append the most probable alive\n        topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)],\n                             axis=2)\n\n        topk_finished = tf.equal(topk_ids, eos_id)\n\n        return topk_seq, topk_log_probs, topk_scores, topk_finished, states\n\n    def inner_loop(i, alive_seq, alive_log_probs, finished_seq,\n                   finished_scores, finished_flags, states):\n        """"""Inner beam search loop.\n\n        There are three groups of tensors, alive, finished, and topk.\n        The alive group contains information about the current alive\n        sequences. The topk group contains information about alive + topk\n        current decoded words the finished group contains information\n        about finished sentences, that is, the ones that have decoded to\n        <EOS>. These are what we return.\n        The general beam search algorithm is as follows:\n        While we haven\'t terminated (pls look at termination condition)\n            1. Grow the current alive to get beam*2 topk sequences\n            2. Among the topk, keep the top beam_size ones that haven\'t\n            reached EOS into alive\n            3. Among the topk, keep the top beam_size ones have reached\n            EOS into finished\n        Repeat\n        To make things simple with using fixed size tensors, we will end\n        up inserting unfinished sequences into finished in the beginning.\n        To stop that we add -ve INF to the score of the unfinished\n        sequence so that when a true finished sequence does appear, it\n        will have a higher score than all the unfinished ones.\n\n        Args:\n            i: loop index\n            alive_seq: Topk sequences decoded so far [batch_size,\n                beam_size, i+1]\n            alive_log_probs: probabilities of the beams. [batch_size,\n                beam_size]\n            finished_seq: Current finished sequences.\n                [batch_size, beam_size, i+1]\n            finished_scores: scores for each of these sequences.\n                [batch_size, beam_size]\n            finished_flags: finished bools for each of these sequences.\n                [batch_size, beam_size]\n            states: dict (possibly nested) of decoding states.\n\n        Returns:\n            Tuple of\n                (Incremented loop index\n                 New alive sequences,\n                 Log probs of the alive sequences,\n                 New finished sequences,\n                 Scores of the new finished sequences,\n                 Flags inidicating which sequence in finished as reached\n                 EOS,\n                 dict of final decoding states)\n        """"""\n\n        # Each inner loop, we carry out three steps:\n        # 1. Get the current topk items.\n        # 2. Extract the ones that have finished and haven\'t finished\n        # 3. Recompute the contents of finished based on scores.\n        topk_seq, topk_log_probs, topk_scores, topk_finished, states =\\\n            grow_topk(i, alive_seq, alive_log_probs, states)\n        alive_seq, alive_log_probs, _, states = grow_alive(\n            topk_seq, topk_scores, topk_log_probs, topk_finished, states)\n        finished_seq, finished_scores, finished_flags, _ = grow_finished(\n            finished_seq, finished_scores, finished_flags, topk_seq,\n            topk_scores, topk_finished)\n\n        return (i + 1, alive_seq, alive_log_probs, finished_seq,\n            finished_scores, finished_flags, states)\n\n    def _is_finished(i, unused_alive_seq, alive_log_probs,\n            unused_finished_seq, finished_scores, finished_in_finished,\n            unused_states):\n        """"""Checking termination condition.\n\n        We terminate when we decoded up to decode_length or the lowest\n        scoring item in finished has a greater score that the higest prob\n        item in alive divided by the max length penalty\n\n        Args:\n            i: loop index\n            alive_log_probs: probabilities of the beams. [batch_size,\n                beam_size]\n            finished_scores: scores for each of these sequences.\n                [batch_size, beam_size]\n            finished_in_finished: finished bools for each of these\n                sequences. [batch_size, beam_size]\n\n        Returns:\n            Bool.\n        """"""\n        if not stop_early:\n            return tf.less(i, decode_length)\n        max_length_penalty = tf.pow(\n            ((5. + tf.cast(decode_length, tf.float32)) / 6.), alpha)\n        # The best possible score of the most likley alive sequence\n        lower_bound_alive_scores = alive_log_probs[:, 0] /\\\n            max_length_penalty\n\n        # Now to compute the lowest score of a finished sequence in\n        # finished\n        # If the sequence isn\'t finished, we multiply it\'s score by 0.\n        # since scores are all -ve, taking the min will give us the score\n        # of the lowest finished item.\n        lowest_score_of_fininshed_in_finished = tf.reduce_min(\n            finished_scores * tf.cast(finished_in_finished, tf.float32),\n            axis=1)\n        # If none of the sequences have finished, then the min will be 0\n        # and we have to replace it by -ve INF if it is. The score of any\n        # seq in alive will be much higher than -ve INF and the\n        # termination condition will not be met.\n        lowest_score_of_fininshed_in_finished += (\n            (1. - tf.cast(tf.reduce_any(finished_in_finished,\n            1), tf.float32)) * -INF)\n\n        bound_is_met = tf.reduce_all(\n            tf.greater(lowest_score_of_fininshed_in_finished,\n            lower_bound_alive_scores))\n\n        return tf.logical_and(\n            tf.less(i, decode_length), tf.logical_not(bound_is_met))\n\n    (_, alive_seq, alive_log_probs, finished_seq, finished_scores,\n     finished_flags, _) = tf.while_loop(\n        _is_finished,\n        inner_loop, [\n            tf.constant(0), alive_seq, alive_log_probs, finished_seq,\n            finished_scores, finished_flags, states\n        ],\n        shape_invariants=[\n            tf.TensorShape([]),\n            tf.TensorShape([None, None, None]),\n            alive_log_probs.get_shape(),\n            tf.TensorShape([None, None, None]),\n            finished_scores.get_shape(),\n            finished_flags.get_shape(),\n            nest.map_structure(get_state_shape_invariants, states),\n        ],\n        parallel_iterations=1,\n        back_prop=False)\n\n    alive_seq.set_shape((None, beam_size, None))\n    finished_seq.set_shape((None, beam_size, None))\n\n    # Accounting for corner case: It\'s possible that no sequence in alive\n    # for a particular batch item ever reached EOS. In that case, we\n    # should just copy the contents of alive for that batch item. tf\n    # reduce_any(finished_flags, 1)\n    # if 0, means that no sequence for that batch index had reached EOS.\n    # We need to do the same for the scores as well.\n    finished_seq = tf.where(\n        tf.reduce_any(finished_flags, 1), finished_seq, alive_seq)\n    finished_scores = tf.where(\n        tf.reduce_any(finished_flags, 1), finished_scores, alive_log_probs)\n    return finished_seq, finished_scores\n'"
texar/tf/utils/dtypes.py,23,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to data types.\n""""""\n\n# pylint: disable=invalid-name, no-member, protected-access\n\nimport six\nimport numpy as np\n\nimport tensorflow as tf\n\n__all__ = [\n    ""get_tf_dtype"",\n    ""is_callable"",\n    ""is_str"",\n    ""is_placeholder"",\n    ""maybe_hparams_to_dict"",\n    ""compat_as_text""\n]\n\n\ndef get_tf_dtype(dtype):  # pylint: disable=too-many-return-statements\n    """"""Returns equivalent tf dtype.\n\n    Args:\n        dtype: A str, python numeric or string type, numpy data type, or\n            tf dtype.\n\n    Returns:\n        The corresponding tf dtype.\n    """"""\n    if dtype in {\'float\', \'float32\', \'tf.float32\', float,\n                 np.float32, tf.float32}:\n        return tf.float32\n    elif dtype in {\'float64\', \'tf.float64\', np.float64, np.float_, tf.float64}:\n        return tf.float64\n    elif dtype in {\'float16\', \'tf.float16\', np.float16, tf.float16}:\n        return tf.float16\n    elif dtype in {\'int\', \'int32\', \'tf.int32\', int, np.int32, tf.int32}:\n        return tf.int32\n    elif dtype in {\'int64\', \'tf.int64\', np.int64, tf.int64}:\n        return tf.int64\n    elif dtype in {\'int16\', \'tf.int16\', np.int16, tf.int16}:\n        return tf.int16\n    elif dtype in {\'bool\', \'tf.bool\', bool, np.bool_, tf.bool}:\n        return tf.bool\n    elif dtype in {\'string\', \'str\', \'tf.string\', str, np.str, tf.string}:\n        return tf.string\n    try:\n        if dtype == {\'unicode\', unicode}:\n            return tf.string\n    except NameError:\n        pass\n\n    raise ValueError(\n        ""Unsupported conversion from type {} to tf dtype"".format(str(dtype)))\n\n\ndef is_callable(x):\n    """"""Return `True` if :attr:`x` is callable.\n    """"""\n    try:\n        _is_callable = callable(x)\n    except BaseException:  # pylint: disable=bare-except\n        _is_callable = hasattr(x, \'__call__\')\n    return _is_callable\n\n\ndef is_str(x):\n    """"""Returns `True` if :attr:`x` is either a str or unicode. Returns `False`\n    otherwise.\n    """"""\n    return isinstance(x, six.string_types)\n\n\ndef is_placeholder(x):\n    """"""Returns `True` if :attr:`x` is a :tf_main:`tf.placeholder <placeholder>`\n    or :tf_main:`tf.placeholder_with_default <placeholder_with_default>`.\n    """"""\n    try:\n        return x._ops.type in [\'Placeholder\', \'PlaceholderWithDefault\']\n    except BaseException:  # pylint: disable=bare-except\n        return False\n\n\ndef maybe_hparams_to_dict(hparams):\n    """"""If :attr:`hparams` is an instance of :class:`~texar.tf.HParams`,\n    converts it to a `dict` and returns. If :attr:`hparams` is a `dict`,\n    returns as is.\n    """"""\n    if hparams is None:\n        return None\n    if isinstance(hparams, dict):\n        return hparams\n    return hparams.todict()\n\n\ndef _maybe_list_to_array(str_list, dtype_as):\n    if isinstance(dtype_as, (list, tuple)):\n        return type(dtype_as)(str_list)\n    elif isinstance(dtype_as, np.ndarray):\n        return np.array(str_list)\n    else:\n        return str_list\n\n\ndef compat_as_text(str_):\n    """"""Converts strings into `unicode` (Python 2) or `str` (Python 3).\n\n    Args:\n        str_: A string or other data types convertible to string, or an\n            `n`-D numpy array or (possibly nested) list of such elements.\n\n    Returns:\n        The converted strings of the same structure/shape as :attr:`str_`.\n    """"""\n    def _recur_convert(s):\n        if isinstance(s, (list, tuple, np.ndarray)):\n            s_ = [_recur_convert(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n        else:\n            try:\n                return tf.compat.as_text(s)\n            except TypeError:\n                return tf.compat.as_text(str(s))\n\n    text = _recur_convert(str_)\n\n    return text\n'"
texar/tf/utils/exceptions.py,0,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTexar defined exceptions.\n""""""\n\n\n__all__ = [\n    ""TexarError""\n]\n\n\nclass TexarError(Exception):\n    """"""\n    Texar error.\n    """"""\n    pass\n'"
texar/tf/utils/mode.py,16,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to mode.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf import context\n\n__all__ = [\n    ""maybe_global_mode"",\n    ""is_train_mode"",\n    ""is_eval_mode"",\n    ""is_predict_mode"",\n    ""is_train_mode_py"",\n    ""is_eval_mode_py"",\n    ""is_predict_mode_py"",\n    ""switch_dropout""\n]\n\n\ndef maybe_global_mode(mode):\n    """"""Returns :func:`texar.tf.global_mode` if :attr:`mode` is `None`,\n    otherwise returns :attr:`mode` as-is.\n    """"""\n    if mode is None:\n        return context.global_mode()\n    else:\n        return mode\n\n\ndef is_train_mode(mode):\n    """"""Returns a bool Tensor indicating whether the global mode is TRAIN.\n    If :attr:`mode` is `None`, the mode is determined by\n    :func:`texar.tf.global_mode`.\n    """"""\n    if mode is None:\n        return context.global_mode_train()\n    else:\n        return tf.equal(mode, tf.estimator.ModeKeys.TRAIN)\n\n\ndef is_eval_mode(mode):\n    """"""Returns a bool Tensor indicating whether the global mode is EVAL.\n    If :attr:`mode` is `None`, the mode is determined by\n    :func:`texar.tf.global_mode`.\n    """"""\n    if mode is None:\n        return context.global_mode_eval()\n    else:\n        return tf.equal(mode, tf.estimator.ModeKeys.EVAL)\n\n\ndef is_predict_mode(mode):\n    """"""Returns a bool Tensor indicating whether the global mode is PREDICT.\n    If :attr:`mode` is `None`, the mode is determined by\n    :func:`texar.tf.global_mode`.\n    """"""\n    if mode is None:\n        return context.global_mode_predict()\n    else:\n        return tf.equal(mode, tf.estimator.ModeKeys.PREDICT)\n\n\ndef is_train_mode_py(mode, default=True):\n    """"""Returns a python boolean indicating whether the mode is TRAIN.\n\n    Args:\n        mode: A string taking value in\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`.\n            Can be `None`.\n        default (bool): The return value when :attr:`mode` is `None`. Default\n            is `True`.\n\n    Returns:\n        A python boolean.\n    """"""\n    if mode is None:\n        return default\n    if mode not in context.valid_modes():\n        raise ValueError(\'Unknown mode: {}\'.format(mode))\n    return mode == tf.estimator.ModeKeys.TRAIN\n\n\ndef is_eval_mode_py(mode, default=False):\n    """"""Returns a python boolean indicating whether the mode is EVAL.\n\n    Args:\n        mode: A string taking value in\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`.\n            Can be `None`.\n        default (bool): The return value when :attr:`mode` is `None`. Default\n            is `False`.\n\n    Returns:\n        A python boolean.\n    """"""\n    if mode is None:\n        return default\n    if mode not in context.valid_modes():\n        raise ValueError(\'Unknown mode: {}\'.format(mode))\n    return mode == tf.estimator.ModeKeys.EVAL\n\n\ndef is_predict_mode_py(mode, default=False):\n    """"""Returns a python boolean indicating whether the mode is PREDICT.\n\n    Args:\n        mode: A string taking value in\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`.\n            Can be `None`.\n        default (bool): The return value when :attr:`mode` is `None`. Default\n            is `False`.\n\n    Returns:\n        A python boolean.\n    """"""\n    if mode is None:\n        return default\n    if mode not in context.valid_modes():\n        raise ValueError(\'Unknown mode: {}\'.format(mode))\n    return mode == tf.estimator.ModeKeys.PREDICT\n\n\ndef switch_dropout(dropout_keep_prob, mode=None):\n    """"""Turns off dropout when not in training mode.\n\n    Args:\n        dropout_keep_prob: Dropout keep probability in training mode\n        mode (optional): A Tensor taking values of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`.\n            Dropout is activated if :attr:`mode` is `TRAIN`.\n            If `None`, the mode is inferred from\n            :func:`texar.tf.global_mode`.\n\n    Returns:\n        A unit Tensor that equals the dropout keep probability in `TRAIN` mode,\n        and `1.0` in other modes.\n    """"""\n    return 1. - (1. - dropout_keep_prob) \\\n        * tf.cast(is_train_mode(mode), tf.float32)\n'"
texar/tf/utils/shapes.py,58,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to tensor shapes.\n""""""\n\n# pylint: disable=no-name-in-module, protected-access, no-member, invalid-name\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.framework import ops\n\n__all__ = [\n    ""transpose_batch_time"",\n    ""get_batch_size"",\n    ""get_rank"",\n    ""mask_sequences"",\n    ""_mask_sequences_tensor"",\n    ""_mask_sequences_py"",\n    ""reduce_with_weights"",\n    ""flatten"",\n    ""shape_list"",\n    ""pad_and_concat"",\n    ""varlength_concat"",\n    ""varlength_concat_py"",\n    ""varlength_roll""\n]\n\n\ndef transpose_batch_time(inputs):\n    """"""Transposes inputs between time-major and batch-major.\n\n    Args:\n        inputs: A Tensor of shape `[batch_size, max_time, ...]` (batch-major)\n            or `[max_time, batch_size, ...]` (time-major), or a (possibly\n            nested) tuple of such elements.\n\n    Returns:\n        A (possibly nested tuple of) Tensor with transposed batch and\n        time dimensions of inputs.\n    """"""\n    flat_input = nest.flatten(inputs)\n    flat_input = [ops.convert_to_tensor(input_) for input_ in flat_input]\n    # pylint: disable=protected-access\n    flat_input = [rnn._transpose_batch_time(input_) for input_ in flat_input]\n    return nest.pack_sequence_as(structure=inputs, flat_sequence=flat_input)\n\n\ndef get_batch_size(tensor):\n    """"""Returns a unit `Tensor` representing the batch size, i.e.,\n    the size of the 1st dimension of :attr:`tensor`.\n    """"""\n    return tf.shape(tensor)[0]\n\n\ndef get_rank(tensor):\n    """"""Returns the tensor rank as a python `int`. The input tensor can also be\n    a python array.\n\n    Args:\n        tensor: A Tensor or python array.\n\n    Returns:\n        A python `int` representing the rank of :attr:`tensor`. Returns\n        `None` if the rank cannot be determined.\n    """"""\n    if tf.contrib.framework.is_tensor(tensor):\n        shape = tensor.shape\n        try:\n            rank = len(shape.as_list())\n        except ValueError:  # when `shape==TensorShape(None)`\n            rank = None\n    else:\n        array = np.asarray(tensor)\n        rank = array.ndim\n    return rank\n\n\ndef mask_sequences(sequence,\n                   sequence_length,\n                   dtype=None,\n                   time_major=False,\n                   tensor_rank=2):\n    """"""Masks out sequence entries that are beyond the respective sequence\n    lengths. Masks along the time dimension.\n\n    :attr:`sequence` and :attr:`sequence_length` can either be python\n    arrays or Tensors, respectively. If both are python arrays (or None), the\n    return will be a python array as well.\n\n    Args:\n        sequence: A Tensor or python array of sequence values.\n            If `time_major==False` (default), this must be a Tensor of shape\n            `[batch_size, max_time, ...]`. The batch and time dimension is\n            exchanged if `time_major==True`.\n        sequence_length: A Tensor or python array of shape `[batch_size]`.\n            Time steps beyond the respective sequence lengths will be\n            made zero.\n        dtype (dtype): Type of :attr:`sequence`. If `None`, infer from\n            :attr:`sequence` automatically.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`sequence` must have shape\n            `[max_time, batch_size, ...]`.\n            If `False` (default), :attr:`sequence` must have\n            shape `[batch_size, max_time, ...]`.\n        tensor_rank (int): The number of dimensions of :attr:`sequence`.\n            Default is 2, i.e., :attr:`sequence` is a 2D Tensor consisting\n            of batch and time dimensions. Ignored if both :attr:`sequence`\n            and :attr:`sequence_length` are python arrays.\n\n    Returns:\n        The masked sequence, i.e., a Tensor or python array of the same shape\n        as :attr:`sequence` but with masked-out entries (set to zero).\n\n        If both :attr:`sequence` and :attr:`sequence_length` are python\n        arrays, the returned value is a python array as well.\n    """"""\n    is_tensor = tf.contrib.framework.is_tensor\n    if is_tensor(sequence) or is_tensor(sequence_length):\n        return _mask_sequences_tensor(\n            sequence, sequence_length, dtype, time_major, tensor_rank)\n    else:\n        return _mask_sequences_py(\n            sequence, sequence_length, dtype, time_major)\n\n\ndef _mask_sequences_tensor(sequence,\n                           sequence_length,\n                           dtype=None,\n                           time_major=False,\n                           tensor_rank=2):\n    """"""Masks out sequence entries that are beyond the respective sequence\n    lengths. Masks along the time dimension.\n\n    Args:\n        sequence: A Tensor of sequence values.\n\n            If `time_major=False` (default), this must be a Tensor of shape:\n                `[batch_size, max_time, d_2, ..., d_rank]`, where the rank of\n                the Tensor is specified with :attr:`tensor_rank`.\n\n            If `time_major=True`, this must be a Tensor of shape:\n                `[max_time, batch_size, d_2, ..., d_rank].`\n        sequence_length: A Tensor of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will be made zero.\n        dtype (dtype): Type of :attr:`sequence`. If `None`, infer from\n            :attr:`sequence` automatically.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`sequence` must have shape\n            `[max_time, batch_size, d_2, ..., d_rank]`.\n            If `False` (default), :attr:`sequence` must have\n            shape `[batch_size, max_time, d_2, ..., d_rank]`.\n        tensor_rank (int): The number of dimensions of :attr:`sequence`.\n            Default is 2, i.e., :attr:`sequence` is a 2D Tensor consisting\n            of batch and time dimensions.\n\n    Returns:\n        The masked sequence, i.e., a Tensor of the same shape as\n        :attr:`sequence` but with masked-out entries (set to zero).\n    """"""\n    if tensor_rank is None:\n        tensor_rank = 2\n    if tensor_rank < 2:\n        raise ValueError(\n            ""tensor_rank must be > 2. Got tensor_rank = {}"".format(tensor_rank))\n    if time_major:\n        sequence = rnn._transpose_batch_time(sequence)\n    max_time = tf.cast(tf.shape(sequence)[1], tf.int32)\n    if dtype is None:\n        dtype = sequence.dtype\n    mask = tf.sequence_mask(\n        tf.cast(sequence_length, tf.int32), max_time, dtype=dtype)\n    for _ in range(2, tensor_rank):\n        mask = tf.expand_dims(mask, axis=-1)\n    sequence = sequence * mask\n    if time_major:\n        sequence = rnn._transpose_batch_time(sequence)\n    return sequence\n\n\ndef _mask_sequences_py(sequence,\n                       sequence_length,\n                       dtype=None,\n                       time_major=False):\n    """"""Masks out sequence entries that are beyond the respective sequence\n    lengths. Masks along the time dimension.\n\n    This is the numpy version of :func:`texar.tf.utils.mask_sequences`.\n\n    Args:\n        sequence: An python array of sequence values.\n\n            If `time_major=False` (default), this must be an array of shape:\n                `[batch_size, max_time, ...]`\n\n            If `time_major=True`, this must be a Tensor of shape:\n                `[max_time, batch_size, ...].`\n        sequence_length: An array of shape `[batch_size]`. Time steps beyond\n            the respective sequence lengths will be made zero.\n        dtype (dtype): Type of :attr:`sequence`. If `None`, infer from\n            :attr:`sequence` automatically.\n        time_major (bool): The shape format of the inputs. If `True`,\n            :attr:`sequence` must have shape\n            `[max_time, batch_size, ...]`.\n            If `False` (default), :attr:`sequence` must have\n            shape `[batch_size, max_time, ...]`.\n\n    Returns:\n        The masked sequence, i.e., an array of the same shape as\n        :attr:`sequence` but with masked-out entries (set to zero).\n    """"""\n    sequence = np.array(sequence)\n    sequence_length = np.array(sequence_length)\n\n    rank = sequence.ndim\n    if rank < 2:\n        raise ValueError(""`sequence` must be 2D or higher order."")\n    batch_size = sequence.shape[0]\n    max_time = sequence.shape[1]\n    dtype = dtype or sequence.dtype\n\n    if time_major:\n        sequence = np.transpose(sequence, axes=[1, 0, 2])\n\n    steps = np.tile(np.arange(max_time), [batch_size, 1])\n    mask = np.asarray(steps < sequence_length[:, None], dtype=dtype)\n    for _ in range(2, rank):\n        mask = np.expand_dims(mask, -1)\n\n    sequence = sequence * mask\n\n    if time_major:\n        sequence = np.transpose(sequence, axes=[1, 0, 2])\n\n    return sequence\n\n\ndef reduce_with_weights(tensor,\n                        weights=None,\n                        average_across_batch=True,\n                        average_across_remaining=False,\n                        sum_over_batch=False,\n                        sum_over_remaining=True,\n                        tensor_rank=None):\n    """"""Weights and reduces tensor.\n\n    Args:\n        tensor: A Tensor to weight and reduce, of shape\n            `[batch_size, ...]`.\n        weights (optional): A Tensor of the same shape and dtype with\n            :attr:`tensor`. For example, this is can be a 0-1 tensor\n            for masking values of :attr:`tensor``.\n        average_across_batch (bool): If set, average the tensor across the\n            batch dimension. Must not set `average_across_batch`\'\n            and `sum_over_batch` at the same time.\n        average_across_remaining (bool): If set, average the\n            tensor across the\n            remaining dimensions. Must not set `average_across_remaining`\'\n            and `sum_over_remaining` at the same time.\n            If :attr:`weights` is given, this is a weighted average.\n        sum_over_batch (bool): If set, sum the tensor across the\n            batch dimension. Must not set `average_across_batch`\n            and `sum_over_batch` at the same time.\n        sum_over_remaining (bool): If set, sum the tensor\n            across the\n            remaining dimension. Must not set `average_across_remaining`\n            and `sum_over_remaining` at the same time.\n            If :attr:`weights` is given, this is a weighted sum.\n        tensor_rank (int, optional): The number of dimensions of\n            :attr:`tensor`. If not given, inferred from :attr:`tensor`\n            automatically.\n\n    Returns:\n        A Tensor.\n\n    Example:\n        .. code-block:: python\n\n            x = tf.constant([[10, 10, 2, 2],\n                             [20, 2, 2, 2]])\n            mask = tf.constant([[1, 1, 0, 0],\n                                [1, 0, 0, 0]])\n\n            z = reduce_with_weights(x, weights=mask)\n            # z == 20\n            # (all 2 in x are masked)\n    """"""\n    if tensor_rank is None:\n        tensor_rank = get_rank(tensor)\n    if tensor_rank is None:\n        raise ValueError(\'Unable to infer the rank of `tensor`. \'\n                         \'Please set `tensor_rank` explicitly.\')\n\n    if weights is not None:\n        tensor = tensor * weights\n\n    if tensor_rank > 1:\n        if average_across_remaining and sum_over_remaining:\n            raise ValueError(""Only one of `average_across_remaining` and ""\n                             ""`sum_over_remaining` can be set."")\n        if average_across_remaining:\n            if weights is None:\n                tensor = tf.reduce_mean(tensor, axis=np.arange(1, tensor_rank))\n            else:\n                tensor = tf.reduce_sum(tensor, axis=np.arange(1, tensor_rank))\n                weights = tf.reduce_sum(weights, axis=np.arange(1, tensor_rank))\n                tensor = tensor / weights\n        elif sum_over_remaining:\n            tensor = tf.reduce_sum(tensor, axis=np.arange(1, tensor_rank))\n\n    if average_across_batch and sum_over_batch:\n        raise ValueError(""Only one of `average_across_batch` and ""\n                         ""`sum_over_batch` can be set."")\n    if sum_over_batch:\n        tensor = tf.reduce_sum(tensor, axis=[0])\n    elif average_across_batch:\n        tensor = tf.reduce_mean(tensor, axis=[0])\n\n    return tensor\n\n\ndef flatten(tensor, preserve_dims, flattened_dim=None):\n    """"""Flattens a tensor whiling keeping several leading dimensions.\n\n    :attr:`preserve_dims` must < tensor\'s rank\n\n    Args:\n        tensor: A Tensor to flatten.\n        preserve_dims (int): The number of leading dimensions to preserve.\n        flatterned_dim (int, optional): The size of the resulting flattened\n            dimension. If not given, infer automatically, which can cause\n            a statically unknown dimension size.\n\n    Returns:\n        A Tensor with rank :attr:`perserve_dims` + 1.\n\n    Example:\n        .. code-block:: python\n\n            x = tf.ones(shape=[d_1, d_2, d_3, d_4])\n            y = flatten(x, 2) # y.shape == [d_1, d_2, d_3 * d_4]\n    """"""\n    if flattened_dim is None:\n        flattened_dim = -1\n    shape = tf.concat([tf.shape(tensor)[:preserve_dims], [flattened_dim]],\n                      axis=0)\n    tensor_ = tf.reshape(tensor, shape)\n    return tensor_\n\n\ndef shape_list(x):\n    r""""""Returns **static** shape of the input Tensor whenever possible.\n\n    Args:\n        x: A Tensor.\n\n    Returns:\n        - If the rank of `x` is unknown, returns the dynamic shape\n          ``tf.shape(x)``\n\n        - Otherwise, returns a list of dims, each of which is either an `int`\n          whenever it can be statically determined, or a scalar Tensor\n          otherwise.\n    """"""\n    x = tf.convert_to_tensor(x)\n    # If unknown rank, return dynamic shape\n    if x.get_shape().dims is None:\n        return tf.shape(x)\n    static = x.get_shape().as_list()\n    shape = tf.shape(x)\n    ret = []\n    for i, dim in enumerate(static):\n        if dim is None:\n            dim = shape[i]\n        ret.append(dim)\n    return ret\n\n\ndef pad_and_concat(values, axis, rank=None, pad_axis=None,\n                   pad_constant_values=0):\n    """"""Concats tensors along one dimension. Pads each of other dimensions of\n    the tensors to the corresponding maximum size if necessary.\n\n    Args:\n        values: A list of Tensors of the same rank.\n        axis (int): A Python int. Dimension along which to concatenate.\n        rank (int, optional): Rank of the tensors. If `None`, inferred\n            automatically from :attr:`values`.\n        pad_axis (int or list, optional): A Python int or a list of int.\n            Dimensions to pad. Paddings are only added to the end of\n            corresponding dimensions. If `None`, all dimensions except the\n            :attr:`axis` dimension are padded.\n        pad_constant_values: The scalar pad value to use. Must be same type\n            as the tensors.\n\n    Returns:\n        A `Tensor` resulting from padding and concatenation of the input\n        tensors.\n\n    Raises:\n        ValueError: If :attr:`rank` is `None` and cannot be inferred from\n            :attr:`values`.\n\n\n    Example:\n\n        .. code-block:: python\n\n            a = tf.ones([1, 2])\n            b = tf.ones([2, 3])\n\n            c = pad_and_concat([a,b], 0)\n            # c.shape == [3, 3]\n            # c == [[1, 1, 0],\n            #       [1, 1, 1],\n            #       [1, 1, 1]]\n\n            d = pad_and_concat([a,b], 1)\n            # d.shape == [2, 5]\n            # d == [[1, 1, 1, 1, 1]\n            #       [0, 0, 1, 1, 1]]\n    """"""\n    if rank is None:\n        for value in values:\n            rank = get_rank(value)\n            if rank is not None:\n                break\n    if rank is None:\n        raise ValueError(\'Cannot determine the rank of the tensors\')\n\n    def _pad_to_size(value, axis_, size):\n        """"""Pads the :attr:`axis_` of a tensor :attr:`value` to the given\n        :attr:`size`. Only pads to the end.\n\n        Args:\n            value: A Tensor.\n            axis_: A Python int.\n            size: A scalar int Tensor or Python int.\n        """"""\n        paddings = np.zeros([rank, 2], dtype=np.int32)\n        paddings[axis_, 1] = 1\n        paddings = paddings * (size - tf.shape(value)[axis_])\n        return tf.pad(value, paddings, mode=\'CONSTANT\',\n                      constant_values=pad_constant_values)\n\n    if pad_axis is None:\n        pad_axis = [r for r in range(rank) if r != axis]\n\n    pad_axis = pad_axis if isinstance(pad_axis, (list, tuple)) else [pad_axis]\n\n    for pa in pad_axis:\n        max_dim_size = tf.reduce_max([tf.shape(v)[pa] for v in values])\n        for i, v in enumerate(values):\n            values[i] = _pad_to_size(v, pa, max_dim_size)\n\n    return tf.concat(values, axis)\n\n\ndef varlength_concat(x, y, x_length, dtype=None, tensor_rank=None):\n    """"""Concatenates rows of `x` and `y` where each row of\n    `x` has a variable length.\n\n    Both `x` and `y` are of numeric dtypes, such as `tf.int32` and `tf.float32`,\n    with mask value `0`. The two tensors must be of the same dtype.\n\n    Args:\n        x: A tensor of shape `[batch_size, x_dim_2, other_dims]`.\n        y: A tensor of shape `[batch_size, y_dim_2, other_dims]`.\n            All dimensions except the 2nd dimension must be the same\n            with those of `x`.\n        x_length: A 1D int tensor of shape `[batch_size]` containing\n            the length of each `x` row.\n            Elements beyond the respective lengths will be\n            made zero.\n        dtype: Type of :attr:`x`. If `None`, inferred from\n            :attr:`x` automatically.\n        tensor_rank (int, optional): The number of dimensions of\n            :attr:`x`. If not given, inferred from :attr:`x`\n            automatically.\n\n    Returns:\n        A Tensor of shape `[batch_size, x_dim_2 + y_dim_2, other_dims]`.\n\n    Example:\n        .. code-block:: python\n\n            x = tf.constant([[1, 1, 0, 0],\n                             [1, 1, 1, 0]])\n            x_length = [2, 3]\n            y = tf.constant([[2, 2, 0],\n                             [2, 2, 2]])\n\n            out = varlength_concat(x, y, x_length)\n            # out = [[1, 1, 2, 2, 0, 0, 0]\n            #        [1, 1, 1, 2, 2, 2, 0]]\n    """"""\n    x = tf.convert_to_tensor(x)\n    y = tf.convert_to_tensor(y)\n    x_length = tf.convert_to_tensor(x_length)\n\n    if tensor_rank is None:\n        tensor_rank = get_rank(x) or get_rank(y)\n    if tensor_rank is None:\n        raise ValueError(\'Unable to infer the rank of `x`. \'\n                         \'Please set `tensor_rank` explicitly.\')\n\n    x_masked = mask_sequences(x, x_length, dtype=dtype, tensor_rank=tensor_rank)\n    zeros_y = tf.zeros_like(y)\n    x_aug = tf.concat([x_masked, zeros_y], axis=1)\n\n    zeros_x = tf.zeros_like(x)\n    y_aug = tf.concat([zeros_x, y], axis=1)\n\n    # Now, x_aug.shape == y_aug.shape\n\n    max_length_x = tf.shape(x)[1]\n    batch_size = tf.shape(x)[0]\n\n    initial_index = tf.constant(0, dtype=tf.int32)\n    initial_outputs_ta = tf.TensorArray(\n        dtype=dtype or x.dtype,\n        size=0,\n        dynamic_size=True)\n\n    def _cond(index, _):\n        return tf.less(index, batch_size)\n\n    def _body(index, outputs_ta):\n        y_aug_i_rolled = tf.roll(\n            input=y_aug[index],\n            shift=x_length[index] - max_length_x,  # shift to left\n            axis=0)\n        xy = x_aug[index] + y_aug_i_rolled\n        return [index + 1, outputs_ta.write(index, xy)]\n\n    res = tf.while_loop(_cond, _body, [initial_index, initial_outputs_ta])\n\n    return res[1].stack()\n\n\ndef varlength_concat_py(x, y, x_length, dtype=None):\n    """"""Concatenates rows of `x` and `y` where each row of\n    `x` has a variable length.\n\n    The function has the same semantic as :func:`varlength_concat`,\n    except that this function is for numpy arrays instead of TF tensors.\n\n    Both `x` and `y` are of numeric dtypes, such as `int32` and `float32`,\n    with mask value `0`. The two arrays must be of the same dtype.\n\n    Args:\n        x: A array of shape `[batch_size, x_dim_2, other_dims]`.\n        y: A array of shape `[batch_size, y_dim_2, other_dims]`.\n            All dimensions except the 2nd dimension must be the same\n            with those of `x`.\n        x_length: A 1D int array of shape `[batch_size]` containing\n            the length of each `x` row.\n            Elements beyond the respective lengths will be\n            made zero.\n        dtype: Type of :attr:`x`. If `None`, inferred from\n            :attr:`x` automatically.\n\n    Returns:\n        An array of shape `[batch_size, x_dim_2 + y_dim_2, other_dims]`.\n\n    Example:\n        .. code-block:: python\n\n            x = np.asarray([[1, 1, 0, 0],\n                            [1, 1, 1, 0]])\n            x_length = [2, 3]\n            y = np.asarray([[2, 2, 0],\n                            [2, 2, 2]])\n\n            out = varlength_concat_py(x, y, x_length)\n            # out = [[1, 1, 2, 2, 0, 0, 0]\n            #        [1, 1, 1, 2, 2, 2, 0]]\n    """"""\n    x = np.asarray(x, dtype=dtype)\n    y = np.asarray(y, dtype=dtype)\n\n    x_masked = mask_sequences(x, x_length, dtype=dtype)\n    zeros_y = np.zeros_like(y)\n    x_aug = np.concatenate([x_masked, zeros_y], axis=1)\n\n    zeros_x = np.zeros_like(x)\n    y_aug = np.concatenate([zeros_x, y], axis=1)\n\n    # Now, x_aug.shape == y_aug.shape\n\n    max_length_x = x.shape[1]\n    batch_size = x.shape[0]\n\n    for index in np.arange(batch_size):\n        y_aug_i_rolled = np.roll(\n            a=y_aug[index],\n            shift=x_length[index] - max_length_x,\n            axis=0)\n        x_aug[index] += y_aug_i_rolled\n\n    return x_aug\n\n\ndef varlength_roll(input, shift, axis=1, dtype=None):\n    """"""Rolls the elements of *each row* of a tensor along an axis for\n    variable steps.\n\n    This is a `tf.while_loop` wrapper of :tf_main:`tf.roll <roll>`. Note the\n    different definition of :attr:`shift` and :attr:`axis` here compared\n    to :tf_main:`tf.roll <roll>`.\n\n    Args:\n        input: A tensor of shape `[batch_size, other_dims]` where\n            `other_dims` can be multiple dimensions.\n        shift: A 1D int tensor of shape `[batch_size]` containing\n            the steps for which each row in the batch are rolled.\n            Positive shifts will roll towards larger indices, while\n            negative shifts will roll towards smaller indices.\n        axis: A scalar int tensor > 0. The dimension that the roll\n            should occur.\n        dtype: Type of :attr:`input`. If `None`, inferred from\n            :attr:`input` automatically.\n\n    Returns:\n        A Tensor of the same shape/dtype as :attr:`input`.\n\n    Example:\n        .. code-block:: python\n\n            x = tf.constant([[0, 0, 1, 0],\n                             [0, 1, 1, 1]])\n            shift = [-2, -1]\n\n            out = varlength_roll(x, shift)\n            # out = [[1, 0, 0, 0]\n            #        [1, 1, 1, 0]]\n\n\n        .. code-block:: python\n\n            x = tf.constant([[1, 2, 3, 4],\n                             [5, 6, 7, 8]])\n            shift = [1, -1]\n\n            out = varlength_roll(x, shift)\n            # out = [[4, 1, 2, 3]\n            #        [6, 7, 8, 5]]\n    """"""\n    x = tf.convert_to_tensor(input)\n    # x = input\n    shift = tf.convert_to_tensor(shift)\n\n    batch_size = tf.shape(x)[0]\n\n    initial_index = tf.constant(0, dtype=tf.int32)\n    initial_outputs_ta = tf.TensorArray(\n        dtype=dtype or x.dtype,\n        size=0,\n        dynamic_size=True)\n\n    def _cond(index, _):\n        return tf.less(index, batch_size)\n\n    def _body(index, outputs_ta):\n        x_i_rolled = tf.roll(\n            input=x[index],\n            shift=shift[index],\n            axis=axis - 1)\n        return [index + 1, outputs_ta.write(index, x_i_rolled)]\n\n    res = tf.while_loop(_cond, _body, [initial_index, initial_outputs_ta])\n\n    return res[1].stack()\n'"
texar/tf/utils/test.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils for unit tests.\n""""""\n\nimport os\n\n\ndef pretrained_test(func):\n    r""""""Tests involving pre-trained checkpoints are skipped using the\n    `@pretrained_test` decorator. They can be tested locally by setting the\n    environment variable `TEST_PRETRAINED=1`.\n    """"""\n    def wrapper(*args, **kwargs):\n        if os.environ.get(\'TEST_PRETRAINED\', 0) or \\\n                os.environ.get(\'TEST_ALL\', 0):\n            return func(*args, **kwargs)\n    return wrapper\n'"
texar/tf/utils/transformer_attentions.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Attentions specific to Transformer.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\n\n# pylint: disable=too-many-arguments, invalid-name, no-member\n\n__all__ = [\n    \'attention_bias_lower_triangle\',\n    \'attention_bias_ignore_padding\',\n    \'attention_bias_local\',\n]\n\n\ndef attention_bias_lower_triangle(length, bias_value=-1e18):\n    """"""Create an bias tensor to be added to attention logits.\n    Allows a query to attend to all positions up to and including its own.\n\n    Args:\n        length: a scalar.\n\n    Returns:\n        a `Tensor` with shape [1, 1, length, length].\n    """"""\n    return attention_bias_local(length, -1, 0, bias_value)\n\n\ndef attention_bias_local(length, max_backward, max_forward, bias_value=-1e18):\n    """"""Create an bias tensor to be added to attention logits.\n    A position may attend to positions at most max_distance from it,\n    forward and backwards.\n\n    This does not actually save any computation.\n\n    Args:\n        length: int\n        max_backward: int, maximum distance backward to attend. Negative\n        values indicate unlimited.\n        max_forward: int, maximum distance forward to attend. Negative\n        values indicate unlimited.\n\n    Returns:\n        a `Tensor` with shape [1, 1, length, length].\n        [batch_size, num_heads, queri_len, queri_len]\n    """"""\n    band = _ones_matrix_band_part(\n        length,\n        length,\n        max_backward,\n        max_forward,\n        out_shape=[1, 1, length, length])\n    return bias_value * (1.0 - band)\n\n\ndef attention_bias_ignore_padding(memory_padding, bias_value=-1e18):\n    """"""Create an bias tensor to be added to attention logits.\n\n    Args:\n        memory_padding: a float `Tensor` with shape [batch, memory_length].\n\n    Returns:\n        a `Tensor` with shape [batch, 1, 1, memory_length].\n        each dim corresponding to batch_size, num_heads, queries_len,\n        memory_length\n    """"""\n    ret = memory_padding * bias_value\n    return tf.expand_dims(tf.expand_dims(ret, axis=1), axis=1)\n\n\ndef _ones_matrix_band_part(rows, cols, num_lower, num_upper,\n    out_shape=None):\n    """"""Matrix band part of ones.\n    """"""\n    if all([isinstance(el, int) for el in [rows, cols, num_lower,\n        num_upper]]):\n        # Needed info is constant, so we construct in numpy\n        if num_lower < 0:\n            num_lower = rows - 1\n        if num_upper < 0:\n            num_upper = cols - 1\n        lower_mask = np.tri(cols, rows, num_lower).T\n        upper_mask = np.tri(rows, cols, num_upper)\n        band = np.ones((rows, cols)) * lower_mask * upper_mask\n        if out_shape:\n            band = band.reshape(out_shape)\n        band = tf.constant(band, tf.float32)\n    else:\n        band = tf.matrix_band_part(tf.ones([rows, cols]),\n                                   tf.cast(num_lower, tf.int64),\n                                   tf.cast(num_upper, tf.int64))\n        if out_shape:\n            band = tf.reshape(band, out_shape)\n    return band\n'"
texar/tf/utils/transformer_utils.py,29,"b'# Copyright 2018 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Modifications copyright (C) 2018 Texar\n# ==============================================================================\n""""""\nThis script is adapted from the tensor2tensor repository.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow_probability import distributions as tfpd\n\n# pylint: disable=invalid-name, too-many-arguments, too-many-locals\n\n\nclass PadRemover(object):\n    """"""Helper to remove padding from a tensor before sending to the experts.\n    The padding is computed for one reference tensor containing the padding mask\n    and then can be applied to any other tensor of shape [dim_origin,...].\n\n    Example::\n\n            input = [\n                [tok1, tok2],\n                [tok3, tok4],\n                [0, 0],\n                [0, 0],\n                [tok5, tok6],\n                [0, 0],\n            ]\n            output = [\n                [tok1, tok2],\n                [tok3, tok4],\n                [tok5, tok6],\n            ]\n    """"""\n\n    def __init__(self, pad_mask):\n        """"""Compute and store the location of the padding.\n\n        Args:\n            pad_mask (tf.Tensor): Reference padding tensor of shape\n                [batch_size,length] or [dim_origin]\n                (dim_origin=batch_size*length)\n                containing non-zeros positive values to indicate padding\n                location.\n        """"""\n        self.nonpad_ids = None\n        self.dim_origin = None\n\n        with tf.name_scope(""pad_reduce/get_ids""):\n            pad_mask = tf.reshape(pad_mask, [-1])    # Flatten the batch\n            # nonpad_ids contains coordinates of zeros rows (as pad_mask is\n            # float32, checking zero equality is done with |x| < epsilon, with\n            # epsilon=1e-9 as standard, here pad_mask only contains positive\n            # values so tf.abs would be redundant)\n            self.nonpad_ids = tf.cast(tf.where(pad_mask < 1e-9), tf.int32)\n            self.dim_origin = tf.shape(pad_mask)[:1]\n\n    def remove(self, x):\n        """"""Remove padding from the given tensor.\n\n        Args:\n            x: A Tensor of shape [dim_origin,...]\n\n        Returns:\n            A tensor of shape [dim_compressed,...] with dim_compressed\n            <= dim_origin\n        """"""\n        with tf.name_scope(""pad_reduce/remove""):\n            x_shape = x.get_shape().as_list()\n            x = tf.gather_nd(\n                x,\n                indices=self.nonpad_ids,\n            )\n            # if not context.in_eager_mode():\n            # This is a hack but for some reason, gather_nd return a tensor of\n            # undefined shape, so the shape is set up manually\n            x.set_shape([None] + x_shape[1:])\n        return x\n\n    def restore(self, x):\n        """"""Add padding back to the given tensor.\n\n        Args:\n            x: A Tensor of shape [dim_compressed,...]\n\n        Returns:\n            A tensor of shape [dim_origin,...] with\n            dim_compressed >= dim_origin. The\n            dim is restored from the original reference tensor\n        """"""\n        with tf.name_scope(""pad_reduce/restore""):\n            x = tf.scatter_nd(\n                indices=self.nonpad_ids,\n                updates=x,\n                shape=tf.concat([self.dim_origin, tf.shape(x)[1:]], axis=0),\n            )\n        return x\n\n\ndef embedding_to_padding(emb):\n    """"""Calculates the padding mask based on which embeddings are all zero.\n    We have hacked symbol_modality to return all-zero embeddings\n    for padding.\n\n    Args:\n        emb: a Tensor with shape [..., depth].\n\n    Returns:\n        a float Tensor with shape [...].\n    """"""\n    emb_sum = tf.reduce_sum(tf.abs(emb), axis=-1)\n    return tf.cast(tf.equal(emb_sum, 0.0), tf.float32)\n\n\ndef smoothing_cross_entropy(logits,\n                            labels,\n                            vocab_size,\n                            confidence,\n                            gaussian=False,\n                            zero_pad=True):\n    """"""Cross entropy with label smoothing to limit over-confidence.\n\n    Args:\n        logits: Tensor of size [batch_size, ?, vocab_size]\n        labels: Tensor of size [batch_size, ?]\n        vocab_size: Tensor representing the size of the vocabulary.\n        confidence: Used to determine on and off values for label\n            smoothing. If `gaussian` is true, `confidence` is the\n            variance to the gaussian distribution.\n        gaussian: Uses a gaussian distribution for label smoothing\n        zero_pad: use 0 as the probabitlity of the padding\n            in the smoothed labels. By setting this, we replicate the\n            numeric calculation of tensor2tensor, which doesn\'t set the\n            <BOS> token in the vocabulary.\n\n    Returns:\n        A float scalar Tensor containing the cross entropy loss.\n    """"""\n    with tf.name_scope(""smoothing_cross_entropy"", values=[logits, labels]):\n        # Low confidence is given to all non-true labels, uniformly.\n        if zero_pad:\n            low_confidence = (1.0 - confidence) / tf.cast(\n                vocab_size - 2, tf.float32)\n        else:\n            low_confidence = (1.0 - confidence) / tf.cast(\n                vocab_size - 1, tf.float32)\n\n        if gaussian and confidence > 0.0:\n            labels = tf.cast(labels, tf.float32)\n            normal_dist = tfpd.Normal(loc=labels, scale=confidence)\n            soft_targets = normal_dist.prob(\n                tf.cast(tf.range(vocab_size), tf.float32)[:, None, None])\n            # Reordering soft_targets from [vocab_size, batch_size, ?]\n            # to match logits: [batch_size, ?, vocab_size]\n            soft_targets = tf.transpose(soft_targets, perm=[1, 2, 0])\n        else:\n            soft_targets = tf.one_hot(\n                tf.cast(labels, tf.int32),\n                depth=vocab_size,\n                on_value=confidence,\n                off_value=low_confidence,\n                dtype=logits.dtype)\n        if zero_pad:\n            soft_targets = tf.concat([tf.expand_dims(\n                tf.zeros_like(labels, dtype=tf.float32), 2),\n                soft_targets[:, :, 1:]], -1)\n\n        if hasattr(tf.nn, \'softmax_cross_entropy_with_logits_v2\'):\n            cross_entropy_fn = tf.nn.softmax_cross_entropy_with_logits_v2\n        else:\n            cross_entropy_fn = tf.nn.softmax_cross_entropy_with_logits\n\n    return cross_entropy_fn(\n        logits=logits, labels=tf.stop_gradient(soft_targets))\n'"
texar/tf/utils/utils.py,18,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nMiscellaneous Utility functions.\n""""""\n\n# pylint: disable=invalid-name, no-member, no-name-in-module, protected-access\n# pylint: disable=redefined-outer-name, too-many-arguments\n\nfrom typing import List, Union\n\nimport inspect\nimport funcsigs\nfrom pydoc import locate\nimport copy\nimport collections\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.utils.dtypes import is_str, is_callable, compat_as_text, \\\n        _maybe_list_to_array\n\n# pylint: disable=anomalous-backslash-in-string\n\nMAX_SEQ_LENGTH = np.iinfo(np.int32).max\n\n# Some modules cannot be imported directly,\n# e.g., `import tensorflow.train` fails.\n# Such modules are treated in a special way in utils like `get_class` as below.\n# _unimportable_modules = {\n#    \'tensorflow.train\', \'tensorflow.keras.regularizers\'\n# }\n\n__all__ = [\n    ""_inspect_getargspec"",\n    ""get_args"",\n    ""get_default_arg_values"",\n    ""check_or_get_class"",\n    ""get_class"",\n    ""check_or_get_instance"",\n    ""get_instance"",\n    ""check_or_get_instance_with_redundant_kwargs"",\n    ""get_instance_with_redundant_kwargs"",\n    ""get_function"",\n    ""call_function_with_redundant_kwargs"",\n    ""get_instance_kwargs"",\n    ""dict_patch"",\n    ""dict_lookup"",\n    ""dict_fetch"",\n    ""dict_pop"",\n    ""flatten_dict"",\n    ""strip_token"",\n    ""strip_eos"",\n    ""strip_bos"",\n    ""strip_special_tokens"",\n    ""str_join"",\n    ""map_ids_to_strs"",\n    ""default_str"",\n    ""uniquify_str"",\n    ""ceildiv"",\n    ""straight_through"",\n    ""truncate_seq_pair"",\n]\n\n\n# TODO(zhiting): complete this\ndef _expand_name(name):\n    """"""Replaces common shorthands with respective full names.\n\n        ""tf.xxx"" --> ""tensorflow.xxx""\n        ""tx.xxx"" --> ""texar.tf.xxx""\n    """"""\n    return name\n\n\ndef _inspect_getargspec(fn):\n    """"""Returns `inspect.getargspec(fn)` for Py2 and `inspect.getfullargspec(fn)`\n    for Py3\n    """"""\n    try:\n        return inspect.getfullargspec(fn)\n    except AttributeError:\n        try:\n            return inspect.getargspec(fn)\n        except TypeError:\n            return inspect.getargspec(fn.__call__)\n\n\ndef get_args(fn):\n    """"""Gets the arguments of a function.\n\n    Args:\n        fn (callable): The function to inspect.\n\n    Returns:\n        list: A list of argument names (str) of the function.\n    """"""\n    argspec = _inspect_getargspec(fn)\n    args = argspec.args\n\n    # Empty args can be because `fn` is decorated. Use `funcsigs.signature`\n    # to re-do the inspect\n    if len(args) == 0:\n        args = funcsigs.signature(fn).parameters.keys()\n        args = list(args)\n\n    return args\n\n\ndef get_default_arg_values(fn):\n    """"""Gets the arguments and respective default values of a function.\n\n    Only arguments with default values are included in the output dictionary.\n\n    Args:\n        fn (callable): The function to inspect.\n\n    Returns:\n        dict: A dictionary that maps argument names (str) to their default\n        values. The dictionary is empty if no arguments have default values.\n    """"""\n    argspec = _inspect_getargspec(fn)\n    if argspec.defaults is None:\n        return {}\n    num_defaults = len(argspec.defaults)\n    return dict(zip(argspec.args[-num_defaults:], argspec.defaults))\n\n\ndef check_or_get_class(class_or_name, module_path=None, superclass=None):\n    """"""Returns the class and checks if the class inherits :attr:`superclass`.\n\n    Args:\n        class_or_name: Name or full path to the class, or the class itself.\n        module_paths (list, optional): Paths to candidate modules to search\n            for the class. This is used if :attr:`class_or_name` is a string and\n            the class cannot be located solely based on :attr:`class_or_name`.\n            The first module in the list that contains the class\n            is used.\n        superclass (optional): A (list of) classes that the target class\n            must inherit.\n\n    Returns:\n        The target class.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_or_name` and\n            :attr:`module_paths`.\n        TypeError: If class does not inherits :attr:`superclass`.\n    """"""\n    class_ = class_or_name\n    if is_str(class_):\n        class_ = get_class(class_, module_path)\n    if superclass is not None:\n        if not issubclass(class_, superclass):\n            raise TypeError(\n                ""A subclass of {} is expected. Got: {}"".format(\n                    superclass, class_))\n    return class_\n\n\ndef get_class(class_name, module_paths=None):\n    """"""Returns the class based on class name.\n\n    Args:\n        class_name (str): Name or full path to the class.\n        module_paths (list): Paths to candidate modules to search for the\n            class. This is used if the class cannot be located solely based on\n            `class_name`. The first module in the list that contains the class\n            is used.\n\n    Returns:\n        The target class.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n    """"""\n    class_ = locate(class_name)\n    if (class_ is None) and (module_paths is not None):\n        for module_path in module_paths:\n            # if module_path in _unimportable_modules:\n            # Special treatment for unimportable modules by directly\n            # accessing the class\n            class_ = locate(\'.\'.join([module_path, class_name]))\n            if class_ is not None:\n                break\n            # else:\n            #    module = importlib.import_module(module_path)\n            #    if class_name in dir(module):\n            #        class_ = getattr(module, class_name)\n            #        break\n\n    if class_ is None:\n        raise ValueError(\n            ""Class not found in {}: {}"".format(module_paths, class_name))\n\n    return class_\n\n\ndef check_or_get_instance(ins_or_class_or_name, kwargs, module_paths=None,\n                          classtype=None):\n    """"""Returns a class instance and checks types.\n\n    Args:\n        ins_or_class_or_name: Can be of 3 types:\n\n            - A class to instantiate.\n            - A string of the name or full path to a class to \\\n              instantiate.\n            - The class instance to check types.\n\n        kwargs (dict): Keyword arguments for the class constructor. Ignored\n            if `ins_or_class_or_name` is a class instance.\n        module_paths (list, optional): Paths to candidate modules to\n            search for the class. This is used if the class cannot be\n            located solely based on :attr:`class_name`. The first module\n            in the list that contains the class is used.\n        classtype (optional): A (list of) class of which the instance must\n            be an instantiation.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n        ValueError: If :attr:`kwargs` contains arguments that are invalid\n            for the class construction.\n        TypeError: If the instance is not an instantiation of\n            :attr:`classtype`.\n    """"""\n    ret = ins_or_class_or_name\n    if is_str(ret) or isinstance(ret, type):\n        ret = get_instance(ret, kwargs, module_paths)\n    if classtype is not None:\n        if not isinstance(ret, classtype):\n            raise TypeError(\n                ""An instance of {} is expected. Got: {}"".format(classtype, ret))\n    return ret\n\n\ndef get_instance(class_or_name, kwargs, module_paths=None):\n    """"""Creates a class instance.\n\n    Args:\n        class_or_name: A class, or its name or full path to a class to\n            instantiate.\n        kwargs (dict): Keyword arguments for the class constructor.\n        module_paths (list, optional): Paths to candidate modules to\n            search for the class. This is used if the class cannot be\n            located solely based on :attr:`class_name`. The first module\n            in the list that contains the class is used.\n\n    Returns:\n        A class instance.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_or_name` and\n            :attr:`module_paths`.\n        ValueError: If :attr:`kwargs` contains arguments that are invalid\n            for the class construction.\n    """"""\n    # Locate the class\n    class_ = class_or_name\n    if is_str(class_):\n        class_ = get_class(class_, module_paths)\n\n    # Check validity of arguments\n    class_args = set(get_args(class_.__init__))\n\n    if kwargs is None:\n        kwargs = {}\n    for key in kwargs.keys():\n        if key not in class_args:\n            raise ValueError(\n                ""Invalid argument for class %s.%s: %s, valid args: %s"" %\n                (class_.__module__, class_.__name__, key, list(class_args)))\n\n    return class_(**kwargs)\n\n\ndef check_or_get_instance_with_redundant_kwargs(\n        ins_or_class_or_name, kwargs, module_paths=None, classtype=None):\n    """"""Returns a class instance and checks types.\n\n    Only those keyword arguments in :attr:`kwargs` that are included in the\n    class construction method are used.\n\n    Args:\n        ins_or_class_or_name: Can be of 3 types:\n\n            - A class to instantiate.\n            - A string of the name or module path to a class to \\\n              instantiate.\n            - The class instance to check types.\n\n        kwargs (dict): Keyword arguments for the class constructor.\n        module_paths (list, optional): Paths to candidate modules to\n            search for the class. This is used if the class cannot be\n            located solely based on :attr:`class_name`. The first module\n            in the list that contains the class is used.\n        classtype (optional): A (list of) classes of which the instance must\n            be an instantiation.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n        ValueError: If :attr:`kwargs` contains arguments that are invalid\n            for the class construction.\n        TypeError: If the instance is not an instantiation of\n            :attr:`classtype`.\n    """"""\n    ret = ins_or_class_or_name\n    if is_str(ret) or isinstance(ret, type):\n        ret = get_instance_with_redundant_kwargs(ret, kwargs, module_paths)\n    if classtype is not None:\n        if not isinstance(ret, classtype):\n            raise TypeError(\n                ""An instance of {} is expected. Got: {}"".format(classtype, ret))\n    return ret\n\n\ndef get_instance_with_redundant_kwargs(\n        class_name, kwargs, module_paths=None):\n    """"""Creates a class instance.\n\n    Only those keyword arguments in :attr:`kwargs` that are included in the\n    class construction method are used.\n\n    Args:\n        class_name (str): A class or its name or module path.\n        kwargs (dict): A dictionary of arguments for the class constructor. It\n            may include invalid arguments which will be ignored.\n        module_paths (list of str): A list of paths to candidate modules to\n            search for the class. This is used if the class cannot be located\n            solely based on :attr:`class_name`. The first module in the list\n            that contains the class is used.\n\n    Returns:\n        A class instance.\n\n    Raises:\n        ValueError: If class is not found based on :attr:`class_name` and\n            :attr:`module_paths`.\n    """"""\n    # Locate the class\n    class_ = get_class(class_name, module_paths)\n\n    # Select valid arguments\n    selected_kwargs = {}\n    class_args = set(get_args(class_.__init__))\n    if kwargs is None:\n        kwargs = {}\n    for key, value in kwargs.items():\n        if key in class_args:\n            selected_kwargs[key] = value\n\n    return class_(**selected_kwargs)\n\n\ndef get_function(fn_or_name, module_paths=None):\n    """"""Returns the function of specified name and module.\n\n    Args:\n        fn_or_name (str or callable): Name or full path to a function, or the\n            function itself.\n        module_paths (list, optional): A list of paths to candidate modules to\n            search for the function. This is used only when the function\n            cannot be located solely based on :attr:`fn_or_name`. The first\n            module in the list that contains the function is used.\n\n    Returns:\n        A function.\n    """"""\n    if is_callable(fn_or_name):\n        return fn_or_name\n\n    fn = locate(fn_or_name)\n    if (fn is None) and (module_paths is not None):\n        for module_path in module_paths:\n            # if module_path in _unimportable_modules:\n            fn = locate(\'.\'.join([module_path, fn_or_name]))\n            if fn is not None:\n                break\n            # module = importlib.import_module(module_path)\n            # if fn_name in dir(module):\n            #    fn = getattr(module, fn_name)\n            #    break\n\n    if fn is None:\n        raise ValueError(\n            ""Method not found in {}: {}"".format(module_paths, fn_or_name))\n\n    return fn\n\n\ndef call_function_with_redundant_kwargs(fn, kwargs):\n    """"""Calls a function and returns the results.\n\n    Only those keyword arguments in :attr:`kwargs` that are included in the\n    function\'s argument list are used to call the function.\n\n    Args:\n        fn (function): A callable. If :attr:`fn` is not a python function,\n            :attr:`fn.__call__` is called.\n        kwargs (dict): A `dict` of arguments for the callable. It\n            may include invalid arguments which will be ignored.\n\n    Returns:\n        The returned results by calling :attr:`fn`.\n    """"""\n    try:\n        fn_args = set(get_args(fn))\n    except TypeError:\n        fn_args = set(get_args(fn.__cal__))\n\n    if kwargs is None:\n        kwargs = {}\n\n    # Select valid arguments\n    selected_kwargs = {}\n    for key, value in kwargs.items():\n        if key in fn_args:\n            selected_kwargs[key] = value\n\n    return fn(**selected_kwargs)\n\n\ndef get_instance_kwargs(kwargs, hparams):\n    """"""Makes a dict of keyword arguments with the following structure:\n\n    `kwargs_ = {\'hparams\': dict(hparams), **kwargs}`.\n\n    This is typically used for constructing a module which takes a set of\n    arguments as well as a argument named `hparams`.\n\n    Args:\n        kwargs (dict): A dict of keyword arguments. Can be `None`.\n        hparams: A dict or an instance of :class:`~texar.tf.HParams` Can be `None`.\n\n    Returns:\n        A `dict` that contains the keyword arguments in :attr:`kwargs`, and\n        an additional keyword argument named `hparams`.\n    """"""\n    if hparams is None or isinstance(hparams, dict):\n        kwargs_ = {\'hparams\': hparams}\n    elif isinstance(hparams, HParams):\n        kwargs_ = {\'hparams\': hparams.todict()}\n    else:\n        raise ValueError(\n            \'`hparams` must be a dict, an instance of HParams, or a `None`.\')\n    kwargs_.update(kwargs or {})\n    return kwargs_\n\n\ndef dict_patch(tgt_dict, src_dict):\n    """"""Recursively patch :attr:`tgt_dict` by adding items from :attr:`src_dict`\n    that do not exist in :attr:`tgt_dict`.\n\n    If respective items in :attr:`src_dict` and :attr:`tgt_dict` are both\n    `dict`, the :attr:`tgt_dict` item is patched recursively.\n\n    Args:\n        tgt_dict (dict): Target dictionary to patch.\n        src_dict (dict): Source dictionary.\n\n    Return:\n        dict: The new :attr:`tgt_dict` that is patched.\n    """"""\n    if src_dict is None:\n        return tgt_dict\n\n    for key, value in src_dict.items():\n        if key not in tgt_dict:\n            tgt_dict[key] = copy.deepcopy(value)\n        elif isinstance(value, dict) and isinstance(tgt_dict[key], dict):\n            tgt_dict[key] = dict_patch(tgt_dict[key], value)\n    return tgt_dict\n\n\ndef dict_lookup(dict_, keys, default=None):\n    """"""Looks up :attr:`keys` in the dict, returns the corresponding values.\n\n    The :attr:`default` is used for keys not present in the dict.\n\n    Args:\n        dict_ (dict): A dictionary for lookup.\n        keys: A numpy array or a (possibly nested) list of keys.\n        default (optional): Value to be returned when a key is not in\n            :attr:`dict_`. Error is raised if :attr:`default` is not given and\n            key is not in the dict.\n\n    Returns:\n        A numpy array of values with the same structure as :attr:`keys`.\n\n    Raises:\n        TypeError: If key is not in :attr:`dict_` and :attr:`default` is `None`.\n    """"""\n    return np.vectorize(lambda x: dict_.get(x, default))(keys)\n\n\ndef dict_fetch(src_dict, tgt_dict_or_keys):\n    """"""Fetches a sub dict of :attr:`src_dict` with the keys in\n    :attr:`tgt_dict_or_keys`.\n\n    Args:\n        src_dict: A dict or instance of :class:`~texar.tf.HParams`.\n            The source dict to fetch values from.\n        tgt_dict_or_keys: A dict, instance of :class:`~texar.tf.HParams`,\n            or a list (or a dict_keys) of keys to be included in the output\n            dict.\n\n    Returns:\n        A new dict that is a subdict of :attr:`src_dict`.\n    """"""\n    if src_dict is None:\n        return src_dict\n\n    if isinstance(tgt_dict_or_keys, HParams):\n        tgt_dict_or_keys = tgt_dict_or_keys.todict()\n    if isinstance(tgt_dict_or_keys, dict):\n        tgt_dict_or_keys = tgt_dict_or_keys.keys()\n    keys = list(tgt_dict_or_keys)\n\n    if isinstance(src_dict, HParams):\n        src_dict = src_dict.todict()\n\n    return {k: src_dict[k] for k in keys if k in src_dict}\n\n\ndef dict_pop(dict_, pop_keys, default=None):\n    """"""Removes keys from a dict and returns their values.\n\n    Args:\n        dict_ (dict): A dictionary from which items are removed.\n        pop_keys: A key or a list of keys to remove and return respective\n            values or :attr:`default`.\n        default (optional): Value to be returned when a key is not in\n            :attr:`dict_`. The default value is `None`.\n\n    Returns:\n        A `dict` of the items removed from :attr:`dict_`.\n    """"""\n    if not isinstance(pop_keys, (list, tuple)):\n        pop_keys = [pop_keys]\n    ret_dict = {key: dict_.pop(key, default) for key in pop_keys}\n    return ret_dict\n\n\ndef flatten_dict(dict_, parent_key="""", sep="".""):\n    """"""Flattens a nested dictionary. Namedtuples within the dictionary are\n    converted to dicts.\n\n    Adapted from:\n    https://github.com/google/seq2seq/blob/master/seq2seq/models/model_base.py\n\n    Args:\n        dict_ (dict): The dictionary to flatten.\n        parent_key (str): A prefix to prepend to each key.\n        sep (str): Separator that intervenes between parent and child keys.\n            E.g., if `sep` == \'.\', then `{ ""a"": { ""b"": 3 } }` is converted\n            into `{ ""a.b"": 3 }`.\n\n    Returns:\n        A new flattened `dict`.\n    """"""\n    items = []\n    for key, value in dict_.items():\n        key_ = parent_key + sep + key if parent_key else key\n        if isinstance(value, collections.MutableMapping):\n            items.extend(flatten_dict(value, key_, sep=sep).items())\n        elif isinstance(value, tuple) and hasattr(value, ""_asdict""):\n            dict_items = collections.OrderedDict(zip(value._fields, value))\n            items.extend(flatten_dict(dict_items, key_, sep=sep).items())\n        else:\n            items.append((key_, value))\n    return dict(items)\n\n\ndef default_str(str_, default_str):\n    """"""Returns :attr:`str_` if it is not `None` or empty, otherwise returns\n    :attr:`default_str`.\n\n    Args:\n        str_: A string.\n        default_str: A string.\n\n    Returns:\n        Either :attr:`str_` or :attr:`default_str`.\n    """"""\n    if str_ is not None and str_ != """":\n        return str_\n    else:\n        return default_str\n\n\ndef uniquify_str(str_, str_set):\n    """"""Uniquifies :attr:`str_` if :attr:`str_` is included in :attr:`str_set`.\n\n    This is done by appending a number to :attr:`str_`. Returns\n    :attr:`str_` directly if it is not included in :attr:`str_set`.\n\n    Args:\n        str_ (string): A string to uniquify.\n        str_set (set, dict, or list): A collection of strings. The returned\n            string is guaranteed to be different from the elements in the\n            collection.\n\n    Returns:\n        The uniquified string. Returns :attr:`str_` directly if it is\n        already unique.\n\n    Example:\n\n        .. code-block:: python\n\n            print(uniquify_str(\'name\', [\'name\', \'name_1\']))\n            # \'name_2\'\n\n    """"""\n    if str_ not in str_set:\n        return str_\n    else:\n        for i in range(1, len(str_set) + 1):\n            unique_str = str_ + ""_%d"" % i\n            if unique_str not in str_set:\n                return unique_str\n    raise ValueError(""Fails to uniquify string: "" + str_)\n\n\ndef _recur_split(s, dtype_as):\n    """"""Splits (possibly nested list of) strings recursively.\n    """"""\n    if is_str(s):\n        return _maybe_list_to_array(s.split(), dtype_as)\n    else:\n        s_ = [_recur_split(si, dtype_as) for si in s]\n        return _maybe_list_to_array(s_, s)\n\n\ndef strip_token(str_, token, is_token_list=False, compat=True):\n    """"""Returns a copy of strings with leading and trailing tokens removed.\n\n    Note that besides :attr:`token`, all leading and trailing whitespace\n    characters are also removed.\n\n    If :attr:`is_token_list` is False, then the function assumes tokens in\n    :attr:`str_` are separated with whitespace character.\n\n    Args:\n        str_: A `str`, or an `n`-D numpy array or (possibly nested)\n            list of `str`.\n        token (str): The token to strip, e.g., the \'<PAD>\' token defined in\n            :class:`~texar.tf.data.SpecialTokens`.PAD\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n        compat (bool): Whether to convert tokens into `unicode` (Python 2)\n            or `str` (Python 3).\n\n    Returns:\n        The stripped strings of the same structure/shape as :attr:`str_`.\n\n    Example:\n\n        .. code-block:: python\n\n            str_ = \'<PAD> a sentence <PAD> <PAD>  \'\n            str_stripped = strip_token(str_, \'<PAD>\')\n            # str_stripped == \'a sentence\'\n\n            str_ = [\'<PAD>\', \'a\', \'sentence\', \'<PAD>\', \'<PAD>\', \'\', \'\']\n            str_stripped = strip_token(str_, \'<PAD>\', is_token_list=True)\n            # str_stripped == \'a sentence\'\n    """"""\n    def _recur_strip(s):\n        if is_str(s):\n            if token == """":\n                return \' \'.join(s.strip().split())\n            else:\n                return \' \'.join(s.strip().split()).\\\n                    replace(\' \' + token, \'\').replace(token + \' \', \'\')\n        else:\n            s_ = [_recur_strip(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    s = str_\n\n    if compat:\n        s = compat_as_text(s)\n\n    if is_token_list:\n        s = str_join(s, compat=False)\n\n    strp_str = _recur_strip(s)\n\n    if is_token_list:\n        strp_str = _recur_split(strp_str, str_)\n\n    return strp_str\n\n\ndef strip_eos(str_, eos_token=\'<EOS>\', is_token_list=False, compat=True):\n    """"""Remove the EOS token and all subsequent tokens.\n\n    If :attr:`is_token_list` is False, then the function assumes tokens in\n    :attr:`str_` are separated with whitespace character.\n\n    Args:\n        str_: A `str`, or an `n`-D numpy array or (possibly nested)\n            list of `str`.\n        eos_token (str): The EOS token. Default is \'<EOS>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.EOS\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n        compat (bool): Whether to convert tokens into `unicode` (Python 2)\n            or `str` (Python 3).\n\n    Returns:\n        Strings of the same structure/shape as :attr:`str_`.\n    """"""\n    def _recur_strip(s):\n        if is_str(s):\n            s_tokens = s.split()\n            if eos_token in s_tokens:\n                return \' \'.join(s_tokens[:s_tokens.index(eos_token)])\n            else:\n                return s\n        else:\n            s_ = [_recur_strip(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    s = str_\n\n    if compat:\n        s = compat_as_text(s)\n\n    if is_token_list:\n        s = str_join(s, compat=False)\n\n    strp_str = _recur_strip(s)\n\n    if is_token_list:\n        strp_str = _recur_split(strp_str, str_)\n\n    return strp_str\n\n\n_strip_eos_ = strip_eos\n\n\ndef strip_bos(str_, bos_token=\'<BOS>\', is_token_list=False, compat=True):\n    """"""Remove all leading BOS tokens.\n\n    Note that besides :attr:`bos_token`, all leading and trailing whitespace\n    characters are also removed.\n\n    If :attr:`is_token_list` is False, then the function assumes tokens in\n    :attr:`str_` are separated with whitespace character.\n\n    Args:\n        str_: A `str`, or an `n`-D numpy array or (possibly nested)\n            list of `str`.\n        bos_token (str): The BOS token. Default is \'<BOS>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.BOS\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n        compat (bool): Whether to convert tokens into `unicode` (Python 2)\n            or `str` (Python 3).\n\n    Returns:\n        Strings of the same structure/shape as :attr:`str_`.\n    """"""\n    def _recur_strip(s):\n        if is_str(s):\n            if bos_token == \'\':\n                return \' \'.join(s.strip().split())\n            else:\n                return \' \'.join(s.strip().split()).replace(bos_token + \' \', \'\')\n        else:\n            s_ = [_recur_strip(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    s = str_\n\n    if compat:\n        s = compat_as_text(s)\n\n    if is_token_list:\n        s = str_join(s, compat=False)\n\n    strp_str = _recur_strip(s)\n\n    if is_token_list:\n        strp_str = _recur_split(strp_str, str_)\n\n    return strp_str\n\n\n_strip_bos_ = strip_bos\n\n\ndef strip_special_tokens(str_, strip_pad=\'<PAD>\', strip_bos=\'<BOS>\',\n                         strip_eos=\'<EOS>\', is_token_list=False, compat=True):\n    """"""Removes special tokens in strings, including:\n\n        - Removes EOS and all subsequent tokens\n        - Removes leading and and trailing PAD tokens\n        - Removes leading BOS tokens\n\n    Note that besides the special tokens, all leading and trailing whitespace\n    characters are also removed.\n\n    This is a joint function of :func:`strip_eos`, :func:`strip_pad`, and\n    :func:`strip_bos`\n\n    Args:\n        str_: A `str`, or an `n`-D numpy array or (possibly nested)\n            list of `str`.\n        strip_pad (str): The PAD token to strip from the strings (i.e., remove\n            the leading and trailing PAD tokens of the strings). Default\n            is \'<PAD>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.PAD.\n            Set to `None` or `False` to disable the stripping.\n        strip_bos (str): The BOS token to strip from the strings (i.e., remove\n            the leading BOS tokens of the strings).\n            Default is \'<BOS>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.BOS.\n            Set to `None` or `False` to disable the stripping.\n        strip_eos (str): The EOS token to strip from the strings (i.e., remove\n            the EOS tokens and all subsequent tokens of the strings).\n            Default is \'<EOS>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.EOS.\n            Set to `None` or `False` to disable the stripping.\n        is_token_list (bool): Whether each sentence in :attr:`str_` is a list\n            of tokens. If False, each sentence in :attr:`str_` is assumed to\n            contain tokens separated with space character.\n        compat (bool): Whether to convert tokens into `unicode` (Python 2)\n            or `str` (Python 3).\n\n    Returns:\n        Strings of the same shape of :attr:`str_` with special tokens stripped.\n    """"""\n    s = str_\n\n    if compat:\n        s = compat_as_text(s)\n\n    if is_token_list:\n        s = str_join(s, compat=False)\n\n    if strip_eos is not None and strip_eos is not False:\n        s = _strip_eos_(s, strip_eos, is_token_list=False, compat=False)\n\n    if strip_pad is not None and strip_pad is not False:\n        s = strip_token(s, strip_pad, is_token_list=False, compat=False)\n\n    if strip_bos is not None and strip_bos is not False:\n        s = _strip_bos_(s, strip_bos, is_token_list=False, compat=False)\n\n    if is_token_list:\n        s = _recur_split(s, str_)\n\n    return s\n\n\ndef str_join(tokens, sep=\' \', compat=True):\n    """"""Concats :attr:`tokens` along the last dimension with intervening\n    occurrences of :attr:`sep`.\n\n    Args:\n        tokens: An `n`-D numpy array or (possibly nested) list of `str`.\n        sep (str): The string intervening between the tokens.\n        compat (bool): Whether to convert tokens into `unicode` (Python 2)\n            or `str` (Python 3).\n\n    Returns:\n        An `(n-1)`-D numpy array (or list) of `str`.\n    """"""\n    def _recur_join(s):\n        if len(s) == 0:\n            return \'\'\n        elif is_str(s[0]):\n            return sep.join(s)\n        else:\n            s_ = [_recur_join(si) for si in s]\n            return _maybe_list_to_array(s_, s)\n\n    if compat:\n        tokens = compat_as_text(tokens)\n\n    str_ = _recur_join(tokens)\n\n    return str_\n\n\ndef map_ids_to_strs(ids, vocab, join=True, strip_pad=\'<PAD>\',\n                    strip_bos=\'<BOS>\', strip_eos=\'<EOS>\', compat=True):\n    """"""Transforms `int` indexes to strings by mapping ids to tokens,\n    concatenating tokens into sentences, and stripping special tokens, etc.\n\n    Args:\n        ids: An n-D numpy array or (possibly nested) list of `int` indexes.\n        vocab: An instance of :class:`~texar.tf.data.Vocab`.\n        join (bool): Whether to concat along the last dimension of the\n            the tokens into a string separated with a space character.\n        strip_pad (str): The PAD token to strip from the strings (i.e., remove\n            the leading and trailing PAD tokens of the strings). Default\n            is \'<PAD>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.PAD.\n            Set to `None` or `False` to disable the stripping.\n        strip_bos (str): The BOS token to strip from the strings (i.e., remove\n            the leading BOS tokens of the strings).\n            Default is \'<BOS>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.BOS.\n            Set to `None` or `False` to disable the stripping.\n        strip_eos (str): The EOS token to strip from the strings (i.e., remove\n            the EOS tokens and all subsequent tokens of the strings).\n            Default is \'<EOS>\' as defined in\n            :class:`~texar.tf.data.SpecialTokens`.EOS.\n            Set to `None` or `False` to disable the stripping.\n\n    Returns:\n        If :attr:`join` is True, returns a `(n-1)`-D numpy array (or list) of\n        concatenated strings. If :attr:`join` is False, returns an `n`-D numpy\n        array (or list) of str tokens.\n\n    Example:\n\n        .. code-block:: python\n\n            text_ids = [[1, 9, 6, 2, 0, 0], [1, 28, 7, 8, 2, 0]]\n\n            text = map_ids_to_strs(text_ids, data.vocab)\n            # text == [\'a sentence\', \'parsed from ids\']\n\n            text = map_ids_to_strs(\n                text_ids, data.vocab, join=False,\n                strip_pad=None, strip_bos=None, strip_eos=None)\n            # text == [[\'<BOS>\', \'a\', \'sentence\', \'<EOS>\', \'<PAD>\', \'<PAD>\'],\n            #          [\'<BOS>\', \'parsed\', \'from\', \'ids\', \'<EOS>\', \'<PAD>\']]\n    """"""\n    tokens = vocab.map_ids_to_tokens_py(ids)\n    if isinstance(ids, (list, tuple)):\n        tokens = tokens.tolist()\n\n    if compat:\n        tokens = compat_as_text(tokens)\n\n    str_ = str_join(tokens, compat=False)\n\n    str_ = strip_special_tokens(\n        str_, strip_pad=strip_pad, strip_bos=strip_bos, strip_eos=strip_eos,\n        compat=False)\n\n    if join:\n        return str_\n    else:\n        return _recur_split(str_, ids)\n\n\ndef ceildiv(a, b):\n    """"""Divides with ceil.\n\n    E.g., `5 / 2 = 2.5`, `ceildiv(5, 2) = 3`.\n\n    Args:\n        a (int): Dividend integer.\n        b (int): Divisor integer.\n\n    Returns:\n        int: Ceil quotient.\n    """"""\n    return -(-a // b)\n\n\ndef straight_through(fw_tensor, bw_tensor):\n    """"""Use a tensor in forward pass while backpropagating gradient to another.\n\n    Args:\n        fw_tensor: A tensor to be used in the forward pass.\n        bw_tensor: A tensor to which gradient is backpropagated. Must have the\n            same shape and type with :attr:`fw_tensor`.\n\n    Returns:\n        A tensor of the same shape and value with :attr:`fw_tensor` but will\n        direct gradient to bw_tensor.\n    """"""\n    return tf.stop_gradient(fw_tensor) + bw_tensor - tf.stop_gradient(bw_tensor)\n\n\ndef truncate_seq_pair(tokens_a: Union[List[int], List[str]],\n                      tokens_b: Union[List[int], List[str]],\n                      max_length: int):\n    r""""""Truncates a sequence pair in place to the maximum length.\n\n    This is a simple heuristic which will always truncate the longer sequence\n    one token at a time. This makes more sense than truncating an equal\n    percent of tokens from each, since if one sequence is very short then\n    each token that\'s truncated likely contains more information than a\n    longer sequence.\n\n    Example:\n\n    .. code-block:: python\n\n        tokens_a = [1, 2, 3, 4, 5]\n        tokens_b = [6, 7]\n        truncate_seq_pair(tokens_a, tokens_b, 5)\n        tokens_a  # [1, 2, 3]\n        tokens_b  # [6, 7]\n\n    Args:\n        tokens_a: A list of tokens or token ids.\n        tokens_b: A list of tokens or token ids.\n        max_length: maximum sequence length.\n    """"""\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n'"
texar/tf/utils/utils_io.py,11,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to input/output.\n""""""\n\n# pylint: disable=invalid-name, redefined-builtin, too-many-arguments\n\nfrom io import open\nimport os\nimport importlib\nimport yaml\n\nimport tensorflow as tf\n\nas_text = tf.compat.as_text\n\n__all__ = [\n    ""load_config_single"",\n    ""load_config"",\n    ""write_paired_text"",\n    ""maybe_create_dir"",\n    ""get_files""\n]\n\n# def get_tf_logger(fname,\n#                  verbosity=tf.logging.INFO,\n#                  to_stdio=False,\n#                  stdio_verbosity=None):\n#    """"""Creates TF logger that allows to specify log filename and whether to\n#    print to stdio at the same time.\n#\n#    Args:\n#        fname (str): The log filename.\n#        verbosity: The threshold for what messages will be logged. Default is\n#           `INFO`. Other options include `DEBUG`, `ERROR`, `FATAL`, and `WARN`.\n#            See :tf_main:`tf.logging <logging>`.\n#        to_stdio (bool): Whether to print messages to stdio at the same time.\n#        stido_verbosity (optional): The verbosity level when printing to stdio.\n#            If `None` (default), the level is set to be the same as\n#            :attr:`verbosity`. Ignored if :attr:`to_stdio` is False.\n#\n#    Returns:\n#        The TF logger.\n#    """"""\n\n\ndef _load_config_python(fname):\n    config = {}\n\n    config_module = importlib.import_module(fname.rstrip(\'.py\'))\n    for key in dir(config_module):\n        if not (key.startswith(\'__\') and key.endswith(\'__\')):\n            config[key] = getattr(config_module, key)\n\n    return config\n\n\ndef _load_config_yaml(fname):\n    with tf.gfile.GFile(fname) as config_file:\n        config = yaml.load(config_file)\n    return config\n\n\ndef load_config_single(fname, config=None):\n    """"""Loads config from a single file.\n\n    The config file can be either a Python file (with suffix \'.py\')\n    or a YAML file. If the filename is not suffixed with \'.py\', the file is\n    parsed as YAML.\n\n    Args:\n        fname (str): The config file name.\n        config (dict, optional): A config dict to which new configurations are\n            added. If `None`, a new config dict is created.\n\n    Returns:\n        A `dict` of configurations.\n    """"""\n    if fname.endswith(\'.py\'):\n        new_config = _load_config_python(fname)\n    else:\n        new_config = _load_config_yaml(fname)\n\n    if config is None:\n        config = new_config\n    else:\n        for key, value in new_config.items():\n            if key in config:\n                if isinstance(config[key], dict):\n                    config[key].update(value)\n                else:\n                    config[key] = value\n            else:\n                config[key] = value\n\n    return config\n\n\ndef load_config(config_path, config=None):\n    """"""Loads configs from (possibly multiple) file(s).\n\n    A config file can be either a Python file (with suffix \'.py\')\n    or a YAML file. If the filename is not suffixed with \'.py\', the file is\n    parsed as YAML.\n\n    Args:\n        config_path: Paths to configuration files. This can be a `list` of\n            config file names, or a path to a directory in which all files\n            are loaded, or a string of multiple file names separated by commas.\n        config (dict, optional): A config dict to which new configurations are\n            added. If `None`, a new config dict is created.\n\n    Returns:\n        A `dict` of configurations.\n    """"""\n    fnames = []\n    if isinstance(config_path, (list, tuple)):\n        fnames = list(config_path)\n    elif tf.gfile.IsDirectory(config_path):\n        for fname in tf.gfile.ListDirectory(config_path):\n            fname = os.path.join(config_path, fname)\n            if not tf.gfile.IsDirectory(fname):\n                fnames.append(fname)\n    else:\n        for fname in config_path.split("",""):\n            fname = fname.strip()\n            if not fname:\n                continue\n            fnames.append(fname)\n\n    if config is None:\n        config = {}\n\n    for fname in fnames:\n        config = load_config_single(fname, config)\n\n    return config\n\n\n# pylint: disable=too-many-locals\ndef write_paired_text(src, tgt, fname, append=False, mode=\'h\', sep=\'\\t\',\n                      src_fname_suffix=\'src\', tgt_fname_suffix=\'tgt\'):\n    """"""Writes paired text to a file.\n\n    Args:\n        src: A list (or array) of `str` source text.\n        tgt: A list (or array) of `str` target text.\n        fname (str): The output filename.\n        append (bool): Whether append content to the end of the file if exists.\n        mode (str): The mode of writing, with the following options:\n\n            - **\'h\'**: The ""horizontal"" mode. Each source target pair is \\\n                written in one line, intervened with :attr:`sep`, e.g.::\n\n                    source_1 target_1\n                    source_2 target_2\n\n            - **\'v\'**: The ""vertical"" mode. Each source target pair is \\\n                written in two consecutive lines, e.g::\n\n                    source_1\n                    target_1\n                    source_2\n                    target_2\n\n            - **\'s\'**: The ""separate"" mode. Each source target pair is \\\n                    written in corresponding lines of two files named \\\n                    as `""{fname}.{src_fname_suffix}""` \\\n                    and `""{fname}.{tgt_fname_suffix}""`, respectively.\n\n        sep (str): The string intervening between source and target. Used\n            when :attr:`mode` is set to \'h\'.\n        src_fname_suffix (str): Used when :attr:`mode` is \'s\'. The suffix to\n            the source output filename. E.g., with\n            `(fname=\'output\', src_fname_suffix=\'src\')`, the output source file\n            is named as `output.src`.\n        tgt_fname_suffix (str): Used when :attr:`mode` is \'s\'. The suffix to\n            the target output filename.\n\n    Returns:\n        The fileanme(s). If `mode` == \'h\' or \'v\', returns\n        :attr:`fname`. If `mode` == \'s\', returns a list of filenames\n        `[""{fname}.src"", ""{fname}.tgt""]`.\n    """"""\n    fmode = \'a\' if append else \'w\'\n    if mode == \'s\':\n        fn_src = \'{}.{}\'.format(fname, src_fname_suffix)\n        fn_tgt = \'{}.{}\'.format(fname, tgt_fname_suffix)\n        with open(fn_src, fmode, encoding=\'utf-8\') as fs:\n            fs.write(as_text(\'\\n\'.join(src)))\n            fs.write(\'\\n\')\n        with open(fn_tgt, fmode, encoding=\'utf-8\') as ft:\n            ft.write(as_text(\'\\n\'.join(tgt)))\n            ft.write(\'\\n\')\n        return fn_src, fn_tgt\n    else:\n        with open(fname, fmode, encoding=\'utf-8\') as f:\n            for s, t in zip(src, tgt):\n                if mode == \'h\':\n                    text = \'{}{}{}\\n\'.format(as_text(s), sep, as_text(t))\n                    f.write(as_text(text))\n                elif mode == \'v\':\n                    text = \'{}\\n{}\\n\'.format(as_text(s), as_text(t))\n                    f.write(as_text(text))\n                else:\n                    raise ValueError(\'Unknown mode: {}\'.format(mode))\n        return fname\n\n\ndef maybe_create_dir(dirname):\n    """"""Creates directory if doesn\'t exist\n    """"""\n    if not tf.gfile.IsDirectory(dirname):\n        tf.gfile.MakeDirs(dirname)\n        return True\n    return False\n\n\ndef get_files(file_paths):\n    """"""Gets a list of file paths given possibly a pattern :attr:`file_paths`.\n\n    Adapted from `tf.contrib.slim.data.parallel_reader.get_data_files`.\n\n    Args:\n        file_paths: A (list of) path to the files. The path can be a pattern,\n            e.g., /path/to/train*, /path/to/train[12]\n\n    Returns:\n        A list of file paths.\n\n    Raises:\n        ValueError: If no files are not found\n    """"""\n    if isinstance(file_paths, (list, tuple)):\n        files = []\n        for f in file_paths:\n            files += get_files(f)\n    else:\n        if \'*\' in file_paths or \'?\' in file_paths or \'[\' in file_paths:\n            files = tf.gfile.Glob(file_paths)\n        else:\n            files = [file_paths]\n    if not files:\n        raise ValueError(\'No data files found in %s\' % (file_paths,))\n    return files\n'"
texar/tf/utils/variables.py,3,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtility functions related to variables.\n""""""\n\n# pylint: disable=invalid-name\n\nimport tensorflow as tf\n\n__all__ = [\n    ""get_unique_named_variable_scope"",\n    ""add_variable"",\n    ""collect_trainable_variables""\n]\n\n\ndef get_unique_named_variable_scope(base_name):\n    """"""Returns a variable scope with a unique name.\n\n    Args:\n        base_name (str): The base name to uniquified.\n\n    Returns:\n        An instance of :tf_main:`variable_scope <variable_scope>`.\n\n    Example:\n\n        .. code-block:: python\n\n            vs = get_unique_named_variable_scope(\'base_name\')\n            with tf.variable_scope(vs):\n                ....\n    """"""\n    with tf.variable_scope(None, default_name=base_name) as vs:\n        return vs\n\n\ndef add_variable(variable, var_list):\n    """"""Adds variable to a given list.\n\n    Args:\n        variable: A (list of) variable(s).\n        var_list (list): The list where the :attr:`variable` are added to.\n    """"""\n    if isinstance(variable, (list, tuple)):\n        for var in variable:\n            add_variable(var, var_list)\n    else:\n        if variable not in var_list:\n            var_list.append(variable)\n\n\ndef collect_trainable_variables(modules):\n    """"""Collects all trainable variables of modules.\n\n    Trainable variables included in multiple modules occur only once in the\n    returned list.\n\n    Args:\n        modules: A (list of) instance of the subclasses of\n            :class:`~texar.tf.modules.ModuleBase`.\n\n    Returns:\n        A list of trainable variables in the modules.\n    """"""\n    if not isinstance(modules, (list, tuple)):\n        modules = [modules]\n\n    var_list = []\n    for mod in modules:\n        add_variable(mod.trainable_variables, var_list)\n\n    return var_list\n'"
examples/seq2seq_exposure_bias/utils/raml_samples_generation/process_samples.py,0,"b'from nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nimport sys\nimport re\nimport argparse\nimport torch\nfrom util import read_corpus\nimport numpy as np\nfrom scipy.misc import comb\nfrom vocab import Vocab, VocabEntry\nimport math\nfrom rouge import Rouge\n\n\ndef is_valid_sample(sent):\n    tokens = sent.split(\' \')\n    return len(tokens) >= 1 and len(tokens) < 50\n\n\ndef sample_from_model(args):\n    para_data = args.parallel_data\n    sample_file = args.sample_file\n    output = args.output\n\n    tgt_sent_pattern = re.compile(r""^\\[(\\d+)\\] (.*?)$"")\n    para_data = [l.strip().split(\' ||| \') for l in open(para_data)]\n\n    f_out = open(output, \'w\')\n    f = open(sample_file)\n    f.readline()\n    for src_sent, tgt_sent in para_data:\n        line = f.readline().strip()\n        assert line.startswith(\'****\')\n        line = f.readline().strip()\n        print(line)\n        assert line.startswith(\'target:\')\n\n        tgt_sent2 = line[len(\'target:\'):]\n        assert tgt_sent == tgt_sent2\n\n        line = f.readline().strip()  # samples\n\n        tgt_sent = \' \'.join(tgt_sent.split(\' \')[1:-1])\n        tgt_samples = set()\n        for i in range(1, 101):\n            line = f.readline().rstrip(\'\\n\')\n            m = tgt_sent_pattern.match(line)\n\n            assert m, line\n            assert int(m.group(1)) == i\n\n            sampled_tgt_sent = m.group(2).strip()\n\n            if is_valid_sample(sampled_tgt_sent):\n                tgt_samples.add(sampled_tgt_sent)\n\n        line = f.readline().strip()\n        assert line.startswith(\'****\')\n\n        tgt_samples.add(tgt_sent)\n        tgt_samples = list(tgt_samples)\n\n        assert len(tgt_samples) > 0\n\n        tgt_ref_tokens = tgt_sent.split(\' \')\n        bleu_scores = []\n        for tgt_sample in tgt_samples:\n            bleu_score = sentence_bleu([tgt_ref_tokens], tgt_sample.split(\' \'))\n            bleu_scores.append(bleu_score)\n\n        tgt_ranks = sorted(range(len(tgt_samples)), key=lambda i: bleu_scores[i], reverse=True)\n\n        print(\'%d samples\' % len(tgt_samples))\n\n        print(\'*\' * 50, file=f_out)\n        print(\'source: \' + src_sent, file=f_out)\n        print(\'%d samples\' % len(tgt_samples), file=f_out)\n        for i in tgt_ranks:\n            print(\'%s ||| %f\' % (tgt_samples[i], bleu_scores[i]), file=f_out)\n        print(\'*\' * 50, file=f_out)\n\n    f_out.close()\n\n\ndef get_new_ngram(ngram, n, vocab):\n    """"""\n    replace ngram `ngram` with a newly sampled ngram of the same length\n    """"""\n\n    new_ngram_wids = [np.random.randint(3, len(vocab)) for i in range(n)]\n    new_ngram = [vocab.id2word[wid] for wid in new_ngram_wids]\n\n    return new_ngram\n\n\ndef sample_ngram(args):\n    src_sents = read_corpus(args.src, \'src\')\n    tgt_sents = read_corpus(args.tgt, \'src\')  # do not read in <s> and </s>\n    f_out = open(args.output, \'w\')\n\n    vocab = torch.load(args.vocab)\n    tgt_vocab = vocab.tgt\n\n    smooth_bleu = args.smooth_bleu\n    sm_func = None\n    if smooth_bleu:\n        sm_func = SmoothingFunction().method3\n\n    for src_sent, tgt_sent in zip(src_sents, tgt_sents):\n        src_sent = \' \'.join(src_sent)\n\n        tgt_len = len(tgt_sent)\n        tgt_samples = []\n        tgt_samples_distort_rates = []    # how many unigrams are replaced\n\n        # generate 100 samples\n\n        # append itself\n        tgt_samples.append(tgt_sent)\n        tgt_samples_distort_rates.append(0)\n\n        for sid in range(args.sample_size - 1):\n            n = np.random.randint(1, min(tgt_len, args.max_ngram_size + 1))  # we do not replace the last token: it must be a period!\n\n            idx = np.random.randint(tgt_len - n)\n            ngram = tgt_sent[idx: idx + n]\n            new_ngram = get_new_ngram(ngram, n, tgt_vocab)\n\n            sampled_tgt_sent = list(tgt_sent)\n            sampled_tgt_sent[idx: idx + n] = new_ngram\n\n            # compute the probability of this sample\n            # prob = 1. / args.max_ngram_size * 1. / (tgt_len - 1 + n) * 1 / (len(tgt_vocab) ** n)\n\n            tgt_samples.append(sampled_tgt_sent)\n            tgt_samples_distort_rates.append(n)\n\n        # compute bleu scores or edit distances and rank the samples by bleu scores\n        rewards = []\n        for tgt_sample, tgt_sample_distort_rate in zip(tgt_samples, tgt_samples_distort_rates):\n            if args.reward == \'bleu\':\n                reward = sentence_bleu([tgt_sent], tgt_sample, smoothing_function=sm_func)\n            elif args.reward == \'rouge\':\n                rouge = Rouge()\n                scores = rouge.get_scores(hyps=[\' \'.join(tgt_sample).decode(\'utf-8\')], refs=[\' \'.join(tgt_sent).decode(\'utf-8\')], avg=True)\n                reward = sum([value[\'f\'] for key, value in scores.items()])\n            else:\n                reward = -tgt_sample_distort_rate\n\n            rewards.append(reward)\n\n        tgt_ranks = sorted(range(len(tgt_samples)), key=lambda i: rewards[i], reverse=True)\n        # convert list of tokens into a string\n        tgt_samples = [\' \'.join(tgt_sample) for tgt_sample in tgt_samples]\n\n        print(\'*\' * 50, file=f_out)\n        print(\'source: \' + src_sent, file=f_out)\n        print(\'%d samples\' % len(tgt_samples), file=f_out)\n        for i in tgt_ranks:\n            print(\'%s ||| %f\' % (tgt_samples[i], rewards[i]), file=f_out)\n        print(\'*\' * 50, file=f_out)\n\n    f_out.close()\n\n\ndef sample_ngram_adapt(args):\n    src_sents = read_corpus(args.src, \'src\')\n    tgt_sents = read_corpus(args.tgt, \'src\')  # do not read in <s> and </s>\n    f_out = open(args.output, \'w\')\n\n    vocab = torch.load(args.vocab)\n    tgt_vocab = vocab.tgt\n\n    max_len = max([len(tgt_sent) for tgt_sent in tgt_sents]) + 1\n\n    for src_sent, tgt_sent in zip(src_sents, tgt_sents):\n        src_sent = \' \'.join(src_sent)\n\n        tgt_len = len(tgt_sent)\n        tgt_samples = []\n\n        # generate 100 samples\n\n        # append itself\n        tgt_samples.append(tgt_sent)\n\n        for sid in range(args.sample_size - 1):\n            max_n = min(tgt_len - 1, 4)\n            bias_n = int(max_n * tgt_len / max_len) + 1\n            assert 1 <= bias_n <= 4, \'bias_n={}, not in [1,4], max_n={}, tgt_len={}, max_len={}\'.format(bias_n, max_n, tgt_len, max_len)\n\n            p = [1.0 / (max_n + 5)] * max_n\n            p[bias_n - 1] = 1 - p[0] * (max_n - 1)\n            assert abs(sum(p) - 1) < 1e-10, \'sum(p) != 1\'\n\n            n = np.random.choice(np.arange(1, int(max_n + 1)), p=p)  # we do not replace the last token: it must be a period!\n            assert n < tgt_len, \'n={}, tgt_len={}\'.format(n, tgt_len)\n\n            idx = np.random.randint(tgt_len - n)\n            ngram = tgt_sent[idx: idx + n]\n            new_ngram = get_new_ngram(ngram, n, tgt_vocab)\n\n            sampled_tgt_sent = list(tgt_sent)\n            sampled_tgt_sent[idx: idx + n] = new_ngram\n\n            tgt_samples.append(sampled_tgt_sent)\n\n        # compute bleu scores and rank the samples by bleu scores\n        bleu_scores = []\n        for tgt_sample in tgt_samples:\n            bleu_score = sentence_bleu([tgt_sent], tgt_sample)\n            bleu_scores.append(bleu_score)\n\n        tgt_ranks = sorted(range(len(tgt_samples)), key=lambda i: bleu_scores[i], reverse=True)\n        # convert list of tokens into a string\n        tgt_samples = [\' \'.join(tgt_sample) for tgt_sample in tgt_samples]\n\n        print(\'*\' * 50, file=f_out)\n        print(\'source: \' + src_sent, file=f_out)\n        print(\'%d samples\' % len(tgt_samples), file=f_out)\n        for i in tgt_ranks:\n            print(\'%s ||| %f\' % (tgt_samples[i], bleu_scores[i]), file=f_out)\n        print(\'*\' * 50, file=f_out)\n\n    f_out.close()\n\n\ndef sample_from_hamming_distance_payoff_distribution(args):\n    src_sents = read_corpus(args.src, \'src\')\n    tgt_sents = read_corpus(args.tgt, \'src\')  # do not read in <s> and </s>\n    f_out = open(args.output, \'w\')\n\n    vocab = torch.load(args.vocab)\n    tgt_vocab = vocab.tgt\n\n    payoff_prob, Z_qs = generate_hamming_distance_payoff_distribution(max(len(sent) for sent in tgt_sents),\n                                                                      vocab_size=len(vocab.tgt),\n                                                                      tau=args.temp)\n\n    for src_sent, tgt_sent in zip(src_sents, tgt_sents):\n        tgt_samples = []  # make sure the ground truth y* is in the samples\n        tgt_sent_len = len(tgt_sent) - 3  # remove <s> and </s> and ending period .\n        tgt_ref_tokens = tgt_sent[1:-1]\n        bleu_scores = []\n\n        # sample an edit distances\n        e_samples = np.random.choice(range(tgt_sent_len + 1), p=payoff_prob[tgt_sent_len], size=args.sample_size,\n                                     replace=True)\n\n        for i, e in enumerate(e_samples):\n            if e > 0:\n                # sample a new tgt_sent $y$\n                old_word_pos = np.random.choice(range(1, tgt_sent_len + 1), size=e, replace=False)\n                new_words = [vocab.tgt.id2word[wid] for wid in np.random.randint(3, len(vocab.tgt), size=e)]\n                new_tgt_sent = list(tgt_sent)\n                for pos, word in zip(old_word_pos, new_words):\n                    new_tgt_sent[pos] = word\n\n                bleu_score = sentence_bleu([tgt_ref_tokens], new_tgt_sent[1:-1])\n                bleu_scores.append(bleu_score)\n            else:\n                new_tgt_sent = list(tgt_sent)\n                bleu_scores.append(1.)\n\n            # print(\'y: %s\' % \' \'.join(new_tgt_sent))\n            tgt_samples.append(new_tgt_sent)\n\n\ndef generate_hamming_distance_payoff_distribution(max_sent_len, vocab_size, tau=1.):\n    """"""compute the q distribution for Hamming Distance (substitution only) as in the RAML paper""""""\n    probs = dict()\n    Z_qs = dict()\n    for sent_len in range(1, max_sent_len + 1):\n        counts = [1.]  # e = 0, count = 1\n        for e in range(1, sent_len + 1):\n            # apply the rescaling trick as in https://gist.github.com/norouzi/8c4d244922fa052fa8ec18d8af52d366\n            count = comb(sent_len, e) * math.exp(-e / tau) * ((vocab_size - 1) ** (e - e / tau))\n            counts.append(count)\n\n        Z_qs[sent_len] = Z_q = sum(counts)\n        prob = [count / Z_q for count in counts]\n        probs[sent_len] = prob\n\n        # print(\'sent_len=%d, %s\' % (sent_len, prob))\n\n    return probs, Z_qs\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--mode\', choices=[\'sample_from_model\', \'sample_ngram_adapt\', \'sample_ngram\'], required=True)\n    parser.add_argument(\'--vocab\', type=str)\n    parser.add_argument(\'--src\', type=str)\n    parser.add_argument(\'--tgt\', type=str)\n    parser.add_argument(\'--parallel_data\', type=str)\n    parser.add_argument(\'--sample_file\', type=str)\n    parser.add_argument(\'--output\', type=str, required=True)\n    parser.add_argument(\'--sample_size\', type=int, default=100)\n    parser.add_argument(\'--reward\', choices=[\'bleu\', \'edit_dist\', \'rouge\'], default=\'bleu\')\n    parser.add_argument(\'--max_ngram_size\', type=int, default=4)\n    parser.add_argument(\'--temp\', type=float, default=0.5)\n    parser.add_argument(\'--smooth_bleu\', action=\'store_true\', default=False)\n\n    args = parser.parse_args()\n\n    if args.mode == \'sample_ngram\':\n        sample_ngram(args)\n    elif args.mode == \'sample_from_model\':\n        sample_from_model(args)\n    elif args.mode == \'sample_ngram_adapt\':\n        sample_ngram_adapt(args)\n'"
examples/seq2seq_exposure_bias/utils/raml_samples_generation/util.py,0,"b'from collections import defaultdict\nimport numpy as np\n\n\ndef read_corpus(file_path, source):\n    data = []\n    for line in open(file_path):\n        sent = line.strip().split(\' \')\n        # only append <s> and </s> to the target sentence\n        if source == \'tgt\':\n            sent = [\'<s>\'] + sent + [\'</s>\']\n        data.append(sent)\n\n    return data\n\n\ndef batch_slice(data, batch_size, sort=True):\n    batch_num = int(np.ceil(len(data) / float(batch_size)))\n    for i in range(batch_num):\n        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n        src_sents = [data[i * batch_size + b][0] for b in range(cur_batch_size)]\n        tgt_sents = [data[i * batch_size + b][1] for b in range(cur_batch_size)]\n\n        if sort:\n            src_ids = sorted(range(cur_batch_size), key=lambda src_id: len(src_sents[src_id]), reverse=True)\n            src_sents = [src_sents[src_id] for src_id in src_ids]\n            tgt_sents = [tgt_sents[src_id] for src_id in src_ids]\n\n        yield src_sents, tgt_sents\n\n\ndef data_iter(data, batch_size, shuffle=True):\n    """"""\n    randomly permute data, then sort by source length, and partition into batches\n    ensure that the length of source sentences in each batch is decreasing\n    """"""\n\n    buckets = defaultdict(list)\n    for pair in data:\n        src_sent = pair[0]\n        buckets[len(src_sent)].append(pair)\n\n    batched_data = []\n    for src_len in buckets:\n        tuples = buckets[src_len]\n        if shuffle:\n            np.random.shuffle(tuples)\n        batched_data.extend(list(batch_slice(tuples, batch_size)))\n\n    if shuffle:\n        np.random.shuffle(batched_data)\n    for batch in batched_data:\n        yield batch\n'"
examples/seq2seq_exposure_bias/utils/raml_samples_generation/vocab.py,0,"b""import argparse\nfrom collections import Counter\nfrom itertools import chain\n\nimport torch\n\nfrom util import read_corpus\n\n\nclass VocabEntry(object):\n    def __init__(self):\n        self.word2id = dict()\n        self.unk_id = 3\n        self.word2id['<pad>'] = 0\n        self.word2id['<s>'] = 1\n        self.word2id['</s>'] = 2\n        self.word2id['<unk>'] = 3\n\n        self.id2word = {v: k for k, v in self.word2id.iteritems()}\n\n    def __getitem__(self, word):\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        return word in self.word2id\n\n    def __setitem__(self, key, value):\n        raise ValueError('vocabulary is readonly')\n\n    def __len__(self):\n        return len(self.word2id)\n\n    def __repr__(self):\n        return 'Vocabulary[size=%d]' % len(self)\n\n    def id2word(self, wid):\n        return self.id2word[wid]\n\n    def add(self, word):\n        if word not in self:\n            wid = self.word2id[word] = len(self)\n            self.id2word[wid] = word\n            return wid\n        else:\n            return self[word]\n\n    @staticmethod\n    def from_corpus(corpus, size, remove_singleton=True):\n        vocab_entry = VocabEntry()\n\n        word_freq = Counter(chain(*corpus))\n        non_singletons = [w for w in word_freq if word_freq[w] > 1]\n        print('number of word types: %d, number of word types w/ frequency > 1: %d' % (len(word_freq),\n                                                                                       len(non_singletons)))\n\n        top_k_words = sorted(word_freq.keys(), reverse=True, key=word_freq.get)[:size]\n\n        for word in top_k_words:\n            if len(vocab_entry) < size:\n                if not (word_freq[word] == 1 and remove_singleton):\n                    vocab_entry.add(word)\n\n        return vocab_entry\n\n\nclass Vocab(object):\n    def __init__(self, src_sents, tgt_sents, src_vocab_size, tgt_vocab_size, remove_singleton=True):\n        assert len(src_sents) == len(tgt_sents)\n\n        print('initialize source vocabulary ..')\n        self.src = VocabEntry.from_corpus(src_sents, src_vocab_size, remove_singleton=remove_singleton)\n\n        print('initialize target vocabulary ..')\n        self.tgt = VocabEntry.from_corpus(tgt_sents, tgt_vocab_size, remove_singleton=remove_singleton)\n\n    def __repr__(self):\n        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--src_vocab_size', default=50000, type=int, help='source vocabulary size')\n    parser.add_argument('--tgt_vocab_size', default=50000, type=int, help='target vocabulary size')\n    parser.add_argument('--include_singleton', action='store_true', default=False, help='whether to include singleton'\n                                                                                        'in the vocabulary (default=False)')\n\n    parser.add_argument('--train_src', type=str, required=True, help='file of source sentences')\n    parser.add_argument('--train_tgt', type=str, required=True, help='file of target sentences')\n\n    parser.add_argument('--output', default='vocab.bin', type=str, help='output vocabulary file')\n\n    args = parser.parse_args()\n\n    print('read in source sentences: %s' % args.train_src)\n    print('read in target sentences: %s' % args.train_tgt)\n\n    src_sents = read_corpus(args.train_src, source='src')\n    tgt_sents = read_corpus(args.train_tgt, source='tgt')\n\n    vocab = Vocab(src_sents, tgt_sents, args.src_vocab_size, args.tgt_vocab_size, remove_singleton=not args.include_singleton)\n    print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n\n    torch.save(vocab, args.output)\n    print('vocabulary saved to %s' % args.output)\n"""
texar/tf/data/data/__init__.py,9,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library data inputs.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.data.data.data_base import *\nfrom texar.tf.data.data.scalar_data import *\nfrom texar.tf.data.data.text_data_base import *\nfrom texar.tf.data.data.mono_text_data import *\nfrom texar.tf.data.data.paired_text_data import *\nfrom texar.tf.data.data.multi_aligned_data import *\nfrom texar.tf.data.data.data_iterators import *\nfrom texar.tf.data.data.dataset_utils import *\nfrom texar.tf.data.data.tfrecord_data import *\n'"
texar/tf/data/data/data_base.py,5,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase data class that is enherited by all data classes.\nA data defines data reading, parsing, batching, and other\npreprocessing operations.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.data.data import dataset_utils as dsutils\nfrom texar.tf.data.data_utils import count_file_lines\n\n__all__ = [\n    ""DataBase""\n]\n\n\nclass DataBase(object):\n    """"""Base class inheritted by all data classes.\n    """"""\n\n    def __init__(self, hparams):\n        self._hparams = HParams(hparams, self.default_hparams())\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""data"",\n            }\n\n        Here:\n\n            ""num_epochs"": int\n                Number of times the dataset should be repeated. An\n                :tf_main:`OutOfRangeError <errors/OutOfRangeError>` signal will\n                be raised after the whole repeated dataset has been iterated\n                through.\n\n                E.g., For training data, set it to 1 (default) so that you\n                will get the signal after each epoch of training. Set to -1\n                to repeat the dataset indefinitely.\n\n            ""batch_size"": int\n                Batch size, i.e., the number of consecutive elements of the\n                dataset to combine in a single batch.\n\n            ""allow_smaller_final_batch"": bool\n               Whether to allow the final batch to be smaller if there are\n               insufficient elements left. If `False`, the final batch is\n               discarded if it is smaller than batch size. Note that,\n               if `True`, `output_shapes` of the resulting dataset\n               will have a a **static** batch_size dimension equal to\n               ""batch_size"".\n\n            ""shuffle"": bool\n                Whether to randomly shuffle the elements of the dataset.\n\n            ""shuffle_buffer_size"": int\n                The buffer size for data shuffling. The larger, the better\n                the resulting data is mixed.\n\n                If `None` (default), buffer size is set to the size of the\n                whole dataset (i.e., make the shuffling the maximally\n                effective).\n\n            ""shard_and_shuffle"": bool\n                Whether to first shard the dataset and then shuffle each\n                block respectively. Useful when the whole data is too large to\n                be loaded efficiently into the memory.\n\n                If `True`, :attr:`shuffle_buffer_size` must be specified to\n                determine the size of each shard.\n\n            ""num_parallel_calls"": int\n                Number of elements from the datasets to process in parallel.\n\n            ""prefetch_buffer_size"": int\n                The maximum number of elements that will be buffered when\n                prefetching.\n\n            max_dataset_size : int\n                Maximum number of instances to include in\n                the dataset. If set to `-1` or greater than the size of\n                dataset, all instances will be included. This constraint is\n                imposed after data shuffling and filtering.\n\n            seed : int, optional\n                The random seed for shuffle.\n\n                Note that if a seed is set, the shuffle order will be exact\n                the same every time when going through the (repeated) dataset.\n\n                For example, consider a dataset with elements [1, 2, 3], with\n                ""num_epochs""`=2` and some fixed seed, the resulting sequence\n                can be: 2 1 3, 1 3 2 | 2 1 3, 1 3 2, ... That is, the orders are\n                different **within** every `num_epochs`, but are the same\n                **across** the `num_epochs`.\n\n            name : str\n                Name of the data.\n        """"""\n        return {\n            ""name"": ""data"",\n            ""num_epochs"": 1,\n            ""batch_size"": 64,\n            ""allow_smaller_final_batch"": True,\n            ""shuffle"": True,\n            ""shuffle_buffer_size"": None,\n            ""shard_and_shuffle"": False,\n            ""num_parallel_calls"": 1,\n            ""prefetch_buffer_size"": 0,\n            ""max_dataset_size"": -1,\n            ""seed"": None\n        }\n\n    @staticmethod\n    def _make_batch(dataset, hparams, padded_batch=False, padding_values=None):\n        dataset = dataset.repeat(hparams.num_epochs)\n        batch_size = hparams[""batch_size""]\n        if hparams[""allow_smaller_final_batch""]:\n            if padded_batch:\n                dataset = dataset.padded_batch(\n                    batch_size, dataset.output_shapes,\n                    padding_values=padding_values)\n            else:\n                dataset = dataset.batch(batch_size)\n        else:\n            dataset = dataset.apply(\n                tf.contrib.data.padded_batch_and_drop_remainder(\n                    batch_size, dataset.output_shapes,\n                    padding_values=padding_values))\n        return dataset\n\n    @staticmethod\n    def _shuffle_dataset(dataset, hparams, dataset_files):\n        dataset_size = None\n        shuffle_buffer_size = hparams[""shuffle_buffer_size""]\n        if hparams[""shard_and_shuffle""]:\n            if shuffle_buffer_size is None:\n                raise ValueError(\n                    ""Dataset hyperparameter \'shuffle_buffer_size\' ""\n                    ""must not be `None` if \'shard_and_shuffle\'=`True`."")\n            dataset_size = count_file_lines(dataset_files)\n            if shuffle_buffer_size >= dataset_size:\n                raise ValueError(\n                    ""Dataset size (%d) <= shuffle_buffer_size (%d). Set ""\n                    ""shuffle_and_shard to `False`."" %\n                    (dataset_size, shuffle_buffer_size))\n            # TODO(zhiting): Use a different seed?\n            dataset = dataset.apply(dsutils.random_shard_dataset(\n                dataset_size, shuffle_buffer_size, hparams[""seed""]))\n            dataset = dataset.shuffle(shuffle_buffer_size + 16,  # add a margin\n                                      seed=hparams[""seed""])\n        elif hparams[""shuffle""]:\n            if shuffle_buffer_size is None:\n                dataset_size = count_file_lines(dataset_files)\n                shuffle_buffer_size = dataset_size\n            dataset = dataset.shuffle(shuffle_buffer_size, seed=hparams[""seed""])\n\n        return dataset, dataset_size\n\n    @property\n    def num_epochs(self):\n        """"""Number of epochs.\n        """"""\n        return self._hparams.num_epochs\n\n    @property\n    def batch_size(self):\n        """"""The batch size.\n        """"""\n        return self._hparams.batch_size\n\n    @property\n    def hparams(self):\n        """"""A :class:`~texar.tf.HParams` instance of the\n        data hyperparameters.\n        """"""\n        return self._hparams\n\n    @property\n    def name(self):\n        """"""Name of the module.\n        """"""\n        return self._hparams.name\n'"
texar/tf/data/data/data_iterators.py,46,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious data iterator classes.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.data.data.data_base import DataBase\nfrom texar.tf.utils.variables import get_unique_named_variable_scope\n\n__all__ = [\n    ""DataIteratorBase"",\n    ""DataIterator"",\n    ""TrainTestDataIterator"",\n    ""FeedableDataIterator"",\n    ""TrainTestFeedableDataIterator""\n]\n\n\nclass DataIteratorBase(object):\n    """"""Base class for all data iterator classes to inherit. A data iterator\n    is a wrapper of :tf_main:`tf.data.Iterator <data/Iterator>`, and can\n    switch between and iterate through **multiple** datasets.\n\n    Args:\n        datasets: Datasets to iterates through. This can be:\n\n            - A single instance of :tf_main:`tf.data.Dataset <data/Dataset>` \\\n            or instance of subclass of :class:`~texar.tf.data.DataBase`.\n            - A `dict` that maps dataset name to \\\n            instance of :tf_main:`tf.data.Dataset <data/Dataset>` or \\\n            subclass of :class:`~texar.tf.data.DataBase`.\n            - A `list` of instances of subclasses of \\\n            :class:`texar.tf.data.DataBase`. The name of instances \\\n            (:attr:`texar.tf.data.DataBase.name`) must be unique.\n    """"""\n\n    def __init__(self, datasets):\n        self._default_dataset_name = \'data\'\n        if isinstance(datasets, (tf.data.Dataset, DataBase)):\n            datasets = {self._default_dataset_name: datasets}\n        elif isinstance(datasets, (list, tuple)):\n            if any(not isinstance(d, DataBase) for d in datasets):\n                raise ValueError(""`datasets` must be an non-empty list of ""\n                                 ""`tx.data.DataBase` instances."")\n            num_datasets = len(datasets)\n            datasets = {d.name: d for d in datasets}\n            if len(datasets) < num_datasets:\n                raise ValueError(""Names of datasets must be unique."")\n\n        _datasets = {}\n        for k, v in datasets.items():  # pylint: disable=invalid-name\n            _datasets[k] = v if isinstance(v, tf.data.Dataset) else v.dataset\n        self._datasets = _datasets\n\n        if len(self._datasets) <= 0:\n            raise ValueError(""`datasets` must not be empty."")\n\n    @property\n    def num_datasets(self):\n        """"""Number of datasets.\n        """"""\n        return len(self._datasets)\n\n    @property\n    def dataset_names(self):\n        """"""A list of dataset names.\n        """"""\n        return list(self._datasets.keys())\n\n\nclass DataIterator(DataIteratorBase):\n    """"""Data iterator that switches and iterates through multiple datasets.\n\n    This is a wrapper of TF reinitializble :tf_main:`iterator <data/Iterator>`.\n\n    Args:\n        datasets: Datasets to iterates through. This can be:\n\n            - A single instance of :tf_main:`tf.data.Dataset <data/Dataset>` \\\n            or instance of subclass of :class:`~texar.tf.data.DataBase`.\n            - A `dict` that maps dataset name to \\\n            instance of :tf_main:`tf.data.Dataset <data/Dataset>` or \\\n            subclass of :class:`~texar.tf.data.DataBase`.\n            - A `list` of instances of subclasses of \\\n            :class:`texar.tf.data.DataBase`. The name of instances \\\n            (:attr:`texar.tf.data.DataBase.name`) must be unique.\n\n    Example:\n\n        .. code-block:: python\n\n            train_data = MonoTextData(hparams_train)\n            test_data = MonoTextData(hparams_test)\n            iterator = DataIterator({\'train\': train_data, \'test\': test_data})\n            batch = iterator.get_next()\n\n            sess = tf.Session()\n\n            for _ in range(200): # Run 200 epochs of train/test\n                # Starts iterating through training data from the beginning\n                iterator.switch_to_dataset(sess, \'train\')\n                while True:\n                    try:\n                        train_batch_ = sess.run(batch)\n                    except tf.errors.OutOfRangeError:\n                        print(""End of training epoch."")\n                # Starts iterating through test data from the beginning\n                iterator.switch_to_dataset(sess, \'test\')\n                while True:\n                    try:\n                        test_batch_ = sess.run(batch)\n                    except tf.errors.OutOfRangeError:\n                        print(""End of test epoch."")\n    """"""\n\n    def __init__(self, datasets):\n        DataIteratorBase.__init__(self, datasets)\n\n        self._variable_scope = get_unique_named_variable_scope(\'data_iterator\')\n        with tf.variable_scope(self._variable_scope):\n            first_dataset = self._datasets[sorted(self.dataset_names)[0]]\n            self._iterator = tf.data.Iterator.from_structure(\n                first_dataset.output_types, first_dataset.output_shapes)\n            self._iterator_init_ops = {\n                name: self._iterator.make_initializer(d)\n                for name, d in self._datasets.items()\n            }\n\n    def switch_to_dataset(self, sess, dataset_name=None):\n        """"""Re-initializes the iterator of a given dataset and starts iterating\n        over the dataset (from the beginning).\n\n        Args:\n            sess: The current tf session.\n            dataset_name (optional): Name of the dataset. If not provided,\n                there must be only one Dataset.\n        """"""\n        if dataset_name is None:\n            if self.num_datasets > 1:\n                raise ValueError(""`dataset_name` is required if there are ""\n                                 ""more than one datasets."")\n            dataset_name = next(iter(self._datasets))\n        if dataset_name not in self._datasets:\n            raise ValueError(""Dataset not found: "", dataset_name)\n        sess.run(self._iterator_init_ops[dataset_name])\n\n    def get_next(self):\n        """"""Returns the next element of the activated dataset.\n        """"""\n        return self._iterator.get_next()\n\n\nclass TrainTestDataIterator(DataIterator):\n    """"""Data iterator that alternatives between train, val, and test datasets.\n\n    :attr:`train`, :attr:`val`, and :attr:`test` can be instance of\n    either :tf_main:`tf.data.Dataset <data/Dataset>` or subclass of\n    :class:`~texar.tf.data.DataBase`. At least one of them must be provided.\n\n    This is a wrapper of :class:`~texar.tf.data.DataIterator`.\n\n    Args:\n        train (optional): Training data.\n        val (optional): Validation data.\n        test (optional): Test data.\n\n    Example:\n\n        .. code-block:: python\n\n            train_data = MonoTextData(hparams_train)\n            val_data = MonoTextData(hparams_val)\n            iterator = TrainTestDataIterator(train=train_data, val=val_data)\n            batch = iterator.get_next()\n\n            sess = tf.Session()\n\n            for _ in range(200): # Run 200 epochs of train/val\n                # Starts iterating through training data from the beginning\n                iterator.switch_to_train_data(sess)\n                while True:\n                    try:\n                        train_batch_ = sess.run(batch)\n                    except tf.errors.OutOfRangeError:\n                        print(""End of training epoch."")\n                # Starts iterating through val data from the beginning\n                iterator.switch_to_val_dataset(sess)\n                while True:\n                    try:\n                        val_batch_ = sess.run(batch)\n                    except tf.errors.OutOfRangeError:\n                        print(""End of val epoch."")\n    """"""\n\n    def __init__(self, train=None, val=None, test=None):\n        dataset_dict = {}\n        self._train_name = \'train\'\n        self._val_name = \'val\'\n        self._test_name = \'test\'\n        if train is not None:\n            dataset_dict[self._train_name] = train\n        if val is not None:\n            dataset_dict[self._val_name] = val\n        if test is not None:\n            dataset_dict[self._test_name] = test\n        if len(dataset_dict) == 0:\n            raise ValueError(""At least one of `train`, `val`, and `test` ""\n                             ""must be provided."")\n\n        DataIterator.__init__(self, dataset_dict)\n\n    def switch_to_train_data(self, sess):\n        """"""Starts to iterate through training data (from the beginning).\n\n        Args:\n            sess: The current tf session.\n        """"""\n        if self._train_name not in self._datasets:\n            raise ValueError(""Training data not provided."")\n        self.switch_to_dataset(sess, self._train_name)\n\n    def switch_to_val_data(self, sess):\n        """"""Starts to iterate through val data (from the beginning).\n\n        Args:\n            sess: The current tf session.\n        """"""\n        if self._val_name not in self._datasets:\n            raise ValueError(""Val data not provided."")\n        self.switch_to_dataset(sess, self._val_name)\n\n    def switch_to_test_data(self, sess):\n        """"""Starts to iterate through test data (from the beginning).\n\n        Args:\n            sess: The current tf session.\n        """"""\n        if self._test_name not in self._datasets:\n            raise ValueError(""Test data not provided."")\n        self.switch_to_dataset(sess, self._test_name)\n\n\nclass FeedableDataIterator(DataIteratorBase):\n    """"""Data iterator that iterates through **multiple** datasets and switches\n    between datasets.\n\n    The iterator can switch to a dataset and resume from where we\n    left off last time we visited the dataset. This is a wrapper of TF\n    feedable :tf_main:`iterator <data/Iterator>`.\n\n    Args:\n        datasets: Datasets to iterates through. This can be:\n\n            - A single instance of :tf_main:`tf.data.Dataset <data/Dataset>` \\\n            or instance of subclass of :class:`~texar.tf.data.DataBase`.\n            - A `dict` that maps dataset name to \\\n            instance of :tf_main:`tf.data.Dataset <data/Dataset>` or \\\n            subclass of :class:`~texar.tf.data.DataBase`.\n            - A `list` of instances of subclasses of \\\n            :class:`texar.tf.data.DataBase`. The name of instances \\\n            (:attr:`texar.tf.data.DataBase.name`) must be unique.\n\n    Example:\n\n        .. code-block:: python\n\n            train_data = MonoTextData(hparams={\'num_epochs\': 200, ...})\n            test_data = MonoTextData(hparams_test)\n            iterator = FeedableDataIterator({\'train\': train_data,\n                                             \'test\': test_data})\n            batch = iterator.get_next()\n\n            sess = tf.Session()\n\n            def _eval_epoch(): # Iterate through test data for one epoch\n                # Initialize and start from beginning of test data\n                iterator.initialize_dataset(sess, \'test\')\n                while True:\n                    try:\n                        fetch_dict = { # Read from test data\n                            iterator.handle: Iterator.get_handle(sess, \'test\')\n                        }\n                        test_batch_ = sess.run(batch, feed_dict=feed_dict)\n                    except tf.errors.OutOfRangeError:\n                        print(""End of val epoch."")\n\n            # Initialize and start from beginning of training data\n            iterator.initialize_dataset(sess, \'train\')\n            step = 0\n            while True:\n                try:\n                    fetch_dict = { # Read from training data\n                        iterator.handle: Iterator.get_handle(sess, \'train\')\n                    }\n                    train_batch_ = sess.run(batch, fetch_dict=fetch_dict)\n\n                    step +=1\n                    if step % 200 == 0: # Evaluate periodically\n                        _eval_epoch()\n                except tf.errors.OutOfRangeError:\n                    print(""End of training."")\n    """"""\n\n    def __init__(self, datasets):\n        DataIteratorBase.__init__(self, datasets)\n\n        self._variable_scope = get_unique_named_variable_scope(\n            \'feedable_data_iterator\')\n        with tf.variable_scope(self._variable_scope):\n            self._handle = tf.placeholder(tf.string, shape=[], name=\'handle\')\n            first_dataset = self._datasets[sorted(self.dataset_names)[0]]\n            self._iterator = tf.data.Iterator.from_string_handle(\n                self._handle, first_dataset.output_types,\n                first_dataset.output_shapes)\n\n            self._dataset_iterators = {\n                name: dataset.make_initializable_iterator()\n                for name, dataset in self._datasets.items()\n            }\n\n    def get_handle(self, sess, dataset_name=None):\n        """"""Returns a dataset handle used to feed the\n        :attr:`handle` placeholder to fetch data from the dataset.\n\n        Args:\n            sess: The current tf session.\n            dataset_name (optional): Name of the dataset. If not provided,\n                there must be only one Dataset.\n\n        Returns:\n            A string handle to be fed to the :attr:`handle` placeholder.\n\n        Example:\n\n            .. code-block:: python\n\n                next_element = iterator.get_next()\n                train_handle = iterator.get_handle(sess, \'train\')\n                # Gets the next training element\n                ne_ = sess.run(next_element,\n                               feed_dict={iterator.handle: train_handle})\n        """"""\n        if dataset_name is None:\n            if self.num_datasets > 1:\n                raise ValueError(""`dataset_name` is required if there are ""\n                                 ""more than one datasets."")\n            dataset_name = next(iter(self._datasets))\n        if dataset_name not in self._datasets:\n            raise ValueError(""Dataset not found: "", dataset_name)\n        return sess.run(self._dataset_iterators[dataset_name].string_handle())\n\n    def restart_dataset(self, sess, dataset_name=None):\n        """"""Restarts datasets so that next iteration will fetch data from\n        the beginning of the datasets.\n\n        Args:\n            sess: The current tf session.\n            dataset_name (optional): A dataset name or a list of dataset names\n                that specifies which dataset(s) to restart. If `None`, all\n                datasets are restart.\n        """"""\n        self.initialize_dataset(sess, dataset_name)\n\n    def initialize_dataset(self, sess, dataset_name=None):\n        """"""Initializes datasets. A dataset must be initialized before being\n        used.\n\n        Args:\n            sess: The current tf session.\n            dataset_name (optional): A dataset name or a list of dataset names\n                that specifies which dataset(s) to initialize. If `None`, all\n                datasets are initialized.\n        """"""\n        if dataset_name is None:\n            dataset_name = self.dataset_names\n        if not isinstance(dataset_name, (tuple, list)):\n            dataset_name = [dataset_name]\n\n        for name in dataset_name:\n            sess.run(self._dataset_iterators[name].initializer)\n\n    def get_next(self):\n        """"""Returns the next element of the activated dataset.\n        """"""\n        return self._iterator.get_next()\n\n    @property\n    def handle(self):\n        """"""The handle placeholder that can be fed with a dataset handle to\n        fetch data from the dataset.\n        """"""\n        return self._handle\n\n\nclass TrainTestFeedableDataIterator(FeedableDataIterator):\n    """"""Feedable data iterator that alternatives between train, val, and test\n    datasets.\n\n    This is a wrapper of :class:`~texar.tf.data.FeedableDataIterator`.\n    The iterator can switch to a dataset and resume from where it was\n    left off when it was visited last time.\n\n    :attr:`train`, :attr:`val`, and :attr:`test` can be instance of\n    either :tf_main:`tf.data.Dataset <data/Dataset>` or subclass of\n    :class:`~texar.tf.data.DataBase`. At least one of them must be provided.\n\n    Args:\n        train (optional): Training data.\n        val (optional): Validation data.\n        test (optional): Test data.\n\n    Example:\n\n        .. code-block:: python\n\n            train_data = MonoTextData(hparams={\'num_epochs\': 200, ...})\n            test_data = MonoTextData(hparams_test)\n            iterator = TrainTestFeedableDataIterator(train=train_data,\n                                                     test=test_data)\n            batch = iterator.get_next()\n\n            sess = tf.Session()\n\n            def _eval_epoch(): # Iterate through test data for one epoch\n                # Initialize and start from beginning of test data\n                iterator.initialize_test_dataset(sess)\n                while True:\n                    try:\n                        fetch_dict = { # Read from test data\n                            iterator.handle: Iterator.get_test_handle(sess)\n                        }\n                        test_batch_ = sess.run(batch, feed_dict=feed_dict)\n                    except tf.errors.OutOfRangeError:\n                        print(""End of test epoch."")\n\n            # Initialize and start from beginning of training data\n            iterator.initialize_train_dataset(sess)\n            step = 0\n            while True:\n                try:\n                    fetch_dict = { # Read from training data\n                        iterator.handle: Iterator.get_train_handle(sess)\n                    }\n                    train_batch_ = sess.run(batch, fetch_dict=fetch_dict)\n\n                    step +=1\n                    if step % 200 == 0: # Evaluate periodically\n                        _eval_epoch()\n                except tf.errors.OutOfRangeError:\n                    print(""End of training."")\n    """"""\n\n    def __init__(self, train=None, val=None, test=None):\n        dataset_dict = {}\n        self._train_name = \'train\'\n        self._val_name = \'val\'\n        self._test_name = \'test\'\n        if train is not None:\n            dataset_dict[self._train_name] = train\n        if val is not None:\n            dataset_dict[self._val_name] = val\n        if test is not None:\n            dataset_dict[self._test_name] = test\n        if len(dataset_dict) == 0:\n            raise ValueError(""At least one of `train`, `val`, and `test` ""\n                             ""must be provided."")\n\n        FeedableDataIterator.__init__(self, dataset_dict)\n\n    def get_train_handle(self, sess):\n        """"""Returns the handle of the training dataset. The handle can be used\n        to feed the :attr:`handle` placeholder to fetch training data.\n\n        Args:\n            sess: The current tf session.\n\n        Returns:\n            A string handle to be fed to the :attr:`handle` placeholder.\n\n        Example:\n\n            .. code-block:: python\n\n                next_element = iterator.get_next()\n                train_handle = iterator.get_train_handle(sess)\n                # Gets the next training element\n                ne_ = sess.run(next_element,\n                               feed_dict={iterator.handle: train_handle})\n        """"""\n        if self._train_name not in self._datasets:\n            raise ValueError(""Training data not provided."")\n        return self.get_handle(sess, self._train_name)\n\n    def get_val_handle(self, sess):\n        """"""Returns the handle of the validation dataset. The handle can be used\n        to feed the :attr:`handle` placeholder to fetch validation data.\n\n        Args:\n            sess: The current tf session.\n\n        Returns:\n            A string handle to be fed to the :attr:`handle` placeholder.\n        """"""\n        if self._val_name not in self._datasets:\n            raise ValueError(""Val data not provided."")\n        return self.get_handle(sess, self._val_name)\n\n    def get_test_handle(self, sess):\n        """"""Returns the handle of the test dataset. The handle can be used\n        to feed the :attr:`handle` placeholder to fetch test data.\n\n        Args:\n            sess: The current tf session.\n\n        Returns:\n            A string handle to be fed to the :attr:`handle` placeholder.\n        """"""\n        if self._test_name not in self._datasets:\n            raise ValueError(""Test data not provided."")\n        return self.get_handle(sess, self._test_name)\n\n    def restart_train_dataset(self, sess):\n        """"""Restarts the training dataset so that next iteration will fetch\n        data from the beginning of the training dataset.\n\n        Args:\n            sess: The current tf session.\n        """"""\n        if self._train_name not in self._datasets:\n            raise ValueError(""Training data not provided."")\n        self.restart_dataset(sess, self._train_name)\n\n    def restart_val_dataset(self, sess):\n        """"""Restarts the validation dataset so that next iteration will fetch\n        data from the beginning of the validation dataset.\n\n        Args:\n            sess: The current tf session.\n        """"""\n        if self._val_name not in self._datasets:\n            raise ValueError(""Val data not provided."")\n        self.restart_dataset(sess, self._val_name)\n\n    def restart_test_dataset(self, sess):\n        """"""Restarts the test dataset so that next iteration will fetch\n        data from the beginning of the test dataset.\n\n        Args:\n            sess: The current tf session.\n        """"""\n        if self._test_name not in self._datasets:\n            raise ValueError(""Test data not provided."")\n        self.restart_dataset(sess, self._test_name)\n'"
texar/tf/data/data/dataset_utils.py,10,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious utilities specific to dataset processing.\n""""""\n\nimport six\n\nimport tensorflow as tf\n\nimport numpy as np\n\nfrom texar.tf.utils import utils\n\n# pylint: disable=invalid-name, too-many-arguments\n\n__all__ = [\n    ""_DataSpec"",\n    ""_connect_name"",\n    ""maybe_tuple"",\n    ""make_partial"",\n    ""make_chained_transformation"",\n    ""make_combined_transformation"",\n    ""random_shard_dataset"",\n]\n\n\nclass _DataSpec(object):\n    """"""Dataset specification. Used to pass necessary info to\n    user-defined tranformation functions.\n\n    Args:\n        dataset: Instance of :tf_main:`tf.data.Dataset <data/Dataset>`.\n        dataset_size (int): Number of data samples.\n        decoder: A (list of) data decoder.\n        vocab: A (list of) :class:`~texar.tf.data.Vocab` instance.\n        embeddidng: A (list of) :class:`~texar.tf.data.Embedding` instance.\n        **kwargs: Any remaining dataset-specific fields.\n    """"""\n    def __init__(self, dataset=None, dataset_size=None, decoder=None,\n                 vocab=None, embedding=None, **kwargs):\n        kwargs[\'dataset\'] = dataset\n        kwargs[\'dataset_size\'] = dataset_size\n        kwargs[\'decoder\'] = decoder\n        kwargs[\'vocab\'] = vocab\n        kwargs[\'embedding\'] = embedding\n        self.__dict__.update(kwargs)\n\n    def add_spec(self, **kwargs):\n        """"""Adds new field(s).\n        """"""\n        self.__dict__.update(kwargs)\n\n    def get_ith_data_spec(self, i):\n        """"""Returns an instance of :class:`_DataSpec` that contains the\n        `i`-th specifications.\n        """"""\n        kwargs = {}\n        for k, v in six.iteritems(self.__dict__):\n            kwargs[k] = v[i] if isinstance(v, (tuple, list)) else v\n        return _DataSpec(**kwargs)\n\n    def set_ith_data_spec(self, i, data_spec, total_count):\n        """"""Sets the `i`-th specification to respective values in\n        :attr:`data_spec`.\n        """"""\n        for k, v in six.iteritems(data_spec.__dict__):\n            if k in self.__dict__:\n                v_ = self.__dict__[k]\n                if isinstance(v_, (tuple, list)):\n                    v_[i] = v\n                else:\n                    new_v_ = [v_] * total_count\n                    new_v_[i] = v\n                    self.__dict__[k] = new_v_\n            else:\n                v_ = [None] * total_count\n                v_[i] = v\n                self.__dict__[k] = v_\n\n\ndef _make_length_filter_fn(length_name, max_length):\n    """"""Returns a predicate function which takes in data sample\n    and returns a bool indicating whether to filter by length.\n    """"""\n    def _filter_fn(data):\n        return data[length_name] <= max_length\n    return _filter_fn\n\n\ndef _make_smaller_batch_filter_fn(batch_size):\n    """"""Returns a predicate function which takes in a batched data\n    and returns a bool indicating whether the batch is of :attr:`batch_size`.\n    """"""\n    def _filter_fn(data):\n        if isinstance(data, (list, tuple)):\n            return _filter_fn(data[0])\n        elif isinstance(data, dict):\n            return _filter_fn(data[next(iter(data))])\n        else:\n            return tf.equal(tf.shape(data)[0], batch_size)\n\n    return _filter_fn\n\n\ndef _make_combined_filter_fn(filter_fns, mode=""and""):\n    """"""Returns a new predicate function that combines multiple\n    predicate functions with certain mode.\n\n    Returns `None` if all elements in :attr:`filter_fns` are `None`.\n\n    Args:\n        filter_fns (list): Filter functions to combine. `None` functions are\n            ignored.\n        mode (str): A mode from `{""and"", ""or""}`.\n    """"""\n    if not any(filter_fns):\n        return None\n\n    def _combined_fn(data):\n        outputs = []\n        for fn in filter_fns:\n            if fn:\n                outputs.append(fn(data))\n        if mode == ""and"":\n            return tf.reduce_all(outputs)\n        elif mode == ""or"":\n            return tf.reduce_any(outputs)\n        else:\n            raise ValueError(""Unknown mode: {}"".format(mode))\n    return _combined_fn\n\n\ndef _connect_name(lhs_name, rhs_name):\n    if not lhs_name:\n        return rhs_name\n    if not rhs_name:\n        return lhs_name\n    return ""{}_{}"".format(lhs_name, rhs_name)\n\n\ndef maybe_tuple(data):\n    """"""Returns `tuple(data)` if :attr:`data` contains more than 1 elements.\n\n    Used to wrap `map_func` inputs.\n    """"""\n    data = tuple(data)\n    data = data if len(data) > 1 else data[0]\n    return data\n\n\ndef make_partial(fn, *args, **kwargs):\n    """"""Returns a new function with single argument by freezing other arguments\n    of :attr:`fn`.\n    """"""\n    def _new_fn(data):\n        return fn(data, *args, **kwargs)\n    return _new_fn\n\n\ndef name_prefix_fn(name_prefix):\n    """"""Returns a function that append a prefix to field names.\n    """"""\n    def _prefix_fn(data):\n        transformed_data = {}\n        for name, value in six.iteritems(data):\n            new_name = _connect_name(name_prefix, name)\n            transformed_data[new_name] = value\n        return transformed_data\n\n    return _prefix_fn\n\n\ndef make_chained_transformation(tran_fns, *args, **kwargs):\n    """"""Returns a dataset transformation function that applies a list of\n    transformations sequentially.\n\n    Args:\n        tran_fns (list): A list of dataset transformation function.\n        *args: Extra arguments for each of the transformation function.\n        **kwargs: Extra keyword arguments for each of the transformation\n            function.\n\n    Returns:\n        A transformation function to be used in\n        :tf_main:`tf.data.Dataset.map <data/Dataset#map>`.\n    """"""\n    def _chained_fn(data):\n        for tran_fns_i in tran_fns:\n            data = tran_fns_i(data, *args, **kwargs)\n        return data\n\n    return _chained_fn\n\n\ndef make_combined_transformation(tran_fns, name_prefix=None, *args, **kwargs):\n    """"""Returns a dataset transformation function that applies\n    transformations to each component of the data.\n\n    The data to be transformed must be a tuple of the same length\n    of :attr:`tran_fns`.\n\n    Args:\n        tran_fns (list): A list of elements where each element is a\n            transformation function or a list of transformation functions.\n        name_prefix (list, optional): Prefix to the field names of each\n            component of the data, to prevent fields with the same name\n            in different components from overriding each other. If not `None`,\n            must be of the same length of :attr:`tran_fns`.\n        *args: Extra arguments for each of the transformation function.\n        **kwargs: Extra keyword arguments for each of the transformation\n            function.\n\n    Returns:\n        A transformation function to be used in\n        :tf_main:`tf.data.Dataset.map <data/Dataset#map>`.\n    """"""\n    if name_prefix and len(name_prefix) != len(tran_fns):\n        raise ValueError(""`name_prefix`, if provided, must be of the same ""\n                         ""length of `tran_fns`."")\n\n    def _combined_fn(data):\n        transformed_data = {}\n        for i, tran_fns_i in enumerate(tran_fns):\n            data_i = data[i]\n            # Process data_i\n            if not isinstance(tran_fns_i, (list, tuple)):\n                tran_fns_i = [tran_fns_i]\n            for tran_fns_ij in tran_fns_i:\n                data_i = tran_fns_ij(data_i, *args, **kwargs)\n            # Add to dict by appending name prefix\n            for name, value in six.iteritems(data_i):\n                new_name = name\n                if name_prefix:\n                    new_name = _connect_name(name_prefix[i], name)\n                if new_name in transformed_data:\n                    raise ValueError(\n                        ""Field name already exists: {}"".format(new_name))\n                transformed_data[new_name] = value\n        return transformed_data\n\n    return _combined_fn\n\n\ndef random_shard_dataset(dataset_size, shard_size, seed=None):\n    """"""Returns a dataset transformation function that randomly shards a\n    dataset.\n    """"""\n    num_shards = utils.ceildiv(dataset_size, shard_size)\n    boundaries = np.linspace(0, dataset_size, num=num_shards, endpoint=False,\n                             dtype=np.int64)  # pylint: disable=no-member\n\n    def _shard_fn(dataset):\n        sharded_dataset = (\n            tf.data.Dataset.from_tensor_slices(boundaries)\n            .shuffle(num_shards, seed=seed)\n            .flat_map(lambda lb: dataset.skip(lb).take(shard_size)))\n        return sharded_dataset\n\n    return _shard_fn\n'"
texar/tf/data/data/mono_text_data.py,25,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nMono text data class that define data reading, parsing, batching, and other\npreprocessing operations.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.dtypes import is_callable\nfrom texar.tf.data.data_utils import count_file_lines\nfrom texar.tf.data.data import dataset_utils as dsutils\nfrom texar.tf.data.data.text_data_base import TextDataBase\nfrom texar.tf.data.data_decoders import TextDataDecoder, VarUttTextDataDecoder\nfrom texar.tf.data.vocabulary import Vocab, SpecialTokens\nfrom texar.tf.data.embedding import Embedding\n\n# pylint: disable=invalid-name, arguments-differ, protected-access, no-member\n\n__all__ = [\n    ""_default_mono_text_dataset_hparams"",\n    ""MonoTextData""\n]\n\n\nclass _LengthFilterMode(object):\n    """"""Options of length filter mode.\n    """"""\n    TRUNC = ""truncate""\n    DISCARD = ""discard""\n\n\ndef _default_mono_text_dataset_hparams():\n    """"""Returns hyperparameters of a mono text dataset with default values.\n\n    See :meth:`texar.tf.MonoTextData.default_hparams` for details.\n    """"""\n    return {\n        ""files"": [],\n        ""compression_type"": None,\n        ""vocab_file"": """",\n        ""embedding_init"": Embedding.default_hparams(),\n        ""delimiter"": "" "",\n        ""max_seq_length"": None,\n        ""length_filter_mode"": ""truncate"",\n        ""pad_to_max_seq_length"": False,\n        ""bos_token"": SpecialTokens.BOS,\n        ""eos_token"": SpecialTokens.EOS,\n        ""other_transformations"": [],\n        ""variable_utterance"": False,\n        ""utterance_delimiter"": ""|||"",\n        ""max_utterance_cnt"": 5,\n        ""data_name"": None,\n        ""@no_typecheck"": [""files""]\n    }\n\n\nclass MonoTextData(TextDataBase):\n    """"""Text data processor that reads single set of text files. This can be\n    used for, e.g., language models, auto-encoders, etc.\n\n    Args:\n        hparams: A `dict` or instance of :class:`~texar.tf.HParams` containing\n            hyperparameters. See :meth:`default_hparams` for the defaults.\n\n    By default, the processor reads raw data files, performs tokenization,\n    batching and other pre-processing steps, and results in a TF Dataset\n    whose element is a python `dict` including three fields:\n\n        - ""text"":\n            A string Tensor of shape `[batch_size, max_time]` containing\n            the **raw** text toknes. `max_time` is the length of the longest\n            sequence in the batch.\n            Short sequences in the batch are padded with **empty string**.\n            BOS and EOS tokens are added as per\n            :attr:`hparams`. Out-of-vocabulary tokens are **NOT** replaced\n            with UNK.\n        - ""text_ids"":\n            An `int64` Tensor of shape `[batch_size, max_time]`\n            containing the token indexes.\n        - ""length"":\n            An `int` Tensor of shape `[batch_size]` containing the\n            length of each sequence in the batch (including BOS and\n            EOS if added).\n\n    If :attr:`\'variable_utterance\'` is set to `True` in :attr:`hparams`, the\n    resulting dataset has elements with four fields:\n\n        - ""text"":\n            A string Tensor of shape\n            `[batch_size, max_utterance, max_time]`, where *max_utterance* is\n            either the maximum number of utterances in each elements of the\n            batch, or :attr:`max_utterance_cnt` as specified in :attr:`hparams`.\n        - ""text_ids"":\n            An `int64` Tensor of shape\n            `[batch_size, max_utterance, max_time]` containing the token\n            indexes.\n        - ""length"":\n            An `int` Tensor of shape `[batch_size, max_utterance]`\n            containing the length of each sequence in the batch.\n        - ""utterance_cnt"":\n            An `int` Tensor of shape `[batch_size]` containing\n            the number of utterances of each element in the batch.\n\n    The above field names can be accessed through :attr:`text_name`,\n    :attr:`text_id_name`, :attr:`length_name`, and\n    :attr:`utterance_cnt_name`, respectively.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'dataset\': { \'files\': \'data.txt\', \'vocab_file\': \'vocab.txt\' },\n                \'batch_size\': 1\n            }\n            data = MonoTextData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #    \'text\': [[\'<BOS>\', \'example\', \'sequence\', \'<EOS>\']],\n            #    \'text_ids\': [[1, 5, 10, 2]],\n            #    \'length\': [4]\n            # }\n    """"""\n\n    def __init__(self, hparams):\n        TextDataBase.__init__(self, hparams)\n        with tf.name_scope(self.name, self.default_hparams()[""name""]):\n            self._make_data()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dicitionary of default hyperparameters:\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to text dataset\n                ""dataset"": {\n                    ""files"": [],\n                    ""compression_type"": None,\n                    ""vocab_file"": """",\n                    ""embedding_init"": {},\n                    ""delimiter"": "" "",\n                    ""max_seq_length"": None,\n                    ""length_filter_mode"": ""truncate"",\n                    ""pad_to_max_seq_length"": False,\n                    ""bos_token"": ""<BOS>""\n                    ""eos_token"": ""<EOS>""\n                    ""other_transformations"": [],\n                    ""variable_utterance"": False,\n                    ""utterance_delimiter"": ""|||"",\n                    ""max_utterance_cnt"": 5,\n                    ""data_name"": None,\n                }\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""mono_text_data"",\n                # (3) Bucketing\n                ""bucket_boundaries"": [],\n                ""bucket_batch_sizes"": None,\n                ""bucket_length_fn"": None,\n            }\n\n        Here:\n\n        1. For the hyperparameters in the :attr:`""dataset""` field:\n\n            ""files"": str or list\n                A (list of) text file path(s).\n\n                Each line contains a single text sequence.\n\n            ""compression_type"": str, optional\n                One of """" (no compression), ""ZLIB"", or ""GZIP"".\n\n            ""vocab_file"": str\n                Path to vocabulary file. Each line of the file should contain\n                one vocabulary token.\n\n                Used to create an instance of :class:`~texar.tf.data.Vocab`.\n\n            ""embedding_init"": dict\n                The hyperparameters for pre-trained embedding loading and\n                initialization.\n\n                The structure and default values are defined in\n                :meth:`texar.tf.data.Embedding.default_hparams`.\n\n            ""delimiter"": str\n                The delimiter to split each line of the text files into tokens.\n\n            ""max_seq_length"": int, optional\n                Maximum length of output sequences. Data samples exceeding the\n                length will be truncated or discarded according to\n                :attr:`""length_filter_mode""`. The length does not include\n                any added\n                :attr:`""bos_token""` or :attr:`""eos_token""`. If `None` (default),\n                no filtering is performed.\n\n            ""length_filter_mode"": str\n                Either ""truncate"" or ""discard"". If ""truncate"" (default),\n                tokens exceeding the :attr:`""max_seq_length""` will be truncated.\n                If ""discard"", data samples longer than the\n                :attr:`""max_seq_length""`\n                will be discarded.\n\n            ""pad_to_max_seq_length"": bool\n                If `True`, pad all data instances to length\n                :attr:`""max_seq_length""`.\n                Raises error if :attr:`""max_seq_length""` is not provided.\n\n            ""bos_token"": str\n                The Begin-Of-Sequence token prepended to each sequence.\n\n                Set to an empty string to avoid prepending.\n\n            ""eos_token"": str\n                The End-Of-Sequence token appended to each sequence.\n\n                Set to an empty string to avoid appending.\n\n            ""other_transformations"": list\n                A list of transformation functions or function names/paths to\n                further transform each single data instance.\n\n                (More documentations to be added.)\n\n            ""variable_utterance"": bool\n                If `True`, each line of the text file is considered to contain\n                multiple sequences (utterances) separated by\n                :attr:`""utterance_delimiter""`.\n\n                For example, in dialog data, each line can contain a series of\n                dialog history utterances. See the example in\n                `examples/hierarchical_dialog` for a use case.\n\n            ""utterance_delimiter"": str\n                The delimiter to split over utterance level. Should not be the\n                same with :attr:`""delimiter""`. Used only when\n                :attr:`""variable_utterance""``==True`.\n\n            ""max_utterance_cnt"": int\n                Maximally allowed number of utterances in a data instance.\n                Extra utterances are truncated out.\n\n            ""data_name"": str\n                Name of the dataset.\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.tf.data.DataBase.default_hparams` for details.\n\n        3. **Bucketing** is to group elements of the dataset together by length\n        and then pad and batch. (See more at\n        :tf_main:`bucket_by_sequence_length\n        <contrib/data/bucket_by_sequence_length>`). For bucketing\n        hyperparameters:\n\n            ""bucket_boundaries"": list\n                An int list containing the upper length boundaries of the\n                buckets.\n\n                Set to an empty list (default) to disable bucketing.\n\n            ""bucket_batch_sizes"": list\n                An int list containing batch size per bucket. Length should be\n                `len(bucket_boundaries) + 1`.\n\n                If `None`, every bucket whill have the same batch size specified\n                in :attr:`batch_size`.\n\n            ""bucket_length_fn"": str or callable\n                Function maps dataset element to `tf.int32` scalar, determines\n                the length of the element.\n\n                This can be a function, or the name or full module path to the\n                function. If function name is given, the function must be in the\n                :mod:`texar.tf.custom` module.\n\n                If `None` (default), length is determined by the number of\n                tokens (including BOS and EOS if added) of the element.\n\n        """"""\n        hparams = TextDataBase.default_hparams()\n        hparams[""name""] = ""mono_text_data""\n        hparams.update({\n            ""dataset"": _default_mono_text_dataset_hparams()\n        })\n        return hparams\n\n    @staticmethod\n    def make_vocab(hparams):\n        """"""Reads vocab file and returns an instance of\n        :class:`texar.tf.data.Vocab`.\n        """"""\n        bos_token = utils.default_str(\n            hparams[""bos_token""], SpecialTokens.BOS)\n        eos_token = utils.default_str(\n            hparams[""eos_token""], SpecialTokens.EOS)\n        vocab = Vocab(hparams[""vocab_file""],\n                      bos_token=bos_token, eos_token=eos_token)\n        return vocab\n\n    @staticmethod\n    def make_embedding(emb_hparams, token_to_id_map):\n        """"""Optionally loads embedding from file (if provided), and returns\n        an instance of :class:`texar.tf.data.Embedding`.\n        """"""\n        embedding = None\n        if emb_hparams[""file""] is not None and len(emb_hparams[""file""]) > 0:\n            embedding = Embedding(token_to_id_map, emb_hparams)\n        return embedding\n\n    @staticmethod\n    def _make_mono_text_dataset(dataset_hparams):\n        dataset = tf.data.TextLineDataset(\n            dataset_hparams[""files""],\n            compression_type=dataset_hparams[""compression_type""])\n        return dataset\n\n    @staticmethod\n    def _make_other_transformations(other_trans_hparams, data_spec):\n        """"""Creates a list of tranformation functions based on the\n        hyperparameters.\n\n        Args:\n            other_trans_hparams (list): A list of transformation functions,\n                names, or full paths.\n            data_spec: An instance of :class:`texar.tf.data._DataSpec` to\n                be passed to transformation functions.\n\n        Returns:\n            A list of transformation functions.\n        """"""\n        other_trans = []\n        for tran in other_trans_hparams:\n            if not is_callable(tran):\n                tran = utils.get_function(tran, [""texar.tf.custom""])\n            other_trans.append(dsutils.make_partial(tran, data_spec))\n        return other_trans\n\n    @staticmethod\n    def _make_processor(dataset_hparams, data_spec, chained=True,\n                        name_prefix=None):\n        # Create data decoder\n        max_seq_length = None\n        if dataset_hparams[""length_filter_mode""] == ""truncate"":\n            max_seq_length = dataset_hparams[""max_seq_length""]\n\n        if not dataset_hparams[""variable_utterance""]:\n            decoder = TextDataDecoder(\n                delimiter=dataset_hparams[""delimiter""],\n                bos_token=dataset_hparams[""bos_token""],\n                eos_token=dataset_hparams[""eos_token""],\n                max_seq_length=max_seq_length,\n                token_to_id_map=data_spec.vocab.token_to_id_map)\n        else:\n            decoder = VarUttTextDataDecoder(\n                sentence_delimiter=dataset_hparams[""utterance_delimiter""],\n                delimiter=dataset_hparams[""delimiter""],\n                bos_token=dataset_hparams[""bos_token""],\n                eos_token=dataset_hparams[""eos_token""],\n                max_seq_length=max_seq_length,\n                max_utterance_cnt=dataset_hparams[""max_utterance_cnt""],\n                token_to_id_map=data_spec.vocab.token_to_id_map)\n\n        # Create other transformations\n        data_spec.add_spec(decoder=decoder)\n        other_trans = MonoTextData._make_other_transformations(\n            dataset_hparams[""other_transformations""], data_spec)\n        if name_prefix:\n            other_trans.append(dsutils.name_prefix_fn(name_prefix))\n\n        data_spec.add_spec(name_prefix=name_prefix)\n\n        if chained:\n            chained_tran = dsutils.make_chained_transformation(\n                [decoder] + other_trans)\n            return chained_tran, data_spec\n        else:\n            return decoder, other_trans, data_spec\n\n    @staticmethod\n    def _make_length_filter(dataset_hparams, length_name, decoder):\n        filter_mode = dataset_hparams[""length_filter_mode""]\n        max_length = dataset_hparams[""max_seq_length""]\n        filter_fn = None\n        if filter_mode == _LengthFilterMode.DISCARD and max_length is not None:\n            max_length += decoder.added_length\n            filter_fn = dsutils._make_length_filter_fn(length_name,\n                                                       max_length)\n        return filter_fn\n\n    def _process_dataset(self, dataset, hparams, data_spec):\n        chained_tran, data_spec = self._make_processor(\n            hparams[""dataset""], data_spec,\n            name_prefix=hparams[""dataset""][""data_name""])\n        num_parallel_calls = hparams[""num_parallel_calls""]\n        dataset = dataset.map(\n            lambda *args: chained_tran(dsutils.maybe_tuple(args)),\n            num_parallel_calls=num_parallel_calls)\n\n        # Filters by length\n        length_name = dsutils._connect_name(\n            data_spec.name_prefix,\n            data_spec.decoder.length_tensor_name)\n        filter_fn = self._make_length_filter(\n            hparams[""dataset""], length_name, data_spec.decoder)\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        # Truncates data count\n        dataset = dataset.take(hparams[""max_dataset_size""])\n\n        return dataset, data_spec\n\n    def _make_bucket_length_fn(self):\n        length_fn = self._hparams.bucket_length_fn\n        if not length_fn:\n            length_fn = lambda x: x[self.length_name]\n        elif not is_callable(length_fn):\n            # pylint: disable=redefined-variable-type\n            length_fn = utils.get_function(length_fn, [""texar.tf.custom""])\n        return length_fn\n\n    @staticmethod\n    def _make_padded_text_and_id_shapes(dataset, dataset_hparams, decoder,\n                                        text_name, text_id_name):\n        max_length = dataset_hparams[\'max_seq_length\']\n        if max_length is None:\n            raise ValueError(""hparams \'max_seq_length\' must be specified ""\n                             ""when \'pad_to_max_seq_length\' is True."")\n        max_length += decoder.added_length\n\n        padded_shapes = dataset.output_shapes\n\n        def _get_new_shape(name):\n            dim = len(padded_shapes[name])\n            if not dataset_hparams[\'variable_utterance\']:\n                if dim != 1:\n                    raise ValueError(\n                        ""Unable to pad data \'%s\' to max seq length. Expected ""\n                        ""1D Tensor, but got %dD Tensor."" % (name, dim))\n                return tf.TensorShape(max_length)\n            else:\n                if dim != 2:\n                    raise ValueError(\n                        ""Unable to pad data \'%s\' to max seq length. Expected ""\n                        ""2D Tensor, but got %dD Tensor."" % (name, dim))\n                return tf.TensorShape([padded_shapes[name][0], max_length])\n\n        text_and_id_shapes = {}\n        if text_name in padded_shapes:\n            text_and_id_shapes[text_name] = _get_new_shape(text_name)\n        if text_id_name in padded_shapes:\n            text_and_id_shapes[text_id_name] = _get_new_shape(text_id_name)\n\n        return text_and_id_shapes\n\n    def _make_padded_shapes(self, dataset, decoder):\n        if not self._hparams.dataset.pad_to_max_seq_length:\n            return None\n\n        text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes(\n            dataset, self._hparams.dataset, decoder,\n            self.text_name, self.text_id_name)\n\n        padded_shapes = dataset.output_shapes\n        padded_shapes.update(text_and_id_shapes)\n\n        return padded_shapes\n\n    def _make_data(self):\n        dataset_hparams = self._hparams.dataset\n\n        # Create vocab and embedding\n        self._vocab = self.make_vocab(dataset_hparams)\n        self._embedding = self.make_embedding(\n            dataset_hparams[""embedding_init""], self._vocab.token_to_id_map_py)\n\n        # Create and shuffle dataset\n        dataset = self._make_mono_text_dataset(dataset_hparams)\n        dataset, dataset_size = self._shuffle_dataset(\n            dataset, self._hparams, self._hparams.dataset.files)\n        self._dataset_size = dataset_size\n\n        # Processing\n        data_spec = dsutils._DataSpec(dataset=dataset,\n                                      dataset_size=self._dataset_size,\n                                      vocab=self._vocab,\n                                      embedding=self._embedding)\n        dataset, data_spec = self._process_dataset(dataset, self._hparams,\n                                                   data_spec)\n        self._data_spec = data_spec\n        self._decoder = data_spec.decoder\n\n        # Batching\n        length_fn = self._make_bucket_length_fn()\n        padded_shapes = self._make_padded_shapes(dataset, self._decoder)\n        dataset = self._make_batch(\n            dataset, self._hparams, length_fn, padded_shapes)\n\n        # Prefetching\n        if self._hparams.prefetch_buffer_size > 0:\n            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)\n\n        self._dataset = dataset\n\n    def list_items(self):\n        """"""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return list(self._dataset.output_types.keys())\n\n    @property\n    def dataset(self):\n        """"""The dataset, an instance of\n        :tf_main:`TF dataset <data/TextLineDataset>`.\n        """"""\n        return self._dataset\n\n    def dataset_size(self):\n        """"""Returns the number of data instances in the data files.\n\n        Note that this is the total data count in the raw files, before any\n        filtering and truncation.\n        """"""\n        if not self._dataset_size:\n            # pylint: disable=attribute-defined-outside-init\n            self._dataset_size = count_file_lines(\n                self._hparams.dataset.files)\n        return self._dataset_size\n\n    @property\n    def vocab(self):\n        """"""The vocabulary, an instance of :class:`~texar.tf.data.Vocab`.\n        """"""\n        return self._vocab\n\n    @property\n    def embedding_init_value(self):\n        """"""The `Tensor` containing the embedding value loaded from file.\n        `None` if embedding is not specified.\n        """"""\n        if self._embedding is None:\n            return None\n        return self._embedding.word_vecs\n\n    @property\n    def text_name(self):\n        """"""The name of text tensor, ""text"" by default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix,\n            self._data_spec.decoder.text_tensor_name)\n        return name\n\n    @property\n    def length_name(self):\n        """"""The name of length tensor, ""length"" by default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix,\n            self._data_spec.decoder.length_tensor_name)\n        return name\n\n    @property\n    def text_id_name(self):\n        """"""The name of text index tensor, ""text_ids"" by default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix,\n            self._data_spec.decoder.text_id_tensor_name)\n        return name\n\n    @property\n    def utterance_cnt_name(self):\n        """"""The name of utterance count tensor, ""utterance_cnt"" by default.\n        """"""\n        if not self._hparams.dataset.variable_utterance:\n            raise ValueError(""`utterance_cnt_name` is not defined."")\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix,\n            self._data_spec.decoder.utterance_cnt_tensor_name)\n        return name\n'"
texar/tf/data/data/multi_aligned_data.py,28,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nData consisting of multiple aligned parts.\n""""""\n\nimport copy\n\nimport tensorflow as tf\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.dtypes import is_str, is_callable\nfrom texar.tf.data.data.text_data_base import TextDataBase\nfrom texar.tf.data.data.scalar_data import ScalarData\nfrom texar.tf.data.data.tfrecord_data import TFRecordData\nfrom texar.tf.data.data.mono_text_data import _default_mono_text_dataset_hparams\nfrom texar.tf.data.data.scalar_data import _default_scalar_dataset_hparams\nfrom texar.tf.data.data.tfrecord_data import _default_tfrecord_dataset_hparams\nfrom texar.tf.data.data.mono_text_data import MonoTextData\nfrom texar.tf.data.data_utils import count_file_lines\nfrom texar.tf.data.data import dataset_utils as dsutils\nfrom texar.tf.data.vocabulary import Vocab, SpecialTokens\nfrom texar.tf.data.embedding import Embedding\n\n# pylint: disable=invalid-name, arguments-differ\n# pylint: disable=protected-access, too-many-instance-attributes\n\n__all__ = [\n    ""_default_dataset_hparams"",\n    ""MultiAlignedData""\n]\n\n\nclass _DataTypes(object):  # pylint: disable=no-init, too-few-public-methods\n    """"""Enumeration of data types.\n    """"""\n    TEXT = ""text""\n    INT = ""int""\n    FLOAT = ""float""\n    TF_RECORD = ""tf_record""\n\n\ndef _is_text_data(data_type):\n    return data_type == _DataTypes.TEXT\n\n\ndef _is_scalar_data(data_type):\n    return data_type == _DataTypes.INT or data_type == _DataTypes.FLOAT\n\n\ndef _is_tfrecord_data(data_type):\n    return data_type == _DataTypes.TF_RECORD\n\n\ndef _default_dataset_hparams(data_type=None):\n    """"""Returns hyperparameters of a dataset with default values.\n\n    See :meth:`texar.tf.data.MultiAlignedData.default_hparams` for details.\n    """"""\n    if not data_type or _is_text_data(data_type):\n        hparams = _default_mono_text_dataset_hparams()\n        hparams.update({\n            ""data_type"": _DataTypes.TEXT,\n            ""vocab_share_with"": None,\n            ""embedding_init_share_with"": None,\n            ""processing_share_with"": None,\n        })\n    elif _is_scalar_data(data_type):\n        hparams = _default_scalar_dataset_hparams()\n    elif _is_tfrecord_data(data_type):\n        hparams = _default_tfrecord_dataset_hparams()\n        hparams.update({\n            ""data_type"": _DataTypes.TF_RECORD,\n        })\n    return hparams\n\n\nclass MultiAlignedData(TextDataBase):\n    """"""Data consisting of multiple aligned parts.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n\n    The processor can read any number of parallel fields as specified in\n    the ""datasets"" list of :attr:`hparams`, and result in a TF Dataset whose\n    element is a python `dict` containing data fields from each of the\n    specified datasets. Fields from a text dataset or TFRecord dataset have\n    names prefixed by its ""data_name"". Fields from a scalar dataset are\n    specified by its ""data_name"".\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'datasets\': [\n                    {\'files\': \'a.txt\', \'vocab_file\': \'v.a\', \'data_name\': \'x\'},\n                    {\'files\': \'b.txt\', \'vocab_file\': \'v.b\', \'data_name\': \'y\'},\n                    {\'files\': \'c.txt\', \'data_type\': \'int\', \'data_name\': \'z\'}\n                ]\n                \'batch_size\': 1\n            }\n            data = MultiAlignedData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #    \'x_text\': [[\'<BOS>\', \'x\', \'sequence\', \'<EOS>\']],\n            #    \'x_text_ids\': [[\'1\', \'5\', \'10\', \'2\']],\n            #    \'x_length\': [4]\n            #    \'y_text\': [[\'<BOS>\', \'y\', \'sequence\', \'1\', \'<EOS>\']],\n            #    \'y_text_ids\': [[\'1\', \'6\', \'10\', \'20\', \'2\']],\n            #    \'y_length\': [5],\n            #    \'z\': [1000],\n            # }\n            ...\n\n            hparams={\n                \'datasets\': [\n                    {\'files\': \'d.txt\', \'vocab_file\': \'v.d\', \'data_name\': \'m\'},\n                    {\n                        \'files\': \'d.tfrecord\',\n                        \'data_type\': \'tf_record\',\n                        ""feature_original_types"": {\n                            \'image\': [\'tf.string\', \'FixedLenFeature\']\n                        },\n                        \'image_options\': {\n                            \'image_feature_name\': \'image\',\n                            \'resize_height\': 512,\n                            \'resize_width\': 512,\n                        },\n                        \'data_name\': \'t\',\n                    }\n                ]\n                \'batch_size\': 1\n            }\n            data = MultiAlignedData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #    \'x_text\': [[\'<BOS>\', \'NewYork\', \'City\', \'Map\', \'<EOS>\']],\n            #    \'x_text_ids\': [[\'1\', \'100\', \'80\', \'65\', \'2\']],\n            #    \'x_length\': [5],\n            #\n            #    # ""t_image"" is a list of a ""numpy.ndarray"" image\n            #    # in this example. Its width equals to 512 and\n            #    # its height equals to 512.\n            #    \'t_image\': [...]\n            # }\n\n    """"""\n    def __init__(self, hparams):\n        TextDataBase.__init__(self, hparams)\n        # Defaultizes hparams of each dataset\n        datasets_hparams = self._hparams.datasets\n        defaultized_datasets_hparams = []\n        for ds_hpms in datasets_hparams:\n            data_type = ds_hpms.get(""data_type"", None)\n            defaultized_ds_hpms = HParams(ds_hpms,\n                                          _default_dataset_hparams(data_type))\n            defaultized_datasets_hparams.append(defaultized_ds_hpms)\n        self._hparams.datasets = defaultized_datasets_hparams\n\n        with tf.name_scope(self.name, self.default_hparams()[""name""]):\n            self._make_data()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dicitionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to text dataset\n                ""datasets"": []\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""multi_aligned_data"",\n            }\n\n        Here:\n\n        1. ""datasets"" is a list of `dict` each of which specifies a\n        dataset which can be text, scalar or TFRecord. The\n        :attr:`""data_name""` field of each dataset is used as the name\n        prefix of the data fields from the respective dataset. The\n        :attr:`""data_name""` field of each dataset should not be the same.\n\n            - For scalar dataset, the allowed hyperparameters and default \\\n            values are the same as the ""dataset"" field of \\\n            :meth:`texar.tf.data.ScalarData.default_hparams`. Note that \\\n            :attr:`""data_type""` must be explicily specified \\\n            (either ""int"" or ""float""). \\\n\n            - For TFRecord dataset, the allowed hyperparameters and default \\\n            values are the same as the ""dataset"" field of \\\n            :meth:`texar.tf.data.TFRecordData.default_hparams`. Note that \\\n            :attr:`""data_type""` must be explicily specified \\\n            (tf_record""). \\\n\n            - For text dataset, the allowed hyperparameters and default values\\\n            are the same as the ""dataset"" filed of \\\n            :meth:`texar.tf.data.MonoTextData.default_hparams`, with several \\\n            extra hyperparameters:\n\n                ""data_type"": str\n                    The type of the dataset, one of {""text"", ""int"", ""float"",\n                    ""tf_record""}. If set to ""int"" or ""float"", the dataset is\n                    considered to be a scalar dataset. If set to ""tf_record"",\n                    the dataset is considered to be a TFRecord dataset.\n                    If not specified or set to ""text"", the dataset is\n                    considered to be a text dataset.\n\n                ""vocab_share_with"": int, optional\n                    Share the vocabulary of a preceding text dataset with the\n                    specified index in the list (starting from 0). The\n                    specified dataset must be a text dataset, and must have\n                    an index smaller than the current dataset.\n\n                    If specified, the vocab file of current dataset is ignored.\n                    Default is `None` which disables the vocab sharing.\n\n                ""embedding_init_share_with"": int, optional\n                    Share the embedding initial value of a preceding text\n                    dataset with the specified index in the list (starting\n                    from 0).\n                    The specified dataset must be a text dataset, and must have\n                    an index smaller than the current dataset.\n\n                    If specified, the :attr:`""embedding_init""` field of\n                    the current dataset is ignored. Default is `None` which\n                    disables the initial value sharing.\n\n                ""processing_share_with"": int, optional\n                    Share the processing configurations of a preceding text\n                    dataset with the specified index in the list (starting\n                    from 0).\n                    The specified dataset must be a text dataset, and must have\n                    an index smaller than the current dataset.\n\n                    If specified, relevant field of the current dataset are\n                    ignored, including ""delimiter"", ""bos_token"", ""eos_token"",\n                    and ""other_transformations"". Default is `None` which\n                    disables the processing sharing.\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.tf.data.DataBase.default_hparams` for details.\n        """"""\n        hparams = TextDataBase.default_hparams()\n        hparams[""name""] = ""multi_aligned_data""\n        hparams[""datasets""] = []\n        return hparams\n\n    @staticmethod\n    def _raise_sharing_error(err_data, shr_data, hparam_name):\n        raise ValueError(\n            ""Must only share specifications with a preceding dataset. ""\n            ""Dataset %d has \'%s=%d\'"" % (err_data, hparam_name, shr_data))\n\n    @staticmethod\n    def make_vocab(hparams):\n        """"""Makes a list of vocabs based on the hparams.\n\n        Args:\n            hparams (list): A list of dataset hyperparameters.\n\n        Returns:\n            A list of :class:`texar.tf.data.Vocab` instances. Some instances\n            may be the same objects if they are set to be shared and have\n            the same other configs.\n        """"""\n        if not isinstance(hparams, (list, tuple)):\n            hparams = [hparams]\n\n        vocabs = []\n        for i, hparams_i in enumerate(hparams):\n            if not _is_text_data(hparams_i[""data_type""]):\n                vocabs.append(None)\n                continue\n\n            proc_shr = hparams_i[""processing_share_with""]\n            if proc_shr is not None:\n                bos_token = hparams[proc_shr][""bos_token""]\n                eos_token = hparams[proc_shr][""eos_token""]\n            else:\n                bos_token = hparams_i[""bos_token""]\n                eos_token = hparams_i[""eos_token""]\n            bos_token = utils.default_str(\n                bos_token, SpecialTokens.BOS)\n            eos_token = utils.default_str(\n                eos_token, SpecialTokens.EOS)\n\n            vocab_shr = hparams_i[""vocab_share_with""]\n            if vocab_shr is not None:\n                if vocab_shr >= i:\n                    MultiAlignedData._raise_sharing_error(\n                        i, vocab_shr, ""vocab_share_with"")\n                if not vocabs[vocab_shr]:\n                    raise ValueError(""Cannot share vocab with dataset %d which ""\n                                     ""does not have a vocab."" % vocab_shr)\n                if bos_token == vocabs[vocab_shr].bos_token and \\\n                        eos_token == vocabs[vocab_shr].eos_token:\n                    vocab = vocabs[vocab_shr]\n                else:\n                    vocab = Vocab(hparams[vocab_shr][""vocab_file""],\n                                  bos_token=bos_token,\n                                  eos_token=eos_token)\n            else:\n                vocab = Vocab(hparams_i[""vocab_file""],\n                              bos_token=bos_token,\n                              eos_token=eos_token)\n            vocabs.append(vocab)\n\n        return vocabs\n\n    @staticmethod\n    def make_embedding(hparams, vocabs):\n        """"""Optionally loads embeddings from files (if provided), and\n        returns respective :class:`texar.tf.data.Embedding` instances.\n        """"""\n        if not isinstance(hparams, (list, tuple)):\n            hparams = [hparams]\n\n        embs = []\n        for i, hparams_i in enumerate(hparams):\n            if not _is_text_data(hparams_i[""data_type""]):\n                embs.append(None)\n                continue\n\n            emb_shr = hparams_i[""embedding_init_share_with""]\n            if emb_shr is not None:\n                if emb_shr >= i:\n                    MultiAlignedData._raise_sharing_error(\n                        i, emb_shr, ""embedding_init_share_with"")\n                if not embs[emb_shr]:\n                    raise ValueError(""Cannot share embedding with dataset %d ""\n                                     ""which does not have an embedding."" %\n                                     emb_shr)\n                if emb_shr != hparams_i[""vocab_share_with""]:\n                    raise ValueError(""\'embedding_init_share_with\' != ""\n                                     ""vocab_share_with. embedding_init can ""\n                                     ""be shared only when vocab is shared."")\n                emb = embs[emb_shr]\n            else:\n                emb = None\n                emb_file = hparams_i[""embedding_init""][""file""]\n                if emb_file and emb_file != """":\n                    emb = Embedding(vocabs[i].token_to_id_map_py,\n                                    hparams_i[""embedding_init""])\n            embs.append(emb)\n\n        return embs\n\n    def _make_dataset(self):\n        datasets = []\n        for _, hparams_i in enumerate(self._hparams.datasets):\n            dtype = hparams_i.data_type\n            if _is_text_data(dtype) or _is_scalar_data(dtype):\n                dataset = tf.data.TextLineDataset(\n                    hparams_i.files,\n                    compression_type=hparams_i.compression_type)\n                datasets.append(dataset)\n            elif _is_tfrecord_data(dtype):\n                dataset = tf.data.TFRecordDataset(filenames=hparams_i.files)\n                num_shards = hparams_i.num_shards\n                shard_id = hparams_i.shard_id\n                if num_shards is not None and shard_id is not None:\n                    dataset = dataset.shard(num_shards, shard_id)\n                datasets.append(dataset)\n            else:\n                raise ValueError(""Unknown data type: %s"" % hparams_i.data_type)\n        return tf.data.Dataset.zip(tuple(datasets))\n\n    # @staticmethod\n    # def _get_name_prefix(dataset_hparams):\n    #    def _dtype_conflict(dtype_1, dtype_2):\n    #        conflict = ((dtype_1 == dtype_2) or\n    #                    (dtype_1 in {_DataTypes.INT, _DataTypes.FLOAT} and\n    #                     dtype_2 in {_DataTypes.INT, _DataTypes.FLOAT}))\n    #        return conflict\n\n    #    name_prefix = [hpms[""data_name""] for hpms in dataset_hparams]\n    #    name_prefix_dict = {}\n    #    for i, np in enumerate(name_prefix):\n    #        ids = name_prefix_dict.get(np, [])\n    #        for j in ids:\n    #            if _dtype_conflict(dataset_hparams[j][""data_type""],\n    #                               dataset_hparams[i][""data_type""]):\n    #                raise ValueError(\n    #                    ""\'data_name\' of the datasets with compatible ""\n    #                    ""data_types cannot be the same: %d-th dataset and ""\n    #                    ""%d-th dataset have the same name \'%s\'"" %\n    #                    (i, j, name_prefix[i]))\n    #        ids.append(i)\n    #        name_prefix_dict[np] = ids\n    #    return name_prefix\n\n    @staticmethod\n    def _get_name_prefix(dataset_hparams):\n        name_prefix = [hpms[""data_name""] for hpms in dataset_hparams]\n        for i in range(1, len(name_prefix)):\n            if name_prefix[i] in name_prefix[:i - 1]:\n                raise ValueError(""Data name duplicated: %s"" % name_prefix[i])\n        return name_prefix\n\n    @staticmethod\n    def _make_processor(dataset_hparams, data_spec, name_prefix):\n        processors = []\n        for i, hparams_i in enumerate(dataset_hparams):\n            data_spec_i = data_spec.get_ith_data_spec(i)\n\n            data_type = hparams_i[""data_type""]\n            if _is_text_data(data_type):\n                tgt_proc_hparams = hparams_i\n                proc_shr = hparams_i[""processing_share_with""]\n                if proc_shr is not None:\n                    tgt_proc_hparams = copy.copy(dataset_hparams[proc_shr])\n                    try:\n                        tgt_proc_hparams[""variable_utterance""] = \\\n                                hparams_i[""variable_utterance""]\n                    except TypeError:\n                        tgt_proc_hparams.variable_utterance = \\\n                                hparams_i[""variable_utterance""]\n\n                processor, data_spec_i = MonoTextData._make_processor(\n                    tgt_proc_hparams, data_spec_i)\n            elif _is_scalar_data(data_type):\n                processor, data_spec_i = ScalarData._make_processor(\n                    hparams_i, data_spec_i, name_prefix=\'\')\n            elif _is_tfrecord_data(data_type):\n                processor, data_spec_i = TFRecordData._make_processor(\n                    hparams_i, data_spec_i, name_prefix=\'\')\n            else:\n                raise ValueError(""Unsupported data type: %s"" % data_type)\n\n            processors.append(processor)\n            data_spec.set_ith_data_spec(i, data_spec_i, len(dataset_hparams))\n\n        tran_fn = dsutils.make_combined_transformation(\n            processors, name_prefix=name_prefix)\n\n        data_spec.add_spec(name_prefix=name_prefix)\n\n        return tran_fn, data_spec\n\n    @staticmethod\n    def _make_length_filter(dataset_hparams, length_name, decoder):\n        filter_fns = []\n        for i, hpms in enumerate(dataset_hparams):\n            if not _is_text_data(hpms[""data_type""]):\n                filter_fn = None\n            else:\n                filter_fn = MonoTextData._make_length_filter(\n                    hpms, length_name[i], decoder[i])\n            filter_fns.append(filter_fn)\n        combined_filter_fn = dsutils._make_combined_filter_fn(filter_fns)\n        return combined_filter_fn\n\n    def _process_dataset(self, dataset, hparams, data_spec):\n        name_prefix = self._get_name_prefix(hparams[""datasets""])\n        # pylint: disable=attribute-defined-outside-init\n        self._name_to_id = {v: k for k, v in enumerate(name_prefix)}\n\n        tran_fn, data_spec = self._make_processor(\n            hparams[""datasets""], data_spec, name_prefix)\n\n        num_parallel_calls = hparams[""num_parallel_calls""]\n        dataset = dataset.map(\n            lambda *args: tran_fn(dsutils.maybe_tuple(args)),\n            num_parallel_calls=num_parallel_calls)\n\n        # Filters by length\n        def _get_length_name(i):\n            if not _is_text_data(hparams[""datasets""][i][""data_type""]):\n                return None\n            name = dsutils._connect_name(\n                data_spec.name_prefix[i],\n                data_spec.decoder[i].length_tensor_name)\n            return name\n        filter_fn = self._make_length_filter(\n            hparams[""datasets""],\n            [_get_length_name(i) for i in range(len(hparams[""datasets""]))],\n            data_spec.decoder)\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        # Truncates data count\n        dataset = dataset.take(hparams[""max_dataset_size""])\n\n        return dataset, data_spec\n\n    def _make_bucket_length_fn(self):\n        length_fn = self._hparams.bucket_length_fn\n        if not length_fn:\n            # Uses the length of the first text data\n            i = -1\n            for i, hparams_i in enumerate(self._hparams.datasets):\n                if _is_text_data(hparams_i[""data_type""]):\n                    break\n            if i < 0:\n                raise ValueError(""Undefined `length_fn`."")\n            length_fn = lambda x: x[self.length_name(i)]\n        elif not is_callable(length_fn):\n            # pylint: disable=redefined-variable-type\n            length_fn = utils.get_function(length_fn, [""texar.tf.custom""])\n        return length_fn\n\n    def _make_padded_shapes(self, dataset, decoders):\n        padded_shapes = dataset.output_shapes\n        for i, hparams_i in enumerate(self._hparams.datasets):\n            if not _is_text_data(hparams_i[""data_type""]):\n                continue\n            if not hparams_i[""pad_to_max_seq_length""]:\n                continue\n            text_and_id_shapes = MonoTextData._make_padded_text_and_id_shapes(\n                dataset, hparams_i, decoders[i],\n                self.text_name(i), self.text_id_name(i))\n\n            padded_shapes.update(text_and_id_shapes)\n\n        return padded_shapes\n\n    def _make_data(self):\n        self._vocab = self.make_vocab(self._hparams.datasets)\n        self._embedding = self.make_embedding(self._hparams.datasets,\n                                              self._vocab)\n\n        # Create dataset\n        dataset = self._make_dataset()\n        dataset, dataset_size = self._shuffle_dataset(\n            dataset, self._hparams, self._hparams.datasets[0].files)\n        self._dataset_size = dataset_size\n\n        # Processing\n        data_spec = dsutils._DataSpec(dataset=dataset,\n                                      dataset_size=self._dataset_size,\n                                      vocab=self._vocab,\n                                      embedding=self._embedding)\n        dataset, data_spec = self._process_dataset(\n            dataset, self._hparams, data_spec)\n        self._data_spec = data_spec\n        self._decoder = data_spec.decoder\n\n        # Batching\n        length_fn = self._make_bucket_length_fn()\n        padded_shapes = self._make_padded_shapes(dataset, self._decoder)\n        dataset = self._make_batch(\n            dataset, self._hparams, length_fn, padded_shapes)\n\n        # Prefetching\n        if self._hparams.prefetch_buffer_size > 0:\n            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)\n\n        self._dataset = dataset\n\n    def list_items(self):\n        """"""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return list(self._dataset.output_types.keys())\n\n    @property\n    def dataset(self):\n        """"""The dataset.\n        """"""\n        return self._dataset\n\n    def dataset_size(self):\n        """"""Returns the number of data instances in the dataset.\n\n        Note that this is the total data count in the raw files, before any\n        filtering and truncation.\n        """"""\n        if not self._dataset_size:\n            # pylint: disable=attribute-defined-outside-init\n            self._dataset_size = count_file_lines(\n                self._hparams.datasets[0].files)\n        return self._dataset_size\n\n    def _maybe_name_to_id(self, name_or_id):\n        if is_str(name_or_id):\n            if name_or_id not in self._name_to_id:\n                raise ValueError(""Unknown data name: {}"".format(name_or_id))\n            return self._name_to_id[name_or_id]\n        return name_or_id\n\n    def vocab(self, name_or_id):\n        """"""Returns the :class:`~texar.tf.data.Vocab` of text dataset by its name\n        or id. `None` if the dataset is not of text type.\n\n        Args:\n            name_or_id (str or int): Data name or the index of text dataset.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        return self._vocab[i]\n\n    def embedding_init_value(self, name_or_id):\n        """"""Returns the `Tensor` of embedding init value of the\n        dataset by its name or id. `None` if the dataset is not of text type.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        return self._embedding[i]\n\n    def text_name(self, name_or_id):\n        """"""The name of text tensor of text dataset by its name or id. If the\n        dataaet is not of text type, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[i],\n            self._data_spec.decoder[i].text_tensor_name)\n        return name\n\n    def length_name(self, name_or_id):\n        """"""The name of length tensor of text dataset by its name or id. If the\n        dataset is not of text type, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[i],\n            self._data_spec.decoder[i].length_tensor_name)\n        return name\n\n    def text_id_name(self, name_or_id):\n        """"""The name of length tensor of text dataset by its name or id. If the\n        dataset is not of text type, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[i],\n            self._data_spec.decoder[i].text_id_tensor_name)\n        return name\n\n    def utterance_cnt_name(self, name_or_id):\n        """"""The name of utterance count tensor of text dataset by its name or id.\n        If the dataset is not variable utterance text data, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_text_data(self._hparams.datasets[i][""data_type""]) or \\\n                not self._hparams.datasets[i][""variable_utterance""]:\n            return None\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[i],\n            self._data_spec.decoder[i].utterance_cnt_tensor_name)\n        return name\n\n    @property\n    def data_name(self, name_or_id):\n        """"""The name of the data tensor of scalar dataset by its name or id..\n        If the dataset is not a scalar data, returns `None`.\n        """"""\n        i = self._maybe_name_to_id(name_or_id)\n        if not _is_scalar_data(self._hparams.datasets[i][""data_type""]):\n            return None\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[i],\n            self._data_spec.decoder[i].data_tensor_name)\n        return name\n'"
texar/tf/data/data/paired_text_data.py,25,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPaired text data that consists of source text and target text.\n""""""\n\nimport copy\n\nimport tensorflow as tf\n\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.dtypes import is_callable\nfrom texar.tf.data.data.mono_text_data import _default_mono_text_dataset_hparams\nfrom texar.tf.data.data.text_data_base import TextDataBase\nfrom texar.tf.data.data.mono_text_data import MonoTextData\nfrom texar.tf.data.data_utils import count_file_lines\nfrom texar.tf.data.data import dataset_utils as dsutils\nfrom texar.tf.data.vocabulary import Vocab, SpecialTokens\nfrom texar.tf.data.embedding import Embedding\n\n# pylint: disable=invalid-name, arguments-differ, not-context-manager\n# pylint: disable=protected-access, too-many-arguments\n\n__all__ = [\n    ""_default_paired_text_dataset_hparams"",\n    ""PairedTextData""\n]\n\n\ndef _default_paired_text_dataset_hparams():\n    """"""Returns hyperparameters of a paired text dataset with default values.\n\n    See :meth:`texar.tf.data.PairedTextData.default_hparams` for details.\n    """"""\n    source_hparams = _default_mono_text_dataset_hparams()\n    source_hparams[""bos_token""] = None\n    source_hparams[""data_name""] = ""source""\n    target_hparams = _default_mono_text_dataset_hparams()\n    target_hparams.update(\n        {\n            ""vocab_share"": False,\n            ""embedding_init_share"": False,\n            ""processing_share"": False,\n            ""data_name"": ""target""\n        }\n    )\n    return {\n        ""source_dataset"": source_hparams,\n        ""target_dataset"": target_hparams\n    }\n\n\n# pylint: disable=too-many-instance-attributes, too-many-public-methods\nclass PairedTextData(TextDataBase):\n    """"""Text data processor that reads parallel source and target text.\n    This can be used in, e.g., seq2seq models.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n\n    By default, the processor reads raw data files, performs tokenization,\n    batching and other pre-processing steps, and results in a TF Dataset\n    whose element is a python `dict` including six fields:\n\n        - ""source_text"":\n            A string Tensor of shape `[batch_size, max_time]` containing\n            the **raw** text toknes of source sequences. `max_time` is the\n            length of the longest sequence in the batch.\n            Short sequences in the batch are padded with **empty string**.\n            By default only EOS token is appended to each sequence.\n            Out-of-vocabulary tokens are **NOT** replaced with UNK.\n        - ""source_text_ids"":\n            An `int64` Tensor of shape `[batch_size, max_time]`\n            containing the token indexes of source sequences.\n        - ""source_length"":\n            An `int` Tensor of shape `[batch_size]` containing the\n            length of each source sequence in the batch (including BOS and/or\n            EOS if added).\n        - ""target_text"":\n            A string Tensor as ""source_text"" but for target sequences. By\n            default both BOS and EOS are added.\n        - ""target_text_ids"":\n            An `int64` Tensor as ""source_text_ids"" but for target sequences.\n        - ""target_length"":\n            An `int` Tensor of shape `[batch_size]` as ""source_length"" but for\n            target sequences.\n\n    If :attr:`\'variable_utterance\'` is set to `True` in :attr:`\'source_dataset\'`\n    and/or :attr:`\'target_dataset\'` of :attr:`hparams`, the corresponding\n    fields ""source_*"" and/or ""target_*"" are respectively changed to contain\n    variable utterance text data, as in :class:`~texar.tf.data.MonoTextData`.\n\n    The above field names can be accessed through :attr:`source_text_name`,\n    :attr:`source_text_id_name`, :attr:`source_length_name`,\n    :attr:`source_utterance_cnt_name`, and those prefixed with `target_`,\n    respectively.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'source_dataset\': {\'files\': \'s\', \'vocab_file\': \'vs\'},\n                \'target_dataset\': {\'files\': [\'t1\', \'t2\'], \'vocab_file\': \'vt\'},\n                \'batch_size\': 1\n            }\n            data = PairedTextData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #    \'source_text\': [[\'source\', \'sequence\', \'<EOS>\']],\n            #    \'source_text_ids\': [[5, 10, 2]],\n            #    \'source_length\': [3]\n            #    \'target_text\': [[\'<BOS>\', \'target\', \'sequence\', \'1\', \'<EOS>\']],\n            #    \'target_text_ids\': [[1, 6, 10, 20, 2]],\n            #    \'target_length\': [5]\n            # }\n\n    """"""\n    def __init__(self, hparams):\n        TextDataBase.__init__(self, hparams)\n        with tf.name_scope(self.name, self.default_hparams()[""name""]):\n            self._make_data()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dicitionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to text dataset\n                ""source_dataset"": {\n                    ""files"": [],\n                    ""compression_type"": None,\n                    ""vocab_file"": """",\n                    ""embedding_init"": {},\n                    ""delimiter"": "" "",\n                    ""max_seq_length"": None,\n                    ""length_filter_mode"": ""truncate"",\n                    ""pad_to_max_seq_length"": False,\n                    ""bos_token"": None,\n                    ""eos_token"": ""<EOS>"",\n                    ""other_transformations"": [],\n                    ""variable_utterance"": False,\n                    ""utterance_delimiter"": ""|||"",\n                    ""max_utterance_cnt"": 5,\n                    ""data_name"": ""source"",\n                },\n                ""target_dataset"": {\n                    # ...\n                    # Same fields are allowed as in ""source_dataset"" with the\n                    # same default values, except the\n                    # following new fields/values:\n                    ""bos_token"": ""<BOS>""\n                    ""vocab_share"": False,\n                    ""embedding_init_share"": False,\n                    ""processing_share"": False,\n                    ""data_name"": ""target""\n                }\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""paired_text_data"",\n                # (3) Bucketing\n                ""bucket_boundaries"": [],\n                ""bucket_batch_sizes"": None,\n                ""bucket_length_fn"": None,\n            }\n\n        Here:\n\n        1. Hyperparameters in the :attr:`""source_dataset""` and\n        attr:`""target_dataset""` fields have the same definition as those\n        in :meth:`texar.tf.data.MonoTextData.default_hparams`, for source and\n        target text, respectively.\n\n        For the new hyperparameters in ""target_dataset"":\n\n            ""vocab_share"": bool\n                Whether to share the vocabulary of source.\n                If `True`, the vocab file of target is ignored.\n\n            ""embedding_init_share"": bool\n                Whether to share the embedding initial value of source. If\n                `True`, :attr:`""embedding_init""` of target is ignored.\n\n                :attr:`""vocab_share""` must be true to share the embedding\n                initial value.\n\n            ""processing_share"": bool\n                Whether to share the processing configurations of source,\n                including\n                ""delimiter"", ""bos_token"", ""eos_token"", and\n                ""other_transformations"".\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.tf.data.DataBase.default_hparams` for details.\n\n        3. For **bucketing** hyperparameters, see\n        :meth:`texar.tf.data.MonoTextData.default_hparams` for details, except\n        that the default bucket_length_fn is the maximum sequence length\n        of source and target sequences.\n\n        """"""\n        hparams = TextDataBase.default_hparams()\n        hparams[""name""] = ""paired_text_data""\n        hparams.update(_default_paired_text_dataset_hparams())\n        return hparams\n\n    @staticmethod\n    def make_vocab(src_hparams, tgt_hparams):\n        """"""Reads vocab files and returns source vocab and target vocab.\n\n        Args:\n            src_hparams (dict or HParams): Hyperparameters of source dataset.\n            tgt_hparams (dict or HParams): Hyperparameters of target dataset.\n\n        Returns:\n            A pair of :class:`texar.tf.data.Vocab` instances. The two instances\n            may be the same objects if source and target vocabs are shared\n            and have the same other configs.\n        """"""\n        src_vocab = MonoTextData.make_vocab(src_hparams)\n\n        if tgt_hparams[""processing_share""]:\n            tgt_bos_token = src_hparams[""bos_token""]\n            tgt_eos_token = src_hparams[""eos_token""]\n        else:\n            tgt_bos_token = tgt_hparams[""bos_token""]\n            tgt_eos_token = tgt_hparams[""eos_token""]\n        tgt_bos_token = utils.default_str(tgt_bos_token,\n                                          SpecialTokens.BOS)\n        tgt_eos_token = utils.default_str(tgt_eos_token,\n                                          SpecialTokens.EOS)\n        if tgt_hparams[""vocab_share""]:\n            if tgt_bos_token == src_vocab.bos_token and \\\n                    tgt_eos_token == src_vocab.eos_token:\n                tgt_vocab = src_vocab\n            else:\n                tgt_vocab = Vocab(src_hparams[""vocab_file""],\n                                  bos_token=tgt_bos_token,\n                                  eos_token=tgt_eos_token)\n        else:\n            tgt_vocab = Vocab(tgt_hparams[""vocab_file""],\n                              bos_token=tgt_bos_token,\n                              eos_token=tgt_eos_token)\n\n        return src_vocab, tgt_vocab\n\n    @staticmethod\n    def make_embedding(src_emb_hparams, src_token_to_id_map,\n                       tgt_emb_hparams=None, tgt_token_to_id_map=None,\n                       emb_init_share=False):\n        """"""Optionally loads source and target embeddings from files\n        (if provided), and returns respective :class:`texar.tf.data.Embedding`\n        instances.\n        """"""\n        src_embedding = MonoTextData.make_embedding(src_emb_hparams,\n                                                    src_token_to_id_map)\n\n        if emb_init_share:\n            tgt_embedding = src_embedding\n        else:\n            tgt_emb_file = tgt_emb_hparams[""file""]\n            tgt_embedding = None\n            if tgt_emb_file is not None and tgt_emb_file != """":\n                tgt_embedding = Embedding(tgt_token_to_id_map, tgt_emb_hparams)\n\n        return src_embedding, tgt_embedding\n\n    def _make_dataset(self):\n        src_dataset = tf.data.TextLineDataset(\n            self._hparams.source_dataset.files,\n            compression_type=self._hparams.source_dataset.compression_type)\n        tgt_dataset = tf.data.TextLineDataset(\n            self._hparams.target_dataset.files,\n            compression_type=self._hparams.target_dataset.compression_type)\n        return tf.data.Dataset.zip((src_dataset, tgt_dataset))\n\n    @staticmethod\n    def _get_name_prefix(src_hparams, tgt_hparams):\n        name_prefix = [\n            src_hparams[""data_name""], tgt_hparams[""data_name""]]\n        if name_prefix[0] == name_prefix[1]:\n            raise ValueError(""\'data_name\' of source and target ""\n                             ""datasets cannot be the same."")\n        return name_prefix\n\n    @staticmethod\n    def _make_processor(src_hparams, tgt_hparams, data_spec, name_prefix):\n        # Create source data decoder\n        data_spec_i = data_spec.get_ith_data_spec(0)\n        src_decoder, src_trans, data_spec_i = MonoTextData._make_processor(\n            src_hparams, data_spec_i, chained=False)\n        data_spec.set_ith_data_spec(0, data_spec_i, 2)\n\n        # Create target data decoder\n        tgt_proc_hparams = tgt_hparams\n        if tgt_hparams[""processing_share""]:\n            tgt_proc_hparams = copy.copy(src_hparams)\n            try:\n                tgt_proc_hparams[""variable_utterance""] = \\\n                        tgt_hparams[""variable_utterance""]\n            except TypeError:\n                tgt_proc_hparams.variable_utterance = \\\n                        tgt_hparams[""variable_utterance""]\n        data_spec_i = data_spec.get_ith_data_spec(1)\n        tgt_decoder, tgt_trans, data_spec_i = MonoTextData._make_processor(\n            tgt_proc_hparams, data_spec_i, chained=False)\n        data_spec.set_ith_data_spec(1, data_spec_i, 2)\n\n        tran_fn = dsutils.make_combined_transformation(\n            [[src_decoder] + src_trans, [tgt_decoder] + tgt_trans],\n            name_prefix=name_prefix)\n\n        data_spec.add_spec(name_prefix=name_prefix)\n\n        return tran_fn, data_spec\n\n    @staticmethod\n    def _make_length_filter(src_hparams, tgt_hparams,\n                            src_length_name, tgt_length_name,\n                            src_decoder, tgt_decoder):\n        src_filter_fn = MonoTextData._make_length_filter(\n            src_hparams, src_length_name, src_decoder)\n        tgt_filter_fn = MonoTextData._make_length_filter(\n            tgt_hparams, tgt_length_name, tgt_decoder)\n        combined_filter_fn = dsutils._make_combined_filter_fn(\n            [src_filter_fn, tgt_filter_fn])\n        return combined_filter_fn\n\n    def _process_dataset(self, dataset, hparams, data_spec):\n        name_prefix = PairedTextData._get_name_prefix(\n            hparams[""source_dataset""], hparams[""target_dataset""])\n        tran_fn, data_spec = self._make_processor(\n            hparams[""source_dataset""], hparams[""target_dataset""],\n            data_spec, name_prefix=name_prefix)\n\n        num_parallel_calls = hparams[""num_parallel_calls""]\n        dataset = dataset.map(\n            lambda *args: tran_fn(dsutils.maybe_tuple(args)),\n            num_parallel_calls=num_parallel_calls)\n\n        # Filters by length\n        src_length_name = dsutils._connect_name(\n            data_spec.name_prefix[0],\n            data_spec.decoder[0].length_tensor_name)\n        tgt_length_name = dsutils._connect_name(\n            data_spec.name_prefix[1],\n            data_spec.decoder[1].length_tensor_name)\n        filter_fn = self._make_length_filter(\n            hparams[""source_dataset""], hparams[""target_dataset""],\n            src_length_name, tgt_length_name,\n            data_spec.decoder[0], data_spec.decoder[1])\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        # Truncates data count\n        dataset = dataset.take(hparams[""max_dataset_size""])\n\n        return dataset, data_spec\n\n    def _make_bucket_length_fn(self):\n        length_fn = self._hparams.bucket_length_fn\n        if not length_fn:\n            length_fn = lambda x: tf.maximum(\n                x[self.source_length_name], x[self.target_length_name])\n        elif not is_callable(length_fn):\n            # pylint: disable=redefined-variable-type\n            length_fn = utils.get_function(length_fn, [""texar.tf.custom""])\n        return length_fn\n\n    def _make_padded_shapes(self, dataset, src_decoder, tgt_decoder):\n        src_text_and_id_shapes = {}\n        if self._hparams.source_dataset.pad_to_max_seq_length:\n            src_text_and_id_shapes = \\\n                    MonoTextData._make_padded_text_and_id_shapes(\n                        dataset, self._hparams.source_dataset, src_decoder,\n                        self.source_text_name, self.source_text_id_name)\n\n        tgt_text_and_id_shapes = {}\n        if self._hparams.target_dataset.pad_to_max_seq_length:\n            tgt_text_and_id_shapes = \\\n                    MonoTextData._make_padded_text_and_id_shapes(\n                        dataset, self._hparams.target_dataset, tgt_decoder,\n                        self.target_text_name, self.target_text_id_name)\n\n        padded_shapes = dataset.output_shapes\n        padded_shapes.update(src_text_and_id_shapes)\n        padded_shapes.update(tgt_text_and_id_shapes)\n\n        return padded_shapes\n\n    def _make_data(self):\n        self._src_vocab, self._tgt_vocab = self.make_vocab(\n            self._hparams.source_dataset, self._hparams.target_dataset)\n\n        tgt_hparams = self._hparams.target_dataset\n        if not tgt_hparams.vocab_share and tgt_hparams.embedding_init_share:\n            raise ValueError(""embedding_init can be shared only when vocab ""\n                             ""is shared. Got `vocab_share=False, ""\n                             ""emb_init_share=True`."")\n        self._src_embedding, self._tgt_embedding = self.make_embedding(\n            self._hparams.source_dataset.embedding_init,\n            self._src_vocab.token_to_id_map_py,\n            self._hparams.target_dataset.embedding_init,\n            self._tgt_vocab.token_to_id_map_py,\n            self._hparams.target_dataset.embedding_init_share)\n\n        # Create dataset\n        dataset = self._make_dataset()\n        dataset, dataset_size = self._shuffle_dataset(\n            dataset, self._hparams, self._hparams.source_dataset.files)\n        self._dataset_size = dataset_size\n\n        # Processing.\n        data_spec = dsutils._DataSpec(\n            dataset=dataset, dataset_size=self._dataset_size,\n            vocab=[self._src_vocab, self._tgt_vocab],\n            embedding=[self._src_embedding, self._tgt_embedding])\n        dataset, data_spec = self._process_dataset(\n            dataset, self._hparams, data_spec)\n        self._data_spec = data_spec\n        self._decoder = data_spec.decoder\n        self._src_decoder = data_spec.decoder[0]\n        self._tgt_decoder = data_spec.decoder[1]\n\n        # Batching\n        length_fn = self._make_bucket_length_fn()\n        padded_shapes = self._make_padded_shapes(\n            dataset, self._src_decoder, self._tgt_decoder)\n        dataset = self._make_batch(\n            dataset, self._hparams, length_fn, padded_shapes)\n\n        # Prefetching\n        if self._hparams.prefetch_buffer_size > 0:\n            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)\n\n        self._dataset = dataset\n\n    def list_items(self):\n        """"""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return list(self._dataset.output_types.keys())\n\n    @property\n    def dataset(self):\n        """"""The dataset.\n        """"""\n        return self._dataset\n\n    def dataset_size(self):\n        """"""Returns the number of data instances in the dataset.\n\n        Note that this is the total data count in the raw files, before any\n        filtering and truncation.\n        """"""\n        if not self._dataset_size:\n            # pylint: disable=attribute-defined-outside-init\n            self._dataset_size = count_file_lines(\n                self._hparams.source_dataset.files)\n        return self._dataset_size\n\n    @property\n    def vocab(self):\n        """"""A pair instances of :class:`~texar.tf.data.Vocab` that are source\n        and target vocabs, respectively.\n        """"""\n        return self._src_vocab, self._tgt_vocab\n\n    @property\n    def source_vocab(self):\n        """"""The source vocab, an instance of :class:`~texar.tf.data.Vocab`.\n        """"""\n        return self._src_vocab\n\n    @property\n    def target_vocab(self):\n        """"""The target vocab, an instance of :class:`~texar.tf.data.Vocab`.\n        """"""\n        return self._tgt_vocab\n\n    @property\n    def source_embedding_init_value(self):\n        """"""The `Tensor` containing the embedding value of source data\n        loaded from file. `None` if embedding is not specified.\n        """"""\n        if self._src_embedding is None:\n            return None\n        return self._src_embedding.word_vecs\n\n    @property\n    def target_embedding_init_value(self):\n        """"""The `Tensor` containing the embedding value of target data\n        loaded from file. `None` if embedding is not specified.\n        """"""\n        if self._tgt_embedding is None:\n            return None\n        return self._tgt_embedding.word_vecs\n\n    def embedding_init_value(self):\n        """"""A pair of `Tensor` containing the embedding values of source and\n        target data loaded from file.\n        """"""\n        src_emb = self.source_embedding_init_value\n        tgt_emb = self.target_embedding_init_value\n        return src_emb, tgt_emb\n\n    @property\n    def source_text_name(self):\n        """"""The name of the source text tensor, ""source_text"" by default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[0],\n            self._src_decoder.text_tensor_name)\n        return name\n\n    @property\n    def source_length_name(self):\n        """"""The name of the source length tensor, ""source_length"" by default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[0],\n            self._src_decoder.length_tensor_name)\n        return name\n\n    @property\n    def source_text_id_name(self):\n        """"""The name of the source text index tensor, ""source_text_ids"" by\n        default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[0],\n            self._src_decoder.text_id_tensor_name)\n        return name\n\n    @property\n    def source_utterance_cnt_name(self):\n        """"""The name of the source text utterance count tensor,\n        ""source_utterance_cnt"" by default.\n        """"""\n        if not self._hparams.source_dataset.variable_utterance:\n            raise ValueError(\n                ""`utterance_cnt_name` of source data is undefined."")\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[0],\n            self._src_decoder.utterance_cnt_tensor_name)\n        return name\n\n    @property\n    def target_text_name(self):\n        """"""The name of the target text tensor, ""target_text"" bt default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[1],\n            self._tgt_decoder.text_tensor_name)\n        return name\n\n    @property\n    def target_length_name(self):\n        """"""The name of the target length tensor, ""target_length"" by default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[1],\n            self._tgt_decoder.length_tensor_name)\n        return name\n\n    @property\n    def target_text_id_name(self):\n        """"""The name of the target text index tensor, ""target_text_ids"" by\n        default.\n        """"""\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[1],\n            self._tgt_decoder.text_id_tensor_name)\n        return name\n\n    @property\n    def target_utterance_cnt_name(self):\n        """"""The name of the target text utterance count tensor,\n        ""target_utterance_cnt"" by default.\n        """"""\n        if not self._hparams.target_dataset.variable_utterance:\n            raise ValueError(\n                ""`utterance_cnt_name` of target data is undefined."")\n        name = dsutils._connect_name(\n            self._data_spec.name_prefix[1],\n            self._tgt_decoder.utterance_cnt_tensor_name)\n        return name\n\n    @property\n    def text_name(self):\n        """"""The name of text tensor, ""text"" by default.\n        """"""\n        return self._src_decoder.text_tensor_name\n\n    @property\n    def length_name(self):\n        """"""The name of length tensor, ""length"" by default.\n        """"""\n        return self._src_decoder.length_tensor_name\n\n    @property\n    def text_id_name(self):\n        """"""The name of text index tensor, ""text_ids"" by default.\n        """"""\n        return self._src_decoder.text_id_tensor_name\n\n    @property\n    def utterance_cnt_name(self):\n        """"""The name of the text utterance count tensor, ""utterance_cnt"" by\n        default.\n        """"""\n        if self._hparams.source_dataset.variable_utterance:\n            return self._src_decoder.utterance_cnt_tensor_name\n        if self._hparams.target_dataset.variable_utterance:\n            return self._tgt_decoder.utterance_cnt_tensor_name\n        raise ValueError(""`utterance_cnt_name` is not defined."")\n'"
texar/tf/data/data/scalar_data.py,10,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious data classes that define data reading, parsing, batching, and other\npreprocessing operations.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.data.data_utils import count_file_lines\nfrom texar.tf.data.data import dataset_utils as dsutils\nfrom texar.tf.data.data.data_base import DataBase\nfrom texar.tf.data.data.mono_text_data import MonoTextData\nfrom texar.tf.data.data_decoders import ScalarDataDecoder\n\n# pylint: disable=invalid-name, arguments-differ, not-context-manager\n\n__all__ = [\n    ""_default_scalar_dataset_hparams"",\n    ""ScalarData""\n]\n\n\ndef _default_scalar_dataset_hparams():\n    """"""Returns hyperparameters of a scalar dataset with default values.\n\n    See :meth:`texar.tf.data.ScalarData.default_hparams` for details.\n    """"""\n    return {\n        ""files"": [],\n        ""compression_type"": None,\n        ""data_type"": ""int"",\n        ""data_name"": None,\n        ""other_transformations"": [],\n        ""@no_typecheck"": [""files""]\n    }\n\n\nclass ScalarData(DataBase):\n    """"""Scalar data where each line of the files is a scalar (int or float),\n    e.g., a data label.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams` for the\n            defaults.\n\n    The processor reads and processes raw data and results in a TF dataset\n    whose element is a python `dict` including one field. The field name is\n    specified in :attr:`hparams[""dataset""][""data_name""]`. If not specified,\n    the default name is `""data""`. The field name can be accessed through\n    :attr:`data_name`.\n\n    This field is a Tensor of shape `[batch_size]` containing a batch of\n    scalars, of either int or float type as specified in :attr:`hparams`.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams={\n                \'dataset\': { \'files\': \'data.txt\', \'data_name\': \'label\' },\n                \'batch_size\': 2\n            }\n            data = ScalarData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #     \'label\': [2, 9]\n            # }\n    """"""\n\n    def __init__(self, hparams):\n        DataBase.__init__(self, hparams)\n        with tf.name_scope(self.name, self.default_hparams()[""name""]):\n            self._make_data()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dicitionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to scalar dataset\n                ""dataset"": {\n                    ""files"": [],\n                    ""compression_type"": None,\n                    ""data_type"": ""int"",\n                    ""other_transformations"": [],\n                    ""data_name"": None,\n                }\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""scalar_data"",\n            }\n\n        Here:\n\n        1. For the hyperparameters in the :attr:`""dataset""` field:\n\n            ""files"": str or list\n                A (list of) file path(s).\n\n                Each line contains a single scalar number.\n\n            ""compression_type"": str, optional\n                One of """" (no compression), ""ZLIB"", or ""GZIP"".\n\n            ""data_type"": str\n                The scalar type. Currently supports ""int"" and ""float"".\n\n            ""other_transformations"": list\n                A list of transformation functions or function names/paths to\n                further transform each single data instance.\n\n                (More documentations to be added.)\n\n            ""data_name"": str\n                Name of the dataset.\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.tf.data.DataBase.default_hparams` for details.\n\n        """"""\n        hparams = DataBase.default_hparams()\n        hparams[""name""] = ""scalar_data""\n        hparams.update({\n            ""dataset"": _default_scalar_dataset_hparams()\n        })\n        return hparams\n\n    @staticmethod\n    def _get_dtype(dtype_hparam):\n        if dtype_hparam == ""int"":\n            dtype = tf.int32\n        elif dtype_hparam == ""float"":\n            dtype = tf.float32\n        else:\n            raise ValueError(""Unknown data type: "" + dtype_hparam)\n        return dtype\n\n    @staticmethod\n    def _make_processor(dataset_hparams, data_spec, chained=True,\n                        name_prefix=None):\n        # Create data decoder\n        decoder = ScalarDataDecoder(\n            ScalarData._get_dtype(dataset_hparams[""data_type""]),\n            data_name=name_prefix)\n        # Create other transformations\n        data_spec.add_spec(decoder=decoder)\n        # pylint: disable=protected-access\n        other_trans = MonoTextData._make_other_transformations(\n            dataset_hparams[""other_transformations""], data_spec)\n\n        data_spec.add_spec(name_prefix=name_prefix)\n\n        if chained:\n            chained_tran = dsutils.make_chained_transformation(\n                [decoder] + other_trans)\n            return chained_tran, data_spec\n        else:\n            return decoder, other_trans, data_spec\n\n    def _process_dataset(self, dataset, hparams, data_spec):\n        chained_tran, data_spec = self._make_processor(\n            hparams[""dataset""], data_spec,\n            name_prefix=hparams[""dataset""][""data_name""])\n        num_parallel_calls = hparams[""num_parallel_calls""]\n        dataset = dataset.map(\n            lambda *args: chained_tran(dsutils.maybe_tuple(args)),\n            num_parallel_calls=num_parallel_calls)\n\n        # Truncates data count\n        dataset = dataset.take(hparams[""max_dataset_size""])\n\n        return dataset, data_spec\n\n    def _make_data(self):\n        dataset_hparams = self._hparams.dataset\n\n        # Create and shuffle dataset\n        dataset = MonoTextData._make_mono_text_dataset(dataset_hparams)\n        dataset, dataset_size = self._shuffle_dataset(\n            dataset, self._hparams, self._hparams.dataset.files)\n        self._dataset_size = dataset_size\n\n        # Processing\n        # pylint: disable=protected-access\n        data_spec = dsutils._DataSpec(dataset=dataset,\n                                      dataset_size=self._dataset_size)\n        dataset, data_spec = self._process_dataset(dataset, self._hparams,\n                                                   data_spec)\n        self._data_spec = data_spec\n        self._decoder = data_spec.decoder  # pylint: disable=no-member\n\n        # Batching\n        dataset = self._make_batch(dataset, self._hparams)\n\n        # Prefetching\n        if self._hparams.prefetch_buffer_size > 0:\n            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)\n\n        self._dataset = dataset\n\n    def list_items(self):\n        """"""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return list(self._dataset.output_types.keys())\n\n    @property\n    def dataset(self):\n        """"""The dataset.\n        """"""\n        return self._dataset\n\n    def dataset_size(self):\n        """"""Returns the number of data instances in the dataset.\n\n        Note that this is the total data count in the raw files, before any\n        filtering and truncation.\n        """"""\n        if not self._dataset_size:\n            # pylint: disable=attribute-defined-outside-init\n            self._dataset_size = count_file_lines(\n                self._hparams.dataset.files)\n        return self._dataset_size\n\n    @property\n    def data_name(self):\n        """"""The name of the data tensor, ""data"" by default if not specified in\n        :attr:`hparams`.\n        """"""\n        return self._decoder.data_tensor_name\n'"
texar/tf/data/data/text_data_base.py,4,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase text data class that is enherited by all text data classes.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.data.data.data_base import DataBase\nfrom texar.tf.data.data import dataset_utils as dsutils\n\n# pylint: disable=protected-access, arguments-differ\n\n__all__ = [\n    ""TextDataBase""\n]\n\n\nclass TextDataBase(DataBase):  # pylint: disable=too-few-public-methods\n    """"""Base class inheritted by all text data classes.\n    """"""\n\n    def __init__(self, hparams):\n        DataBase.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of default hyperparameters.\n\n        See the specific subclasses for the details.\n        """"""\n        hparams = DataBase.default_hparams()\n        hparams.update({\n            ""bucket_boundaries"": [],\n            ""bucket_batch_sizes"": None,\n            ""bucket_length_fn"": None})\n        return hparams\n\n    @staticmethod\n    def _make_batch(dataset, hparams, element_length_func,\n                    padded_shapes=None, padding_values=None):\n        dataset = dataset.repeat(hparams.num_epochs)\n\n        batch_size = hparams[""batch_size""]\n        bucket_boundaries = hparams[""bucket_boundaries""]\n        if padded_shapes is None:\n            padded_shapes = dataset.output_shapes\n\n        if len(bucket_boundaries) == 0:\n            if hparams[""allow_smaller_final_batch""]:\n                dataset = dataset.padded_batch(\n                    batch_size, padded_shapes, padding_values=padding_values)\n            else:\n                dataset = dataset.apply(\n                    tf.contrib.data.padded_batch_and_drop_remainder(\n                        batch_size, padded_shapes,\n                        padding_values=padding_values))\n        else:\n            bucket_batch_size = hparams[""bucket_batch_sizes""]\n            if bucket_batch_size is None:\n                bucket_batch_size = [batch_size] * (len(bucket_boundaries) + 1)\n            dataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(\n                element_length_func, bucket_boundaries, bucket_batch_size,\n                padded_shapes=padded_shapes, padding_values=padding_values))\n            if not hparams[""allow_smaller_final_batch""]:\n                if len(set(bucket_batch_size)) > 1:\n                    raise ValueError(\n                        ""Batch size of every bucket must be the same if ""\n                        ""smaller final batch is not allowed."")\n                batch_size = bucket_batch_size[0]\n                filter_fn = dsutils._make_smaller_batch_filter_fn(batch_size)\n                dataset = dataset.filter(\n                    lambda *args: filter_fn(dsutils.maybe_tuple(args)))\n\n        return dataset\n'"
texar/tf/data/data/tfrecord_data.py,25,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nData class that supports reading TFRecord data and data type converting.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.data.data import dataset_utils as dsutils\nfrom texar.tf.data.data.data_base import DataBase\nfrom texar.tf.data.data.mono_text_data import MonoTextData\nfrom texar.tf.data.data_decoders import TFRecordDataDecoder\n\n# pylint: disable=invalid-name, arguments-differ, not-context-manager\n\n__all__ = [\n    ""_default_tfrecord_dataset_hparams"",\n    ""TFRecordData""\n]\n\n\ndef _default_tfrecord_dataset_hparams():\n    """"""Returns hyperparameters of a TFRecord dataset with default values.\n\n    See :meth:`texar.tf.data.TFRecordData.default_hparams` for details.\n    """"""\n    return {\n        ""files"": [],\n        ""feature_original_types"": {},\n        ""feature_convert_types"": {},\n        ""image_options"": {},\n        ""compression_type"": None,\n        ""other_transformations"": [],\n        ""num_shards"": None,\n        ""shard_id"": None,\n        ""data_name"": None,\n        ""@no_typecheck"": [\n            ""files"",\n            ""feature_original_types"",\n            ""feature_convert_types"",\n            ""image_options""\n        ],\n    }\n\n\nclass TFRecordData(DataBase):\n    """"""TFRecord data which loads and processes TFRecord files.\n\n    This module can be used to process image data, features, etc.\n\n    Args:\n        hparams (dict): Hyperparameters. See :meth:`default_hparams`\n            for the defaults.\n\n    The module reads and restores data from TFRecord files and\n    results in a TF Dataset whose element is a Python `dict` that maps feature\n    names to feature values. The features names and dtypes are specified in\n    :attr:`hparams[""dataset""][""feature_original_types""]`.\n\n    The module also provides simple processing options for image data, such\n    as image resize.\n\n    Example:\n\n        .. code-block:: python\n\n            # Read data from TFRecord file\n            hparams={\n                \'dataset\': {\n                    \'files\': \'image1.tfrecord\',\n                    \'feature_original_types\': {\n                        \'height\': [\'tf.int64\', \'FixedLenFeature\'],\n                        \'width\': [\'tf.int64\', \'FixedLenFeature\'],\n                        \'label\': [\'tf.int64\', \'FixedLenFeature\'],\n                        \'image_raw\': [\'tf.string\', \'FixedLenFeature\']\n                    }\n                },\n                \'batch_size\': 1\n            }\n            data = TFRecordData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #    \'data\': {\n            #        \'height\': [239],\n            #        \'width\': [149],\n            #        \'label\': [1],\n            #\n            #        # \'image_raw\' is a list of image data bytes in this\n            #        # example.\n            #        \'image_raw\': [...],\n            #    }\n            # }\n\n        .. code-block:: python\n\n            # Read image data from TFRecord file and do resizing\n            hparams={\n                \'dataset\': {\n                    \'files\': \'image2.tfrecord\',\n                    \'feature_original_types\': {\n                        \'label\': [\'tf.int64\', \'FixedLenFeature\'],\n                        \'image_raw\': [\'tf.string\', \'FixedLenFeature\']\n                    },\n                    \'image_options\': {\n                        \'image_feature_name\': \'image_raw\',\n                        \'resize_height\': 512,\n                        \'resize_width\': 512,\n                    }\n                },\n                \'batch_size\': 1\n            }\n            data = TFRecordData(hparams)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            iterator.switch_to_dataset(sess) # initializes the dataset\n            batch_ = sess.run(batch)\n            # batch_ == {\n            #    \'data\': {\n            #        \'label\': [1],\n            #\n            #        # ""image_raw"" is a list of a ""numpy.ndarray"" image\n            #        # in this example. Each image has a width of 512 and\n            #        # height of 512.\n            #        \'image_raw\': [...]\n            #    }\n            # }\n\n    """"""\n\n    def __init__(self, hparams):\n        DataBase.__init__(self, hparams)\n        with tf.name_scope(self.name, self.default_hparams()[""name""]):\n            self._make_data()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dicitionary of default hyperparameters.\n\n        .. code-block:: python\n\n            {\n                # (1) Hyperparams specific to TFRecord dataset\n                \'dataset\': {\n                    \'files\': [],\n                    \'feature_original_types\': {},\n                    \'feature_convert_types\': {},\n                    \'image_options\': {},\n                    ""num_shards"": None,\n                    ""shard_id"": None,\n                    ""other_transformations"": [],\n                    ""data_name"": None,\n                }\n                # (2) General hyperparams\n                ""num_epochs"": 1,\n                ""batch_size"": 64,\n                ""allow_smaller_final_batch"": True,\n                ""shuffle"": True,\n                ""shuffle_buffer_size"": None,\n                ""shard_and_shuffle"": False,\n                ""num_parallel_calls"": 1,\n                ""prefetch_buffer_size"": 0,\n                ""max_dataset_size"": -1,\n                ""seed"": None,\n                ""name"": ""tfrecord_data"",\n            }\n\n        Here:\n\n        1. For the hyperparameters in the :attr:`""dataset""` field:\n\n            ""files"": str or list\n                A (list of) TFRecord file path(s).\n\n            ""feature_original_types"": dict\n                The feature names (str) with their data types and length types,\n                key and value in pair\n                `feature_name: [dtype, feature_len_type, len]`,\n\n                - `dtype` is a :tf_main:`TF Dtype <dtypes/DType>` such as\\\n                `tf.string` and `tf.int32`, or its string name such as \\\n                \'tf.string\' and \'tf.int32\'. The feature will be read from the\\\n                files and parsed into this dtype.\n\n                - `feature_len_type` is of type `str`, and can be either \\\n                \'FixedLenFeature\' or \'VarLenFeature\' for fixed length \\\n                features and non-fixed length features, respectively.\n\n                - `len` is an `int` and is optional. It is the length for \\\n                \'FixedLenFeature\'. Ignored if \'VarLenFeature\' is used.\n\n                Example:\n\n                .. code-block:: python\n\n                    feature_original_types = {\n                        ""input_ids"": [""tf.int64"", ""FixedLenFeature"", 128],\n                        ""label_ids"": [""tf.int64"", ""FixedLenFeature""],\n                        ""name_lists"": [""tf.string"", ""VarLenFeature""],\n                    }\n\n            ""feature_convert_types"": dict, optional\n                Specifies dtype converting after reading the data files. This\n                `dict` maps feature names to desired data dtypes. For example,\n                you can first read a feature into dtype `tf.float64` by\n                specifying in ""feature_original_types"" above, and convert\n                the feature to dtype ""tf.int64"" by specifying here.\n                Features not specified here will not do dtype-convert.\n\n                - `dtype` is a :tf_main:`TF Dtype <dtypes/DType>` such as\\\n                `tf.string` and `tf.int32`, or its string name such as \\\n                \'tf.string\' and \'tf.int32\'.\n\n                Be noticed that this converting process is after all the data\n                are restored, `feature_original_types` has to be set firstly.\n\n                Example:\n\n                .. code-block:: python\n\n                    feature_convert_types = {\n                        ""input_ids"": ""tf.int32"",\n                        ""label_ids"": ""tf.int32"",\n                    }\n\n            ""image_options"": dict, optional\n                Specifies the image feature name and performs image resizing,\n                includes three fields:\n\n                - ""image_feature_name"":\n                    A `str`, the name of the feature which contains\n                    the image data. If set, the image data\n                    will be restored in format `numpy.ndarray`.\n                - ""resize_height"":\n                    A `int`, the height of the image after resizing.\n                - ""resize_width"":\n                    A `int`, the width of the image after resizing\n\n                If either `resize_height` or `resize_width` is not set,\n                image data will be restored with original shape.\n            ""num_shards"": int, optional\n                The number of data shards in distributed mode. Usually set to\n                the number of processes in distributed computing.\n                Used in combination with :attr:`""shard_id""`.\n            ""shard_id"": int, optional\n                Sets the unique id to identify a shard. The module will\n                processes only the corresponding shard of the whole data.\n                Used in combination with :attr:`""num_shards""`.\n\n                E.g., in a case of distributed computing on 2 GPUs, the hparams\n                of the data module for the two processes can be as below,\n                respectively.\n\n                For gpu 0:\n\n                .. code-block:: python\n\n                    dataset: {\n                        ...\n                        ""num_shards"": 2,\n                        ""shard_id"": 0\n                    }\n\n                For gpu 1:\n\n                .. code-block:: python\n\n                    dataset: {\n                        ...\n                        ""num_shards"": 2,\n                        ""shard_id"": 1\n                    }\n\n                Also refer to `examples/bert` for a use case.\n\n            ""other_transformations"": list\n                A list of transformation functions or function names/paths to\n                further transform each single data instance.\n            ""data_name"": str\n                Name of the dataset.\n\n        2. For the **general** hyperparameters, see\n        :meth:`texar.tf.data.DataBase.default_hparams` for details.\n\n        """"""\n        hparams = DataBase.default_hparams()\n        hparams[""name""] = ""tfrecord_data""\n        hparams.update({\n            ""dataset"": _default_tfrecord_dataset_hparams()\n        })\n        return hparams\n\n    def _read_TFRecord_data(self):\n        filenames = self._hparams.dataset.files\n        dataset = tf.data.TFRecordDataset(filenames=filenames)\n        return dataset\n\n    @staticmethod\n    def _make_processor(dataset_hparams, data_spec, chained=True,\n                        name_prefix=None):\n        # Create data decoder\n        decoder = TFRecordDataDecoder(\n            feature_original_types=dataset_hparams.feature_original_types,\n            feature_convert_types=dataset_hparams.feature_convert_types,\n            image_options=dataset_hparams.image_options)\n        # Create other transformations\n        data_spec.add_spec(decoder=decoder)\n        # pylint: disable=protected-access\n        other_trans = MonoTextData._make_other_transformations(\n            dataset_hparams[""other_transformations""], data_spec)\n\n        data_spec.add_spec(name_prefix=name_prefix)\n\n        if chained:\n            chained_tran = dsutils.make_chained_transformation(\n                [decoder] + other_trans)\n            return chained_tran, data_spec\n        else:\n            return decoder, other_trans, data_spec\n\n    def _process_dataset(self, dataset, hparams, data_spec):\n        chained_tran, data_spec = self._make_processor(\n            hparams[""dataset""], data_spec,\n            name_prefix=hparams[""dataset""][""data_name""])\n        num_parallel_calls = hparams[""num_parallel_calls""]\n        dataset = dataset.map(\n            lambda *args: chained_tran(dsutils.maybe_tuple(args)),\n            num_parallel_calls=num_parallel_calls)\n\n        # Truncates data count\n        dataset = dataset.take(hparams[""max_dataset_size""])\n\n        return dataset, data_spec\n\n    def _make_data(self):\n        dataset = self._read_TFRecord_data()\n        # Create and shuffle dataset\n        num_shards = self._hparams.dataset.num_shards\n        shard_id = self._hparams.dataset.shard_id\n        if num_shards is not None and shard_id is not None:\n            dataset = dataset.shard(num_shards, shard_id)\n        dataset, dataset_size = self._shuffle_dataset(\n            dataset, self._hparams, self._hparams.dataset.files)\n        self._dataset_size = dataset_size\n\n        # Processing\n        # pylint: disable=protected-access\n        data_spec = dsutils._DataSpec(dataset=dataset,\n                                      dataset_size=self._dataset_size)\n        dataset, data_spec = self._process_dataset(dataset, self._hparams,\n                                                   data_spec)\n        self._data_spec = data_spec\n        self._decoder = data_spec.decoder  # pylint: disable=no-member\n        # Batching\n        dataset = self._make_batch(dataset, self._hparams)\n        # Prefetching\n        if self._hparams.prefetch_buffer_size > 0:\n            dataset = dataset.prefetch(self._hparams.prefetch_buffer_size)\n\n        self._dataset = dataset\n        self.dataset = dataset\n\n    def list_items(self):\n        """"""Returns the list of item names that the data can produce.\n\n        Returns:\n            A list of strings.\n        """"""\n        return sorted(list(self._dataset.output_types.keys()))\n\n    @property\n    def feature_names(self):\n        """"""A list of feature names.\n        """"""\n        return self.list_items()\n'"
texar/tf/data/tokenizers/__init__.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTokenizer modules of Texar library.\n""""""\n\nfrom texar.tf.data.tokenizers.bert_tokenizer import *\nfrom texar.tf.data.tokenizers.gpt2_tokenizer import *\nfrom texar.tf.data.tokenizers.tokenizer_base import *\nfrom texar.tf.data.tokenizers.xlnet_tokenizer import *\n'"
texar/tf/data/tokenizers/bert_tokenizer.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained BERT tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_bert.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\n\nfrom texar.tf.modules.pretrained.bert import PretrainedBERTMixin\nfrom texar.tf.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.tf.data.tokenizers.bert_tokenizer_utils import \\\n    load_vocab, BasicTokenizer, WordpieceTokenizer\nfrom texar.tf.utils.utils import truncate_seq_pair\n\n__all__ = [\n    \'BERTTokenizer\',\n]\n\n\nclass BERTTokenizer(PretrainedBERTMixin, TokenizerBase):\n    r""""""Pre-trained BERT Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `bert-base-uncased`). Please refer to\n            :class:`~texar.torch.modules.PretrainedBERTMixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n    _MAX_INPUT_SIZE = {\n        # Standard BERT\n        \'bert-base-uncased\': 512,\n        \'bert-large-uncased\': 512,\n        \'bert-base-cased\': 512,\n        \'bert-large-cased\': 512,\n        \'bert-base-multilingual-uncased\': 512,\n        \'bert-base-multilingual-cased\': 512,\n        \'bert-base-chinese\': 512,\n    }\n    _VOCAB_FILE_NAMES = {\'vocab_file\': \'vocab.txt\'}\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        super().__init__(hparams=None)\n\n        self.config = {\n            \'tokenize_chinese_chars\': self.hparams[\'tokenize_chinese_chars\'],\n            \'do_lower_case\': self.hparams[\'do_lower_case\'],\n            \'do_basic_tokenize\': self.hparams[\'do_basic_tokenize\'],\n            \'non_split_tokens\': self.hparams[\'non_split_tokens\'],\n        }\n\n        if self.pretrained_model_dir is not None:\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_NAMES[\'vocab_file\'])\n            assert self.pretrained_model_name is not None\n            if self._MAX_INPUT_SIZE.get(self.pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[self.pretrained_model_name]\n        else:\n            vocab_file = self.hparams[\'vocab_file\']\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path ""\n                             ""\'{}"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = dict((ids, tok) for tok, ids in self.vocab.items())\n\n        self.do_basic_tokenize = self.hparams[\'do_basic_tokenize\']\n        if self.do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(\n                do_lower_case=self.hparams[""do_lower_case""],\n                never_split=self.hparams[""non_split_tokens""],\n                tokenize_chinese_chars=self.hparams[""tokenize_chinese_chars""])\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab,\n                                                      unk_token=self.unk_token)\n\n    def _map_text_to_token(self, text: str) -> List[str]:  # type: ignore\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(\n                    text, never_split=self.all_special_tokens):\n                assert token is not None\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def save_vocab(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(save_dir):\n            save_dir = os.path.join(save_dir,\n                                    self._VOCAB_FILE_NAMES[\'vocab_file\'])\n        with open(save_dir, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(),\n                                             key=lambda kv: kv[1]):\n                if index != token_index:\n                    print(""Saving vocabulary to {}: vocabulary indices are not ""\n                          ""consecutive. Please check that the vocabulary is ""\n                          ""not corrupted!"".format(save_dir))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n\n        return (save_dir, )\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.vocab)\n\n    def _map_token_to_id(self, token: str) -> int:\n        r""""""Maps a token to an id using the vocabulary.""""""\n        unk_id = self.vocab.get(self.unk_token)\n        assert unk_id is not None\n        return self.vocab.get(token, unk_id)\n\n    def _map_id_to_token(self, index: int) -> str:\n        r""""""Maps an id to a token using the vocabulary.\n        """"""\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) to a single string.""""""\n        out_string = \' \'.join(tokens).replace(\' ##\', \'\').strip()\n        return out_string\n\n    def encode_text(self,\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None) -> \\\n            Tuple[List[int], List[int], List[int]]:\n        r""""""Adds special tokens to a sequence or sequence pair and computes the\n        corresponding segment ids and input mask for BERT specific tasks.\n        The sequence will be truncated if its length is larger than\n        ``max_seq_length``.\n\n        A BERT sequence has the following format:\n        `[cls_token]` X `[sep_token]`\n\n        A BERT sequence pair has the following format:\n        `[cls_token]` A `[sep_token]` B `[sep_token]`\n\n        Args:\n            text_a: The first input text.\n            text_b: The second input text.\n            max_seq_length: Maximum sequence length.\n\n        Returns:\n            A tuple of `(input_ids, segment_ids, input_mask)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special token ids.\n            - ``segment_ids``: A list of segment ids.\n            - ``input_mask``: A list of mask ids. The mask has 1 for real\n              tokens and 0 for padding tokens. Only real tokens are\n              attended to.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        cls_token_id = self._map_token_to_id(self.cls_token)\n        sep_token_id = self._map_token_to_id(self.sep_token)\n\n        token_ids_a = self.map_text_to_id(text_a)\n        assert isinstance(token_ids_a, list)\n\n        token_ids_b = None\n        if text_b:\n            token_ids_b = self.map_text_to_id(text_b)\n\n        if token_ids_b:\n            assert isinstance(token_ids_b, list)\n            # Modifies `token_ids_a` and `token_ids_b` in place so that the\n            # total length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            truncate_seq_pair(token_ids_a, token_ids_b, max_seq_length - 3)\n\n            input_ids = ([cls_token_id] + token_ids_a + [sep_token_id] +\n                         token_ids_b + [sep_token_id])\n            segment_ids = [0] * (len(token_ids_a) + 2) + \\\n                          [1] * (len(token_ids_b) + 1)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            token_ids_a = token_ids_a[:max_seq_length - 2]\n\n            input_ids = [cls_token_id] + token_ids_a + [sep_token_id]\n            segment_ids = [0] * len(input_ids)\n\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the maximum sequence length.\n        input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n        segment_ids = segment_ids + [0] * (max_seq_length - len(segment_ids))\n        input_mask = input_mask + [0] * (max_seq_length - len(input_mask))\n\n        assert len(input_ids) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n\n        return input_ids, segment_ids, input_mask\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""bert-base-uncased"",\n                ""vocab_file"": None,\n                ""max_len"": 512,\n                ""unk_token"": ""[UNK]"",\n                ""sep_token"": ""[SEP]"",\n                ""pad_token"": ""[PAD]"",\n                ""cls_token"": ""[CLS]"",\n                ""mask_token"": ""[MASK]"",\n                ""tokenize_chinese_chars"": True,\n                ""do_lower_case"": True,\n                ""do_basic_tokenize"": True,\n                ""non_split_tokens"": None,\n                ""name"": ""bert_tokenizer"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained BERT model.\n\n        `""vocab_file""`: str or None\n            The path to a one-wordpiece-per-line vocabulary file.\n\n        `""max_len""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""unk_token""`: str\n            Unknown token.\n\n        `""sep_token""`: str\n            Separation token.\n\n        `""pad_token""`: str\n            Padding token.\n\n        `""cls_token""`: str\n            Classification token.\n\n        `""mask_token""`: str\n            Masking token.\n\n        `""tokenize_chinese_chars""`: bool\n            Whether to tokenize Chinese characters.\n\n        `""do_lower_case""`: bool\n            Whether to lower case the input\n            Only has an effect when `do_basic_tokenize=True`\n\n        `""do_basic_tokenize""`: bool\n            Whether to do basic tokenization before wordpiece.\n\n        `""non_split_tokens""`: list\n            List of tokens which will never be split during tokenization.\n            Only has an effect when `do_basic_tokenize=True`\n\n        `""name""`: str\n            Name of the tokenizer.\n\n        """"""\n        return {\n            \'pretrained_model_name\': \'bert-base-uncased\',\n            \'vocab_file\': None,\n            \'max_len\': 512,\n            \'unk_token\': \'[UNK]\',\n            \'sep_token\': \'[SEP]\',\n            \'pad_token\': \'[PAD]\',\n            \'cls_token\': \'[CLS]\',\n            \'mask_token\': \'[MASK]\',\n            \'tokenize_chinese_chars\': True,\n            \'do_lower_case\': True,\n            \'do_basic_tokenize\': True,\n            \'non_split_tokens\': None,\n            \'name\': \'bert_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained BERT tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'max_len\': 512,\n            \'unk_token\': \'[UNK]\',\n            \'sep_token\': \'[SEP]\',\n            \'pad_token\': \'[PAD]\',\n            \'cls_token\': \'[CLS]\',\n            \'mask_token\': \'[MASK]\',\n            \'tokenize_chinese_chars\': True,\n            \'do_lower_case\': True,\n            \'do_basic_tokenize\': True,\n            \'non_split_tokens\': None,\n        }\n'"
texar/tf/data/tokenizers/bert_tokenizer_utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils of pre-trained BERT tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_bert.py`\n""""""\n\nfrom typing import Dict, List, Optional\n\nimport collections\nimport unicodedata\n\n\n__all__ = [\n    ""load_vocab"",\n    ""BasicTokenizer"",\n    ""WordpieceTokenizer"",\n]\n\n\ndef load_vocab(vocab_file: str) -> Dict[str, int]:\n    r""""""Loads a vocabulary file into a dictionary.""""""\n    vocab: Dict[str, int] = collections.OrderedDict()\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\'\\n\')\n        vocab[token] = index\n    return vocab\n\n\nclass BasicTokenizer:\n    r""""""Runs basic tokenization (punctuation splitting, lower casing, etc.).\n\n    Args:\n        do_lower_case: Whether to lower case the input.\n        never_split: A list of tokens not to split.\n        tokenize_chinese_chars: Whether to tokenize Chinese characters.\n            This should likely be deactivated for Japanese:\n            see:\n            `https://github.com/huggingface/pytorch-pretrained-BERT/issues/328`\n    """"""\n\n    def __init__(self, do_lower_case: bool = True,\n                 never_split: Optional[List[str]] = None,\n                 tokenize_chinese_chars: bool = True):\n        if never_split is None:\n            never_split = []\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n        self.tokenize_chinese_chars = tokenize_chinese_chars\n\n    def tokenize(self, text: str,\n                 never_split: Optional[List[str]] = None) -> \\\n            List[str]:\n        r""""""Basic tokenization of a piece of text.\n\n        Split on white spaces only, for sub-word tokenization, see\n        WordPieceTokenizer.\n\n        Args:\n            text: An input string.\n            never_split: A list of tokens not to split.\n        """"""\n        never_split = self.never_split + (never_split\n                                          if never_split is not None else [])\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        # see: https://github.com/google-research/bert/blob/master/\n        # tokenization.py#L201\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    @classmethod\n    def _run_strip_accents(cls, text: str) -> str:\n        r""""""Strips accents from a piece of text.\n\n        Example:\n            accented_string = \'M\xc3\xa1laga\'\n            _run_strip_accents(accented_string)  # \'Malaga\'\n        """"""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    @classmethod\n    def _run_split_on_punc(cls, text: str,\n                           never_split: Optional[List[str]] = None) -> \\\n            List[str]:\n        r""""""Splits punctuation on a piece of text.\n\n        Example:\n            text = \'Texar-PyTorch is an open-source toolkit based on PyTorch.\'\n            _run_split_on_punc(text)\n            # [\'Texar\', \'-\', \'PyTorch is an open\', \'-\',\n            # \'source toolkit based on PyTorch\', \'.\']\n        """"""\n        if never_split is not None and text in never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text: str) -> str:\n        r""""""Adds whitespace around any CJK character.\n\n        Example:\n            text = \'\xe4\xbb\x8a\xe5\xa4\xa9\xe5\xa4\xa9\xe6\xb0\x94\xe4\xb8\x8d\xe9\x94\x99\'\n            _tokenize_chinese_chars(text)\n            # \' \xe4\xbb\x8a  \xe5\xa4\xa9  \xe5\xa4\xa9  \xe6\xb0\x94  \xe4\xb8\x8d  \xe9\x94\x99 \'\n        """"""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    @classmethod\n    def _is_chinese_char(cls, cp: int) -> bool:\n        r""""""Checks whether cp is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode\n        # block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean\n        # characters, despite its name. The modern Korean Hangul alphabet is a\n        # different block, as is Japanese Hiragana and Katakana. Those\n        # alphabets are used to write space-separated words, so they are not\n        # treated specially and handled like the all of the other languages.\n        if ((0x4E00 <= cp <= 0x9FFF) or\n                (0x3400 <= cp <= 0x4DBF) or\n                (0x20000 <= cp <= 0x2A6DF) or\n                (0x2A700 <= cp <= 0x2B73F) or\n                (0x2B740 <= cp <= 0x2B81F) or\n                (0x2B820 <= cp <= 0x2CEAF) or\n                (0xF900 <= cp <= 0xFAFF) or\n                (0x2F800 <= cp <= 0x2FA1F)):\n            return True\n\n        return False\n\n    @classmethod\n    def _clean_text(cls, text: str) -> str:\n        r""""""Performs invalid character removal and whitespace cleanup on text.\n\n        Example:\n            text = \'Texar-PyTorch\\tis an open-source\\ntoolkit based on PyTorch.\'\n            _clean_text(text)\n            # \'Texar-PyTorch is an open-source toolkit based on PyTorch.\'\n        """"""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer:\n    r""""""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab: Dict[str, int],\n                 unk_token: str,\n                 max_input_chars_per_word: int = 100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text: str) -> List[str]:\n        r""""""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n            input = ""unaffable""\n            output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n            text: A single token or whitespace separated tokens. This should\n                have already been passed through `BasicTokenizer`.\n\n        Returns:\n            A list of wordpiece tokens.\n        """"""\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            assert token is not None\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef whitespace_tokenize(text: str) -> List[str]:\n    r""""""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens: List[str] = text.split()\n    return tokens\n\n\ndef _is_whitespace(char: str) -> bool:\n    r""""""Checks whether `char` is a whitespace character.\n\n    Note: this function is not standard and should be considered for BERT\n    tokenization only. See the comments for more details.\n    """"""\n    # \\t, \\n, and \\r are technically control characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char in ("" "", ""\\t"", ""\\n"", ""\\r""):\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char: str) -> bool:\n    r""""""Checks whether `char` is a control character.\n\n    Note: this function is not standard and should be considered for BERT\n    tokenization only. See the comments for more details.\n    """"""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char in (""\\t"", ""\\n"", ""\\r""):\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char: str) -> bool:\n    r""""""Checks whether `char` is a punctuation character.\n\n    Note: this function is not standard and should be considered for BERT\n    tokenization only. See the comments for more details.\n    """"""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((33 <= cp <= 47) or (58 <= cp <= 64) or\n            (91 <= cp <= 96) or (123 <= cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
texar/tf/data/tokenizers/gpt2_tokenizer.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained GPT-2 tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_gpt2.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\nimport json\nimport regex as re\n\nfrom texar.tf.modules.pretrained.gpt2 import PretrainedGPT2Mixin\nfrom texar.tf.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.tf.data.tokenizers.gpt2_tokenizer_utils import \\\n    bytes_to_unicode, get_pairs\n\n__all__ = [\n    \'GPT2Tokenizer\',\n]\n\n\nclass GPT2Tokenizer(TokenizerBase, PretrainedGPT2Mixin):\n    r""""""Pre-trained GPT2 Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `117M`). Please refer to\n            :class:`~texar.torch.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n    _MAX_INPUT_SIZE = {\n        \'gpt2-small\': 1024,\n        \'gpt2-medium\': 1024,\n        \'gpt2-large\': 1024,\n        \'gpt2-xl\': 1024,\n    }\n    _DEPRECATED_MAX_INPUT_SIZE = {\n        \'117M\': 1024,\n        \'345M\': 1024,\n    }\n    _MAX_INPUT_SIZE.update(_DEPRECATED_MAX_INPUT_SIZE)\n\n    _VOCAB_FILE_NAMES = {\n        \'vocab_file\': \'encoder.json\',\n        \'merges_file\': \'vocab.bpe\',\n    }\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        super().__init__(hparams=None)\n\n        self.config = {\n            \'errors\': self.hparams[\'errors\']\n        }\n\n        if self.pretrained_model_dir is not None:\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_NAMES[\'vocab_file\'])\n            merges_file = os.path.join(self.pretrained_model_dir,\n                                       self._VOCAB_FILE_NAMES[\'merges_file\'])\n            assert pretrained_model_name is not None\n            if self._MAX_INPUT_SIZE.get(pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[pretrained_model_name]\n        else:\n            vocab_file = self.hparams[\'vocab_file\']\n            merges_file = self.hparams[\'merges_file\']\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path ""\n                             ""\'{}"".format(vocab_file))\n\n        if not os.path.isfile(merges_file):\n            raise ValueError(""Can\'t find a merges file at path ""\n                             ""\'{}"".format(merges_file))\n\n        with open(vocab_file) as fp:\n            self.encoder = json.load(fp)\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = self.hparams[""errors""]  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        with open(merges_file, encoding=\'utf-8\') as fp:\n            bpe_data = fp.read().split(\'\\n\')[1:-1]\n        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache: Dict[str, str] = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for\n        # capitalized versions of contractions\n        self.pat = re.compile(\n            r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?\n            [^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def _map_text_to_token(self, text: str) -> List[str]:  # type: ignore\n        r""""""Tokenize a string. """"""\n        bpe_tokens: List[str] = []\n        for token in re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(\n                bpe_token for bpe_token in self._bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def save_vocab(self, save_dir: str) -> Tuple[str, str]:\n        r""""""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Vocabulary path ({}) should be a ""\n                             ""directory"".format(save_dir))\n\n        vocab_file = os.path.join(save_dir,\n                                  self._VOCAB_FILE_NAMES[\'vocab_file\'])\n        merge_file = os.path.join(save_dir,\n                                  self._VOCAB_FILE_NAMES[\'merges_file\'])\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(),\n                                                  key=lambda kv: kv[1]):\n                if index != token_index:\n                    print(""Saving vocabulary to {}: BPE merge indices are ""\n                          ""not consecutive. Please check that the tokenizer ""\n                          ""is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        return (vocab_file, merge_file)\n\n    def _bpe(self, token: str) -> str:\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(\n                pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word: List[str] = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except ValueError:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 \\\n                        and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.encoder)\n\n    def _map_token_to_id(self, token: str) -> int:\n        r""""""Maps a token to an id using the vocabulary.""""""\n        return self.encoder.get(token, self.encoder.get(self.unk_token))\n\n    def _map_id_to_token(self, index: int) -> str:\n        r""""""Maps an id to a token using the vocabulary.""""""\n        token = self.decoder.get(index)\n        assert isinstance(token, str)\n        return token\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.""""""\n        text = \'\'.join(tokens)\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\n            \'utf-8\', errors=self.errors)\n        return text\n\n    def encode_text(  # type: ignore\n            self,\n            text: str,\n            max_seq_length: Optional[int] = None,\n            append_eos_token: bool = True) -> Tuple[List[int], int]:\n        r""""""Adds special tokens to a sequence and computes the corresponding\n        sequence length for GPT2 specific tasks. The sequence will be truncated\n        if its length is larger than ``max_seq_length``.\n\n        A GPT2 sequence has the following format:\n        `[bos_token]` X `[eos_token]` `[pad_token]`\n\n        Args:\n            text: Input text.\n            max_seq_length: Maximum sequence length.\n            append_eos_token: Whether to append ``eos_token`` after the\n                sequence.\n\n        Returns:\n            A tuple of `(input_ids, seq_len)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special tokens.\n            - ``seq_len``: The sequence length.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        token_ids = self.map_text_to_id(text)\n        assert isinstance(token_ids, list)\n\n        bos_token_id = self._map_token_to_id(self.bos_token)\n        eos_token_id = self._map_token_to_id(self.eos_token)\n        pad_token_id = self._map_token_to_id(self.pad_token)\n\n        if append_eos_token:\n            input_ids = token_ids[:max_seq_length - 2]\n            input_ids = [bos_token_id] + input_ids + [eos_token_id]\n        else:\n            input_ids = token_ids[:max_seq_length - 1]\n            input_ids = [bos_token_id] + input_ids\n\n        seq_len = len(input_ids)\n\n        # Pad up to the maximum sequence length.\n        input_ids = input_ids + [pad_token_id] * (max_seq_length - seq_len)\n\n        assert len(input_ids) == max_seq_length\n\n        return input_ids, seq_len\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""117M"",\n                ""vocab_file"": None,\n                ""merges_file"": None,\n                ""max_len"": 1024,\n                ""bos_token"": ""<|endoftext|>"",\n                ""eos_token"": ""<|endoftext|>"",\n                ""unk_token"": ""<|endoftext|>"",\n                ""pad_token"": ""<|endoftext|>"",\n                ""errors"": ""replace"",\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained GPT2 model.\n\n        `""vocab_file""`: str or None\n            The path to a vocabulary json file mapping tokens to ids.\n\n        `""merges_file""`: str or None\n            The path to a merges file.\n\n        `""max_len""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""bos_token""`: str\n            Beginning of sentence token\n\n        `""eos_token""`: str\n            End of sentence token\n\n        `""unk_token""`: str\n            Unknown token\n\n        `""pad_token""`: str\n            Padding token\n\n        `""errors""`: str\n            Response when mapping tokens to text fails. The possible values are\n            `ignore`, `replace`, and `strict`.\n\n        `""name""`: str\n            Name of the tokenizer.\n        """"""\n        return {\n            \'pretrained_model_name\': \'117M\',\n            \'vocab_file\': None,\n            \'merges_file\': None,\n            \'max_len\': 1024,\n            \'bos_token\': \'<|endoftext|>\',\n            \'eos_token\': \'<|endoftext|>\',\n            \'unk_token\': \'<|endoftext|>\',\n            \'pad_token\': \'<|endoftext|>\',\n            \'errors\': \'replace\',\n            \'name\': \'gpt2_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained GPT2 tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'merges_file\': None,\n            \'max_len\': 1024,\n            \'bos_token\': \'<|endoftext|>\',\n            \'eos_token\': \'<|endoftext|>\',\n            \'unk_token\': \'<|endoftext|>\',\n            \'pad_token\': \'<|endoftext|>\',\n            \'errors\': \'replace\',\n        }\n'"
texar/tf/data/tokenizers/gpt2_tokenizer_utils.py,0,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils of pre-trained GPT2 tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_gpt2.py`\n""""""\n\nfrom functools import lru_cache\n\n__all__ = [\n    ""bytes_to_unicode"",\n    ""get_pairs"",\n]\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    r""""""Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings. This means you need a\n    large number of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing\n    around 5K for decent coverage. This is a significant percentage of your\n    normal, say, 32K bpe vocab. To avoid that, we want lookup tables between\n    utf-8 bytes and unicode strings.\n\n    Note that this function avoids the mapping to whitespace and control\n    characters, which is designed specifically for GPT-2 BPE.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"") + 1)) + list(\n        range(ord(""\xc2\xa1""), ord(""\xc2\xac"") + 1)) + list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    r""""""Return set of symbol pairs in a word. Word is represented as tuple of\n    symbols (symbols being variable-length strings).\n\n    Example:\n        word = ""texar""\n        get_pairs(word)\n        # {(\'t\', \'e\'), (\'e\', \'x\'), (\'x\', \'a\'), (\'a\', \'r\')}\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n'"
texar/tf/data/tokenizers/tokenizer_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for all tokenizers.\n\nThe code structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_utils.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple, overload\n\nimport json\nimport os\nimport warnings\n\nfrom texar.tf.module_base import ModuleBase\n\n__all__ = [\n    ""TokenizerBase"",\n]\n\nSPECIAL_TOKENS_MAP_FILE = \'special_tokens_map.json\'\nADDED_TOKENS_FILE = \'added_tokens.json\'\nCONFIG_FILE = \'config.json\'\n\n\nclass TokenizerBase(ModuleBase):\n    r""""""Base class inherited by all tokenizer classes. This class\n    handles downloading and loading pre-trained tokenizer and adding tokens to\n    the vocabulary.\n\n    Derived class can set up a few special tokens to be used in common scripts\n    and internals: :attr:`bos_token`, :attr:`eos_token`, :attr:`unk_token`,\n    :attr:`sep_token`, :attr:`pad_token`, :attr:`cls_token`,\n    :attr:`mask_token`, and :attr:`additional_special_tokens`.\n\n    We defined an :attr:`added_tokens_encoder` to add new tokens to the\n    vocabulary without having to handle the specific vocabulary augmentation\n    methods of the various underlying dictionary structures (`BPE`,\n    `sentencepiece` ...).\n    """"""\n\n    _IS_PRETRAINED: bool\n    _MAX_INPUT_SIZE: Dict[str, Optional[int]]\n    _VOCAB_FILE_NAMES: Dict[str, str]\n    _SPECIAL_TOKENS_ATTRIBUTES = [""bos_token"", ""eos_token"", ""unk_token"",\n                                  ""sep_token"", ""pad_token"", ""cls_token"",\n                                  ""mask_token"", ""additional_special_tokens""]\n\n    def __init__(self, hparams):\n        super().__init__(hparams=hparams)\n\n        self.config = None\n\n        self.bos_token = None\n        self.eos_token = None\n        self.unk_token = None\n        self.sep_token = None\n        self.pad_token = None\n        self.cls_token = None\n        self.mask_token = None\n        self.additional_special_tokens = []\n\n        self.max_len = int(1e12)\n        self.added_tokens_encoder = {}\n        self.added_tokens_decoder = {}\n\n        for key, value in self.hparams.items():\n            if key in self._SPECIAL_TOKENS_ATTRIBUTES:\n                if key == \'additional_special_tokens\':\n                    assert isinstance(value, (list, tuple)) and \\\n                           all(isinstance(v, str) for v in value)\n                else:\n                    assert isinstance(value, str)\n                setattr(self, key, value)\n\n    @classmethod\n    def load(cls, pretrained_model_path: str, configs: Optional[Dict] = None):\n        r""""""Instantiate a tokenizer from the vocabulary files or the saved\n        tokenizer files.\n\n        Args:\n            pretrained_model_path: The path to a vocabulary file or a folder\n                that contains the saved pre-trained tokenizer files.\n            configs: Tokenizer configurations. You can overwrite the original\n                tokenizer configurations saved in the configuration file\n                by this dictionary.\n\n        Returns:\n            A tokenizer instance.\n        """"""\n        vocab_files = {}\n        # Look for the tokenizer main vocabulary files\n        for file_id, file_name in cls._VOCAB_FILE_NAMES.items():\n            full_file_name: Optional[str]\n            if os.path.isdir(pretrained_model_path):\n                # If a directory is provided we look for the standard file name\n                full_file_name = os.path.join(pretrained_model_path, file_name)\n            else:\n                # If a path to a file is provided we use it (will only work\n                # for non-BPE tokenizer using a single vocabulary file)\n                full_file_name = pretrained_model_path\n            if not os.path.exists(full_file_name):\n                print(""Can\'t find file {}. We won\'t load it."".format(\n                    full_file_name))\n                full_file_name = None\n            vocab_files[file_id] = full_file_name\n\n        # Look for the additional tokens files\n        all_vocab_files_names = {\n            \'added_tokens_file\': ADDED_TOKENS_FILE,\n            \'special_tokens_map_file\': SPECIAL_TOKENS_MAP_FILE,\n            \'config_file\': CONFIG_FILE}\n\n        # If a path to a file was provided, get the parent directory\n        saved_directory = pretrained_model_path\n        if os.path.exists(saved_directory) and not os.path.isdir(\n                saved_directory):\n            saved_directory = os.path.dirname(saved_directory)\n\n        for file_id, file_name in all_vocab_files_names.items():\n            full_file_name = os.path.join(saved_directory, file_name)\n            if not os.path.exists(full_file_name):\n                print(""Can\'t find file {}. We won\'t load it."".format(\n                    full_file_name))\n                full_file_name = None\n            vocab_files[file_id] = full_file_name\n\n        if all(full_file_name is None for full_file_name in\n               vocab_files.values()):\n            raise ValueError(""Can\'t find tokenizer files in {}."".format(\n                saved_directory))\n\n        kwargs: Dict[str, Any]\n        if cls._IS_PRETRAINED:\n            kwargs = {\'pretrained_model_name\': None}\n        else:\n            kwargs = {}\n\n        added_tokens_file = vocab_files.pop(\'added_tokens_file\', None)\n        special_tokens_map_file = vocab_files.pop(\n            \'special_tokens_map_file\', None)\n        tokenizer_config_file = vocab_files.pop(\'config_file\', None)\n\n        for args_name, file_path in vocab_files.items():\n            if args_name not in kwargs:\n                kwargs[args_name] = file_path\n\n        if special_tokens_map_file is not None:\n            with open(special_tokens_map_file, encoding=""utf-8"") as f:\n                special_tokens_map = json.load(f)\n            for key, value in special_tokens_map.items():\n                if key not in kwargs:\n                    kwargs[key] = value\n\n        if tokenizer_config_file is not None:\n            with open(tokenizer_config_file, encoding=""utf-8"") as f:\n                tokenizer_config = json.load(f)\n            for key, value in tokenizer_config.items():\n                if key not in kwargs:\n                    kwargs[key] = value\n\n        if configs is not None:\n            for key, value in configs.items():\n                kwargs[key] = value\n\n        tokenizer = cls(hparams=kwargs)\n\n        # Add supplementary tokens.\n        if added_tokens_file is not None:\n            with open(added_tokens_file, encoding=""utf-8"") as f:\n                added_tok_encoder = json.load(f)\n            added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n\n        return tokenizer\n\n    def save(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the tokenizer vocabulary files (with added tokens), tokenizer\n        configuration file and a dictionary mapping special token class\n        attributes (:attr:`cls_token`, :attr:`unk_token`, ...) to their values\n        (`<unk>`, `<cls>`, ...) to a directory, so that it can be re-loaded\n        using the :meth:`~load`.\n\n        Args:\n            save_dir: The path to a folder in which the tokenizer files\n                will be saved.\n\n        Return:\n            The paths to the vocabulary file, added token file, special token\n            mapping file, and the configuration file.\n        """"""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Saving directory ({}) should be a ""\n                             ""directory"".format(save_dir))\n\n        special_tokens_map_file = os.path.join(save_dir,\n                                               SPECIAL_TOKENS_MAP_FILE)\n        added_tokens_file = os.path.join(save_dir, ADDED_TOKENS_FILE)\n        config_file = os.path.join(save_dir, CONFIG_FILE)\n\n        with open(special_tokens_map_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n\n        with open(added_tokens_file, \'w\', encoding=\'utf-8\') as f:\n            if self.added_tokens_encoder:\n                out_str = json.dumps(self.added_tokens_encoder,\n                                     ensure_ascii=False)\n            else:\n                out_str = u""{}""\n            f.write(out_str)\n\n        with open(config_file, \'w\', encoding=\'utf-8\') as f:\n            if self.config:\n                out_str = json.dumps(self.config, ensure_ascii=False)\n            else:\n                out_str = u""{}""\n            f.write(out_str)\n\n        vocab_files = self.save_vocab(save_dir)\n        return vocab_files + (special_tokens_map_file, added_tokens_file,\n                              config_file)\n\n    def save_vocab(self, save_dir):\n        r""""""Save the tokenizer vocabulary to a directory. This method does not\n        save added tokens, special token mappings, and the configuration file.\n\n        Please use :meth:`~save` to save the full tokenizer state so\n        that it can be reloaded using :meth:`~load`.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def vocab_size(self) -> int:\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        return self.vocab_size + len(self.added_tokens_encoder)\n\n    def add_tokens(self, new_tokens: List[Optional[str]]) -> int:\n        r""""""Add a list of new tokens to the tokenizer class. If the new tokens\n        are not in the vocabulary, they are added to the\n        :attr:`added_tokens_encoder` with indices starting from the last index\n        of the current vocabulary.\n\n        Args:\n            new_tokens: A list of new tokens.\n\n        Returns:\n            Number of tokens added to the vocabulary which can be used to\n            correspondingly increase the size of the associated model embedding\n            matrices.\n        """"""\n        if not new_tokens:\n            return 0\n\n        to_add_tokens = []\n        for token in new_tokens:\n            assert isinstance(token, str)\n            if token != self.unk_token and \\\n                    (self.map_token_to_id(token) ==\n                     self.map_token_to_id(self.unk_token)):\n                to_add_tokens.append(token)\n\n        added_tok_encoder = dict((tok, len(self) + i) for i, tok in\n                                 enumerate(to_add_tokens))\n        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n        self.added_tokens_encoder.update(added_tok_encoder)\n        self.added_tokens_decoder.update(added_tok_decoder)\n\n        return len(to_add_tokens)\n\n    def add_special_tokens(self, special_tokens_dict: Dict[str, str]) -> int:\n        r""""""Add a dictionary of special tokens to the encoder and link them to\n        class attributes. If the special tokens are not in the vocabulary, they\n        are added to it and indexed starting from the last index of the\n        current vocabulary.\n\n        Args:\n            special_tokens_dict: A dictionary mapping special token class\n                attributes (:attr:`cls_token`, :attr:`unk_token`, ...) to their\n                values (`<unk>`, `<cls>`, ...).\n\n        Returns:\n            Number of tokens added to the vocabulary which can be used to\n            correspondingly increase the size of the associated model embedding\n            matrices.\n        """"""\n        if not special_tokens_dict:\n            return 0\n\n        added_tokens = 0\n        for key, value in special_tokens_dict.items():\n            assert key in self._SPECIAL_TOKENS_ATTRIBUTES\n            if key == \'additional_special_tokens\':\n                assert isinstance(value, (list, tuple)) and all(\n                    isinstance(t, str) for t in value)\n                added_tokens += self.add_tokens(value)\n            else:\n                assert isinstance(value, str)\n                added_tokens += self.add_tokens([value])\n            setattr(self, key, value)\n\n        return added_tokens\n\n    def map_text_to_token(self, text: Optional[str],\n                          **kwargs) -> List[str]:\n        r""""""Maps a string to a sequence of tokens (string), using the\n        tokenizer. Split in words for word-based vocabulary or sub-words for\n        sub-word-based vocabularies (`BPE`/`SentencePiece`/`WordPiece`).\n        This function also takes care of the added tokens.\n\n        Args:\n            text: A input string.\n\n        Return:\n            A list of tokens.\n        """"""\n\n        def split_on_tokens(tok_list, string):\n            if not string:\n                return []\n            if not tok_list:\n                return self._map_text_to_token(string, **kwargs)\n            tok = tok_list[0]\n            split_text = string.split(tok)\n            return sum((split_on_tokens(tok_list[1:], sub_text.strip()) + [tok]\n                        for sub_text in split_text), [])[:-1]\n\n        added_tokens = list(\n            self.added_tokens_encoder.keys()) + self.all_special_tokens\n        tokenized_text = split_on_tokens(added_tokens, text)\n        return tokenized_text\n\n    def _map_text_to_token(self, text: str, **kwargs) -> List[str]:\n        r""""""Maps a string to a sequence of tokens (string), using the\n        tokenizer. Split in words for word-based vocabulary or sub-words for\n        sub-word-based vocabularies (`BPE`/`SentencePiece`/`WordPiece`).\n        This function does not take care of the added tokens.\n        """"""\n        raise NotImplementedError\n\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=unused-argument,function-redefined\n\n    @overload\n    def map_token_to_id(self, tokens: str) -> int:\n        ...\n\n    @overload\n    def map_token_to_id(self, tokens: List[str]) -> List[int]:\n        ...\n\n    def map_token_to_id(self, tokens):\n        r""""""Maps a single token or a sequence of tokens to a integer id\n        (resp.) a sequence of ids, using the vocabulary.\n\n        Args:\n            tokens: A single token or a list of tokens.\n\n        Returns:\n            A single token id or a list of token ids.\n        """"""\n        if isinstance(tokens, str):\n            return self._map_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._map_token_to_id_with_added_voc(token))\n        if len(ids) > self.max_len:\n            warnings.warn(\n                ""Token indices sequence length is longer than the specified ""\n                ""maximum sequence length for this model ({} > {}). Running ""\n                ""this sequence through the model will result in indexing ""\n                ""errors"".format(len(ids), self.max_len), UserWarning)\n        return ids\n\n    # pylint: enable=unused-argument,function-redefined\n\n    def _map_token_to_id_with_added_voc(self, token: str) -> int:\n        if token in self.added_tokens_encoder:\n            return self.added_tokens_encoder[token]\n        return self._map_token_to_id(token)\n\n    def _map_token_to_id(self, token: str) -> int:\n        raise NotImplementedError\n\n    def map_text_to_id(self, text: str) -> List[int]:\n        r""""""Maps a string to a sequence of ids (integer), using the\n        tokenizer and vocabulary. Same as\n        `self.map_token_to_id(self.map_text_to_token(text))`.\n\n        Args:\n            text: A input string.\n\n        Returns:\n            A single token id or a list of token ids.\n        """"""\n        return self.map_token_to_id(self.map_text_to_token(text))\n\n    # TODO: Remove these once pylint supports function stubs.\n    # pylint: disable=unused-argument,function-redefined\n\n    @overload\n    def map_id_to_token(self, token_ids: int,\n                        skip_special_tokens: bool = False) -> str:\n        ...\n\n    @overload\n    def map_id_to_token(self, token_ids: List[int],\n                        skip_special_tokens: bool = False) -> List[str]:\n        ...\n\n    def map_id_to_token(self, token_ids, skip_special_tokens=False):\n        r""""""Maps a single id or a sequence of ids to a token (resp.) a\n        sequence of tokens, using the vocabulary and added tokens.\n\n        Args:\n            token_ids: A single token id or a list of token ids.\n            skip_special_tokens: Whether to skip the special tokens.\n\n        Returns:\n            A single token or a list of tokens.\n        """"""\n        if isinstance(token_ids, int):\n            if token_ids in self.added_tokens_decoder:\n                return self.added_tokens_decoder[token_ids]\n            else:\n                return self._map_id_to_token(token_ids)\n        tokens = []\n        for index in token_ids:\n            if index in self.all_special_ids and skip_special_tokens:\n                continue\n            if index in self.added_tokens_decoder:\n                tokens.append(self.added_tokens_decoder[index])\n            else:\n                tokens.append(self._map_id_to_token(index))\n        return tokens\n\n    # pylint: enable=unused-argument,function-redefined\n\n    def _map_id_to_token(self, token_id: int) -> str:\n        raise NotImplementedError\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.\n        The most simple way to do it is :python:`\' \'.join(tokens)`, but we\n        often want to remove sub-word tokenization artifacts at the same time.\n        """"""\n        raise NotImplementedError\n\n    def map_id_to_text(self, token_ids: List[int],\n                       skip_special_tokens: bool = False,\n                       clean_up_tokenization_spaces: bool = True) -> str:\n        r""""""Maps a sequence of ids (integer) to a string, using the\n        tokenizer and vocabulary with options to remove special tokens and\n        clean up tokenization spaces.\n\n        Args:\n            token_ids: A list of token ids.\n            skip_special_tokens: Whether to skip the special tokens.\n            clean_up_tokenization_spaces: Whether to clean up a list of simple\n                English tokenization artifacts like spaces before punctuations\n                and abbreviated forms.\n        """"""\n        filtered_tokens = self.map_id_to_token(\n            token_ids, skip_special_tokens=skip_special_tokens)\n        text = self.map_token_to_text(filtered_tokens)\n        if clean_up_tokenization_spaces:\n            text = self.clean_up_tokenization(text)\n        return text\n\n    def encode_text(self,\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None):\n        r""""""Adds special tokens to a sequence or sequence pair and computes\n        other information such as segment ids, input mask, and sequence length\n        for specific tasks.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def special_tokens_map(self) -> Dict[str, str]:\n        r""""""A dictionary mapping special token class attributes\n        (:attr:`cls_token`, :attr:`unk_token`, ...) to their values\n        (`<unk>`, `<cls>`, ...)\n        """"""\n        set_attr = {}\n        for attr in self._SPECIAL_TOKENS_ATTRIBUTES:\n            attr_value = getattr(self, attr)\n            if attr_value:\n                set_attr[attr] = attr_value\n        return set_attr\n\n    @property\n    def all_special_tokens(self) -> List[str]:\n        r""""""List all the special tokens (`<unk>`, `<cls>`, ...) mapped to class\n        attributes (:attr:`cls_token`, :attr:`unk_token`, ...).\n        """"""\n        all_toks: List[str] = []\n        set_attr = self.special_tokens_map\n        for attr_value in set_attr.values():\n            all_toks = all_toks + (\n                attr_value if isinstance(attr_value, (list, tuple)) else [\n                    attr_value])\n        all_toks = list(set(all_toks))\n        return all_toks\n\n    @property\n    def all_special_ids(self) -> List[int]:\n        r""""""List the vocabulary indices of the special tokens\n        (`<unk>`, `<cls>`, ...) mapped to class attributes\n        (:attr:`cls_token`, :attr:`unk_token`, ...).\n        """"""\n        all_toks = self.all_special_tokens\n        all_ids: List[int] = [self.map_token_to_id(t) for t in all_toks]\n        return all_ids\n\n    @staticmethod\n    def clean_up_tokenization(out_string: str) -> str:\n        r""""""Clean up a list of simple English tokenization artifacts like\n        spaces before punctuations and abbreviated forms.\n        """"""\n        out_string = out_string.replace(\' .\', \'.\').replace(\' ?\', \'?\'). \\\n            replace(\' !\', \'!\').replace(\' ,\', \',\').replace("" \' "", ""\'""). \\\n            replace("" n\'t"", ""n\'t"").replace("" \'m"", ""\'m""). \\\n            replace("" do not"", "" don\'t"").replace("" \'s"", ""\'s""). \\\n            replace("" \'ve"", ""\'ve"").replace("" \'re"", ""\'re"")\n        return out_string\n'"
texar/tf/data/tokenizers/xlnet_tokenizer.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained XLNet Tokenizer.\n\nCode structure adapted from:\n    `https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/tokenization_xlnet.py`\n""""""\n\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport os\nimport unicodedata\nfrom shutil import copyfile\nimport sentencepiece as spm\n\nfrom texar.tf.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.tf.data.tokenizers.tokenizer_base import TokenizerBase\nfrom texar.tf.utils.utils import truncate_seq_pair\n\n__all__ = [\n    ""XLNetTokenizer"",\n]\n\nSPIECE_UNDERLINE = u\'\xe2\x96\x81\'\n\nSEG_ID_A = 0\nSEG_ID_B = 1\nSEG_ID_CLS = 2\nSEG_ID_SEP = 3\nSEG_ID_PAD = 4\n\n\nclass XLNetTokenizer(PretrainedXLNetMixin, TokenizerBase):\n    r""""""Pre-trained XLNet Tokenizer.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name of\n            pre-trained model (e.g., `xlnet-base-uncased`). Please refer to\n            :class:`~texar.torch.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If None, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n    """"""\n\n    _IS_PRETRAINED = True\n    _MAX_INPUT_SIZE = {\n        \'xlnet-base-cased\': None,\n        \'xlnet-large-cased\': None,\n    }\n    _VOCAB_FILE_NAMES = {\'vocab_file\': \'spiece.model\'}\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        self.load_pretrained_config(pretrained_model_name, cache_dir, hparams)\n\n        super().__init__(hparams=None)\n\n        self.__dict__: Dict\n\n        self.config = {\n            \'do_lower_case\': self.hparams[\'do_lower_case\'],\n            \'remove_space\': self.hparams[\'remove_space\'],\n            \'keep_accents\': self.hparams[\'keep_accents\'],\n        }\n\n        if self.pretrained_model_dir is not None:\n            vocab_file = os.path.join(self.pretrained_model_dir,\n                                      self._VOCAB_FILE_NAMES[\'vocab_file\'])\n            assert pretrained_model_name is not None\n            if self._MAX_INPUT_SIZE.get(pretrained_model_name):\n                self.max_len = self._MAX_INPUT_SIZE[pretrained_model_name]\n        else:\n            vocab_file = self.hparams[\'vocab_file\']\n            if self.hparams.get(\'max_len\'):\n                self.max_len = self.hparams[\'max_len\']\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path ""\n                             ""\'{}"".format(vocab_file))\n\n        self.do_lower_case = self.hparams[""do_lower_case""]\n        self.remove_space = self.hparams[""remove_space""]\n        self.keep_accents = self.hparams[""keep_accents""]\n        self.vocab_file = vocab_file\n\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    # spm.SentencePieceProcessor() is a SwigPyObject object which cannot be\n    # pickled. We need to define __getstate__ here.\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[""sp_model""] = None\n        state[""vocab_file""] = None\n        return state, self.vocab_file\n\n    # spm.SentencePieceProcessor() is a SwigPyObject object which cannot be\n    # pickled. We need to define __setstate__ here.\n    def __setstate__(self, d):\n        self.__dict__, self.vocab_file = d\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    def _preprocess_text(self, inputs: str) -> str:\n        r""""""Pre-process the text, including removing space,\n        stripping accents, and lower-casing the text.\n        """"""\n        if self.remove_space:\n            outputs = \' \'.join(inputs.strip().split())\n        else:\n            outputs = inputs\n        outputs = outputs.replace(""``"", \'""\').replace(""\'\'"", \'""\')\n\n        if not self.keep_accents:\n            outputs = unicodedata.normalize(\'NFKD\', outputs)\n            outputs = \'\'.join([c for c in outputs if not\n            unicodedata.combining(c)])\n        if self.do_lower_case:\n            outputs = outputs.lower()\n\n        return outputs\n\n    def _map_text_to_token(self, text: str,  # type: ignore\n                           sample: bool = False) -> List[str]:\n        text = self._preprocess_text(text)\n        if not sample:\n            pieces = self.sp_model.EncodeAsPieces(text)\n        else:\n            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n\n        new_pieces: List[str] = []\n        for piece in pieces:\n            if len(piece) > 1 and piece[-1] == \',\' and piece[-2].isdigit():\n                cur_pieces = self.sp_model.EncodeAsPieces(\n                    piece[:-1].replace(SPIECE_UNDERLINE, \'\'))\n                if piece[0] != SPIECE_UNDERLINE and \\\n                        cur_pieces[0][0] == SPIECE_UNDERLINE:\n                    if len(cur_pieces[0]) == 1:\n                        cur_pieces = cur_pieces[1:]\n                    else:\n                        cur_pieces[0] = cur_pieces[0][1:]\n                cur_pieces.append(piece[-1])\n                new_pieces.extend(cur_pieces)\n            else:\n                new_pieces.append(piece)\n\n        return new_pieces\n\n    def save_vocab(self, save_dir: str) -> Tuple[str]:\n        r""""""Save the sentencepiece vocabulary (copy original file) to\n        a directory.\n        """"""\n        if not os.path.isdir(save_dir):\n            raise ValueError(""Vocabulary path ({}) should be a ""\n                             ""directory"".format(save_dir))\n        out_vocab_file = os.path.join(save_dir,\n                                      self._VOCAB_FILE_NAMES[\'vocab_file\'])\n\n        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n            copyfile(self.vocab_file, out_vocab_file)\n\n        return (out_vocab_file,)\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self.sp_model)\n\n    def _map_token_to_id(self, token: str) -> int:\n        return self.sp_model.PieceToId(token)\n\n    def _map_id_to_token(self, index: int) -> str:\n        token = self.sp_model.IdToPiece(index)\n        return token\n\n    def map_token_to_text(self, tokens: List[str]) -> str:\n        r""""""Maps a sequence of tokens (string) in a single string.""""""\n        out_string = \'\'.join(tokens).replace(SPIECE_UNDERLINE, \' \').strip()\n        return out_string\n\n    def encode_text(self,\n                    text_a: str,\n                    text_b: Optional[str] = None,\n                    max_seq_length: Optional[int] = None) -> \\\n            Tuple[List[int], List[int], List[int]]:\n        r""""""Adds special tokens to a sequence or sequence pair and computes the\n        corresponding segment ids and input mask for XLNet specific tasks.\n        The sequence will be truncated if its length is larger than\n        ``max_seq_length``.\n\n        A XLNet sequence has the following format:\n        X `[sep_token]` `[cls_token]`\n\n        A XLNet sequence pair has the following format:\n        `[cls_token]` A `[sep_token]` B `[sep_token]`\n\n        Args:\n            text_a: The first input text.\n            text_b: The second input text.\n            max_seq_length: Maximum sequence length.\n\n        Returns:\n            A tuple of `(input_ids, segment_ids, input_mask)`, where\n\n            - ``input_ids``: A list of input token ids with added\n              special token ids.\n            - ``segment_ids``: A list of segment ids.\n            - ``input_mask``: A list of mask ids. The mask has 1 for real\n              tokens and 0 for padding tokens. Only real tokens are\n              attended to.\n        """"""\n        if max_seq_length is None:\n            max_seq_length = self.max_len\n\n        cls_token_id = self._map_token_to_id(self.cls_token)\n        sep_token_id = self._map_token_to_id(self.sep_token)\n\n        token_ids_a = self.map_text_to_id(text_a)\n        assert isinstance(token_ids_a, list)\n\n        token_ids_b = None\n        if text_b:\n            token_ids_b = self.map_text_to_id(text_b)\n\n        if token_ids_b:\n            assert isinstance(token_ids_b, list)\n            # Modifies `token_ids_a` and `token_ids_b` in place so that the\n            # total length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            truncate_seq_pair(token_ids_a, token_ids_b, max_seq_length - 3)\n\n            input_ids = (token_ids_a + [sep_token_id] + token_ids_b +\n                         [sep_token_id] + [cls_token_id])\n            segment_ids = [SEG_ID_A] * (len(token_ids_a) + 1) + \\\n                          [SEG_ID_B] * (len(token_ids_b) + 1) + [SEG_ID_CLS]\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            token_ids = token_ids_a[:max_seq_length - 2]\n\n            input_ids = token_ids + [sep_token_id] + [cls_token_id]\n            segment_ids = [SEG_ID_A] * (len(input_ids) - 1) + [SEG_ID_CLS]\n\n        input_mask = [0] * len(input_ids)\n\n        # Zero-pad up to the maximum sequence length.\n        input_ids = [0] * (max_seq_length - len(input_ids)) + input_ids\n        input_mask = [1] * (max_seq_length - len(input_mask)) + input_mask\n        segment_ids = ([SEG_ID_PAD] * (max_seq_length - len(segment_ids)) +\n                       segment_ids)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        return input_ids, segment_ids, input_mask\n\n    @staticmethod\n    def default_hparams() -> Dict[str, Any]:\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The tokenizer is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the tokenizer is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the tokenizer is defined by the\n          configurations in `hparams`.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""xlnet-base-cased"",\n                ""vocab_file"": None,\n                ""max_len"": None,\n                ""bos_token"": ""<s>"",\n                ""eos_token"": ""</s>"",\n                ""unk_token"": ""<unk>"",\n                ""sep_token"": ""<sep>"",\n                ""pad_token"": ""<pad>"",\n                ""cls_token"": ""<cls>"",\n                ""mask_token"": ""<mask>"",\n                ""additional_special_tokens"": [""<eop>"", ""<eod>""],\n                ""do_lower_case"": False,\n                ""remove_space"": True,\n                ""keep_accents"": False,\n            }\n\n        Here:\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained XLNet model.\n\n        `""vocab_file""`: str or None\n            The path to a sentencepiece vocabulary file.\n\n        `""max_len""`: int or None\n            The maximum sequence length that this model might ever be used with.\n\n        `""bos_token""`: str\n            Beginning of sentence token.\n\n        `""eos_token""`: str\n            End of sentence token.\n\n        `""unk_token""`: str\n            Unknown token.\n\n        `""sep_token""`: str\n            Separation token.\n\n        `""pad_token""`: str\n            Padding token.\n\n        `""cls_token""`: str\n            Classification token.\n\n        `""mask_token""`: str\n            Masking token.\n\n        `""additional_special_tokens""`: list\n            A list of additional special tokens.\n\n        `""do_lower_case""`: bool\n            Whether to lower-case the text.\n\n        `""remove_space""`: bool\n            Whether to remove the space in the text.\n\n        `""keep_accents""`: bool\n            Whether to keep the accents in the text.\n\n        `""name""`: str\n            Name of the tokenizer.\n        """"""\n        return {\n            \'pretrained_model_name\': \'xlnet-base-cased\',\n            \'vocab_file\': None,\n            \'max_len\': None,\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'unk_token\': \'<unk>\',\n            \'sep_token\': \'<sep>\',\n            \'pad_token\': \'<pad>\',\n            \'cls_token\': \'<cls>\',\n            \'mask_token\': \'<mask>\',\n            \'additional_special_tokens\': [\'<eop>\', \'<eod>\'],\n            \'do_lower_case\': False,\n            \'remove_space\': True,\n            \'keep_accents\': False,\n            \'name\': \'xlnet_tokenizer\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name: str,\n                          cache_dir: str):\n        r""""""Returns the configuration of the pre-trained XLNet tokenizer.""""""\n        return {\n            \'vocab_file\': None,\n            \'max_len\': None,\n            \'bos_token\': \'<s>\',\n            \'eos_token\': \'</s>\',\n            \'unk_token\': \'<unk>\',\n            \'sep_token\': \'<sep>\',\n            \'pad_token\': \'<pad>\',\n            \'cls_token\': \'<cls>\',\n            \'mask_token\': \'<mask>\',\n            \'additional_special_tokens\': [\'<eop>\', \'<eod>\'],\n            \'do_lower_case\': False,\n            \'remove_space\': True,\n            \'keep_accents\': False,\n        }\n'"
texar/tf/models/seq2seq/__init__.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library seq2seq models.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.models.seq2seq.seq2seq_base import *\nfrom texar.tf.models.seq2seq.basic_seq2seq import *\n'"
texar/tf/models/seq2seq/basic_seq2seq.py,9,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe basic seq2seq model without attention.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.models.seq2seq.seq2seq_base import Seq2seqBase\nfrom texar.tf.modules.decoders.beam_search_decode import beam_search_decode\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.shapes import get_batch_size\n\n# pylint: disable=protected-access, too-many-arguments, unused-argument\n\n__all__ = [\n    ""BasicSeq2seq""\n]\n\n\nclass BasicSeq2seq(Seq2seqBase):\n    """"""The basic seq2seq model (without attention).\n\n    Example:\n\n        .. code-block:: python\n\n            model = BasicSeq2seq(data_hparams, model_hparams)\n            exor = tx.run.Executor(\n                model=model,\n                data_hparams=data_hparams,\n                config=run_config)\n            exor.train_and_evaluate(\n                max_train_steps=10000,\n                eval_steps=100)\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, data_hparams, hparams=None):\n        Seq2seqBase.__init__(self, data_hparams, hparams=hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        Same as :meth:`~texar.tf.models.Seq2seqBase.default_hparams` of\n        :class:`~texar.tf.models.Seq2seqBase`.\n        """"""\n        hparams = Seq2seqBase.default_hparams()\n        hparams.update({\n            ""name"": ""basic_seq2seq""\n        })\n        return hparams\n\n    def _build_decoder(self):\n        kwargs = {\n            ""vocab_size"": self._tgt_vocab.size,\n            ""hparams"": self._hparams.decoder_hparams.todict()\n        }\n        self._decoder = utils.check_or_get_instance(\n            self._hparams.decoder, kwargs,\n            [""texar.tf.modules"", ""texar.tf.custom""])\n\n    def _get_predictions(self, decoder_results, features, labels, loss=None):\n        preds = {}\n\n        preds.update(features)\n\n        if labels is not None:\n            preds.update(labels)\n\n        preds.update(utils.flatten_dict({\'decode\': decoder_results}))\n        preds[\'decode.outputs.sample\'] = self._tgt_vocab.map_ids_to_tokens(\n            preds[\'decode.outputs.sample_id\'])\n\n        if loss is not None:\n            preds[\'loss\'] = loss\n\n        return preds\n\n    def embed_source(self, features, labels, mode):\n        """"""Embeds the inputs.\n        """"""\n        return self._src_embedder(ids=features[""source_text_ids""], mode=mode)\n\n    def embed_target(self, features, labels, mode):\n        """"""Embeds the target inputs. Used in training.\n        """"""\n        return self._tgt_embedder(ids=labels[""target_text_ids""], mode=mode)\n\n    def encode(self, features, labels, mode):\n        """"""Encodes the inputs.\n        """"""\n        embedded_source = self.embed_source(features, labels, mode)\n\n        outputs, final_state = self._encoder(\n            embedded_source,\n            sequence_length=features[""source_length""],\n            mode=mode)\n\n        return {\'outputs\': outputs, \'final_state\': final_state}\n\n    def _connect(self, encoder_results, features, labels, mode):\n        """"""Transforms encoder final state into decoder initial state.\n        """"""\n        enc_state = encoder_results[""final_state""]\n        possible_kwargs = {\n            ""inputs"": enc_state,\n            ""batch_size"": get_batch_size(enc_state)\n        }\n        outputs = utils.call_function_with_redundant_kwargs(\n            self._connector._build, possible_kwargs)\n        return outputs\n\n    def _decode_train(self, initial_state, encoder_results, features,\n                      labels, mode):\n        return self._decoder(\n            initial_state=initial_state,\n            decoding_strategy=self._hparams.decoding_strategy_train,\n            inputs=self.embed_target(features, labels, mode),\n            sequence_length=labels[\'target_length\'] - 1,\n            mode=mode)\n\n    def _decode_infer(self, initial_state, encoder_results, features,\n                      labels, mode):\n        start_token = self._tgt_vocab.bos_token_id\n        start_tokens = tf.ones_like(features[\'source_length\']) * start_token\n\n        max_l = self._decoder.hparams.max_decoding_length_infer\n\n        if self._hparams.beam_search_width > 1:\n            return beam_search_decode(\n                decoder_or_cell=self._decoder,\n                embedding=self._tgt_embedder.embedding,\n                start_tokens=start_tokens,\n                end_token=self._tgt_vocab.eos_token_id,\n                beam_width=self._hparams.beam_search_width,\n                initial_state=initial_state,\n                max_decoding_length=max_l)\n        else:\n            return self._decoder(\n                initial_state=initial_state,\n                decoding_strategy=self._hparams.decoding_strategy_infer,\n                embedding=self._tgt_embedder.embedding,\n                start_tokens=start_tokens,\n                end_token=self._tgt_vocab.eos_token_id,\n                mode=mode)\n\n    def decode(self, encoder_results, features, labels, mode):\n        """"""Decodes.\n        """"""\n        initial_state = self._connect(encoder_results, features, labels, mode)\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            outputs, final_state, sequence_length = self._decode_infer(\n                initial_state, encoder_results, features, labels, mode)\n        else:\n            outputs, final_state, sequence_length = self._decode_train(\n                initial_state, encoder_results, features, labels, mode)\n\n        return {\'outputs\': outputs,\n                \'final_state\': final_state,\n                \'sequence_length\': sequence_length}\n'"
texar/tf/models/seq2seq/seq2seq_base.py,26,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for seq2seq models.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.models.model_base import ModelBase\nfrom texar.tf.losses.mle_losses import sequence_sparse_softmax_cross_entropy\nfrom texar.tf.data.data.paired_text_data import PairedTextData\nfrom texar.tf.core.optimization import get_train_op\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.variables import collect_trainable_variables\n\n# pylint: disable=too-many-instance-attributes, unused-argument,\n# pylint: disable=too-many-arguments, no-self-use\n\n__all__ = [\n    ""Seq2seqBase""\n]\n\n\nclass Seq2seqBase(ModelBase):\n    """"""Base class inherited by all seq2seq model classes.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, data_hparams, hparams=None):\n        ModelBase.__init__(self, hparams)\n\n        self._data_hparams = HParams(data_hparams,\n                                     PairedTextData.default_hparams())\n\n        self._src_vocab = None\n        self._tgt_vocab = None\n        self._src_embedder = None\n        self._tgt_embedder = None\n        self._connector = None\n        self._encoder = None\n        self._decoder = None\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""source_embedder"": ""WordEmbedder"",\n                ""source_embedder_hparams"": {},\n                ""target_embedder"": ""WordEmbedder"",\n                ""target_embedder_hparams"": {},\n                ""embedder_share"": True,\n                ""embedder_hparams_share"": True,\n                ""encoder"": ""UnidirectionalRNNEncoder"",\n                ""encoder_hparams"": {},\n                ""decoder"": ""BasicRNNDecoder"",\n                ""decoder_hparams"": {},\n                ""decoding_strategy_train"": ""train_greedy"",\n                ""decoding_strategy_infer"": ""infer_greedy"",\n                ""beam_search_width"": 0,\n                ""connector"": ""MLPTransformConnector"",\n                ""connector_hparams"": {},\n                ""optimization"": {},\n                ""name"": ""seq2seq"",\n            }\n\n        Here:\n\n        ""source_embedder"": str or class or instance\n            Word embedder for source text. Can be a class, its name or module\n            path, or a class instance.\n\n        ""source_embedder_hparams"": dict\n            Hyperparameters for constructing the source embedder. E.g.,\n            See :meth:`~texar.tf.modules.WordEmbedder.default_hparams` for\n            hyperparameters of :class:`~texar.tf.modules.WordEmbedder`. Ignored\n            if ""source_embedder"" is an instance.\n\n        ""target_embedder"", ""target_embedder_hparams"":\n            Same as ""source_embedder"" and ""source_embedder_hparams"" but for\n            target text embedder.\n\n        ""embedder_share"": bool\n            Whether to share the source and target embedder. If `True`,\n            source embedder will be used to embed target text.\n\n        ""embedder_hparams_share"": bool\n            Whether to share the embedder configurations. If `True`,\n            target embedder will be created with ""source_embedder_hparams"".\n            But the two embedders have different set of trainable variables.\n\n        ""encoder"", ""encoder_hparams"":\n            Same as ""source_embedder"" and ""source_embedder_hparams"" but for\n            encoder.\n\n        ""decoder"", ""decoder_hparams"":\n            Same as ""source_embedder"" and ""source_embedder_hparams"" but for\n            decoder.\n\n        ""decoding_strategy_train"": str\n            The decoding strategy in training mode. See\n            :meth:`~texar.tf.modules.RNNDecoderBase._build` for details.\n\n        ""decoding_strategy_infer"": str\n            The decoding strategy in eval/inference mode.\n\n        ""beam_search_width"": int\n            Beam width. If > 1, beam search is used in eval/inference mode.\n\n        ""connector"", ""connector_hparams"":\n            The connector class and hyperparameters. A connector transforms\n            an encoder final state to a decoder initial state.\n\n        ""optimization"": dict\n            Hyperparameters of optimizating the model. See\n            :func:`~texar.tf.core.default_optimization_hparams` for details.\n\n        ""name"": str\n            Name of the model.\n        """"""\n        hparams = ModelBase.default_hparams()\n        hparams.update({\n            ""name"": ""seq2seq"",\n            ""source_embedder"": ""WordEmbedder"",\n            ""source_embedder_hparams"": {},\n            ""target_embedder"": ""WordEmbedder"",\n            ""target_embedder_hparams"": {},\n            ""embedder_share"": True,\n            ""embedder_hparams_share"": True,\n            ""encoder"": ""UnidirectionalRNNEncoder"",\n            ""encoder_hparams"": {},\n            ""decoder"": ""BasicRNNDecoder"",\n            ""decoder_hparams"": {},\n            ""decoding_strategy_train"": ""train_greedy"",\n            ""decoding_strategy_infer"": ""infer_greedy"",\n            ""beam_search_width"": 0,\n            ""connector"": ""MLPTransformConnector"",\n            ""connector_hparams"": {},\n            ""optimization"": {}\n        })\n        return hparams\n\n    def _build_vocab(self):\n        self._src_vocab, self._tgt_vocab = PairedTextData.make_vocab(\n            self._data_hparams.source_dataset,\n            self._data_hparams.target_dataset)\n\n    def _build_embedders(self):\n        kwargs = {\n            ""vocab_size"": self._src_vocab.size,\n            ""hparams"": self._hparams.source_embedder_hparams.todict()\n        }\n        self._src_embedder = utils.check_or_get_instance(\n            self._hparams.source_embedder, kwargs,\n            [""texar.tf.modules"", ""texar.tf.custom""])\n\n        if self._hparams.embedder_share:\n            self._tgt_embedder = self._src_embedder\n        else:\n            kwargs = {\n                ""vocab_size"": self._tgt_vocab.size,\n            }\n            if self._hparams.embedder_hparams_share:\n                kwargs[""hparams""] = \\\n                        self._hparams.source_embedder_hparams.todict()\n            else:\n                kwargs[""hparams""] = \\\n                        self._hparams.target_embedder_hparams.todict()\n            self._tgt_embedder = utils.check_or_get_instance(\n                self._hparams.target_embedder, kwargs,\n                [""texar.tf.modules"", ""texar.tf.custom""])\n\n    def _build_encoder(self):\n        kwargs = {\n            ""hparams"": self._hparams.encoder_hparams.todict()\n        }\n        self._encoder = utils.check_or_get_instance(\n            self._hparams.encoder, kwargs,\n            [""texar.tf.modules"", ""texar.tf.custom""])\n\n    def _build_decoder(self):\n        raise NotImplementedError\n\n    def _build_connector(self):\n        kwargs = {\n            ""output_size"": self._decoder.state_size,\n            ""hparams"": self._hparams.connector_hparams.todict()\n        }\n        self._connector = utils.check_or_get_instance(\n            self._hparams.connector, kwargs,\n            [""texar.tf.modules"", ""texar.tf.custom""])\n\n    def get_loss(self, decoder_results, features, labels):\n        """"""Computes the training loss.\n        """"""\n        return sequence_sparse_softmax_cross_entropy(\n            labels=labels[\'target_text_ids\'][:, 1:],\n            logits=decoder_results[\'outputs\'].logits,\n            sequence_length=decoder_results[\'sequence_length\'])\n\n    def _get_predictions(self, decoder_results, features, labels, loss=None):\n        raise NotImplementedError\n\n    def _get_train_op(self, loss):\n        varlist = collect_trainable_variables(\n            [self._src_embedder, self._tgt_embedder, self._encoder,\n             self._connector, self._decoder])\n        return get_train_op(\n            loss, variables=varlist, hparams=self._hparams.optimization)\n\n    def _get_eval_metric_ops(self, decoder_results, features, labels):\n        return None\n\n    def embed_source(self, features, labels, mode):\n        """"""Embeds the inputs.\n        """"""\n        raise NotImplementedError\n\n    def embed_target(self, features, labels, mode):\n        """"""Embeds the target inputs. Used in training.\n        """"""\n        raise NotImplementedError\n\n    def encode(self, features, labels, mode):\n        """"""Encodes the inputs.\n        """"""\n        raise NotImplementedError\n\n    def _connect(self, encoder_results, features, labels, mode):\n        """"""Transforms encoder final state into decoder initial state.\n        """"""\n        raise NotImplementedError\n\n    def decode(self, encoder_results, features, labels, mode):\n        """"""Decodes.\n        """"""\n        raise NotImplementedError\n\n    def _build(self, features, labels, params, mode, config=None):\n        self._build_vocab()\n        self._build_embedders()\n        self._build_encoder()\n        self._build_decoder()\n        self._build_connector()\n\n        encoder_results = self.encode(features, labels, mode)\n        decoder_results = self.decode(encoder_results, features, labels, mode)\n\n        loss, train_op, preds, eval_metric_ops = None, None, None, None\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            preds = self._get_predictions(decoder_results, features, labels)\n        else:\n            loss = self.get_loss(decoder_results, features, labels)\n\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                train_op = self._get_train_op(loss)\n            if mode == tf.estimator.ModeKeys.EVAL:\n                eval_metric_ops = self._get_eval_metric_ops(\n                    decoder_results, features, labels)\n\n            preds = self._get_predictions(decoder_results, features, labels,\n                                          loss)\n\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=preds,\n            loss=loss,\n            train_op=train_op,\n            eval_metric_ops=eval_metric_ops)\n\n    def get_input_fn(self, mode, hparams=None):\n        """"""Creates an input function `input_fn` that provides input data\n        for the model in an :tf_main:`Estimator <estimator/Estimator>`.\n        See, e.g., :tf_main:`tf.estimator.train_and_evaluate\n        <estimator/train_and_evaluate>`.\n\n        Args:\n            mode: One of members in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`.\n            hparams: A `dict` or an :class:`~texar.tf.HParams` instance\n                containing the hyperparameters of\n                :class:`~texar.tf.data.PairedTextData`. See\n                :meth:`~texar.tf.data.PairedTextData.default_hparams` for the\n                the structure and default values of the hyperparameters.\n\n        Returns:\n            An input function that returns a tuple `(features, labels)`\n            when called. `features` contains data fields that are related\n            to source text, and `labels` contains data fields related\n            to target text. See :class:`~texar.tf.data.PairedTextData` for\n            all data fields.\n        """"""\n        def _input_fn():\n            data = PairedTextData(hparams)\n\n            iterator = data.dataset.make_initializable_iterator()\n            tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS,\n                                 iterator.initializer)\n\n            batch = iterator.get_next()\n\n            features, labels = {}, {}\n            for key, value in batch.items():\n                if key.startswith(\'source_\'):\n                    features[key] = value\n                else:\n                    labels[key] = value\n            return features, labels\n\n        return _input_fn\n'"
texar/tf/modules/classifiers/__init__.py,5,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library classifiers.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.classifiers.conv_classifiers import *\nfrom texar.tf.modules.classifiers.rnn_classifiers import *\nfrom texar.tf.modules.classifiers.bert_classifier import *\nfrom texar.tf.modules.classifiers.gpt2_classifier import *\nfrom texar.tf.modules.classifiers.xlnet_classifier import *\n'"
texar/tf/modules/classifiers/bert_classifier.py,29,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBERT classifiers.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.core.layers import get_layer\nfrom texar.tf.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.tf.modules.encoders.bert_encoder import BERTEncoder\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.modules.pretrained.bert import PretrainedBERTMixin\nfrom texar.tf.utils.utils import dict_fetch\n\n# pylint: disable=too-many-arguments, invalid-name, no-member,\n# pylint: disable=too-many-branches, too-many-locals, too-many-statements\n\n__all__ = [\n    ""BERTClassifier""\n]\n\n\nclass BERTClassifier(ClassifierBase, PretrainedBERTMixin):\n    r""""""Classifier based on BERT modules. Please see\n    :class:`~texar.tf.modules.PretrainedBERTMixin` for a brief description\n    of BERT.\n\n    This is a combination of the\n    :class:`~texar.tf.modules.BertEncoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.tf.modules.BERTEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``bert-base-uncased``). Please refer to\n            :class:`~texar.tf.modules.PretrainedBERTMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n    _ENCODER_CLASS = BERTEncoder\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n\n        super(BERTClassifier, self).__init__(hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            # Creates the underlying encoder\n            encoder_hparams = dict_fetch(\n                hparams, BERTEncoder.default_hparams())\n            if encoder_hparams is not None:\n                encoder_hparams[\'name\'] = None\n            self._encoder = BERTEncoder(\n                pretrained_model_name=pretrained_model_name,\n                cache_dir=cache_dir,\n                hparams=encoder_hparams)\n\n            # Creates an dropout layer\n            drop_kwargs = {""rate"": self._hparams.dropout}\n            layer_hparams = {""type"": ""Dropout"", ""kwargs"": drop_kwargs}\n            self._dropout_layer = get_layer(hparams=layer_hparams)\n\n            # Creates an additional classification layer if needed\n            self._num_classes = self._hparams.num_classes\n            if self._num_classes <= 0:\n                self._logit_layer = None\n            else:\n                logit_kwargs = self._hparams.logit_layer_kwargs\n                if logit_kwargs is None:\n                    logit_kwargs = {}\n                elif not isinstance(logit_kwargs, HParams):\n                    raise ValueError(\n                        ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n                else:\n                    logit_kwargs = logit_kwargs.todict()\n                logit_kwargs.update({""units"": self._num_classes})\n                if \'name\' not in logit_kwargs:\n                    logit_kwargs[\'name\'] = ""logit_layer""\n\n                layer_hparams = {""type"": ""Dense"", ""kwargs"": logit_kwargs}\n                self._logit_layer = get_layer(hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in BertEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": ""cls_time"",\n                ""max_seq_length"": None,\n                ""dropout"": 0.1,\n                ""name"": ""bert_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n        :class:`~texar.tf.modules.BertEncoder`.\n        See the :meth:`~texar.tf.modules.BertEncoder.default_hparams`.\n        An instance of BertEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional :tf_main:`Dense <layers/Dense>`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to `num_classes`.\n                Ignored if no extra logit layer is appended.\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the first time step (which is the `CLS` token).\n                  Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""max_seq_length""`: int, optional\n                Maximum possible length of input sequences. Required if\n                `clas_strategy` is `all_time`.\n\n            `""dropout""`: float\n                The dropout rate of the BERT encoder output.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n\n        hparams = BERTEncoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": None,\n            ""dropout"": 0.1,\n            ""name"": ""bert_classifier""\n        })\n        return hparams\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               segment_ids=None,\n               mode=None,\n               **kwargs):\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.tf.modules.BertEncoder`.\n\n        Args:\n            inputs: A 2D Tensor of shape `[batch_size, max_time]`,\n                containing the token ids of tokens in input sequences.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n            segment_ids (optional): A 2D Tensor of shape\n                `[batch_size, max_time]`, containing the segment ids\n                of tokens in input sequences. If `None` (default), a tensor\n                with all elements set to zero is used.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n            **kwargs: Keyword arguments.\n\n        Returns:\n            A tuple `(logits, pred)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ""clas_strategy""==""cls_time"" or ""all_time""\n\n                - If ""num_classes""==1, `logits` and `pred` are of both \\\n                  shape `[batch_size]`\n                - If ""num_classes"">1, `logits` is of shape \\\n                  `[batch_size, num_classes]` and `pred` is of shape \\\n                  `[batch_size]`.\n\n            - If ""clas_strategy""==""time_wise"",\n\n                - If ""num_classes""==1, `logits` and `pred` are of both \\\n                  shape `[batch_size, max_time]`\n                - If ""num_classes"">1, `logits` is of shape \\\n                  `[batch_size, max_time, num_classes]` and `pred` is of shape \\\n                  `[batch_size, max_time]`.\n        """"""\n\n        enc_outputs, pooled_output = self._encoder(inputs, sequence_length,\n                                                   segment_ids, mode)\n\n        # Compute logits\n        stra = self._hparams.clas_strategy\n        if stra == \'time_wise\':\n            logits = enc_outputs\n        elif stra == \'cls_time\':\n            logits = pooled_output\n        elif stra == \'all_time\':\n            # Pad `enc_outputs` to have max_seq_length before flatten\n            length_diff = self._hparams.max_seq_length - tf.shape(inputs)[1]\n            length_diff = tf.reshape(length_diff, [1, 1])\n            # Set `paddings = [[0, 0], [0, length_dif], [0, 0]]`\n            paddings = tf.pad(length_diff, paddings=[[1, 1], [1, 0]])\n            logit_input = tf.pad(enc_outputs, paddings=paddings)\n            logit_input_dim = self._hparams.hidden_size * \\\n                              self._hparams.max_seq_length\n            logits = tf.reshape(logit_input, [-1, logit_input_dim])\n        else:\n            raise ValueError(\'Unknown classification strategy: {}\'.format(stra))\n\n        if self._logit_layer is not None:\n            logits = self._dropout_layer(logits, training=mode)\n            logits = self._logit_layer(logits)\n\n        # Compute predications\n        num_classes = self._hparams.num_classes\n        is_binary = num_classes == 1\n        is_binary = is_binary or (num_classes <= 0 and logits.shape[-1] == 1)\n\n        if stra == \'time_wise\':\n            if is_binary:\n                pred = tf.squeeze(tf.greater(logits, 0), -1)\n                logits = tf.squeeze(logits, -1)\n            else:\n                pred = tf.argmax(logits, axis=-1)\n        else:\n            if is_binary:\n                pred = tf.greater(logits, 0)\n                logits = tf.reshape(logits, [-1])\n            else:\n                pred = tf.argmax(logits, axis=-1)\n            pred = tf.reshape(pred, [-1])\n        pred = tf.cast(pred, tf.int64)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            if self._logit_layer:\n                self._add_trainable_variable(\n                    self._logit_layer.trainable_variables)\n            self._built = True\n\n        return logits, pred\n'"
texar/tf/modules/classifiers/classifier_base.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for encoders.\n""""""\n\nfrom texar.tf.module_base import ModuleBase\n\n__all__ = [\n    ""ClassifierBase""\n]\n\n\nclass ClassifierBase(ModuleBase):\n    """"""Base class inherited by all classifier classes.\n    """"""\n\n    def __init__(self, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""classifier""\n        }\n\n    def _build(self, inputs, *args, **kwargs):\n        """"""Classifies the inputs.\n\n        Args:\n          inputs: Inputs to the classifier.\n          *args: Other arguments.\n          **kwargs: Keyword arguments.\n\n        Returns:\n          Classification results.\n        """"""\n        raise NotImplementedError\n'"
texar/tf/modules/classifiers/conv_classifiers.py,19,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious classifier classes.\n""""""\n\n# pylint: disable=not-context-manager, too-many-arguments, too-many-locals\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.exceptions import TexarError\nfrom texar.tf.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.tf.modules.encoders.conv_encoders import Conv1DEncoder\nfrom texar.tf.utils import utils\nfrom texar.tf.hyperparams import HParams\n\n__all__ = [\n    ""Conv1DClassifier""\n]\n\n\nclass Conv1DClassifier(ClassifierBase):\n    """"""Simple Conv-1D classifier.\n    This is a combination of the\n    :class:`~texar.tf.modules.Conv1DEncoder` with a classification layer.\n\n    Args:\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    Example:\n\n        .. code-block:: python\n\n            clas = Conv1DClassifier(hparams={\'num_classes\': 10})\n\n            inputs = tf.random_uniform([64, 20, 256])\n            logits, pred = clas(inputs)\n            # logits == Tensor of shape [64, 10]\n            # pred   == Tensor of shape [64]\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, hparams=None):\n        ClassifierBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            encoder_hparams = utils.dict_fetch(\n                hparams, Conv1DEncoder.default_hparams())\n            self._encoder = Conv1DEncoder(hparams=encoder_hparams)\n\n            # Add an additional dense layer if needed\n            self._num_classes = self._hparams.num_classes\n            if self._num_classes > 0:\n                if self._hparams.num_dense_layers <= 0:\n                    self._encoder.append_layer({""type"": ""Flatten""})\n\n                logit_kwargs = self._hparams.logit_layer_kwargs\n                if logit_kwargs is None:\n                    logit_kwargs = {}\n                elif not isinstance(logit_kwargs, HParams):\n                    raise ValueError(\n                        ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n                else:\n                    logit_kwargs = logit_kwargs.todict()\n                logit_kwargs.update({""units"": self._num_classes})\n                if \'name\' not in logit_kwargs:\n                    logit_kwargs[\'name\'] = ""logit_layer""\n\n                self._encoder.append_layer(\n                    {""type"": ""Dense"", ""kwargs"": logit_kwargs})\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in Conv1DEncoder\n                ...\n\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": {\n                    ""use_bias"": False\n                },\n                ""name"": ""conv1d_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in :class:`~texar.tf.modules.Conv1DEncoder`.\n        See the :meth:`~texar.tf.modules.Conv1DEncoder.default_hparams`.\n        An instance of Conv1DEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            ""num_classes"": int\n                Number of classes:\n\n                - If **`> 0`**, an additional :tf_main:`Dense <layers/Dense>` \\\n                layer is appended to the encoder to compute the logits over \\\n                classes.\n                - If **`<= 0`**, no dense layer is appended. The number of \\\n                classes is assumed to be the final dense layer size of the \\\n                encoder.\n\n            ""logit_layer_kwargs"": dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to ""num_classes"".\n                Ignored if no extra logit layer is appended.\n\n            ""name"": str\n                Name of the classifier.\n        """"""\n        hparams = Conv1DEncoder.default_hparams()\n        hparams.update({\n            ""name"": ""conv1d_classifier"",\n            ""num_classes"": 2,  # set to <=0 to avoid appending output layer\n            ""logit_layer_kwargs"": {""use_bias"": False}\n        })\n        return hparams\n\n    def _build(self,    # pylint: disable=arguments-differ\n               inputs,\n               sequence_length=None,\n               dtype=None,\n               mode=None):\n        """"""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in :class:`~texar.tf.modules.Conv1DEncoder`.\n\n        The predictions of binary classification (""num_classes""=1) and\n        multi-way classification (""num_classes"">1) are different, as explained\n        below.\n\n        Args:\n            inputs: The inputs to the network, which is a 3D tensor. See\n                :class:`~texar.tf.modules.Conv1DEncoder` for more details.\n            sequence_length (optional): An int tensor of shape `[batch_size]`\n                containing the length of each element in :attr:`inputs`.\n                If given, time steps beyond the length will first be masked out\n                before feeding to the layers.\n            dtype (optional): Type of the inputs. If not provided, infers\n                from inputs automatically.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`,\n                :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            A tuple `(logits, pred)`, where\n\n            - **`logits`** is a Tensor of shape `[batch_size, num_classes]`\\\n            for `num_classes` >1, and `[batch_size]` for `num_classes` =1 \\\n            (i.e., binary classification).\n            - **`pred`** is the prediction, a Tensor of shape `[batch_size]` \\\n            and type `tf.int64`. For binary classification, the standard \\\n            sigmoid function is used for prediction, and the class labels are \\\n            `{0, 1}`.\n        """"""\n        logits = self._encoder(inputs, sequence_length, dtype, mode)\n\n        num_classes = self._hparams.num_classes\n        is_binary = num_classes == 1\n        is_binary = is_binary or (num_classes <= 0 and logits.shape[1] == 1)\n\n        if is_binary:\n            pred = tf.greater(logits, 0)\n            logits = tf.reshape(logits, [-1])\n        else:\n            pred = tf.argmax(logits, 1)\n        pred = tf.cast(tf.reshape(pred, [-1]), tf.int64)\n\n        self._built = True\n\n        return logits, pred\n\n    @property\n    def trainable_variables(self):\n        """"""The list of trainable variables of the module.\n        """"""\n        if not self._built:\n            raise TexarError(\n                ""Attempting to access trainable_variables before module %s ""\n                ""was fully built. The module is built once it is called, ""\n                ""e.g., with `%s(...)`"" % (self.name, self.name))\n        return self._encoder.trainable_variables\n\n    @property\n    def num_classes(self):\n        """"""The number of classes.\n        """"""\n        return self._num_classes\n\n    @property\n    def nn(self):  # pylint: disable=invalid-name\n        """"""The classifier neural network.\n        """"""\n        return self._encoder\n\n    def has_layer(self, layer_name):\n        """"""Returns `True` if the network with the name exists. Returns `False`\n        otherwise.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._encoder.has_layer(layer_name)\n\n    def layer_by_name(self, layer_name):\n        """"""Returns the layer with the name. Returns \'None\' if the layer name\n        does not exist.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._encoder.layer_by_name(layer_name)\n\n    @property\n    def layers_by_name(self):\n        """"""A dictionary mapping layer names to the layers.\n        """"""\n        return self._encoder.layers_by_name\n\n    @property\n    def layers(self):\n        """"""A list of the layers.\n        """"""\n        return self._encoder.layers\n\n    @property\n    def layer_names(self):\n        """"""A list of uniquified layer names.\n        """"""\n        return self._encoder.layer_names\n\n    def layer_outputs_by_name(self, layer_name):\n        """"""Returns the output tensors of the layer with the specified name.\n        Returns `None` if the layer name does not exist.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._encoder.layer_outputs_by_name(layer_name)\n\n    @property\n    def layer_outputs(self):\n        """"""A list containing output tensors of each layer.\n        """"""\n        return self._encoder.layer_outputs\n'"
texar/tf/modules/classifiers/gpt2_classifier.py,30,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGPT2 classifiers.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.core.layers import get_layer\nfrom texar.tf.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.tf.modules.encoders.gpt2_encoder import GPT2Encoder\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.modules.pretrained.gpt2 import PretrainedGPT2Mixin\nfrom texar.tf.utils.utils import dict_fetch\n\n\n__all__ = [\n    ""GPT2Classifier"",\n]\n\n\nclass GPT2Classifier(ClassifierBase, PretrainedGPT2Mixin):\n    r""""""Classifier based on GPT2 modules. Please see\n    :class:`~texar.tf.modules.PretrainedGPT2Mixin` for a brief description\n    of GPT2.\n\n    This is a combination of the\n    :class:`~texar.tf.modules.GPT2Encoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.tf.modules.GPT2Encoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``gpt2-small``). Please refer to\n            :class:`~texar.tf.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            encoder_hparams = dict_fetch(\n                hparams, GPT2Encoder.default_hparams())\n            if encoder_hparams is not None:\n                encoder_hparams[\'name\'] = None\n\n            self._encoder = GPT2Encoder(\n                pretrained_model_name=pretrained_model_name,\n                cache_dir=cache_dir,\n                hparams=encoder_hparams)\n\n            # Creates an dropout layer\n            drop_kwargs = {""rate"": self._hparams.dropout}\n            layer_hparams = {""type"": ""Dropout"", ""kwargs"": drop_kwargs}\n            self._dropout_layer = get_layer(hparams=layer_hparams)\n\n            # Creates an additional classification layer if needed\n            self._num_classes = self._hparams.num_classes\n            if self._num_classes <= 0:\n                self._logit_layer = None\n            else:\n                logit_kwargs = self._hparams.logit_layer_kwargs\n                if logit_kwargs is None:\n                    logit_kwargs = {}\n                elif not isinstance(logit_kwargs, HParams):\n                    raise ValueError(\n                        ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n                else:\n                    logit_kwargs = logit_kwargs.todict()\n                logit_kwargs.update({""units"": self._num_classes})\n                if \'name\' not in logit_kwargs:\n                    logit_kwargs[\'name\'] = ""logit_layer""\n\n                layer_hparams = {""type"": ""Dense"", ""kwargs"": logit_kwargs}\n                self._logit_layer = get_layer(hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in GPT2Encoder\n                ...\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": `cls_time`,\n                ""max_seq_length"": None,\n                ""dropout"": 0.1,\n                ""name"": `gpt2_classifier`\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n        :class:`~texar.tf.modules.GPT2Encoder`.\n        See the :meth:`~texar.tf.modules.GPT2Encoder.default_hparams`.\n        An instance of GPT2Encoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional :tf_main:`Dense <layers/Dense>`\n                  layer is appended to the encoder to compute the logits over\n                  classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""`: dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to `num_classes`.\n                Ignored if no extra logit layer is appended.\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the first time step (which is the `CLS` token).\n                  Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""max_seq_length""`: int, optional\n                Maximum possible length of input sequences. Required if\n                `clas_strategy` is `all_time`.\n\n            `""dropout""`: float\n                The dropout rate of the BERT encoder output.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n        hparams = GPT2Encoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""cls_time"",\n            ""max_seq_length"": None,\n            ""dropout"": 0.1,\n            ""name"": ""gpt2_classifier""\n        })\n        return hparams\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               mode=None,\n               **kwargs):\n        r""""""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.tf.modules.GPT2Encoder`.\n\n        Args:\n            inputs: A 2D Tensor of shape `[batch_size, max_time]`,\n                containing the token ids of tokens in input sequences.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n            **kwargs: Keyword arguments.\n\n        Returns:\n            A tuple `(logits, pred)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ""clas_strategy""==""cls_time"" or ""all_time""\n\n                - If ""num_classes""==1, `logits` and `pred` are of both \\\n                  shape `[batch_size]`\n                - If ""num_classes"">1, `logits` is of shape \\\n                  `[batch_size, num_classes]` and `pred` is of shape \\\n                  `[batch_size]`.\n\n            - If ""clas_strategy""==""time_wise"",\n\n                - If ""num_classes""==1, `logits` and `pred` are of both \\\n                  shape `[batch_size, max_time]`\n                - If ""num_classes"">1, `logits` is of shape \\\n                  `[batch_size, max_time, num_classes]` and `pred` is of shape \\\n                  `[batch_size, max_time]`.\n        """"""\n        enc_outputs = self._encoder(inputs, sequence_length, mode)\n\n        # Compute logits\n        strategy = self._hparams.clas_strategy\n        if strategy == \'time_wise\':\n            logits = enc_outputs\n        elif strategy == ""cls_time"":\n            if sequence_length is None:\n                logits = enc_outputs[:, -1, :]\n            else:\n                logits = tf.stack([enc_outputs[batch_idx, time_idx - 1, :]\n                                   for batch_idx, time_idx in\n                                   enumerate(sequence_length)], axis=0)\n        elif strategy == ""all_time"":\n            # Pad `enc_outputs` to have max_seq_length before flatten\n            length_diff = self._hparams.max_seq_length - tf.shape(inputs)[1]\n            length_diff = tf.reshape(length_diff, [1, 1])\n            # Set `paddings = [[0, 0], [0, length_dif], [0, 0]]`\n            paddings = tf.pad(length_diff, paddings=[[1, 1], [1, 0]])\n            logit_input = tf.pad(enc_outputs, paddings=paddings)\n            logit_input_dim = (self._hparams.encoder.dim *\n                               self._hparams.max_seq_length)\n            logits = tf.reshape(logit_input, [-1, logit_input_dim])\n        else:\n            raise ValueError(\'Unknown classification strategy: {}\'.format(\n                strategy))\n\n        if self._logit_layer is not None:\n            logits = self._dropout_layer(logits, training=mode)\n            logits = self._logit_layer(logits)\n\n        # Compute predications\n        num_classes = self._hparams.num_classes\n        is_binary = num_classes == 1\n        is_binary = is_binary or (num_classes <= 0 and logits.shape[-1] == 1)\n\n        if strategy == \'time_wise\':\n            if is_binary:\n                pred = tf.squeeze(tf.greater(logits, 0), -1)\n                logits = tf.squeeze(logits, -1)\n            else:\n                pred = tf.argmax(logits, axis=-1)\n        else:\n            if is_binary:\n                pred = tf.greater(logits, 0)\n                logits = tf.reshape(logits, [-1])\n            else:\n                pred = tf.argmax(logits, axis=-1)\n            pred = tf.reshape(pred, [-1])\n        pred = tf.cast(pred, tf.int64)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            if self._logit_layer:\n                self._add_trainable_variable(\n                    self._logit_layer.trainable_variables)\n            self._built = True\n\n        return logits, pred\n'"
texar/tf/modules/classifiers/rnn_classifiers.py,31,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RNN classifiers.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.contrib.framework import nest\n\nfrom texar.tf.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.tf.modules.encoders.rnn_encoders import \\\n        UnidirectionalRNNEncoder, _forward_single_output_layer\nfrom texar.tf.core import layers\nfrom texar.tf.utils import utils, shapes\nfrom texar.tf.hyperparams import HParams\n\n# pylint: disable=too-many-arguments, invalid-name, no-member,\n# pylint: disable=too-many-branches, too-many-locals, too-many-statements\n\n__all__ = [\n    ""UnidirectionalRNNClassifier""\n]\n\n# def RNNClassifierBase(ClassifierBase):\n#    """"""Base class inherited by all RNN classifiers.\n#    """"""\n#\n#    def __init__(self, hparams=None):\n#        ClassifierBase.__init__(self, hparams)\n\n\nclass UnidirectionalRNNClassifier(ClassifierBase):\n    """"""One directional RNN classifier.\n    This is a combination of the\n    :class:`~texar.tf.modules.UnidirectionalRNNEncoder` with a classification\n    layer. Both step-wise classification and sequence-level classification\n    are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in\n    :class:`~texar.tf.modules.UnidirectionalRNNEncoder`.\n\n    Args:\n        cell: (RNNCell, optional) If not specified,\n            a cell is created as specified in :attr:`hparams[""rnn_cell""]`.\n        cell_dropout_mode (optional): A Tensor taking value of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, which\n            toggles dropout in the RNN cell (e.g., activates dropout in\n            TRAIN mode). If `None`, :func:`~texar.tf.global_mode` is used.\n            Ignored if :attr:`cell` is given.\n        output_layer (optional): An instance of\n            :tf_main:`tf.layers.Layer <layers/Layer>`. Applies to the RNN cell\n            output of each step. If `None` (default), the output layer is\n            created as specified in :attr:`hparams[""output_layer""]`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 cell=None,\n                 cell_dropout_mode=None,\n                 output_layer=None,\n                 hparams=None):\n        ClassifierBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            # Creates the underlying encoder\n            encoder_hparams = utils.dict_fetch(\n                hparams, UnidirectionalRNNEncoder.default_hparams())\n            if encoder_hparams is not None:\n                encoder_hparams[\'name\'] = None\n            self._encoder = UnidirectionalRNNEncoder(\n                cell=cell,\n                cell_dropout_mode=cell_dropout_mode,\n                output_layer=output_layer,\n                hparams=encoder_hparams)\n\n            # Creates an additional classification layer if needed\n            self._num_classes = self._hparams.num_classes\n            if self._num_classes <= 0:\n                self._logit_layer = None\n            else:\n                logit_kwargs = self._hparams.logit_layer_kwargs\n                if logit_kwargs is None:\n                    logit_kwargs = {}\n                elif not isinstance(logit_kwargs, HParams):\n                    raise ValueError(\n                        ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n                else:\n                    logit_kwargs = logit_kwargs.todict()\n                logit_kwargs.update({""units"": self._num_classes})\n                if \'name\' not in logit_kwargs:\n                    logit_kwargs[\'name\'] = ""logit_layer""\n\n                layer_hparams = {""type"": ""Dense"", ""kwargs"": logit_kwargs}\n                self._logit_layer = layers.get_layer(hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in UnidirectionalRNNEncoder\n                ...\n\n                # (2) Additional hyperparameters\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""clas_strategy"": ""final_time"",\n                ""max_seq_length"": None,\n                ""name"": ""unidirectional_rnn_classifier""\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n        :class:`~texar.tf.modules.UnidirectionalRNNEncoder`.\n        See the :meth:`~texar.tf.modules.UnidirectionalRNNEncoder.default_hparams`.\n        An instance of UnidirectionalRNNEncoder is created for feature\n        extraction.\n\n        2. Additional hyperparameters:\n\n            ""num_classes"": int\n                Number of classes:\n\n                - If **`> 0`**, an additional :tf_main:`Dense <layers/Dense>` \\\n                layer is appended to the encoder to compute the logits over \\\n                classes.\n                - If **`<= 0`**, no dense layer is appended. The number of \\\n                classes is assumed to be the final dense layer size of the \\\n                encoder.\n\n            ""logit_layer_kwargs"": dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to ""num_classes"".\n                Ignored if no extra logit layer is appended.\n\n            ""clas_strategy"": str\n                The classification strategy, one of:\n\n                - **""final_time""**: Sequence-leve classification based on \\\n                the output of the final time step. One sequence has one class.\n                - **""all_time""**: Sequence-level classification based on \\\n                the output of all time steps. One sequence has one class.\n                - **""time_wise""**: Step-wise classfication, i.e., make \\\n                classification for each time step based on its output.\n\n            ""max_seq_length"": int, optional\n                Maximum possible length of input sequences. Required if\n                ""clas_strategy"" is ""all_time"".\n\n            ""name"": str\n                Name of the classifier.\n        """"""\n        hparams = UnidirectionalRNNEncoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""final_time"",\n            ""max_seq_length"": None,\n            ""name"": ""unidirectional_rnn_classifier""\n        })\n        return hparams\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               initial_state=None,\n               time_major=False,\n               mode=None,\n               **kwargs):\n        """"""Feeds the inputs through the network and makes classification.\n\n        The arguments are the same as in\n        :class:`~texar.tf.modules.UnidirectionalRNNEncoder`.\n\n        Args:\n            inputs: A 3D Tensor of shape `[batch_size, max_time, dim]`.\n                The first two dimensions\n                `batch_size` and `max_time` may be exchanged if\n                `time_major=True` is specified.\n            sequence_length (optional): A 1D int tensor of shape `[batch_size]`.\n                Sequence lengths\n                of the batch inputs. Used to copy-through state and zero-out\n                outputs when past a batch element\'s sequence length.\n            initial_state (optional): Initial state of the RNN.\n            time_major (bool): The shape format of the :attr:`inputs` and\n                :attr:`outputs` Tensors. If `True`, these tensors are of shape\n                `[max_time, batch_size, depth]`. If `False` (default),\n                these tensors are of shape `[batch_size, max_time, depth]`.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. Controls output layer dropout\n                if the output layer is specified with :attr:`hparams`.\n                If `None` (default), :func:`texar.tf.global_mode()`\n                is used.\n            return_cell_output (bool): Whether to return the output of the RNN\n                cell. This is the results prior to the output layer.\n            **kwargs: Optional keyword arguments of\n                :tf_main:`tf.nn.dynamic_rnn <nn/dynamic_rnn>`,\n                such as `swap_memory`, `dtype`, `parallel_iterations`, etc.\n\n        Returns:\n            A tuple `(logits, pred)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ""clas_strategy""==""final_time"" or ""all_time""\n\n                - If ""num_classes""==1, `logits` and `pred` are of both \\\n                shape `[batch_size]`\n                - If ""num_classes"">1, `logits` is of shape \\\n                `[batch_size, num_classes]` and `pred` is of shape \\\n                `[batch_size]`.\n\n            - If ""clas_strategy""==""time_wise"",\n\n                - If ""num_classes""==1, `logits` and `pred` are of both \\\n                shape `[batch_size, max_time]`\n                - If ""num_classes"">1, `logits` is of shape \\\n                `[batch_size, max_time, num_classes]` and `pred` is of shape \\\n                `[batch_size, max_time]`.\n                - If `time_major` is `True`, the batch and time dimensions are\\\n                exchanged.\n        """"""\n        enc_outputs, _, enc_output_size = self._encoder(\n            inputs=inputs,\n            sequence_length=sequence_length,\n            initial_state=initial_state,\n            time_major=time_major,\n            mode=mode,\n            return_output_size=True,\n            **kwargs)\n\n        # Flatten enc_outputs\n        enc_outputs_flat = nest.flatten(enc_outputs)\n        enc_output_size_flat = nest.flatten(enc_output_size)\n        enc_output_dims_flat = [np.prod(xs) for xs in enc_output_size_flat]\n        enc_outputs_flat = [shapes.flatten(x, 2, xs) for x, xs\n                            in zip(enc_outputs_flat, enc_output_dims_flat)]\n        if len(enc_outputs_flat) == 1:\n            enc_outputs_flat = enc_outputs_flat[0]\n        else:\n            enc_outputs_flat = tf.concat(enc_outputs_flat, axis=2)\n\n        # Compute logits\n        stra = self._hparams.clas_strategy\n        if stra == \'time_wise\':\n            logits = enc_outputs_flat\n        elif stra == \'final_time\':\n            if time_major:\n                logits = enc_outputs_flat[-1, :, :]\n            else:\n                logits = enc_outputs_flat[:, -1, :]\n        elif stra == \'all_time\':\n            if self._logit_layer is None:\n                raise ValueError(\n                    \'logit layer must not be `None` if \'\n                    \'clas_strategy=""all_time"". Specify the logit layer by \'\n                    \'either passing the layer in the constructor or \'\n                    \'specifying the hparams.\')\n            if self._hparams.max_seq_length is None:\n                raise ValueError(\n                    \'hparams.max_seq_length must not be `None` if \'\n                    \'clas_strategy=""all_time""\')\n        else:\n            raise ValueError(\'Unknown classification strategy: {}\'.format(stra))\n\n        if self._logit_layer is not None:\n            logit_input_dim = np.sum(enc_output_dims_flat)\n            if stra == \'time_wise\':\n                logits, _ = _forward_single_output_layer(\n                    logits, logit_input_dim, self._logit_layer)\n            elif stra == \'final_time\':\n                logits = self._logit_layer(logits)\n            elif stra == \'all_time\':\n                # Pad `enc_outputs_flat` to have max_seq_length before flatten\n                length_diff = self._hparams.max_seq_length - tf.shape(inputs)[1]\n                length_diff = tf.reshape(length_diff, [1, 1])\n                # Set `paddings = [[0, 0], [0, length_dif], [0, 0]]`\n                paddings = tf.pad(length_diff, paddings=[[1, 1], [1, 0]])\n                logit_input = tf.pad(enc_outputs_flat, paddings=paddings)\n\n                logit_input_dim *= self._hparams.max_seq_length\n                logit_input = tf.reshape(logit_input, [-1, logit_input_dim])\n\n                logits = self._logit_layer(logit_input)\n\n        # Compute predications\n        num_classes = self._hparams.num_classes\n        is_binary = num_classes == 1\n        is_binary = is_binary or (num_classes <= 0 and logits.shape[-1] == 1)\n\n        if stra == \'time_wise\':\n            if is_binary:\n                pred = tf.squeeze(tf.greater(logits, 0), -1)\n                logits = tf.squeeze(logits, -1)\n            else:\n                pred = tf.argmax(logits, axis=-1)\n        else:\n            if is_binary:\n                pred = tf.greater(logits, 0)\n                logits = tf.reshape(logits, [-1])\n            else:\n                pred = tf.argmax(logits, axis=-1)\n            pred = tf.reshape(pred, [-1])\n        pred = tf.cast(pred, tf.int64)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            # Add trainable variables of `self._logit_layer`\n            # which may be constructed externally.\n            if self._logit_layer:\n                self._add_trainable_variable(\n                    self._logit_layer.trainable_variables)\n            self._built = True\n\n        return logits, pred\n\n    @property\n    def num_classes(self):\n        """"""The number of classes, specified in :attr:`hparams`.\n        """"""\n        return self._hparams.num_classes\n'"
texar/tf/modules/classifiers/xlnet_classifier.py,31,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nXLNet classifiers.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.core.layers import get_layer, get_initializer\nfrom texar.tf.modules.classifiers.classifier_base import ClassifierBase\nfrom texar.tf.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.tf.utils.utils import dict_fetch\n\n# pylint: disable=too-many-arguments, invalid-name, no-member,\n# pylint: disable=too-many-branches, too-many-locals, too-many-statements\n\n__all__ = [\n    ""XLNetClassifier""\n]\n\n\nclass XLNetClassifier(ClassifierBase, PretrainedXLNetMixin):\n    """"""Classifier based on XLNet modules. Please see\n    :class:`~texar.tf.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    This is a combination of the :class:`~texar.tf.modules.XLNetEncoder` with a\n    classification layer. Both step-wise classification and sequence-level\n    classification are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in :class:`~texar.tf.modules.XLNetEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.tf.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n        super(XLNetClassifier, self).__init__(hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            tf.get_variable_scope().set_initializer(\n                get_initializer(self._hparams.initializer))\n            # Creates the underlying encoder\n            encoder_hparams = dict_fetch(\n                hparams, XLNetEncoder.default_hparams())\n            if encoder_hparams is not None:\n                encoder_hparams[\'name\'] = ""encoder""\n            self._encoder = XLNetEncoder(\n                pretrained_model_name=pretrained_model_name,\n                cache_dir=cache_dir,\n                hparams=encoder_hparams)\n            if self._hparams.use_projection:\n                self.projection = get_layer(hparams={\n                    ""type"": ""Dense"",\n                    ""kwargs"": {\n                        ""units"": self._encoder.output_size\n                    }\n                })\n\n            # Creates an dropout layer\n            drop_kwargs = {""rate"": self._hparams.dropout}\n            layer_hparams = {""type"": ""Dropout"", ""kwargs"": drop_kwargs}\n            self._dropout_layer = get_layer(hparams=layer_hparams)\n\n            # Creates an additional classification layer if needed\n            self._num_classes = self._hparams.num_classes\n            if self._num_classes <= 0:\n                self._logit_layer = None\n            else:\n                logit_kwargs = self._hparams.logit_layer_kwargs\n                if logit_kwargs is None:\n                    logit_kwargs = {}\n                elif not isinstance(logit_kwargs, HParams):\n                    raise ValueError(\n                        ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n                else:\n                    logit_kwargs = logit_kwargs.todict()\n                logit_kwargs.update({""units"": self._num_classes})\n                if \'name\' not in logit_kwargs:\n                    logit_kwargs[\'name\'] = ""logit_layer""\n\n                layer_hparams = {""type"": ""Dense"", ""kwargs"": logit_kwargs}\n                self._logit_layer = get_layer(hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in XLNetEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""clas_strategy"": ""cls_time"",\n                ""use_projection"": True,\n                ""num_classes"": 2,\n                ""logit_layer_kwargs"": None,\n                ""name"": ""xlnet_classifier"",\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n            :class:`~texar.tf.modules.XLNetEncoder`.\n            See the :meth:`~texar.tf.modules.XLNetEncoder.default_hparams`.\n            An instance of XLNetEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""clas_strategy""`: str\n                The classification strategy, one of:\n\n                - **cls_time**: Sequence-level classification based on the\n                  output of the last time step (which is the `CLS` token).\n                  Each sequence has a class.\n                - **all_time**: Sequence-level classification based on\n                  the output of all time steps. Each sequence has a class.\n                - **time_wise**: Step-wise classification, i.e., make\n                  classification for each time step based on its output.\n\n            `""use_projection""`: bool\n                If `True`, an additional `Dense` layer is added after the\n                summary step.\n\n            `""num_classes""`: int\n                Number of classes:\n\n                - If **> 0**, an additional dense layer is appended to the\n                  encoder to compute the logits over classes.\n                - If **<= 0**, no dense layer is appended. The number of\n                  classes is assumed to be the final dense layer size of the\n                  encoder.\n\n            `""logit_layer_kwargs""` : dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to ""num_classes"".\n                Ignored if no extra logit layer is appended.\n\n            `""name""`: str\n                Name of the classifier.\n        """"""\n        hparams = XLNetEncoder.default_hparams()\n        hparams.update({\n            ""num_classes"": 2,\n            ""logit_layer_kwargs"": None,\n            ""clas_strategy"": ""cls_time"",\n            ""dropout"": 0.1,\n            ""use_projection"": True,\n            ""name"": ""xlnet_classifier""\n        })\n        return hparams\n\n    def param_groups(self, lr=None, lr_layer_scale=1.0,\n                     decay_base_params=False):\n        r""""""Create parameter groups for optimizers. When\n        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form\n        separate groups with different base learning rates.\n\n        This method should be called before applying gradients to the variables\n        through the optimizer. Particularly, after calling the optimizer\'s\n        `compute_gradients` method, the user can call this method to get\n        variable-specific learning rates for the network. The gradients for each\n        variables can then be scaled accordingly. These scaled gradients are\n        finally applied by calling optimizer\'s `apply_gradients` method.\n\n        Args:\n            lr (float): The learning rate. Can be omitted if\n                :attr:`lr_layer_decay_rate` is 1.0.\n            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer\n                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.\n            decay_base_params (bool): If `True`, treat non-layer parameters\n                (e.g. embeddings) as if they\'re in layer 0. If `False`, these\n                parameters are not scaled.\n\n        Returns: A dict mapping tensorflow variables to their learning rates.\n        """"""\n        vars_to_learning_rates = {}\n        if lr_layer_scale != 1.0:\n            if lr is None:\n                raise ValueError(\n                    ""lr must be specified when lr_layer_decay_rate is not 1.0"")\n\n            scope = self.variable_scope.name\n            projection_vars = tf.trainable_variables(scope=scope + ""/dense"")\n            logits_vars = tf.trainable_variables(\n                scope=self.variable_scope.name + ""/logit_layer"")\n            finetune_vars = projection_vars + logits_vars\n            for var in finetune_vars:\n                vars_to_learning_rates[var] = lr\n\n            vars_to_learning_rates.update(\n                self._encoder.param_groups(lr=lr,\n                                           lr_layer_scale=lr_layer_scale,\n                                           decay_base_params=decay_base_params))\n        else:\n            for variable in self.trainable_variables:\n                vars_to_learning_rates[variable] = lr\n\n        return vars_to_learning_rates\n\n    def _build(self, token_ids, segment_ids=None, input_mask=None, mode=None):\n        r""""""Feeds the inputs through the network and makes classification.\n\n        Args:\n            token_ids: Shape `[batch_size, max_time]`.\n            segment_ids: Shape `[batch_size, max_time]`.\n            input_mask: Float tensor of shape `[batch_size, max_time]`. Note\n                that positions with value 1 are masked out.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            A tuple `(logits, preds)`, containing the logits over classes and\n            the predictions, respectively.\n\n            - If ``clas_strategy`` is ``cls_time`` or ``all_time``:\n\n                - If ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, num_classes]`` and ``pred`` is of shape\n                  ``[batch_size]``.\n\n            - If ``clas_strategy`` is ``time_wise``:\n\n                - ``num_classes`` == 1, ``logits`` and ``pred`` are both of\n                  shape ``[batch_size, max_time]``.\n                - If ``num_classes`` > 1, ``logits`` is of shape\n                  ``[batch_size, max_time, num_classes]`` and ``pred`` is of\n                  shape ``[batch_size, max_time]``.\n        """"""\n        is_training = is_train_mode(mode)\n        output, _ = self._encoder(token_ids, segment_ids, input_mask=input_mask,\n                                  mode=mode)\n        strategy = self._hparams.clas_strategy\n        if strategy == ""time_wise"":\n            summary = output\n        elif strategy == ""cls_time"":\n            summary = output[:, -1]\n        elif strategy == ""all_time"":\n            length_diff = self._hparams.max_seq_len - tf.shape(token_ids)[1]\n            summary_input = tf.pad(output,\n                                   paddings=[[0, 0], [0, length_diff], [0, 0]])\n            summary_input_dim = \\\n                self._encoder.output_size * self._hparams.max_seq_len\n            summary = tf.reshape(summary_input, shape=[-1, summary_input_dim])\n        else:\n            raise ValueError(""Unknown classification strategy: {}""\n                             .format(strategy))\n\n        if self._hparams.use_projection:\n            summary = tf.tanh(self.projection(summary))\n        # summary: (batch_size, hidden_dim)\n        summary = self._dropout_layer(summary, training=is_training)\n\n        logits = (self._logit_layer(summary) if self._logit_layer is not None\n                  else summary)\n\n        # Compute predictions\n        num_classes = self._hparams.num_classes\n        is_binary = num_classes == 1 or (num_classes <= 0\n                                         and logits.shape[-1] == 1)\n\n        if strategy == ""time_wise"":\n            if is_binary:\n                pred = tf.squeeze(tf.greater(logits, 0), -1)\n                logits = tf.squeeze(logits, -1)\n            else:\n                pred = tf.argmax(logits, axis=-1)\n        else:\n            if is_binary:\n                pred = tf.greater(logits, 0)\n                logits = tf.reshape(logits, [-1])\n            else:\n                pred = tf.argmax(logits, axis=-1)\n            pred = tf.reshape(pred, [-1])\n\n        pred = tf.to_int64(pred)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            if self._logit_layer:\n                self._add_trainable_variable(\n                    self._logit_layer.trainable_variables)\n            self._built = True\n\n        return logits, pred\n'"
texar/tf/modules/connectors/__init__.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library connectors.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.connectors.connector_base import *\nfrom texar.tf.modules.connectors.connectors import *\n'"
texar/tf/modules/connectors/connector_base.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for connectors that transform inputs into specified output shape.\n""""""\n\nfrom texar.tf.module_base import ModuleBase\n\n__all__ = [\n    ""ConnectorBase""\n]\n\n\nclass ConnectorBase(ModuleBase):\n    """"""Base class inherited by all connector classes. A connector is to\n    transform inputs into outputs with any specified structure and shape.\n    For example, tranforming the final state of an encoder to the initial\n    state of a decoder, and performing stochastic sampling in between as\n    in Variational Autoencoders (VAEs).\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set `output_size` to `dim` to generate output of\n            shape `[batch_size, dim]`.\n            Can be an `int`, a tuple of `int`, a Tensorshape, or a tuple of\n            TensorShapes.\n            For example, to transform inputs to have decoder state size, set\n            `output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n\n    def __init__(self, output_size, hparams=None):\n        ModuleBase.__init__(self, hparams)\n        self._output_size = output_size\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""connector""\n        }\n\n    def _build(self, *args, **kwargs):\n        """"""Transforms inputs to outputs with specified shape.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def output_size(self):\n        """"""The output size.\n        """"""\n        return self._output_size\n'"
texar/tf/modules/connectors/connectors.py,38,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious connectors.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import distributions as tf_dstr\nfrom tensorflow.python.util import nest    # pylint: disable=E0611\n\nfrom texar.tf.modules.connectors.connector_base import ConnectorBase\nfrom texar.tf.core import layers\nfrom texar.tf.utils.utils import get_function, check_or_get_instance\n\n# pylint: disable=too-many-locals, arguments-differ\n# pylint: disable=too-many-arguments, invalid-name, no-member\n\n__all__ = [\n    ""ConstantConnector"",\n    ""ForwardConnector"",\n    ""MLPTransformConnector"",\n    ""ReparameterizedStochasticConnector"",\n    ""StochasticConnector"",\n    # ""ConcatConnector""\n]\n\n\ndef _assert_same_size(outputs, output_size):\n    """"""Check if outputs match output_size\n\n    Args:\n        outputs: A Tensor or a (nested) tuple of tensors\n        output_size: Can be an Integer, a TensorShape, or a (nested) tuple of\n            Integers or TensorShape.\n    """"""\n    nest.assert_same_structure(outputs, output_size)\n    flat_output_size = nest.flatten(output_size)\n    flat_output = nest.flatten(outputs)\n\n    for (output, size) in zip(flat_output, flat_output_size):\n        if isinstance(size, tf.TensorShape):\n            if output.shape == size:\n                pass\n        elif output[0].shape != tf.TensorShape(size):\n            raise ValueError(\n                ""The output size does not match the the required output_size"")\n\n\ndef _get_tensor_depth(x):\n    """"""Returns the size of a tensor excluding the first dimension\n    (typically the batch dimension).\n\n    Args:\n        x: A tensor.\n    """"""\n    return np.prod(x.get_shape().as_list()[1:])\n\n\ndef _mlp_transform(inputs, output_size, activation_fn=tf.identity):\n    """"""Transforms inputs through a fully-connected layer that creates the output\n    with specified size.\n\n    Args:\n        inputs: A Tensor of shape `[batch_size, d1, ..., dn]`, or a (nested)\n            tuple of such elements. The dimensions `d1, ..., dn` will be flatten\n            and transformed by a dense layer.\n        output_size: Can be an Integer, a TensorShape, or a (nested) tuple of\n            Integers or TensorShape.\n        activation_fn: Activation function applied to the output.\n\n    Returns:\n        If :attr:`output_size` is an Integer or a TensorShape, returns a Tensor\n        of shape `[batch_size x output_size]`. If :attr:`output_size` is a tuple\n        of Integers or TensorShape, returns a tuple having the same structure as\n        :attr:`output_size`, where each element Tensor has the same size as\n        defined in :attr:`output_size`.\n    """"""\n    # Flatten inputs\n    flat_input = nest.flatten(inputs)\n    dims = [_get_tensor_depth(x) for x in flat_input]\n    flat_input = [tf.reshape(x, ([-1, d])) for x, d in zip(flat_input, dims)]\n    concat_input = tf.concat(flat_input, 1)\n\n    # Get output dimension\n    flat_output_size = nest.flatten(output_size)\n    if isinstance(flat_output_size[0], tf.TensorShape):\n        size_list = [0] * len(flat_output_size)\n        for (i, shape) in enumerate(flat_output_size):\n            size_list[i] = np.prod([dim.value for dim in shape])\n    else:\n        size_list = flat_output_size\n    sum_output_size = sum(size_list)\n\n    # fc_output = tf.contrib.layers.fully_connected(\n    #    concat_input, sum_output_size, activation_fn=activation_fn)\n    fc_output = tf.layers.dense(\n        concat_input, sum_output_size, activation=activation_fn)\n\n    flat_output = tf.split(fc_output, size_list, axis=1)\n\n    if isinstance(flat_output_size[0], tf.TensorShape):\n        for (i, shape) in enumerate(flat_output_size):\n            flat_output[i] = tf.reshape(flat_output[i], [-1] + shape.as_list())\n    output = nest.pack_sequence_as(structure=output_size,\n                                   flat_sequence=flat_output)\n\n    return output\n\n\nclass ConstantConnector(ConnectorBase):\n    """"""Creates a constant Tensor or (nested) tuple of Tensors that\n    contains a constant value.\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set `output_size` to `dim` to generate output of\n            shape `[batch_size, dim]`.\n            Can be an `int`, a tuple of `int`, a Tensorshape, or a tuple of\n            TensorShapes.\n            For example, to transform inputs to have decoder state size, set\n            `output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    This connector does not have trainable parameters.\n    See :meth:`_build` for the inputs and outputs of the connector.\n\n    Example:\n\n        .. code-block:: python\n\n            connector = Connector(cell.state_size)\n            zero_state = connector(batch_size=64, value=0.)\n            one_state = connector(batch_size=64, value=1.)\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n    def __init__(self, output_size, hparams=None):\n        ConnectorBase.__init__(self, output_size, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""value"": 0.,\n                ""name"": ""constant_connector""\n            }\n\n        Here:\n\n        ""value"": float\n            The constant scalar that the output tensor(s) has. Ignored if\n            `value` is given to :meth:`_build`.\n\n        ""name"": str\n            Name of the connector.\n        """"""\n        return {\n            ""value"": 0.,\n            ""name"": ""constant_connector""\n        }\n\n    def _build(self, batch_size, value=None):\n        """"""Creates output tensor(s) that has the given value.\n\n        Args:\n            batch_size: An `int` or `int` scalar Tensor, the batch size.\n            value (optional): A scalar, the value that\n                the output tensor(s) has. If `None`, ""value"" in :attr:`hparams`\n                is used.\n\n        Returns:\n            A (structure of) tensor whose structure is the same as\n            :attr:`output_size`, with value speicified by\n            `value` or :attr:`hparams`.\n        """"""\n        value_ = value\n        if value_ is None:\n            value_ = self.hparams.value\n        output = nest.map_structure(\n            lambda x: tf.constant(value_, shape=[batch_size, x]),\n            self._output_size)\n\n        self._built = True\n\n        return output\n\n\nclass ForwardConnector(ConnectorBase):\n    """"""Transforms inputs to have specified structure.\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set `output_size` to `dim` to generate output of\n            shape `[batch_size, dim]`.\n            Can be an `int`, a tuple of `int`, a Tensorshape, or a tuple of\n            TensorShapes.\n            For example, to transform inputs to have decoder state size, set\n            `output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    This connector does not have trainable parameters.\n    See :meth:`_build` for the inputs and outputs of the connector.\n\n    The input to the connector must have the same structure with\n    :attr:`output_size`, or must have the same number of elements and be\n    re-packable into the structure of :attr:`output_size`. Note that if input\n    is or contains a `dict` instance, the keys will be sorted to pack in\n    deterministic order (See\n    :tf_main:`pack_sequence_as <contrib/framework/nest/pack_sequence_as>`\n    for more details).\n\n    Example:\n\n        .. code-block:: python\n\n            cell = LSTMCell(num_units=256)\n            # cell.state_size == LSTMStateTuple(c=256, h=256)\n\n            connector = ForwardConnector(cell.state_size)\n            output = connector([tensor_1, tensor_2])\n            # output == LSTMStateTuple(c=tensor_1, h=tensor_2)\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, output_size, hparams=None):\n        ConnectorBase.__init__(self, output_size, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""forward_connector""\n            }\n\n        Here:\n\n        ""name"": str\n            Name of the connector.\n        """"""\n        return {\n            ""name"": ""forward_connector""\n        }\n\n    def _build(self, inputs):\n        """"""Transforms inputs to have the same structure as with\n        :attr:`output_size`. Values of the inputs are not changed.\n\n        :attr:`inputs` must either have the same structure, or have the same\n        number of elements with :attr:`output_size`.\n\n        Args:\n            inputs: The input (structure of) tensor to pass forward.\n\n        Returns:\n            A (structure of) tensors that re-packs `inputs` to have\n            the specified structure of `output_size`.\n        """"""\n        output = inputs\n        try:\n            nest.assert_same_structure(inputs, self._output_size)\n        except (ValueError, TypeError):\n            flat_input = nest.flatten(inputs)\n            output = nest.pack_sequence_as(\n                self._output_size, flat_input)\n\n        self._built = True\n\n        return output\n\n\nclass MLPTransformConnector(ConnectorBase):\n    """"""Transforms inputs with an MLP layer and packs the results into the\n    specified structure and size.\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set `output_size` to `dim` to generate output of\n            shape `[batch_size, dim]`.\n            Can be an `int`, a tuple of `int`, a Tensorshape, or a tuple of\n            TensorShapes.\n            For example, to transform inputs to have decoder state size, set\n            `output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs of the connector.\n\n    The input to the connector can have arbitrary structure and size.\n\n    Example:\n\n        .. code-block:: python\n\n            cell = LSTMCell(num_units=256)\n            # cell.state_size == LSTMStateTuple(c=256, h=256)\n\n            connector = MLPTransformConnector(cell.state_size)\n            inputs = tf.zeros([64, 10])\n            output = connector(inputs)\n            # output == LSTMStateTuple(c=tensor_of_shape_(64, 256),\n            #                          h=tensor_of_shape_(64, 256))\n\n        .. code-block:: python\n\n            ## Use to connect encoder and decoder with different state size\n            encoder = UnidirectionalRNNEncoder(...)\n            _, final_state = encoder(inputs=...)\n\n            decoder = BasicRNNDecoder(...)\n            connector = MLPTransformConnector(decoder.state_size)\n\n            _ = decoder(\n                initial_state=connector(final_state),\n                ...)\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, output_size, hparams=None):\n        ConnectorBase.__init__(self, output_size, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""activation_fn"": ""identity"",\n                ""name"": ""mlp_connector""\n            }\n\n        Here:\n\n        ""activation_fn"": str or callable\n            The activation function applied to the outputs of the MLP\n            transformation layer. Can\n            be a function, or its name or module path.\n\n        ""name"": str\n            Name of the connector.\n        """"""\n        return {\n            ""activation_fn"": ""identity"",\n            ""name"": ""mlp_connector""\n        }\n\n    def _build(self, inputs):\n        """"""Transforms inputs with an MLP layer and packs the results to have\n        the same structure as specified by :attr:`output_size`.\n\n        Args:\n            inputs: Input (structure of) tensors to be transformed. Must be a\n                Tensor of shape `[batch_size, ...]` or a (nested) tuple of\n                such Tensors. That is, the first dimension of (each) tensor\n                must be the batch dimension.\n\n        Returns:\n            A Tensor or a (nested) tuple of Tensors of the same structure of\n            `output_size`.\n        """"""\n        activation_fn = layers.get_activation_fn(self.hparams.activation_fn)\n\n        output = _mlp_transform(inputs, self._output_size, activation_fn)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return output\n\n\nclass ReparameterizedStochasticConnector(ConnectorBase):\n    """"""Samples from a distribution with reparameterization trick, and\n    transforms samples into specified size.\n\n    Reparameterization allows gradients to be back-propagated through the\n    stochastic samples. Used in, e.g., Variational Autoencoders (VAEs).\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set `output_size` to `dim` to generate output of\n            shape `[batch_size, dim]`.\n            Can be an `int`, a tuple of `int`, a Tensorshape, or a tuple of\n            TensorShapes.\n            For example, to transform inputs to have decoder state size, set\n            `output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    Example:\n\n        .. code-block:: python\n\n            cell = LSTMCell(num_units=256)\n            # cell.state_size == LSTMStateTuple(c=256, h=256)\n\n            connector = ReparameterizedStochasticConnector(cell.state_size)\n\n            kwargs = {\n                \'loc\': tf.zeros([batch_size, 10]),\n                \'scale_diag\': tf.ones([batch_size, 10])\n            }\n            output, sample = connector(distribution_kwargs=kwargs)\n            # output == LSTMStateTuple(c=tensor_of_shape_(batch_size, 256),\n            #                          h=tensor_of_shape_(batch_size, 256))\n            # sample == Tensor([batch_size, 10])\n\n\n            kwargs = {\n                \'loc\': tf.zeros([10]),\n                \'scale_diag\': tf.ones([10])\n            }\n            output_, sample_ = connector(distribution_kwargs=kwargs,\n                                         num_samples=batch_size_)\n            # output_ == LSTMStateTuple(c=tensor_of_shape_(batch_size_, 256),\n            #                           h=tensor_of_shape_(batch_size_, 256))\n            # sample == Tensor([batch_size_, 10])\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, output_size, hparams=None):\n        ConnectorBase.__init__(self, output_size, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""activation_fn"": ""identity"",\n                ""name"": ""reparameterized_stochastic_connector""\n            }\n\n        Here:\n\n        ""activation_fn"": str\n            The activation function applied to the outputs of the MLP\n            transformation layer. Can\n            be a function, or its name or module path.\n\n        ""name"": str\n            Name of the connector.\n        """"""\n        return {\n            ""activation_fn"": ""tensorflow.identity"",\n            ""name"": ""reparameterized_stochastic_connector""\n        }\n\n    def _build(self,\n               distribution=\'MultivariateNormalDiag\',\n               distribution_kwargs=None,\n               transform=True,\n               num_samples=None):\n        """"""Samples from a distribution and optionally performs transformation\n        with an MLP layer.\n\n        The distribution must be reparameterizable, i.e.,\n        `distribution.reparameterization_type = FULLY_REPARAMETERIZED`.\n\n        Args:\n            distribution: A instance of subclass of\n                :tf_main:`TF Distribution <distributions/Distribution>`,\n                or :tf_hmpg:`tensorflow_probability Distribution <probability>`,\n                Can be a class, its name or module path, or a class instance.\n            distribution_kwargs (dict, optional): Keyword arguments for the\n                distribution constructor. Ignored if `distribution` is a\n                class instance.\n            transform (bool): Whether to perform MLP transformation of the\n                distribution samples. If `False`, the structure/shape of a\n                sample must match :attr:`output_size`.\n            num_samples (optional): An `int` or `int` Tensor. Number of samples\n                to generate. If not given, generate a single sample. Note\n                that if batch size has already been included in\n                `distribution`\'s dimensionality, `num_samples` should be\n                left as `None`.\n\n        Returns:\n            A tuple (output, sample), where\n\n            - output: A Tensor or a (nested) tuple of Tensors with the same \\\n            structure and size of :attr:`output_size`. The batch dimension \\\n            equals :attr:`num_samples` if specified, or is determined by the \\\n            distribution dimensionality. If :attr:`transform` is `False`, \\\n            :attr:`output` will be equal to :attr:`sample`.\n            - sample: The sample from the distribution, prior to transformation.\n\n        Raises:\n            ValueError: If distribution cannot be reparametrized.\n            ValueError: The output does not match :attr:`output_size`.\n        """"""\n        dstr = check_or_get_instance(\n            distribution, distribution_kwargs,\n            [""tensorflow.distributions"", ""tensorflow_probability.distributions"",\n             ""texar.tf.custom""])\n\n        if dstr.reparameterization_type == tf_dstr.NOT_REPARAMETERIZED:\n            raise ValueError(\n                ""Distribution is not reparameterized: %s"" % dstr.name)\n\n        if num_samples:\n            sample = dstr.sample(num_samples)\n        else:\n            sample = dstr.sample()\n\n        # if dstr.event_shape == []:\n        #    sample = tf.reshape(\n        #        sample,\n        #        sample.shape.concatenate(tf.TensorShape(1)))\n\n        # sample = tf.cast(sample, tf.float32)\n        if transform:\n            fn_modules = [\'tensorflow\', \'tensorflow.nn\', \'texar.tf.custom\']\n            activation_fn = get_function(self.hparams.activation_fn, fn_modules)\n            output = _mlp_transform(sample, self._output_size, activation_fn)\n        else:\n            output = sample\n\n        _assert_same_size(output, self._output_size)\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return output, sample\n\n\nclass StochasticConnector(ConnectorBase):\n    """"""Samples from a distribution and transforms samples into specified size.\n\n    The connector is the same as\n    :class:`~texar.tf.modules.ReparameterizedStochasticConnector`, except that\n    here reparameterization is disabled, and thus the gradients cannot be\n    back-propagated through the stochastic samples.\n\n    Args:\n        output_size: Size of output **excluding** the batch dimension. For\n            example, set `output_size` to `dim` to generate output of\n            shape `[batch_size, dim]`.\n            Can be an `int`, a tuple of `int`, a Tensorshape, or a tuple of\n            TensorShapes.\n            For example, to transform inputs to have decoder state size, set\n            `output_size=decoder.state_size`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, output_size, hparams=None):\n        ConnectorBase.__init__(self, output_size, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""activation_fn"": ""identity"",\n                ""name"": ""stochastic_connector""\n            }\n\n        Here:\n\n        ""activation_fn"": str\n            The activation function applied to the outputs of the MLP\n            transformation layer. Can\n            be a function, or its name or module path.\n\n        ""name"": str\n            Name of the connector.\n        """"""\n        return {\n            ""activation_fn"": ""tensorflow.identity"",\n            ""name"": ""stochastic_connector""\n        }\n\n    def _build(self,\n               distribution=\'MultivariateNormalDiag\',\n               distribution_kwargs=None,\n               transform=True,\n               num_samples=None):\n        """"""Samples from a distribution and optionally performs transformation\n        with an MLP layer.\n\n        The inputs and outputs are the same as\n        :class:`~texar.tf.modules.ReparameterizedStochasticConnector` except that\n        the distribution does not need to be reparameterizable, and gradient\n        cannot be back-propagate through the samples.\n\n        Args:\n            distribution: A instance of subclass of\n                :tf_main:`TF Distribution <distributions/Distribution>`,\n                or :tf_hmpg:`tensorflow_probability Distribution <probability>`.\n                Can be a class, its name or module path, or a class instance.\n            distribution_kwargs (dict, optional): Keyword arguments for the\n                distribution constructor. Ignored if `distribution` is a\n                class instance.\n            transform (bool): Whether to perform MLP transformation of the\n                distribution samples. If `False`, the structure/shape of a\n                sample must match :attr:`output_size`.\n            num_samples (optional): An `int` or `int` Tensor. Number of samples\n                to generate. If not given, generate a single sample. Note\n                that if batch size has already been included in\n                `distribution`\'s dimensionality, `num_samples` should be\n                left as `None`.\n\n        Returns:\n            A tuple (output, sample), where\n\n            - output: A Tensor or a (nested) tuple of Tensors with the same \\\n            structure and size of :attr:`output_size`. The batch dimension \\\n            equals :attr:`num_samples` if specified, or is determined by the \\\n            distribution dimensionality. If :attr:`transform` is `False`, \\\n            :attr:`output` will be equal to :attr:`sample`.\n            - sample: The sample from the distribution, prior to transformation.\n\n        Raises:\n            ValueError: The output does not match :attr:`output_size`.\n        """"""\n        dstr = check_or_get_instance(\n            distribution, distribution_kwargs,\n            [""tensorflow.distributions"", ""tensorflow_probability.distributions"",\n             ""texar.tf.custom""])\n\n        if num_samples:\n            sample = dstr.sample(num_samples)\n        else:\n            sample = dstr.sample()\n\n        if dstr.event_shape == []:\n            sample = tf.reshape(sample,\n                                sample.shape.concatenate(tf.TensorShape(1)))\n\n        # Disable gradients through samples\n        sample = tf.stop_gradient(sample)\n\n        sample = tf.cast(sample, tf.float32)\n\n        if transform:\n            fn_modules = [\'tensorflow\', \'tensorflow.nn\', \'texar.tf.custom\']\n            activation_fn = get_function(self.hparams.activation_fn, fn_modules)\n            output = _mlp_transform(sample, self._output_size, activation_fn)\n        else:\n            output = sample\n\n        _assert_same_size(output, self._output_size)\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return output, sample\n\n\n# class ConcatConnector(ConnectorBase):\n#    """"""Concatenates multiple connectors into one connector. Used in, e.g.,\n#    semi-supervised variational autoencoders, disentangled representation\n#    learning, and other models.\n#\n#    Args:\n#        output_size: Size of output excluding the batch dimension (eg.\n#            :attr:`output_size = p` if :attr:`output.shape` is :attr:`[N, p]`).\n#            Can be an int, a tuple of int, a Tensorshape, or a tuple of\n#            TensorShapes.\n#            For example, to transform to decoder state size, set\n#            `output_size=decoder.cell.state_size`.\n#        hparams (dict): Hyperparameters of the connector.\n#    """"""\n#\n#    def __init__(self, output_size, hparams=None):\n#        ConnectorBase.__init__(self, output_size, hparams)\n#\n#    @staticmethod\n#    def default_hparams():\n#        """"""Returns a dictionary of hyperparameters with default values.\n#\n#        Returns:\n#            .. code-block:: python\n#\n#                {\n#                    ""activation_fn"": ""tensorflow.identity"",\n#                    ""name"": ""concat_connector""\n#                }\n#\n#            Here:\n#\n#            ""activation_fn"": (str or callable)\n#                The name or full path to the activation function applied to\n#                the outputs of the MLP layer. The activation functions can be:\n#\n#                - Built-in activation functions defined in :mod:`tf` or \\\n#                  :mod:`tf.nn`, e.g., :tf_main:`identity <identity>`.\n#                - User-defined activation functions in `texar.tf.custom`.\n#                - External activation functions. Must provide the full path, \\\n#                  e.g., ""my_module.my_activation_fn"".\n#\n#                The default value is :attr:`""identity""`, i.e., the MLP\n#                transformation is linear.\n#\n#            ""name"": str\n#                Name of the connector.\n#\n#                The default value is ""concat_connector"".\n#        """"""\n#        return {\n#            ""activation_fn"": ""tensorflow.identity"",\n#            ""name"": ""concat_connector""\n#        }\n#\n#    def _build(self, connector_inputs, transform=True):\n#        """"""Concatenate multiple input connectors\n#\n#        Args:\n#            connector_inputs: a list of connector states\n#            transform (bool): If `True`, then the output are automatically\n#                transformed to match :attr:`output_size`.\n#\n#        Returns:\n#            A Tensor or a (nested) tuple of Tensors of the same structure of\n#            the decoder state.\n#        """"""\n#        connector_inputs = [tf.cast(connector, tf.float32)\n#                            for connector in connector_inputs]\n#        output = tf.concat(connector_inputs, axis=1)\n#\n#        if transform:\n#            fn_modules = [\'texar.tf.custom\', \'tensorflow\', \'tensorflow.nn\']\n#            activation_fn = get_function(self.hparams.activation_fn,\n#                                         fn_modules)\n#            output = _mlp_transform(output, self._output_size, activation_fn)\n#        _assert_same_size(output, self._output_size)\n#\n#        self._add_internal_trainable_variables()\n#        self._built = True\n#\n#        return output\n'"
texar/tf/modules/decoders/__init__.py,7,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library decoders.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.decoders.beam_search_decode import *\nfrom texar.tf.modules.decoders.gpt2_decoder import *\nfrom texar.tf.modules.decoders.rnn_decoder_base import *\nfrom texar.tf.modules.decoders.rnn_decoders import *\nfrom texar.tf.modules.decoders.tf_helpers import *\nfrom texar.tf.modules.decoders.rnn_decoder_helpers import *\nfrom texar.tf.modules.decoders.transformer_decoders import *\n'"
texar/tf/modules/decoders/beam_search_decode.py,11,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBeam search decoding for RNN decoders.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import \\\n    dynamic_decode, AttentionWrapperState, AttentionWrapper, \\\n    BeamSearchDecoder, tile_batch\n\nfrom texar.tf.modules.decoders.rnn_decoder_base import RNNDecoderBase\n# pylint: disable=too-many-arguments, protected-access, too-many-locals\n# pylint: disable=invalid-name\n\n__all__ = [\n    ""beam_search_decode""\n]\n\n\ndef _get_initial_state(initial_state,\n                       tiled_initial_state,\n                       cell,\n                       batch_size,\n                       beam_width,\n                       dtype):\n    if tiled_initial_state is None:\n        if isinstance(initial_state, AttentionWrapperState):\n            raise ValueError(\n                \'`initial_state` must not be an AttentionWrapperState. Use \'\n                \'a plain cell state instead, which will be wrapped into an \'\n                \'AttentionWrapperState automatically.\')\n        if initial_state is None:\n            tiled_initial_state = cell.zero_state(batch_size * beam_width,\n                                                  dtype)\n        else:\n            tiled_initial_state = tile_batch(initial_state,\n                                             multiplier=beam_width)\n\n    if isinstance(cell, AttentionWrapper) and \\\n            not isinstance(tiled_initial_state, AttentionWrapperState):\n        zero_state = cell.zero_state(batch_size * beam_width, dtype)\n        tiled_initial_state = zero_state.clone(cell_state=tiled_initial_state)\n\n    return tiled_initial_state\n\n\ndef beam_search_decode(decoder_or_cell,\n                       embedding,\n                       start_tokens,\n                       end_token,\n                       beam_width,\n                       initial_state=None,\n                       tiled_initial_state=None,\n                       output_layer=None,\n                       length_penalty_weight=0.0,\n                       max_decoding_length=None,\n                       output_time_major=False,\n                       **kwargs):\n    """"""Performs beam search sampling decoding.\n\n    Args:\n        decoder_or_cell: An instance of\n            subclass of :class:`~texar.tf.modules.RNNDecoderBase`,\n            or an instance of :tf_main:`RNNCell <contrib/rnn/RNNCell>`. The\n            decoder or RNN cell to perform decoding.\n        embedding: A callable that takes a vector tensor of indexes (e.g.,\n            an instance of subclass of :class:`~texar.tf.modules.EmbedderBase`),\n            or the :attr:`params` argument for\n            :tf_main:`tf.nn.embedding_lookup <nn/embedding_lookup>`.\n        start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n        end_token: `int32` scalar, the token that marks end of decoding.\n        beam_width (int): Python integer, the number of beams.\n        initial_state (optional): Initial state of decoding. If `None`\n            (default), zero state is used.\n\n            The state must **not** be tiled with\n            :tf_main:`tile_batch <contrib/seq2seq/tile_batch>`.\n            If you have an already-tiled initial state, use\n            :attr:`tiled_initial_state` instead.\n\n            In the case of attention RNN decoder, `initial_state` must\n            **not** be an :tf_main:`AttentionWrapperState\n            <contrib/seq2seq/AttentionWrapperState>`. Instead, it must be a\n            state of the wrapped `RNNCell`, which state will be wrapped into\n            `AttentionWrapperState` automatically.\n\n            Ignored if :attr:`tiled_initial_state` is given.\n        tiled_initial_state (optional): Initial state that has been tiled\n            (typicaly with :tf_main:`tile_batch <contrib/seq2seq/tile_batch>`)\n            so that the batch dimension has size `batch_size * beam_width`.\n\n            In the case of attention RNN decoder, this can be either a state\n            of the wrapped `RNNCell`, or an `AttentionWrapperState`.\n\n            If not given, :attr:`initial_state` is used.\n        output_layer (optional): A :tf_main:`Layer <layers/Layer>` instance to\n            apply to the RNN output prior to storing the result or sampling. If\n            `None` and :attr:`decoder_or_cell` is a decoder, the decoder\'s\n            output layer will be used.\n        length_penalty_weight: Float weight to penalize length.\n            Disabled with `0.0` (default).\n        max_decoding_length (optional): A int scalar Tensor indicating the\n            maximum allowed number of decoding steps. If `None` (default),\n            decoding will continue until the end token is encountered.\n        output_time_major (bool): If `True`, outputs are returned as\n            time major tensors. If `False` (default), outputs are returned\n            as batch major tensors.\n        **kwargs: Other keyword arguments for :tf_main:`dynamic_decode\n            <contrib/seq2seq/dynamic_decode>` except argument\n            `maximum_iterations` which is set to :attr:`max_decoding_length`.\n\n    Returns:\n        A tuple `(outputs, final_state, sequence_length)`, where\n\n        - outputs: An instance of :tf_main:`FinalBeamSearchDecoderOutput \\\n        <contrib/seq2seq/FinalBeamSearchDecoderOutput>`.\n        - final_state: An instance of :tf_main:`BeamSearchDecoderState \\\n        <contrib/seq2seq/BeamSearchDecoderState>`.\n        - sequence_length: A Tensor of shape `[batch_size]` containing \\\n        the lengths of samples.\n\n    Example:\n\n        .. code-block:: python\n\n            ## Beam search with basic RNN decoder\n\n            embedder = WordEmbedder(vocab_size=data.vocab.size)\n            decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n\n            outputs, _, _, = beam_search_decode(\n                decoder_or_cell=decoder,\n                embedding=embedder,\n                start_tokens=[data.vocab.bos_token_id] * 100,\n                end_token=data.vocab.eos_token_id,\n                beam_width=5,\n                max_decoding_length=60)\n\n            sample_ids = sess.run(outputs.predicted_ids)\n            sample_text = tx.utils.map_ids_to_strs(sample_id[:,:,0], data.vocab)\n            print(sample_text)\n            # [\n            #   the first sequence sample .\n            #   the second sequence sample .\n            #   ...\n            # ]\n\n        .. code-block:: python\n\n            ## Beam search with attention RNN decoder\n\n            # Encodes the source\n            enc_embedder = WordEmbedder(data.source_vocab.size, ...)\n            encoder = UnidirectionalRNNEncoder(...)\n\n            enc_outputs, enc_state = encoder(\n                inputs=enc_embedder(data_batch[\'source_text_ids\']),\n                sequence_length=data_batch[\'source_length\'])\n\n            # Decodes while attending to the source\n            dec_embedder = WordEmbedder(vocab_size=data.target_vocab.size, ...)\n            decoder = AttentionRNNDecoder(\n                memory=enc_outputs,\n                memory_sequence_length=data_batch[\'source_length\'],\n                vocab_size=data.target_vocab.size)\n\n            # Beam search\n            outputs, _, _, = beam_search_decode(\n                decoder_or_cell=decoder,\n                embedding=dec_embedder,\n                start_tokens=[data.vocab.bos_token_id] * 100,\n                end_token=data.vocab.eos_token_id,\n                beam_width=5,\n                initial_state=enc_state,\n                max_decoding_length=60)\n    """"""\n    if isinstance(decoder_or_cell, RNNDecoderBase):\n        cell = decoder_or_cell._get_beam_search_cell(beam_width=beam_width)\n    elif isinstance(decoder_or_cell, tf.contrib.rnn.RNNCell):\n        cell = decoder_or_cell\n    else:\n        raise ValueError(""`decoder` must be an instance of a subclass of ""\n                         ""either `RNNDecoderBase` or `RNNCell`."")\n\n    start_tokens = tf.convert_to_tensor(\n        start_tokens, dtype=tf.int32, name=""start_tokens"")\n    if start_tokens.get_shape().ndims != 1:\n        raise ValueError(""`start_tokens` must be a vector"")\n    batch_size = tf.size(start_tokens)\n\n    initial_state = _get_initial_state(\n        initial_state, tiled_initial_state, cell,\n        batch_size, beam_width, tf.float32)\n\n    if output_layer is None and isinstance(decoder_or_cell, RNNDecoderBase):\n        output_layer = decoder_or_cell.output_layer\n\n    def _decode():\n        beam_docoder = BeamSearchDecoder(\n            cell=cell,\n            embedding=embedding,\n            start_tokens=start_tokens,\n            end_token=end_token,\n            initial_state=initial_state,\n            beam_width=beam_width,\n            output_layer=None if output_layer is tf.identity else output_layer,\n            length_penalty_weight=length_penalty_weight)\n\n        if \'maximum_iterations\' in kwargs:\n            raise ValueError(\'Use `max_decoding_length` to set the maximum \'\n                             \'allowed number of decoding steps.\')\n        outputs, final_state, _ = dynamic_decode(\n            decoder=beam_docoder,\n            output_time_major=output_time_major,\n            maximum_iterations=max_decoding_length,\n            **kwargs)\n\n        return outputs, final_state, final_state.lengths\n\n    if isinstance(decoder_or_cell, RNNDecoderBase):\n        vs = decoder_or_cell.variable_scope\n        with tf.variable_scope(vs, reuse=tf.AUTO_REUSE):\n            return _decode()\n    else:\n        return _decode()\n'"
texar/tf/modules/decoders/dynamic_decode.py,36,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Modifications copyright (C) 2019 Texar\n# ==============================================================================\n""""""\nUtility functions for decoding. This file is modified from\n`tf.contrib.seq2seq.dynamic_decode`.\n""""""\n\n# pylint: disable=invalid-name, no-member, protected-access\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Decoder as TFDecoder\nfrom tensorflow.python.framework import tensor_shape, tensor_util\nfrom tensorflow.python.util import nest\n\n\n__all__ = [\n    ""dynamic_decode""\n]\n\n\ndef _concat(prefix, suffix, static=False):\n    r""""""Concat that enables int, Tensor, or TensorShape values.\n    This function takes a size specification, which can be an integer, a\n    TensorShape, or a Tensor, and converts it into a concatenated Tensor\n    (if static = False) or a list of integers (if static = True).\n\n    Args:\n        prefix: The prefix; usually the batch size (and/or time step size).\n          (TensorShape, int, or Tensor.)\n        suffix: TensorShape, int, or Tensor.\n        static: If `True`, return a python list with possibly unknown\n          dimensions. Otherwise return a `Tensor`.\n\n    Returns:\n        shape: the concatenation of prefix and suffix.\n\n    Raises:\n        ValueError: if `suffix` is not a scalar or vector (or TensorShape).\n        ValueError: if prefix or suffix was `None` and asked for dynamic\n          Tensors out.\n    """"""\n    if isinstance(prefix, tf.Tensor):\n        p = prefix\n        p_static = tensor_util.constant_value(prefix)\n        if p.shape.ndims == 0:\n            p = tf.expand_dims(p, 0)\n        elif p.shape.ndims != 1:\n            raise ValueError(""prefix tensor must be either a scalar or vector, ""\n                             ""but saw tensor: %s"" % p)\n    else:\n        p = tensor_shape.as_shape(prefix)\n        p_static = p.as_list() if p.ndims is not None else None\n        p = (\n            tf.constant(p.as_list(), dtype=tf.int32)\n            if p.is_fully_defined() else None)\n    if isinstance(suffix, tf.Tensor):\n        s = suffix\n        s_static = tensor_util.constant_value(suffix)\n        if s.shape.ndims == 0:\n            s = tf.expand_dims(s, 0)\n        elif s.shape.ndims != 1:\n            raise ValueError(""suffix tensor must be either a scalar or vector, ""\n                             ""but saw tensor: %s"" % s)\n    else:\n        s = tensor_shape.as_shape(suffix)\n        s_static = s.as_list() if s.ndims is not None else None\n        s = (\n            tf.constant(s.as_list(), dtype=tf.int32)\n            if s.is_fully_defined() else None)\n\n    if static:\n        shape = tensor_shape.as_shape(p_static).concatenate(s_static)\n        shape = shape.as_list() if shape.ndims is not None else None\n    else:\n        if p is None or s is None:\n            raise ValueError(""Provided a prefix or suffix of None: %s and %s"" %\n                             (prefix, suffix))\n        shape = tf.concat((p, s), 0)\n    return shape\n\n\ndef _zero_state_tensors(state_size, batch_size, dtype):\n    r""""""Create tensors of zeros based on state_size, batch_size, and dtype.""""""\n\n    def get_state_shape(s):\n        r""""""Combine s with batch_size to get a proper tensor shape.""""""\n\n        c = _concat(batch_size, s)\n        size = tf.zeros(c, dtype=dtype)\n        return size\n\n    return nest.map_structure(get_state_shape, state_size)\n\n\ndef _create_zero_outputs(size, dtype, batch_size):\n    r""""""Create a zero outputs Tensor structure.""""""\n\n    def _create(s, d):\n        return _zero_state_tensors(s, batch_size, d)\n\n    return nest.map_structure(_create, size, dtype)\n\n\ndef _transpose_batch_time(x):\n    r""""""Transposes the batch and time dimensions of a Tensor.\n\n    If the input tensor has rank < 2 it returns the original tensor. Retains as\n    much of the static shape information as possible.\n\n    Args:\n        x: A Tensor.\n\n    Returns:\n        x transposed along the first two dimensions.\n    """"""\n    x_static_shape = x.get_shape()\n    if x_static_shape.ndims is not None and x_static_shape.ndims < 2:\n        return x\n\n    x_rank = tf.rank(x)\n    x_t = tf.transpose(\n        x, tf.concat(([1, 0], tf.range(2, x_rank)), axis=0))\n    x_t.set_shape(\n        tensor_shape.TensorShape(\n            [x_static_shape.dims[1].value,\n             x_static_shape.dims[0].value]).concatenate(x_static_shape[2:]))\n    return x_t\n\n\ndef dynamic_decode(decoder,\n                   output_time_major=False,\n                   impute_finished=False,\n                   maximum_iterations=None,\n                   parallel_iterations=32,\n                   swap_memory=False,\n                   scope=None):\n    r""""""Perform dynamic decoding with `decoder`.\n\n    Calls initialize() once and step() repeatedly on the Decoder object.\n\n    Args:\n      decoder: A `Decoder` instance.\n      output_time_major: Python boolean.  Default: `False` (batch major).  If\n        `True`, outputs are returned as time major tensors (this mode is\n        faster). Otherwise, outputs are returned as batch major tensors\n        (this adds extra time to the computation).\n      impute_finished: Python boolean.  If `True`, then states for batch\n        entries which are marked as finished get copied through and the\n        corresponding outputs get zeroed out.  This causes some slowdown at\n        each time step, but ensures that the final state and outputs have\n        the correct values and that backprop ignores time steps that were\n        marked as finished.\n      maximum_iterations: `int32` scalar, maximum allowed number of decoding\n        steps.  Default is `None` (decode until the decoder is fully done).\n      parallel_iterations: Argument passed to `tf.while_loop`.\n      swap_memory: Argument passed to `tf.while_loop`.\n      scope: Optional variable scope to use.\n\n    Returns:\n      `(final_outputs, final_state, final_sequence_lengths)`.\n    Raises:\n      TypeError: if `decoder` is not an instance of `Decoder`.\n      ValueError: if `maximum_iterations` is provided but is not a scalar.\n    """"""\n    if not isinstance(decoder, TFDecoder):\n        raise TypeError(""Expected decoder to be type Decoder, but saw: %s"" %\n                        type(decoder))\n\n    with tf.variable_scope(scope, ""decoder"") as varscope:\n        # Properly cache variable values inside the while_loop\n        if varscope.caching_device is None:\n            varscope.set_caching_device(lambda op: op.device)\n\n        if maximum_iterations is not None:\n            maximum_iterations = tf.convert_to_tensor(\n                maximum_iterations, dtype=tf.int32, name=""maximum_iterations"")\n            if maximum_iterations.get_shape().ndims != 0:\n                raise ValueError(""maximum_iterations must be a scalar"")\n\n        initial_finished, initial_inputs, initial_state = decoder.initialize()\n\n        zero_outputs = _create_zero_outputs(decoder.output_size,\n                                            decoder.output_dtype,\n                                            decoder.batch_size)\n\n        if maximum_iterations is not None:\n            initial_finished = tf.logical_or(\n                initial_finished, 0 >= maximum_iterations)\n        initial_sequence_lengths = tf.zeros_like(\n            initial_finished, dtype=tf.int32)\n        initial_time = tf.constant(0, dtype=tf.int32)\n\n        def _shape(batch_size, from_shape):\n            if (not isinstance(from_shape, tensor_shape.TensorShape) or\n                    from_shape.ndims == 0):\n                return None\n            else:\n                batch_size = tensor_util.constant_value(\n                    tf.convert_to_tensor(\n                        batch_size, name=""batch_size""))\n                return tensor_shape.TensorShape([batch_size]).\\\n                    concatenate(from_shape)\n\n        dynamic_size = True\n\n        def _create_ta(s, d):\n            return tf.TensorArray(\n                dtype=d,\n                size=0 if dynamic_size else maximum_iterations,\n                dynamic_size=dynamic_size,\n                element_shape=_shape(decoder.batch_size, s))\n\n        initial_outputs_ta = nest.map_structure(_create_ta, decoder.output_size,\n                                                decoder.output_dtype)\n\n        def condition(unused_time, unused_outputs_ta, unused_state,\n                      unused_inputs, finished, unused_sequence_lengths):\n            cond = tf.logical_not(tf.reduce_all(finished))\n            cond_time = (maximum_iterations is None or\n                         unused_time < maximum_iterations)\n            ret = tf.logical_and(cond, tf.convert_to_tensor(cond_time))\n            return ret\n\n        def body(time, outputs_ta, state, inputs, finished, sequence_lengths):\n            r""""""Internal while_loop body.\n\n            Args:\n                time: scalar int32 tensor.\n                outputs_ta: structure of TensorArray.\n                state: (structure of) state tensors and TensorArrays.\n                inputs: (structure of) input tensors.\n                finished: bool tensor (keeping track of what\'s finished).\n                sequence_lengths: int32 tensor (keeping track of time of\n                    finish).\n\n            Returns:\n                `(time + 1, outputs_ta, next_state, next_inputs, next_finished,\n                next_sequence_lengths)`.\n            """"""\n            (next_outputs, state) = decoder.step(time, inputs, state)\n\n            # Check if the maximum iteration is met. If it is met, do not\n            # compute the next inputs.\n            reach_max = tf.equal(time + 1, maximum_iterations)\n            (decoder_finished, next_inputs, decoder_state) = tf.cond(\n                reach_max,\n                lambda: (tf.cast(tf.ones_like(finished), tf.bool),\n                         inputs, state),\n                lambda: decoder.next_inputs(time, next_outputs, state)\n            )\n            if decoder.tracks_own_finished:\n                next_finished = decoder_finished\n            else:\n                next_finished = tf.logical_or(decoder_finished, finished)\n            next_sequence_lengths = tf.where(\n                tf.logical_not(finished),\n                tf.fill(tf.shape(sequence_lengths), time + 1),\n                sequence_lengths)\n\n            nest.assert_same_structure(state, decoder_state)\n            nest.assert_same_structure(outputs_ta, next_outputs)\n            nest.assert_same_structure(inputs, next_inputs)\n\n            # Zero out output values past finish\n            if impute_finished:\n                emit = nest.map_structure(\n                    lambda out, zero: tf.where(finished, zero, out),\n                    next_outputs,\n                    zero_outputs)\n            else:\n                emit = next_outputs\n\n            # Copy through states past finish\n            def _maybe_copy_state(new, cur):\n                # TensorArrays and scalar states get passed through.\n                if isinstance(cur, tf.TensorArray):\n                    pass_through = True\n                else:\n                    new.set_shape(cur.shape)\n                    pass_through = (new.shape.ndims == 0)\n                return new if pass_through else tf.where(finished, cur, new)\n\n            if impute_finished:\n                next_state = nest.map_structure(\n                    _maybe_copy_state, decoder_state, state)\n            else:\n                next_state = decoder_state\n\n            outputs_ta = nest.map_structure(lambda ta, out: ta.write(time, out),\n                                            outputs_ta, emit)\n            return (time + 1, outputs_ta, next_state, next_inputs,\n                    next_finished, next_sequence_lengths)\n\n        res = tf.while_loop(\n            condition,\n            body,\n            loop_vars=(\n                initial_time,\n                initial_outputs_ta,\n                initial_state,\n                initial_inputs,\n                initial_finished,\n                initial_sequence_lengths,\n            ),\n            parallel_iterations=parallel_iterations,\n            maximum_iterations=maximum_iterations,\n            swap_memory=swap_memory)\n\n        final_outputs_ta = res[1]\n        final_state = res[2]\n        final_sequence_lengths = res[5]\n\n        final_outputs = nest.map_structure(lambda ta: ta.stack(),\n                                           final_outputs_ta)\n\n        try:\n            final_outputs, final_state = decoder.finalize(\n                final_outputs, final_state, final_sequence_lengths)\n        except NotImplementedError:\n            pass\n\n        if not output_time_major:\n            final_outputs = nest.map_structure(_transpose_batch_time,\n                                               final_outputs)\n\n    return final_outputs, final_state, final_sequence_lengths\n'"
texar/tf/modules/decoders/gpt2_decoder.py,13,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGPT2 decoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.decoders.transformer_decoders import TransformerDecoder\nfrom texar.tf.modules.embedders import PositionEmbedder, WordEmbedder\nfrom texar.tf.modules.pretrained.gpt2 import PretrainedGPT2Mixin\n\n\n__all__ = [\n    ""GPT2Decoder"",\n]\n\n\nclass GPT2Decoder(PretrainedGPT2Mixin):\n    r""""""Raw GPT2 Transformer for decoding sequences. Please see\n    :class:`~texar.tf.modules.PretrainedGPT2Mixin` for a brief description\n    of GPT2.\n\n    This module basically stacks\n    :class:`~texar.tf.modules.WordEmbedder`,\n    :class:`~texar.tf.modules.PositionEmbedder`,\n    :class:`~texar.tf.modules.TransformerDecoder`.\n\n    This module supports the architecture first proposed\n    in `(Radford et al.)` GPT2.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``gpt2-small``). Please refer to\n            :class:`~texar.tf.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n    _IS_DECODE = True\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n\n        super().__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        with tf.variable_scope(self.variable_scope):\n\n            # Word embedding\n            self.word_embedder = WordEmbedder(\n                vocab_size=self._hparams.vocab_size,\n                hparams=self._hparams.embed)\n\n            # Position embedding\n            self.position_embedder = PositionEmbedder(\n                position_size=self._hparams.position_size,\n                hparams=self._hparams.position_embed)\n\n            # The GPT2 decoder (a TransformerDecoder)\n            self.decoder = TransformerDecoder(\n                vocab_size=self._hparams.vocab_size,\n                output_layer=tf.transpose(self.word_embedder.embedding, (1, 0)),\n                hparams=self._hparams.decoder)\n\n    def embed_tokens(self, tokens, positions):\n        word_embeds = self.word_embedder(tokens)\n        pos_embeds = self.position_embedder(positions)\n        return word_embeds + pos_embeds\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The decoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the decoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""gpt2_decoder"",\n                ""pretrained_model_name"": ""gpt2-small"",\n                ""vocab_size"": 50257,\n                ""context_size"": 1024,\n                ""embedding_size"": 768,\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""position_size"": 1024,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n\n                # hparams for TransformerDecoder\n                ""decoder"": {\n                    ""dim"": 768,\n                    ""num_blocks"": 12,\n                    ""use_gpt_config"": True,\n                    ""embedding_dropout"": 0,\n                    ""residual_dropout"": 0,\n                    ""multihead_attention"": {\n                        ""use_bias"": True,\n                        ""num_units"": 768,\n                        ""num_heads"": 12,\n                        ""dropout_rate"": 0.0,\n                        ""output_dim"": 768\n                    },\n                    ""initializer"": {\n                        ""type"": ""variance_scaling_initializer"",\n                        ""kwargs"": {\n                            ""factor"": 1.0,\n                            ""mode"": ""FAN_AVG"",\n                            ""uniform"": True\n                        }\n                    },\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""type"": ""Dense"",\n                                ""kwargs"": {\n                                    ""activation"": ""gelu"",\n                                    ""name"": ""intermediate"",\n                                    ""units"": 3072,\n                                    ""use_bias"": True\n                                }\n                            },\n                            {\n                                ""type"": ""Dense"",\n                                ""kwargs"": {\n                                    ""activation"": None,\n                                    ""name"": ""output"",\n                                    ""units"": 3072,\n                                    ""use_bias"": True\n                                }\n                            }\n                        ],\n                        ""name"": ""ffn""\n                    }\n                },\n                ""name"": ""gpt2_decoder"",\n            }\n\n        Here:\n\n        The default parameters are values for 124M GPT2 model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained GPT2 model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in `GPT2Model`.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""position_size""`:  int\n            The maximum sequence length that this model might ever be used with.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        return {\n            \'decoder\': {\n                \'name\': \'decoder\',\n                \'dim\': 768,\n                \'num_blocks\': 12,\n                \'embedding_dropout\': 0,\n                \'residual_dropout\': 0,\n                \'multihead_attention\': {\n                    \'name\': \'self\',\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    ""dropout_rate"": 0.0,\n                    \'output_dim\': 768\n                },\n                \'initializer\': {\n                    \'type\': \'variance_scaling_initializer\',\n                    \'kwargs\': {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                    }\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\n                                \'activation\': \'gelu\',\n                                \'name\': \'intermediate\',\n                                \'units\': 3072,\n                                \'use_bias\': True\n                            }\n                        },\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\n                                \'activation\': None,\n                                \'name\': \'output\',\n                                \'units\': 768,\n                                \'use_bias\': True\n                            }\n                        }\n                    ],\n                    \'name\': \'ffn\',\n                },\n            },\n\n            \'pretrained_model_name\': \'gpt2-small\',\n            \'vocab_size\': 50257,\n            \'context_size\': 1024,\n            \'embedding_size\': 768,\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'position_size\': 1024,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n            \'name\': \'gpt2_decoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    def _build(self,\n               decoding_strategy=\'train_greedy\',\n               inputs=None,\n               memory=None,\n               memory_sequence_length=None,\n               memory_attention_bias=None,\n               beam_width=None,\n               length_penalty=0.,\n               start_tokens=None,\n               end_token=None,\n               context=None,\n               context_sequence_length=None,\n               softmax_temperature=None,\n               max_decoding_length=None,\n               impute_finished=False,\n               helper=None,\n               mode=None):\n        r""""""Performs decoding. Has exact the same interfaces with\n        :meth:`texar.tf.modules.TransformerDecoder._build` except inputs\n        which is a tensor with shape `[batch_size, max_time]`. Please refer to\n        it for the detailed usage.\n        """"""\n        if inputs is not None:\n            batch_size, max_time = inputs.shape.as_list()\n            time = tf.expand_dims(tf.range(max_time), 0)\n            time = tf.broadcast_to(time, [batch_size, max_time])\n            inputs = self.embed_tokens(inputs, time)\n\n        outputs = self.decoder._build(\n            decoding_strategy=decoding_strategy,\n            inputs=inputs,\n            memory=memory,\n            memory_sequence_length=memory_sequence_length,\n            memory_attention_bias=memory_attention_bias,\n            beam_width=beam_width,\n            length_penalty=length_penalty,\n            start_tokens=start_tokens,\n            end_token=end_token,\n            context=context,\n            context_sequence_length=context_sequence_length,\n            softmax_temperature=softmax_temperature,\n            max_decoding_length=max_decoding_length,\n            impute_finished=impute_finished,\n            embedding=lambda a, b: self.embed_tokens(a, b),\n            helper=helper,\n            mode=mode)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n            self.init_pretrained_weights(self.variable_scope.name,\n                                         load_output_layer=True)\n\n        return outputs\n'"
texar/tf/modules/decoders/rnn_decoder_base.py,40,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for RNN decoders.\n""""""\n\n# pylint: disable=too-many-arguments, no-name-in-module\n# pylint: disable=too-many-branches, protected-access, too-many-locals\n# pylint: disable=arguments-differ, unused-argument\n\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Decoder as TFDecoder\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.util import nest\n\nfrom texar.tf.core import layers\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.mode import is_train_mode, is_train_mode_py\nfrom texar.tf.modules.decoders.dynamic_decode import dynamic_decode\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.modules.decoders import rnn_decoder_helpers\nfrom texar.tf.utils.dtypes import is_callable\nfrom texar.tf.utils.shapes import shape_list\nfrom texar.tf.modules.decoders import tf_helpers as tx_helper\n\n__all__ = [\n    ""RNNDecoderBase"",\n    ""_make_output_layer""\n]\n\n\ndef _make_output_layer_from_tensor(output_layer_tensor, vocab_size,\n                                   output_layer_bias, variable_scope):\n    """"""Creates a dense layer from a Tensor. Used to tie word embedding\n    with the output layer weight.\n    """"""\n    affine_bias = None\n    if output_layer_bias:\n        with tf.variable_scope(variable_scope):\n            affine_bias = tf.get_variable(\'affine_bias\', [vocab_size])\n\n    def _outputs_to_logits(outputs):\n        shape = shape_list(outputs)\n        dim = shape[-1]\n        outputs = tf.reshape(outputs, [-1, dim])\n        logits = tf.matmul(outputs, output_layer_tensor)\n        if affine_bias is not None:\n            logits += affine_bias\n        logits = tf.reshape(logits, shape[:-1] + [vocab_size])\n        return logits\n\n    return _outputs_to_logits\n\n\ndef _make_output_layer(output_layer, vocab_size,\n                       output_layer_bias, variable_scope):\n    """"""Makes a decoder output layer.\n    """"""\n    _vocab_size = vocab_size\n    if is_callable(output_layer):\n        _output_layer = output_layer\n    elif tf.contrib.framework.is_tensor(output_layer):\n        _vocab_size = shape_list(output_layer)[1]\n        _output_layer = _make_output_layer_from_tensor(\n            output_layer, _vocab_size, output_layer_bias, variable_scope)\n    elif output_layer is None:\n        if _vocab_size is None:\n            raise ValueError(\n                ""Either `output_layer` or `vocab_size` must be provided. ""\n                ""Set `output_layer=tf.identity` if no output layer is ""\n                ""wanted."")\n        with tf.variable_scope(variable_scope):\n            # pylint: disable=redefined-variable-type\n            _output_layer = tf.layers.Dense(\n                units=_vocab_size, use_bias=output_layer_bias)\n    else:\n        raise ValueError(\n            ""output_layer should be a callable layer, a tensor, or None. ""\n            ""Unsupported type: "", type(output_layer)\n        )\n\n    return _output_layer, _vocab_size\n\n\nclass RNNDecoderBase(ModuleBase, TFDecoder):\n    """"""Base class inherited by all RNN decoder classes.\n    See :class:`~texar.tf.modules.BasicRNNDecoder` for the argumenrts.\n\n    See :meth:`_build` for the inputs and outputs of RNN decoders in general.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 cell=None,\n                 vocab_size=None,\n                 output_layer=None,\n                 cell_dropout_mode=None,\n                 hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        self._helper = None\n        self._initial_state = None\n\n        # Make rnn cell\n        with tf.variable_scope(self.variable_scope):\n            if cell is not None:\n                self._cell = cell\n            else:\n                self._cell = layers.get_rnn_cell(\n                    self._hparams.rnn_cell, cell_dropout_mode)\n        self._beam_search_cell = None\n\n        # Make the output layer\n        self._output_layer, self._vocab_size = _make_output_layer(\n            output_layer, vocab_size, self._hparams.output_layer_bias,\n            self.variable_scope)\n\n        self.max_decoding_length = None\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        The hyperparameters are the same as in\n        :meth:`~texar.tf.modules.BasicRNNDecoder.default_hparams` of\n        :class:`~texar.tf.modules.BasicRNNDecoder`, except that the default\n        ""name"" here is ""rnn_decoder"".\n        """"""\n        return {\n            ""rnn_cell"": layers.default_rnn_cell_hparams(),\n            ""helper_train"": rnn_decoder_helpers.default_helper_train_hparams(),\n            ""helper_infer"": rnn_decoder_helpers.default_helper_infer_hparams(),\n            ""max_decoding_length_train"": None,\n            ""max_decoding_length_infer"": None,\n            ""name"": ""rnn_decoder"",\n            ""output_layer_bias"": True,\n        }\n\n    def _build(self,\n               decoding_strategy=""train_greedy"",\n               initial_state=None,\n               inputs=None,\n               sequence_length=None,\n               embedding=None,\n               start_tokens=None,\n               end_token=None,\n               softmax_temperature=None,\n               max_decoding_length=None,\n               impute_finished=False,\n               output_time_major=False,\n               input_time_major=False,\n               helper=None,\n               mode=None,\n               **kwargs):\n        """"""Performs decoding. This is a shared interface for both\n        :class:`~texar.tf.modules.BasicRNNDecoder` and\n        :class:`~texar.tf.modules.AttentionRNNDecoder`.\n\n        The function provides **3 ways** to specify the\n        decoding method, with varying flexibility:\n\n        1. The :attr:`decoding_strategy` argument: A string taking value of:\n\n            - **""train_greedy""**: decoding in teacher-forcing fashion\n              (i.e., feeding `ground truth` to decode the next step), and each\n              sample is obtained by taking the `argmax` of the RNN output\n              logits. Arguments\n              :attr:`(inputs, sequence_length, input_time_major)` are required\n              for this strategy, and argument :attr:`embedding` is optional.\n            - **""infer_greedy""**: decoding in inference fashion (i.e., feeding\n              the `generated` sample to decode the next step), and each sample\n              is obtained by taking the `argmax` of the RNN output logits.\n              Arguments :attr:`(embedding, start_tokens, end_token)` are\n              required for this strategy, and argument\n              :attr:`max_decoding_length` is optional.\n            - **""infer_sample""**: decoding in inference fashion, and each\n              sample is obtained by `random sampling` from the RNN output\n              distribution. Arguments\n              :attr:`(embedding, start_tokens, end_token)` are\n              required for this strategy, and argument\n              :attr:`max_decoding_length` is optional.\n\n          This argument is used only when argument :attr:`helper` is `None`.\n\n          Example:\n\n            .. code-block:: python\n\n                embedder = WordEmbedder(vocab_size=data.vocab.size)\n                decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n\n                # Teacher-forcing decoding\n                outputs_1, _, _ = decoder(\n                    decoding_strategy=\'train_greedy\',\n                    inputs=embedder(data_batch[\'text_ids\']),\n                    sequence_length=data_batch[\'length\']-1)\n\n                # Random sample decoding. Gets 100 sequence samples\n                outputs_2, _, sequence_length = decoder(\n                    decoding_strategy=\'infer_sample\',\n                    start_tokens=[data.vocab.bos_token_id]*100,\n                    end_token=data.vocab.eos.token_id,\n                    embedding=embedder,\n                    max_decoding_length=60)\n\n        2. The :attr:`helper` argument: An instance of subclass of\n           :class:`texar.tf.modules.Helper`. This\n           provides a superset of decoding strategies than above, for example:\n\n            - :class:`~texar.tf.modules.TrainingHelper` corresponding to the\n              ""train_greedy"" strategy.\n            - :class:`~texar.tf.modules.GreedyEmbeddingHelper` and\n              :class:`~texar.tf.modules.SampleEmbeddingHelper` corresponding to\n              the ""infer_greedy"" and ""infer_sample"", respectively.\n            - :class:`~texar.tf.modules.TopKSampleEmbeddingHelper` for Top-K\n              sample decoding.\n            - :class:`ScheduledEmbeddingTrainingHelper` and\n              :class:`ScheduledOutputTrainingHelper` for scheduled\n              sampling.\n            - :class:`~texar.tf.modules.SoftmaxEmbeddingHelper` and\n              :class:`~texar.tf.modules.GumbelSoftmaxEmbeddingHelper` for\n              soft decoding and gradient backpropagation.\n\n          Helpers give the maximal flexibility of configuring the decoding\n          strategy.\n\n          Example:\n\n            .. code-block:: python\n\n                embedder = WordEmbedder(vocab_size=data.vocab.size)\n                decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n\n                # Teacher-forcing decoding, same as above with\n                # `decoding_strategy=\'train_greedy\'`\n                helper_1 = tx.modules.TrainingHelper(\n                    inputs=embedders(data_batch[\'text_ids\']),\n                    sequence_length=data_batch[\'length\']-1)\n                outputs_1, _, _ = decoder(helper=helper_1)\n\n                # Gumbel-softmax decoding\n                helper_2 = GumbelSoftmaxEmbeddingHelper(\n                    embedding=embedder,\n                    start_tokens=[data.vocab.bos_token_id]*100,\n                    end_token=data.vocab.eos_token_id,\n                    tau=0.1)\n                outputs_2, _, sequence_length = decoder(\n                    max_decoding_length=60, helper=helper_2)\n\n        3. :attr:`hparams[""helper_train""]` and :attr:`hparams[""helper_infer""]`:\n           Specifying the helper through hyperparameters. Train and infer\n           strategy is toggled based on :attr:`mode`. Appriopriate arguments\n           (e.g., :attr:`inputs`, :attr:`start_tokens`, etc) are selected to\n           construct the helper. Additional arguments for helper constructor\n           can be provided either through :attr:`**kwargs`, or through\n           :attr:`hparams[""helper_train/infer""][""kwargs""]`.\n\n           This means is used only when both :attr:`decoding_strategy` and\n           :attr:`helper` are `None`.\n\n           Example:\n\n             .. code-block:: python\n\n                 h = {\n                     ""helper_infer"": {\n                         ""type"": ""GumbelSoftmaxEmbeddingHelper"",\n                         ""kwargs"": { ""tau"": 0.1 }\n                     }\n                 }\n                 embedder = WordEmbedder(vocab_size=data.vocab.size)\n                 decoder = BasicRNNDecoder(vocab_size=data.vocab.size,\n                                           hparams=h)\n\n                 # Gumbel-softmax decoding\n                 output, _, _ = decoder(\n                     decoding_strategy=None, # Sets to None explicit\n                     embedding=embedder,\n                     start_tokens=[data.vocab.bos_token_id]*100,\n                     end_token=data.vocab.eos_token_id,\n                     max_decoding_length=60,\n                     mode=tf.estimator.ModeKeys.PREDICT)\n                         # PREDICT mode also shuts down dropout\n\n        Args:\n            decoding_strategy (str): A string specifying the decoding\n                strategy. Different arguments are required based on the\n                strategy. Ignored if :attr:`helper` is given.\n            initial_state (optional): Initial state of decoding.\n                If `None` (default), zero state is used.\n\n            inputs (optional): Input tensors for teacher forcing decoding.\n                Used when :attr:`decoding_strategy` is set to\n                ``""train_greedy""``, or when `hparams`-configured helper is used.\n\n                - If `embedding` is `None`, `inputs` is directly fed to\n                  the decoder. E.g., in `""train_greedy""` strategy, `inputs` must\n                  be a 3D Tensor of shape `[batch_size, max_time, emb_dim]` (or\n                  `[max_time, batch_size, emb_dim]` if\n                  `input_time_major` == `True`).\n                - If `embedding` is given, `inputs` is used as index to look up\n                  embeddings and feed in the decoder. E.g., if `embedding` is\n                  an instance of :class:`~texar.tf.modules.WordEmbedder`,\n                  then :attr:`inputs` is usually a 2D int Tensor\n                  `[batch_size, max_time]` (or `[max_time, batch_size]` if\n                  `input_time_major` == `True`) containing the token indexes.\n            sequence_length (optional): A 1D int Tensor containing the\n                sequence length of :attr:`inputs`.\n                Used when `decoding_strategy=""train_greedy""` or\n                `hparams`-configured helper is used.\n            embedding (optional): Embedding used when:\n\n                - ""infer_greedy"" or ""infer_sample"" `decoding_strategy` is\n                  used. This can be a callable or the `params` argument for\n                  :tf_main:`embedding_lookup <nn/embedding_lookup>`.\n                  If a callable, it can take a vector tensor of token `ids`,\n                  or take two arguments (`ids`, `times`), where `ids`\n                  is a vector tensor of token ids, and `times` is a vector\n                  tensor\n                  of time steps (i.e., position ids). The latter case can be\n                  used when attr:`embedding` is a combination of word embedding\n                  and position embedding. `embedding` is required in this case.\n                - ""train_greedy"" `decoding_strategy` is used.\n                  This can be a callable or the `params` argument for\n                  :tf_main:`embedding_lookup <nn/embedding_lookup>`.\n                  If a callable, it can take :attr:`inputs` and returns\n                  the input embedding. `embedding` is optional in this case.\n            start_tokens (optional): A int Tensor of shape `[batch_size]`,\n                the start tokens. Used when `decoding_strategy=""infer_greedy""`\n                or `""infer_sample""`, or when the helper specified in `hparams`\n                is used.\n\n                Example:\n\n                    .. code-block:: python\n\n                        data = tx.data.MonoTextData(hparams)\n                        iterator = DataIterator(data)\n                        batch = iterator.get_next()\n\n                        bos_token_id = data.vocab.bos_token_id\n                        start_tokens=tf.ones_like(batch[\'length\'])*bos_token_id\n\n            end_token (optional): A int 0D Tensor, the token that marks end\n                of decoding.\n                Used when `decoding_strategy=""infer_greedy""` or\n                `""infer_sample""`, or when the helper specified in `hparams`\n                is used.\n            softmax_temperature (optional): A float 0D Tensor, value to divide\n                the logits by before computing the softmax. Larger values\n                (above 1.0) result in more random samples. Must > 0. If `None`,\n                1.0 is used.\n                Used when `decoding_strategy=""infer_sample""`.\n            max_decoding_length: A int scalar Tensor indicating the maximum\n                allowed number of decoding steps. If `None` (default), either\n                `hparams[""max_decoding_length_train""]` or\n                `hparams[""max_decoding_length_infer""]` is used\n                according to :attr:`mode`.\n            impute_finished (bool): If `True`, then states for batch\n                entries which are marked as finished get copied through and\n                the corresponding outputs get zeroed out.  This causes some\n                slowdown at each time step, but ensures that the final state\n                and outputs have the correct values and that backprop ignores\n                time steps that were marked as finished.\n            output_time_major (bool): If `True`, outputs are returned as\n                time major tensors. If `False` (default), outputs are returned\n                as batch major tensors.\n            input_time_major (optional): Whether the :attr:`inputs` tensor is\n                time major.\n                Used when `decoding_strategy=""train_greedy""` or\n                `hparams`-configured helper is used.\n            helper (optional): An instance of\n                :class:`texar.tf.modules.Helper`\n                that defines the decoding strategy. If given,\n                `decoding_strategy`\n                and helper configs in :attr:`hparams` are ignored.\n            mode (str, optional): A string taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`. If\n                `TRAIN`, training related hyperparameters are used (e.g.,\n                `hparams[\'max_decoding_length_train\']`), otherwise,\n                inference related hyperparameters are used (e.g.,\n                `hparams[\'max_decoding_length_infer\']`).\n                If `None` (default), `TRAIN` mode is used.\n            **kwargs: Other keyword arguments for constructing helpers\n                defined by `hparams[""helper_trainn""]` or\n                `hparams[""helper_infer""]`.\n\n        Returns:\n            `(outputs, final_state, sequence_lengths)`, where\n\n            - **`outputs`**: an object containing the decoder output on all\n              time steps.\n            - **`final_state`**: is the cell state of the final time step.\n            - **`sequence_lengths`**: is an int Tensor of shape `[batch_size]`\n              containing the length of each sample.\n        """"""\n        # Helper\n        if helper is not None:\n            pass\n        elif decoding_strategy is not None:\n            if decoding_strategy == ""train_greedy"":\n                helper = rnn_decoder_helpers._get_training_helper(\n                    inputs, sequence_length, embedding, input_time_major)\n            elif decoding_strategy == ""infer_greedy"":\n                helper = tx_helper.GreedyEmbeddingHelper(\n                    embedding, start_tokens, end_token)\n            elif decoding_strategy == ""infer_sample"":\n                helper = tx_helper.SampleEmbeddingHelper(\n                    embedding, start_tokens, end_token, softmax_temperature)\n            else:\n                raise ValueError(\n                    ""Unknown decoding strategy: {}"".format(decoding_strategy))\n        else:\n            if is_train_mode_py(mode):\n                kwargs_ = copy.copy(self._hparams.helper_train.kwargs.todict())\n                helper_type = self._hparams.helper_train.type\n            else:\n                kwargs_ = copy.copy(self._hparams.helper_infer.kwargs.todict())\n                helper_type = self._hparams.helper_infer.type\n            kwargs_.update({\n                ""inputs"": inputs,\n                ""sequence_length"": sequence_length,\n                ""time_major"": input_time_major,\n                ""embedding"": embedding,\n                ""start_tokens"": start_tokens,\n                ""end_token"": end_token,\n                ""softmax_temperature"": softmax_temperature})\n            kwargs_.update(kwargs)\n            helper = rnn_decoder_helpers.get_helper(helper_type, **kwargs_)\n        self._helper = helper\n\n        # Initial state\n        if initial_state is not None:\n            self._initial_state = initial_state\n        else:\n            self._initial_state = self.zero_state(\n                batch_size=self.batch_size, dtype=tf.float32)\n\n        # Maximum decoding length\n        max_l = max_decoding_length\n        if max_l is None:\n            max_l_train = self._hparams.max_decoding_length_train\n            if max_l_train is None:\n                max_l_train = utils.MAX_SEQ_LENGTH\n            max_l_infer = self._hparams.max_decoding_length_infer\n            if max_l_infer is None:\n                max_l_infer = utils.MAX_SEQ_LENGTH\n            max_l = tf.cond(is_train_mode(mode),\n                            lambda: max_l_train, lambda: max_l_infer)\n        self.max_decoding_length = max_l\n        # Decode\n        outputs, final_state, sequence_lengths = dynamic_decode(\n            decoder=self, impute_finished=impute_finished,\n            maximum_iterations=max_l, output_time_major=output_time_major)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            # Add trainable variables of `self._cell` which may be\n            # constructed externally.\n            self._add_trainable_variable(\n                layers.get_rnn_cell_trainable_variables(self._cell))\n            if isinstance(self._output_layer, tf.layers.Layer):\n                self._add_trainable_variable(\n                    self._output_layer.trainable_variables)\n            # Add trainable variables of `self._beam_search_rnn_cell` which\n            # may already be constructed and used.\n            if self._beam_search_cell is not None:\n                self._add_trainable_variable(\n                    self._beam_search_cell.trainable_variables)\n\n            self._built = True\n\n        return outputs, final_state, sequence_lengths\n\n    def _get_beam_search_cell(self, **kwargs):\n        self._beam_search_cell = self._cell\n        return self._cell\n\n    def _rnn_output_size(self):\n        size = self._cell.output_size\n        if self._output_layer is tf.identity:\n            return size\n        else:\n            # To use layer\'s compute_output_shape, we need to convert the\n            # RNNCell\'s output_size entries into shapes with an unknown\n            # batch size.  We then pass this through the layer\'s\n            # compute_output_shape and read off all but the first (batch)\n            # dimensions to get the output size of the rnn with the layer\n            # applied to the top.\n            output_shape_with_unknown_batch = nest.map_structure(\n                lambda s: tensor_shape.TensorShape([None]).concatenate(s),\n                size)\n            layer_output_shape = self._output_layer.compute_output_shape(\n                output_shape_with_unknown_batch)\n            return nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n    @property\n    def batch_size(self):\n        return self._helper.batch_size\n\n    @property\n    def output_size(self):\n        """"""Output size of one step.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def output_dtype(self):\n        """"""Types of output of one step.\n        """"""\n        raise NotImplementedError\n\n    def initialize(self, name=None):\n        # Inherits from TFDecoder\n        # All RNN decoder classes must implement this\n        raise NotImplementedError\n\n    def step(self, time, inputs, state, name=None):\n        # Inherits from TFDecoder\n        # All RNN decoder classes must implement this\n        raise NotImplementedError\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        # Inherits from TFDecoder\n        # All RNN decoder classes must implement this\n        raise NotImplementedError\n\n    @property\n    def cell(self):\n        """"""The RNN cell.\n        """"""\n        return self._cell\n\n    def zero_state(self, batch_size, dtype):\n        """"""Zero state of the RNN cell.\n        Equivalent to :attr:`decoder.cell.zero_state`.\n        """"""\n        return self._cell.zero_state(\n            batch_size=batch_size, dtype=dtype)\n\n    @property\n    def state_size(self):\n        """"""The state size of decoder cell.\n        Equivalent to :attr:`decoder.cell.state_size`.\n        """"""\n        return self.cell.state_size\n\n    @property\n    def vocab_size(self):\n        """"""The vocab size.\n        """"""\n        return self._vocab_size\n\n    @property\n    def output_layer(self):\n        """"""The output layer.\n        """"""\n        return self._output_layer\n'"
texar/tf/modules/decoders/rnn_decoder_helpers.py,53,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious helper classes and utilities for RNN decoders.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow_probability import distributions as tfpd\n\nfrom texar.tf.modules.decoders.tf_helpers import \\\n        Helper, TrainingHelper, GreedyEmbeddingHelper\nfrom texar.tf.modules.embedders.embedder_utils import soft_embedding_lookup\nfrom texar.tf.utils import utils\n\n# pylint: disable=not-context-manager, too-many-arguments\n# pylint: disable=too-many-instance-attributes\n\n__all__ = [\n    ""default_helper_train_hparams"",\n    ""default_helper_infer_hparams"",\n    ""get_helper"",\n    ""_get_training_helper"",\n    ""TopKSampleEmbeddingHelper"",\n    ""SoftmaxEmbeddingHelper"",\n    ""GumbelSoftmaxEmbeddingHelper"",\n]\n\n\ndef default_helper_train_hparams():\n    """"""Returns default hyperparameters of an RNN decoder helper in the training\n    phase.\n\n    See also :meth:`~texar.tf.modules.decoders.rnn_decoder_helpers.get_helper`\n    for information of the hyperparameters.\n\n    Returns:\n        dict: A dictionary with following structure and values:\n\n        .. code-block:: python\n\n            {\n                # The `helper_type` argument for `get_helper`, i.e., the name\n                # or full path to the helper class.\n                ""type"": ""TrainingHelper"",\n\n                # The `**kwargs` argument for `get_helper`, i.e., additional\n                # keyword arguments for constructing the helper.\n                ""kwargs"": {}\n            }\n    """"""\n    return {\n        ""type"": ""TrainingHelper"",\n        ""kwargs"": {}\n    }\n\n\ndef default_helper_infer_hparams():\n    """"""Returns default hyperparameters of an RNN decoder helper in the inference\n    phase.\n\n    See also :meth:`~texar.tf.modules.decoders.rnn_decoder_helpers.get_helper`\n    for information of the hyperparameters.\n\n    Returns:\n        dict: A dictionary with following structure and values:\n\n        .. code-block:: python\n\n            {\n                # The `helper_type` argument for `get_helper`, i.e., the name\n                # or full path to the helper class.\n                ""type"": ""SampleEmbeddingHelper"",\n\n                # The `**kwargs` argument for `get_helper`, i.e., additional\n                # keyword arguments for constructing the helper.\n                ""kwargs"": {}\n            }\n    """"""\n    return {\n        ""type"": ""SampleEmbeddingHelper"",\n        ""kwargs"": {}\n    }\n\n\ndef get_helper(helper_type,\n               inputs=None,\n               sequence_length=None,\n               embedding=None,\n               start_tokens=None,\n               end_token=None,\n               **kwargs):\n    """"""Creates a Helper instance.\n\n    Args:\n        helper_type: A :tf_main:`Helper <contrib/seq2seq/Helper>` class, its\n            name or module path, or a class instance. If a class instance\n            is given, it is returned directly.\n        inputs (optional): Inputs to the RNN decoder, e.g., ground truth\n            tokens for teacher forcing decoding.\n        sequence_length (optional): A 1D int Tensor containing the\n            sequence length of :attr:`inputs`.\n        embedding (optional): A callable that takes a vector tensor of\n            indexes (e.g., an instance of subclass of\n            :class:`~texar.tf.modules.EmbedderBase`), or the `params` argument\n            for `embedding_lookup` (e.g., the embedding Tensor).\n        start_tokens (optional): A int Tensor of shape `[batch_size]`,\n            the start tokens.\n        end_token (optional): A int 0D Tensor, the token that marks end\n            of decoding.\n        **kwargs: Additional keyword arguments for constructing the helper.\n\n    Returns:\n        A helper instance.\n    """"""\n    module_paths = [\n        \'texar.tf.modules.decoders.rnn_decoder_helpers\',\n        \'texar.tf.modules.decoders.tf_helpers\',\n        # \'tensorflow.contrib.seq2seq\',\n        \'texar.tf.custom\']\n    class_kwargs = {""inputs"": inputs,\n                    ""sequence_length"": sequence_length,\n                    ""embedding"": embedding,\n                    ""start_tokens"": start_tokens,\n                    ""end_token"": end_token}\n    class_kwargs.update(kwargs)\n    return utils.check_or_get_instance_with_redundant_kwargs(\n        helper_type, class_kwargs, module_paths)\n\n\ndef _get_training_helper(  # pylint: disable=invalid-name\n        inputs, sequence_length, embedding=None, time_major=False, name=None):\n    """"""Returns an instance of :tf_main:`TrainingHelper\n    <contrib/seq2seq/TrainingHelper>` given embeddings.\n\n    Args:\n        inputs: If :attr:`embedding` is given, this is sequences of input\n            token indexes. If :attr:`embedding` is `None`, this is passed to\n            TrainingHelper directly.\n        sequence_length (1D Tensor): Lengths of input token sequences.\n        embedding (optional): The `params` argument of\n            :tf_main:`tf.nn.embedding_lookup\n            <nn/embedding_lookup>` (e.g., the embedding Tensor); or a callable\n            that takes a vector of integer indexes and returns respective\n            embedding (e.g., an instance of subclass of\n            :class:`~texar.tf.modules.EmbedderBase`).\n        time_major (bool): Whether the tensors in `inputs` are time major.\n            If `False` (default), they are assumed to be batch major.\n        name (str, optional): Name scope for any created operations.\n\n    Returns:\n        An instance of TrainingHelper.\n\n    Raises:\n        ValueError: if `sequence_length` is not a 1D tensor.\n    """"""\n    if embedding is None:\n        return TrainingHelper(inputs=inputs,\n                              sequence_length=sequence_length,\n                              time_major=time_major,\n                              name=name)\n\n    with tf.name_scope(name, ""TrainingHelper"", [embedding, inputs]):\n        if callable(embedding):\n            embedding_fn = embedding\n        else:\n            embedding_fn = (\n                lambda ids: tf.nn.embedding_lookup(embedding, ids))\n        emb_inputs = embedding_fn(inputs)\n    helper = TrainingHelper(inputs=emb_inputs,\n                            sequence_length=sequence_length,\n                            time_major=time_major,\n                            name=name)\n    return helper\n\n\ndef _top_k_logits(logits, k):\n    """"""Adapted from\n    https://github.com/openai/gpt-2/blob/master/src/sample.py#L63-L77\n    """"""\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n        tf.equal(k, 0),\n        lambda: logits,\n        lambda: _top_k(),\n    )\n\n\nclass TopKSampleEmbeddingHelper(GreedyEmbeddingHelper):\n    """"""A helper for use during inference.\n\n    Samples from `top_k` most likely candidates from a vocab distribution,\n    and passes the result through an embedding layer to get the next input.\n    """"""\n\n    def __init__(self, embedding, start_tokens, end_token, top_k=10,\n                 softmax_temperature=None, seed=None):\n        """"""Initializer.\n\n        Args:\n            embedding: A callable or the `params` argument for\n                `embedding_lookup`. If a callable, it can take a vector tensor\n                of token `ids`, or take two arguments (`ids`, `times`),\n                where `ids` is a vector\n                tensor of token ids, and `times` is a vector tensor of current\n                time steps (i.e., position ids). The latter case can be used\n                when attr:`embedding` is a combination of word embedding and\n                position embedding.\n            start_tokens: `int32` vector shaped `[batch_size]`, the start\n                tokens.\n            end_token: `int32` scalar, the token that marks end of decoding.\n            top_k: `int32` scalar tensor. Number of top candidates to sample\n                from. Must be `>=0`. If set to 0, samples from all candidates\n                (i.e., regular random sample decoding).\n            softmax_temperature (optional): `float32` scalar, value to\n                divide the logits by before computing the softmax. Larger values\n                (above 1.0) result in more random samples, while smaller values\n                push the sampling distribution towards the argmax. Must be\n                strictly greater than 0. Defaults to 1.0.\n            seed (optional): The sampling seed.\n\n        Raises:\n            ValueError: if `start_tokens` is not a 1D tensor or `end_token` is\n            not a scalar.\n        """"""\n        super(TopKSampleEmbeddingHelper, self).__init__(\n            embedding, start_tokens, end_token)\n        self._top_k = top_k\n        self._softmax_temperature = softmax_temperature\n        self._seed = seed\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Gets a sample for one step.""""""\n        del time, state  # unused by sample_fn\n        # Outputs are logits, we sample from the top_k candidates\n        if not isinstance(outputs, tf.Tensor):\n            raise TypeError(""Expected outputs to be a single Tensor, got: %s"" %\n                            type(outputs))\n        if self._softmax_temperature is None:\n            logits = outputs\n        else:\n            logits = outputs / self._softmax_temperature\n\n        logits = _top_k_logits(logits, k=self._top_k)\n\n        sample_id_sampler = tfpd.Categorical(logits=logits)\n        sample_ids = sample_id_sampler.sample(seed=self._seed)\n\n        return sample_ids\n\n\nclass SoftmaxEmbeddingHelper(Helper):\n    """"""A helper that feeds softmax probabilities over vocabulary\n    to the next step.\n    Uses the softmax probability vector to pass through word embeddings to\n    get the next input (i.e., a mixed word embedding).\n\n    A subclass of\n    :tf_main:`Helper <contrib/seq2seq/Helper>`.\n    Used as a helper to :class:`~texar.tf.modules.RNNDecoderBase` :meth:`_build`\n    in inference mode.\n\n    Args:\n        embedding: A callable or the `params` argument for\n            :tf_main:`tf.nn.embedding_lookup <nn/embedding_lookup>`.\n            If a callable, it can take a float tensor named `soft_ids` which is\n            a distribution over indexes. For example, the shape of the tensor\n            is typically `[batch_size, vocab_size]`. The callable can also\n            take two arguments (`soft_ids`, `times`), where `soft_ids` is\n            as above, and `times` is an int vector tensor of current\n            time steps (i.e., position ids). The latter case can be used\n            when attr:`embedding` is a combination of word embedding and\n            position embedding.\n        start_tokens: An int tensor shaped `[batch_size]`. The\n            start tokens.\n        end_token: An int scalar tensor. The token that marks end of\n            decoding.\n        tau: A float scalar tensor, the softmax temperature.\n        embedding_size (optional): An int scalar tensor, the number of\n            embedding vectors. Usually it is the vocab size. Required if\n            :attr:`embedding` is a callable.\n        stop_gradient (bool): Whether to stop the gradient backpropagation\n            when feeding softmax vector to the next step.\n        use_finish (bool): Whether to stop decoding once `end_token` is\n            generated. If `False`, decoding will continue until\n            `max_decoding_length` of the decoder is reached.\n    """"""\n\n    def __init__(self, embedding, start_tokens, end_token, tau,\n                 embedding_size=None, stop_gradient=False, use_finish=True):\n        if callable(embedding):\n            self._embedding_fn = embedding\n\n            if embedding_size is None:\n                raise ValueError(\'`embedding_size` must be provided if \'\n                                 \'`embedding` is a callable.\')\n            self._embedding_size = tf.convert_to_tensor(\n                embedding_size, dtype=tf.int32, name=""embedding_size"")\n        else:\n            self._embedding_fn = (\n                lambda soft_ids: soft_embedding_lookup(embedding, soft_ids))\n            self._embedding_size = tf.shape(embedding)[0]\n\n        self._start_tokens = tf.convert_to_tensor(\n            start_tokens, dtype=tf.int32, name=""start_tokens"")\n        self._end_token = tf.convert_to_tensor(\n            end_token, dtype=tf.int32, name=""end_token"")\n        if self._start_tokens.get_shape().ndims != 1:\n            raise ValueError(""start_tokens must be a vector"")\n        self._batch_size = array_ops.size(start_tokens)\n        if self._end_token.get_shape().ndims != 0:\n            raise ValueError(""end_token must be a scalar"")\n\n        soft_start_tokens = tf.one_hot(\n            self._start_tokens, self._embedding_size, dtype=tf.float32)\n        self._embedding_args_cnt = len(utils.get_args(self._embedding_fn))\n        if self._embedding_args_cnt == 1:\n            self._start_inputs = self._embedding_fn(soft_ids=soft_start_tokens)\n        elif self._embedding_args_cnt == 2:\n            # Position index is 0 in the beginning\n            times = tf.zeros([self._batch_size], dtype=tf.int32)\n            self._start_inputs = self._embedding_fn(\n                soft_ids=soft_start_tokens, times=times)\n        else:\n            raise ValueError(\'`embedding` should expect 1 or 2 arguments.\')\n\n        self._batch_size = tf.size(self._start_tokens)\n        self._tau = tau\n        self._stop_gradient = stop_gradient\n        self._use_finish = use_finish\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def sample_ids_dtype(self):\n        return tf.float32\n\n    @property\n    def sample_ids_shape(self):\n        # A trick to convert a scalar Tensor `self._embedding_size` to\n        # a `TensorShape`\n        oh = tf.one_hot(0, self._embedding_size)\n        return oh.get_shape()[:1]\n\n    def initialize(self, name=None):\n        finished = tf.tile([False], [self._batch_size])\n        return (finished, self._start_inputs)\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Returns `sample_id` which is softmax distributions over vocabulary\n        with temperature `tau`. Shape = `[batch_size, vocab_size]`\n        """"""\n        sample_ids = tf.nn.softmax(outputs / self._tau)\n        return sample_ids\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        if self._use_finish:\n            hard_ids = tf.argmax(sample_ids, axis=-1, output_type=tf.int32)\n            finished = tf.equal(hard_ids, self._end_token)\n        else:\n            finished = tf.tile([False], [self._batch_size])\n        all_finished = tf.reduce_all(finished)\n\n        if self._stop_gradient:\n            sample_ids = tf.stop_gradient(sample_ids)\n\n        if self._embedding_args_cnt == 1:\n            del time, outputs  # unused by next_inputs_fn\n            next_inputs = tf.cond(\n                all_finished,\n                # If we\'re finished, the next_inputs value doesn\'t matter\n                lambda: self._start_inputs,\n                lambda: self._embedding_fn(soft_ids=sample_ids))\n        elif self._embedding_args_cnt == 2:\n            # Prepare the position embedding of the next step\n            times = tf.ones(self._batch_size, dtype=tf.int32) * (time + 1)\n            next_inputs = tf.cond(\n                all_finished,\n                # If we\'re finished, the next_inputs value doesn\'t matter\n                lambda: self._start_inputs,\n                lambda: self._embedding_fn(soft_ids=sample_ids, times=times))\n\n        return (finished, next_inputs, state)\n\n\nclass GumbelSoftmaxEmbeddingHelper(SoftmaxEmbeddingHelper):\n    """"""A helper that feeds gumbel softmax sample to the next step.\n    Uses the gumbel softmax vector to pass through word embeddings to\n    get the next input (i.e., a mixed word embedding).\n\n    A subclass of\n    :tf_main:`Helper <contrib/seq2seq/Helper>`.\n    Used as a helper to :class:`~texar.tf.modules.RNNDecoderBase` :meth:`_build`\n    in inference mode.\n\n    Same as :class:`~texar.tf.modules.SoftmaxEmbeddingHelper` except that here\n    gumbel softmax (instead of softmax) is used.\n\n    Args:\n        embedding: A callable or the `params` argument for\n            :tf_main:`tf.nn.embedding_lookup <nn/embedding_lookup>`.\n            If a callable, it can take a float tensor named `soft_ids` which is\n            a distribution over indexes. For example, the shape of the tensor\n            is typically `[batch_size, vocab_size]`. The callable can also\n            take two arguments (`soft_ids`, `times`), where `soft_ids` is\n            as above, and `times` is an int vector tensor of current\n            time steps (i.e., position ids). The latter case can be used\n            when attr:`embedding` is a combination of word embedding and\n            position embedding.\n        start_tokens: An int tensor shaped `[batch_size]`. The\n            start tokens.\n        end_token: An int scalar tensor. The token that marks end of\n            decoding.\n        tau: A float scalar tensor, the softmax temperature.\n        embedding_size (optional): An int scalar tensor, the number of\n            embedding vectors. Usually it is the vocab size. Required if\n            :attr:`embedding` is a callable.\n        straight_through (bool): Whether to use straight through gradient\n            between time steps. If `True`, a single token with highest\n            probability (i.e., greedy sample) is fed to the next step and\n            gradient is computed using straight through. If `False` (default),\n            the soft gumbel-softmax distribution is fed to the next step.\n        stop_gradient (bool): Whether to stop the gradient backpropagation\n            when feeding softmax vector to the next step.\n        use_finish (bool): Whether to stop decoding once `end_token` is\n            generated. If `False`, decoding will continue until\n            `max_decoding_length` of the decoder is reached.\n    """"""\n    def __init__(self, embedding, start_tokens, end_token, tau,\n                 embedding_size=None, straight_through=False,\n                 stop_gradient=False, use_finish=True):\n        super(GumbelSoftmaxEmbeddingHelper, self).__init__(\n            embedding, start_tokens, end_token, tau, embedding_size,\n            stop_gradient, use_finish)\n        self._straight_through = straight_through\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Returns `sample_id` of shape `[batch_size, vocab_size]`. If\n        `straight_through` is False, this is gumbel softmax distributions over\n        vocabulary with temperature `tau`. If `straight_through` is True,\n        this is one-hot vectors of the greedy samples.\n        """"""\n        sample_ids = tf.nn.softmax(outputs / self._tau)\n        sample_ids = tfpd.RelaxedOneHotCategorical(\n            self._tau, logits=outputs).sample()\n        if self._straight_through:\n            size = tf.shape(sample_ids)[-1]\n            sample_ids_hard = tf.cast(\n                tf.one_hot(tf.argmax(sample_ids, -1), size), sample_ids.dtype)\n            sample_ids = tf.stop_gradient(sample_ids_hard - sample_ids) \\\n                         + sample_ids\n        return sample_ids\n'"
texar/tf/modules/decoders/rnn_decoders.py,39,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RNN decoders.\n""""""\n\n# pylint: disable=no-name-in-module, too-many-arguments, too-many-locals\n# pylint: disable=not-context-manager, protected-access, invalid-name\n\nimport collections\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import AttentionWrapper\nfrom tensorflow.python.util import nest\nfrom tensorflow.contrib.seq2seq import tile_batch\n\nfrom texar.tf.modules.decoders.rnn_decoder_base import RNNDecoderBase\nfrom texar.tf.utils import utils\n\n__all__ = [\n    ""BasicRNNDecoderOutput"",\n    ""AttentionRNNDecoderOutput"",\n    ""BasicRNNDecoder"",\n    ""AttentionRNNDecoder""\n]\n\n\nclass BasicRNNDecoderOutput(\n        collections.namedtuple(""BasicRNNDecoderOutput"",\n                               (""logits"", ""sample_id"", ""cell_output""))):\n    """"""The outputs of basic RNN decoder that include both RNN outputs and\n    sampled ids at each step. This is also used to store results of all the\n    steps after decoding the whole sequence.\n\n    Attributes:\n        logits: The outputs of RNN (at each step/of all steps) by applying the\n            output layer on cell outputs. E.g., in\n            :class:`~texar.tf.modules.BasicRNNDecoder` with default\n            hyperparameters, this is a Tensor of\n            shape `[batch_size, max_time, vocab_size]` after decoding the\n            whole sequence.\n        sample_id: The sampled results (at each step/of all steps). E.g., in\n            BasicRNNDecoder with decoding strategy of train_greedy,\n            this is a Tensor\n            of shape `[batch_size, max_time]` containing the sampled token\n            indexes of all steps.\n        cell_output: The output of RNN cell (at each step/of all steps).\n            This is the results prior to the output layer. E.g., in\n            BasicRNNDecoder with default\n            hyperparameters, this is a Tensor of\n            shape `[batch_size, max_time, cell_output_size]` after decoding\n            the whole sequence.\n    """"""\n    pass\n\n\nclass AttentionRNNDecoderOutput(\n        collections.namedtuple(\n            ""AttentionRNNDecoderOutput"",\n            [""logits"", ""sample_id"", ""cell_output"",\n             ""attention_scores"", ""attention_context""])):\n    r""""""The outputs of attention RNN decoders that additionally include\n    attention results.\n\n    Attributes:\n        logits: The outputs of RNN (at each step/of all steps) by applying the\n            output layer on cell outputs. E.g., in\n            :class:`~texar.tf.modules.AttentionRNNDecoder`, this is a Tensor of\n            shape `[batch_size, max_time, vocab_size]` after decoding.\n        sample_id: The sampled results (at each step/of all steps). E.g., in\n            :class:`~texar.tf.modules.AttentionRNNDecoder` with decoding\n            strategy of train_greedy, this\n            is a Tensor of shape `[batch_size, max_time]` containing the\n            sampled token indexes of all steps.\n        cell_output: The output of RNN cell (at each step/of all steps).\n            This is the results prior to the output layer. E.g., in\n            AttentionRNNDecoder with default\n            hyperparameters, this is a Tensor of\n            shape `[batch_size, max_time, cell_output_size]` after decoding\n            the whole sequence.\n        attention_scores: A single or tuple of `Tensor(s)` containing the\n            alignments emitted (at the previous time step/of all time steps)\n            for each attention mechanism.\n        attention_context: The attention emitted (at the previous time step/of\n            all time steps).\n    """"""\n    pass\n\n\nclass BasicRNNDecoder(RNNDecoderBase):\n    """"""Basic RNN decoder.\n\n    Args:\n        cell (RNNCell, optional): An instance of\n            :tf_main:`RNNCell <ontrib/rnn/RNNCell>`. If `None`\n            (default), a cell is created as specified in\n            :attr:`hparams`.\n        cell_dropout_mode (optional): A Tensor taking value of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, which\n            toggles dropout in the RNN cell (e.g., activates dropout in\n            TRAIN mode). If `None`, :func:`~texar.tf.global_mode` is used.\n            Ignored if :attr:`cell` is given.\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        output_layer (optional): An output layer that transforms cell output\n            to logits. This can be:\n\n            - A callable layer, e.g., an instance \\\n            of :tf_main:`tf.layers.Layer <layers/Layer>`.\n            - A tensor. A dense layer will be created using the tensor \\\n            as the kernel weights. The bias of the dense layer is determined by\\\n            `hparams.output_layer_bias`. This can be used to tie the output \\\n            layer with the input embedding matrix, as proposed in \\\n            https://arxiv.org/pdf/1608.05859.pdf\n            - `None`. A dense layer will be created based on attr:`vocab_size`\\\n            and `hparams.output_layer_bias`.\n            - If no output layer after the cell output is needed, set \\\n            `(vocab_size=None, output_layer=tf.identity)`.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`~texar.tf.modules.RNNDecoderBase._build` for the inputs and outputs\n    of the decoder. The decoder returns\n    `(outputs, final_state, sequence_lengths)`, where `outputs` is an instance\n    of :class:`~texar.tf.modules.BasicRNNDecoderOutput`.\n\n    Example:\n\n        .. code-block:: python\n\n            embedder = WordEmbedder(vocab_size=data.vocab.size)\n            decoder = BasicRNNDecoder(vocab_size=data.vocab.size)\n\n            # Training loss\n            outputs, _, _ = decoder(\n                decoding_strategy=\'train_greedy\',\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\']-1)\n\n            loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n                labels=data_batch[\'text_ids\'][:, 1:],\n                logits=outputs.logits,\n                sequence_length=data_batch[\'length\']-1)\n\n            # Inference sample\n            outputs, _, _ = decoder(\n                decoding_strategy=\'infer_sample\',\n                start_tokens=[data.vocab.bos_token_id]*100,\n                end_token=data.vocab.eos.token_id,\n                embedding=embedder,\n                max_decoding_length=60,\n                mode=tf.estimator.ModeKeys.PREDICT)\n\n            sample_id = sess.run(outputs.sample_id)\n            sample_text = tx.utils.map_ids_to_strs(sample_id, data.vocab)\n            print(sample_text)\n            # [\n            #   the first sequence sample .\n            #   the second sequence sample .\n            #   ...\n            # ]\n    """"""\n\n    def __init__(self,\n                 cell=None,\n                 cell_dropout_mode=None,\n                 vocab_size=None,\n                 output_layer=None,\n                 hparams=None):\n        RNNDecoderBase.__init__(\n            self, cell, vocab_size, output_layer, cell_dropout_mode, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""rnn_cell"": default_rnn_cell_hparams(),\n                ""max_decoding_length_train"": None,\n                ""max_decoding_length_infer"": None,\n                ""helper_train"": {\n                    ""type"": ""TrainingHelper"",\n                    ""kwargs"": {}\n                }\n                ""helper_infer"": {\n                    ""type"": ""SampleEmbeddingHelper"",\n                    ""kwargs"": {}\n                }\n                ""name"": ""basic_rnn_decoder""\n            }\n\n        Here:\n\n        ""rnn_cell"": dict\n            A dictionary of RNN cell hyperparameters. Ignored if\n            :attr:`cell` is given to the decoder constructor.\n            The default value is defined in\n            :func:`~texar.tf.core.default_rnn_cell_hparams`.\n\n        ""max_decoding_length_train"": int or None\n            Maximum allowed number of decoding steps in training mode.\n            If `None` (default), decoding is\n            performed until fully done, e.g., encountering the <EOS> token.\n            Ignored if `max_decoding_length` is given when calling\n            the decoder.\n\n        ""max_decoding_length_infer"": int or None\n            Same as ""max_decoding_length_train"" but for inference mode.\n\n        ""helper_train"": dict\n            The hyperparameters of the helper used in training.\n            ""type"" can be a helper class, its name or module path, or a\n            helper instance. If a class name is given, the class must be\n            from module :tf_main:`tf.contrib.seq2seq <contrib/seq2seq>`,\n            :mod:`texar.tf.modules`, or :mod:`texar.tf.custom`. This is used\n            only when both `decoding_strategy` and `helper` augments are\n            `None` when calling the decoder. See\n            :meth:`~texar.tf.modules.RNNDecoderBase._build` for more details.\n\n        ""helper_infer"": dict\n            Same as ""helper_train"" but during inference mode.\n\n        ""name"": str\n            Name of the decoder.\n\n            The default value is ""basic_rnn_decoder"".\n        """"""\n        hparams = RNNDecoderBase.default_hparams()\n        hparams[""name""] = ""basic_rnn_decoder""\n        return hparams\n\n    def initialize(self, name=None):\n        return self._helper.initialize() + (self._initial_state,)\n\n    def step(self, time, inputs, state, name=None):\n        cell_outputs, cell_state = self._cell(inputs, state)\n        logits = self._output_layer(cell_outputs)\n        sample_ids = self._helper.sample(\n            time=time, outputs=logits, state=cell_state)\n        outputs = BasicRNNDecoderOutput(logits, sample_ids, cell_outputs)\n        return outputs, cell_state\n\n    def next_inputs(self, time, outputs, state):\n        (finished, next_inputs, next_state) = self._helper.next_inputs(\n            time=time,\n            outputs=outputs.logits,\n            state=state,\n            sample_ids=outputs.sample_id)\n        return finished, next_inputs, next_state\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        return outputs, final_state\n\n    @property\n    def output_size(self):\n        """"""Output size of one step.\n        """"""\n        return BasicRNNDecoderOutput(\n            logits=self._rnn_output_size(),\n            sample_id=self._helper.sample_ids_shape,\n            cell_output=self._cell.output_size)\n\n    @property\n    def output_dtype(self):\n        """"""Types of output of one step.\n        """"""\n        # Assume the dtype of the cell is the output_size structure\n        # containing the input_state\'s first component\'s dtype.\n        # Return that structure and the sample_ids_dtype from the helper.\n        dtype = nest.flatten(self._initial_state)[0].dtype\n        return BasicRNNDecoderOutput(\n            logits=nest.map_structure(lambda _: dtype, self._rnn_output_size()),\n            sample_id=self._helper.sample_ids_dtype,\n            cell_output=nest.map_structure(\n                lambda _: dtype, self._cell.output_size))\n\n\nclass AttentionRNNDecoder(RNNDecoderBase):\n    """"""RNN decoder with attention mechanism.\n\n    Args:\n        memory: The memory to query, e.g., the output of an RNN encoder. This\n            tensor should be shaped `[batch_size, max_time, dim]`.\n        memory_sequence_length (optional): A tensor of shape `[batch_size]`\n            containing the sequence lengths for the batch\n            entries in memory. If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n        cell (RNNCell, optional): An instance of `RNNCell`. If `None`, a cell\n            is created as specified in :attr:`hparams`.\n        cell_dropout_mode (optional): A Tensor taking value of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, which\n            toggles dropout in the RNN cell (e.g., activates dropout in\n            TRAIN mode). If `None`, :func:`~texar.tf.global_mode` is used.\n            Ignored if :attr:`cell` is given.\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        output_layer (optional): An output layer that transforms cell output\n            to logits. This can be:\n\n            - A callable layer, e.g., an instance \\\n            of :tf_main:`tf.layers.Layer <layers/Layer>`.\n            - A tensor. A dense layer will be created using the tensor \\\n            as the kernel weights. The bias of the dense layer is determined by\\\n            `hparams.output_layer_bias`. This can be used to tie the output \\\n            layer with the input embedding matrix, as proposed in \\\n            https://arxiv.org/pdf/1608.05859.pdf\n            - `None`. A dense layer will be created based on attr:`vocab_size`\\\n            and `hparams.output_layer_bias`.\n            - If no output layer after the cell output is needed, set \\\n            `(vocab_size=None, output_layer=tf.identity)`.\n        cell_input_fn (callable, optional): A callable that produces RNN cell\n            inputs. If `None` (default), the default is used:\n            `lambda inputs, attention: tf.concat([inputs, attention], -1)`,\n            which cancats regular RNN cell inputs with attentions.\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`~texar.tf.modules.RNNDecoderBase._build` for the inputs and outputs\n    of the decoder. The decoder returns\n    `(outputs, final_state, sequence_lengths)`, where `outputs` is an instance\n    of :class:`~texar.tf.modules.AttentionRNNDecoderOutput`.\n\n    Example:\n\n        .. code-block:: python\n\n            # Encodes the source\n            enc_embedder = WordEmbedder(data.source_vocab.size, ...)\n            encoder = UnidirectionalRNNEncoder(...)\n\n            enc_outputs, _ = encoder(\n                inputs=enc_embedder(data_batch[\'source_text_ids\']),\n                sequence_length=data_batch[\'source_length\'])\n\n            # Decodes while attending to the source\n            dec_embedder = WordEmbedder(vocab_size=data.target_vocab.size, ...)\n            decoder = AttentionRNNDecoder(\n                memory=enc_outputs,\n                memory_sequence_length=data_batch[\'source_length\'],\n                vocab_size=data.target_vocab.size)\n\n            outputs, _, _ = decoder(\n                decoding_strategy=\'train_greedy\',\n                inputs=dec_embedder(data_batch[\'target_text_ids\']),\n                sequence_length=data_batch[\'target_length\']-1)\n    """"""\n    def __init__(self,\n                 memory,\n                 memory_sequence_length=None,\n                 cell=None,\n                 cell_dropout_mode=None,\n                 vocab_size=None,\n                 output_layer=None,\n                 # attention_layer=None, # TODO(zhiting): only valid for tf>=1.0\n                 cell_input_fn=None,\n                 hparams=None):\n        RNNDecoderBase.__init__(\n            self, cell, vocab_size, output_layer, cell_dropout_mode, hparams)\n\n        attn_hparams = self._hparams[\'attention\']\n        attn_kwargs = attn_hparams[\'kwargs\'].todict()\n\n        # Parse the \'probability_fn\' argument\n        if \'probability_fn\' in attn_kwargs:\n            prob_fn = attn_kwargs[\'probability_fn\']\n            if prob_fn is not None and not callable(prob_fn):\n                prob_fn = utils.get_function(\n                    prob_fn,\n                    [\'tensorflow.nn\', \'tensorflow.contrib.sparsemax\',\n                     \'tensorflow.contrib.seq2seq\'])\n            attn_kwargs[\'probability_fn\'] = prob_fn\n\n        attn_kwargs.update({\n            ""memory_sequence_length"": memory_sequence_length,\n            ""memory"": memory})\n        self._attn_kwargs = attn_kwargs\n        attn_modules = [\'tensorflow.contrib.seq2seq\', \'texar.tf.custom\']\n        # Use variable_scope to ensure all trainable variables created in\n        # the attention mechanism are collected\n        with tf.variable_scope(self.variable_scope):\n            attention_mechanism = utils.check_or_get_instance(\n                attn_hparams[""type""], attn_kwargs, attn_modules,\n                classtype=tf.contrib.seq2seq.AttentionMechanism)\n\n        self._attn_cell_kwargs = {\n            ""attention_layer_size"": attn_hparams[""attention_layer_size""],\n            ""alignment_history"": attn_hparams[""alignment_history""],\n            ""output_attention"": attn_hparams[""output_attention""],\n        }\n        self._cell_input_fn = cell_input_fn\n        # Use variable_scope to ensure all trainable variables created in\n        # AttentionWrapper are collected\n        with tf.variable_scope(self.variable_scope):\n            # if attention_layer is not None:\n            #    self._attn_cell_kwargs[""attention_layer_size""] = None\n            attn_cell = AttentionWrapper(\n                self._cell,\n                attention_mechanism,\n                cell_input_fn=self._cell_input_fn,\n                # attention_layer=attention_layer,\n                **self._attn_cell_kwargs)\n            self._cell = attn_cell\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values:\n\n        Common hyperparameters are the same as in\n        :class:`~texar.tf.modules.BasicRNNDecoder`.\n        :meth:`~texar.tf.modules.BasicRNNDecoder.default_hparams`.\n        Additional hyperparameters are for attention mechanism\n        configuration.\n\n        .. code-block:: python\n\n            {\n                ""attention"": {\n                    ""type"": ""LuongAttention"",\n                    ""kwargs"": {\n                        ""num_units"": 256,\n                    },\n                    ""attention_layer_size"": None,\n                    ""alignment_history"": False,\n                    ""output_attention"": True,\n                },\n                # The following hyperparameters are the same as with\n                # `BasicRNNDecoder`\n                ""rnn_cell"": default_rnn_cell_hparams(),\n                ""max_decoding_length_train"": None,\n                ""max_decoding_length_infer"": None,\n                ""helper_train"": {\n                    ""type"": ""TrainingHelper"",\n                    ""kwargs"": {}\n                }\n                ""helper_infer"": {\n                    ""type"": ""SampleEmbeddingHelper"",\n                    ""kwargs"": {}\n                }\n                ""name"": ""attention_rnn_decoder""\n            }\n\n        Here:\n\n        ""attention"": dict\n            Attention hyperparameters, including:\n\n            ""type"": str or class or instance\n                The attention type. Can be an attention class, its name or\n                module path, or a class instance. The class must be a subclass\n                of :tf_main:`TF AttentionMechanism\n                <contrib/seq2seq/AttentionMechanism>`. If class name is\n                given, the class must be from modules\n                :tf_main:`tf.contrib.seq2seq <contrib/seq2seq>` or\n                :mod:`texar.tf.custom`.\n\n                Example:\n\n                    .. code-block:: python\n\n                        # class name\n                        ""type"": ""LuongAttention""\n                        ""type"": ""BahdanauAttention""\n                        # module path\n                        ""type"": ""tf.contrib.seq2seq.BahdanauMonotonicAttention""\n                        ""type"": ""my_module.MyAttentionMechanismClass""\n                        # class\n                        ""type"": tf.contrib.seq2seq.LuongMonotonicAttention\n                        # instance\n                        ""type"": LuongAttention(...)\n\n            ""kwargs"": dict\n                keyword arguments for the attention class constructor.\n                Arguments :attr:`memory` and\n                :attr:`memory_sequence_length` should **not** be\n                specified here because they are given to the decoder\n                constructor. Ignored if ""type"" is an attention class\n                instance. For example\n\n                Example:\n\n                    .. code-block:: python\n\n                        ""type"": ""LuongAttention"",\n                        ""kwargs"": {\n                            ""num_units"": 256,\n                            ""probability_fn"": tf.nn.softmax\n                        }\n\n                    Here ""probability_fn"" can also be set to the string name\n                    or module path to a probability function.\n\n                ""attention_layer_size"": int or None\n                    The depth of the attention (output) layer. The context and\n                    cell output are fed into the attention layer to generate\n                    attention at each time step.\n                    If `None` (default), use the context as attention at each\n                    time step.\n\n                ""alignment_history"": bool\n                    whether to store alignment history from all time steps\n                    in the final output state. (Stored as a time major\n                    `TensorArray` on which you must call `stack()`.)\n\n                ""output_attention"": bool\n                    If `True` (default), the output at each time step is\n                    the attention value. This is the behavior of Luong-style\n                    attention mechanisms. If `False`, the output at each\n                    time step is the output of `cell`.  This is the\n                    beahvior of Bhadanau-style attention mechanisms.\n                    In both cases, the `attention` tensor is propagated to\n                    the next time step via the state and is used there.\n                    This flag only controls whether the attention mechanism\n                    is propagated up to the next cell in an RNN stack or to\n                    the top RNN output.\n        """"""\n        hparams = RNNDecoderBase.default_hparams()\n        hparams[""name""] = ""attention_rnn_decoder""\n        hparams[""attention""] = {\n            ""type"": ""LuongAttention"",\n            ""kwargs"": {\n                ""num_units"": 256,\n            },\n            ""attention_layer_size"": None,\n            ""alignment_history"": False,\n            ""output_attention"": True,\n        }\n        return hparams\n\n    # pylint: disable=arguments-differ\n    def _get_beam_search_cell(self, beam_width):\n        """"""Returns the RNN cell for beam search decoding.\n        """"""\n        with tf.variable_scope(self.variable_scope, reuse=True):\n            attn_kwargs = copy.copy(self._attn_kwargs)\n\n            memory = attn_kwargs[\'memory\']\n            attn_kwargs[\'memory\'] = tile_batch(memory, multiplier=beam_width)\n\n            memory_seq_length = attn_kwargs[\'memory_sequence_length\']\n            if memory_seq_length is not None:\n                attn_kwargs[\'memory_sequence_length\'] = tile_batch(\n                    memory_seq_length, beam_width)\n\n            attn_modules = [\'tensorflow.contrib.seq2seq\', \'texar.tf.custom\']\n            bs_attention_mechanism = utils.check_or_get_instance(\n                self._hparams.attention.type, attn_kwargs, attn_modules,\n                classtype=tf.contrib.seq2seq.AttentionMechanism)\n\n            bs_attn_cell = AttentionWrapper(\n                self._cell._cell,\n                bs_attention_mechanism,\n                cell_input_fn=self._cell_input_fn,\n                **self._attn_cell_kwargs)\n\n            self._beam_search_cell = bs_attn_cell\n\n            return bs_attn_cell\n\n    def initialize(self, name=None):\n        helper_init = self._helper.initialize()\n\n        flat_initial_state = nest.flatten(self._initial_state)\n        dtype = flat_initial_state[0].dtype\n        initial_state = self._cell.zero_state(\n            batch_size=tf.shape(flat_initial_state[0])[0], dtype=dtype)\n        initial_state = initial_state.clone(cell_state=self._initial_state)\n\n        return [helper_init[0], helper_init[1], initial_state]\n\n    def step(self, time, inputs, state, name=None):\n        wrapper_outputs, wrapper_state = self._cell(inputs, state)\n        # Essentisally the same as in BasicRNNDecoder.step()\n        logits = self._output_layer(wrapper_outputs)\n        sample_ids = self._helper.sample(\n            time=time, outputs=logits, state=wrapper_state)\n\n        attention_scores = wrapper_state.alignments\n        attention_context = wrapper_state.attention\n        outputs = AttentionRNNDecoderOutput(\n            logits, sample_ids, wrapper_outputs,\n            attention_scores, attention_context)\n\n        return (outputs, wrapper_state)\n\n    def next_inputs(self, time, outputs, state):\n        (finished, next_inputs, state) = self._helper.next_inputs(\n            time=time,\n            outputs=outputs.logits,\n            state=state,\n            sample_ids=outputs.sample_id)\n        return (finished, next_inputs, state)\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        return outputs, final_state\n\n    def _alignments_size(self):\n        # Reimplementation of the alignments_size of each of\n        # AttentionWrapper.attention_mechanisms. The original implementation\n        # of `_BaseAttentionMechanism._alignments_size`:\n        #\n        #    self._alignments_size = (self._keys.shape[1].value or\n        #                       array_ops.shape(self._keys)[1])\n        #\n        # can be `None` when the seq length of encoder outputs are priori\n        # unknown.\n        alignments_size = []\n        for am in self._cell._attention_mechanisms:\n            az = (am._keys.shape[1].value or tf.shape(am._keys)[1:-1])\n            alignments_size.append(az)\n        return self._cell._item_or_tuple(alignments_size)\n\n    @property\n    def output_size(self):\n        return AttentionRNNDecoderOutput(\n            logits=self._rnn_output_size(),\n            sample_id=self._helper.sample_ids_shape,\n            cell_output=self._cell.output_size,\n            attention_scores=self._alignments_size(),\n            attention_context=self._cell.state_size.attention)\n\n    @property\n    def output_dtype(self):\n        """"""Types of output of one step.\n        """"""\n        # Assume the dtype of the cell is the output_size structure\n        # containing the input_state\'s first component\'s dtype.\n        # Return that structure and the sample_ids_dtype from the helper.\n        dtype = nest.flatten(self._initial_state)[0].dtype\n        return AttentionRNNDecoderOutput(\n            logits=nest.map_structure(lambda _: dtype, self._rnn_output_size()),\n            sample_id=self._helper.sample_ids_dtype,\n            cell_output=nest.map_structure(\n                lambda _: dtype, self._cell.output_size),\n            attention_scores=nest.map_structure(\n                lambda _: dtype, self._alignments_size()),\n            attention_context=nest.map_structure(\n                lambda _: dtype, self._cell.state_size.attention))\n\n    def zero_state(self, batch_size, dtype):\n        """"""Returns zero state of the basic cell.\n        Equivalent to :attr:`decoder.cell._cell.zero_state`.\n        """"""\n        return self._cell._cell.zero_state(batch_size=batch_size, dtype=dtype)\n\n    def wrapper_zero_state(self, batch_size, dtype):\n        """"""Returns zero state of the attention-wrapped cell.\n        Equivalent to :attr:`decoder.cell.zero_state`.\n        """"""\n        return self._cell.zero_state(batch_size=batch_size, dtype=dtype)\n\n    @property\n    def state_size(self):\n        """"""The state size of the basic cell.\n        Equivalent to :attr:`decoder.cell._cell.state_size`.\n        """"""\n        return self._cell._cell.state_size\n\n    @property\n    def wrapper_state_size(self):\n        """"""The state size of the attention-wrapped cell.\n        Equivalent to :attr:`decoder.cell.state_size`.\n        """"""\n        return self._cell.state_size\n'"
texar/tf/modules/decoders/tf_helpers.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Modifications copyright (C) 2019 Texar\n# ==============================================================================\n""""""A library of helpers for use with Texar RNN/Transformer decoders.\n\nAdapted from the `tensorflow.contrib.seq2seq` package.\n""""""\n\n# pylint: disable=no-name-in-module\n\nimport abc\n\nimport six\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow_probability import distributions as tfpd\nfrom tensorflow.python.util import nest\n\nfrom texar.tf.utils.shapes import shape_list\nfrom texar.tf.utils.utils import get_args\n\n__all__ = [\n    ""Helper"",\n    ""TrainingHelper"",\n    ""GreedyEmbeddingHelper"",\n    ""SampleEmbeddingHelper"",\n    ""CustomHelper"",\n    ""ScheduledEmbeddingTrainingHelper"",\n    ""ScheduledOutputTrainingHelper"",\n    ""InferenceHelper"",\n]\n\n_transpose_batch_time = decoder._transpose_batch_time  # pylint: disable=protected-access\n\n\ndef _unstack_ta(inp):\n    return tensor_array_ops.TensorArray(\n        dtype=inp.dtype, size=array_ops.shape(inp)[0],\n        element_shape=inp.get_shape()[1:]).unstack(inp)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Helper(object):\n    """"""Interface for implementing different decoding strategies in\n    :class:`RNN decoders <texar.tf.modules.RNNDecoderBase>` and\n    :class:`Transformer decoder <texar.tf.modules.TransformerDecoder>`.\n\n    Adapted from the `tensorflow.contrib.seq2seq` package.\n    """"""\n\n    @abc.abstractproperty\n    def batch_size(self):\n        """"""Batch size of tensor returned by `sample`.\n\n        Returns a scalar int32 tensor.\n        """"""\n        raise NotImplementedError(""batch_size has not been implemented"")\n\n    @abc.abstractproperty\n    def sample_ids_shape(self):\n        """"""Shape of tensor returned by `sample`, excluding the batch dimension.\n\n        Returns a `TensorShape`.\n        """"""\n        raise NotImplementedError(""sample_ids_shape has not been implemented"")\n\n    @abc.abstractproperty\n    def sample_ids_dtype(self):\n        """"""DType of tensor returned by `sample`.\n\n        Returns a DType.\n        """"""\n        raise NotImplementedError(""sample_ids_dtype has not been implemented"")\n\n    @abc.abstractmethod\n    def initialize(self, name=None):\n        """"""Returns `(initial_finished, initial_inputs)`.""""""\n        pass\n\n    @abc.abstractmethod\n    def sample(self, time, outputs, state, name=None):\n        """"""Returns `sample_ids`.""""""\n        pass\n\n    @abc.abstractmethod\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        """"""Returns `(finished, next_inputs, next_state)`.""""""\n        pass\n\n\nclass CustomHelper(Helper):\n    """"""Base abstract class that allows the user to customize decoding.""""""\n\n    def __init__(self, initialize_fn, sample_fn, next_inputs_fn,\n                 sample_ids_shape=None, sample_ids_dtype=None):\n        """"""Initializer.\n\n        Args:\n          initialize_fn: callable that returns `(finished, next_inputs)`\n            for the first iteration.\n          sample_fn: callable that takes `(time, outputs, state)`\n            and emits tensor `sample_ids`.\n          next_inputs_fn: callable that takes `(time, outputs, state, sample_ids)`\n            and emits `(finished, next_inputs, next_state)`.\n          sample_ids_shape: Either a list of integers, or a 1-D Tensor of type\n            `int32`, the shape of each value in the `sample_ids` batch. Defaults to\n            a scalar.\n          sample_ids_dtype: The dtype of the `sample_ids` tensor. Defaults to int32.\n        """"""\n        self._initialize_fn = initialize_fn\n        self._sample_fn = sample_fn\n        self._next_inputs_fn = next_inputs_fn\n        self._batch_size = None\n        self._sample_ids_shape = tensor_shape.TensorShape(sample_ids_shape or [])\n        self._sample_ids_dtype = sample_ids_dtype or dtypes.int32\n\n    @property\n    def batch_size(self):\n        if self._batch_size is None:\n            raise ValueError(""batch_size accessed before initialize was called"")\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return self._sample_ids_shape\n\n    @property\n    def sample_ids_dtype(self):\n        return self._sample_ids_dtype\n\n    def initialize(self, name=None):\n        with ops.name_scope(name, ""%sInitialize"" % type(self).__name__):\n            (finished, next_inputs) = self._initialize_fn()\n            if self._batch_size is None:\n                self._batch_size = array_ops.size(finished)\n        return (finished, next_inputs)\n\n    def sample(self, time, outputs, state, name=None):\n        with ops.name_scope(\n                name, ""%sSample"" % type(self).__name__, (time, outputs, state)):\n            return self._sample_fn(time=time, outputs=outputs, state=state)\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        with ops.name_scope(\n                name, ""%sNextInputs"" % type(self).__name__, (time, outputs, state)):\n            return self._next_inputs_fn(\n                time=time, outputs=outputs, state=state, sample_ids=sample_ids)\n\n\nclass TrainingHelper(Helper):\n    """"""A helper for use during training. Performs teacher-forcing decoding.\n\n    Returned sample_ids are the argmax of the RNN output logits.\n\n    Note that for teacher-forcing decoding, Texar\'s decoders provide a simpler\n    interface by specifying `decoding_strategy=\'train_greedy\'` when calling a\n    decoder (see, e.g.,,\n    :meth:`RNN decoder <texar.tf.modules.RNNDecoderBase._build>`). In this case,\n    use of TrainingHelper is not necessary.\n    """"""\n\n    def __init__(self, inputs, sequence_length, time_major=False, name=None):\n        """"""Initializer.\n\n        Args:\n          inputs: A (structure of) input tensors.\n          sequence_length: An int32 vector tensor.\n          time_major: Python bool.  Whether the tensors in `inputs` are time major.\n            If `False` (default), they are assumed to be batch major.\n          name: Name scope for any created operations.\n\n        Raises:\n          ValueError: if `sequence_length` is not a 1D tensor.\n        """"""\n        with ops.name_scope(name, ""TrainingHelper"", [inputs, sequence_length]):\n            inputs = ops.convert_to_tensor(inputs, name=""inputs"")\n            self._inputs = inputs\n            if not time_major:\n                inputs = nest.map_structure(_transpose_batch_time, inputs)\n\n            self._input_tas = nest.map_structure(_unstack_ta, inputs)\n            self._sequence_length = ops.convert_to_tensor(\n                sequence_length, name=""sequence_length"")\n            if self._sequence_length.get_shape().ndims != 1:\n                raise ValueError(\n                    ""Expected sequence_length to be a vector, but received shape: %s"" %\n                    self._sequence_length.get_shape())\n\n            self._zero_inputs = nest.map_structure(\n                lambda inp: array_ops.zeros_like(inp[0, :]), inputs)\n            self._start_inputs = self._zero_inputs\n            self._batch_size = shape_list(sequence_length)[0]\n\n    @property\n    def inputs(self):\n        return self._inputs\n\n    @property\n    def sequence_length(self):\n        return self._sequence_length\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return tensor_shape.TensorShape([])\n\n    @property\n    def sample_ids_dtype(self):\n        return dtypes.int32\n\n    def initialize(self, name=None):\n        with ops.name_scope(name, ""TrainingHelperInitialize""):\n            finished = math_ops.equal(0, self._sequence_length)\n            all_finished = math_ops.reduce_all(finished)\n            next_inputs = control_flow_ops.cond(\n                all_finished, lambda: self._zero_inputs,\n                lambda: nest.map_structure(lambda inp: inp.read(0), self._input_tas))\n            return (finished, next_inputs)\n\n    def sample(self, time, outputs, name=None, **unused_kwargs):\n        """"""Gets a sample for one step.""""""\n        with ops.name_scope(name, ""TrainingHelperSample"", [time, outputs]):\n            sample_ids = math_ops.cast(\n                math_ops.argmax(outputs, axis=-1), dtypes.int32)\n            return sample_ids\n\n    def next_inputs(self, time, outputs, state, name=None, **unused_kwargs):\n        """"""Gets the inputs for next step.""""""\n        with ops.name_scope(name, ""TrainingHelperNextInputs"",\n                            [time, outputs, state]):\n            next_time = time + 1\n            finished = (next_time >= self._sequence_length)\n            all_finished = math_ops.reduce_all(finished)\n\n            def read_from_ta(inp):\n                return inp.read(next_time)\n\n            next_inputs = control_flow_ops.cond(\n                all_finished, lambda: self._zero_inputs,\n                lambda: nest.map_structure(read_from_ta, self._input_tas))\n            return (finished, next_inputs, state)\n\n\nclass ScheduledEmbeddingTrainingHelper(TrainingHelper):\n    """"""A training helper that adds scheduled sampling.\n\n    Returns -1s for sample_ids where no sampling took place; valid sample id\n    values elsewhere.\n    """"""\n\n    def __init__(self, inputs, sequence_length, embedding, sampling_probability,\n                 time_major=False, seed=None, scheduling_seed=None, name=None):\n        """"""Initializer.\n\n        Args:\n          inputs: A (structure of) input tensors.\n          sequence_length: An int32 vector tensor.\n          embedding: A callable or the `params` argument for `embedding_lookup`.\n            If a callable, it can take a vector tensor of token `ids`,\n            or take two arguments (`ids`, `times`), where `ids` is a vector\n            tensor of token ids, and `times` is a vector tensor of current\n            time steps (i.e., position ids). The latter case can be used when\n            attr:`embedding` is a combination of word embedding and position\n            embedding.\n          sampling_probability: A 0D `float32` tensor: the probability of sampling\n            categorically from the output ids instead of reading directly from the\n            inputs.\n          time_major: Python bool.  Whether the tensors in `inputs` are time major.\n            If `False` (default), they are assumed to be batch major.\n          seed: The sampling seed.\n          scheduling_seed: The schedule decision rule sampling seed.\n          name: Name scope for any created operations.\n\n        Raises:\n          ValueError: if `sampling_probability` is not a scalar or vector.\n        """"""\n        with ops.name_scope(name, ""ScheduledEmbeddingSamplingWrapper"",\n                            [embedding, sampling_probability]):\n            if callable(embedding):\n                self._embedding_fn = embedding\n            else:\n                self._embedding_fn = (\n                    lambda ids: embedding_ops.embedding_lookup(embedding, ids))\n\n            self._embedding_args_cnt = len(get_args(self._embedding_fn))\n            if self._embedding_args_cnt != 1 and self._embedding_args_cnt != 2:\n                raise ValueError(\'`embedding` should expect 1 or 2 arguments.\')\n\n            self._sampling_probability = ops.convert_to_tensor(\n                sampling_probability, name=""sampling_probability"")\n            if self._sampling_probability.get_shape().ndims not in (0, 1):\n                raise ValueError(\n                    ""sampling_probability must be either a scalar or a vector. ""\n                    ""saw shape: %s"" % (self._sampling_probability.get_shape()))\n            self._seed = seed\n            self._scheduling_seed = scheduling_seed\n            super(ScheduledEmbeddingTrainingHelper, self).__init__(\n                inputs=inputs,\n                sequence_length=sequence_length,\n                time_major=time_major,\n                name=name)\n\n    def initialize(self, name=None):\n        return super(ScheduledEmbeddingTrainingHelper, self).initialize(\n            name=name)\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Gets a sample for one step.""""""\n        with ops.name_scope(name, ""ScheduledEmbeddingTrainingHelperSample"",\n                            [time, outputs, state]):\n            # Return -1s where we did not sample, and sample_ids elsewhere\n            select_sampler = tfpd.Bernoulli(\n                probs=self._sampling_probability, dtype=dtypes.bool)\n            select_sample = select_sampler.sample(\n                sample_shape=self.batch_size, seed=self._scheduling_seed)\n            sample_id_sampler = tfpd.Categorical(logits=outputs)\n            return array_ops.where(\n                select_sample,\n                sample_id_sampler.sample(seed=self._seed),\n                gen_array_ops.fill([self.batch_size], -1))\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        """"""Gets the outputs for next step.""""""\n        with ops.name_scope(name, ""ScheduledEmbeddingTrainingHelperNextInputs"",\n                            [time, outputs, state, sample_ids]):\n            (finished, base_next_inputs, state) = (\n                super(ScheduledEmbeddingTrainingHelper, self).next_inputs(\n                    time=time,\n                    outputs=outputs,\n                    state=state,\n                    sample_ids=sample_ids,\n                    name=name))\n\n            def maybe_sample():\n                """"""Perform scheduled sampling.""""""\n                where_sampling = math_ops.cast(\n                    array_ops.where(sample_ids > -1), dtypes.int32)\n                where_not_sampling = math_ops.cast(\n                    array_ops.where(sample_ids <= -1), dtypes.int32)\n                sample_ids_sampling = array_ops.gather_nd(sample_ids, where_sampling)\n                inputs_not_sampling = array_ops.gather_nd(\n                    base_next_inputs, where_not_sampling)\n\n                if self._embedding_args_cnt == 1:\n                    sampled_next_inputs = self._embedding_fn(\n                        sample_ids_sampling)\n                elif self._embedding_args_cnt == 2:\n                    # Prepare the position embedding of the next step\n                    times = tf.ones(self._batch_size,\n                                    dtype=tf.int32) * (time + 1)\n                    sampled_next_inputs = self._embedding_fn(\n                        sample_ids_sampling, times)\n                base_shape = array_ops.shape(base_next_inputs)\n                return (array_ops.scatter_nd(indices=where_sampling,\n                                             updates=sampled_next_inputs,\n                                             shape=base_shape)\n                        + array_ops.scatter_nd(indices=where_not_sampling,\n                                               updates=inputs_not_sampling,\n                                               shape=base_shape))\n\n            all_finished = math_ops.reduce_all(finished)\n            next_inputs = control_flow_ops.cond(\n                all_finished, lambda: base_next_inputs, maybe_sample)\n            return (finished, next_inputs, state)\n\n\nclass ScheduledOutputTrainingHelper(TrainingHelper):\n    """"""A training helper that adds scheduled sampling directly to outputs.\n\n    Returns False for sample_ids where no sampling took place; True elsewhere.\n    """"""\n\n    def __init__(self, inputs, sequence_length, sampling_probability,\n                 time_major=False, seed=None, next_inputs_fn=None,\n                 auxiliary_inputs=None, name=None):\n        """"""Initializer.\n\n        Args:\n          inputs: A (structure) of input tensors.\n          sequence_length: An int32 vector tensor.\n          sampling_probability: A 0D `float32` tensor: the probability of sampling\n            from the outputs instead of reading directly from the inputs.\n          time_major: Python bool.  Whether the tensors in `inputs` are time major.\n            If `False` (default), they are assumed to be batch major.\n          seed: The sampling seed.\n          next_inputs_fn: (Optional) callable to apply to the RNN outputs to create\n            the next input when sampling. If `None` (default), the RNN outputs will\n            be used as the next inputs.\n          auxiliary_inputs: An optional (structure of) auxiliary input tensors with\n            a shape that matches `inputs` in all but (potentially) the final\n            dimension. These tensors will be concatenated to the sampled output or\n            the `inputs` when not sampling for use as the next input.\n          name: Name scope for any created operations.\n\n        Raises:\n          ValueError: if `sampling_probability` is not a scalar or vector.\n        """"""\n        with ops.name_scope(name, ""ScheduledOutputTrainingHelper"",\n                            [inputs, auxiliary_inputs, sampling_probability]):\n            self._sampling_probability = ops.convert_to_tensor(\n                sampling_probability, name=""sampling_probability"")\n            if self._sampling_probability.get_shape().ndims not in (0, 1):\n                raise ValueError(\n                    ""sampling_probability must be either a scalar or a vector. ""\n                    ""saw shape: %s"" % (self._sampling_probability.get_shape()))\n\n            if auxiliary_inputs is None:\n                maybe_concatenated_inputs = inputs\n            else:\n                inputs = ops.convert_to_tensor(inputs, name=""inputs"")\n                auxiliary_inputs = ops.convert_to_tensor(\n                    auxiliary_inputs, name=""auxiliary_inputs"")\n                maybe_concatenated_inputs = nest.map_structure(\n                    lambda x, y: array_ops.concat((x, y), -1),\n                    inputs, auxiliary_inputs)\n                if not time_major:\n                    auxiliary_inputs = nest.map_structure(\n                        _transpose_batch_time, auxiliary_inputs)\n\n            self._auxiliary_input_tas = (\n                nest.map_structure(_unstack_ta, auxiliary_inputs)\n                if auxiliary_inputs is not None else None)\n\n            self._seed = seed\n\n            self._next_inputs_fn = next_inputs_fn\n\n            super(ScheduledOutputTrainingHelper, self).__init__(\n                inputs=maybe_concatenated_inputs,\n                sequence_length=sequence_length,\n                time_major=time_major,\n                name=name)\n\n    def initialize(self, name=None):\n        return super(ScheduledOutputTrainingHelper, self).initialize(name=name)\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Gets a sample for one step.""""""\n        with ops.name_scope(name, ""ScheduledOutputTrainingHelperSample"",\n                            [time, outputs, state]):\n            sampler = tfpd.Bernoulli(probs=self._sampling_probability)\n            return sampler.sample(sample_shape=self.batch_size, seed=self._seed)\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        """"""Gets the next inputs for next step.""""""\n        with ops.name_scope(name, ""ScheduledOutputTrainingHelperNextInputs"",\n                            [time, outputs, state, sample_ids]):\n            (finished, base_next_inputs, state) = (\n                super(ScheduledOutputTrainingHelper, self).next_inputs(\n                    time=time,\n                    outputs=outputs,\n                    state=state,\n                    sample_ids=sample_ids,\n                    name=name))\n            sample_ids = math_ops.cast(sample_ids, dtypes.bool)\n\n            def maybe_sample():\n                """"""Perform scheduled sampling.""""""\n\n                def maybe_concatenate_auxiliary_inputs(outputs_, indices=None):\n                    """"""Concatenate outputs with auxiliary inputs, if they exist.""""""\n                    if self._auxiliary_input_tas is None:\n                        return outputs_\n\n                    next_time = time + 1\n                    auxiliary_inputs = nest.map_structure(\n                        lambda ta: ta.read(next_time), self._auxiliary_input_tas)\n                    if indices is not None:\n                        auxiliary_inputs = array_ops.gather_nd(auxiliary_inputs, indices)\n                    return nest.map_structure(\n                        lambda x, y: array_ops.concat((x, y), -1),\n                        outputs_, auxiliary_inputs)\n\n                if self._next_inputs_fn is None:\n                    return array_ops.where(\n                        sample_ids, maybe_concatenate_auxiliary_inputs(outputs),\n                        base_next_inputs)\n\n                where_sampling = math_ops.cast(\n                    array_ops.where(sample_ids), dtypes.int32)\n                where_not_sampling = math_ops.cast(\n                    array_ops.where(math_ops.logical_not(sample_ids)), dtypes.int32)\n                outputs_sampling = array_ops.gather_nd(outputs, where_sampling)\n                inputs_not_sampling = array_ops.gather_nd(base_next_inputs,\n                                                          where_not_sampling)\n                sampled_next_inputs = maybe_concatenate_auxiliary_inputs(\n                    self._next_inputs_fn(outputs_sampling), where_sampling)\n\n                base_shape = array_ops.shape(base_next_inputs)\n                return (array_ops.scatter_nd(indices=where_sampling,\n                                             updates=sampled_next_inputs,\n                                             shape=base_shape)\n                        + array_ops.scatter_nd(indices=where_not_sampling,\n                                               updates=inputs_not_sampling,\n                                               shape=base_shape))\n\n            all_finished = math_ops.reduce_all(finished)\n            no_samples = math_ops.logical_not(math_ops.reduce_any(sample_ids))\n            next_inputs = control_flow_ops.cond(\n                math_ops.logical_or(all_finished, no_samples),\n                lambda: base_next_inputs, maybe_sample)\n            return (finished, next_inputs, state)\n\n\nclass GreedyEmbeddingHelper(Helper):\n    """"""A helper for use during inference.\n\n    Uses the argmax of the output (treated as logits) and passes the\n    result through an embedding layer to get the next input.\n\n    Note that for greedy decoding, Texar\'s decoders provide a simpler\n    interface by specifying `decoding_strategy=\'infer_greedy\'` when calling a\n    decoder (see, e.g.,,\n    :meth:`RNN decoder <texar.tf.modules.RNNDecoderBase._build>`). In this case,\n    use of GreedyEmbeddingHelper is not necessary.\n    """"""\n\n    def __init__(self, embedding, start_tokens, end_token):\n        """"""Initializer.\n\n        Args:\n          embedding: A callable or the `params` argument for `embedding_lookup`.\n            If a callable, it can take a vector tensor of `ids` (argmax ids),\n            or take two arguments (`ids`, `times`), where `ids` is a vector\n            tensor of argmax ids, and `times` is a vector tensor of current\n            time steps (i.e., position ids). The latter case can be used when\n            attr:`embedding` is a combination of word embedding and position\n            embedding.\n            The returned tensor will be returned by :meth:`next_inputs`.\n          start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n          end_token: `int32` scalar, the token that marks end of decoding.\n\n        Raises:\n          ValueError: if `start_tokens` is not a 1D tensor or `end_token` is not a\n            scalar.\n        """"""\n        if callable(embedding):\n            self._embedding_fn = embedding\n        else:\n            self._embedding_fn = (\n                lambda ids: embedding_ops.embedding_lookup(embedding, ids))\n\n        self._start_tokens = ops.convert_to_tensor(\n            start_tokens, dtype=dtypes.int32, name=""start_tokens"")\n        self._end_token = ops.convert_to_tensor(\n            end_token, dtype=dtypes.int32, name=""end_token"")\n        if self._start_tokens.get_shape().ndims != 1:\n            raise ValueError(""start_tokens must be a vector"")\n        self._batch_size = shape_list(start_tokens)[0]\n        if self._end_token.get_shape().ndims != 0:\n            raise ValueError(""end_token must be a scalar"")\n\n        self._embedding_args_cnt = len(get_args(self._embedding_fn))\n        if self._embedding_args_cnt == 1:\n            self._start_inputs = self._embedding_fn(self._start_tokens)\n        elif self._embedding_args_cnt == 2:\n            # Position index is 0 in the beginning\n            times = tf.zeros([self._batch_size], dtype=tf.int32)\n            self._start_inputs = self._embedding_fn(self._start_tokens, times)\n        else:\n            raise ValueError(\'`embedding` should expect 1 or 2 arguments.\')\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return tensor_shape.TensorShape([])\n\n    @property\n    def sample_ids_dtype(self):\n        return dtypes.int32\n\n    def initialize(self, name=None):\n        finished = array_ops.tile([False], [self._batch_size])\n        return finished, self._start_inputs\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Gets a sample for one step.""""""\n        del time, state  # unused by sample_fn\n        # Outputs are logits, use argmax to get the most probable id\n        if not isinstance(outputs, ops.Tensor):\n            raise TypeError(""Expected outputs to be a single Tensor, got: %s"" %\n                            type(outputs))\n        sample_ids = math_ops.argmax(outputs, axis=-1, output_type=dtypes.int32)\n        return sample_ids\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        """"""Gets the inputs for next step.""""""\n        finished = math_ops.equal(sample_ids, self._end_token)\n        all_finished = math_ops.reduce_all(finished)\n\n        if self._embedding_args_cnt == 1:\n            del time, outputs  # unused by next_inputs_fn\n            next_inputs = control_flow_ops.cond(\n                all_finished,\n                # If we\'re finished, the next_inputs value doesn\'t matter\n                lambda: self._start_inputs,\n                lambda: self._embedding_fn(sample_ids))\n        elif self._embedding_args_cnt == 2:\n            del outputs\n            # Prepare the position embedding of the next step\n            times = tf.ones(self._batch_size, dtype=tf.int32) * (time + 1)\n            next_inputs = control_flow_ops.cond(\n                all_finished,\n                # If we\'re finished, the next_inputs value doesn\'t matter\n                lambda: self._start_inputs,\n                lambda: self._embedding_fn(sample_ids, times))\n\n        return finished, next_inputs, state\n\n\nclass SampleEmbeddingHelper(GreedyEmbeddingHelper):\n    """"""A helper for use during inference.\n\n    Uses sampling (from a distribution) instead of argmax and passes the\n    result through an embedding layer to get the next input.\n\n    Note that for sample decoding, Texar\'s decoders provide a simpler\n    interface by specifying `decoding_strategy=\'infer_sample\'` when calling a\n    decoder (see, e.g.,,\n    :meth:`RNN decoder <texar.tf.modules.RNNDecoderBase._build>`). In this case,\n    use of SampleEmbeddingHelper is not necessary.\n    """"""\n\n    def __init__(self, embedding, start_tokens, end_token,\n                 softmax_temperature=None, seed=None):\n        """"""Initializer.\n\n        Args:\n          embedding: A callable or the `params` argument for `embedding_lookup`.\n            If a callable, it can take a vector tensor of token `ids`,\n            or take two arguments (`ids`, `times`), where `ids` is a vector\n            tensor of token ids, and `times` is a vector tensor of current\n            time steps (i.e., position ids). The latter case can be used when\n            attr:`embedding` is a combination of word embedding and position\n            embedding.\n            The returned tensor will be returned by :meth:`next_inputs`.\n          start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n          end_token: `int32` scalar, the token that marks end of decoding.\n          softmax_temperature: (Optional) `float32` scalar, value to divide the\n            logits by before computing the softmax. Larger values (above 1.0) result\n            in more random samples, while smaller values push the sampling\n            distribution towards the argmax. Must be strictly greater than 0.\n            Defaults to 1.0.\n          seed: (Optional) The sampling seed.\n\n        Raises:\n          ValueError: if `start_tokens` is not a 1D tensor or `end_token` is not a\n            scalar.\n        """"""\n        super(SampleEmbeddingHelper, self).__init__(\n            embedding, start_tokens, end_token)\n        self._softmax_temperature = softmax_temperature\n        self._seed = seed\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Gets a sample for one step.""""""\n        del time, state  # unused by sample_fn\n        # Outputs are logits, we sample instead of argmax (greedy).\n        if not isinstance(outputs, ops.Tensor):\n            raise TypeError(""Expected outputs to be a single Tensor, got: %s"" %\n                            type(outputs))\n        if self._softmax_temperature is None:\n            logits = outputs\n        else:\n            logits = outputs / self._softmax_temperature\n\n        sample_id_sampler = tfpd.Categorical(logits=logits)\n        sample_ids = sample_id_sampler.sample(seed=self._seed)\n\n        return sample_ids\n\n\nclass InferenceHelper(Helper):\n    """"""A helper to use during inference with a custom sampling function.""""""\n\n    def __init__(self, sample_fn, sample_shape, sample_dtype,\n                 start_inputs, end_fn, next_inputs_fn=None):\n        """"""Initializer.\n\n        Args:\n          sample_fn: A callable that takes `outputs` and emits tensor `sample_ids`.\n          sample_shape: Either a list of integers, or a 1-D Tensor of type `int32`,\n            the shape of the each sample in the batch returned by `sample_fn`.\n          sample_dtype: the dtype of the sample returned by `sample_fn`.\n          start_inputs: The initial batch of inputs.\n          end_fn: A callable that takes `sample_ids` and emits a `bool` vector\n            shaped `[batch_size]` indicating whether each sample is an end token.\n          next_inputs_fn: (Optional) A callable that takes `sample_ids` and returns\n            the next batch of inputs. If not provided, `sample_ids` is used as the\n            next batch of inputs.\n        """"""\n        self._sample_fn = sample_fn\n        self._end_fn = end_fn\n        self._sample_shape = tensor_shape.TensorShape(sample_shape)\n        self._sample_dtype = sample_dtype\n        self._next_inputs_fn = next_inputs_fn\n        self._batch_size = array_ops.shape(start_inputs)[0]\n        self._start_inputs = ops.convert_to_tensor(\n            start_inputs, name=""start_inputs"")\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return self._sample_shape\n\n    @property\n    def sample_ids_dtype(self):\n        return self._sample_dtype\n\n    def initialize(self, name=None):\n        finished = array_ops.tile([False], [self._batch_size])\n        return (finished, self._start_inputs)\n\n    def sample(self, time, outputs, state, name=None):\n        """"""Gets a sample for one step.""""""\n        del time, state  # unused by sample\n        return self._sample_fn(outputs)\n\n    def next_inputs(self, time, outputs, state, sample_ids, name=None):\n        """"""Gets the outputs for next step.""""""\n        del time, outputs  # unused by next_inputs\n        if self._next_inputs_fn is None:\n            next_inputs = sample_ids\n        else:\n            next_inputs = self._next_inputs_fn(sample_ids)\n        finished = self._end_fn(sample_ids)\n        return (finished, next_inputs, state)\n'"
texar/tf/modules/decoders/transformer_decoders.py,73,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTransformer decoder.\n""""""\n\n# pylint: disable=no-name-in-module, too-many-arguments, too-many-locals\n# pylint: disable=invalid-name, too-many-instance-attributes,\n# pylint: disable=too-many-branches, redefined-variable-type\n\nimport collections\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Decoder as TFDecoder\n\nfrom texar.tf.core import layers\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.modules.networks.networks import FeedForwardNetwork\nfrom texar.tf.modules.encoders.transformer_encoders import \\\n    default_transformer_poswise_net_hparams\nfrom texar.tf.modules.encoders.multihead_attention import \\\n    MultiheadAttentionEncoder\nfrom texar.tf.modules.decoders.rnn_decoder_base import _make_output_layer\nfrom texar.tf.modules.decoders import tf_helpers as tx_helper\nfrom texar.tf.utils import beam_search, transformer_attentions as attn\nfrom texar.tf.utils.shapes import shape_list\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.modules.decoders.dynamic_decode import dynamic_decode\n\n\n__all__ = [\n    ""TransformerDecoderOutput"",\n    ""TransformerDecoder""\n]\n\n\nclass TransformerDecoderOutput(collections.namedtuple(\n        ""TransformerDecoderOutput"",\n        (""logits"", ""sample_id""))):\n    """"""The output of :class:`TransformerDecoder`.\n\n    Attributes:\n        logits: A float Tensor of shape\n            `[batch_size, max_time, vocab_size]` containing the logits.\n        sample_id: An int Tensor of shape `[batch_size, max_time]`\n            containing the sampled token indexes.\n    """"""\n\n\nclass TransformerDecoder(ModuleBase, TFDecoder):\n    """"""Transformer decoder that applies multi-head self-attention for\n    sequence decoding.\n\n    It is a stack of :class:`~texar.tf.modules.encoders.MultiheadAttentionEncoder`,\n    :class:`~texar.tf.modules.FeedForwardNetwork` and residual connections.\n\n    Args:\n        vocab_size (int, optional): Vocabulary size. Required if\n            :attr:`output_layer` is `None`.\n        output_layer (optional): An output layer that transforms cell output\n            to logits. This can be:\n\n            - A callable layer, e.g., an instance \\\n            of :tf_main:`tf.layers.Layer <layers/Layer>`.\n            - A tensor. A dense layer will be created using the tensor \\\n            as the kernel weights. The bias of the dense layer is determined by\\\n            `hparams.output_layer_bias`. This can be used to tie the output \\\n            layer with the input embedding matrix, as proposed in \\\n            https://arxiv.org/pdf/1608.05859.pdf\n            - `None`. A dense layer will be created based on attr:`vocab_size`\\\n            and `hparams.output_layer_bias`.\n            - If no output layer in the end is needed, set \\\n            `(vocab_size=None, output_layer=tf.identity)`.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 vocab_size=None,\n                 output_layer=None,\n                 hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    layers.get_initializer(self._hparams.initializer))\n\n            # Make the output layer\n            self._output_layer, self._vocab_size = _make_output_layer(\n                output_layer, vocab_size, self._hparams.output_layer_bias,\n                self.variable_scope)\n\n            # Make attention and poswise networks\n            self.multihead_attentions = {\n                \'self_att\': [],\n                \'encdec_att\': []\n            }\n            self.poswise_networks = []\n            for i in range(self._hparams.num_blocks):\n                layer_name = \'layer_{}\'.format(i)\n                with tf.variable_scope(layer_name):\n                    with tf.variable_scope(""self_attention""):\n                        multihead_attention = MultiheadAttentionEncoder(\n                            self._hparams.multihead_attention)\n                        self.multihead_attentions[\'self_att\'].append(\n                            multihead_attention)\n\n                    if self._hparams.dim != \\\n                            multihead_attention.hparams.output_dim:\n                        raise ValueError(\'The output dimenstion of \'\n                                         \'MultiheadEncoder should be equal \'\n                                         \'to the dim of TransformerDecoder\')\n\n                    with tf.variable_scope(\'encdec_attention\'):\n                        multihead_attention = MultiheadAttentionEncoder(\n                            self._hparams.multihead_attention)\n                        self.multihead_attentions[\'encdec_att\'].append(\n                            multihead_attention)\n\n                    if self._hparams.dim != \\\n                            multihead_attention.hparams.output_dim:\n                        raise ValueError(\'The output dimenstion of \'\n                                         \'MultiheadEncoder should be equal \'\n                                         \'to the dim of TransformerDecoder\')\n\n                    pw_net = FeedForwardNetwork(\n                        hparams=self._hparams[\'poswise_feedforward\'])\n                    final_dim = pw_net.hparams.layers[-1][\'kwargs\'][\'units\']\n                    if self._hparams.dim != final_dim:\n                        raise ValueError(\n                            \'The output dimenstion of \'\n                            \'""poswise_feedforward"" should be equal \'\n                            \'to the ""dim"" of TransformerDecoder.\')\n                    self.poswise_networks.append(pw_net)\n\n            # Built in _build()\n            self.context = None\n            self.context_sequence_length = None\n            self.embedding = None\n            self._helper = None\n            self._cache = None\n            self.max_decoding_length = None\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # Same as in TransformerEncoder\n                ""num_blocks"": 6,\n                ""dim"": 512,\n                ""embedding_dropout"": 0.1,\n                ""residual_dropout"": 0.1,\n                ""poswise_feedforward"": default_transformer_poswise_net_hparams,\n                ""multihead_attention"": {\n                    \'name\': \'multihead_attention\',\n                    \'num_units\': 512,\n                    \'output_dim\': 512,\n                    \'num_heads\': 8,\n                    \'dropout_rate\': 0.1,\n                    \'output_dim\': 512,\n                    \'use_bias\': False,\n                },\n                ""initializer"": None,\n                ""name"": ""transformer_decoder""\n                # Additional for TransformerDecoder\n                ""embedding_tie"": True,\n                ""output_layer_bias"": False,\n                ""max_decoding_length"": int(1e10),\n            }\n\n        Here:\n\n        ""num_blocks"": int\n            Number of stacked blocks.\n\n        ""dim"": int\n            Hidden dimension of the encoder.\n\n        ""embedding_dropout"": float\n            Dropout rate of the input word and position embeddings.\n\n        ""residual_dropout"":  float\n            Dropout rate of the residual connections.\n\n        ""poswise_feedforward"": dict\n            Hyperparameters for a feed-forward network used in residual\n            connections.\n            Make sure the dimension of the output tensor is equal to `dim`.\n\n            See :func:`~texar.tf.modules.default_transformer_poswise_net_hparams`\n            for details.\n\n        ""multihead_attention"": dict\n            Hyperparameters for the multihead attention strategy.\n            Make sure the `output_dim` in this module is equal to `dim`.\n\n            See :func:`~texar.tf.modules.MultiheadAttentionEncoder.default_hparams`\n            for details.\n\n        ""initializer"": dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.tf.core.get_initializer` for details.\n\n        ""output_layer_bias"": bool\n            Whether to use bias to the output layer.\n            Used only if :attr:`output_layer` is `None` when constructing\n            the class instance.\n\n        ""max_decoding_length"": int\n            The maximum allowed number of decoding steps.\n            Set to a very large number of avoid the length constraint.\n            Ignored if provided in :meth:`_build` or\n            ""train_greedy"" decoding is used.\n\n            Length penalty coefficient. Refer to\n            https://arxiv.org/abs/1609.08144 for more details.\n\n        ""name"": str\n            Name of the module.\n        """"""\n        return {\n            ""num_blocks"": 6,\n            ""dim"": 512,\n            ""embedding_tie"": True,\n            ""output_layer_bias"": False,\n            ""max_decoding_length"": int(1e10),\n            ""embedding_dropout"": 0.1,\n            ""residual_dropout"": 0.1,\n            ""poswise_feedforward"": default_transformer_poswise_net_hparams(),\n            \'multihead_attention\': {\n                \'name\': \'multihead_attention\',\n                \'num_units\': 512,\n                \'num_heads\': 8,\n                \'dropout_rate\': 0.1,\n                \'output_dim\': 512,\n                \'use_bias\': False,\n            },\n            ""initializer"": None,\n            ""name"": ""transformer_decoder"",\n        }\n\n    def _inputs_to_outputs(self, inputs, cache):\n        """"""The function is called in dynamic decoding.\n\n        `inputs` should be of shape `[batch_size, dim]`.\n\n        Returns outputs (i.e. logits) of shape `[batch_size, vocab_size]`\n        and updated cache.\n        """"""\n        outputs = self._self_attention_stack(\n            tf.expand_dims(inputs, axis=1),\n            memory=cache.get(\'memory\'),\n            cache=cache,\n        )\n        outputs = self._output_layer(outputs)\n        outputs = tf.squeeze(outputs, axis=[1])\n        return outputs, cache\n\n    def _input_ids_to_outputs(self, input_ids, step, cache):\n        """"""The function is called in beam-search decoding.\n\n        `inputs` should be of shape `[batch_size]`.\n\n        Returns outputs (i.e. logits) of shape `[batch_size, vocab_size]`\n        and updated cache.\n        """"""\n        _batch_size = shape_list(input_ids)[0]\n        times = tf.ones([_batch_size], dtype=tf.int32) * step\n        inputs = self.embedding(input_ids, times)\n\n        outputs = self._self_attention_stack(\n            tf.expand_dims(inputs, axis=1),\n            memory=cache.get(\'memory\'),\n            cache=cache,\n        )\n        outputs = self._output_layer(outputs)\n        outputs = tf.squeeze(outputs, axis=[1])\n        return outputs, cache\n\n    def _build(self,  # pylint: disable=arguments-differ, too-many-statements\n               decoding_strategy=\'train_greedy\',\n               inputs=None,\n               memory=None,\n               memory_sequence_length=None,\n               memory_attention_bias=None,\n               beam_width=None,\n               length_penalty=0.,\n               start_tokens=None,\n               end_token=None,\n               context=None,\n               context_sequence_length=None,\n               softmax_temperature=None,\n               max_decoding_length=None,\n               impute_finished=False,\n               embedding=None,\n               helper=None,\n               mode=None):\n        """"""Performs decoding.\n\n        The interface is mostly the same with that of RNN decoders\n        (see :meth:`~texar.tf.modules.RNNDecoderBase._build`). The main\n        difference is that, here, `sequence_length` is not needed, and\n        continuation generation is additionally supported.\n\n        The function provides **3 ways** to specify the decoding method, with\n        varying flexibility:\n\n        1. The :attr:`decoding_strategy` argument.\n\n            - **""train_greedy""**: decoding in teacher-forcing fashion (i.e.,\n              feeding ground truth to decode the next step), and for each step\n              sample is obtained by taking the `argmax` of logits.\n              Argument :attr:`inputs` is required for this strategy.\n            - **""infer_greedy""**: decoding in inference fashion (i.e., feeding\n              `generated` sample to decode the next step), and for each step\n              sample is obtained by taking the `argmax` of logits.\n              Arguments :attr:`(start_tokens, end_token, embedding)` are\n              required for this strategy, and argument\n              :attr:`max_decoding_length` is optional.\n            - **""infer_sample""**: decoding in inference fashion, and for each\n              step sample is obtained by `random sampling` from the logits.\n              Arguments :attr:`(start_tokens, end_token, embedding)` are\n              required for this strategy, and argument\n              :attr:`max_decoding_length` is optional.\n\n          This argument is used only when arguments :attr:`helper` and\n          :attr:`beam_width` are both `None`.\n\n        2. The :attr:`helper` argument: An instance of subclass of\n           :class:`texar.tf.modules.Helper`.\n           This provides a superset of decoding strategies than above.\n           The interface is the same as in RNN decoders.\n           Please refer to :meth:`texar.tf.modules.RNNDecoderBase._build` for\n           detailed usage and examples.\n\n           Note that, here, though using a\n           :class:`~texar.tf.modules.TrainingHelper` corresponds to the\n           ""train_greedy"" strategy above and will get the same output results,\n           the implementation is *slower* than\n           directly setting `decoding_strategy = ""train_greedy""`.\n\n           Argument :attr:`max_decoding_length` is optional.\n\n        3. **Beam search**: set :attr:`beam_width` to use beam search decoding.\n           Arguments :attr:`(start_tokens, end_token)` are required,\n           and argument :attr:`max_decoding_length` is optional.\n\n        Args:\n            memory (optional): The memory to attend, e.g., the output of an RNN\n                encoder. A Tensor of shape `[batch_size, memory_max_time, dim]`.\n            memory_sequence_length (optional): A Tensor of shape `[batch_size]`\n                containing the sequence lengths for the batch entries in\n                memory. Used to create attention bias of\n                :attr:`memory_attention_bias` is not given. Ignored if\n                `memory_attention_bias` is provided.\n            memory_attention_bias (optional): A Tensor of shape\n                `[batch_size, num_heads, memory_max_time, dim]`.\n                An attention bias typically sets the value of a padding\n                position to a large negative value for masking. If not given,\n                :attr:`memory_sequence_length` is used to automatically\n                create an attention bias.\n            inputs (optional): Input tensor for teacher forcing decoding, of\n                shape `[batch_size, target_max_time, emb_dim]` containing the\n                target sequence word embeddings.\n                Used when :attr:`decoding_strategy` is set to ""train_greedy"".\n            decoding_strategy (str): A string specifying the decoding\n                strategy, including ""train_greedy"", ""infer_greedy"",\n                ""infer_sample"".\n                Different arguments are required based on the\n                strategy. See above for details. Ignored if\n                :attr:`beam_width` or :attr:`helper` is set.\n            beam_width (int): Set to use beam search. If given,\n                :attr:`decoding_strategy` is ignored.\n            length_penalty (float): Length penalty coefficient used in beam\n                search decoding. Refer to https://arxiv.org/abs/1609.08144\n                for more details.\n                It Should be larger if longer sentences are wanted.\n            start_tokens (optional): An int Tensor of shape `[batch_size]`,\n                containing the start tokens.\n                Used when :attr:`decoding_strategy` = ""infer_greedy"" or\n                ""infer_sample"", or :attr:`beam_width` is set.\n                Ignored when context is set.\n            end_token (optional): An int 0D Tensor, the token that marks end\n                of decoding.\n                Used when :attr:`decoding_strategy` = ""infer_greedy"" or\n                ""infer_sample"", or :attr:`beam_width` is set.\n            context (optional): An int Tensor of shape `[batch_size, length]`,\n                containing the starting tokens for decoding.\n                If context is set, the start_tokens will be ignored.\n            context_sequence_length (optional): specify the length of context.\n            softmax_temperature (optional): A float 0D Tensor, value to divide\n                the logits by before computing the softmax. Larger values\n                (above 1.0) result in more random samples. Must > 0. If `None`,\n                1.0 is used.\n                Used when `decoding_strategy = ""infer_sample""`.\n            max_decoding_length (optional): An int scalar Tensor indicating\n                the maximum allowed number of decoding steps.\n                If `None` (default), use `""max_decoding_length""` defined in\n                :attr:`hparams`. Ignored in ""train_greedy"" decoding.\n            impute_finished (bool): If `True`, then states for batch\n                entries which are marked as finished get copied through and\n                the corresponding outputs get zeroed out.  This causes some\n                slowdown at each time step, but ensures that the final state\n                and outputs have the correct values and that backprop ignores\n                time steps that were marked as finished. Ignored in\n                ""train_greedy"" decoding.\n            embedding (optional): Embedding used when\n                ""infer_greedy"" or ""infer_sample"" `decoding_strategy`, or\n                beam search, is used. This can be\n                a callable or the `params` argument for\n                :tf_main:`embedding_lookup <nn/embedding_lookup>`.\n                If a callable, it can take a vector tensor of token `ids`,\n                or take two arguments (`ids`, `times`), where `ids`\n                is a vector tensor of token ids, and `times` is a vector tensor\n                of time steps (i.e., position ids). The latter case can be used\n                when attr:`embedding` is a combination of word embedding and\n                position embedding.\n            helper (optional): An instance of\n                :tf_main:`Helper <contrib/seq2seq/Helper>` that defines the\n                decoding strategy. If given, :attr:`decoding_strategy` is\n                ignored.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. Controls dropout mode.\n                If `None` (default), :func:`texar.tf.global_mode`\n                is used.\n\n        Returns:\n\n            - For **""train_greedy""** decoding, returns an instance of\n              :class:`~texar.tf.modules.TransformerDecoderOutput` which contains\n              `sample_id` and `logits`.\n\n            - For **""infer_greedy""** and **""infer_sample""** decoding or\n              decoding with :attr:`helper`, returns\n              a tuple `(outputs, sequence_lengths)`, where `outputs` is an\n              instance of :class:`~texar.tf.modules.TransformerDecoderOutput` as\n              in ""train_greedy"", and `sequence_lengths` is a Tensor of shape\n              `[batch_size]` containing the length of each sample.\n\n            - For **beam search** decoding, returns a `dict` containing keys\n              ""sample_id"" and ""log_prob"".\n\n                - **""sample_id""** is an int Tensor of shape\n                  `[batch_size, max_time, beam_width]` containing generated\n                  token indexes. `sample_id[:,:,0]` is the highest-probable\n                  sample.\n                - **""log_prob""** is a float Tensor of shape\n                  `[batch_size, beam_width]` containing the log probability\n                  of each sequence sample.\n        """"""\n\n        if memory is not None:\n            if memory_attention_bias is None:\n                if memory_sequence_length is None:\n                    raise ValueError(\n                        ""`memory_sequence_length` is required if ""\n                        ""`memory_attention_bias` is not given."")\n\n                enc_padding = 1 - tf.sequence_mask(\n                    memory_sequence_length, shape_list(memory)[1],\n                    dtype=tf.float32)\n                memory_attention_bias = attn.attention_bias_ignore_padding(\n                    enc_padding)\n\n        # record the context, which will be used in step function\n        # for dynamic_decode\n        if context is not None:\n            start_tokens = context[:, 0]\n            self.context = context[:, 1:]\n            self.context_sequence_length = context_sequence_length - 1\n        else:\n            self.context = None\n\n        self.embedding = embedding\n\n        if helper is None and beam_width is None and \\\n                decoding_strategy == \'train_greedy\':  # Teacher-forcing\n\n            decoder_self_attention_bias = (\n                attn.attention_bias_lower_triangle(\n                    shape_list(inputs)[1]))\n\n            decoder_output = self._self_attention_stack(\n                inputs,\n                memory,\n                decoder_self_attention_bias=decoder_self_attention_bias,\n                memory_attention_bias=memory_attention_bias,\n                cache=None,\n                mode=mode)\n            logits = self._output_layer(decoder_output)\n            preds = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n            rets = TransformerDecoderOutput(\n                logits=logits,\n                sample_id=preds\n            )\n\n        else:\n            if max_decoding_length is None:\n                max_decoding_length = self._hparams.max_decoding_length\n            self.max_decoding_length = max_decoding_length\n            if beam_width is None:  # Inference-like decoding\n                # Prepare helper\n                if helper is None:\n                    if decoding_strategy == ""infer_greedy"":\n                        helper = tx_helper.GreedyEmbeddingHelper(\n                            embedding, start_tokens, end_token)\n                    elif decoding_strategy == ""infer_sample"":\n                        helper = tx_helper.SampleEmbeddingHelper(\n                            embedding, start_tokens, end_token,\n                            softmax_temperature)\n                    else:\n                        raise ValueError(\n                            ""Unknown decoding strategy: {}"".format(\n                                decoding_strategy))\n                self._helper = helper\n\n                self._cache = self._init_cache(memory, memory_attention_bias,\n                                               beam_search_decoding=False)\n                if context is not None:\n                    self.context = tf.pad(\n                        self.context,\n                        [[0, 0],\n                         [0, max_decoding_length - shape_list(self.context)[1]]]\n                    )\n\n                outputs, _, sequence_lengths = dynamic_decode(\n                    decoder=self,\n                    impute_finished=impute_finished,\n                    maximum_iterations=max_decoding_length,\n                    output_time_major=False,\n                    scope=self.variable_scope)\n\n                if context is not None:\n                    # Here the length of sample_id will be larger than that\n                    # of logit by 1, because there will be a additional\n                    # start_token in the returned sample_id.\n                    # the start_id should be the first token of the\n                    # given context\n                    outputs = TransformerDecoderOutput(\n                        logits=outputs.logits,\n                        sample_id=tf.concat(\n                            [tf.expand_dims(start_tokens, 1),\n                             outputs.sample_id],\n                            axis=1\n                        )\n                    )\n                    sequence_lengths = sequence_lengths + 1\n                rets = outputs, sequence_lengths\n\n            else:  # Beam-search decoding\n                # Ignore `decoding_strategy`; Assume `helper` is not set\n                if helper is not None:\n                    raise ValueError(""Must not set \'beam_width\' and \'helper\' ""\n                                     ""simultaneously."")\n                _batch_size = shape_list(start_tokens)[0]\n                self._cache = self._init_cache(memory, memory_attention_bias,\n                                               beam_search_decoding=True,\n                                               batch_size=_batch_size)\n\n                # The output format is different when running beam search\n                sample_id, log_prob = self._beam_decode(\n                    start_tokens,\n                    end_token,\n                    beam_width=beam_width,\n                    length_penalty=length_penalty,\n                    decode_length=max_decoding_length,\n                )\n                rets = {\n                    \'sample_id\': sample_id,\n                    \'log_prob\': log_prob\n                }\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return rets\n\n    def _self_attention_stack(self,\n                              inputs,\n                              memory,\n                              decoder_self_attention_bias=None,\n                              memory_attention_bias=None,\n                              cache=None,\n                              mode=None):\n        """"""Stacked multihead attention module.\n        """"""\n\n        def _layer_norm(x, scope):\n            return layers.layer_normalize(x, reuse=tf.AUTO_REUSE, scope=scope)\n\n        inputs = tf.layers.dropout(inputs,\n                                   rate=self._hparams.embedding_dropout,\n                                   training=is_train_mode(mode))\n        if cache is not None:\n            if memory is not None:\n                memory_attention_bias = \\\n                    cache[\'memory_attention_bias\']\n        else:\n            assert decoder_self_attention_bias is not None\n\n        x = inputs\n        for i in range(self._hparams.num_blocks):\n            layer_name = \'layer_{}\'.format(i)\n            layer_cache = cache[layer_name] if cache is not None else None\n            with tf.variable_scope(layer_name) as layer_scope:\n                with tf.variable_scope(""self_attention""):\n                    multihead_attention = \\\n                        self.multihead_attentions[\'self_att\'][i]\n                    selfatt_output = multihead_attention(\n                        queries=_layer_norm(x, layer_scope),\n                        memory=None,\n                        memory_attention_bias=decoder_self_attention_bias,\n                        cache=layer_cache,\n                        mode=mode,\n                    )\n                    x = x + tf.layers.dropout(\n                        selfatt_output,\n                        rate=self._hparams.residual_dropout,\n                        training=is_train_mode(mode),\n                    )\n                if memory is not None:\n                    with tf.variable_scope(\'encdec_attention\') as \\\n                            encdec_attention_scope:\n                        multihead_attention = \\\n                            self.multihead_attentions[\'encdec_att\'][i]\n                        encdec_output = multihead_attention(\n                            queries=_layer_norm(x, encdec_attention_scope),\n                            memory=memory,\n                            memory_attention_bias=memory_attention_bias,\n                            mode=mode,\n                        )\n                        x = x + tf.layers.dropout(\n                            encdec_output,\n                            rate=self._hparams.residual_dropout,\n                            training=is_train_mode(mode))\n                poswise_network = self.poswise_networks[i]\n                with tf.variable_scope(\'past_poswise_ln\') as \\\n                        past_poswise_ln_scope:\n                    sub_output = tf.layers.dropout(\n                        poswise_network(_layer_norm(x, past_poswise_ln_scope)),\n                        rate=self._hparams.residual_dropout,\n                        training=is_train_mode(mode),\n                    )\n                    x = x + sub_output\n\n        return _layer_norm(x, scope=self.variable_scope)\n\n    def _init_cache(self, memory, memory_attention_bias, beam_search_decoding,\n                    batch_size=None):\n        """"""Returns an initialized cache.\n\n        In order to support both inference-like decoding and beam-search\n        decoding, the elements of each layer must be initialized and extended\n        as different structure respectively. Specifically, when inference-like\n        decoding, tf.TensorArray is used, which satisfies the shape consistency\n        check in the while-loop in tf.contrib.seq2seq.dynamic_decode. When\n        beam-search decoding, a tf.Tensor of shape\n        `[batch_size, current_steps, num_units]` is maintained, where\n        `current_steps` is the number of steps currently decoded.\n        """"""\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        def _shape(batch_size, from_shape):\n            if (not isinstance(from_shape, tf.TensorShape) or\n                    from_shape.ndims == 0):\n                return tf.TensorShape(None)\n\n            batch_size = tf.contrib.util.constant_value(\n                tf.convert_to_tensor(\n                    batch_size, name=""batch_size""))\n            return tf.TensorShape([batch_size]).concatenate(from_shape)\n\n        def _create_ta(s, d):\n            return tf.TensorArray(\n                dtype=d,\n                size=0,\n                dynamic_size=True,\n                clear_after_read=False,\n                element_shape=_shape(batch_size, s))\n\n        def _create_empty_tensor(s, d):\n            return tf.zeros(\n                [batch_size, 0] + s.as_list(),\n                dtype=d)\n\n        _create_fn = _create_empty_tensor if beam_search_decoding else \\\n            _create_ta\n\n        s = tf.TensorShape([self._hparams.multihead_attention.num_units])\n\n        if memory is not None:\n            cache = {\n                \'memory\': memory,\n                \'memory_attention_bias\': memory_attention_bias,\n            }\n            for l in range(self._hparams.num_blocks):\n                cache[\'layer_{}\'.format(l)] = {\n                    \'self_keys\': _create_fn(s, tf.float32),\n                    \'self_values\': _create_fn(s, tf.float32),\n                    \'memory_keys\': _create_fn(s, tf.float32),\n                    \'memory_values\': _create_fn(s, tf.float32),\n                }\n        else:\n            cache = {}\n            for l in range(self._hparams.num_blocks):\n                cache[\'layer_{}\'.format(l)] = {\n                    \'self_keys\': _create_fn(s, tf.float32),\n                    \'self_values\': _create_fn(s, tf.float32),\n                }\n\n        return cache\n\n    def _beam_decode(self,\n                     start_tokens,\n                     end_token,\n                     decode_length,\n                     beam_width,\n                     length_penalty):\n        def _symbols_to_logits_fn(ids, step, cache):\n            return self._input_ids_to_outputs(\n                ids[:, -1], step, cache)\n\n        outputs, log_prob = beam_search.beam_search(\n            _symbols_to_logits_fn,\n            start_tokens,\n            beam_width,\n            decode_length,\n            self._vocab_size,\n            length_penalty,\n            eos_id=end_token,\n            states=self._cache)\n\n        # Ignores <BOS>\n        outputs = outputs[:, :, 1:]\n        # shape = [batch_size, seq_length, beam_width]\n        outputs = tf.transpose(outputs, [0, 2, 1])\n        return (outputs, log_prob)\n\n    @property\n    def batch_size(self):\n        return self._helper.batch_size\n\n    @property\n    def output_size(self):\n        """"""Output size of one step.\n        """"""\n        return TransformerDecoderOutput(\n            logits=tf.TensorShape([self._vocab_size]),\n            sample_id=self._helper.sample_ids_shape)\n\n    @property\n    def output_dtype(self):\n        """"""Types of output of one step.\n        """"""\n        return TransformerDecoderOutput(\n            logits=tf.float32,\n            sample_id=self._helper.sample_ids_dtype)\n\n    def initialize(self, name=None):\n        """"""Called before any decoding iterations.\n\n        This methods computes initial input values and initial state\n        (i.e. cache).\n\n        Args:\n            name: Name scope for any created operations.\n\n        Returns:\n            `(finished, initial_inputs, initial_state)`, representing\n            initial values of `finished` flags, inputs and state (i.e. cache).\n        """"""\n        return self._helper.initialize() + (self._cache,)\n\n    def step(self, time, inputs, state, name=None):\n        """"""Called per step of decoding.\n\n        Args:\n            time: Scalar `int32` tensor. Current step number.\n            inputs: Input tensor for this time step.\n            state: State (i.e. cache) from previous time step.\n            name: Name scope for any created operations.\n\n        Returns:\n            `(outputs, next_state, next_inputs, finished)`. `outputs` is an\n            object containing the decoder output, `next_state` is the state\n            (i.e. cache), `next_inputs` is the tensor that should be used\n            as input for the next step, `finished` is a boolean tensor telling\n            whether the sequence is complete, for each sequence in the batch.\n        """"""\n\n        outputs, state = self._inputs_to_outputs(inputs, state)\n        sample_ids = self._helper.sample(\n            time=time, outputs=outputs, state=state)\n        if self.context is not None:\n            _times = tf.ones([self.batch_size], dtype=tf.int32) * time\n            sample_ids = tf.where(\n                self.context_sequence_length > _times,\n                self.context[:, time],\n                sample_ids\n            )\n\n        wrapper_outputs = TransformerDecoderOutput(\n            logits=outputs,\n            sample_id=sample_ids)\n        return (wrapper_outputs, state)\n\n    def next_inputs(self, time, outputs, state):\n        (finished, next_inputs, state) = self._helper.next_inputs(\n            time=time,\n            outputs=outputs.logits,\n            state=state,\n            sample_ids=outputs.sample_id)\n        return (finished, next_inputs, state)\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        return outputs, final_state\n\n    @property\n    def vocab_size(self):\n        """"""The vocab size.\n        """"""\n        return self._vocab_size\n'"
texar/tf/modules/embedders/__init__.py,3,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar library embedders.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.embedders.embedder_base import *\nfrom texar.tf.modules.embedders.embedders import *\nfrom texar.tf.modules.embedders.position_embedders import *\n'"
texar/tf/modules/embedders/embedder_base.py,4,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nThe base embedder class.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.modules.embedders import embedder_utils\nfrom texar.tf.utils.shapes import shape_list\n\n# pylint: disable=invalid-name\n\n__all__ = [\n    ""EmbedderBase""\n]\n\n\nclass EmbedderBase(ModuleBase):\n    r""""""The base embedder class that all embedder classes inherit.\n\n    Args:\n        num_embeds (int, optional): The number of embedding elements, e.g.,\n            the vocabulary size of a word embedder.\n        hparams (dict or HParams, optional): Embedder hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n    """"""\n\n    def __init__(self, num_embeds=None, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        self._num_embeds = num_embeds\n\n    # pylint: disable=attribute-defined-outside-init\n    def _init_parameterized_embedding(self, init_value, num_embeds, hparams):\n        self._embedding = embedder_utils.get_embedding(\n            hparams, init_value, num_embeds, self.variable_scope)\n        if hparams.trainable:\n            self._add_trainable_variable(self._embedding)\n\n        self._num_embeds = shape_list(self._embedding)[0]\n\n        self._dim = shape_list(self._embedding)[1:]\n        self._dim_rank = len(self._dim)\n        if self._dim_rank == 1:\n            self._dim = self._dim[0]\n\n    def _get_dropout_layer(self, hparams, ids_rank=None, dropout_input=None,\n                           dropout_strategy=None):\n        r""""""Creates dropout layer according to dropout strategy.\n        Called in :meth:`_build`.\n        """"""\n        dropout_layer = None\n\n        st = dropout_strategy\n        st = hparams.dropout_strategy if st is None else st\n\n        if hparams.dropout_rate > 0.:\n            if st == \'element\':\n                noise_shape = None\n            elif st == \'item\':\n                assert dropout_input is not None\n                assert ids_rank is not None\n                noise_shape = (shape_list(dropout_input)[:ids_rank]\n                               + [1] * self._dim_rank)\n            elif st == \'item_type\':\n                noise_shape = [None] + [1] * self._dim_rank  # type: ignore\n            else:\n                raise ValueError(\'Unknown dropout strategy: {}\'.format(st))\n\n            dropout_layer = tf.layers.Dropout(\n                rate=hparams.dropout_rate, noise_shape=noise_shape)\n\n        return dropout_layer\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""embedder""\n            }\n        """"""\n        return {\n            ""name"": ""embedder""\n        }\n\n    @property\n    def num_embeds(self):\n        r""""""The number of embedding elements.\n        """"""\n        return self._num_embeds\n'"
texar/tf/modules/embedders/embedder_utils.py,17,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utils of embedder.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.core import layers\n\n__all__ = [\n    ""default_embedding_hparams"",\n    ""get_embedding"",\n    ""soft_embedding_lookup""\n]\n\n\ndef default_embedding_hparams():\n    r""""""Returns a ``dict`` of hyperparameters and default values of a embedder.\n\n     See :meth:`~texar.tf.modules.WordEmbedder.default_hparams` for details.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""embedding"",\n                ""dim"": 100,\n                ""initializer"": None,\n                ""regularizer"": {\n                    ""type"": ""L1L2"",\n                    ""kwargs"": {\n                        ""l1"": 0.,\n                        ""l2"": 0.\n                    }\n                },\n                ""dropout_rate"": 0.,\n                ""dropout_strategy"": \'element\',\n                ""trainable"": True,\n            }\n\n        Here:\n\n        `""name""`: str\n            Name of the embedding variable.\n\n        `""dim""`: int or list\n            Embedding dimension. Can be a list of integers to yield embeddings\n            with dimensionality > 1.\n\n        `""initializer""`: dict or None\n            Hyperparameters of the initializer for the embedding values. An\n            example is as\n\n            .. code-block:: python\n\n                {\n                    ""type"": ""random_uniform_initializer"",\n                    ""kwargs"": {\n                        ""minval"": -0.1,\n                        ""maxval"": 0.1,\n                        ""seed"": None\n                    }\n                }\n\n            which corresponds to :tf_main:`tf.random_uniform_initializer\n            <random_uniform_initializer>`, and includes:\n\n            `""type""`: str or initializer instance\n                Name, full path, or instance of the initializer class; Or name\n                or full path to a function that returns the initializer class.\n                The class or function can be\n\n                - Built-in initializer defined in\n                  :tf_main:`tf.initializers <initializers>`, e.g.,\n                  :tf_main:`random_uniform <random_uniform_initializer>`\n                  (a.k.a :class:`tf.random_uniform_initializer`), or\n                  in :mod:`tf`, e.g., :tf_main:`glorot_uniform_initializer\n                  <glorot_uniform_initializer>`, or in\n                  :tf_main:`tf.keras.initializers <keras/initializers>`.\n                - User-defined initializer in :mod:`texar.tf.custom`.\n                - External initializer. Must provide the full path,\n                  e.g., :attr:`""my_module.MyInitializer""`, or the instance.\n\n            `""kwargs""`: dict\n                A dictionary of arguments for constructor of the\n                initializer class or for the function. An initializer is\n                created by ``initialzier = initializer_class_or_fn(**kwargs)``\n                where :attr:`initializer_class_or_fn` is specified in\n                :attr:`""type""`.\n                Ignored if :attr:`""type""` is an initializer instance.\n\n        `""regularizer""`: dict\n            Hyperparameters of the regularizer for the embedding values. The\n            regularizer must be an instance of\n            the base :tf_main:`Regularizer <keras/regularizers/Regularizer>`\n            class. The hyperparameters include:\n\n            `""type""`: str or regularizer instance\n                Name, full path, or instance of the regularizer class. The\n                class can be\n\n                - Built-in regularizer defined in\n                  :tf_main:`tf.keras.regularizers <keras/regularizers>`, e.g.,\n                  :tf_main:`L1L2 <keras/regularizers/L1L2>`.\n                - User-defined regularizer in :mod:`texar.tf.custom`. The\n                  regularizer class should inherit the base class\n                  :tf_main:`Regularizer <keras/regularizers/Regularizer>`.\n                - External regularizer. Must provide the full path,\n                  e.g., :attr:`""my_module.MyRegularizer""`, or the instance.\n\n            `""kwargs""`: dict\n                A dictionary of arguments for constructor of the\n                regularizer class. A regularizer is created by\n                calling `regularizer_class(**kwargs)` where\n                :attr:`regularizer_class` is specified in :attr:`""type""`.\n                Ignored if :attr:`""type""` is a regularizer instance.\n\n            The default value corresponds to\n            :tf_main:`L1L2 <keras/regularizers/L1L2>` with `(l1=0, l2=0)`,\n            which disables regularization.\n\n        `""dropout_rate""`: float\n            The dropout rate between 0 and 1. E.g., `dropout_rate=0.1` would\n            drop out 10% of the embedding.\n\n        `""dropout_strategy""`: str\n            The dropout strategy. Can be one of the following\n\n            - ``""element""``: The regular strategy that drops individual elements\n              in the embedding vectors.\n            - ``""item""``: Drops individual items (e.g., words) entirely. For\n              example, for the word sequence ""the simpler the better"", the\n              strategy can yield ""_ simpler the better"", where the first ""the""\n              is dropped.\n            - ``""item_type""``: Drops item types (e.g., word types). For example,\n              for the above sequence, the strategy can yield ""_ simpler _\n              better"", where the word type ""the"" is dropped. The dropout will\n              never yield ""_ simpler the better"" as in the ``""item""`` strategy.\n\n        `""trainable""`: bool\n            Whether the embedding is trainable.\n    """"""\n    return {\n        ""name"": ""embedding"",\n        ""dim"": 100,\n        ""initializer"": None,\n        ""regularizer"": layers.default_regularizer_hparams(),\n        ""dropout_rate"": 0.,\n        ""dropout_strategy"": \'element\',\n        ""trainable"": True,\n        ""@no_typecheck"": [""dim""]\n    }\n\n\ndef get_embedding(hparams=None,\n                  init_value=None,\n                  num_embeds=None,\n                  variable_scope=\'Embedding\'):\n    r""""""Creates embedding variable if not exists.\n\n    Args:\n        hparams (dict or HParams, optional): Embedding hyperparameters. Missing\n            hyperparameters are set to default values. See\n            :func:`~texar.tf.modules.default_embedding_hparams`\n            for all hyperparameters and default values.\n\n            If :attr:`init_value` is given, :attr:`hparams[""initializer""]`,\n            and :attr:`hparams[""dim""]` are ignored.\n        init_value (Tensor or numpy array, optional): Initial values of the\n            embedding variable. If not given, embedding is initialized as\n            specified in :attr:`hparams[""initializer""]`.\n        num_embeds (int, optional): The number of embedding items\n            (e.g., vocabulary size). Required if :attr:`init_value` is\n            not provided.\n        variable_scope (str or VariableScope, optional): Variable scope of\n            the embedding variable.\n\n    Returns:\n        Variable or Tensor: A 2D `Variable` or `Tensor` of the same shape with\n        :attr:`init_value` or of the shape ``[num_embeds, hparams[""dim""]]``.\n    """"""\n    with tf.variable_scope(variable_scope):\n        if hparams is None or isinstance(hparams, dict):\n            hparams = HParams(hparams, default_embedding_hparams())\n        regularizer = layers.get_regularizer(hparams[""regularizer""])\n        if init_value is None:\n            initializer = layers.get_initializer(hparams[""initializer""])\n            dim = hparams[""dim""]\n            if not isinstance(hparams[""dim""], (list, tuple)):\n                dim = [dim]\n            embedding = tf.get_variable(name=\'w\',\n                                        shape=[num_embeds] + dim,\n                                        initializer=initializer,\n                                        regularizer=regularizer,\n                                        trainable=hparams[""trainable""])\n        else:\n            init_value = tf.cast(init_value, tf.float32)\n            embedding = tf.get_variable(name=\'w\',\n                                        initializer=init_value,\n                                        regularizer=regularizer,\n                                        trainable=hparams[""trainable""])\n\n        return embedding\n\n\ndef soft_embedding_lookup(embedding, soft_ids):\n    r""""""Transforms soft ids (e.g., probability distribution over ids) into\n    embeddings, by mixing the embedding vectors with the soft weights.\n\n    Args:\n        embedding: A Tensor of shape ``[num_classes] + embedding-dim``\n            containing the embedding vectors. Embedding can have\n            dimensionality > 1, i.e., :attr:`embedding` can be of shape\n            ``[num_classes, emb_dim_1, emb_dim_2, ...]``\n        soft_ids: A Tensor of weights (probabilities) used to mix the\n            embedding vectors.\n\n    Returns:\n        A Tensor of shape ``shape(soft_ids)[:-1] + shape(embedding)[1:]``. For\n        example, if ``shape(soft_ids) = [batch_size, max_time, vocab_size]``\n        and ``shape(embedding) = [vocab_size, emb_dim]``, then the return tensor\n        has shape ``[batch_size, max_time, emb_dim]``.\n\n    Example::\n\n        decoder_outputs, ... = decoder(...)\n        soft_seq_emb = soft_embedding_lookup(\n            embedding, tf.nn.softmax(decoder_outputs.logits))\n    """"""\n    return tf.tensordot(tf.cast(soft_ids, tf.float32), embedding, [-1, 0])\n'"
texar/tf/modules/embedders/embedders.py,12,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious embedders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.embedders.embedder_base import EmbedderBase\nfrom texar.tf.modules.embedders import embedder_utils\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.utils.shapes import get_rank\n\n__all__ = [\n    ""WordEmbedder""\n]\n\n\nclass WordEmbedder(EmbedderBase):\n    r""""""Simple word embedder that maps indexes into embeddings. The indexes\n    can be soft (e.g., distributions over vocabulary).\n\n    Either :attr:`init_value` or :attr:`vocab_size` is required. If both are\n    given, there must be ``init_value.shape[0]==vocab_size``.\n\n    Args:\n        init_value (optional): A Tensor or numpy array that contains the\n            initial value of embeddings. It is typically of shape\n            ``[vocab_size] + embedding-dim``. Embedding can have dimensionality\n            > 1.\n\n            If `None`, embedding is initialized as specified in\n            ``hparams[""initializer""]``. Otherwise, the\n            ``""initializer""`` and ``""dim""`` hyperparameters in\n            :attr:`hparams` are ignored.\n        vocab_size (int, optional): The vocabulary size. Required if\n            :attr:`init_value` is not given.\n        hparams (dict, optional): Embedder hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs of the embedder.\n\n    Example:\n\n        .. code-block:: python\n\n            ids = tf.random_uniform(shape=[32, 10], maxval=10, dtype=tf.int64)\n            soft_ids = tf.random_uniform(shape=[32, 10, 100])\n\n            embedder = WordEmbedder(vocab_size=100, hparams={\'dim\': 256})\n            ids_emb = embedder(ids=ids) # shape: [32, 10, 256]\n            soft_ids_emb = embedder(soft_ids=soft_ids) # shape: [32, 10, 256]\n\n        .. code-block:: python\n\n            # Use with Texar data module\n            hparams={\n                \'dataset\': {\n                    \'embedding_init\': {\'file\': \'word2vec.txt\'}\n                    ...\n                },\n            }\n            data = MonoTextData(data_params)\n            iterator = DataIterator(data)\n            batch = iterator.get_next()\n\n            # Use data vocab size\n            embedder_1 = WordEmbedder(vocab_size=data.vocab.size)\n            emb_1 = embedder_1(batch[\'text_ids\'])\n\n            # Use pre-trained embedding\n            embedder_2 = WordEmbedder(init_value=data.embedding_init_value)\n            emb_2 = embedder_2(batch[\'text_ids\'])\n\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, init_value=None, vocab_size=None, hparams=None):\n        EmbedderBase.__init__(self, hparams=hparams)\n\n        if init_value is None and vocab_size is None:\n            raise ValueError(\n                ""Either `init_value` or `vocab_size` is required."")\n\n        self._init_parameterized_embedding(init_value, vocab_size,\n                                           self._hparams)\n\n        self._vocab_size = vocab_size\n        if vocab_size is None:\n            self._vocab_size = self._num_embeds\n        if self._vocab_size != self._num_embeds:\n            raise ValueError(\n                \'vocab_size must equal to init_value.shape[0].\'\n                \'Got %d and %d\' % (self._vocab_size, self._num_embeds))\n\n        self._built = True\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""dim"": 100,\n                ""dropout_rate"": 0,\n                ""dropout_strategy"": \'element\',\n                ""trainable"": True,\n                ""initializer"": {\n                    ""type"": ""random_uniform_initializer"",\n                    ""kwargs"": {\n                        ""minval"": -0.1,\n                        ""maxval"": 0.1,\n                        ""seed"": None\n                    }\n                },\n                ""regularizer"": {\n                    ""type"": ""L1L2"",\n                    ""kwargs"": {\n                        ""l1"": 0.,\n                        ""l2"": 0.\n                    }\n                },\n                ""name"": ""word_embedder"",\n            }\n\n        Here:\n\n        `""dim""`: int or list\n            Embedding dimension. Can be a list of integers to yield embeddings\n            with dimensionality > 1.\n\n            Ignored if :attr:`init_value` is given to the embedder constructor.\n\n        `""dropout_rate""`: float\n            The dropout rate between 0 and 1. E.g., ``dropout_rate=0.1`` would\n            drop out 10% of the embedding. Set to 0 to disable dropout.\n\n        `""dropout_strategy""`: str\n            The dropout strategy. Can be one of the following\n\n            - ``""element""``: The regular strategy that drops individual\n              elements of embedding vectors.\n            - ``""item""``: Drops individual items (e.g., words) entirely. E.g.,\n              for the word sequence ""the simpler the better"", the strategy can\n              yield ""_ simpler the better"", where the first ""the"" is dropped.\n            - :attr:`""item_type""`: Drops item types (e.g., word types). E.g.,\n              for the above sequence, the strategy can yield\n              ""_ simpler _ better"", where the word type ""the"" is dropped.\n              The dropout will never yield ""_ simpler the better"" as in the\n              ``""item""`` strategy.\n\n        `""trainable""`: bool\n            Whether the embedding is trainable.\n\n        `""initializer""`: dict or None\n            Hyperparameters of the initializer for embedding values. See\n            :func:`~texar.tf.core.get_initializer` for the details. Ignored if\n            :attr:`init_value` is given to the embedder constructor.\n\n        `""regularizer""`: dict\n            Hyperparameters of the regularizer for embedding values. See\n            :func:`~texar.tf.core.get_regularizer` for the details.\n\n        `""name""`: str\n            Name of the embedding variable.\n        """"""\n        hparams = embedder_utils.default_embedding_hparams()\n        hparams[""name""] = ""word_embedder""\n        return hparams\n\n    def _build(self, ids=None, soft_ids=None, mode=None, **kwargs):\n        r""""""Embeds (soft) ids.\n\n        Either :attr:`ids` or :attr:`soft_ids` must be given, and they\n        must not be given at the same time.\n\n        Args:\n            ids (optional): An integer tensor containing the ids to embed.\n            soft_ids (optional): A tensor of weights (probabilities) used to\n                mix the embedding vectors.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout is\n                controlled by :func:`texar.tf.global_mode`.\n            kwargs: Additional keyword arguments for\n                :tf_main:`tf.nn.embedding_lookup <nn/embedding_lookup>` besides\n                :attr:`params` and :attr:`ids`.\n\n        Returns:\n            If :attr:`ids` is given, returns a Tensor of shape\n            ``shape(ids) + embedding-dim``. For example,\n            if ``shape(ids) = [batch_size, max_time]``\n            and ``shape(embedding) = [vocab_size, emb_dim]``, then the return\n            tensor has shape ``[batch_size, max_time, emb_dim]``.\n\n            If :attr:`soft_ids` is given, returns a Tensor of shape\n            ``shape(soft_ids)[:-1] + embdding-dim``. For example,\n            if ``shape(soft_ids) = [batch_size, max_time, vocab_size]``\n            and ``shape(embedding) = [vocab_size, emb_dim]``, then the return\n            tensor has shape ``[batch_size, max_time, emb_dim]``.\n        """"""\n        if ids is not None:\n            if soft_ids is not None:\n                raise ValueError(\n                    \'Must not specify `ids` and `soft_ids` at the same time.\')\n            ids_rank = get_rank(ids)\n        elif soft_ids is not None:\n            ids_rank = get_rank(soft_ids) - 1\n        else:\n            raise ValueError(\'Either `ids` or `soft_ids` must be given.\')\n\n        embedding = self._embedding\n\n        is_training = is_train_mode(mode)\n        if self._hparams.dropout_strategy == \'item_type\':\n            dropout_layer = self._get_dropout_layer(self._hparams)\n            if dropout_layer:\n                embedding = dropout_layer.apply(inputs=embedding,\n                                                training=is_training)\n\n        if ids is not None:\n            outputs = tf.nn.embedding_lookup(embedding, ids, **kwargs)\n        else:\n            outputs = embedder_utils.soft_embedding_lookup(embedding, soft_ids)\n\n        if self._hparams.dropout_strategy != \'item_type\':\n            dropout_layer = self._get_dropout_layer(\n                self._hparams, ids_rank=ids_rank, dropout_input=outputs)\n            if dropout_layer:\n                outputs = dropout_layer.apply(\n                    inputs=outputs, training=is_training)\n\n        return outputs\n\n    @property\n    def embedding(self):\n        r""""""The embedding tensor, of shape ``[vocab_size] + dim``.\n        """"""\n        return self._embedding\n\n    @property\n    def dim(self):\n        r""""""The embedding dimension.\n        """"""\n        return self._dim\n\n    @property\n    def vocab_size(self):\n        r""""""The vocabulary size.\n        """"""\n        return self._vocab_size\n'"
texar/tf/modules/embedders/position_embedders.py,29,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious position embedders.\n""""""\n\nimport math\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.embedders.embedder_base import EmbedderBase\nfrom texar.tf.modules.embedders import embedder_utils\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.utils.shapes import mask_sequences\nfrom texar.tf.utils.shapes import shape_list\n\n# pylint: disable=arguments-differ, invalid-name\n\n__all__ = [\n    ""PositionEmbedder"",\n    ""SinusoidsPositionEmbedder""\n]\n\n\nclass PositionEmbedder(EmbedderBase):\n    r""""""Simple position embedder that maps position indexes into embeddings\n    via lookup.\n\n    Either :attr:`init_value` or :attr:`position_size` is required. If both are\n    given, there must be ``init_value.shape[0]==position_size``.\n\n    Args:\n        init_value (optional): A Tensor or numpy array that contains the\n            initial value of embeddings. It is typically of shape\n            ``[position_size, embedding dim]``\n\n            If `None`, embedding is initialized as specified in\n            ``hparams[""initializer""]``. Otherwise, the\n            ``""initializer""`` and ``""dim""``\n            hyperparameters in :attr:`hparams` are ignored.\n        position_size (int, optional): The number of possible positions, e.g.,\n            the maximum sequence length. Required if :attr:`init_value` is\n            not given.\n        hparams (dict, optional): Embedder hyperparameters. If it is not\n            specified, the default hyperparameter setting is used. See\n            :attr:`default_hparams` for the structure and default values.\n\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, init_value=None, position_size=None, hparams=None):\n        EmbedderBase.__init__(self, hparams=hparams)\n\n        if init_value is None and position_size is None:\n            raise ValueError(\n                ""Either `init_value` or `position_size` is required.""\n            )\n\n        self._init_parameterized_embedding(\n            init_value, position_size, self._hparams\n        )\n\n        self._position_size = position_size\n        if position_size is None:\n            self._position_size = self._num_embeds\n        if self._position_size != self._num_embeds:\n            raise ValueError(\n                ""position_size must equal to init_value.shape[0].""\n                ""Got %d and %d"" % (self._position_size, self._num_embeds)\n            )\n\n        self._built = True\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""dim"": 100,\n                ""initializer"": {\n                    ""type"": ""random_uniform_initializer"",\n                    ""kwargs"": {\n                        ""minval"": -0.1,\n                        ""maxval"": 0.1,\n                        ""seed"": None\n                    }\n                },\n                ""regularizer"": {\n                    ""type"": ""L1L2"",\n                    ""kwargs"": {\n                        ""l1"": 0.,\n                        ""l2"": 0.\n                    }\n                },\n                ""dropout_rate"": 0,\n                ""trainable"": True,\n                ""name"": ""position_embedder""\n            }\n\n        The hyperparameters have the same meaning as those in\n        :meth:`texar.tf.modules.WordEmbedder.default_hparams`.\n        """"""\n        hparams = embedder_utils.default_embedding_hparams()\n        hparams[""name""] = ""position_embedder""\n        return hparams\n\n    def _build(self, positions=None, sequence_length=None, mode=None, **kwargs):\n        r""""""Embeds the positions.\n\n        Either :attr:`positions` or :attr:`sequence_length` is required:\n\n            - If both are given, :attr:`sequence_length` is used to mask out\n              embeddings of those time steps beyond the respective sequence\n              lengths.\n            - If only :attr:`sequence_length` is given, then positions\n              from 0 to ``sequence_length-1`` are embedded.\n\n        Args:\n            positions (optional): An integer tensor containing the position\n                ids to embed.\n            sequence_length (optional): An integer tensor of shape\n                ``[batch_size]``. Time steps beyond the respective sequence\n                lengths will have zero-valued embeddings.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout will be\n                controlled by :func:`texar.tf.global_mode`.\n            kwargs: Additional keyword arguments for\n                :tf_main:`tf.nn.embedding_lookup <nn/embedding_lookup>` besides\n                :attr:`params` and :attr:`ids`.\n\n        Returns:\n            A `Tensor` of shape `shape(inputs) + embedding dimension`.\n        """"""\n        # Gets embedder inputs\n        # pylint:disable=too-many-locals\n        inputs = positions\n        if positions is None:\n            if sequence_length is None:\n                raise ValueError(\n                    ""Either `positions` or `sequence_length` is required.""\n                )\n            max_length = tf.reduce_max(sequence_length)\n            single_inputs = tf.range(start=0, limit=max_length, dtype=tf.int32)\n            # Expands `single_inputs` to have shape [batch_size, max_length]\n            expander = tf.expand_dims(tf.ones_like(sequence_length), -1)\n            inputs = expander * tf.expand_dims(single_inputs, 0)\n        ids_rank = len(inputs.shape.dims)\n\n        embedding = self._embedding\n\n        is_training = is_train_mode(mode)\n\n        # Gets dropout strategy\n        st = self._hparams.dropout_strategy\n        if positions is None and st == ""item"":\n            # If `inputs` is based on `sequence_length`, then dropout\n            # strategies \'item\' and \'item_type\' have the same effect, we\n            # use \'item_type\' to avoid unknown noise_shape in the \'item\'\n            # strategy\n            st = ""item_type""\n\n        # Dropouts as \'item_type\' before embedding\n        if st == ""item_type"":\n            dropout_layer = self._get_dropout_layer(\n                self._hparams, dropout_strategy=st\n            )\n            if dropout_layer:\n                embedding = dropout_layer.apply(\n                    inputs=embedding, training=is_training\n                )\n\n        # Embeds\n        outputs = tf.nn.embedding_lookup(embedding, inputs, **kwargs)\n\n        # Dropouts as \'item\' or \'elements\' after embedding\n        if st != ""item_type"":\n            dropout_layer = self._get_dropout_layer(\n                self._hparams,\n                ids_rank=ids_rank,\n                dropout_input=outputs,\n                dropout_strategy=st,\n            )\n            if dropout_layer:\n                outputs = dropout_layer.apply(\n                    inputs=outputs, training=is_training\n                )\n\n        # Optionally masks\n        if sequence_length is not None:\n            outputs = mask_sequences(\n                outputs,\n                sequence_length,\n                tensor_rank=len(inputs.shape.dims) + self._dim_rank,\n            )\n\n        return outputs\n\n    @property\n    def embedding(self):\n        r""""""The embedding tensor.\n        """"""\n        return self._embedding\n\n    @property\n    def dim(self):\n        r""""""The embedding dimension.\n        """"""\n        return self._dim\n\n    @property\n    def position_size(self):\n        r""""""The position size, i.e., maximum number of positions.\n        """"""\n        return self._position_size\n\n\nclass SinusoidsPositionEmbedder(EmbedderBase):\n    r""""""Sinusoid position embedder that maps position indexes into embeddings\n    via sinusoid calculation. This module does not have trainable parameters.\n    Used in, e.g., Transformer models\n    `(Vaswani et al.) ""Attention Is All You Need""`.\n\n    Each channel of the input Tensor is incremented by a sinusoid of a\n    different frequency and phase.\n    This allows attention to learn to use absolute and relative positions.\n\n    Timing signals should be added to some precursors of both the query\n    and the memory inputs to attention.\n    The use of relative position is possible because `sin(x+y)` and\n    `cos(x+y)` can be expressed in terms of `y`, `sin(x)`, and `cos(x)`.\n    In particular, we use a geometric sequence of timescales starting with\n    min_timescale and ending with max_timescale.  The number of different\n    timescales is equal to ``dim / 2``. For each timescale, we\n    generate the two sinusoidal signals `sin(timestep/timescale)` and\n    `cos(timestep/timescale)`.  All of these sinusoids are concatenated in\n    the dim dimension.\n\n    Args:\n        position_size (int): The number of possible positions, e.g., the maximum\n            sequence length. Set ``position_size=None`` and\n            ``hparams[\'cache_embeddings\']=False`` to enable infinite large or\n            negative position indexes.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, position_size, hparams=None):\n        EmbedderBase.__init__(self, hparams=hparams)\n\n        self._num_embeds = position_size\n        self._dim = self._hparams.dim\n        self._cache_embeddings = self._hparams.cache_embeddings\n\n        num_timescales = self._dim // 2\n        min_timescale = self._hparams.min_timescale\n        max_timescale = self._hparams.max_timescale\n\n        log_timescale_increment = math.log(\n            float(max_timescale) / float(min_timescale)\n        ) / (tf.cast(num_timescales, tf.float32) - 1)\n        num_range = tf.range(num_timescales, dtype=tf.float32)\n        inv_timescales = min_timescale * tf.exp(\n            num_range * -log_timescale_increment\n        )\n        self.inv_timescales = inv_timescales\n\n        if self._cache_embeddings:\n            if position_size is None:\n                raise ValueError(\n                    ""\'position_size\' must not be None when ""\n                    ""\'cache_embeddings\' is set to True"")\n            positions = tf.range(position_size, dtype=tf.float32)\n            signal = self._compute_embeddings(positions)\n            self.signal = signal\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values\n        We use a geometric sequence of timescales starting with\n        min_timescale and ending with max_timescale. The number of different\n        timescales is equal to ``dim/2``.\n\n        .. code-block:: python\n\n            {\n                \'min_timescale\': 1.0,\n                \'max_timescale\': 10000.0,\n                \'dim\': 512,\n                \'cache_embeddings\': True,\n                \'name\':\'sinusoid_posisiton_embedder\',\n            }\n\n        Here:\n\n        `""cache_embeddings""`: bool\n            If `True`, precompute embeddings for positions in range\n            `[0, position_size - 1]`. This leads to faster lookup but\n            requires lookup indices to be within this range.\n\n            If `False`, embeddings are computed on-the-fly during lookup.\n            Set to `False` if your application needs to handle sequences\n            of arbitrary length, or requires embeddings at negative\n            positions.\n        """"""\n        hparams = {\n            ""min_timescale"": 1.0,\n            ""max_timescale"": 1.0e4,\n            ""dim"": 512,\n            ""cache_embeddings"": True,\n            ""name"": ""sinusoid_posisiton_embedder"",\n        }\n        return hparams\n\n    def _compute_embeddings(self, positions):\n        inv_timescales = self.inv_timescales\n        scaled_time = tf.reshape(tf.cast(positions, inv_timescales.dtype),\n                                 (-1, 1)) * tf.expand_dims(inv_timescales, 0)\n        signal = tf.concat(\n            [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1\n        )\n        signal = tf.pad(signal, [[0, 0], [0, tf.mod(self._dim, 2)]])\n        signal = tf.reshape(signal, shape_list(positions) + [self._dim])\n        return signal\n\n    def _build(self, positions=None, sequence_length=None):\n        r""""""Embeds.\n        Either :attr:`positions` or :attr:`sequence_length` is required:\n\n        - If both are given, :attr:`sequence_length` is used to mask out\n          embeddings of those time steps beyond the respective sequence \\\n          lengths.\n        - If only :attr:`sequence_length` is given, then positions\n          from `0` to `sequence_length-1` are embedded.\n\n        Args:\n            positions (optional): An integer tensor containing the position\n                ids to embed.\n            sequence_length (optional): An integer tensor of shape\n                ``[batch_size]``. Time steps beyond\n                the respective sequence lengths will have zero-valued\n                embeddings.\n\n        Returns:\n            A Tensor of shape ``[batch_size, max_time, dim]``.\n        """"""\n        if positions is None:\n            if sequence_length is None:\n                raise ValueError(\n                    ""Either `positions` or `sequence_length` is required.""\n                )\n            max_length = tf.reduce_max(sequence_length)\n            single_inputs = tf.range(start=0, limit=max_length, dtype=tf.int32)\n            # Expands `single_inputs` to have shape [batch_size, max_length]\n            expander = tf.expand_dims(tf.ones_like(sequence_length), -1)\n            inputs = expander * tf.expand_dims(single_inputs, 0)\n        else:\n            inputs = positions\n\n        if self._cache_embeddings:\n            outputs = tf.nn.embedding_lookup(self.signal, inputs)\n        else:\n            outputs = self._compute_embeddings(inputs)\n\n        return outputs\n'"
texar/tf/modules/encoders/__init__.py,9,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library encoders.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.encoders.encoder_base import *\nfrom texar.tf.modules.encoders.bert_encoder import *\nfrom texar.tf.modules.encoders.gpt2_encoder import *\nfrom texar.tf.modules.encoders.conv_encoders import *\nfrom texar.tf.modules.encoders.hierarchical_encoders import *\nfrom texar.tf.modules.encoders.multihead_attention import *\nfrom texar.tf.modules.encoders.rnn_encoders import *\nfrom texar.tf.modules.encoders.transformer_encoders import *\nfrom texar.tf.modules.encoders.xlnet_encoder import *\n'"
texar/tf/modules/encoders/bert_encoder.py,27,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBERT encoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.core.layers import get_initializer, get_layer\nfrom texar.tf.modules.encoders.transformer_encoders import TransformerEncoder\nfrom texar.tf.modules.embedders.embedders import WordEmbedder\nfrom texar.tf.modules.embedders.position_embedders import PositionEmbedder\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.modules.pretrained.bert import PretrainedBERTMixin\n\n__all__ = [\n    ""BERTEncoder"",\n]\n\n\nclass BERTEncoder(EncoderBase, PretrainedBERTMixin):\n    r""""""Raw BERT Transformer for encoding sequences. Please see\n    :class:`~texar.tf.modules.PretrainedBERTMixin` for a brief description\n    of BERT.\n\n    This module basically stacks\n    :class:`~texar.tf.modules.WordEmbedder`,\n    :class:`~texar.tf.modules.PositionEmbedder`,\n    :class:`~texar.tf.modules.TransformerEncoder` and a dense pooler.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``bert-base-uncased``). Please refer to\n            :class:`~texar.tf.modules.PretrainedBERTMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n        super(BERTEncoder, self).__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        with tf.variable_scope(self.variable_scope):\n\n            # Word embedding\n            self.word_embedder = WordEmbedder(\n                vocab_size=self._hparams.vocab_size,\n                hparams=self._hparams.embed)\n\n            # Segment embedding for each type of tokens\n            self.segment_embedder = WordEmbedder(\n                vocab_size=self._hparams.type_vocab_size,\n                hparams=self._hparams.segment_embed)\n\n            # Position embedding\n            self.position_embedder = PositionEmbedder(\n                position_size=self._hparams.position_size,\n                hparams=self._hparams.position_embed)\n\n            # The BERT encoder (a TransformerEncoder)\n            self.encoder = TransformerEncoder(hparams=self._hparams.encoder)\n\n            with tf.variable_scope(""pooler""):\n                kwargs_i = {""units"": self._hparams.hidden_size,\n                            ""activation"": tf.tanh}\n                layer_hparams = {""type"": ""Dense"", ""kwargs"": kwargs_i}\n                self.pooler = get_layer(hparams=layer_hparams)\n\n    def reset_parameters(self):\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    get_initializer(self._hparams.initializer))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""bert-base-uncased"",\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""vocab_size"": 30522,\n                ""segment_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""token_type_embeddings""\n                },\n                ""type_vocab_size"": 2,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n                ""position_size"": 512,\n\n                ""encoder"": {\n                    ""dim"": 768,\n                    ""embedding_dropout"": 0.1,\n                    ""multihead_attention"": {\n                        ""dropout_rate"": 0.1,\n                        ""name"": ""self"",\n                        ""num_heads"": 12,\n                        ""num_units"": 768,\n                        ""output_dim"": 768,\n                        ""use_bias"": True\n                    },\n                    ""name"": ""encoder"",\n                    ""num_blocks"": 12,\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {   ""kwargs"": {\n                                    ""activation"": ""gelu"",\n                                    ""name"": ""intermediate"",\n                                    ""units"": 3072,\n                                    ""use_bias"": True\n                                },\n                                ""type"": ""Dense""\n                            },\n                            {   ""kwargs"": {""activation"": None,\n                                ""name"": ""output"",\n                                ""units"": 768,\n                                ""use_bias"": True\n                                },\n                                ""type"": ""Dense""\n                            }\n                        ]\n                    },\n                    ""residual_dropout"": 0.1,\n                    ""use_bert_config"": True\n                },\n                ""hidden_size"": 768,\n                ""initializer"": None,\n                ""name"": ""bert_encoder""\n            }\n\n        Here:\n\n        The default parameters are values for uncased BERT-Base model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained BERT model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in BERT model.\n\n        `""segment_embed""`: dict\n            Hyperparameters for segment embedding layer.\n\n        `""type_vocab_size""`: int\n            The vocabulary size of the `segment_ids` passed into `BertModel`.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""position_size""`: int\n            The maximum sequence length that this model might ever be used with.\n\n        `""encoder""`: dict\n            Hyperparameters for the TransformerEncoder.\n            See :func:`~texar.tf.modules.TransformerEncoder.default_harams`\n            for details.\n\n        `""hidden_size""`: int\n            Size of the pooler dense layer.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.tf.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n\n        return {\n            \'pretrained_model_name\': \'bert-base-uncased\',\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'vocab_size\': 30522,\n            \'segment_embed\': {\n                \'dim\': 768,\n                \'name\': \'token_type_embeddings\'\n            },\n            \'type_vocab_size\': 2,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n            \'position_size\': 512,\n\n            \'encoder\': {\n                \'dim\': 768,\n                \'embedding_dropout\': 0.1,\n                \'multihead_attention\': {\n                    \'dropout_rate\': 0.1,\n                    \'name\': \'self\',\n                    \'num_heads\': 12,\n                    \'num_units\': 768,\n                    \'output_dim\': 768,\n                    \'use_bias\': True\n                },\n                \'name\': \'encoder\',\n                \'num_blocks\': 12,\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'kwargs\': {\n                                \'activation\': \'gelu\',\n                                \'name\': \'intermediate\',\n                                \'units\': 3072,\n                                \'use_bias\': True\n                            },\n                            \'type\': \'Dense\'\n                        },\n                        {\n                            \'kwargs\': {\n                                \'activation\': None,\n                                \'name\': \'output\',\n                                \'units\': 768,\n                                \'use_bias\': True\n                            },\n                            \'type\': \'Dense\'\n                        }\n                    ]\n                },\n                \'residual_dropout\': 0.1,\n                \'use_bert_config\': True\n            },\n            \'hidden_size\': 768,\n            \'initializer\': None,\n            \'name\': \'bert_encoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               segment_ids=None,\n               mode=None,\n               **kwargs):\n        """"""Encodes the inputs.\n\n        Args:\n            inputs: A 2D Tensor of shape `[batch_size, max_time]`,\n                containing the token ids of tokens in the input sequences.\n            segment_ids (optional): A 2D Tensor of shape\n                `[batch_size, max_time]`, containing the segment ids\n                of tokens in input sequences. If `None` (default), a\n                tensor with all elements set to zero is used.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n            **kwargs: Keyword arguments.\n\n        Returns:\n            A pair :attr:`(outputs, pooled_output)`\n\n                - :attr:`outputs`:  A Tensor of shape \\\n                `[batch_size, max_time, dim]` containing the \\\n                 encoded vectors.\n\n                - :attr:`pooled_output`: A Tensor of size \\\n                `[batch_size, hidden_size]` which is the output of a \\\n                pooler berts on top of the hidden state associated \\\n                to the first character of the input (`CLS`), see BERT\'s \\\n                paper.\n        """"""\n\n        if segment_ids is None:\n            segment_ids = tf.zeros_like(inputs)\n\n        word_embeds = self.word_embedder(inputs)\n\n        segment_embeds = self.segment_embedder(segment_ids)\n\n        batch_size = tf.shape(inputs)[0]\n        pos_length = tf.ones([batch_size], tf.int32) * tf.shape(inputs)[1]\n        pos_embeds = self.position_embedder(sequence_length=pos_length)\n\n        input_embeds = word_embeds + segment_embeds + pos_embeds\n\n        if sequence_length is None:\n            sequence_length = tf.ones([batch_size], tf.int32) \\\n                              * tf.shape(inputs)[1]\n\n        output = self.encoder(input_embeds, sequence_length, mode)\n\n        with tf.variable_scope(""pooler""):\n            # taking the hidden state corresponding to the first token.\n            first_token_tensor = tf.squeeze(output[:, 0:1, :], axis=1)\n            pooled_output = self.pooler(first_token_tensor)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n            self.init_pretrained_weights(self.variable_scope.name)\n\n        return output, pooled_output\n'"
texar/tf/modules/encoders/conv_encoders.py,7,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious convolutional network encoders.\n""""""\n\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.modules.networks.conv_networks import Conv1DNetwork\n\n__all__ = [\n    ""Conv1DEncoder""\n]\n\n\nclass Conv1DEncoder(Conv1DNetwork, EncoderBase):\n    """"""Simple Conv-1D encoder which consists of a sequence of conv layers\n    followed with a sequence of dense layers.\n\n    Wraps :class:`~texar.tf.modules.Conv1DNetwork` to be a subclass of\n    :class:`~texar.tf.modules.EncoderBase`. Has exact the same functionality\n    with :class:`~texar.tf.modules.Conv1DNetwork`.\n    """"""\n\n    def __init__(self, hparams=None):  # pylint: disable=super-init-not-called\n        Conv1DNetwork.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        The same as :meth:`~texar.tf.modules.Conv1DNetwork.default_hparams`\n        of :class:`~texar.tf.modules.Conv1DNetwork`, except that the default name\n        is \'conv_encoder\'.\n        """"""\n        hparams = Conv1DNetwork.default_hparams()\n        hparams[\'name\'] = \'conv_encoder\'\n        return hparams\n'"
texar/tf/modules/encoders/encoder_base.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for encoders.\n""""""\n\nfrom texar.tf.module_base import ModuleBase\n\n__all__ = [\n    ""EncoderBase""\n]\n\n\nclass EncoderBase(ModuleBase):\n    """"""Base class inherited by all encoder classes.\n    """"""\n\n    def __init__(self, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""encoder""\n        }\n\n    def _build(self, inputs, *args, **kwargs):\n        """"""Encodes the inputs.\n\n        Args:\n          inputs: Inputs to the encoder.\n          *args: Other arguments.\n          **kwargs: Keyword arguments.\n\n        Returns:\n          Encoding results.\n        """"""\n        raise NotImplementedError\n'"
texar/tf/modules/encoders/gpt2_encoder.py,20,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nGPT2 encoders.\n""""""\n\nfrom typing import Optional\n\nimport tensorflow as tf\n\nfrom texar.tf.core.layers import get_initializer\nfrom texar.tf.modules.embedders.embedders import WordEmbedder\nfrom texar.tf.modules.embedders.position_embedders import PositionEmbedder\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.modules.encoders.transformer_encoders import TransformerEncoder\nfrom texar.tf.modules.pretrained.gpt2 import PretrainedGPT2Mixin\n\n__all__ = [\n    ""GPT2Encoder"",\n]\n\n\nclass GPT2Encoder(EncoderBase, PretrainedGPT2Mixin):\n    r""""""Raw GPT2 Transformer for encoding sequences. Please see\n    :class:`~texar.tf.modules.PretrainedGPT2Mixin` for a brief description\n    of GPT2.\n\n    This module basically stacks\n    :class:`~texar.tf.modules.WordEmbedder`,\n    :class:`~texar.tf.modules.PositionEmbedder`,\n    :class:`~texar.tf.modules.TransformerEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``gpt2-small``). Please refer to\n            :class:`~texar.tf.modules.PretrainedGPT2Mixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name: Optional[str] = None,\n                 cache_dir: Optional[str] = None,\n                 hparams=None):\n        super(GPT2Encoder, self).__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        with tf.variable_scope(self.variable_scope):\n\n            # Word embedding\n            self.word_embedder = WordEmbedder(\n                vocab_size=self._hparams.vocab_size,\n                hparams=self._hparams.embed)\n\n            # Position embedding\n            self.position_embedder = PositionEmbedder(\n                position_size=self._hparams.position_size,\n                hparams=self._hparams.position_embed)\n\n            # The GPT2 encoder (a TransformerEncoder)\n            self.encoder = TransformerEncoder(hparams=self._hparams.encoder)\n\n    def reset_parameters(self):\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    get_initializer(self._hparams.initializer))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by the\n          configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": ""gpt2-small"",\n                ""vocab_size"": 50257,\n                ""context_size"": 1024,\n                ""embedding_size"": 768,\n                ""embed"": {\n                    ""dim"": 768,\n                    ""name"": ""word_embeddings""\n                },\n                ""position_size"": 1024,\n                ""position_embed"": {\n                    ""dim"": 768,\n                    ""name"": ""position_embeddings""\n                },\n\n                ""encoder"": {\n                    ""dim"": 768,\n                    ""num_blocks"": 12,\n                    ""use_gpt_config"": True,\n                    ""embedding_dropout"": 0,\n                    ""residual_dropout"": 0,\n                    ""multihead_attention"": {\n                        ""use_bias"": True,\n                        ""num_units"": 768,\n                        ""num_heads"": 12,\n                        ""output_dim"": 768\n                    },\n                    ""initializer"": {\n                        ""type"": ""variance_scaling_initializer"",\n                        ""kwargs"": {\n                            ""factor"": 1.0,\n                            ""mode"": ""FAN_AVG"",\n                            ""uniform"": True\n                        }\n                    },\n                    ""poswise_feedforward"": {\n                        ""layers"": [\n                            {\n                                ""type"": ""Dense"",\n                                ""kwargs"": {\n                                    ""activation"": ""gelu"",\n                                    ""name"": ""intermediate"",\n                                    ""units"": 3072,\n                                    ""use_bias"": True\n                                }\n                            },\n                            {\n                                ""type"": ""Dense"",\n                                ""kwargs"": {\n                                    ""activation"": None,\n                                    ""name"": ""output"",\n                                    ""units"": 3072,\n                                    ""use_bias"": True\n                                }\n                            }\n                        ],\n                        ""name"": ""ffn""\n                    }\n                },\n                ""initializer"": None,\n                ""name"": ""gpt2_encoder"",\n            }\n\n        Here:\n\n        The default parameters are values for 124M GPT2 model.\n\n        `""pretrained_model_name""`: str or None\n            The name of the pre-trained GPT2 model. If None, the model\n            will be randomly initialized.\n\n        `""embed""`: dict\n            Hyperparameters for word embedding layer.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in `GPT2Model`.\n\n        `""position_embed""`: dict\n            Hyperparameters for position embedding layer.\n\n        `""position_size""`:  int\n            The maximum sequence length that this model might ever be used with.\n\n        `""decoder""`: dict\n            Hyperparameters for the TransformerDecoder.\n            See :func:`~texar.torch.modules.TransformerDecoder.default_hparams`\n            for details.\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.torch.core.get_initializer` for details.\n\n        `""name""`: str\n            Name of the module.\n        """"""\n        return {\n            \'encoder\': {\n                \'name\': \'encoder\',\n                \'dim\': 768,\n                \'num_blocks\': 12,\n                \'use_bert_config\': False,\n                \'embedding_dropout\': 0,\n                \'residual_dropout\': 0,\n                \'multihead_attention\': {\n                    \'name\': \'self\',\n                    \'use_bias\': True,\n                    \'num_units\': 768,\n                    \'num_heads\': 12,\n                    \'output_dim\': 768\n                },\n                \'initializer\': {\n                    \'type\': \'variance_scaling_initializer\',\n                    \'kwargs\': {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                    }\n                },\n                \'poswise_feedforward\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\n                                \'activation\': \'gelu\',\n                                \'name\': \'intermediate\',\n                                \'units\': 3072,\n                                \'use_bias\': True\n                            }\n                        },\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\n                                \'activation\': None,\n                                \'name\': \'output\',\n                                \'units\': 768,\n                                \'use_bias\': True\n                            }\n                        }\n                    ],\n                    \'name\': \'ffn\',\n                },\n            },\n\n            \'pretrained_model_name\': \'gpt2-small\',\n            \'vocab_size\': 50257,\n            \'context_size\': 1024,\n            \'embedding_size\': 768,\n            \'embed\': {\n                \'dim\': 768,\n                \'name\': \'word_embeddings\'\n            },\n            \'position_size\': 1024,\n            \'position_embed\': {\n                \'dim\': 768,\n                \'name\': \'position_embeddings\'\n            },\n            \'initializer\': None,\n            \'name\': \'gpt2_encoder\',\n            \'@no_typecheck\': [\'pretrained_model_name\'],\n        }\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               mode=None,\n               **kwargs):\n        r""""""Encodes the inputs.\n\n        Args:\n            inputs: A 2D Tensor of shape `[batch_size, max_time]`,\n                containing the token ids of tokens in the input sequences.\n            sequence_length (optional): A 1D Tensor of shape `[batch_size]`.\n                Input tokens beyond respective sequence lengths are masked\n                out automatically.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n            **kwargs: Keyword arguments.\n\n        Returns:\n            outputs:  A Tensor of shape\n            `[batch_size, max_time, dim]` containing the encoded vectors.\n        """"""\n        word_embeds = self.word_embedder(inputs)\n\n        batch_size = tf.shape(inputs)[0]\n        pos_length = tf.ones([batch_size], tf.int32) * tf.shape(inputs)[1]\n        pos_embeds = self.position_embedder(sequence_length=pos_length)\n\n        inputs_embeds = word_embeds + pos_embeds\n\n        if sequence_length is None:\n            sequence_length = tf.ones([batch_size], tf.int32) \\\n                              * tf.shape(inputs)[1]\n\n        output = self.encoder(inputs_embeds, sequence_length, mode)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n            self.init_pretrained_weights(self.variable_scope.name,\n                                         load_output_layer=False)\n        return output\n'"
texar/tf/modules/encoders/hierarchical_encoders.py,23,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious encoders that encode data with hierarchical structure.\n""""""\n\nimport collections\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMStateTuple\nfrom tensorflow.python.util import nest    # pylint: disable=E0611\n\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.utils import utils\n\n# pylint: disable=invalid-name, too-many-arguments, too-many-locals\n\n__all__ = [\n    ""HierarchicalRNNEncoder""\n]\n\n\nclass HierarchicalRNNEncoder(EncoderBase):\n    """"""A hierarchical encoder that stacks basic RNN encoders into two layers.\n    Can be used to encode long, structured sequences, e.g. paragraphs, dialog\n    history, etc.\n\n    Args:\n        encoder_major (optional): An instance of subclass of\n            :class:`~texar.tf.modules.RNNEncoderBase`\n            The high-level encoder taking final\n            states from low-level encoder as its\n            inputs. If not specified, an encoder\n            is created as specified in\n            :attr:`hparams[""encoder_major""]`.\n        encoder_minor (optional): An instance of subclass of\n            :class:`~texar.tf.modules.RNNEncoderBase`\n            The low-level encoder. If not\n            specified, an encoder is created as specified\n            in :attr:`hparams[""encoder_minor""]`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs of the encoder.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, encoder_major=None, encoder_minor=None,\n                 hparams=None):\n        EncoderBase.__init__(self, hparams)\n\n        encoder_major_hparams = utils.get_instance_kwargs(\n            None, self._hparams.encoder_major_hparams)\n        encoder_minor_hparams = utils.get_instance_kwargs(\n            None, self._hparams.encoder_minor_hparams)\n\n        if encoder_major is not None:\n            self._encoder_major = encoder_major\n        else:\n            with tf.variable_scope(self.variable_scope.name):\n                with tf.variable_scope(\'encoder_major\'):\n                    self._encoder_major = utils.check_or_get_instance(\n                        self._hparams.encoder_major_type,\n                        encoder_major_hparams,\n                        [\'texar.tf.modules.encoders\', \'texar.tf.custom\'])\n\n        if encoder_minor is not None:\n            self._encoder_minor = encoder_minor\n        elif self._hparams.config_share:\n            with tf.variable_scope(self.variable_scope.name):\n                with tf.variable_scope(\'encoder_minor\'):\n                    self._encoder_minor = utils.check_or_get_instance(\n                        self._hparams.encoder_major_type,\n                        encoder_major_hparams,\n                        [\'texar.tf.modules.encoders\', \'texar.tf.custom\'])\n        else:\n            with tf.variable_scope(self.variable_scope.name):\n                with tf.variable_scope(\'encoder_minor\'):\n                    self._encoder_minor = utils.check_or_get_instance(\n                        self._hparams.encoder_minor_type,\n                        encoder_minor_hparams,\n                        [\'texar.tf.modules.encoders\', \'texar.tf.custom\'])\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                ""encoder_major_type"": ""UnidirectionalRNNEncoder"",\n                ""encoder_major_hparams"": {},\n                ""encoder_minor_type"": ""UnidirectionalRNNEncoder"",\n                ""encoder_minor_hparams"": {},\n                ""config_share"": False,\n                ""name"": ""hierarchical_encoder_wrapper""\n            }\n\n        Here:\n\n        ""encoder_major_type"": str or class or instance\n            The high-level encoder. Can be a RNN encoder class, its name or\n            module path, or a class instance.\n            Ignored if `encoder_major` is given to the encoder constructor.\n\n        ""encoder_major_hparams"": dict\n            The hyperparameters for the high-level encoder. The high-level\n            encoder is created with\n            :python:`encoder_class(hparams=encoder_major_hparams)`.\n            Ignored if `encoder_major` is given to the encoder constructor,\n            or if ""encoder_major_type"" is an encoder instance.\n\n        ""encoder_minor_type"": str or class or instance\n            The low-level encoder. Can be a RNN encoder class, its name or\n            module path, or a class instance.\n            Ignored if `encoder_minor` is given to the encoder constructor,\n            or if ""config_share"" is True.\n\n        ""encoder_minor_hparams"": dict\n            The hyperparameters for the low-level encoder. The high-level\n            encoder is created with\n            :python:`encoder_class(hparams=encoder_minor_hparams)`.\n            Ignored if `encoder_minor` is given to the encoder constructor,\n            or if ""config_share"" is True,\n            or if ""encoder_minor_type"" is an encoder instance.\n\n        ""config_share"":\n            Whether to use encoder_major\'s hyperparameters\n            to construct encoder_minor.\n\n        ""name"":\n            Name of the encoder.\n        """"""\n        hparams = {\n            ""name"": ""hierarchical_encoder"",\n            ""encoder_major_type"": ""UnidirectionalRNNEncoder"",\n            ""encoder_major_hparams"": {},\n            ""encoder_minor_type"": ""UnidirectionalRNNEncoder"",\n            ""encoder_minor_hparams"": {},\n            ""config_share"": False,\n            ""@no_typecheck"": [\n                \'encoder_major_hparams\',\n                \'encoder_minor_hparams\'\n            ]\n        }\n        hparams.update(EncoderBase.default_hparams())\n        return hparams\n\n    def _build(self,\n               inputs,\n               order=\'btu\',\n               medium=None,\n               sequence_length_major=None,\n               sequence_length_minor=None,\n               **kwargs):\n        """"""Encodes the inputs.\n\n        Args:\n            inputs: A 4-D tensor of shape `[B, T, U, dim]`, where\n\n                - B: batch_size\n                - T: the max length of high-level sequences. E.g., the max \\\n                number of utterances in dialog history.\n                - U: the max length of low-level sequences. E.g., the max \\\n                length of each utterance in dialog history.\n                - dim: embedding dimension\n\n                The order of first three dimensions can be changed\n                according to :attr:`order`.\n\n            order: A 3-char string containing \'b\', \'t\', and \'u\',\n                that specifies the order of inputs dimensions above.\n                Following four can be accepted:\n\n                    - **\'btu\'**: None of the encoders are time-major.\n                    - **\'utb\'**: Both encoders are time-major.\n                    - **\'tbu\'**: The major encoder is time-major.\n                    - **\'ubt\'**: The minor encoder is time-major.\n\n            medium (optional): A list of callables that subsequently process the\n                final states of minor encoder and obtain the inputs\n                for the major encoder.\n                If not specified, :meth:`flatten` is used for processing\n                the minor\'s final states.\n            sequence_length_major (optional): The `sequence_length` argument\n                sent to major encoder. This is a 1-D Tensor of shape\n                `[B]`.\n            sequence_length_minor (optional): The `sequence_length` argument\n                sent to minor encoder. It can be either a 1-D Tensor of shape\n                `[B*T]`, or a 2-D Tensor of shape `[B, T]` or `[T, B]`\n                according to :attr:`order`.\n            **kwargs: Other keyword arguments for the major and minor encoders,\n                such as `initial_state`, etc.\n                Note that `sequence_length`, and `time_major`\n                must not be included here.\n                `time_major` is derived from :attr:`order` automatically.\n                By default, arguments will be sent to both major and minor\n                encoders. To specify which encoder an argument should be sent\n                to, add \'_minor\'/\'_major\' as its suffix.\n\n                Note that `initial_state_minor` must have a batch dimension\n                of size `B*T`. If you have an initial state of batch dimension\n                = `T`, use :meth:`tile_initial_state_minor` to tile it\n                according to `order`.\n\n        Returns:\n            A tuple `(outputs, final_state)` by the major encoder.\n\n            See\n            the return values of `_build()` method of respective encoder class\n            for details.\n        """"""\n\n        def _kwargs_split(kwargs):\n            kwargs_minor, kwargs_major = {}, {}\n            for k, v in kwargs.items():\n                if len(k) >= 6 and k[-6:] == [\'_minor\']:\n                    kwargs_minor[k[:-6]] = v\n                if len(k) >= 6 and k[-6:] == [\'_major\']:\n                    kwargs_major[k[:-6]] = v\n            return kwargs_minor, kwargs_major\n\n        kwargs_minor, kwargs_major = _kwargs_split(kwargs)\n        if sequence_length_minor is not None:\n            sequence_length_minor = tf.reshape(sequence_length_minor, [-1])\n        kwargs_minor[\'sequence_length\'] = sequence_length_minor\n        kwargs_major[\'sequence_length\'] = sequence_length_major\n\n        expand, shape = self._get_flatten_order(\n            order, kwargs_minor, kwargs_major, tf.shape(inputs))\n\n        inputs = tf.reshape(inputs, shape + [inputs.shape[3]])\n\n        _, states_minor = self._encoder_minor(inputs, **kwargs_minor)\n\n        self.states_minor_before_medium = states_minor\n\n        if medium is None:\n            states_minor = self.flatten(states_minor)\n        else:\n            if not isinstance(medium, collections.Sequence):\n                medium = [medium]\n            for fn in medium:\n                if isinstance(fn, str) and fn == \'flatten\':\n                    states_minor = self.flatten(states_minor)\n                else:\n                    states_minor = fn(states_minor)\n\n        self.states_minor_after_medium = states_minor\n\n        states_minor = tf.reshape(\n            states_minor, tf.concat([expand, tf.shape(states_minor)[1:]], 0))\n\n        outputs_major, states_major = self._encoder_major(states_minor,\n                                                          **kwargs_major)\n\n        # Add trainable variables of `self._cell` which may be constructed\n        # externally\n        if not self._built:\n            self._add_trainable_variable(\n                self._encoder_minor.trainable_variables)\n            self._add_trainable_variable(\n                self._encoder_major.trainable_variables)\n            self._built = True\n\n        return outputs_major, states_major\n\n    @staticmethod\n    def tile_initial_state_minor(initial_state, order, inputs_shape):\n        """"""Tiles an initial state to be used for encoder minor.\n\n        The batch dimension of :attr:`initial_state` must equal `T`. The\n        state will be copied for `B` times and used to start encoding each\n        low-level sequence. For example, the first utterance in each dialog\n        history in the batch will have the same initial state.\n\n        Args:\n            initial_state: Initial state with the batch dimension of size `T`.\n            order (str): The dimension order of inputs. Must be the same as\n                used in :meth:`_build`.\n            inputs_shape: Shape of `inputs` for :meth:`_build`. Can usually\n                be Obtained with `tf.shape(inputs)`.\n\n        Returns:\n            A tiled initial state with batch dimension of size `B*T`\n        """"""\n        def _nest_tile(t, multiplier):\n            return nest.map_structure(lambda x: tf.tile(x, multiplier), t)\n\n        if order == \'btu\':\n            return _nest_tile(initial_state, inputs_shape[0])\n        elif order == \'ubt\':\n            return _nest_tile(initial_state, inputs_shape[1])\n        elif order == \'utb\':\n            return tf.contrib.seq2seq.tile_batch(initial_state, inputs_shape[2])\n        elif order == \'tbu\':\n            return tf.contrib.seq2seq.tile_batch(initial_state, inputs_shape[1])\n        else:\n            raise ValueError(\'Unknown order: {}\'.format(order))\n\n    @staticmethod\n    def _get_flatten_order(order, kwargs_minor, kwargs_major, shape):\n        if order == \'btu\':\n            kwargs_minor.setdefault(\'time_major\', False)\n            kwargs_major.setdefault(\'time_major\', False)\n            expand = shape[0:2]\n            shape = [shape[0] * shape[1], shape[2]]\n        elif order == \'utb\':\n            kwargs_minor.setdefault(\'time_major\', True)\n            kwargs_major.setdefault(\'time_major\', True)\n            expand = shape[1:3]\n            shape = [shape[0], shape[1] * shape[2]]\n        elif order == \'tbu\':\n            kwargs_minor.setdefault(\'time_major\', False)\n            kwargs_major.setdefault(\'time_major\', True)\n            expand = shape[0:2]\n            shape = [shape[0] * shape[1], shape[2]]\n        elif order == \'ubt\':\n            kwargs_minor.setdefault(\'time_major\', True)\n            kwargs_major.setdefault(\'time_major\', False)\n            expand = shape[1:3]\n            shape = [shape[0], shape[1] * shape[2]]\n        else:\n            raise ValueError(\'Unknown order: {}\'.format(order))\n\n        return expand, shape\n\n    @staticmethod\n    def flatten(x):\n        """"""Flattens a cell state by concatenating a sequence of cell\n        states along the last dimension. If the cell states are\n        :tf_main:`LSTMStateTuple <contrib/rnn/LSTMStateTuple>`, only the\n        hidden `LSTMStateTuple.h` is used.\n\n        This process is used by default if :attr:`medium` is not provided\n        to :meth:`_build`.\n        """"""\n        if isinstance(x, LSTMStateTuple):\n            return x.h\n        if isinstance(x, collections.Sequence):\n            return tf.concat(\n                [HierarchicalRNNEncoder.flatten(v) for v in x], -1)\n        else:\n            return x\n\n    @property\n    def encoder_major(self):\n        """"""The high-level encoder.\n        """"""\n        return self._encoder_major\n\n    @property\n    def encoder_minor(self):\n        """"""The low-level encoder.\n        """"""\n        return self._encoder_minor\n'"
texar/tf/modules/encoders/multihead_attention.py,30,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTransformer encoders with multihead self attention.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.core import layers\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.utils.shapes import shape_list\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.utils import transpose_batch_time\n\n# pylint: disable=too-many-locals, invalid-name, arguments-differ\n# pylint: disable=too-many-arguments\n\n__all__ = [\n    ""MultiheadAttentionEncoder""\n]\n\n\nclass MultiheadAttentionEncoder(EncoderBase):\n    """"""Multihead Attention Encoder\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n    def __init__(self, hparams=None):\n        EncoderBase.__init__(self, hparams)\n        use_bias = self._hparams.use_bias\n\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    layers.get_initializer(self._hparams.initializer))\n\n            self.Q_dense = tf.layers.Dense(self._hparams.num_units,\n                                           use_bias=use_bias,\n                                           name=\'query\')\n            self.K_dense = tf.layers.Dense(self._hparams.num_units,\n                                           use_bias=use_bias,\n                                           name=\'key\')\n            self.V_dense = tf.layers.Dense(self._hparams.num_units,\n                                           use_bias=use_bias,\n                                           name=\'value\')\n            self.O_dense = tf.layers.Dense(self._hparams.output_dim,\n                                           use_bias=use_bias,\n                                           name=\'output\')\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""initializer"": None,\n                \'num_heads\': 8,\n                \'output_dim\': 512,\n                \'num_units\': 512,\n                \'dropout_rate\': 0.1,\n                \'use_bias\': False,\n                ""name"": ""multihead_attention""\n            }\n\n        Here:\n\n        ""initializer"": dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.tf.core.get_initializer` for details.\n\n        ""num_heads"": int\n            Number of heads for attention calculation.\n\n        ""output_dim"": int\n            Output dimension of the returned tensor.\n\n        ""num_units"": int\n            Hidden dimension of the unsplitted attention space.\n            Should be devisible by `num_heads`.\n\n        ""dropout_rate: : float\n            Dropout rate in the attention.\n\n        ""use_bias"": bool\n            Use bias when projecting the key, value and query.\n\n        ""name"": str\n            Name of the module.\n        """"""\n        return {\n            \'initializer\': None,\n            \'num_heads\': 8,\n            \'output_dim\': 512,\n            \'num_units\': 512,\n            \'dropout_rate\': 0.1,\n            \'use_bias\': False,\n            ""name"": ""multihead_attention"",\n        }\n\n    def _build(self, queries, memory, memory_attention_bias,\n               cache=None, mode=None):\n        """"""Encodes the inputs.\n\n        Args:\n            queries: A 3d tensor with shape of [batch, length_query,\n                depth_query].\n            memory: A 3d tensor with shape of [batch, length_key, depth_key].\n            memory_attention_bias: A 3d tensor with shape of\n                [batch, length_key, num_units].\n            cache: Memory cache only when inferencing the sentence from sractch.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL` and `PREDICT`. Controls dropout mode.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            A Tensor of shape `[batch_size, max_time, dim]` containing the\n            encoded vectors.\n        """"""\n\n        with tf.variable_scope(self.variable_scope):\n            num_heads = self._hparams.num_heads\n            num_units = self._hparams.num_units\n            if num_units % num_heads:\n                raise ValueError(""Value depth (%d) must be divisible by ""\n                                 ""the number of attention heads (%d).""\n                                 % (num_units, num_heads))\n\n            def _update_and_return(layer, key):\n                if memory is None:\n                    # Self Attention\n                    out = layer(queries)\n\n                    if cache is not None:\n                        # \'decoder self attention when dynamic decoding\'\n                        key = \'self_{}\'.format(key)\n                        res = cache[key]\n                        if isinstance(res, tf.TensorArray):\n                            # inference-like decoding\n                            # TODO(zhiting): This writing op may cause a bug\n                            # on CPU--it looks the two TensorArray\n                            # cache[\'self_keys\'] and cache[\'self_values\']\n                            # will mix up starting from certain step, causing\n                            # shape mismatch. This op looks fine on GPU.\n                            res = res.write(\n                                res.size(), tf.squeeze(out, axis=[1]))\n                            out = transpose_batch_time(res.stack())\n                        else:\n                            # normal decoding\n                            res = tf.concat([res, out], axis=1)\n                            out = res\n                        cache[key] = res\n\n                else:\n                    # encoder decoder attention\n                    if cache is not None:\n                        key = \'memory_{}\'.format(key)\n                        res = cache[key]\n                        if isinstance(res, tf.TensorArray):\n                            # inference-like decoding\n                            size = res.size()\n                            false_fn = lambda: transpose_batch_time(res.stack())\n                        else:\n                            # normal decoding\n                            size = tf.shape(res)[1]\n                            false_fn = lambda: res\n                        out = tf.cond(\n                            tf.equal(size, 0),\n                            true_fn=lambda: layer(memory),\n                            false_fn=false_fn)\n                    else:\n                        out = layer(memory)\n\n                return out\n\n            Q = self.Q_dense(queries)\n            K = _update_and_return(self.K_dense, \'keys\')\n            V = _update_and_return(self.V_dense, \'values\')\n\n            Q_ = self._split_heads(Q)\n            K_ = self._split_heads(K)\n            V_ = self._split_heads(V)\n            # [batch_size, num_heads, seq_length, memory_depth]\n            key_depth_per_head = num_units // num_heads\n            Q_ *= key_depth_per_head**-0.5\n\n            logits = tf.matmul(Q_, K_, transpose_b=True)\n            if memory_attention_bias is not None:\n                logits += memory_attention_bias\n            weights = tf.nn.softmax(logits, name=""attention_weights"")\n            weights = tf.layers.dropout(weights,\n                                        rate=self._hparams.dropout_rate,\n                                        training=is_train_mode(mode))\n            outputs = tf.matmul(weights, V_)\n\n            outputs = self._combine_heads(outputs)\n            outputs = self.O_dense(outputs)\n            # (batch_size, length_query, output_dim)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return outputs\n\n    def _split_heads(self, x):\n        """"""Split channels (dimension 2) into multiple heads,\n        becomes dimension 1).\n\n        Must ensure `x.shape[-1]` can be deviced by num_heads\n        """"""\n        depth = shape_list(x)[-1]\n        splitted_x = tf.reshape(x, [tf.shape(x)[0], tf.shape(x)[1],\n                                    self._hparams.num_heads,\n                                    depth // self._hparams.num_heads])\n        return tf.transpose(splitted_x, [0, 2, 1, 3])\n\n    def _combine_heads(self, x):\n        """"""\n        Args:\n            x: A Tensor of shape `[batch, num_heads, seq_len, dim]`\n\n        Returns:\n            A Tensor of shape `[batch, seq_len, num_heads * dim]`\n        """"""\n        t = tf.transpose(x, [0, 2, 1, 3])  # [batch, seq_len, num_heads, dim]\n        num_heads, dim = shape_list(t)[-2:]\n        assert num_heads == self._hparams.num_heads\n        return tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], num_heads * dim])\n'"
texar/tf/modules/encoders/rnn_encoders.py,38,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious RNN encoders.\n""""""\n\nimport functools\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.contrib.framework import nest\n\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.modules.networks.conv_networks import _to_list\nfrom texar.tf.core import layers\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.utils.shapes import mask_sequences\nfrom texar.tf.hyperparams import HParams\n\n# pylint: disable=too-many-arguments, too-many-locals, invalid-name, no-member\n\n__all__ = [\n    ""_forward_single_output_layer"",\n    ""RNNEncoderBase"",\n    ""UnidirectionalRNNEncoder"",\n    ""BidirectionalRNNEncoder""\n]\n\n\ndef _default_output_layer_hparams():\n    return {\n        ""num_layers"": 0,\n        ""layer_size"": 128,\n        ""activation"": ""identity"",\n        ""final_layer_activation"": None,\n        ""other_dense_kwargs"": None,\n        ""dropout_layer_ids"": [],\n        ""dropout_rate"": 0.5,\n        ""variational_dropout"": False,\n        ""@no_typecheck"": [""activation"", ""final_layer_activation"",\n                          ""layer_size"", ""dropout_layer_ids""]\n    }\n\n\ndef _build_dense_output_layer(hparams):\n    nlayers = hparams.num_layers\n\n    if nlayers <= 0:\n        return None\n\n    layer_size = _to_list(\n        hparams.layer_size, \'output_layer.layer_size\', nlayers)\n\n    other_kwargs = hparams.other_dense_kwargs or {}\n    if isinstance(other_kwargs, HParams):\n        other_kwargs = other_kwargs.todict()\n    if not isinstance(other_kwargs, dict):\n        raise ValueError(\n            ""hparams \'output_layer.other_dense_kwargs\' must be a dict."")\n\n    dense_layers = []\n    for i in range(nlayers):\n        if i == nlayers - 1:\n            activation = hparams.final_layer_activation\n        else:\n            activation = hparams.activation\n\n        kwargs_i = {""units"": layer_size[i],\n                    ""activation"": activation,\n                    ""name"": ""dense_%d"" % (i + 1)}\n        kwargs_i.update(other_kwargs)\n\n        layer_hparams = {""type"": ""Dense"", ""kwargs"": kwargs_i}\n        dense_layers.append(layers.get_layer(hparams=layer_hparams))\n\n    if len(dense_layers) == 1:\n        dense_layers = dense_layers[0]\n\n    return dense_layers\n\n\ndef _forward_single_output_layer(inputs, input_size, output_layer):\n    """"""Forwards the input through a single output layer.\n\n    Args:\n        inputs: A Tensor of shape `[batch_size, max_time] + input_size` if\n            :attr:`time_major=False`, or shape\n            `[max_time, batch_size] + input_size` if :attr:`time_major=True`.\n        input_size: An `int` or 1D `int` array.\n    """"""\n    dim = np.prod(input_size)\n    inputs_flat = inputs\n    inputs_flat = tf.reshape(inputs_flat, [-1, dim])\n    # Feed to the layer\n    output_flat = output_layer(inputs_flat)\n    output_size = output_layer.compute_output_shape([1, dim]).as_list()[1:]\n    output_size = np.array(output_size)\n    # Reshape output to [batch_size/max_time, max_time/batch_size] + output_size\n    output_shape = tf.concat([tf.shape(inputs)[:2], output_size], axis=0)\n    output = tf.reshape(output_flat, output_shape)\n    return output, output_size\n\n\ndef _apply_dropout(inputs, time_major, hparams, training):\n    """"""Applies dropout to the inputs.\n\n    :attr:`inputs` is a Tensor of shape `[batch_size, max_time, dim]`\n    if :attr:`time_major=False`, or shape `[max_time, batch_size, dim]`\n    if :attr:`time_major=True`.\n    """"""\n    noise_shape = None\n    if hparams.variational_dropout:\n        if time_major:\n            noise_shape = [1, None, None]\n        else:\n            noise_shape = [None, 1, None]\n    return tf.layers.dropout(inputs, rate=hparams.dropout_rate,\n                             noise_shape=noise_shape, training=training)\n\n\ndef _forward_output_layers(inputs, input_size, output_layer, time_major,\n                           hparams, mode, sequence_length=None):\n    """"""Forwards inputs through the output layers.\n\n    Args:\n        inputs: A Tensor of shape `[batch_size, max_time] + input_size` if\n            :attr:`time_major=False`, or shape\n            `[max_time, batch_size] + input_size` if :attr:`time_major=True`.\n\n    Returns:\n        A pair :attr:`(outputs, outputs_size), where\n\n        - :attr:`outputs`: A Tensor of shape \\\n          `[batch_size, max_time] + outputs_size`.\n\n        - :attr:`outputs_size`: An `int` or 1D `int` array representing the \\\n          output size.\n    """"""\n    if output_layer is None:\n        return inputs, input_size\n\n    if hparams is None:\n        # output_layer was passed in from the constructor\n        if isinstance(output_layer, (list, tuple)):\n            raise ValueError(\'output_layer must not be a list or tuple.\')\n        output, output_size = _forward_single_output_layer(\n            inputs, input_size, output_layer)\n    else:\n        # output_layer was built based on hparams\n        output_layer = _to_list(output_layer)\n\n        dropout_layer_ids = _to_list(hparams.dropout_layer_ids)\n        if len(dropout_layer_ids) > 0:\n            training = is_train_mode(mode)\n\n        output = inputs\n        output_size = input_size\n        for i, layer in enumerate(output_layer):\n            if i in dropout_layer_ids:\n                output = _apply_dropout(output, time_major, hparams, training)\n            output, output_size = _forward_single_output_layer(\n                output, output_size, layer)\n\n        if len(output_layer) in dropout_layer_ids:\n            output = _apply_dropout(output, time_major, hparams, training)\n\n    if sequence_length is not None:\n        output = mask_sequences(\n            output, sequence_length, time_major=time_major, tensor_rank=3)\n\n    return output, output_size\n\n\ndef _apply_rnn_encoder_output_layer(output_layer, time_major, hparams, mode,\n                                    cell_outputs, cell_output_size):\n    map_func = functools.partial(\n        _forward_output_layers,\n        output_layer=output_layer,\n        time_major=time_major,\n        hparams=hparams,\n        mode=mode)\n    cell_outputs_flat = nest.flatten(cell_outputs)\n    cell_output_size_flat = nest.flatten(cell_output_size)\n    o = [map_func(inputs=x, input_size=xs)\n         for x, xs in zip(cell_outputs_flat, cell_output_size_flat)]\n    outputs_flat, output_size_flat = zip(*o)\n    outputs = nest.pack_sequence_as(cell_outputs, outputs_flat)\n    output_size = nest.pack_sequence_as(cell_outputs, output_size_flat)\n    return outputs, output_size\n\n\nclass RNNEncoderBase(EncoderBase):\n    """"""Base class for all RNN encoder classes to inherit.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n\n    def __init__(self, hparams=None):\n        EncoderBase.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""rnn_encoder""\n            }\n        """"""\n        return {\n            ""name"": ""rnn_encoder""\n        }\n\n    def _build(self, inputs, *args, **kwargs):\n        """"""Encodes the inputs.\n\n        Args:\n            inputs: Inputs to the encoder.\n            *args: Other arguments.\n            **kwargs: Keyword arguments.\n\n        Returns:\n            Encoding results.\n        """"""\n        raise NotImplementedError\n\n\nclass UnidirectionalRNNEncoder(RNNEncoderBase):\n    """"""One directional RNN encoder.\n\n    Args:\n        cell: (RNNCell, optional) If not specified,\n            a cell is created as specified in :attr:`hparams[""rnn_cell""]`.\n        cell_dropout_mode (optional): A Tensor taking value of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, which\n            toggles dropout in the RNN cell (e.g., activates dropout in\n            TRAIN mode). If `None`, :func:`~texar.tf.global_mode` is used.\n            Ignored if :attr:`cell` is given.\n        output_layer (optional): An instance of\n            :tf_main:`tf.layers.Layer <layers/Layer>`. Applies to the RNN cell\n            output of each step. If `None` (default), the output layer is\n            created as specified in :attr:`hparams[""output_layer""]`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs of the encoder.\n\n    Example:\n\n        .. code-block:: python\n\n            # Use with embedder\n            embedder = WordEmbedder(vocab_size, hparams=emb_hparams)\n            encoder = UnidirectionalRNNEncoder(hparams=enc_hparams)\n\n            outputs, final_state = encoder(\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\'])\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 cell=None,\n                 cell_dropout_mode=None,\n                 output_layer=None,\n                 hparams=None):\n        RNNEncoderBase.__init__(self, hparams)\n\n        # Make RNN cell\n        with tf.variable_scope(self.variable_scope):\n            if cell is not None:\n                self._cell = cell\n            else:\n                self._cell = layers.get_rnn_cell(\n                    self._hparams.rnn_cell, cell_dropout_mode)\n\n        # Make output layer\n        with tf.variable_scope(self.variable_scope):\n            if output_layer is not None:\n                self._output_layer = output_layer\n                self._output_layer_hparams = None\n            else:\n                self._output_layer = _build_dense_output_layer(\n                    self._hparams.output_layer)\n                self._output_layer_hparams = self._hparams.output_layer\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""rnn_cell"": default_rnn_cell_hparams(),\n                ""output_layer"": {\n                    ""num_layers"": 0,\n                    ""layer_size"": 128,\n                    ""activation"": ""identity"",\n                    ""final_layer_activation"": None,\n                    ""other_dense_kwargs"": None,\n                    ""dropout_layer_ids"": [],\n                    ""dropout_rate"": 0.5,\n                    ""variational_dropout"": False\n                },\n                ""name"": ""unidirectional_rnn_encoder""\n            }\n\n        Here:\n\n        ""rnn_cell"": dict\n            A dictionary of RNN cell hyperparameters. Ignored if\n            :attr:`cell` is given to the encoder constructor.\n\n            The default value is defined in\n            :func:`~texar.tf.core.default_rnn_cell_hparams`.\n\n        ""output_layer"": dict\n            Output layer hyperparameters. Ignored if :attr:`output_layer`\n            is given to the encoder constructor. Includes:\n\n            ""num_layers"": int\n                The number of output (dense) layers. Set to 0 to avoid any\n                output layers applied to the cell outputs..\n\n            ""layer_size"": int or list\n                The size of each of the output (dense) layers.\n\n                If an `int`, each output layer will have the same size. If\n                a list, the length must equal to :attr:`num_layers`.\n\n            ""activation"": str or callable or None\n                Activation function for each of the output (dense)\n                layer except for the final layer. This can be\n                a function, or its string name or module path.\n                If function name is given, the function must be from\n                module :tf_main:`tf.nn <nn>` or :tf_main:`tf < >`.\n                For example\n\n                .. code-block:: python\n\n                    ""activation"": ""relu"" # function name\n                    ""activation"": ""my_module.my_activation_fn"" # module path\n                    ""activation"": my_module.my_activation_fn # function\n\n                Default is `None` which maintains a linear activation.\n\n            ""final_layer_activation"": str or callable or None\n                The activation function for the final output layer.\n\n            ""other_dense_kwargs"": dict or None\n                Other keyword arguments to construct each of the output\n                dense layers, e.g., `use_bias`. See\n                :tf_main:`Dense <layers/Dense>` for the keyword arguments.\n\n            ""dropout_layer_ids"": int or list\n                The indexes of layers (starting from `0`) whose inputs\n                are applied with dropout. The index = :attr:`num_layers`\n                means dropout applies to the final layer output. E.g.,\n\n                .. code-block:: python\n\n                    {\n                        ""num_layers"": 2,\n                        ""dropout_layer_ids"": [0, 2]\n                    }\n\n                will leads to a series of layers as\n                `-dropout-layer0-layer1-dropout-`.\n\n                The dropout mode (training or not) is controlled\n                by the :attr:`mode` argument of :meth:`_build`.\n\n            ""dropout_rate"": float\n                The dropout rate, between 0 and 1. E.g.,\n                `""dropout_rate"": 0.1` would drop out 10% of elements.\n\n            ""variational_dropout"": bool\n                Whether the dropout mask is the same across all time steps.\n\n        ""name"": str\n            Name of the encoder\n        """"""\n        hparams = RNNEncoderBase.default_hparams()\n        hparams.update({\n            ""rnn_cell"": layers.default_rnn_cell_hparams(),\n            ""output_layer"": _default_output_layer_hparams(),\n            ""name"": ""unidirectional_rnn_encoder""\n        })\n        return hparams\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               initial_state=None,\n               time_major=False,\n               mode=None,\n               return_cell_output=False,\n               return_output_size=False,\n               **kwargs):\n        """"""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape `[batch_size, max_time, dim]`.\n                The first two dimensions\n                :attr:`batch_size` and :attr:`max_time` are exchanged if\n                :attr:`time_major=True` is specified.\n            sequence_length (optional): A 1D int tensor of shape `[batch_size]`.\n                Sequence lengths\n                of the batch inputs. Used to copy-through state and zero-out\n                outputs when past a batch element\'s sequence length.\n            initial_state (optional): Initial state of the RNN.\n            time_major (bool): The shape format of the :attr:`inputs` and\n                :attr:`outputs` Tensors. If `True`, these tensors are of shape\n                `[max_time, batch_size, depth]`. If `False` (default),\n                these tensors are of shape `[batch_size, max_time, depth]`.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. Controls output layer dropout\n                if the output layer is specified with :attr:`hparams`.\n                If `None` (default), :func:`texar.tf.global_mode`\n                is used.\n            return_cell_output (bool): Whether to return the output of the RNN\n                cell. This is the results prior to the output layer.\n            return_output_size (bool): Whether to return the size of the\n                output (i.e., the results after output layers).\n            **kwargs: Optional keyword arguments of\n                :tf_main:`tf.nn.dynamic_rnn <nn/dynamic_rnn>`,\n                such as `swap_memory`, `dtype`, `parallel_iterations`, etc.\n\n        Returns:\n            - By default (both `return_cell_output` and \\\n            `return_output_size` are False), returns a pair \\\n            :attr:`(outputs, final_state)`\n\n                - :attr:`outputs`: The RNN output tensor by the output layer \\\n                (if exists) or the RNN cell (otherwise). The tensor is of \\\n                shape `[batch_size, max_time, output_size]` if \\\n                `time_major` is False, or \\\n                `[max_time, batch_size, output_size]` if \\\n                `time_major` is True. \\\n                If RNN cell output is a (nested) tuple of Tensors, then the \\\n                :attr:`outputs` will be a (nested) tuple having the same \\\n                nest structure as the cell output.\n\n                - :attr:`final_state`: The final state of the RNN, which is a \\\n                Tensor of shape `[batch_size] + cell.state_size` or \\\n                a (nested) tuple of Tensors if `cell.state_size` is a (nested)\\\n                tuple.\n\n            - If `return_cell_output` is True, returns a triple \\\n            :attr:`(outputs, final_state, cell_outputs)`\n\n                - :attr:`cell_outputs`: The outputs by the RNN cell prior to \\\n                the \\\n                output layer, having the same structure with :attr:`outputs` \\\n                except for the `output_dim`.\n\n            - If `return_output_size` is `True`, returns a tuple \\\n            :attr:`(outputs, final_state, output_size)`\n\n                - :attr:`output_size`: A (possibly nested tuple of) int \\\n                representing the size of :attr:`outputs`. If a single int or \\\n                an int array, then `outputs` has shape \\\n                `[batch/time, time/batch] + output_size`. If \\\n                a (nested) tuple, then `output_size` has the same \\\n                structure as with `outputs`.\n\n            - If both `return_cell_output` and \\\n            `return_output_size` are True, returns \\\n            :attr:`(outputs, final_state, cell_outputs, output_size)`.\n        """"""\n        if (\'dtype\' not in kwargs) and (initial_state is None):\n            cell_outputs, state = tf.nn.dynamic_rnn(\n                cell=self._cell,\n                inputs=inputs,\n                sequence_length=sequence_length,\n                initial_state=initial_state,\n                time_major=time_major,\n                dtype=tf.float32,\n                **kwargs)\n        else:\n            cell_outputs, state = tf.nn.dynamic_rnn(\n                cell=self._cell,\n                inputs=inputs,\n                sequence_length=sequence_length,\n                initial_state=initial_state,\n                time_major=time_major,\n                **kwargs)\n\n        outputs, output_size = _apply_rnn_encoder_output_layer(\n            self._output_layer, time_major, self._output_layer_hparams,\n            mode, cell_outputs, self._cell.output_size)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            # Add trainable variables of `self._cell` and `self._output_layer`\n            # which may be constructed externally.\n            self._add_trainable_variable(\n                layers.get_rnn_cell_trainable_variables(self._cell))\n            if self._output_layer and \\\n                    not isinstance(self._output_layer, (list, tuple)):\n                self._add_trainable_variable(\n                    self._output_layer.trainable_variables)\n            self._built = True\n\n        rets = (outputs, state)\n        if return_cell_output:\n            rets += (cell_outputs, )\n        if return_output_size:\n            rets += (output_size, )\n        return rets\n\n    @property\n    def cell(self):\n        """"""The RNN cell.\n        """"""\n        return self._cell\n\n    @property\n    def state_size(self):\n        """"""The state size of encoder cell.\n\n        Same as :attr:`encoder.cell.state_size`.\n        """"""\n        return self.cell.state_size\n\n    @property\n    def output_layer(self):\n        """"""The output layer.\n        """"""\n        return self._output_layer\n\n\nclass BidirectionalRNNEncoder(RNNEncoderBase):\n    """"""Bidirectional forward-backward RNN encoder.\n\n    Args:\n        cell_fw (RNNCell, optional): The forward RNN cell. If not given,\n            a cell is created as specified in :attr:`hparams[""rnn_cell_fw""]`.\n        cell_bw (RNNCell, optional): The backward RNN cell. If not given,\n            a cell is created as specified in :attr:`hparams[""rnn_cell_bw""]`.\n        cell_dropout_mode (optional): A tensor taking value of\n            :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, which\n            toggles dropout in the RNN cells (e.g., activates dropout in\n            TRAIN mode). If `None`, :func:`~texar.tf.global_mode()` is\n            used. Ignored if respective cell is given.\n        output_layer_fw (optional): An instance of\n            :tf_main:`tf.layers.Layer <layers/Layer>`. Apply to the forward\n            RNN cell output of each step. If `None` (default), the output\n            layer is created as specified in :attr:`hparams[""output_layer_fw""]`.\n        output_layer_bw (optional): An instance of\n            :tf_main:`tf.layers.Layer <layers/Layer>`. Apply to the backward\n            RNN cell output of each step. If `None` (default), the output\n            layer is created as specified in :attr:`hparams[""output_layer_bw""]`.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs of the encoder.\n\n    Example:\n\n        .. code-block:: python\n\n            # Use with embedder\n            embedder = WordEmbedder(vocab_size, hparams=emb_hparams)\n            encoder = BidirectionalRNNEncoder(hparams=enc_hparams)\n\n            outputs, final_state = encoder(\n                inputs=embedder(data_batch[\'text_ids\']),\n                sequence_length=data_batch[\'length\'])\n            # outputs == (outputs_fw, outputs_bw)\n            # final_state == (final_state_fw, final_state_bw)\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 cell_fw=None,\n                 cell_bw=None,\n                 cell_dropout_mode=None,\n                 output_layer_fw=None,\n                 output_layer_bw=None,\n                 hparams=None):\n        RNNEncoderBase.__init__(self, hparams)\n\n        # Make RNN cells\n        with tf.variable_scope(self.variable_scope):\n            if cell_fw is not None:\n                self._cell_fw = cell_fw\n            else:\n                self._cell_fw = layers.get_rnn_cell(\n                    self._hparams.rnn_cell_fw, cell_dropout_mode)\n\n            if cell_bw is not None:\n                self._cell_bw = cell_bw\n            elif self._hparams.rnn_cell_share_config:\n                self._cell_bw = layers.get_rnn_cell(\n                    self._hparams.rnn_cell_fw, cell_dropout_mode)\n            else:\n                self._cell_bw = layers.get_rnn_cell(\n                    self._hparams.rnn_cell_bw, cell_dropout_mode)\n\n        # Make output layers\n        with tf.variable_scope(self.variable_scope):\n            if output_layer_fw is not None:\n                self._output_layer_fw = output_layer_fw\n                self._output_layer_hparams_fw = None\n            else:\n                self._output_layer_fw = _build_dense_output_layer(\n                    self._hparams.output_layer_fw)\n                self._output_layer_hparams_fw = self._hparams.output_layer_fw\n\n            if output_layer_bw is not None:\n                self._output_layer_bw = output_layer_bw\n                self._output_layer_hparams_bw = None\n            elif self._hparams.output_layer_share_config:\n                self._output_layer_bw = _build_dense_output_layer(\n                    self._hparams.output_layer_fw)\n                self._output_layer_hparams_bw = self._hparams.output_layer_fw\n            else:\n                self._output_layer_bw = _build_dense_output_layer(\n                    self._hparams.output_layer_bw)\n                self._output_layer_hparams_bw = self._hparams.output_layer_bw\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""rnn_cell_fw"": default_rnn_cell_hparams(),\n                ""rnn_cell_bw"": default_rnn_cell_hparams(),\n                ""rnn_cell_share_config"": True,\n                ""output_layer_fw"": {\n                    ""num_layers"": 0,\n                    ""layer_size"": 128,\n                    ""activation"": ""identity"",\n                    ""final_layer_activation"": None,\n                    ""other_dense_kwargs"": None,\n                    ""dropout_layer_ids"": [],\n                    ""dropout_rate"": 0.5,\n                    ""variational_dropout"": False\n                },\n                ""output_layer_bw"": {\n                    # Same hyperparams and default values as ""output_layer_fw""\n                    # ...\n                },\n                ""output_layer_share_config"": True,\n                ""name"": ""bidirectional_rnn_encoder""\n            }\n\n        Here:\n\n        ""rnn_cell_fw"": dict\n            Hyperparameters of the forward RNN cell.\n            Ignored if :attr:`cell_fw` is given to the encoder constructor.\n\n            The default value is defined in\n            :func:`~texar.tf.core.default_rnn_cell_hparams`.\n\n        ""rnn_cell_bw"": dict\n            Hyperparameters of the backward RNN cell.\n            Ignored if :attr:`cell_bw` is given to the encoder constructor\n            , or if :attr:`""rnn_cell_share_config""` is `True`.\n\n            The default value is defined in\n            :meth:`~texar.tf.core.default_rnn_cell_hparams`.\n\n        ""rnn_cell_share_config"": bool\n            Whether share hyperparameters of the backward cell with the\n            forward cell. Note that the cell parameters (variables) are not\n            shared.\n\n        ""output_layer_fw"": dict\n            Hyperparameters of the forward output layer. Ignored if\n            :attr:`output_layer_fw` is given to the constructor.\n            See the ""output_layer"" field of\n            :meth:`~texar.tf.modules.UnidirectionalRNNEncoder.default_hparams` for\n            details.\n\n        ""output_layer_bw"": dict\n            Hyperparameters of the backward output layer. Ignored if\n            :attr:`output_layer_bw` is given to the constructor. Have the\n            same structure and defaults with :attr:`""output_layer_fw""`.\n\n            Ignored if :attr:`""output_layer_share_config""` is True.\n\n        ""output_layer_share_config"": bool\n            Whether share hyperparameters of the backward output layer\n            with the forward output layer. Note that the layer parameters\n            (variables) are not shared.\n\n        ""name"": str\n            Name of the encoder\n        """"""\n        hparams = RNNEncoderBase.default_hparams()\n        hparams.update({\n            ""rnn_cell_fw"": layers.default_rnn_cell_hparams(),\n            ""rnn_cell_bw"": layers.default_rnn_cell_hparams(),\n            ""rnn_cell_share_config"": True,\n            ""output_layer_fw"": _default_output_layer_hparams(),\n            ""output_layer_bw"": _default_output_layer_hparams(),\n            ""output_layer_share_config"": True,\n            ""name"": ""bidirectional_rnn_encoder""\n        })\n        return hparams\n\n    def _build(self,\n               inputs,\n               sequence_length=None,\n               initial_state_fw=None,\n               initial_state_bw=None,\n               time_major=False,\n               mode=None,\n               return_cell_output=False,\n               return_output_size=False,\n               **kwargs):\n        """"""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape `[batch_size, max_time, dim]`.\n                The first two dimensions\n                `batch_size` and `max_time` may be exchanged if\n                `time_major=True` is specified.\n            sequence_length (optional): A 1D int tensor of shape `[batch_size]`.\n                Sequence lengths\n                of the batch inputs. Used to copy-through state and zero-out\n                outputs when past a batch element\'s sequence length.\n            initial_state (optional): Initial state of the RNN.\n            time_major (bool): The shape format of the :attr:`inputs` and\n                :attr:`outputs` Tensors. If `True`, these tensors are of shape\n                `[max_time, batch_size, depth]`. If `False` (default),\n                these tensors are of shape `[batch_size, max_time, depth]`.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. Controls output layer dropout\n                if the output layer is specified with :attr:`hparams`.\n                If `None` (default), :func:`texar.tf.global_mode()`\n                is used.\n            return_cell_output (bool): Whether to return the output of the RNN\n                cell. This is the results prior to the output layer.\n            **kwargs: Optional keyword arguments of\n                :tf_main:`tf.nn.dynamic_rnn <nn/dynamic_rnn>`,\n                such as `swap_memory`, `dtype`, `parallel_iterations`, etc.\n\n        Returns:\n            - By default (both `return_cell_output` and `return_output_size` \\\n            are False), returns a pair :attr:`(outputs, final_state)`\n\n                - :attr:`outputs`: A tuple `(outputs_fw, outputs_bw)` \\\n                containing \\\n                the forward and the backward RNN outputs, each of which is of \\\n                shape `[batch_size, max_time, output_dim]` if \\\n                `time_major` is False, or \\\n                `[max_time, batch_size, output_dim]` if \\\n                `time_major` is True. \\\n                If RNN cell output is a (nested) tuple of Tensors, then \\\n                `outputs_fw` and `outputs_bw` will be a (nested) tuple having \\\n                the same structure as the cell output.\n\n                - :attr:`final_state`: A tuple \\\n                `(final_state_fw, final_state_bw)` \\\n                containing the final states of the forward and backward \\\n                RNNs, each of which is a \\\n                Tensor of shape `[batch_size] + cell.state_size`, or \\\n                a (nested) tuple of Tensors if `cell.state_size` is a (nested)\\\n                tuple.\n\n            - If `return_cell_output` is True, returns a triple \\\n            :attr:`(outputs, final_state, cell_outputs)` where\n\n                - :attr:`cell_outputs`: A tuple \\\n                `(cell_outputs_fw, cell_outputs_bw)` containting the outputs \\\n                by the forward and backward RNN cells prior to the \\\n                output layers, having the same structure with :attr:`outputs` \\\n                except for the `output_dim`.\n\n            - If `return_output_size` is True, returns a tuple \\\n            :attr:`(outputs, final_state, output_size)` where\n\n                - :attr:`output_size`: A tupple \\\n                `(output_size_fw, output_size_bw)` containing the size of \\\n                `outputs_fw` and `outputs_bw`, respectively. \\\n                Take `*_fw` for example, \\\n                `output_size_fw` is a (possibly nested tuple of) int. \\\n                If a single int or an int array, then `outputs_fw` has shape \\\n                `[batch/time, time/batch] + output_size_fw`. If \\\n                a (nested) tuple, then `output_size_fw` has the same \\\n                structure as with `outputs_fw`. The same applies to  \\\n                `output_size_bw`.\n\n            - If both `return_cell_output` and \\\n            `return_output_size` are True, returns \\\n            :attr:`(outputs, final_state, cell_outputs, output_size)`.\n        """"""\n        no_initial_state = initial_state_fw is None and initial_state_bw is None\n        if (\'dtype\' not in kwargs) and no_initial_state:\n            cell_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=self._cell_fw,\n                cell_bw=self._cell_bw,\n                inputs=inputs,\n                sequence_length=sequence_length,\n                initial_state_fw=initial_state_fw,\n                initial_state_bw=initial_state_bw,\n                time_major=time_major,\n                dtype=tf.float32,\n                **kwargs)\n        else:\n            cell_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=self._cell_fw,\n                cell_bw=self._cell_bw,\n                inputs=inputs,\n                sequence_length=sequence_length,\n                initial_state_fw=initial_state_fw,\n                initial_state_bw=initial_state_bw,\n                time_major=time_major,\n                **kwargs)\n\n        outputs_fw, output_size_fw = _apply_rnn_encoder_output_layer(\n            self._output_layer_fw, time_major, self._output_layer_hparams_fw,\n            mode, cell_outputs[0], self._cell_fw.output_size)\n\n        outputs_bw, output_size_bw = _apply_rnn_encoder_output_layer(\n            self._output_layer_bw, time_major, self._output_layer_hparams_bw,\n            mode, cell_outputs[1], self._cell_bw.output_size)\n\n        outputs = (outputs_fw, outputs_bw)\n        output_size = (output_size_fw, output_size_bw)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            # Add trainable variables of cells and output layers\n            # which may be constructed externally.\n            self._add_trainable_variable(\n                layers.get_rnn_cell_trainable_variables(self._cell_fw))\n            self._add_trainable_variable(\n                layers.get_rnn_cell_trainable_variables(self._cell_bw))\n            if self._output_layer_fw and \\\n                    not isinstance(self._output_layer_fw, (list, tuple)):\n                self._add_trainable_variable(\n                    self._output_layer_fw.trainable_variables)\n            if self._output_layer_bw and \\\n                    not isinstance(self._output_layer_bw, (list, tuple)):\n                self._add_trainable_variable(\n                    self._output_layer_bw.trainable_variables)\n            self._built = True\n\n        returns = (outputs, states)\n        if return_cell_output:\n            returns += (cell_outputs, )\n        if return_output_size:\n            returns += (output_size, )\n        return returns\n\n    @property\n    def cell_fw(self):\n        """"""The forward RNN cell.\n        """"""\n        return self._cell_fw\n\n    @property\n    def cell_bw(self):\n        """"""The backward RNN cell.\n        """"""\n        return self._cell_bw\n\n    @property\n    def state_size_fw(self):\n        """"""The state size of the forward encoder cell.\n\n        Same as :attr:`encoder.cell_fw.state_size`.\n        """"""\n        return self.cell_fw.state_size\n\n    @property\n    def state_size_bw(self):\n        """"""The state size of the backward encoder cell.\n\n        Same as :attr:`encoder.cell_bw.state_size`.\n        """"""\n        return self.cell_bw.state_size\n\n    @property\n    def output_layer_fw(self):\n        """"""The output layer of the forward RNN.\n        """"""\n        return self._output_layer_fw\n\n    @property\n    def output_layer_bw(self):\n        """"""The output layer of the backward RNN.\n        """"""\n        return self._output_layer_bw\n'"
texar/tf/modules/encoders/transformer_encoders.py,35,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nTransformer encoders with multihead self attention.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.core import layers\nfrom texar.tf.utils import transformer_attentions as attn, transformer_utils\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.modules.encoders.multihead_attention import MultiheadAttentionEncoder\nfrom texar.tf.modules.networks.networks import FeedForwardNetwork\nfrom texar.tf.utils.shapes import shape_list\nfrom texar.tf.utils.mode import is_train_mode\n\n# pylint: disable=too-many-locals, invalid-name\n# pylint: disable=arguments-differ, too-many-branches, too-many-statements\n\n__all__ = [\n    ""default_transformer_poswise_net_hparams"",\n    ""TransformerEncoder""\n]\n\n\ndef default_transformer_poswise_net_hparams(output_dim=512):\n    """"""Returns default hyperparameters of a\n    :class:`~texar.tf.modules.FeedForwardNetwork` as a pos-wise network used\n    in :class:`~texar.tf.modules.TransformerEncoder` and\n    :class:`~texar.tf.modules.TransformerDecoder`.\n\n    This is a 2-layer dense network with dropout in-between.\n\n    .. code-block:: python\n\n        {\n            ""layers"": [\n                {\n                    ""type"": ""Dense"",\n                    ""kwargs"": {\n                        ""name"": ""conv1"",\n                        ""units"": output_dim*4,\n                        ""activation"": ""relu"",\n                        ""use_bias"": True,\n                    }\n                },\n                {\n                    ""type"": ""Dropout"",\n                    ""kwargs"": {\n                        ""rate"": 0.1,\n                    }\n                },\n                {\n                    ""type"": ""Dense"",\n                    ""kwargs"": {\n                        ""name"": ""conv2"",\n                        ""units"": output_dim,\n                        ""use_bias"": True,\n                    }\n                }\n            ],\n            ""name"": ""ffn""\n        }\n\n    Args:\n        output_dim (int): The size of output dense layer.\n    """"""\n    return {\n        ""layers"": [\n            {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""name"": ""conv1"",\n                    ""units"": output_dim * 4,\n                    ""activation"": ""relu"",\n                    ""use_bias"": True,\n                }\n            },\n            {\n                ""type"": ""Dropout"",\n                ""kwargs"": {\n                    ""rate"": 0.1,\n                }\n            },\n            {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""name"": ""conv2"",\n                    ""units"": output_dim,\n                    ""use_bias"": True,\n                }\n            }\n        ],\n        ""name"": ""ffn""\n    }\n\n\nclass TransformerEncoder(EncoderBase):\n    """"""Transformer encoder that applies multi-head self attention for encoding\n    sequences.\n\n    This module basically stacks\n    :class:`~texar.tf.modules.encoders.MultiheadAttentionEncoder`,\n    :class:`~texar.tf.modules.FeedForwardNetwork` and residual connections.\n\n    This module supports two types of architectures, namely, the standard\n    Transformer Encoder architecture first proposed in\n    `(Vaswani et al.) ""Attention is All You Need""`, and\n    the variant first used in `(Devlin et al.)` BERT. See\n    :meth:`default_hparams` for the nuance between the two types of\n    architectures.\n\n    Args:\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n    def __init__(self, hparams=None):\n        EncoderBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    layers.get_initializer(self._hparams.initializer))\n\n            self.multihead_attention_list = []\n            self.poswise_networks = []\n            for i in range(self._hparams.num_blocks):\n                with tf.variable_scope(""layer_{}"".format(i)):\n\n                    with tf.variable_scope(\'attention\'):\n                        mh_attn = MultiheadAttentionEncoder(\n                            self._hparams.multihead_attention)\n                        self.multihead_attention_list.append(mh_attn)\n\n                        if self._hparams.dim != mh_attn.hparams.output_dim:\n                            raise ValueError(\n                                \'The ""dim"" in the hparams of \'\n                                \'""multihead_attention"" should be equal to the \'\n                                \'""dim"" of TransformerEncoder\')\n\n                    pw_net = FeedForwardNetwork(\n                        hparams=self._hparams[\'poswise_feedforward\'])\n                    final_dim = pw_net.hparams.layers[-1][\'kwargs\'][\'units\']\n                    if self._hparams.dim != final_dim:\n                        raise ValueError(\n                            \'The output dimenstion of \'\n                            \'""poswise_feedforward"" should be equal \'\n                            \'to the ""dim"" of TransformerEncoder.\')\n                    self.poswise_networks.append(pw_net)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""num_blocks"": 6,\n                ""dim"": 512,\n                \'use_bert_config\': False,\n                ""embedding_dropout"": 0.1,\n                ""residual_dropout"": 0.1,\n                ""poswise_feedforward"": default_transformer_poswise_net_hparams,\n                \'multihead_attention\': {\n                    \'name\': \'multihead_attention\',\n                    \'num_units\': 512,\n                    \'output_dim\': 512,\n                    \'num_heads\': 8,\n                    \'dropout_rate\': 0.1,\n                    \'output_dim\': 512,\n                    \'use_bias\': False,\n                },\n                ""initializer"": None,\n                ""name"": ""transformer_encoder""\n            }\n\n        Here:\n\n        ""num_blocks"": int\n            Number of stacked blocks.\n\n        ""dim"": int\n            Hidden dimension of the encoders.\n\n        ""use_bert_config"": bool\n            If `False`, apply the standard Transformer Encoder architecture from\n            the original paper `(Vaswani et al.) ""Attention is All You Need""`.\n            If `True`, apply the Transformer Encoder architecture used in BERT\n            `(Devlin et al.)`.\n\n            The differences lie in:\n\n                1. The standard arch restricts the word embedding of PAD token \\\n                   to all zero. The BERT arch does not.\n\n                2. The attention bias for padding tokens: \\\n                   The standard arch uses `-1e8` for nagative attention mask. \\\n                   BERT uses `-1e4` instead.\n\n                3. The residual connections between internal tensors: \\\n                   In BERT, a residual layer connects the tensors *after* \\\n                   layer normalization. In the standard arch, the tensors are \\\n                   connected *before* layer normalization.\n\n        ""embedding_dropout"": float\n            Dropout rate of the input embedding.\n\n        ""residual_dropout"":  float\n            Dropout rate of the residual connections.\n\n        ""poswise_feedforward"": dict\n            Hyperparameters for a feed-forward network used in residual\n            connections.\n            Make sure the dimension of the output tensor is equal to `dim`.\n\n            See :func:`~texar.tf.modules.default_transformer_poswise_net_hparams`\n            for details.\n\n        ""multihead_attention"": dict\n            Hyperparameters for the multihead attention strategy.\n            Make sure the ""output_dim"" in this module is equal to ""dim"".\n            See :func:`~texar.tf.modules.MultiheadAttentionEncoder.default_harams`\n            for details.\n\n        ""initializer"": dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.tf.core.get_initializer` for details.\n\n        ""name"": str\n            Name of the module.\n        """"""\n        return {\n            \'num_blocks\': 6,\n            \'dim\': 512,\n            \'use_bert_config\': False,\n            \'embedding_dropout\': 0.1,\n            \'residual_dropout\': 0.1,\n            \'poswise_feedforward\': default_transformer_poswise_net_hparams(),\n            \'multihead_attention\': {\n                \'name\': \'multihead_attention\',\n                \'num_units\': 512,\n                \'num_heads\': 8,\n                \'dropout_rate\': 0.1,\n                \'output_dim\': 512,\n                \'use_bias\': False,\n            },\n            \'initializer\': None,\n            \'name\': \'transformer_encoder\',\n        }\n\n    def _build(self, inputs, sequence_length, mode=None):\n        """"""Encodes the inputs.\n\n        Args:\n            inputs: A 3D Tensor of shape `[batch_size, max_time, dim]`,\n                containing the embedding of input sequences. Note that\n                the embedding dimension `dim` must equal ""dim"" in\n                :attr:`hparams`. The input embedding is typically an aggregation\n                of word embedding and position embedding.\n            sequence_length: A 1D Tensor of shape `[batch_size]`. Input tokens\n                beyond respective sequence lengths are masked out\n                automatically.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            A Tensor of shape `[batch_size, max_time, dim]` containing the\n            encoded vectors.\n        """"""\n        # Multiply input embedding with the sqrt of its dimension for\n        # normalization\n\n        inputs_padding = 1 - tf.sequence_mask(\n            sequence_length, tf.shape(inputs)[1], dtype=tf.float32)\n        if self._hparams.use_bert_config:\n            ignore_padding = attn.attention_bias_ignore_padding(\n                inputs_padding, bias_value=-1e4)\n        else:\n            ignore_padding = attn.attention_bias_ignore_padding(\n                inputs_padding)\n        encoder_self_attention_bias = ignore_padding\n\n        input_embedding = inputs\n\n        if self._hparams.use_bert_config:\n            x = layers.layer_normalize(input_embedding)\n            x = tf.layers.dropout(x,\n                                  rate=self._hparams.embedding_dropout,\n                                  training=is_train_mode(mode))\n        else:\n            x = tf.layers.dropout(input_embedding,\n                                  rate=self._hparams.embedding_dropout,\n                                  training=is_train_mode(mode))\n\n        # Just to keep consistent with BERT, actually makes no difference\n        if self._hparams.use_bert_config:\n            pad_remover = None\n        else:\n            pad_remover = transformer_utils.PadRemover(inputs_padding)\n\n        for i in range(self._hparams.num_blocks):\n            with tf.variable_scope(""layer_{}"".format(i)):\n                multihead_attention = self.multihead_attention_list[i]\n\n                # trivial difference between BERT and original Transformer\n                if self._hparams.use_bert_config:\n                    _queries_input = x\n                else:\n                    _queries_input = layers.layer_normalize(x)\n\n                attention_output = multihead_attention(\n                    queries=_queries_input,\n                    memory=_queries_input,\n                    memory_attention_bias=encoder_self_attention_bias,\n                    mode=mode,\n                )\n                attention_output = tf.layers.dropout(\n                    attention_output,\n                    rate=self._hparams.residual_dropout,\n                    training=is_train_mode(mode),\n                )\n                x = x + attention_output\n                with tf.variable_scope(\'output\'):\n                    if self._hparams.use_bert_config:\n                        x = layers.layer_normalize(x)\n                        y = x\n                    else:\n                        y = layers.layer_normalize(x)\n\n                poswise_network = self.poswise_networks[i]\n                with tf.variable_scope(poswise_network.variable_scope):\n                    original_shape = shape_list(y)\n                    y = tf.reshape(y, [-1, self._hparams.dim])\n                    if pad_remover:\n                        y = tf.expand_dims(pad_remover.remove(y), axis=0)\n                        # [1, batch_size*seq_length, hidden_dim]\n                    layer_output = poswise_network(y, mode=mode)\n                    sub_output = tf.layers.dropout(\n                        layer_output,\n                        rate=self._hparams.residual_dropout,\n                        training=is_train_mode(mode)\n                    )\n                    if pad_remover:\n                        sub_output = tf.reshape(\n                            pad_remover.restore(tf.squeeze(sub_output, axis=0)),\n                            original_shape)\n                    else:\n                        sub_output = tf.reshape(sub_output, original_shape)\n\n                    x = x + sub_output\n                    if self._hparams.use_bert_config:\n                        x = layers.layer_normalize(x)\n\n        if not self._hparams.use_bert_config:\n            x = layers.layer_normalize(x)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return x\n'"
texar/tf/modules/encoders/xlnet_encoder.py,73,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nXLNet encoders.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.mode import is_train_mode\n\nfrom texar.tf.core.layers import get_initializer, get_layer\nfrom texar.tf.modules.embedders.embedders import WordEmbedder\nfrom texar.tf.modules.encoders.encoder_base import EncoderBase\nfrom texar.tf.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.tf.modules.pretrained.xlnet_utils import \\\n    (PositionWiseFF, RelativePositionalEncoding, RelativeMutiheadAttention)\nfrom texar.tf.utils.utils import dict_fetch\n\n__all__ = [\n    ""XLNetEncoder""\n]\n\n\nclass XLNetEncoder(EncoderBase, PretrainedXLNetMixin):\n    r""""""Raw XLNet module for encoding sequences. Please see\n    :class:`~texar.tf.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.tf.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n        super(XLNetEncoder, self).__init__(hparams=hparams)\n\n        self.load_pretrained_config(pretrained_model_name, cache_dir)\n\n        num_layers = self._hparams.num_layers\n        use_segments = self._hparams.use_segments\n        untie_r = self._hparams.untie_r\n\n        with tf.variable_scope(self.variable_scope):\n\n            if untie_r:\n                self.r_w_bias = tf.get_variable(\'r_w_bias\',\n                                                [num_layers,\n                                                 self._hparams.num_heads,\n                                                 self._hparams.head_dim],\n                                                dtype=tf.float32)\n                self.r_r_bias = tf.get_variable(\'r_r_bias\',\n                                                [num_layers,\n                                                 self._hparams.num_heads,\n                                                 self._hparams.head_dim],\n                                                dtype=tf.float32)\n            else:\n                self.r_w_bias = tf.get_variable(\'r_w_bias\',\n                                                [self._hparams.num_heads,\n                                                 self._hparams.head_dim],\n                                                dtype=tf.float32)\n                self.r_r_bias = tf.get_variable(\'r_r_bias\',\n                                                [self._hparams.num_heads,\n                                                 self._hparams.head_dim],\n                                                dtype=tf.float32)\n\n            if use_segments:\n                self.segment_embed = tf.get_variable(\'seg_embed\',\n                                                     [num_layers, 2,\n                                                      self._hparams.num_heads,\n                                                      self._hparams.head_dim],\n                                                     dtype=tf.float32)\n                self.r_s_bias = (tf.get_variable(\'r_s_bias\',\n                                                 [num_layers,\n                                                  self._hparams.num_heads,\n                                                  self._hparams.head_dim],\n                                                 dtype=tf.float32) if untie_r\n                                 else tf.get_variable(\'r_s_bias\',\n                                                      [self._hparams.num_heads,\n                                                       self._hparams.head_dim],\n                                                      dtype=tf.float32))\n            else:\n                self.segment_embed = None\n                self.r_s_bias = None\n\n            # Word embedding\n            self.word_embedder = WordEmbedder(\n                vocab_size=self._hparams.vocab_size,\n                hparams={""dim"": self._hparams.hidden_dim})\n\n            # Position embedding\n            self.pos_embed = RelativePositionalEncoding(hparams={\n                ""dim"": self._hparams.hidden_dim,\n                ""max_seq_len"": self._hparams.max_seq_len\n            })\n\n            self.attn_layers = []\n            self.ff_layers = []\n            rel_attn_hparams = dict_fetch(\n                self._hparams, RelativeMutiheadAttention.default_hparams())\n            rel_attn_hparams[""name""] = ""rel_attn""\n\n            ff_hparams = dict_fetch(\n                self._hparams, PositionWiseFF.default_hparams())\n            ff_hparams[""name""] = ""ff""\n\n            for i in range(num_layers):\n                with tf.variable_scope(""layer_{}"".format(i)):\n                    if self._hparams.untie_r:\n                        if use_segments:\n                            self.attn_layers.append(RelativeMutiheadAttention(\n                                self.r_r_bias[i], self.r_w_bias[i],\n                                self.r_s_bias[i],\n                                self.segment_embed[i],\n                                hparams=rel_attn_hparams))\n                        else:\n                            self.attn_layers.append(RelativeMutiheadAttention(\n                                self.r_r_bias[i], self.r_w_bias[i],\n                                hparams=rel_attn_hparams))\n                    else:\n                        if use_segments:\n                            self.attn_layers.append(RelativeMutiheadAttention(\n                                self.r_r_bias, self.r_w_bias,\n                                self.r_s_bias,\n                                self.segment_embed[i],\n                                hparams=rel_attn_hparams))\n                        else:\n                            self.attn_layers.append(RelativeMutiheadAttention(\n                                self.r_r_bias, self.r_w_bias,\n                                hparams=rel_attn_hparams))\n                    self.ff_layers.append(PositionWiseFF(hparams=ff_hparams))\n\n            dropout_hparams = {\n                ""type"": ""Dropout"",\n                ""kwargs"": {\n                    ""rate"": self._hparams.dropout\n                }\n            }\n            self.dropout = get_layer(hparams=dropout_hparams)\n\n            self.mask_embed = tf.get_variable(\n                \'mask_emb\', [1, 1, self.hparams.hidden_dim], dtype=tf.float32)\n\n    def reset_parameters(self):\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    get_initializer(self._hparams.initializer))\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        * The encoder arch is determined by the constructor argument\n          :attr:`pretrained_model_name` if it\'s specified. In this case,\n          `hparams` are ignored.\n        * Otherwise, the encoder arch is determined by\n          `hparams[\'pretrained_model_name\']` if it\'s specified. All other\n          configurations in `hparams` are ignored.\n        * If the above two are `None`, the encoder arch is defined by\n          the configurations in `hparams` and weights are randomly initialized.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""xlnet_encoder"",\n                ""pretrained_model_name"": ""xlnet-base-cased"",\n                ""untie_r"": True,\n                ""num_layers"": 12,\n                ""mem_len"": 0,\n                ""reuse_len"": 0,\n                ""initializer"": None,\n                ""num_heads"": 12,\n                ""hidden_dim"": 768,\n                ""head_dim"": 64,\n                ""dropout"": 0.1,\n                ""attention_dropout"": 0.1,\n                ""use_segments"": True,\n                ""ffn_inner_dim"": 3072,\n                ""activation"": \'gelu\',\n                ""vocab_size"": 32000,\n                ""max_seq_len"": 512,\n            }\n\n\n\n        Here:\n\n        The default parameters are values for cased XLNet-Base model.\n\n\n        `""pretrained_model_name""`: str or None\n             The name of the pre-trained bert model. If None, the model\n             will be randomly initialized.\n\n        `""untie_r""`: bool\n            Boolean value to indicate if biases should be untied for all the\n            layers\n\n        `""num_layers""`: int\n            Number of layers in the network\n\n        `""mem_len""`: int\n            Length of the memory to be used during attention score calculation.\n\n        `""reuse_len""`: int\n            Length of the memory that can be re-used\n\n        `""initializer""`: dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.tf.core.get_initializer` for details.\n\n        `""num_heads""`: int\n            Number of heads in the attention\n\n        `""hidden_dim""`: int\n            Hidden dimension of the embeddings\n\n        `""head_dim""`: int\n            Size of the vectors after head projection.\n\n        `""dropout""`: float\n            Dropout rate for layers\n\n        `""attention_dropout""`: float\n            Dropout rate for attention layers\n\n        `""use_segments""`: bool\n            Boolean to indicate if the input has segments\n\n        `""ffn_inner_dim""`: int\n            Dimension of PositionWise FF network\'s hidden layer\n\n        `""activation""`: str or callable\n            Activation function applied to the output of the PositionWise FF.\n            See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n        `""vocab_size""`: int\n            The vocabulary size of `inputs` in `XLNet`.\n\n        `""max_seq_len""`: int\n            Maximum len of the sequence allowed in one segment\n\n        `""name""`: str\n            Name of the module.\n        """"""\n\n        return {\n            ""name"": ""xlnet_encoder"",\n            \'pretrained_model_name\': \'xlnet-base-cased\',\n            ""untie_r"": True,\n            ""num_layers"": 12,\n            ""mem_len"": 0,\n            ""reuse_len"": 0,\n            # initializer\n            ""initializer"": None,\n            # layer\n            ""num_heads"": 12,\n            ""hidden_dim"": 768,\n            ""head_dim"": 64,\n            ""dropout"": 0.1,\n            ""attention_dropout"": 0.1,\n            ""use_segments"": True,\n            # ffn\n            ""ffn_inner_dim"": 3072,\n            ""activation"": \'gelu\',\n            # embedding\n            ""vocab_size"": 32000,\n            ""max_seq_len"": 512,\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    def param_groups(self, lr=None, lr_layer_scale=1.0,\n                     decay_base_params=False):\n        r""""""Create parameter groups for optimizers. When\n        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form\n        separate groups with different base learning rates.\n\n        This method should be called before applying gradients to the variables\n        through the optimizer. Particularly, after calling the optimizer\'s\n        `compute_gradients` method, the user can call this method to get\n        variable-specific learning rates for the network. The gradients for each\n        variables can then be scaled accordingly. These scaled gradients are\n        finally applied by calling optimizer\'s `apply_gradients` method.\n\n        Example:\n\n            .. code-block:: python\n\n                grads_and_vars = optimizer.compute_gradients(loss)\n\n                vars_to_grads = {key: value for key, value in grads_and_vars}\n\n                vars_to_learning_rates = xlnet_encoder.param_groups(\n                                                        lr=1,\n                                                        ly_layer_scale=0.75)\n\n                for key in vars_to_grads.keys():\n                    vars_to_grads[key] *= vars_to_learning_rates[key]\n\n                train_op = optimizer.apply_gradients(zip(\n                    *vars_to_grads.items()))\n\n\n        Args:\n            lr (float): The learning rate. Can be omitted if\n                :attr:`lr_layer_decay_rate` is 1.0.\n            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer\n                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.\n            decay_base_params (bool): If `True`, treat non-layer parameters\n                (e.g. embeddings) as if they\'re in layer 0. If `False`, these\n                parameters are not scaled.\n\n        Returns: A dict mapping tensorflow variables to their learning rates.\n        """"""\n        vars_to_learning_rates = {}\n        if lr_layer_scale != 1.0:\n            if lr is None:\n                raise ValueError(\n                    ""lr must be specified when lr_layer_decay_rate is not 1.0"")\n\n            num_layers = self._hparams.num_layers\n            scope = self.variable_scope.name\n            base_var_names = [\'r_w_bias\', \'r_r_bias\', \'word_embedder\']\n\n            if self._hparams.use_segments:\n                base_var_names.extend([\'r_s_bias\', \'seg_embed\'])\n\n            for var in base_var_names:\n                tf_variable = tf.trainable_variables(scope=scope + ""/"" + var)[0]\n                vars_to_learning_rates[tf_variable] = \\\n                    lr * (lr_layer_scale ** num_layers if decay_base_params\n                          else 1.0)\n\n            for idx in range(num_layers):\n                decay_rate = lr_layer_scale ** (num_layers - idx - 1)\n                layer_variables = tf.trainable_variables(\n                    scope=scope + ""/"" + ""layer_{}"".format(idx))\n                for variable in layer_variables:\n                    vars_to_learning_rates[variable] = lr * decay_rate\n        else:\n            for variable in self.trainable_variables:\n                vars_to_learning_rates[variable] = lr\n\n        return vars_to_learning_rates\n\n    @property\n    def output_size(self):\n        r""""""The last dimension of the encoder output.\n\n        Note: The :meth:`_build` returns two tensors of shapes\n        `[batch_size, max_time, hidden_dim]` and\n        `[batch_size, cache_len, hidden_dim]`. `output_size` here equals\n        `hidden_dim`\n        """"""\n        return self._hparams.hidden_dim\n\n    @staticmethod\n    def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None):\n        r""""""Cache hidden states into memory.""""""\n        assert mem_len > 0\n\n        if reuse_len is not None and reuse_len > 0:\n            curr_out = curr_out[:reuse_len]\n\n        if prev_mem is None:\n            new_mem = curr_out[-mem_len:]\n        else:\n            new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]\n\n        return tf.stop_gradient(new_mem)\n\n    def _create_mask(self, qlen, mlen, dtype=tf.float32, same_length=False):\n        r""""""Create causal attention mask.""""""\n        attn_mask = tf.ones([qlen, qlen], dtype=dtype)\n        mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n        mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n        attn_mask_pad = tf.zeros([qlen, mlen], dtype=dtype)\n        ret = tf.concat([attn_mask_pad, mask_u - mask_dia], axis=1)\n        if same_length:\n            mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n            ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]],\n                            axis=1)\n\n        return ret\n\n    def _build(self, token_ids, segment_ids=None, input_mask=None,\n               memory=None, permute_mask=None, target_mapping=None,\n               bi_data=False, clamp_len=None, cache_len=0, same_length=False,\n               attn_type=\'bi\', two_stream=False, mode=None):\n        r""""""Compute XLNet representations for the input.\n\n        Args:\n            token_ids: Shape `[batch_size, max_time]`.\n            segment_ids: Shape `[batch_size, max_time]`.\n            input_mask: Float tensor of shape `[batch_size, max_time]`. Note that\n                positions with value 1 are masked out.\n            memory: Memory from previous batches. A list of length `num_layers`,\n                each tensor of shape `[batch_size, mem_len, hidden_dim]`.\n            permute_mask: The permutation mask. Float tensor of shape\n                `[batch_size, max_time, max_time]`.\n                A value of 0 for ``permute_mask[i, j, k]`` indicates that\n                position `i` attends to position `j` in batch `k`.\n            target_mapping: The target token mapping. Float tensor of shape\n                `[batch_size, num_targets, max_time]`.\n                A value of 1 for ``target_mapping[i, j, k]`` indicates that\n                the `i`-th target token (in order of permutation) in batch `k`\n                is the token at position `j`.\n                Each row ``target_mapping[i, :, k]`` can have no more than one\n                value of 1.\n            bi_data (bool): Whether to use bidirectional data input pipeline.\n            clamp_len (int): Clamp all relative distances larger than\n                :attr:`clamp_len`. A value of -1 means no clamping.\n            cache_len (int): Length of memory (number of tokens) to cache.\n            same_length (bool): Whether to use the same attention length for\n                each token.\n            attn_type (str): Attention type. Supported values are `""uni""`\n                and `""bi""`.\n            two_stream (bool): Whether to use two-stream attention. Only set to\n                `True` when pre-training or generating text. Defaults to\n                `False`.\n\n        Returns: A tuple of `(output, new_memory)`:\n\n            - **output**: The final layer output representations. Shape\n              `[batch_size, max_time, hidden_dim]`.\n            - **new_memory**: The memory of the current batch.\n              If `cache_len` is 0, then `new_memory` is `None`. Otherwise, it is\n              a list of length `num_layers`, each tensor of shape\n              `[batch_size, cache_len, hidden_dim]`.\n              This can be used as the :attr:`memory` argument in the next batch.\n        """"""\n        return self._execute(self.word_embedder(token_ids),\n                             segment_ids=segment_ids, input_mask=input_mask,\n                             memory=memory, permute_mask=permute_mask,\n                             target_mapping=target_mapping, bi_data=bi_data,\n                             clamp_len=clamp_len, cache_len=cache_len,\n                             same_length=same_length, attn_type=attn_type,\n                             two_stream=two_stream, mode=mode)\n\n    def _execute(self, word_embed, segment_ids=None,  # noqa: C901\n                 input_mask=None, memory=None, permute_mask=None,\n                 target_mapping=None, bi_data=False, clamp_len=None,\n                 cache_len=0, same_length=False, attn_type=\'bi\',\n                 two_stream=False, mode=None):\n        r""""""Compute XLNet representations for the input. This layer exists\n        because :class:`XLNetDecoder` compute embeddings in the decoder helper.\n        `word_embed` has shape `[batch_size, max_time, word_embed_dim]`.\n        Please refer to :meth:`_build` for the detailed information of other\n        arguments.\n        """"""\n        # word_embed: [max_time, batch_size, word_embed_dim]\n        word_embed = tf.transpose(word_embed, perm=[1, 0, 2])\n        # segment_ids: [max_time, batch_size]\n        if segment_ids is not None:\n            segment_ids = tf.transpose(segment_ids, perm=[1, 0])\n        # input_mask: [max_time, batch_size]\n        if input_mask is not None:\n            input_mask = tf.transpose(input_mask, perm=[1, 0])\n        # memory: A list of length num_layers\n        # each tensor of shape [mem_len, batch_size, hidden_dim]\n        if memory is not None:\n            memory = [tf.transpose(m, perm=[1, 0, 2]) for m in memory]\n        # permute_mask: [max_time, max_time, batch_size]\n        if permute_mask is not None:\n            permute_mask = tf.transpose(permute_mask, perm=[1, 2, 0])\n        # target_mapping: [num_targets, max_time, batch_size]\n        if target_mapping is not None:\n            target_mapping = tf.transpose(target_mapping, perm=[1, 2, 0])\n\n        max_time = tf.shape(word_embed)[0]\n        batch_size = tf.shape(word_embed)[1]\n        mem_len = tf.shape(memory[0])[0] if memory is not None else 0\n        tot_len = max_time + mem_len\n        reuse_len = self._hparams.reuse_len\n        is_training = is_train_mode(mode)\n\n        # Attention mask\n        # causal attention mask\n        if attn_type == \'uni\':\n            attn_mask = self._create_mask(max_time, mem_len, tf.float32,\n                                          same_length)\n            attn_mask = attn_mask[:, :, None, None]\n        elif attn_type == \'bi\':\n            attn_mask = None\n        else:\n            raise ValueError(\'Unsupported attention type: {}\'.format(attn_type))\n\n        # data mask: input mask & perm mask\n        if input_mask is not None and permute_mask is not None:\n            data_mask = input_mask[None] + permute_mask\n        elif input_mask is not None and permute_mask is None:\n            data_mask = input_mask[None]\n        elif input_mask is None and permute_mask is not None:\n            data_mask = permute_mask\n        else:\n            data_mask = None\n\n        if data_mask is not None:\n            # all mems can be attended to\n            mems_mask = tf.zeros([tf.shape(data_mask)[0], mem_len, batch_size],\n                                 dtype=tf.float32)\n            data_mask = tf.concat([mems_mask, data_mask], 1)\n            if attn_mask is None:\n                attn_mask = data_mask[:, :, :, None]\n            else:\n                attn_mask += data_mask[:, :, :, None]\n\n        if attn_mask is not None:\n            attn_mask = tf.cast(attn_mask > 0, dtype=tf.float32)\n\n        if attn_mask is not None:\n            non_tgt_mask = -tf.eye(max_time, dtype=tf.float32)\n            non_tgt_mask = tf.concat([tf.zeros([max_time, mem_len],\n                                               dtype=tf.float32),\n                                      non_tgt_mask], axis=-1)\n            non_tgt_mask = tf.cast(\n                (attn_mask + non_tgt_mask[:, :, None, None]) > 0,\n                dtype=tf.float32)\n        else:\n            non_tgt_mask = None\n\n        # Segment embedding\n        if segment_ids is not None:\n            mem_pad = tf.zeros([mem_len, batch_size], dtype=tf.int32)\n            cat_ids = tf.concat([mem_pad, segment_ids], 0)\n            segment_matrix = tf.cast(\n                tf.logical_not(\n                    tf.equal(segment_ids[:, None], cat_ids[None, :])),\n                tf.int32)\n            segment_matrix = tf.one_hot(segment_matrix, 2, dtype=tf.float32)\n        else:\n            segment_matrix = None\n\n        # Position embedding\n        pos_embed = self.pos_embed(\n            batch_size, max_time, tot_len, clamp_len, attn_type, bi_data)\n        pos_embed = self.dropout(pos_embed,\n                                 training=is_training)\n\n        states_h = self.dropout(word_embed,\n                                training=is_training)\n\n        if two_stream:\n            if target_mapping is not None:\n                word_embed_q = tf.tile(\n                    self.mask_embed, [tf.shape(target_mapping)[0],\n                                      batch_size, 1])\n            else:\n                word_embed_q = word_embed\n            states_g = self.dropout(word_embed_q)\n        else:\n            states_g = None\n\n        new_memory = []\n        num_layers = self._hparams.num_layers\n        for i in range(num_layers):\n            cur_memory = memory[i] if memory is not None else None\n            if cache_len > 0:\n                new_memory.append(\n                    self._cache_mem(states_h, cur_memory, cache_len, reuse_len))\n            states_h, states_g = self.attn_layers[i](\n                states_h=states_h, pos_embed=pos_embed, states_g=states_g,\n                segment_mat=segment_matrix, attn_mask_h=non_tgt_mask,\n                attn_mask_g=attn_mask, target_mapping=None, memory=cur_memory,\n                mode=mode)\n            ff_layer = self.ff_layers[i]\n            states_h = ff_layer(states_h, mode=mode)\n\n            if states_g is not None:\n                states_g = ff_layer(states_g, mode=mode)\n\n        output = self.dropout(states_h if states_g is None else states_g,\n                              training=is_training)\n\n        # Now output: [max_time, batch_size, hidden_dim]\n        # new_memory: None or A list of length num_layers,\n        # each tensor of shape [cache_len, batch_size, hidden_dim]\n        output = tf.transpose(output, perm=[1, 0, 2])\n        if new_memory is not None:\n            new_memory = [tf.transpose(m, perm=[1, 0, 2]) for m in new_memory]\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n            if self.pretrained_model_dir:\n                self.init_pretrained_weights(self.variable_scope.name)\n\n        if cache_len == 0:\n            return output, None\n\n        return output, new_memory\n'"
texar/tf/modules/memory/__init__.py,2,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nMemory modules.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.memory.memory_network import *\nfrom texar.tf.modules.memory.embed_fns import *\n'"
texar/tf/modules/memory/embed_fns.py,6,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nSome embed_fn s used in :class:`~texar.tf.modules.memory.MemNetBase` and its\nsubclasses.\n""""""\n\n# pylint: disable=invalid-name, too-many-arguments\n\n__all__ = [\n    \'default_memnet_embed_fn_hparams\',\n]\n\n\ndef default_memnet_embed_fn_hparams():\n    """"""Returns a dictionary of hyperparameters with default hparams for\n    :func:`~texar.tf.modules.memory.default_embed_fn`\n\n    .. code-block:: python\n\n        {\n            ""embedding"": {\n                ""dim"": 100\n            },\n            ""temporal_embedding"": {\n                ""dim"": 100\n            },\n            ""combine_mode"": ""add""\n        }\n\n    Here:\n\n    ""embedding"": dict, optional\n        Hyperparameters for embedding operations. See\n        :meth:`~texar.tf.modules.WordEmbedder.default_hparams` of\n        :class:`~texar.tf.modules.WordEmbedder` for details. If `None`, the\n        default hyperparameters are used.\n\n    ""temporal_embedding"": dict, optional\n        Hyperparameters for temporal embedding operations. See\n        :meth:`~texar.tf.modules.PositionEmbedder.default_hparams` of\n        :class:`~texar.tf.modules.PositionEmbedder` for details. If `None`, the\n        default hyperparameters are used.\n\n    ""combine_mode"": str\n        Either **\'add\'** or **\'concat\'**. If \'add\', memory\n        embedding and temporal embedding are added up. In this case the two\n        embedders must have the same dimension. If \'concat\', the two\n        embeddings are concated.\n    """"""\n    return {\n        ""embedding"": {\n            ""dim"": 100\n        },\n        ""temporal_embedding"": {\n            ""dim"": 100\n        },\n        ""combine_mode"": ""add""\n    }\n'"
texar/tf/modules/memory/memory_network.py,42,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nEnd-to-end memory network described in\n(Sukhbaatar et al.) End-To-End Memory Networks\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.modules.embedders import WordEmbedder, PositionEmbedder\nfrom texar.tf.utils.mode import switch_dropout\nfrom texar.tf.modules.memory.embed_fns import default_memnet_embed_fn_hparams\n\n# pylint: disable=invalid-name, too-many-instance-attributes, too-many-arguments\n# pylint: disable=too-many-locals\n\n__all__ = [\n    \'MemNetBase\',\n    \'MemNetRNNLike\',\n]\n\n\nclass MemNetSingleLayer(ModuleBase):\n    """"""An A-C layer for memory network.\n\n    Args:\n        H (optional): The matrix :attr:`H` multiplied to :attr:`o` at the end.\n        hparams (dict or HParams, optional): Memory network single layer\n            hyperparameters. If it is not specified, the default hyperparameter\n            setting is used. See :attr:`default_hparams` for the structure and\n            default values.\n    """"""\n\n    def __init__(self, H=None, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        self._H = H\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""memnet_single_layer""\n            }\n\n        Here:\n\n        ""name"": str\n            Name of the memory network single layer.\n        """"""\n        return {\n            ""name"": ""memnet_single_layer""\n        }\n\n    def _build(self, u, m, c, **kwargs):\n        """"""An A-C operation with memory and query vector.\n\n        Args:\n            u (Tensor): The input query `Tensor` of shape `[None, memory_dim]`.\n            m (Tensor): Output of A operation. Should be in shape\n                `[None, memory_size, memory_dim]`.\n            c (Tensor): Output of C operation. Should be in shape\n                `[None, memory_size, memory_dim]`.\n\n        Returns:\n            A `Tensor` of shape same as :attr:`u`.\n        """"""\n        # Input memory representation\n        p = tf.matmul(m, tf.expand_dims(u, axis=2))\n        p = tf.transpose(p, perm=[0, 2, 1])\n\n        p = tf.nn.softmax(p)  # equ. (1)\n\n        # Output memory representation\n        o = tf.matmul(p, c)  # equ. (2)\n        o = tf.squeeze(o, axis=[1])\n\n        if self._H:\n            u = tf.matmul(u, self._H)  # RNN-like style\n        u_ = tf.add(u, o)  # u^{k+1} = H u^k + o^k\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            if self._H:\n                self._add_trainable_variable(self._H)\n            self._built = True\n\n        return u_\n\n\nclass MemNetBase(ModuleBase):\n    """"""Base class inherited by all memory network classes.\n\n    Args:\n        raw_memory_dim (int): Dimension size of raw memory entries\n            (before embedding). For example,\n            if a raw memory entry is a word, this is the **vocabulary size**\n            (imagine a one-hot representation of word). If a raw memory entry\n            is a dense vector, this is the dimension size of the vector.\n        input_embed_fn (optional): A callable that embeds raw memory entries\n            as inputs.\n            This corresponds to the `A` embedding operation in\n            (Sukhbaatar et al.)\n            If not provided, a default embedding operation is created as\n            specified in :attr:`hparams`. See\n            :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`\n            for details.\n        output_embed_fn (optional): A callable that embeds raw memory entries\n            as outputs.\n            This corresponds to the `C` embedding operation in\n            (Sukhbaatar et al.)\n            If not provided, a default embedding operation is created as\n            specified in :attr:`hparams`. See\n            :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`\n            for details.\n        query_embed_fn (optional): A callable that embeds query.\n            This corresponds to the `B` embedding operation in\n            (Sukhbaatar et al.). If not provided and ""use_B"" is True\n            in :attr:`hparams`, a default embedding operation is created as\n            specified in :attr:`hparams`. See\n            :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`\n            for details.\n            Notice: If you\'d like to customize this callable, please follow\n            the same number and style of dimensions as in `input_embed_fn` or\n            `output_embed_fn`, and assume that the 2nd dimension of its\n            input and output (which corresponds to `memory_size`) is 1.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n\n    def __init__(self,\n                 raw_memory_dim,\n                 input_embed_fn=None,\n                 output_embed_fn=None,\n                 query_embed_fn=None,\n                 hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        self._raw_memory_dim = raw_memory_dim\n\n        self._n_hops = self._hparams.n_hops\n        self._relu_dim = self._hparams.relu_dim\n        self._memory_size = self._hparams.memory_size\n\n        with tf.variable_scope(self.variable_scope):\n            self._A, self._C, self._B, self._memory_dim = self._build_embed_fn(\n                input_embed_fn, output_embed_fn, query_embed_fn)\n\n            self.H = None\n            if self.hparams.use_H:\n                self.H = tf.get_variable(\n                    name=""H"", shape=[self._memory_dim, self._memory_dim])\n\n    def _build_embed_fn(self, input_embed_fn, output_embed_fn, query_embed_fn):\n        # Optionally creates embed_fn\'s\n        memory_dim = self.hparams.memory_dim\n        mdim_A, mdim_C, mdim_B = None, None, None\n\n        A = input_embed_fn\n        if input_embed_fn is None:\n            A, mdim_A = self.get_default_embed_fn(\n                self._memory_size, self._hparams.A)\n            memory_dim = mdim_A\n\n        C = output_embed_fn\n        if output_embed_fn is None:\n            C, mdim_C = self.get_default_embed_fn(\n                self._memory_size, self._hparams.C)\n            if mdim_A is not None and mdim_A != mdim_C:\n                raise ValueError(\'Embedding config `A` and `C` must have \'\n                                 \'the same output dimension.\')\n            memory_dim = mdim_C\n\n        B = query_embed_fn\n        if query_embed_fn is None and self._hparams.use_B:\n            B, mdim_B = self.get_default_embed_fn(1, self._hparams.B)\n            if mdim_A is not None and mdim_A != mdim_B:\n                raise ValueError(\'Embedding config `A` and `B` must have \'\n                                 \'the same output dimension.\')\n            if mdim_C is not None and mdim_C != mdim_B:\n                raise ValueError(\'Embedding config `C` and `B` must have \'\n                                 \'the same output dimension.\')\n            memory_dim = mdim_B\n\n        return A, C, B, memory_dim\n\n    def get_default_embed_fn(self, memory_size, embed_fn_hparams):\n        """"""Creates a default embedding function. Can be used for A, C, or B\n        operation.\n\n        For B operation (i.e., query_embed_fn), :attr:`memory_size` must be 1.\n\n        The function is a combination of both memory embedding and temporal\n        embedding, with the combination method specified by ""combine_mode"" in\n        the `embed_fn_hparams`.\n\n        .. role:: python(code)\n           :language: python\n\n        Args:\n            embed_fn_hparams (dict or HParams): Hyperparameter of the\n                embedding function. See\n                :func:`~texar.tf.modules.default_memnet_embed_fn` for details.\n\n        Returns:\n            A tuple `(embed_fn, memory_dim)`, where\n\n            - **`memory_dim`** is the dimension of memory entry embedding, \\\n            inferred from :attr:`embed_fn_hparams`.\n\n                - If `combine_mode` == \'add\', `memory_dim` is the \\\n                embedder dimension.\n                - If `combine_mode` == \'concat\', `memory_dim` is the sum \\\n                of the memory embedder dimension and the temporal embedder \\\n                dimension.\n\n            - **`embed_fn`** is an embedding function that takes in memory \\\n            and returns memory embedding. \\\n            Specifically, the function has signature \\\n            :python:`memory_embedding= embed_fn(memory=None, soft_memory=None)`\\\n            where one of `memory` and `soft_memory` is provided (but not both).\n\n            Args:\n                memory: An `int` Tensor of shape\n                    `[batch_size, memory_size]`\n                    containing memory indexes used for embedding lookup.\n                soft_memory: A Tensor of shape\n                    `[batch_size, memory_size, raw_memory_dim]`\n                    containing soft weights used to mix the embedding vectors.\n\n            Returns:\n                A Tensor of shape `[batch_size, memory_size, memory_dim]`\n                containing the memory entry embeddings.\n\n        """"""\n        # memory embedder\n        embedder = WordEmbedder(\n            vocab_size=self._raw_memory_dim,\n            hparams=embed_fn_hparams[""embedding""]\n        )\n        # temporal embedder\n        temporal_embedder = PositionEmbedder(\n            position_size=memory_size,\n            hparams=embed_fn_hparams[""temporal_embedding""]\n        )\n\n        combine = embed_fn_hparams[\'combine_mode\']\n        if combine == \'add\':\n            if embedder.dim != temporal_embedder.dim:\n                raise ValueError(\'`embedding` and `temporal_embedding` must \'\n                                 \'have the same dimension for ""add"" \'\n                                 \'combination.\')\n            memory_dim = embedder.dim\n        elif combine == \'concat\':\n            memory_dim = embedder.dim + temporal_embedder.dim\n\n        def _embed_fn(memory, soft_memory, mode=None):\n            if memory is None and soft_memory is None:\n                raise ValueError(\n                    ""Either `memory` or `soft_memory` is required."")\n            if memory is not None and soft_memory is not None:\n                raise ValueError(\n                    ""Must not specify `memory` and `soft_memory` at the ""\n                    ""same time."")\n\n            embedded_memory = embedder(\n                ids=memory, soft_ids=soft_memory, mode=mode)\n            temporal_embedded = temporal_embedder(\n                sequence_length=tf.constant([memory_size]), mode=mode)\n            temporal_embedded = tf.tile(\n                temporal_embedded, [tf.shape(embedded_memory)[0], 1, 1])\n\n            if combine == \'add\':\n                return tf.add(embedded_memory, temporal_embedded)\n            elif combine == \'concat\':\n                return tf.concat([embedded_memory, temporal_embedded], axis=-1)\n            else:\n                raise ValueError(\'Unknown combine method: {}\'.format(combine))\n\n        return _embed_fn, memory_dim\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""n_hops"": 1,\n                ""memory_dim"": 100,\n                ""relu_dim"": 50,\n                ""memory_size"": 100,\n                ""A"": default_embed_fn_hparams,\n                ""C"": default_embed_fn_hparams,\n                ""B"": default_embed_fn_hparams,\n                ""use_B"": False,\n                ""use_H"": False,\n                ""dropout_rate"": 0,\n                ""variational"": False,\n                ""name"": ""memnet"",\n            }\n\n        Here:\n\n        ""n_hops"": int\n            Number of hops.\n\n        ""memory_dim"": int\n            Memory dimension, i.e., the dimension size of a memory entry\n            embedding. Ignored if at least one of the embedding functions is\n            created according to :attr:`hparams`. In this case\n            :attr:`memory_dim` is inferred from the created embed_fn.\n\n        ""relu_dim"": int\n            Number of elements in :attr:`memory_dim` that have relu at the end\n            of each hop.\n            Should be not less than 0 and not more than :attr`memory_dim`.\n\n        ""memory_size"": int\n            Number of entries in memory.\n\n            For example, the number of sentences {x_i} in Fig.1(a) of\n            (Sukhbaatar et al.) End-To-End Memory Networks.\n\n        ""use_B"": bool\n            Whether to create the query embedding function. Ignored if\n            `query_embed_fn` is given to the constructor.\n\n        ""use_H"": bool\n            Whether to perform a linear transformation with matrix `H` at\n            the end of each A-C layer.\n\n        ""dropout_rate"": float\n            The dropout rate to apply to the output of each hop. Should\n            be between 0 and 1.\n            E.g., `dropout_rate=0.1` would drop out 10% of the units.\n\n        ""variational"": bool\n            Whether to share dropout masks after each hop.\n        """"""\n        return {\n            ""n_hops"": 1,\n            ""memory_dim"": 100,\n            ""relu_dim"": 50,\n            ""memory_size"": 100,\n            ""A"": default_memnet_embed_fn_hparams(),\n            ""C"": default_memnet_embed_fn_hparams(),\n            ""B"": default_memnet_embed_fn_hparams(),\n            ""use_B"": False,\n            ""use_H"": False,\n            ""dropout_rate"": 0,\n            ""variational"": False,\n            ""name"": ""memnet"",\n        }\n\n    def _build(self, memory, query, **kwargs):\n        raise NotImplementedError\n\n    @property\n    def memory_size(self):\n        """"""The memory size.\n        """"""\n        return self._memory_size\n\n    @property\n    def raw_memory_dim(self):\n        """"""The dimension of memory element (or vocabulary size).\n        """"""\n        return self._raw_memory_dim\n\n    @property\n    def memory_dim(self):\n        """"""The dimension of embedded memory and all vectors in hops.\n        """"""\n        return self._memory_dim\n\n\nclass MemNetRNNLike(MemNetBase):\n    """"""An implementation of multi-layer end-to-end memory network,\n    with RNN-like weight tying described in\n    (Sukhbaatar et al.) End-To-End Memory Networks .\n\n    See :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn` for default\n    embedding functions. Customized embedding functions must follow\n    the same signature.\n\n    Args:\n        raw_memory_dim (int): Dimension size of raw memory entries\n            (before embedding). For example,\n            if a raw memory entry is a word, this is the **vocabulary size**\n            (imagine a one-hot representation of word). If a raw memory entry\n            is a dense vector, this is the dimension size of the vector.\n        input_embed_fn (optional): A callable that embeds raw memory entries\n            as inputs.\n            This corresponds to the `A` embedding operation in\n            (Sukhbaatar et al.)\n            If not provided, a default embedding operation is created as\n            specified in :attr:`hparams`. See\n            :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`\n            for details.\n        output_embed_fn (optional): A callable that embeds raw memory entries\n            as outputs.\n            This corresponds to the `C` embedding operation in\n            (Sukhbaatar et al.)\n            If not provided, a default embedding operation is created as\n            specified in :attr:`hparams`. See\n            :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`\n            for details.\n        query_embed_fn (optional): A callable that embeds query.\n            This corresponds to the `B` embedding operation in\n            (Sukhbaatar et al.). If not provided and ""use_B"" is True\n            in :attr:`hparams`, a default embedding operation is created as\n            specified in :attr:`hparams`. See\n            :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`\n            for details.\n            For customized query_embed_fn, note that the function must follow\n            the signature of the default embed_fn where `memory_size` must\n            be 1.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n\n    def __init__(self,\n                 raw_memory_dim,\n                 input_embed_fn=None,\n                 output_embed_fn=None,\n                 query_embed_fn=None,\n                 hparams=None):\n        MemNetBase.__init__(self, raw_memory_dim, input_embed_fn,\n                            output_embed_fn, query_embed_fn, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            self._AC = MemNetSingleLayer(\n                self.H,\n                hparams={""name"": ""AC""})\n\n            self._W = tf.layers.Dense(\n                units=raw_memory_dim,\n                use_bias=False,\n                name=""W"")\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""n_hops"": 1,\n                ""memory_dim"": 100,\n                ""relu_dim"": 50,\n                ""memory_size"": 100,\n                ""A"": default_embed_fn_hparams,\n                ""C"": default_embed_fn_hparams,\n                ""B"": default_embed_fn_hparams,\n                ""use_B"": False,\n                ""use_H"": True,\n                ""dropout_rate"": 0,\n                ""variational"": False,\n                ""name"": ""memnet_rnnlike"",\n            }\n\n        Here:\n\n        ""n_hops"": int\n            Number of hops.\n\n        ""memory_dim"": int\n            Memory dimension, i.e., the dimension size of a memory entry\n            embedding. Ignored if at least one of the embedding functions is\n            created according to :attr:`hparams`. In this case\n            :attr:`memory_dim` is inferred from the created embed_fn.\n\n        ""relu_dim"": int\n            Number of elements in :attr:`memory_dim` that have relu at the end\n            of each hop.\n            Should be not less than 0 and not more than :attr`memory_dim`.\n\n        ""memory_size"": int\n            Number of entries in memory.\n\n            For example, the number of sentences {x_i} in Fig.1(a) of\n            (Sukhbaatar et al.) End-To-End Memory Networks.\n\n        ""use_B"": bool\n            Whether to create the query embedding function. Ignored if\n            `query_embed_fn` is given to the constructor.\n\n        ""use_H"": bool\n            Whether to perform a linear transformation with matrix `H` at\n            the end of each A-C layer.\n\n        ""dropout_rate"": float\n            The dropout rate to apply to the output of each hop. Should\n            be between 0 and 1.\n            E.g., `dropout_rate=0.1` would drop out 10% of the units.\n\n        ""variational"": bool\n            Whether to share dropout masks after each hop.\n        """"""\n        hparams = MemNetBase.default_hparams()\n        hparams.update({\n            ""use_H"": True,\n            ""name"": ""memnet_rnnlike""\n        })\n        return hparams\n\n    def _build(self, memory=None, query=None, soft_memory=None, soft_query=None,\n               mode=None, **kwargs):\n        """"""Pass the :attr:`memory` and :attr:`query` through the memory network\n        and return the :attr:`logits` after the final matrix.\n\n        Only one of :attr:`memory` and :attr:`soft_memory` can be specified.\n        They should not be specified at the same time.\n\n        Args:\n            memory (optional): Memory used in A/C operations. By default, it\n                should be an integer tensor of shape\n                `[batch_size, memory_size]`,\n                containing the ids to embed if provided.\n            query (optional): Query vectors as the intial input of the memory\n                network.\n                If you\'d like to apply some transformation (e.g., embedding)\n                on it before it\'s fed into the network, please set `use_B` to\n                True and add `query_embed_fn` when constructing this instance.\n                If `query_embed_fn` is set to\n                :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`,\n                it should be of shape `[batch_size]`.\n                If `use_B` is not set, it should be of shape\n                `[batch_size, memory_dim]`.\n            soft_memory (optional): Soft memory used in A/C operations. By\n                default, it should be a tensor of shape\n                `[batch_size, memory_size, raw_memory_dim]`,\n                containing the weights used to mix the embedding vectors.\n                If you\'d like to apply a matrix multiplication on the memory,\n                this option can also be used.\n            soft_query (optional): Query vectors as the intial input of the\n                memory network.\n                If you\'d like to apply some transformation (e.g., embedding)\n                on it before it\'s fed into the network, please set `use_B` to\n                True and add `query_embed_fn` when constructing this instance.\n                Similar to :attr:`soft_memory`, if `query_embed_fn` is set to\n                :meth:`~texar.tf.modules.MemNetBase.get_default_embed_fn`,\n                then it must be of shape `[batch_size, raw_memory_dim]`.\n                Ignored if `use_B` is not set.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout is\n                controlled by :func:`texar.tf.global_mode`.\n        """"""\n        if self._B is not None:\n            def _unsqueeze(x):\n                return x if x is None else tf.expand_dims(x, 1)\n            query = tf.squeeze(\n                self._B(_unsqueeze(query), _unsqueeze(soft_query), mode=mode),\n                1)\n        self._u = [query]\n        self._m = self._A(memory, soft_memory, mode=mode)\n        self._c = self._C(memory, soft_memory, mode=mode)\n\n        keep_prob = switch_dropout(1 - self.hparams.dropout_rate, mode=mode)\n        if self.hparams.variational:\n            with tf.variable_scope(""variational_dropout""):\n                noise = tf.random_uniform(tf.shape(self._u[-1]))\n                random_tensor = keep_prob + noise\n                binary_tensor = tf.floor(random_tensor)\n\n            def _variational_dropout(val):\n                return tf.math.div(val, keep_prob) * binary_tensor\n\n        for _ in range(self._n_hops):\n            u_ = self._AC(self._u[-1], self._m, self._c)\n            if self._relu_dim == 0:\n                pass\n            elif self._relu_dim == self._memory_dim:\n                u_ = tf.nn.relu(u_)\n            elif 0 < self._relu_dim < self._memory_dim:\n                linear_part = u_[:, : self._memory_dim - self._relu_dim]\n                relu_part = u_[:, self._memory_dim - self._relu_dim:]\n                relued_part = tf.nn.relu(relu_part)\n                u_ = tf.concat(axis=1, values=[linear_part, relued_part])\n            else:\n                raise ValueError(\n                    ""relu_dim = {} is illegal"".format(self._relu_dim))\n            if self.hparams.variational:\n                u_ = _variational_dropout(u_)\n            else:\n                u_ = tf.nn.dropout(u_, keep_prob)\n            self._u.append(u_)\n\n        logits = self._W(self._u[-1])\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._built = True\n\n        return logits\n'"
texar/tf/modules/networks/__init__.py,3,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of networks.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.networks.network_base import *\nfrom texar.tf.modules.networks.networks import *\nfrom texar.tf.modules.networks.conv_networks import *\n'"
texar/tf/modules/networks/conv_networks.py,18,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious convolutional networks.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.networks.network_base import FeedForwardNetworkBase\nfrom texar.tf.modules.networks.network_base import _build_layers\nfrom texar.tf.core.layers import get_pooling_layer_hparams, get_activation_fn\nfrom texar.tf.utils.utils import uniquify_str\nfrom texar.tf.utils.shapes import mask_sequences\nfrom texar.tf.hyperparams import HParams\n\n# pylint: disable=too-many-arguments, too-many-locals\n\n__all__ = [\n    ""_to_list"",\n    ""Conv1DNetwork""\n]\n\n\ndef _to_list(value, name=None, list_length=None):\n    """"""Converts hparam value into a list.\n\n    If :attr:`list_length` is given,\n    then the canonicalized :attr:`value` must be of\n    length :attr:`list_length`.\n    """"""\n    if not isinstance(value, (list, tuple)):\n        if list_length is not None:\n            value = [value] * list_length\n        else:\n            value = [value]\n    if list_length is not None and len(value) != list_length:\n        name = \'\' if name is None else name\n        raise ValueError(""hparams \'%s\' must be a list of length %d""\n                         % (name, list_length))\n    return value\n\n\nclass Conv1DNetwork(FeedForwardNetworkBase):\n    """"""Simple Conv-1D network which consists of a sequence of conv layers\n    followed with a sequence of dense layers.\n\n    Args:\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs. The inputs must be a\n    3D Tensor of shape `[batch_size, length, channels]` (default), or\n    `[batch_size, channels, length]` (if `data_format` is set to\n    `\'channels_last\'` through :attr:`hparams`). For example, for sequence\n    classification, `length` corresponds to time steps, and `channels`\n    corresponds to embedding dim.\n\n    Example:\n\n        .. code-block:: python\n\n            nn = Conv1DNetwork() # Use the default structure\n\n            inputs = tf.random_uniform([64, 20, 256])\n            outputs = nn(inputs)\n            # outputs == Tensor of shape [64, 128], cuz the final dense layer\n            # has size 128.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self, hparams=None):\n        FeedForwardNetworkBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            layer_hparams = self._build_layer_hparams()\n            _build_layers(self, layers=None, layer_hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Conv layers\n                ""num_conv_layers"": 1,\n                ""filters"": 128,\n                ""kernel_size"": [3, 4, 5],\n                ""conv_activation"": ""relu"",\n                ""conv_activation_kwargs"": None,\n                ""other_conv_kwargs"": None,\n                # (2) Pooling layers\n                ""pooling"": ""MaxPooling1D"",\n                ""pool_size"": None,\n                ""pool_strides"": 1,\n                ""other_pool_kwargs"": None,\n                # (3) Dense layers\n                ""num_dense_layers"": 1,\n                ""dense_size"": 128,\n                ""dense_activation"": ""identity"",\n                ""dense_activation_kwargs"": None,\n                ""final_dense_activation"": None,\n                ""final_dense_activation_kwargs"": None,\n                ""other_dense_kwargs"": None,\n                # (4) Dropout\n                ""dropout_conv"": [1],\n                ""dropout_dense"": [],\n                ""dropout_rate"": 0.75,\n                # (5) Others\n                ""name"": ""conv1d_network"",\n            }\n\n        Here:\n\n        1. For **convolutional** layers:\n\n            ""num_conv_layers"": int\n                Number of convolutional layers.\n\n            ""filters"": int or list\n                The number of filters in the convolution, i.e., the\n                dimensionality\n                of the output space. If ""num_conv_layers"" > 1, ""filters"" must be\n                a list of ""num_conv_layers"" integers.\n\n            ""kernel_size"": int or list\n                Lengths of 1D convolution windows.\n\n                - If ""num_conv_layers"" == 1, this can be a list of arbitrary \\\n                number\\\n                of `int` denoting different sized conv windows. The number of \\\n                filters of each size is specified by ""filters"". For example,\\\n                the default values will create 3 sets of filters, each of which\\\n                has kernel size of 3, 4, and 5, respectively, and has filter\\\n                number 128.\n                - If ""num_conv_layers"" > 1, this must be a list of length \\\n                ""num_conv_layers"". Each element can be an `int` or a list \\\n                of arbitrary number of `int` denoting the kernel size of \\\n                respective layer.\n\n            ""conv_activation"": str or callable\n                Activation function applied to the output of the convolutional\n                layers. Set to ""indentity"" to maintain a linear activation.\n                See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n            ""conv_activation_kwargs"": dict, optional\n                Keyword arguments for conv layer activation functions.\n                See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n            ""other_conv_kwargs"": dict, optional\n                Other keyword arguments for\n                :tf_main:`tf.layers.Conv1D <layers/Conv1d>` constructor, e.g.,\n                ""data_format"", ""padding"", etc.\n\n        2. For **pooling** layers:\n\n            ""pooling"": str or class or instance\n                Pooling layer after each of the convolutional layer(s). Can\n                a pooling layer class, its name or module path, or a class\n                instance.\n\n            ""pool_size"": int or list, optional\n                Size of the pooling window. If an `int`, all pooling layer\n                will have the same pool size. If a list, the list length must\n                equal ""num_conv_layers"". If `None` and the pooling type\n                is either\n                :tf_main:`MaxPooling <layers/MaxPooling1D>` or\n                :tf_main:`AveragePooling <layers/AveragePooling1D>`, the\n                pool size will be set to input size. That is, the output of\n                the pooling layer is a single unit.\n\n            ""pool_strides"": int or list, optional\n                Strides of the pooling operation. If an `int`, all pooling layer\n                will have the same stride. If a list, the list length must\n                equal ""num_conv_layers"".\n\n            ""other_pool_kwargs"": dict, optional\n                Other keyword arguments for pooling layer class constructor.\n\n        3. For **dense** layers (note that here dense layers always follow conv\n           and pooling layers):\n\n            ""num_dense_layers"": int\n                Number of dense layers.\n\n            ""dense_size"": int or list\n                Number of units of each dense layers. If an `int`, all dense\n                layers will have the same size. If a list of `int`, the list\n                length must equal ""num_dense_layers"".\n\n            ""dense_activation"": str or callable\n                Activation function applied to the output of the dense\n                layers **except** the last dense layer output . Set to\n                ""indentity"" to maintain a linear activation.\n                See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n            ""dense_activation_kwargs"": dict, optional\n                Keyword arguments for dense layer activation functions before\n                the last dense layer.\n                See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n            ""final_dense_activation"": str or callable\n                Activation function applied to the output of the **last** dense\n                layer. Set to `None` or\n                ""indentity"" to maintain a linear activation.\n                See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n            ""final_dense_activation_kwargs"": dict, optional\n                Keyword arguments for the activation function of last\n                dense layer.\n                See :func:`~texar.tf.core.get_activation_fn` for more details.\n\n            ""other_dense_kwargs"": dict, optional\n                Other keyword arguments for\n                :tf_main:`Dense <layers/Dense>`\n                layer class constructor.\n\n        4. For **dropouts**:\n\n            ""dropout_conv"": int or list\n                The indexes of conv layers (starting from `0`) whose **inputs**\n                are applied with dropout. The index = :attr:`num_conv_layers`\n                means dropout applies to the final conv layer output. E.g.,\n\n                .. code-block:: python\n\n                    {\n                        ""num_conv_layers"": 2,\n                        ""dropout_conv"": [0, 2]\n                    }\n\n                will leads to a series of layers as\n                `-dropout-conv0-conv1-dropout-`.\n\n                The dropout mode (training or not) is controlled\n                by the :attr:`mode` argument of :meth:`_build`.\n\n            ""dropout_dense"": int or list\n                Same as ""dropout_conv"" but applied to dense layers (index\n                starting from `0`).\n\n            ""dropout_rate"": float\n                The dropout rate, between 0 and 1. E.g.,\n                `""dropout_rate"": 0.1` would drop out 10% of elements.\n\n        5. Others:\n\n            ""name"": str\n                Name of the network.\n        """"""\n        return {\n            # Conv layers\n            ""num_conv_layers"": 1,\n            ""filters"": 128,\n            ""kernel_size"": [3, 4, 5],\n            ""conv_activation"": ""relu"",\n            ""conv_activation_kwargs"": None,\n            ""other_conv_kwargs"": None,\n            # Pooling layers\n            ""pooling"": ""MaxPooling1D"",\n            ""pool_size"": None,\n            ""pool_strides"": 1,\n            ""other_pool_kwargs"": None,\n            # Dense layers\n            ""num_dense_layers"": 1,\n            ""dense_size"": 128,\n            ""dense_activation"": ""identity"",\n            ""dense_activation_kwargs"": None,\n            ""final_dense_activation"": None,\n            ""final_dense_activation_kwargs"": None,\n            ""other_dense_kwargs"": None,\n            # Dropout\n            ""dropout_conv"": [1],\n            ""dropout_dense"": [],\n            ""dropout_rate"": 0.75,\n            # Others\n            ""name"": ""conv1d_network"",\n            ""@no_typecheck"": [""filters"", ""kernel_size"", ""conv_activation"",\n                              ""pool_size"", ""pool_strides"",\n                              ""dense_size"", ""dense_activation"",\n                              ""dropout_conv"", ""dropout_dense""]\n        }\n\n    def _build_pool_hparams(self):\n        pool_type = self._hparams.pooling\n        if pool_type == ""MaxPooling"":\n            pool_type = ""MaxPooling1D""\n        elif pool_type == ""AveragePooling"":\n            pool_type = ""AveragePooling1D""\n\n        npool = self._hparams.num_conv_layers\n        pool_size = _to_list(self._hparams.pool_size, ""pool_size"", npool)\n        strides = _to_list(self._hparams.pool_strides, ""pool_strides"", npool)\n\n        other_kwargs = self._hparams.other_pool_kwargs or {}\n        if isinstance(other_kwargs, HParams):\n            other_kwargs = other_kwargs.todict()\n        if not isinstance(other_kwargs, dict):\n            raise ValueError(""hparams[\'other_pool_kwargs\'] must be a dict."")\n\n        pool_hparams = []\n        for i in range(npool):\n            kwargs_i = {""pool_size"": pool_size[i], ""strides"": strides[i],\n                        ""name"": ""pool_%d"" % (i + 1)}\n            kwargs_i.update(other_kwargs)\n            pool_hparams_ = get_pooling_layer_hparams({""type"": pool_type,\n                                                       ""kwargs"": kwargs_i})\n            pool_hparams.append(pool_hparams_)\n\n        return pool_hparams\n\n    def _build_conv1d_hparams(self, pool_hparams):\n        """"""Creates the hparams for each of the conv layers usable for\n        :func:`texar.tf.core.layers.get_layer`.\n        """"""\n        nconv = self._hparams.num_conv_layers\n        if len(pool_hparams) != nconv:\n            raise ValueError(""`pool_hparams` must be of length %d"" % nconv)\n\n        filters = _to_list(self._hparams.filters, \'filters\', nconv)\n\n        if nconv == 1:\n            kernel_size = _to_list(self._hparams.kernel_size)\n            if not isinstance(kernel_size[0], (list, tuple)):\n                kernel_size = [kernel_size]\n        elif nconv > 1:\n            kernel_size = _to_list(self._hparams.kernel_size,\n                                   \'kernel_size\', nconv)\n            kernel_size = [_to_list(ks) for ks in kernel_size]\n\n        other_kwargs = self._hparams.other_conv_kwargs or {}\n        if isinstance(other_kwargs, HParams):\n            other_kwargs = other_kwargs.todict()\n        if not isinstance(other_kwargs, dict):\n            raise ValueError(""hparams[\'other_conv_kwargs\'] must be a dict."")\n\n        conv_pool_hparams = []\n        activation_fn = get_activation_fn(\n            self._hparams.conv_activation,\n            self._hparams.conv_activation_kwargs)\n        for i in range(nconv):\n            hparams_i = []\n            names = []\n            for ks_ij in kernel_size[i]:\n                name = uniquify_str(""conv_%d"" % (i + 1), names)\n                names.append(name)\n                conv_kwargs_ij = {\n                    ""filters"": filters[i],\n                    ""kernel_size"": ks_ij,\n                    ""activation"": activation_fn,\n                    ""name"": name\n                }\n                conv_kwargs_ij.update(other_kwargs)\n                hparams_i.append(\n                    {""type"": ""Conv1D"", ""kwargs"": conv_kwargs_ij})\n            if len(hparams_i) == 1:\n                conv_pool_hparams.append([hparams_i[0], pool_hparams[i]])\n            else:  # creates MergeLayer\n                mrg_kwargs_layers = []\n                for hparams_ij in hparams_i:\n                    seq_kwargs_j = {""layers"": [hparams_ij, pool_hparams[i]]}\n                    mrg_kwargs_layers.append(\n                        {""type"": ""SequentialLayer"", ""kwargs"": seq_kwargs_j})\n                mrg_hparams = {""type"": ""MergeLayer"",\n                               ""kwargs"": {""layers"": mrg_kwargs_layers,\n                                          ""name"": ""conv_pool_%d"" % (i + 1)}}\n                conv_pool_hparams.append(mrg_hparams)\n\n        return conv_pool_hparams\n\n    def _build_dense_hparams(self):\n        ndense = self._hparams.num_dense_layers\n        dense_size = _to_list(self._hparams.dense_size, \'dense_size\', ndense)\n\n        other_kwargs = self._hparams.other_dense_kwargs or {}\n        if isinstance(other_kwargs, HParams):\n            other_kwargs = other_kwargs.todict()\n        if not isinstance(other_kwargs, dict):\n            raise ValueError(""hparams[\'other_dense_kwargs\'] must be a dict."")\n\n        dense_hparams = []\n        activation_fn = get_activation_fn(\n            self._hparams.dense_activation,\n            self._hparams.dense_activation_kwargs)\n        for i in range(ndense):\n            if i == ndense - 1:\n                activation_fn = get_activation_fn(\n                    self._hparams.final_dense_activation,\n                    self._hparams.final_dense_activation_kwargs)\n\n            kwargs_i = {""units"": dense_size[i],\n                        ""activation"": activation_fn,\n                        ""name"": ""dense_%d"" % (i + 1)}\n            kwargs_i.update(other_kwargs)\n\n            dense_hparams.append({""type"": ""Dense"", ""kwargs"": kwargs_i})\n\n        return dense_hparams\n\n    def _build_layer_hparams(self):\n        pool_hparams = self._build_pool_hparams()\n        conv_pool_hparams = self._build_conv1d_hparams(pool_hparams)\n        dense_hparams = self._build_dense_hparams()\n\n        def _dropout_hparams(layer_id):\n            return {""type"": ""Dropout"",\n                    ""kwargs"": {""rate"": self._hparams.dropout_rate,\n                               ""name"": ""dropout_%d"" % layer_id}}\n        dropout_conv = _to_list(self._hparams.dropout_conv)\n        dropout_dense = _to_list(self._hparams.dropout_dense)\n\n        layers_hparams = []\n        nconv = self._hparams.num_conv_layers\n        for conv_i in range(nconv):\n            if conv_i in dropout_conv:\n                layers_hparams.append(_dropout_hparams(conv_i))\n            if isinstance(conv_pool_hparams[conv_i], (list, tuple)):\n                layers_hparams += conv_pool_hparams[conv_i]\n            else:\n                layers_hparams.append(conv_pool_hparams[conv_i])\n        if nconv in dropout_conv:\n            layers_hparams.append(_dropout_hparams(nconv))\n\n        ndense = self._hparams.num_dense_layers\n        if ndense > 0:  # Add flatten layers before dense layers\n            layers_hparams.append({""type"": ""Flatten""})\n        for dense_i in range(ndense):\n            if dense_i in dropout_dense:\n                layers_hparams.append(_dropout_hparams(dense_i + nconv))\n            layers_hparams.append(dense_hparams[dense_i])\n        if ndense in dropout_dense:\n            layers_hparams.append(_dropout_hparams(ndense + nconv))\n\n        return layers_hparams\n\n    def _build(self,    # pylint: disable=arguments-differ\n               inputs,\n               sequence_length=None,\n               dtype=None,\n               mode=None):\n        """"""Feeds forward inputs through the network layers and returns outputs.\n\n        Args:\n            inputs: The inputs to the network, which is a 3D tensor.\n            sequence_length (optional): An int tensor of shape `[batch_size]`\n                containing the length of each element in :attr:`inputs`.\n                If given, time steps beyond the length will first be masked out\n                before feeding to the layers.\n            dtype (optional): Type of the inputs. If not provided, infers\n                from inputs automatically.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`,\n                :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            The output of the final layer.\n        """"""\n        if sequence_length is not None:\n            inputs = mask_sequences(\n                inputs, sequence_length, dtype=dtype, time_major=False,\n                tensor_rank=3)\n        return super(Conv1DNetwork, self)._build(inputs, mode=mode)\n'"
texar/tf/modules/networks/network_base.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for feed forward neural networks.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.utils import TexarError\nfrom texar.tf.core.layers import get_layer\nfrom texar.tf.utils.utils import uniquify_str\nfrom texar.tf.utils.mode import is_train_mode\n\n# pylint: disable=too-many-instance-attributes, arguments-differ\n# pylint: disable=protected-access\n\n__all__ = [\n    ""_build_layers"",\n    ""FeedForwardNetworkBase""\n]\n\n\ndef _build_layers(network, layers=None, layer_hparams=None):\n    """"""Builds layers.\n\n    Either :attr:`layer_hparams` or :attr:`layers` must be\n    provided. If both are given, :attr:`layers` will be used.\n\n    Args:\n        network: An instance of a subclass of\n            :class:`~texar.tf.modules.networks.network_base.FeedForwardNetworkBase`\n        layers (optional): A list of layer instances.\n        layer_hparams (optional): A list of layer hparams, each to which\n            is fed to :func:`~texar.tf.core.layers.get_layer` to create the\n            layer instance.\n    """"""\n    with tf.variable_scope(network.variable_scope):\n        if layers is not None:\n            network._layers = layers\n        else:\n            if layer_hparams is None:\n                raise ValueError(\n                    \'Either `layer` or `layer_hparams` is required.\')\n            network._layers = []\n            for _, hparams in enumerate(layer_hparams):\n                network._layers.append(get_layer(hparams=hparams))\n\n    for layer in network._layers:\n        layer_name = uniquify_str(layer.name, network._layer_names)\n        network._layer_names.append(layer_name)\n        network._layers_by_name[layer_name] = layer\n\n\nclass FeedForwardNetworkBase(ModuleBase):\n    """"""Base class inherited by all feed-forward network classes.\n\n    Args:\n        hparams (dict, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`_build` for the inputs and outputs.\n    """"""\n\n    def __init__(self, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        self._layers = []\n        self._layer_names = []\n        self._layers_by_name = {}\n        self._layer_outputs = []\n        self._layer_outputs_by_name = {}\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""NN""\n            }\n        """"""\n        return {\n            ""name"": ""NN""\n        }\n\n    def _build(self, inputs, mode=None):\n        """"""Feeds forward inputs through the network layers and returns outputs.\n\n        Args:\n            inputs: The inputs to the network. The requirements on inputs\n                depends on the first layer and subsequent layers in the\n                network.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`,\n                :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            The output of the network.\n        """"""\n        training = is_train_mode(mode)\n\n        prev_outputs = inputs\n        for layer_id, layer in enumerate(self._layers):\n            if isinstance(layer, tf.layers.Dropout) or \\\n                    isinstance(layer, tf.layers.BatchNormalization):\n                outputs = layer(prev_outputs, training=training)\n            else:\n                outputs = layer(prev_outputs)\n            self._layer_outputs.append(outputs)\n            self._layer_outputs_by_name[self._layer_names[layer_id]] = outputs\n            prev_outputs = outputs\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            # Add trainable variables of `self._layers` which may be\n            # constructed externally.\n            for layer in self._layers:\n                self._add_trainable_variable(layer.trainable_variables)\n            self._built = True\n\n        return outputs\n\n    def append_layer(self, layer):\n        """"""Appends a layer to the end of the network. The method is only\n        feasible before :attr:`_build` is called.\n\n        Args:\n            layer: A :tf_main:`tf.layers.Layer <layers/Layer>` instance, or\n                a dict of layer hyperparameters.\n        """"""\n        if self._built:\n            raise TexarError(""`FeedForwardNetwork.append_layer` can be ""\n                             ""called only before `_build` is called."")\n\n        with tf.variable_scope(self.variable_scope):\n            layer_ = layer\n            if not isinstance(layer_, tf.layers.Layer):\n                layer_ = get_layer(hparams=layer_)\n            self._layers.append(layer_)\n            layer_name = uniquify_str(layer_.name, self._layer_names)\n            self._layer_names.append(layer_name)\n            self._layers_by_name[layer_name] = layer_\n\n    def has_layer(self, layer_name):\n        """"""Returns `True` if the network with the name exists. Returns `False`\n        otherwise.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return layer_name in self._layers_by_name\n\n    def layer_by_name(self, layer_name):\n        """"""Returns the layer with the name. Returns \'None\' if the layer name\n        does not exist.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._layers_by_name.get(layer_name, None)\n\n    @property\n    def layers_by_name(self):\n        """"""A dictionary mapping layer names to the layers.\n        """"""\n        return self._layers_by_name\n\n    @property\n    def layers(self):\n        """"""A list of the layers.\n        """"""\n        return self._layers\n\n    @property\n    def layer_names(self):\n        """"""A list of uniquified layer names.\n        """"""\n        return self._layer_names\n\n    def layer_outputs_by_name(self, layer_name):\n        """"""Returns the output tensors of the layer with the specified name.\n        Returns `None` if the layer name does not exist.\n\n        Args:\n            layer_name (str): Name of the layer.\n        """"""\n        return self._layer_outputs_by_name.get(layer_name, None)\n\n    @property\n    def layer_outputs(self):\n        """"""A list containing output tensors of each layer.\n        """"""\n        return self._layer_outputs\n'"
texar/tf/modules/networks/networks.py,7,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nVarious neural networks and related utilities.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.networks.network_base import FeedForwardNetworkBase\nfrom texar.tf.modules.networks.network_base import _build_layers\n\n__all__ = [\n    ""FeedForwardNetwork""\n]\n\n\nclass FeedForwardNetwork(FeedForwardNetworkBase):\n    """"""Feed-forward neural network that consists of a sequence of layers.\n\n    Args:\n        layers (list, optional): A list of :tf_main:`Layer <layers/Layer>`\n            instances composing the network. If not given, layers are created\n            according to :attr:`hparams`.\n        hparams (dict, optional): Embedder hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    See :meth:`~texar.tf.modules.RNNDecoderBase._build` of\n    :class:`~texar.tf.modules.FeedForwardNetworkBase` for the inputs and outputs.\n\n    Example:\n\n        .. code-block:: python\n\n            hparams = { # Builds a two-layer dense NN\n                ""layers"": [\n                    { ""type"": ""Dense"", ""kwargs"": { ""units"": 256 },\n                    { ""type"": ""Dense"", ""kwargs"": { ""units"": 10 }\n                ]\n            }\n            nn = FeedForwardNetwork(hparams=hparams)\n\n            inputs = tf.random_uniform([64, 100])\n            outputs = nn(inputs)\n            # outputs == Tensor of shape [64, 10]\n    """"""\n\n    def __init__(self, layers=None, hparams=None):\n        FeedForwardNetworkBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            _build_layers(\n                self, layers=layers, layer_hparams=self._hparams.layers)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""layers"": [],\n                ""name"": ""NN""\n            }\n\n        Here:\n\n        ""layers"": list\n            A list of layer hyperparameters. See :func:`~texar.tf.core.get_layer`\n            for the details of layer hyperparameters.\n\n        ""name"": str\n            Name of the network.\n        """"""\n        return {\n            ""layers"": [],\n            ""name"": ""NN""\n        }\n'"
texar/tf/modules/policies/__init__.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of Texar policies.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.policies.policy_nets import *\n'"
texar/tf/modules/policies/policy_nets.py,15,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Policy models based on feed forward networks.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow_probability import distributions as tfpd\n\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.agents.agent_utils import Space\nfrom texar.tf.utils import utils\nfrom texar.tf.utils.dtypes import get_tf_dtype\n\n# pylint: disable=no-member\n\n__all__ = [\n    \'PolicyNetBase\',\n    \'CategoricalPolicyNet\'\n]\n\n\nclass PolicyNetBase(ModuleBase):\n    """"""Policy net that takes in states and outputs actions.\n\n    Args:\n        network (optional): A network that takes in state and returns\n            outputs for generating actions. For example, an instance of subclass\n            of :class:`~texar.tf.modules.FeedForwardNetworkBase`. If `None`,\n            a network is created as specified in :attr:`hparams`.\n        network_kwargs (dict, optional): Keyword arguments for network\n            constructor.\n            Note that the `hparams` argument for network\n            constructor is specified in the ""network_hparams"" field of\n            :attr:`hparams` and should not be included in `network_kwargs`.\n            Ignored if :attr:`network` is given.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n    def __init__(self,\n                 network=None,\n                 network_kwargs=None,\n                 hparams=None):\n        ModuleBase.__init__(self, hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            self._build_network(network, network_kwargs)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                \'network_type\': \'FeedForwardNetwork\',\n                \'network_hparams\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                    ]\n                },\n                \'distribution_kwargs\': None,\n                \'name\': \'policy_net\',\n            }\n\n        Here:\n\n        ""network_type"": str or class or instance\n            A network that takes in state and returns outputs for\n            generating actions. This can be a class, its name or module path,\n            or a class instance. Ignored if `network` is given to the\n            constructor.\n\n        ""network_hparams"": dict\n            Hyperparameters for the network. With the :attr:`network_kwargs`\n            argument to the constructor, a network is created with\n            :python:`network_class(**network_kwargs, hparams=network_hparams)`.\n\n            For example, the default values creates a two-layer dense network.\n\n        ""distribution_kwargs"": dict, optional\n            Keyword arguments for distribution constructor. A distribution\n            would be created for action sampling.\n\n        ""name"": str\n            Name of the policy.\n        """"""\n        return {\n            \'network_type\': \'FeedForwardNetwork\',\n            \'network_hparams\': {\n                \'layers\': [\n                    {\n                        \'type\': \'Dense\',\n                        \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                    },\n                    {\n                        \'type\': \'Dense\',\n                        \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                    },\n                ]\n            },\n            \'distribution_kwargs\': None,\n            \'name\': \'policy_net\',\n            \'@no_typecheck\': [\'network_type\', \'network_hparams\']\n        }\n\n    def _build_network(self, network, kwargs):\n        if network is not None:\n            self._network = network\n        else:\n            kwargs = utils.get_instance_kwargs(\n                kwargs, self._hparams.network_hparams)\n            self._network = utils.check_or_get_instance(\n                self._hparams.network_type,\n                kwargs,\n                module_paths=[\'texar.tf.modules\', \'texar.tf.custom\'])\n\n    def _build(self, inputs, mode=None):  # pylint: disable=arguments-differ\n        raise NotImplementedError\n\n    @property\n    def network(self):\n        """"""The network.\n        """"""\n        return self._network\n\n\n# TODO(zhiting): Allow structured discrete actions.\nclass CategoricalPolicyNet(PolicyNetBase):\n    """"""Policy net with Categorical distribution for discrete scalar actions.\n\n    This is a combination of a network with a top-layer distribution for\n    action sampling.\n\n    Args:\n        action_space (optional): An instance of :class:`~texar.tf.agents.Space`\n            specifying the action space. If not given, an discrete action space\n            `[0, high]` is created with `high` specified in :attr:`hparams`.\n        network (optional): A network that takes in state and returns\n            outputs for generating actions. For example, an instance of subclass\n            of :class:`~texar.tf.modules.FeedForwardNetworkBase`. If `None`,\n            a network is created as specified in :attr:`hparams`.\n        network_kwargs (dict, optional): Keyword arguments for network\n            constructor.\n            Note that the `hparams` argument for network\n            constructor is specified in the ""network_hparams"" field of\n            :attr:`hparams` and should not be included in `network_kwargs`.\n            Ignored if :attr:`network` is given.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 action_space=None,\n                 network=None,\n                 network_kwargs=None,\n                 hparams=None):\n        PolicyNetBase.__init__(self, hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            if action_space is None:\n                action_space = Space(\n                    low=0, high=self._hparams.action_space, dtype=np.int32)\n            self._action_space = action_space\n            self._append_output_layer()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                \'network_type\': \'FeedForwardNetwork\',\n                \'network_hparams\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                    ]\n                },\n                \'distribution_kwargs\': {\n                    \'dtype\': \'int32\',\n                    \'validate_args\': False,\n                    \'allow_nan_stats\': True\n                },\n                \'action_space\': 2,\n                \'make_output_layer\': True,\n                \'name\': \'categorical_policy_net\'\n            }\n\n        Here:\n\n        ""distribution_kwargs"": dict\n            Keyword arguments for the :tf_main:`Categorical\n            <distributions/Categorical>` distribution constructor. Arguments\n            `logits` and `probs` should not be included as they are inferred\n            from the inputs. Argument `dtype` can be a string (e.g., `int32`)\n            and will be converted to a corresponding tf dtype.\n\n        ""action_space"": int\n            Upper bound of the action space. The resulting action space is\n            all discrete scalar numbers between 0 and the upper bound specified\n            here (both inclusive).\n\n        ""make_output_layer"": bool\n            Whether to append a dense layer to the network to transform\n            features to logits for action sampling. If `False`, the final layer\n            output of network must match the action space.\n\n        See :class:`~texar.tf.modules.PolicyNetBase.default_hparams` for details\n        of other hyperparameters.\n        """"""\n        hparams = PolicyNetBase.default_hparams()\n        hparams.update({\n            \'distribution_kwargs\': {\n                \'dtype\': \'int32\',\n                \'validate_args\': False,\n                \'allow_nan_stats\': True\n            },\n            \'action_space\': 2,\n            \'make_output_layer\': True,\n            \'name\': \'categorical_policy_net\'\n        })\n        return hparams\n\n    def _append_output_layer(self):\n        if not self._hparams.make_output_layer:\n            return\n\n        if self._action_space.shape != ():\n            raise ValueError(\'Only scalar discrete action is supported.\')\n        else:\n            output_size = self._action_space.high - self._action_space.low\n\n        layer_hparams = {\n            \'type\': \'Dense\',\n            \'kwargs\': {\'units\': output_size}\n        }\n        self._network.append_layer(layer_hparams)\n\n    def _build(self, inputs, mode=None):\n        """"""Takes in states and outputs actions.\n\n        Args:\n            inputs: Inputs to the policy network with the first dimension\n                the batch dimension.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`,\n                :func:`texar.tf.global_mode` is used.\n\n        Returns\n            A `dict` including fields `""logits""`, `""action""`, and `""dist""`,\n            where\n\n            - **""logits""**: A Tensor of shape \\\n            `[batch_size] + action_space size` used for categorical \\\n            distribution sampling.\n            - **""action""**: A Tensor of shape \\\n            `[batch_size] + action_space.shape`.\n            - **""dist""**: The \\\n            :tf_main:`Categorical <distributions/Categorical>` based on the \\\n            logits.\n        """"""\n        logits = self._network(inputs, mode=mode)\n\n        dkwargs = self._hparams.distribution_kwargs.todict()\n        dkwargs[\'dtype\'] = get_tf_dtype(dkwargs[\'dtype\'])\n        dist = tfpd.Categorical(logits=logits, **dkwargs)\n\n        action = dist.sample()\n        to_shape = [-1]  # for batch dimension\n        to_shape.extend(list(self._action_space.shape))\n        action = tf.reshape(action, to_shape)\n\n        outputs = {\n            ""logits"": logits,\n            ""action"": action,\n            ""dist"": dist\n        }\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._add_trainable_variable(self._network.trainable_variables)\n            self._built = True\n\n        return outputs\n\n    @property\n    def action_space(self):\n        """"""An instance of :class:`~texar.tf.agents.Space` specifiying the\n        action space.\n        """"""\n        return self._action_space\n'"
texar/tf/modules/pretrained/__init__.py,4,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nPre-trained modules of Texar library.\n""""""\n\nfrom texar.tf.modules.pretrained.pretrained_base import *\nfrom texar.tf.modules.pretrained.bert import *\nfrom texar.tf.modules.pretrained.gpt2 import *\nfrom texar.tf.modules.pretrained.xlnet import *\n'"
texar/tf/modules/pretrained/bert.py,7,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of BERT Modules.\n""""""\n\nimport collections\nimport json\nimport os\nimport re\n\nfrom abc import ABCMeta\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.pretrained.pretrained_base import PretrainedMixin\n\n__all__ = [\n    ""PretrainedBERTMixin"",\n]\n\n_BERT_PATH = ""https://storage.googleapis.com/bert_models/""\n\n\nclass PretrainedBERTMixin(PretrainedMixin):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the BERT model.\n\n    The BERT model was proposed in (`Devlin et al`. 2018)\n    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n    . A bidirectional Transformer language model pre-trained on large text\n    corpora. Available model names include:\n\n      * ``bert-base-uncased``: 12-layer, 768-hidden, 12-heads,\n        110M parameters.\n      * ``bert-large-uncased``: 24-layer, 1024-hidden, 16-heads,\n        340M parameters.\n      * ``bert-base-cased``: 12-layer, 768-hidden, 12-heads , 110M parameters.\n      * ``bert-large-cased``: 24-layer, 1024-hidden, 16-heads,\n        340M parameters.\n      * ``bert-base-multilingual-uncased``: 102 languages, 12-layer,\n        768-hidden, 12-heads, 110M parameters.\n      * ``bert-base-multilingual-cased``: 104 languages, 12-layer, 768-hidden,\n        12-heads, 110M parameters.\n      * ``bert-base-chinese``: Chinese Simplified and Traditional, 12-layer,\n        768-hidden, 12-heads, 110M parameters.\n\n    We provide the following BERT classes:\n\n      * :class:`~texar.tf.modules.BERTEncoder` for text encoding.\n      * :class:`~texar.tf.modules.BERTClassifier` for text classification and\n        sequence tagging.\n\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1810.04805\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    _MODEL_NAME = ""BERT""\n    _MODEL2URL = {\n        \'bert-base-uncased\':\n            _BERT_PATH + ""2018_10_18/uncased_L-12_H-768_A-12.zip"",\n        \'bert-large-uncased\':\n            _BERT_PATH + ""2018_10_18/uncased_L-24_H-1024_A-16.zip"",\n        \'bert-base-cased\':\n            _BERT_PATH + ""2018_10_18/cased_L-12_H-768_A-12.zip"",\n        \'bert-large-cased\':\n            _BERT_PATH + ""2018_10_18/cased_L-24_H-1024_A-16.zip"",\n        \'bert-base-multilingual-uncased\':\n            _BERT_PATH + ""2018_11_23/multi_cased_L-12_H-768_A-12.zip"",\n        \'bert-base-multilingual-cased\':\n            _BERT_PATH + ""2018_11_03/multilingual_L-12_H-768_A-12.zip"",\n        \'bert-base-chinese\':\n            _BERT_PATH + ""2018_11_03/chinese_L-12_H-768_A-12.zip"",\n    }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name, cache_dir):\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n\n        for file in files:\n            if file.endswith(\'config.json\'):\n                config_path = os.path.join(root, file)\n                with open(config_path) as f:\n                    config_ckpt = json.loads(f.read())\n\n        if config_path is None:\n            raise ValueError(""Cannot find the config file in {}"".format(\n                cache_dir))\n\n        configs = {}\n        hidden_dim = config_ckpt[\'hidden_size\']\n        configs[\'hidden_size\'] = config_ckpt[\'hidden_size\']\n        configs[\'embed\'] = {\n            \'name\': \'word_embeddings\',\n            \'dim\': hidden_dim}\n        configs[\'vocab_size\'] = config_ckpt[\'vocab_size\']\n\n        configs[\'segment_embed\'] = {\n            \'name\': \'token_type_embeddings\',\n            \'dim\': hidden_dim}\n        configs[\'type_vocab_size\'] = config_ckpt[\'type_vocab_size\']\n\n        configs[\'position_embed\'] = {\n            \'name\': \'position_embeddings\',\n            \'dim\': hidden_dim}\n        configs[\'position_size\'] = config_ckpt[\'max_position_embeddings\']\n\n        configs[\'encoder\'] = {\n            \'name\': \'encoder\',\n            \'embedding_dropout\': config_ckpt[\'hidden_dropout_prob\'],\n            \'num_blocks\': config_ckpt[\'num_hidden_layers\'],\n            \'multihead_attention\': {\n                \'use_bias\': True,\n                \'num_units\': hidden_dim,\n                \'num_heads\': config_ckpt[\'num_attention_heads\'],\n                \'output_dim\': hidden_dim,\n                \'dropout_rate\': config_ckpt[\'attention_probs_dropout_prob\'],\n                \'name\': \'self\'\n            },\n            \'residual_dropout\': config_ckpt[\'hidden_dropout_prob\'],\n            \'dim\': hidden_dim,\n            \'use_bert_config\': True,\n            \'poswise_feedforward\': {\n                ""layers"": [\n                    {\n                        \'type\': \'Dense\',\n                        \'kwargs\': {\n                            \'name\': \'intermediate\',\n                            \'units\': config_ckpt[\'intermediate_size\'],\n                            \'activation\': config_ckpt[\'hidden_act\'],\n                            \'use_bias\': True,\n                        }\n                    },\n                    {\n                        \'type\': \'Dense\',\n                        \'kwargs\': {\n                            \'name\': \'output\',\n                            \'units\': hidden_dim,\n                            \'activation\': None,\n                            \'use_bias\': True,\n                        }\n                    },\n                ],\n            },\n        }\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name,\n                              cache_dir, scope_name, **kwargs):\n        tvars = tf.trainable_variables()\n        init_checkpoint = os.path.abspath(os.path.join(cache_dir,\n                                                       \'bert_model.ckpt\'))\n        if init_checkpoint:\n            assignment_map, initialized_variable_names = \\\n                self._get_assignment_map_from_checkpoint(\n                    tvars, init_checkpoint, scope_name)\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    def _get_assignment_map_from_checkpoint(self, tvars, init_checkpoint,\n                                            scope_name):\n        r""""""`https://github.com/google-research/bert/blob/master/modeling.py`\n\n        Compute the union of the current variables and checkpoint variables.\n        Because the variable scope of the original BERT and Texar\n        implementation, we need to build a assignment map to match the\n        variables.\n        """"""\n        initialized_variable_names = {}\n\n        name_to_variable = collections.OrderedDict()\n        for var in tvars:\n            name = var.name\n            m = re.match(""^(.*):\\\\d+$"", name)\n            if m is not None:\n                name = m.group(1)\n            name_to_variable[name] = var\n\n        init_vars = tf.train.list_variables(init_checkpoint)\n\n        assignment_map = {\n            \'bert/embeddings/word_embeddings\':\n                scope_name + \'/word_embeddings/w\',\n            \'bert/embeddings/token_type_embeddings\':\n                scope_name + \'/token_type_embeddings/w\',\n            \'bert/embeddings/position_embeddings\':\n                scope_name + \'/position_embeddings/w\',\n            \'bert/embeddings/LayerNorm/beta\':\n                scope_name + \'/encoder/LayerNorm/beta\',\n            \'bert/embeddings/LayerNorm/gamma\':\n                scope_name + \'/encoder/LayerNorm/gamma\',\n        }\n        for check_name, model_name in assignment_map.items():\n            initialized_variable_names[model_name] = 1\n            initialized_variable_names[model_name + "":0""] = 1\n\n        for check_name, _ in init_vars:\n            if check_name.startswith(\'bert\'):\n                if check_name.startswith(\'bert/embeddings\'):\n                    continue\n                check_name_scope = check_name.replace(""bert/"", scope_name + \'/\')\n                model_name = re.sub(\n                    \'layer_\\\\d+/output/dense\',\n                    lambda x: x.group(0).replace(\'output/dense\', \'ffn/output\'),\n                    check_name_scope)\n                if model_name == check_name_scope:\n                    model_name = re.sub(\n                        \'layer_\\\\d+/output/LayerNorm\',\n                        lambda x: x.group(0).replace(\'output/LayerNorm\',\n                                                     \'ffn/LayerNorm\'),\n                        check_name_scope)\n                if model_name == check_name_scope:\n                    model_name = re.sub(\n                        \'layer_\\\\d+/intermediate/dense\',\n                        lambda x: x.group(0).replace(\'intermediate/dense\',\n                                                     \'ffn/intermediate\'),\n                        check_name_scope)\n                if model_name == check_name_scope:\n                    model_name = re.sub(\'attention/output/dense\',\n                                        \'attention/self/output\',\n                                        check_name_scope)\n                if model_name == check_name_scope:\n                    model_name = check_name_scope.replace(\n                        \'attention/output/LayerNorm\', \'output/LayerNorm\')\n\n                if model_name in name_to_variable.keys():\n                    assignment_map[check_name] = model_name\n                    initialized_variable_names[model_name] = 1\n                    initialized_variable_names[model_name + "":0""] = 1\n                else:\n                    tf.logging.info(\n                        \'model name:{} not exist\'.format(model_name))\n\n        return assignment_map, initialized_variable_names\n'"
texar/tf/modules/pretrained/gpt2.py,15,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of GPT2 Modules.\n""""""\n\nimport collections\nimport json\nimport os\nimport re\nimport warnings\n\nfrom abc import ABC\nfrom typing import Any, Dict\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom texar.tf.modules.pretrained.pretrained_base import PretrainedMixin\n\n__all__ = [\n    ""PretrainedGPT2Mixin"",\n]\n\n_GPT2_PATH = ""https://storage.googleapis.com/gpt-2/models/""\n_CHECKPOINT_FILES = [\n    ""checkpoint"", ""encoder.json"", ""hparams.json"", ""vocab.bpe"",\n    ""model.ckpt.data-00000-of-00001"", ""model.ckpt.index"", ""model.ckpt.meta""]\n\n\nclass PretrainedGPT2Mixin(PretrainedMixin, ABC):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the GPT2 model.\n\n    The GPT2 model was proposed in\n    `Language Models are Unsupervised Multitask Learners`_\n    by `Radford et al.` from OpenAI. It is a unidirectional Transformer model\n    pre-trained using the vanilla language modeling objective on a large corpus.\n\n    The available GPT2 models are as follows:\n\n      * ``gpt2-small``: Small version of GPT-2, 124M parameters.\n      * ``gpt2-medium``: Medium version of GPT-2, 355M parameters.\n      * ``gpt2-large``: Large version of GPT-2, 774M parameters.\n      * ``gpt2-xl``: XL version of GPT-2, 1558M parameters.\n\n    We provide the following GPT2 classes:\n\n      * :class:`~texar.tf.modules.GPT2Encoder` for text encoding.\n      * :class:`~texar.tf.modules.GPT2Decoder` for text generation and\n        decoding.\n      * :class:`~texar.tf.modules.GPT2Classifier` for text classification and\n        sequence tagging.\n\n    .. _`Language Models are Unsupervised Multitask Learners`:\n        https://openai.com/blog/better-language-models/\n    """"""\n    _IS_DECODE = False\n    _MODEL_NAME = ""GPT2""\n    _MODEL2URL = {\n        \'gpt2-small\': [_GPT2_PATH + f""124M/{file}""\n                       for file in _CHECKPOINT_FILES],\n        \'gpt2-medium\': [_GPT2_PATH + f""355M/{file}""\n                        for file in _CHECKPOINT_FILES],\n        \'gpt2-large\': [_GPT2_PATH + f""774M/{file}""\n                       for file in _CHECKPOINT_FILES],\n        \'gpt2-xl\': [_GPT2_PATH + f""1558M/{file}""\n                    for file in _CHECKPOINT_FILES],\n    }\n\n    # Raise warning for the deprecated pre-trained model names\n    class MyDict(dict):\n        def __contains__(self, key):\n            if key == \'117M\':\n                warnings.warn(""Pre-trained model name \'117M\' is deprecated, ""\n                              ""use \'gpt2-small\' instead."", UserWarning)\n                return True\n            elif key == \'345M\':\n                warnings.warn(""Pre-trained model name \'345M\' is deprecated, ""\n                              ""use \'gpt2-medium\' instead."", UserWarning)\n                return True\n            else:\n                return super().__contains__(key)\n\n    _DEPRECATED_MODEL2URL = {\n        \'117M\': [_GPT2_PATH + f""124M/{file}"" for file in _CHECKPOINT_FILES],\n        \'345M\': [_GPT2_PATH + f""355M/{file}"" for file in _CHECKPOINT_FILES],\n    }\n    _MODEL2URL.update(_DEPRECATED_MODEL2URL)\n    _MODEL2URL = MyDict(_MODEL2URL)  # type: ignore\n\n    def _transform_config(self, pretrained_model_name: str,\n                          cache_dir: str) -> Dict[str, Any]:\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n        for file in files:\n            if file.endswith(\'hparams.json\'):\n                config_path = os.path.join(root, file)\n        if config_path is None:\n            raise ValueError(f""Cannot find the config file in {cache_dir}"")\n\n        with open(config_path) as f:\n            config_gpt = json.loads(f.read())\n\n        hidden_dim = config_gpt[""n_embd""]\n        configs = {\n            ""vocab_size"": config_gpt[""n_vocab""],\n            ""context_size"": config_gpt[""n_ctx""],\n            ""embedding_size"": config_gpt[""n_embd""], ""embed"": {\n                ""dim"": hidden_dim,\n            },\n            ""position_size"": config_gpt[""n_ctx""],\n            ""position_embed"": {\n                ""dim"": hidden_dim\n            }\n        }\n\n        module_name = ""decoder"" if self._IS_DECODE else ""encoder""\n        configs.update({module_name: {\n            ""dim"": hidden_dim,\n            ""num_blocks"": config_gpt[""n_layer""],\n            ""embedding_dropout"": 0,\n            ""residual_dropout"": 0,\n            ""multihead_attention"": {\n                ""use_bias"": True,\n                ""num_units"": hidden_dim,\n                ""num_heads"": config_gpt[""n_head""],\n                ""output_dim"": hidden_dim,\n            },\n            ""initializer"": {\n                ""type"": ""variance_scaling_initializer"",\n                ""kwargs"": {\n                        \'factor\': 1.0,\n                        \'mode\': \'FAN_AVG\',\n                        \'uniform\': True\n                },\n            },\n            ""poswise_feedforward"": {\n                ""layers"": [\n                    {\n                        ""type"": ""Dense"",\n                        ""kwargs"": {\n                            \'name\': \'intermediate\',\n                            \'activation\': \'gelu\',\n                            ""units"": hidden_dim * 4,\n                            ""use_bias"": True,\n                        }\n                    },\n                    {\n                        ""type"": ""Dense"",\n                        ""kwargs"": {\n                            \'activation\': None,\n                            \'name\': \'output\',\n                            ""units"": hidden_dim,\n                            ""use_bias"": True,\n                        }\n                    }\n                ],\n            },\n        }})\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name, cache_dir,\n                              scope_name, load_output_layer=True, **kwargs):\n        r""""""Initialize model parameters from weights stored in the pre-trained\n        checkpoint.\n\n        Args:\n            pretrained_model_name (str): Name of the pre-trained model.\n            cache_dir (str): Path to the cache directory.\n            scope_name (str): Scope name of the model.\n            load_output_layer (bool): If `False`, will not load weights of the\n                output layer. Set this argument to `False` when loading weights\n                into a GPT2 encoder. Defaults to `True`.\n        """"""\n        init_checkpoint = os.path.abspath(os.path.join(cache_dir,\n                                                       \'model.ckpt\'))\n        ckpt = tf.train.load_checkpoint(init_checkpoint)\n        ckpt_params = {key: ckpt.get_tensor(key) for key in\n                       ckpt.get_variable_to_shape_map().keys()}\n\n        tvars = tf.trainable_variables()\n        name_to_variable = collections.OrderedDict()\n        for var in tvars:\n            name = var.name\n            m = re.match(""^(.*):\\\\d+$"", name)\n            if m is not None:\n                name = m.group(1)\n            name_to_variable[name] = var\n\n        if load_output_layer:\n            global_tensor_map = {\n                \'model/wte\': scope_name + \'/word_embeddings/w\',\n                \'model/wpe\': scope_name + \'/position_embeddings/w\',\n                \'model/ln_f/b\': scope_name + \'/decoder/beta\',\n                \'model/ln_f/g\': scope_name + \'/decoder/gamma\',\n            }\n\n            layer_tensor_map = {\n                ""ln_1/b"": scope_name + \'/layer_{}/beta\',\n                ""ln_1/g"": scope_name + \'/layer_{}/gamma\',\n                ""ln_2/b"": scope_name + \'/layer_{}/past_poswise_ln/beta\',\n                ""ln_2/g"": scope_name + \'/layer_{}/past_poswise_ln/gamma\',\n                ""mlp/c_fc/b"": scope_name + \'/decoder/layer_{}\'\n                                           \'/ffn/intermediate/bias\',\n                ""mlp/c_fc/w"": scope_name + \'/decoder/layer_{}\'\n                                           \'/ffn/intermediate/kernel\',\n                ""mlp/c_proj/b"": scope_name + \'/decoder/layer_{}/ffn/output/\'\n                                             \'bias\',\n                ""mlp/c_proj/w"": scope_name + \'/decoder/layer_{}/ffn/output/\'\n                                             \'kernel\',\n                ""attn/c_attn/b"": None,\n                ""attn/c_attn/w"": None,\n                ""attn/c_proj/b"": scope_name + \'/decoder/layer_{}\'\n                                              \'/self_attention/self/output/\'\n                                              \'bias\',\n                ""attn/c_proj/w"": scope_name + \'/decoder/layer_{}\'\n                                              \'/self_attention/self/output/\'\n                                              \'kernel\',\n            }\n        else:\n            global_tensor_map = {\n                \'model/wte\': scope_name + \'/word_embeddings/w\',\n                \'model/wpe\': scope_name + \'/position_embeddings/w\',\n                \'model/ln_f/b\': scope_name + \'/encoder/LayerNorm/beta\',\n                \'model/ln_f/g\': scope_name + \'/encoder/LayerNorm/gamma\',\n            }\n\n            layer_tensor_map = {\n                ""ln_1/b"": scope_name + \'/encoder/layer_{}/LayerNorm/beta\',\n                ""ln_1/g"": scope_name + \'/encoder/layer_{}/LayerNorm/gamma\',\n                ""ln_2/b"": scope_name + \'/encoder/layer_{}/output/\'\n                                       \'LayerNorm/beta\',\n                ""ln_2/g"": scope_name + \'/encoder/layer_{}/output/\'\n                                       \'LayerNorm/gamma\',\n                ""mlp/c_fc/b"": scope_name + \'/encoder/layer_{}\'\n                                           \'/ffn/intermediate/bias\',\n                ""mlp/c_fc/w"": scope_name + \'/encoder/layer_{}\'\n                                           \'/ffn/intermediate/kernel\',\n                ""mlp/c_proj/b"": scope_name + \'/encoder/layer_{}/ffn/output/\'\n                                             \'bias\',\n                ""mlp/c_proj/w"": scope_name + \'/encoder/layer_{}/ffn/output/\'\n                                             \'kernel\',\n                ""attn/c_attn/b"": None,\n                ""attn/c_attn/w"": None,\n                ""attn/c_proj/b"": scope_name + \'/encoder/layer_{}\'\n                                              \'/attention/self/output/bias\',\n                ""attn/c_proj/w"": scope_name + \'/encoder/layer_{}\'\n                                              \'/attention/self/output/kernel\',\n            }\n\n        for name, array in ckpt_params.items():\n            if name in global_tensor_map:\n                v_name = global_tensor_map[name]\n                pointer = name_to_variable[v_name]\n                pointer._initializer_op = tf.assign(pointer._variable, array)\n            else:\n                name_tmp = name.split(""/"")\n                layer_no = name_tmp[1][1:]\n                name = ""/"".join(name_tmp[2:])\n\n                if name in layer_tensor_map:\n                    if name == ""attn/c_attn/b"":\n                        if load_output_layer:\n                            K = name_to_variable[\n                                scope_name + \'/decoder/layer_\' + layer_no +\n                                \'/self_attention/self/key/bias\']\n                            Q = name_to_variable[\n                                scope_name + \'/decoder/layer_\' + layer_no +\n                                \'/self_attention/self/query/bias\']\n                            V = name_to_variable[\n                                scope_name + \'/decoder/layer_\' + layer_no +\n                                \'/self_attention/self/value/bias\']\n                        else:\n                            K = name_to_variable[\n                                scope_name + \'/encoder/layer_\' + layer_no +\n                                \'/attention/self/key/bias\']\n                            Q = name_to_variable[\n                                scope_name + \'/encoder/layer_\' + layer_no +\n                                \'/attention/self/query/bias\']\n                            V = name_to_variable[\n                                scope_name + \'/encoder/layer_\' + layer_no +\n                                \'/attention/self/value/bias\']\n\n                        index_d = array.shape[-1] // 3\n\n                        Q_w = array[:index_d]\n                        K_w = array[index_d: 2 * index_d]\n                        V_w = array[2 * index_d:]\n\n                        K._initializer_op = tf.assign(K._variable, K_w)\n                        Q._initializer_op = tf.assign(Q._variable, Q_w)\n                        V._initializer_op = tf.assign(V._variable, V_w)\n                    elif name == ""attn/c_attn/w"":\n                        if load_output_layer:\n                            K = name_to_variable[\n                                scope_name + \'/decoder/layer_\' + layer_no +\n                                \'/self_attention/self/key/kernel\']\n                            Q = name_to_variable[\n                                scope_name + \'/decoder/layer_\' + layer_no +\n                                \'/self_attention/self/query/kernel\']\n                            V = name_to_variable[\n                                scope_name + \'/decoder/layer_\' + layer_no +\n                                \'/self_attention/self/value/kernel\']\n                        else:\n                            K = name_to_variable[\n                                scope_name + \'/encoder/layer_\' + layer_no +\n                                \'/attention/self/key/kernel\']\n                            Q = name_to_variable[\n                                scope_name + \'/encoder/layer_\' + layer_no +\n                                \'/attention/self/query/kernel\']\n                            V = name_to_variable[\n                                scope_name + \'/encoder/layer_\' + layer_no +\n                                \'/attention/self/value/kernel\']\n\n                        index_d = array.shape[-1] // 3\n\n                        Q_w = np.transpose(array[0, :, :index_d])\n                        K_w = np.transpose(array[0, :, index_d: 2 * index_d])\n                        V_w = np.transpose(array[0, :, 2 * index_d:])\n\n                        K._initializer_op = tf.assign(K._variable, K_w)\n                        Q._initializer_op = tf.assign(Q._variable, Q_w)\n                        V._initializer_op = tf.assign(V._variable, V_w)\n                    elif (name == ""attn/c_proj/w"" or name == ""mlp/c_fc/w"" or\n                          name == ""mlp/c_proj/w""):\n                        v_name = layer_tensor_map[name]\n                        pointer = name_to_variable[v_name.format(layer_no)]\n                        pointer._initializer_op = tf.assign(pointer._variable,\n                                                            array[0])\n                    else:\n                        v_name = layer_tensor_map[name]\n                        pointer = name_to_variable[v_name.format(layer_no)]\n                        pointer._initializer_op = tf.assign(pointer._variable,\n                                                            array)\n'"
texar/tf/modules/pretrained/pretrained_base.py,3,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for Pre-trained Modules.\n""""""\n\nimport os\nimport sys\n\nfrom abc import ABCMeta, abstractmethod\nfrom pathlib import Path\n\nfrom texar.tf.data.data_utils import maybe_download\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.module_base import ModuleBase\n\n__all__ = [\n    ""default_download_dir"",\n    ""set_default_download_dir"",\n    ""PretrainedMixin"",\n]\n\n_default_texar_download_dir = None\n\n\ndef default_download_dir(name):\n    r""""""Return the directory to which packages will be downloaded by default.\n    """"""\n    global _default_texar_download_dir  # pylint: disable=global-statement\n    if _default_texar_download_dir is None:\n        if sys.platform == \'win32\' and \'APPDATA\' in os.environ:\n            # On Windows, use %APPDATA%\n            home_dir = Path(os.environ[\'APPDATA\'])\n        else:\n            # Otherwise, install in the user\'s home directory.\n            home_dir = Path(os.environ[""HOME""])\n\n        if os.access(str(home_dir), os.W_OK):\n            _default_texar_download_dir = home_dir / \'texar_data\'\n        else:\n            raise ValueError(""The path {} is not writable. Please manually ""\n                             ""specify the download directory"".format(home_dir))\n\n    if not _default_texar_download_dir.exists():\n        _default_texar_download_dir.mkdir(parents=True)\n\n    return _default_texar_download_dir / name\n\n\ndef set_default_download_dir(path):\n    if isinstance(path, str):\n        path = Path(path)\n    elif not isinstance(path, Path):\n        raise ValueError(""`path` must be a string or a pathlib.Path object"")\n\n    if not os.access(str(path), os.W_OK):\n        raise ValueError(\n            ""The specified download directory {} is not writable"".format(path))\n\n    global _default_texar_download_dir  # pylint: disable=global-statement\n    _default_texar_download_dir = path\n\n\nclass PretrainedMixin(ModuleBase):\n    r""""""A mixin class for all pre-trained classes to inherit.\n    """"""\n    __metaclass__ = ABCMeta\n\n    _MODEL_NAME = None\n    _MODEL2URL = None\n\n    pretrained_model_dir = None\n\n    @classmethod\n    def available_checkpoints(cls):\n        return list(cls._MODEL2URL.keys())\n\n    def _name_to_variable(self, name):\n        r""""""Find the corresponding variable given the specified name.\n        """"""\n        pointer = self\n        for m_name in name.split("".""):\n            if m_name.isdigit():\n                num = int(m_name)\n                pointer = pointer[num]  # type: ignore\n            else:\n                pointer = getattr(pointer, m_name)\n        return pointer  # type: ignore\n\n    def load_pretrained_config(self,\n                               pretrained_model_name=None,\n                               cache_dir=None,\n                               hparams=None):\n        r""""""Load paths and configurations of the pre-trained model.\n\n        Args:\n            pretrained_model_name (optional): A str with the name\n                of a pre-trained model to load. If `None`, will use the model\n                name in :attr:`hparams`.\n            cache_dir (optional): The path to a folder in which the\n                pre-trained models will be cached. If `None` (default),\n                a default directory will be used.\n            hparams (dict or HParams, optional): Hyperparameters. Missing\n                hyperparameter will be set to default values. See\n                :meth:`default_hparams` for the hyperparameter structure\n                and default values.\n        """"""\n        if not hasattr(self, ""_hparams""):\n            self._hparams = HParams(hparams, self.default_hparams())\n        else:\n            # Probably already parsed by subclasses. We rely on subclass\n            # implementations to get this right.\n            # As a sanity check, we require `hparams` to be `None` in this case.\n            if hparams is not None:\n                raise ValueError(\n                    ""`self._hparams` is already assigned, but `hparams` ""\n                    ""argument is not None."")\n\n        self.pretrained_model_dir = None\n        self.pretrained_model_name = pretrained_model_name\n\n        if self.pretrained_model_name is None:\n            self.pretrained_model_name = self._hparams.pretrained_model_name\n        if self.pretrained_model_name is not None:\n            self.pretrained_model_dir = self.download_checkpoint(\n                self.pretrained_model_name, cache_dir)\n            pretrained_model_hparams = self._transform_config(\n                self.pretrained_model_name, self.pretrained_model_dir)\n            self._hparams = HParams(\n                pretrained_model_hparams, self._hparams.todict())\n\n    def init_pretrained_weights(self, scope_name, **kwargs):\n        if self.pretrained_model_dir:\n            self._init_from_checkpoint(\n                self.pretrained_model_name,\n                self.pretrained_model_dir, scope_name, **kwargs)\n        else:\n            self.reset_parameters()\n\n    def reset_parameters(self):\n        r""""""Initialize parameters of the pre-trained model. This method is only\n        called if pre-trained checkpoints are not loaded.\n        """"""\n        pass\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""pretrained_model_name"": None,\n                ""name"": ""pretrained_base""\n            }\n        """"""\n        return {\n            \'pretrained_model_name\': None,\n            \'name\': ""pretrained_base"",\n            \'@no_typecheck\': [\'pretrained_model_name\']\n        }\n\n    @classmethod\n    def download_checkpoint(cls, pretrained_model_name, cache_dir=None):\n        r""""""Download the specified pre-trained checkpoint, and return the\n        directory in which the checkpoint is cached.\n\n        Args:\n            pretrained_model_name (str): Name of the model checkpoint.\n            cache_dir (str, optional): Path to the cache directory. If `None`,\n                uses the default directory (user\'s home directory).\n\n        Returns:\n            Path to the cache directory.\n        """"""\n        if pretrained_model_name in cls._MODEL2URL:\n            download_path = cls._MODEL2URL[pretrained_model_name]\n        else:\n            raise ValueError(\n                ""Pre-trained model not found: {}"".format(pretrained_model_name))\n\n        if cache_dir is None:\n            cache_path = default_download_dir(cls._MODEL_NAME)\n        else:\n            cache_path = Path(cache_dir)\n        cache_path = cache_path / pretrained_model_name\n\n        if not cache_path.exists():\n            if isinstance(download_path, list):\n                for path in download_path:\n                    maybe_download(path, str(cache_path))\n            else:\n                filename = download_path.split(\'/\')[-1]\n                maybe_download(download_path, str(cache_path), extract=True)\n                folder = None\n                for file in cache_path.iterdir():\n                    if file.is_dir():\n                        folder = file\n                assert folder is not None\n                (cache_path / filename).unlink()\n                for file in folder.iterdir():\n                    file.rename(file.parents[1] / file.name)\n                folder.rmdir()\n            print(""Pre-trained {} checkpoint {} cached to {}"".format(\n                cls._MODEL_NAME, pretrained_model_name, cache_path))\n        else:\n            print(""Using cached pre-trained {} checkpoint from {}."".format(\n                cls._MODEL_NAME, cache_path))\n\n        return str(cache_path)\n\n    @classmethod\n    @abstractmethod\n    def _transform_config(cls, pretrained_model_name, cache_dir):\n        r""""""Load the official configuration file and transform it into\n        Texar-style hyperparameters.\n\n        Args:\n            pretrained_model_name (str): Name of the pre-trained model.\n            cache_dir (str): Path to the cache directory.\n\n        Returns:\n            dict: Texar module hyperparameters.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def _init_from_checkpoint(self, pretrained_model_name, cache_dir,\n                              scope_name, **kwargs):\n        r""""""Initialize model parameters from weights stored in the pre-trained\n        checkpoint.\n\n        Args:\n            pretrained_model_name (str): Name of the pre-trained model.\n            cache_dir (str): Path to the cache directory.\n            scope_name: Variable scope.\n            **kwargs: Additional arguments for specific models.\n        """"""\n        raise NotImplementedError\n'"
texar/tf/modules/pretrained/xlnet.py,5,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nUtils of XLNet Modules.\n""""""\n\nimport collections\nimport json\nimport os\nimport re\n\nfrom abc import ABCMeta\n\nimport tensorflow as tf\n\nfrom texar.tf.modules.pretrained.pretrained_base import PretrainedMixin\n\n__all__ = [\n    ""PretrainedXLNetMixin"",\n]\n\n_XLNET_PATH = ""https://storage.googleapis.com/xlnet/released_models/""\n\n\nclass PretrainedXLNetMixin(PretrainedMixin):\n    r""""""A mixin class to support loading pre-trained checkpoints for modules\n    that implement the XLNet model.\n\n    The XLNet model was proposed in\n    `XLNet: Generalized Autoregressive Pretraining for Language Understanding`_\n    by `Yang et al.` It is based on the Transformer-XL model, pre-trained on a\n    large corpus using a language modeling objective that considers all\n    permutations of the input sentence.\n\n    The available XLNet models are as follows:\n\n      * ``xlnet-based-cased``: 12-layer, 768-hidden, 12-heads. This model is\n        trained on full data (different from the one in the paper).\n      * ``xlnet-large-cased``: 24-layer, 1024-hidden, 16-heads.\n\n    We provide the following XLNet classes:\n\n      * :class:`~texar.torch.modules.XLNetEncoder` for text encoding.\n      * :class:`~texar.torch.modules.XLNetDecoder` for text generation and\n        decoding.\n      * :class:`~texar.torch.modules.XLNetClassifier` for text classification\n        and sequence tagging.\n      * :class:`~texar.torch.modules.XLNetRegressor` for text regression.\n\n    .. _`XLNet: Generalized Autoregressive Pretraining for Language Understanding`:\n        http://arxiv.org/abs/1906.08237\n    """"""\n\n    __metaclass__ = ABCMeta\n\n    _MODEL_NAME = ""XLNet""\n    _MODEL2URL = {\n        \'xlnet-base-cased\':\n            _XLNET_PATH + ""cased_L-12_H-768_A-12.zip"",\n        \'xlnet-large-cased\':\n            _XLNET_PATH + ""cased_L-24_H-1024_A-16.zip"",\n    }\n\n    @classmethod\n    def _transform_config(cls, pretrained_model_name, cache_dir):\n        info = list(os.walk(cache_dir))\n        root, _, files = info[0]\n        config_path = None\n        for file in files:\n            if file.endswith(\'config.json\'):\n                config_path = os.path.join(root, file)\n        if config_path is None:\n            raise ValueError(""Cannot find the config file in {}"".format(\n                cache_dir))\n\n        with open(config_path) as f:\n            config_ckpt = json.loads(f.read())\n\n        configs = {\n            ""head_dim"": config_ckpt[""d_head""],\n            ""ffn_inner_dim"": config_ckpt[""d_inner""],\n            ""hidden_dim"": config_ckpt[""d_model""],\n            ""activation"": config_ckpt[""ff_activation""],\n            ""num_heads"": config_ckpt[""n_head""],\n            ""num_layers"": config_ckpt[""n_layer""],\n            ""vocab_size"": config_ckpt[""n_token""],\n            ""untie_r"": config_ckpt[""untie_r""]\n        }\n\n        return configs\n\n    def _init_from_checkpoint(self, pretrained_model_name,\n                              cache_dir, scope_name, **kwargs):\n\n        tvars = tf.trainable_variables()\n        init_checkpoint = os.path.join(cache_dir, \'xlnet_model.ckpt\')\n        if init_checkpoint:\n            assignment_map, initialized_variable_names = \\\n                self._get_assignment_map_from_checkpoint(\n                    tvars, init_checkpoint, scope_name)\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    def _get_assignment_map_from_checkpoint(self, tvars, init_checkpoint,\n                                            scope_name):\n        r""""""\n        Compute the union of the current variables and checkpoint variables.\n        Because of the variable scope of the original XLNet and Texar\n        implementation, we need to build a assignment map to match the variables.\n        """"""\n        assignment_map = {}\n        initialized_variable_names = {}\n\n        name_to_variable = collections.OrderedDict()\n        for var in tvars:\n            name = var.name\n            m = re.match(""^(.*):\\\\d+$"", name)\n            if m is not None:\n                name = m.group(1)\n            name_to_variable[name] = var\n\n        init_vars = tf.train.list_variables(init_checkpoint)\n\n        for check_name, _ in init_vars:\n            check_name_scope = check_name.replace(\n                \'model/transformer/\', scope_name + \'/\')\n            model_name = check_name_scope\n            if check_name.startswith(\'model/lm_loss/bias\'):\n                model_name = scope_name + \'/lm_loss/bias\'\n            elif check_name.startswith(\'model/transformer/mask_emb\'):\n                model_name = check_name_scope.replace(\n                    \'mask_emb/mask_emb\', \'mask_emb\')\n            elif check_name.startswith(\'model/transformer/word_embedding\'):\n                model_name = scope_name + \'/word_embedder/w\'\n            elif re.match(\'model/transformer/r_[r,s,w]_bias\', check_name):\n                model_name = check_name_scope\n            elif re.match(\'model/transformer/seg_embed\', check_name):\n                model_name = check_name_scope\n            elif re.match(\'model/transformer/layer_\\\\d+/rel_attn/[q,k,v,r,o]\',\n                          check_name):\n                model_name = check_name_scope\n            elif re.match(\'model/transformer/layer_\\\\d+/rel_attn/LayerNorm\',\n                          check_name):\n                model_name = check_name_scope.replace(\'LayerNorm/\', \'\')\n            elif re.match(\'model/transformer/layer_\\\\d+/ff/layer_[1,2]\',\n                          check_name):\n                model_name = check_name_scope.replace(\'ff/layer_1\', \'ff/dense\')\n                if model_name == check_name_scope:\n                    model_name = check_name_scope.replace(\n                        \'ff/layer_2\', \'ff/dense_1\')\n            elif re.match(\'model/transformer/layer_\\\\d+/ff/LayerNorm\',\n                          check_name):\n                model_name = check_name_scope.replace(\'LayerNorm/\', \'\')\n\n            if model_name in name_to_variable.keys():\n                assignment_map[check_name] = model_name\n                initialized_variable_names[model_name] = 1\n                initialized_variable_names[model_name + "":0""] = 1\n            else:\n                tf.logging.info(\'model name:{} not exist\'.format(model_name))\n\n        return assignment_map, initialized_variable_names\n'"
texar/tf/modules/pretrained/xlnet_utils.py,56,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModel Utils of XLNet Modules.\nAdapted from\nhttps://github.com/zihangdai/xlnet/blob/master/modeling.py\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.core import layers\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.utils.mode import is_train_mode\n\n\n__all__ = [\n    \'PositionWiseFF\',\n    \'PositionalEmbedding\',\n    \'RelativePositionalEncoding\',\n    \'RelativeMutiheadAttention\'\n]\n\n\nclass PositionWiseFF(ModuleBase):\n    r""""""Position Wise feed forward.""""""\n    def __init__(self, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n        hidden_dim = self._hparams.hidden_dim\n        ffn_inner_dim = self._hparams.ffn_inner_dim\n        dropout = self._hparams.dropout\n        activation = self._hparams.activation\n        if activation == \'gelu\':\n            activation = layers.gelu\n\n        with tf.variable_scope(self.variable_scope):\n            tf.get_variable_scope().set_initializer(\n                layers.get_initializer(self._hparams.initializer))\n            l1_hparams = {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""units"": ffn_inner_dim,\n                    ""activation"": activation\n                }\n            }\n            self.linear1 = layers.get_layer(hparams=l1_hparams)\n            dropout_hparams = {\n                ""type"": ""Dropout"",\n                ""kwargs"": {\n                    ""rate"": dropout\n                }\n            }\n            self.dropout = layers.get_layer(hparams=dropout_hparams)\n            l2_hparams = {\n                ""type"": ""Dense"",\n                ""kwargs"": {\n                    ""units"": hidden_dim\n                }\n            }\n            self.linear2 = layers.get_layer(hparams=l2_hparams)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""hidden_dim"": 768,\n                ""ffn_inner_dim"": 3072,\n                ""dropout"": 0.1,\n                ""activation"": \'gelu\'\n            }\n\n        Here\n\n        `""hidden_dim""`: int\n            Dimension of the layer fed as input to feed forward network\n\n        `""ffn_inner_dim""`: int\n            Inner dimension of the feed forward layer\n\n        `""dropout""`: float\n            Dropout rate for layers\n\n        `""activation""`: str or callable\n            Activation function applied to the output of the PositionWise FF.\n            See :func:`~texar.tf.core.get_activation_fn` for more details.\n        """"""\n        return {\n            ""name"": ""ff"",\n            ""initializer"": None,\n            ""hidden_dim"": 768,\n            ""ffn_inner_dim"": 3072,\n            ""dropout"": 0.1,\n            ""activation"": \'gelu\',\n        }\n\n    def _build(self, input, mode=None):\n        r""""""Compute feed forward for the input.\n\n        Args:\n            input: Input tensor of size `(max_time, batch_size, hidden_dim)`\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout is\n                controlled by :func:`texar.tf.global_mode`.\n\n        :returns: A tensor output of the position wise feed forward network\n        """"""\n        is_training = is_train_mode(mode)\n        output = self.linear1(input)\n        output = self.dropout(output, training=is_training)\n        output = self.linear2(output)\n        output = self.dropout(output, training=is_training)\n\n        # residual + layer norm\n        output = tf.contrib.layers.layer_norm(\n            input + output, begin_norm_axis=-1, scope=self.variable_scope,\n            reuse=tf.AUTO_REUSE)\n\n        return output\n\n\nclass PositionalEmbedding(ModuleBase):\n    r""""""Sinosoidal Positional Embedding.\n    """"""\n\n    # TODO(avinash) : See if this can be merged with Sinosoidal Position\n    # Embedder\n    def __init__(self, embed_dim):\n        ModuleBase.__init__(self)\n        freq_seq = tf.range(0.0, embed_dim, 2.0)\n        self.inv_freq = 1 / (10000 ** (freq_seq / embed_dim))\n\n    def _build(self, pos_seq):\n        r""""""Compute sinosoidal positional embeddings.\n\n        Args:\n            pos_seq: A 1D tensor of position sequences\n\n        :returns: A 2D tensor of sinosoidal embeddings for the sequence.\n        """"""\n        pos_seq = tf.dtypes.cast(pos_seq, dtype=self.inv_freq.dtype)\n        sinusoid_inp = tf.einsum(\'i,d->id\', pos_seq, self.inv_freq)\n        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n        return pos_emb\n\n\nclass RelativePositionalEncoding(ModuleBase):\n    r""""""Relative positional encodings.""""""\n    def __init__(self, hparams=None):\n        ModuleBase.__init__(self, hparams)\n        self.sinusoid_embed = PositionalEmbedding(self._hparams.dim)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""dim"": 768,\n                ""max_seq_len"": 512\n            }\n\n        Here\n\n        `""dim""`: int\n            Dimension size of the positional embedding\n\n        `""max_seq_len""`: int\n            Maximum size of the sequence length\n        """"""\n        return {\n            ""name"": ""relative_positional_encoder"",\n            ""dim"": 768,\n            ""max_seq_len"": 512\n        }\n\n    def _create_positional_embedding(self, start, end, step, batch_size,\n                                     clamp_len=None):\n        pos_seq = tf.range(start, end, step)\n        if clamp_len is not None:\n            pos_seq = tf.clip_by_value(pos_seq, -clamp_len, clamp_len)\n        pos_emb = self.sinusoid_embed(pos_seq)\n        pos_emb = pos_emb[:, None, :]\n\n        if batch_size is not None:\n            pos_emb = tf.tile(pos_emb, [1, batch_size, 1])\n\n        return pos_emb\n\n    def _build(self, batch_size, max_time, total_len, clamp_len=None,\n               attn_type=\'bi\', bi_data=True):\n        r""""""Compute relative positional encoding.\n\n        Args\n            batch_size: int\n                Batch size of the input\n\n            max_time: int\n                Sequence length of the input\n\n            total_len: int\n                Sequence length + Memory length\n\n            clamp_len (optional): int\n                Clamp all relative distances larger than clamp_len.\n                None means no clamping.\n\n            attn_type (optional): str\n                Attention type. Supported values are `""uni""` and `""bi""`.\n\n            bi_data (optional): bool\n                Whether to use bidirectional data input pipeline. Usually set to\n                True during pretraining and False during finetuning.\n\n        :returns: A tensor of shape `[total_len + max_time, batch_size, dim]`\n            (if attn_type == `""bi""`) or of shape `[total_len, batch_size, dim]`\n            (if attn_type == `""uni""`) representing relative positional encoding\n            of the sequence.\n        """"""\n        if attn_type == \'bi\':\n            start, end = total_len, -max_time\n        elif attn_type == \'uni\':\n            start, end = total_len, -1\n        else:\n            raise ValueError(""Unknown `attn_type` {}"".format(attn_type))\n\n        if bi_data:\n            if batch_size % 2 != 0:\n                raise ValueError(""`batch_size` must be an even number"")\n            fwd_pos_embed = self._create_positional_embedding(\n                start, end, -1, batch_size // 2, clamp_len)\n            bwd_pos_embed = self._create_positional_embedding(\n                -start, -end, 1, batch_size // 2, clamp_len)\n            pos_embed = tf.concat([fwd_pos_embed, bwd_pos_embed], axis=1)\n        else:\n            pos_embed = self._create_positional_embedding(\n                start, end, -1, batch_size, clamp_len)\n        return pos_embed\n\n\nclass RelativeMutiheadAttention(ModuleBase):\n    r""""""Compute relative multi-head attention for XLNet encoder.\n\n    This module computes relative multi-head attention as explained in\n    `Transformer-XL, (Zihang et. al)` and in `XLNet (Zhiling et. al)`.\n\n    Args:\n        r_r_bias: A tensor of shape `(num_heads, head_dim)`.\n            The bias value added to query head while computing position based\n            attention score.\n\n        r_w_bias: A tensor of shape `(num_heads, head_dim)`.\n            The bias value added to query head while computing content based\n            attention score.\n\n        r_s_bias (optional): A tensor of shape `(num_heads, head_dim)`.\n            The bias value added to query head while computing segment based\n            attention score.\n\n        segment_embed (optional): A tensor of shape `(2, num_heads, head_dim)`\n            if use_segments is True. Otherwise, this is set to None.\n\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture\n            and default values.\n    """"""\n    def __init__(self, r_r_bias, r_w_bias, r_s_bias=None, segment_embed=None,\n                 hparams=None):\n        ModuleBase.__init__(self, hparams=hparams)\n\n        self.num_heads = self._hparams.num_heads\n        self.head_dim = self._hparams.head_dim\n        hidden_dim = self._hparams.hidden_dim\n\n        with tf.variable_scope(self.variable_scope):\n            if self._hparams.initializer:\n                tf.get_variable_scope().set_initializer(\n                    layers.get_initializer(self._hparams.initializer))\n\n            # Official implementation creates these head variables.\n            # If we create dense layers instead, there would be dimension\n            # mismatch while loading the tensors\n            # TODO(avinash) : Can we reshape tensors while loading the ckpt?\n            self.q_head = tf.get_variable(\n                \'q/kernel\', [hidden_dim, self.num_heads, self.head_dim])\n\n            self.k_head = tf.get_variable(\n                \'k/kernel\', [hidden_dim, self.num_heads, self.head_dim])\n\n            self.v_head = tf.get_variable(\n                \'v/kernel\', [hidden_dim, self.num_heads, self.head_dim])\n\n            self.k_head_r = tf.get_variable(\n                \'r/kernel\', [hidden_dim, self.num_heads, self.head_dim])\n\n            self.dropout = layers.get_layer(hparams={\n                ""type"": ""Dropout"",\n                ""kwargs"": {\n                    ""rate"": self._hparams.dropout\n                }\n            })\n\n            self.dropout_attn = layers.get_layer(hparams={\n                ""type"": ""Dropout"",\n                ""kwargs"": {\n                    ""rate"": self._hparams.attention_dropout\n                }\n            })\n\n            self.output_projection = tf.get_variable(\n                \'o/kernel\', [hidden_dim, self.num_heads, self.head_dim])\n\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n\n            if self._hparams.use_segments:\n                self.segment_embed = segment_embed\n                self.r_s_bias = r_s_bias\n\n            self.scale = 1 / (self.head_dim ** 0.5)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                ""name"": ""rel_attn"",\n                ""initializer"": None,\n                ""num_heads"": 12,\n                ""hidden_dim"": 768,\n                ""head_dim"": 64,\n                ""dropout"": 0.1,\n                ""attention_dropout"": 0.1,\n                ""use_segments"": True\n            }\n\n\n\n        Here:\n\n        The default parameters are values for cased XLNet-Base model.\n\n        ""initializer"": dict, optional\n            Hyperparameters of the default initializer that initializes\n            variables created in this module.\n            See :func:`~texar.tf.core.get_initializer` for details.\n\n        ""num_heads"": int\n            Number of heads in the attention\n\n        ""hidden_dim"": int\n            Hidden dimension of the embeddings\n\n        ""head_dim"": int\n            Size of the vectors after head projection.\n\n        ""dropout"": float\n            Dropout rate for layers\n\n        ""attention_dropout"": float\n            Dropout rate for attention layers\n\n        ""use_segments"": bool\n            Boolean to indicate if the input has segments\n\n        ""name"": str\n            Name of the module.\n        """"""\n        return {\n            ""name"": ""rel_attn"",\n            ""initializer"": None,\n            ""num_heads"": 12,\n            ""hidden_dim"": 768,\n            ""head_dim"": 64,\n            ""dropout"": 0.1,\n            ""attention_dropout"": 0.1,\n            ""use_segments"": True,\n        }\n\n    @staticmethod\n    def _rel_shift(x, klen=-1):\n        """"""Perform relative shift to form the relative attention score.""""""\n        x_size = tf.shape(x)\n\n        x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n        x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n        x = tf.slice(x, [0, 0, 0, 0], [-1, klen, -1, -1])\n\n        return x\n\n    def _compute_attention_score(self, q_head, k_head_h, v_head_h, k_head_r,\n                                 segment_mat, attn_mask=None, mode=None):\n        is_training = is_train_mode(mode)\n\n        # Content based attention score.\n        q_head_rw = q_head + self.r_w_bias\n        # attn_ac: (max_time, tot_len, batch_size, n_head)\n        attn_ac = tf.einsum(\'ibnd,jbnd->ijbn\', q_head_rw, k_head_h)\n\n        # Position based attention score.\n        q_head_rr = q_head + self.r_r_bias\n        # attn_bd: (max_time, tot_len, batch_size, n_head)\n        attn_bd = tf.einsum(\'ibnd,jbnd->ijbn\', q_head_rr, k_head_r)\n        attn_bd = self._rel_shift(attn_bd, klen=tf.shape(attn_ac)[1])\n\n        # Segment based attention score.\n        if segment_mat is None:\n            attn_ef = 0\n        else:\n            q_head_rs = q_head + self.r_s_bias\n            attn_ef = tf.einsum(\n                \'ibnd,snd->ibns\', q_head_rs, self.segment_embed)\n            attn_ef = tf.einsum(\'ijbs,ibns->ijbn\', segment_mat, attn_ef)\n\n        # Merge attention scores and perform masking.\n        # attn_score: (max_time, tot_len, batch_size, n_head)\n        attn_score = (attn_ac + attn_bd + attn_ef) * self.scale\n        if attn_mask is not None:\n            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n            attn_score = attn_score - 1e30 * attn_mask\n\n        # attention probability\n        attn_prob = tf.nn.softmax(attn_score, 1)\n        attn_prob = self.dropout_attn(attn_prob, training=is_training)\n\n        # attention output\n        attn_vec = tf.einsum(\'ijbn,jbnd->ibnd\', attn_prob, v_head_h)\n\n        return attn_vec\n\n    def _post_attention(self, attn_vec, mode=None):\n        is_training = is_train_mode(mode)\n        attn_out = tf.einsum(\'ibnd,hnd->ibh\', attn_vec, self.output_projection)\n        attn_out = self.dropout(attn_out, training=is_training)\n        return attn_out\n\n    def _build(self, states_h, pos_embed, states_g=None, segment_mat=None,\n               attn_mask_h=None, attn_mask_g=None, target_mapping=None,\n               memory=None, mode=None):\n        r""""""Compute relative multi-head attention with relative positional\n        encoding.\n\n        Args:\n            states_h: A content representation tensor of shape\n                `[max_time, batch_size, hidden_dim]`\n\n            pos_embed: Position embedding tensor of shape\n                `[max_time, batch_size, hidden_dim]`.\n\n            states_g (optional): A query representation tensor of shape\n                `[max_time, batch_size, hidden_dim]`. This tensor is set during\n                decoding.\n\n            segment_mat (optional): A tensor of size\n                `[max_time, tot_len, batch_size]` indicating if tokens are in the\n                 same seqment. A value at `(i, j, k)` of `1` indicates tokens at\n                  `i` and `j` are not in the same sequence in batch k.\n\n            attn_mask_h (optional): A tensor of shape\n                `[max_time, max_time, batch_size, 1]` Attention mask used while\n                computing attention score for `states_h`\n\n            attn_mask_g (optional): A tensor of shape\n                `[max_time, max_time, batch_size, 1]` Attention mask used while\n                computing attention score for `states_g`\n\n            target_mapping (optional): The target token mapping. Float tensor of\n                shape `[num_targets, max_time, batch_size]`.\n                A value of 1 for ``target_mapping[i, j, k]`` indicates that\n                the `i`-th target token (in order of permutation) in batch `k`\n                is the token at position `j`.\n                Each row ``target_mapping[i, :, k]`` can have no more than one\n                value of 1.\n\n            memory (optional): Memory from previous batches. A list of length\n                `num_layers`, each a tensor of shape\n                `[mem_len, batch_size, hidden_dim]`.\n\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`, dropout is\n                controlled by :func:`texar.tf.global_mode`.\n\n        :returns: Returns output states for `states_h` and `states_g`\n            (`states_g` is not None)\n        """"""\n        batch_size = tf.shape(states_h)[1]\n\n        if memory is not None and memory.shape.ndims > 1:\n            concat_input = tf.concat([memory, states_h], axis=0)\n        else:\n            concat_input = states_h\n\n        # Content heads.\n        q_head_h = tf.einsum(\'ibh,hnd->ibnd\', states_h, self.q_head)\n        k_head_h = tf.einsum(\'ibh,hnd->ibnd\', concat_input, self.k_head)\n        v_head_h = tf.einsum(\'ibh,hnd->ibnd\', concat_input, self.v_head)\n\n        # Positional heads.\n        k_head_r = tf.einsum(\'ibh,hnd->ibnd\', pos_embed, self.k_head_r)\n\n        # Core attention ops.\n        attn_vec_h = self._compute_attention_score(\n            q_head_h, k_head_h, v_head_h, k_head_r, segment_mat, attn_mask_h,\n            mode)\n\n        # Post attention processing.\n        attn_out_h = self._post_attention(attn_vec_h, mode=mode)\n\n        output_h = tf.contrib.layers.layer_norm(\n            attn_out_h + states_h, begin_norm_axis=-1,\n            scope=self.variable_scope, reuse=tf.AUTO_REUSE)\n\n        if states_g is not None:\n            q_head_g = tf.einsum(\'ibh,hnd->ibnd\', states_g, self.q_head)\n            shape = tf.shape(q_head_g)\n            q_head_g = tf.reshape(\n                q_head_g,\n                shape=(shape[0], batch_size, self.num_heads, self.head_dim))\n            if target_mapping is not None:\n                q_head_g = tf.einsum(\n                    \'mbnd,mlb->lbnd\', q_head_g, target_mapping)\n            attn_vec_g = self._compute_attention_score(\n                q_head_g, k_head_h, v_head_h, k_head_r,\n                segment_mat, attn_mask_g, mode)\n            if target_mapping is not None:\n                attn_vec_g = tf.einsum(\n                    \'lbnd,mlb->mbnd\', attn_vec_g, target_mapping)\n            attn_out_g = self._post_attention(attn_vec_g, mode=mode)\n            output_g = tf.contrib.layers.layer_norm(\n                attn_out_g + states_g, begin_norm_axis=-1,\n                scope=self.variable_scope, reuse=tf.AUTO_REUSE)\n        else:\n            output_g = None\n\n        return output_h, output_g\n'"
texar/tf/modules/qnets/__init__.py,1,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library qnets.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.qnets.qnets import *\n'"
texar/tf/modules/qnets/qnets.py,13,"b'# Copyright 2018 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Q networks for RL.\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom texar.tf.module_base import ModuleBase\nfrom texar.tf.agents.agent_utils import Space\nfrom texar.tf.utils import utils\n\n# pylint: disable=no-member\n\n__all__ = [\n    \'QNetBase\',\n    \'CategoricalQNet\'\n]\n\n\nclass QNetBase(ModuleBase):\n    """"""Base class inheritted by all Q net classes. A Q net takes in states\n    and outputs Q value of actions.\n\n    Args:\n        network (optional): A network that takes in state and returns\n            Q values. For example, an instance of subclass\n            of :class:`~texar.tf.modules.FeedForwardNetworkBase`. If `None`,\n            a network is created as specified in :attr:`hparams`.\n        network_kwargs (dict, optional): Keyword arguments for network\n            constructor.\n            Note that the `hparams` argument for network\n            constructor is specified in the ""network_hparams"" field of\n            :attr:`hparams` and should not be included in `network_kwargs`.\n            Ignored if :attr:`network` is given.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n    """"""\n    def __init__(self,\n                 network=None,\n                 network_kwargs=None,\n                 hparams=None):\n        ModuleBase.__init__(self, hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            self._build_network(network, network_kwargs)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. role:: python(code)\n           :language: python\n\n        .. code-block:: python\n\n            {\n                \'network_type\': \'FeedForwardNetwork\',\n                \'network_hparams\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                    ]\n                },\n                \'name\': \'q_net\',\n            }\n\n        Here:\n\n        ""network_type"": str or class or instance\n            A network that takes in state and returns outputs for\n            generating actions. This can be a class, its name or module path,\n            or a class instance. Ignored if `network` is given to the\n            constructor.\n\n        ""network_hparams"": dict\n            Hyperparameters for the network. With the :attr:`network_kwargs`\n            argument to the constructor, a network is created with\n            :python:`network_class(**network_kwargs, hparams=network_hparams)`.\n\n            For example, the default values creates a two-layer dense network.\n\n        ""name"": str\n            Name of the Q net.\n        """"""\n        return {\n            \'network_type\': \'FeedForwardNetwork\',\n            \'network_hparams\': {\n                \'layers\': [\n                    {\n                        \'type\': \'Dense\',\n                        \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                    },\n                    {\n                        \'type\': \'Dense\',\n                        \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                    },\n                ]\n            },\n            \'name\': \'q_net\',\n            \'@no_typecheck\': [\'network_type\', \'network_hparams\']\n        }\n\n    def _build_network(self, network, kwargs):\n        if network is not None:\n            self._network = network\n        else:\n            kwargs = utils.get_instance_kwargs(\n                kwargs, self._hparams.network_hparams)\n            self._network = utils.check_or_get_instance(\n                self._hparams.network_type,\n                kwargs,\n                module_paths=[\'texar.tf.modules\', \'texar.tf.custom\'])\n\n    def _build(self, inputs, mode=None):  # pylint: disable=arguments-differ\n        raise NotImplementedError\n\n    @property\n    def network(self):\n        """"""The network.\n        """"""\n        return self._network\n\n\nclass CategoricalQNet(QNetBase):\n    """"""Q net with categorical scalar action space.\n\n    Args:\n        action_space (optional): An instance of :class:`~texar.tf.agents.Space`\n            specifying the action space. If not given, an discrete action space\n            `[0, high]` is created with `high` specified in :attr:`hparams`.\n        network (optional): A network that takes in state and returns\n            Q values. For example, an instance of subclass\n            of :class:`~texar.tf.modules.FeedForwardNetworkBase`. If `None`,\n            a network is created as specified in :attr:`hparams`.\n        network_kwargs (dict, optional): Keyword arguments for network\n            constructor.\n            Note that the `hparams` argument for network\n            constructor is specified in the ""network_hparams"" field of\n            :attr:`hparams` and should not be included in `network_kwargs`.\n            Ignored if :attr:`network` is given.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparamerter will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter sturcture and\n            default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n    def __init__(self,\n                 action_space=None,\n                 network=None,\n                 network_kwargs=None,\n                 hparams=None):\n        QNetBase.__init__(self, hparams=hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            if action_space is None:\n                action_space = Space(\n                    low=0, high=self._hparams.action_space, dtype=np.int32)\n            self._action_space = action_space\n            self._append_output_layer()\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                \'network_type\': \'FeedForwardNetwork\',\n                \'network_hparams\': {\n                    \'layers\': [\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                        {\n                            \'type\': \'Dense\',\n                            \'kwargs\': {\'units\': 256, \'activation\': \'relu\'}\n                        },\n                    ]\n                },\n                \'action_space\': 2,\n                \'make_output_layer\': True,\n                \'name\': \'q_net\'\n            }\n\n        Here:\n\n        ""action_space"": int\n            Upper bound of the action space. The resulting action space is\n            all discrete scalar numbers between 0 and the upper bound specified\n            here (both inclusive).\n\n        ""make_output_layer"": bool\n            Whether to append a dense layer to the network to transform\n            features to Q values. If `False`, the final layer\n            output of network must match the action space.\n\n        See :class:`~texar.tf.modules.QNetBase.default_hparams` for details\n        of other hyperparameters.\n        """"""\n        hparams = QNetBase.default_hparams()\n        hparams.update({\n            \'action_space\': 2,\n            \'make_output_layer\': True})\n        return hparams\n\n    def _append_output_layer(self):\n        if not self._hparams.make_output_layer:\n            return\n\n        if self._action_space.shape != ():\n            raise ValueError(\'Only scalar discrete action is supported.\')\n        else:\n            output_size = self._action_space.high - self._action_space.low\n\n        layer_hparams = {\n            \'type\': \'Dense\',\n            \'kwargs\': {\'units\': output_size}}\n        self._network.append_layer(layer_hparams)\n\n    def _build(self, inputs, mode=None):\n        """"""Takes in states and outputs Q values.\n\n        Args:\n            inputs: Inputs to the Q net with the first dimension\n                the batch dimension.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`, including\n                `TRAIN`, `EVAL`, and `PREDICT`. If `None`,\n                :func:`texar.tf.global_mode` is used.\n\n        Returns\n            A `dict` including fields `""qvalues""`.\n            where\n\n            - **""qvalues""**: A Tensor of shape \\\n            `[batch_size] + action_space size` containing Q values of all\\\n            possible actions.\n        """"""\n        outputs = {\n            ""qvalues"": self._network(inputs, mode=mode)\n        }\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            self._add_trainable_variable(self._network.trainable_variables)\n            self._built = True\n\n        return outputs\n\n    @property\n    def action_space(self):\n        """"""An instance of :class:`~texar.tf.agents.Space` specifiying the\n        action space.\n        """"""\n        return self._action_space\n'"
texar/tf/modules/regressors/__init__.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nModules of texar library regressors.\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom texar.tf.modules.regressors.xlnet_regressor import *\n'"
texar/tf/modules/regressors/regressor_base.py,1,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nBase class for Regressors.\n""""""\n\nfrom texar.tf.module_base import ModuleBase\n\n__all__ = [\n    ""RegressorBase""\n]\n\n\nclass RegressorBase(ModuleBase):\n    """"""Base class inherited by all regressor classes.\n    """"""\n\n    def __init__(self, hparams=None):\n        ModuleBase.__init__(self, hparams)\n\n    @staticmethod\n    def default_hparams():\n        """"""Returns a dictionary of hyperparameters with default values.\n        """"""\n        return {\n            ""name"": ""regressor""\n        }\n\n    def _build(self, inputs, *args, **kwargs):\n        """"""Runs regressors on inputs.\n\n        Args:\n          inputs: Inputs to the regressor.\n          *args: Other arguments.\n          **kwargs: Keyword arguments.\n\n        Returns:\n          Regression output.\n        """"""\n        raise NotImplementedError\n'"
texar/tf/modules/regressors/xlnet_regressor.py,24,"b'# Copyright 2019 The Texar Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""\nXLNet Regressor.\n""""""\n\nimport tensorflow as tf\n\nfrom texar.tf.utils.mode import is_train_mode\nfrom texar.tf.core.layers import get_layer, get_initializer\nfrom texar.tf.modules.regressors.regressor_base import RegressorBase\nfrom texar.tf.modules.encoders.xlnet_encoder import XLNetEncoder\nfrom texar.tf.hyperparams import HParams\nfrom texar.tf.modules.pretrained.xlnet import PretrainedXLNetMixin\nfrom texar.tf.utils.utils import dict_fetch\n\n# pylint: disable=too-many-arguments, invalid-name, no-member,\n# pylint: disable=too-many-branches, too-many-locals, too-many-statements\n\n__all__ = [\n    ""XLNetRegressor""\n]\n\n\nclass XLNetRegressor(RegressorBase, PretrainedXLNetMixin):\n    """"""Regressor based on XLNet modules. Please see\n    :class:`~texar.tf.modules.PretrainedXLNetMixin` for a brief description\n    of XLNet.\n\n    This is a combination of the :class:`~texar.tf.modules.XLNetEncoder` with a\n    classification layer. Both step-wise classification and sequence-level\n    classification are supported, specified in :attr:`hparams`.\n\n    Arguments are the same as in :class:`~texar.tf.modules.XLNetEncoder`.\n\n    Args:\n        pretrained_model_name (optional): a `str`, the name\n            of pre-trained model (e.g., ``xlnet-based-cased``). Please refer to\n            :class:`~texar.tf.modules.PretrainedXLNetMixin` for\n            all supported models.\n            If `None`, the model name in :attr:`hparams` is used.\n        cache_dir (optional): the path to a folder in which the\n            pre-trained models will be cached. If `None` (default),\n            a default directory (``texar_data`` folder under user\'s home\n            directory) will be used.\n        hparams (dict or HParams, optional): Hyperparameters. Missing\n            hyperparameters will be set to default values. See\n            :meth:`default_hparams` for the hyperparameter structure\n            and default values.\n\n    .. document private functions\n    .. automethod:: _build\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name=None,\n                 cache_dir=None,\n                 hparams=None):\n        RegressorBase.__init__(self, hparams)\n\n        with tf.variable_scope(self.variable_scope):\n            tf.get_variable_scope().set_initializer(\n                get_initializer(self._hparams.initializer))\n            # Creates the underlying encoder\n            encoder_hparams = dict_fetch(\n                hparams, XLNetEncoder.default_hparams())\n            if encoder_hparams is not None:\n                encoder_hparams[\'name\'] = ""encoder""\n            self._encoder = XLNetEncoder(\n                pretrained_model_name=pretrained_model_name,\n                cache_dir=cache_dir,\n                hparams=encoder_hparams)\n            if self._hparams.use_projection:\n                self.projection = get_layer(hparams={\n                    ""type"": ""Dense"",\n                    ""kwargs"": {\n                        ""units"": self._encoder.output_size\n                    }\n                })\n\n            # Creates an dropout layer\n            drop_kwargs = {""rate"": self._hparams.dropout}\n            layer_hparams = {""type"": ""Dropout"", ""kwargs"": drop_kwargs}\n            self._dropout_layer = get_layer(hparams=layer_hparams)\n\n            logit_kwargs = self._hparams.logit_layer_kwargs\n            if logit_kwargs is None:\n                logit_kwargs = {}\n            elif not isinstance(logit_kwargs, HParams):\n                raise ValueError(\n                    ""hparams[\'logit_layer_kwargs\'] must be a dict."")\n            else:\n                logit_kwargs = logit_kwargs.todict()\n            logit_kwargs.update({""units"": 1})\n            if \'name\' not in logit_kwargs:\n                logit_kwargs[\'name\'] = ""logit_layer""\n\n            layer_hparams = {""type"": ""Dense"", ""kwargs"": logit_kwargs}\n            self._logit_layer = get_layer(hparams=layer_hparams)\n\n    @staticmethod\n    def default_hparams():\n        r""""""Returns a dictionary of hyperparameters with default values.\n\n        .. code-block:: python\n\n            {\n                # (1) Same hyperparameters as in XLNetEncoder\n                ...\n                # (2) Additional hyperparameters\n                ""regr_strategy"": ""cls_time"",\n                ""use_projection"": True,\n                ""logit_layer_kwargs"": None,\n                ""name"": ""xlnet_regressor"",\n            }\n\n        Here:\n\n        1. Same hyperparameters as in\n           :class:`~texar.tf.modules.XLNetEncoder`.\n           See the :meth:`~texar.tf.modules.XLNetEncoder.default_hparams`.\n           An instance of XLNetEncoder is created for feature extraction.\n\n        2. Additional hyperparameters:\n\n            `""regr_strategy""`: str\n                The regression strategy, one of:\n\n                - **cls_time**: Sequence-level regression based on the\n                  output of the first time step (which is the `CLS` token).\n                  Each sequence has a prediction.\n                - **all_time**: Sequence-level regression based on\n                  the output of all time steps. Each sequence has a prediction.\n                - **time_wise**: Step-wise regression, i.e., make\n                  regression for each time step based on its output.\n\n            `""logit_layer_kwargs""` : dict\n                Keyword arguments for the logit Dense layer constructor,\n                except for argument ""units"" which is set to ""num_classes"".\n                Ignored if no extra logit layer is appended.\n\n            `""use_projection""`: bool\n                If `True`, an additional dense layer is added after\n                the summary step.\n\n            `""name""`: str\n                Name of the regressor.\n        """"""\n        hparams = XLNetEncoder.default_hparams()\n        hparams.update({\n            ""logit_layer_kwargs"": None,\n            ""regr_strategy"": ""cls_time"",\n            ""dropout"": 0.1,\n            ""use_projection"": True,\n            ""name"": ""xlnet_regressor""\n        })\n        return hparams\n\n    def param_groups(self, lr=None, lr_layer_scale=1.0,\n                     decay_base_params=False):\n        r""""""Create parameter groups for optimizers. When\n        :attr:`lr_layer_decay_rate` is not 1.0, parameters from each layer form\n        separate groups with different base learning rates.\n\n        This method should be called before applying gradients to the variables\n        through the optimizer. Particularly, after calling the optimizer\'s\n        `compute_gradients` method, the user can call this method to get\n        variable-specific learning rates for the network. The gradients for each\n        variables can then be scaled accordingly. These scaled gradients are\n        finally applied by calling optimizer\'s `apply_gradients` method.\n\n        Args:\n            lr (float): The learning rate. Can be omitted if\n                :attr:`lr_layer_decay_rate` is 1.0.\n            lr_layer_scale (float): Per-layer LR scaling rate. The `i`-th layer\n                will be scaled by `lr_layer_scale ^ (num_layers - i - 1)`.\n            decay_base_params (bool): If `True`, treat non-layer parameters\n                (e.g. embeddings) as if they\'re in layer 0. If `False`, these\n                parameters are not scaled.\n\n        Returns: A dict mapping tensorflow variables to their learning rates.\n        """"""\n        vars_to_learning_rates = {}\n        if lr_layer_scale != 1.0:\n            if lr is None:\n                raise ValueError(\n                    ""lr must be specified when lr_layer_decay_rate is not 1.0"")\n\n            scope = self.variable_scope.name\n            projection_vars = tf.trainable_variables(scope=scope + ""/dense"")\n            logits_vars = tf.trainable_variables(\n                scope=self.variable_scope.name + ""/logit_layer"")\n            finetune_vars = projection_vars + logits_vars\n            for var in finetune_vars:\n                vars_to_learning_rates[var] = lr\n\n            vars_to_learning_rates.update(\n                self._encoder.param_groups(lr=lr,\n                                           lr_layer_scale=lr_layer_scale,\n                                           decay_base_params=decay_base_params))\n        else:\n            for variable in self.trainable_variables:\n                vars_to_learning_rates[variable] = lr\n\n        return vars_to_learning_rates\n\n    def _build(self, token_ids, segment_ids=None, input_mask=None, mode=None):\n        r""""""Feeds the inputs through the network and makes regression.\n\n        Args:\n            token_ids: Shape `[batch_size, max_time]`.\n            segment_ids: Shape `[batch_size, max_time]`.\n            input_mask: Float tensor of shape `[batch_size, max_time]`. Note\n                that positions with value 1 are masked out.\n            mode (optional): A tensor taking value in\n                :tf_main:`tf.estimator.ModeKeys <estimator/ModeKeys>`,\n                including `TRAIN`, `EVAL`, and `PREDICT`. Used to toggle\n                dropout.\n                If `None` (default), :func:`texar.tf.global_mode` is used.\n\n        Returns:\n            Regression predictions.\n\n            - If ``regr_strategy`` is ``cls_time`` or ``all_time``, predictions\n              have shape `[batch_size]`.\n\n            - If ``clas_strategy`` is ``time_wise``, predictions have shape\n              `[batch_size, max_time]`.\n        """"""\n        is_training = is_train_mode(mode)\n        output, _ = self._encoder(token_ids, segment_ids, input_mask=input_mask,\n                                  mode=mode)\n\n        strategy = self._hparams.regr_strategy\n        if strategy == ""time_wise"":\n            summary = output\n        elif strategy == ""cls_time"":\n            summary = output[:, -1]\n        elif strategy == ""all_time"":\n            length_diff = self._hparams.max_seq_len - tf.shape(token_ids)[1]\n            summary_input = tf.pad(output,\n                                   paddings=[[0, 0], [0, length_diff], [0, 0]])\n            summary_input_dim = \\\n                self._encoder.output_size * self._hparams.max_seq_len\n            summary = tf.reshape(summary_input, shape=[-1, summary_input_dim])\n        else:\n            raise ValueError(""Unknown classification strategy: {}"".\n                             format(strategy))\n\n        if self._hparams.use_projection:\n            summary = tf.tanh(self.projection(summary))\n\n        # summary: (batch_size, hidden_dim)\n        summary = self._dropout_layer(summary, training=is_training)\n\n        logits = tf.squeeze(self._logit_layer(summary), -1)\n\n        if not self._built:\n            self._add_internal_trainable_variables()\n            if self._logit_layer:\n                self._add_trainable_variable(\n                    self._logit_layer.trainable_variables)\n            self._built = True\n\n        return logits\n'"
