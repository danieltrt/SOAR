file_path,api_count,code
eval_ckpt_file.py,6,"b""import tensorflow as tf\nimport argparse\nfrom data.eval_data_reader import load_bin\nfrom losses.face_losses import arcface_loss\nfrom nets.L_Resnet_E_IR import get_resnet\nimport tensorlayer as tl\nfrom verification import ver_test\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='input information')\n    parser.add_argument('--ckpt_file', default='/home/aurora/workspaces2018/InsightFace_TF/output/ckpt_model_c/InsightFace_iter_best_',\n                       type=str, help='the ckpt file path')\n    # parser.add_argument('--eval_datasets', default=['lfw', 'cfp_ff', 'cfp_fp', 'agedb_30'], help='evluation datasets')\n    parser.add_argument('--eval_datasets', default=['agedb_30'], help='evluation datasets')\n    parser.add_argument('--eval_db_path', default='./datasets/faces_ms1m_112x112', help='evluate datasets base path')\n    parser.add_argument('--image_size', default=[112, 112], help='the image size')\n    parser.add_argument('--net_depth', default=50, help='resnet depth, default is 50')\n    parser.add_argument('--num_output', default=85164, help='the image size')\n    parser.add_argument('--batch_size', default=32, help='batch size to train network')\n    parser.add_argument('--ckpt_index_list',\n                        default=['1950000.ckpt'], help='ckpt file indexes')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = get_args()\n    ver_list = []\n    ver_name_list = []\n    for db in args.eval_datasets:\n        print('begin db %s convert.' % db)\n        data_set = load_bin(db, args.image_size, args)\n        ver_list.append(data_set)\n        ver_name_list.append(db)\n\n    images = tf.placeholder(name='img_inputs', shape=[None, *args.image_size, 3], dtype=tf.float32)\n    labels = tf.placeholder(name='img_labels', shape=[None, ], dtype=tf.int64)\n    dropout_rate = tf.placeholder(name='dropout_rate', dtype=tf.float32)\n\n    w_init_method = tf.contrib.layers.xavier_initializer(uniform=False)\n    net = get_resnet(images, args.net_depth, type='ir', w_init=w_init_method, trainable=False, keep_rate=dropout_rate)\n    embedding_tensor = net.outputs\n    # mv_mean = tl.layers.get_variables_with_name('resnet_v1_50/bn0/moving_mean', False, True)[0]\n    # 3.2 get arcface loss\n    logit = arcface_loss(embedding=net.outputs, labels=labels, w_init=w_init_method, out_num=args.num_output)\n\n    sess = tf.Session()\n    saver = tf.train.Saver()\n\n    result_index = []\n    for file_index in args.ckpt_index_list:\n        feed_dict_test = {}\n        path = args.ckpt_file + file_index\n        saver.restore(sess, path)\n        print('ckpt file %s restored!' % file_index)\n        feed_dict_test.update(tl.utils.dict_to_one(net.all_drop))\n        feed_dict_test[dropout_rate] = 1.0\n        results = ver_test(ver_list=ver_list, ver_name_list=ver_name_list, nbatch=0, sess=sess,\n                           embedding_tensor=embedding_tensor, batch_size=args.batch_size, feed_dict=feed_dict_test,\n                           input_placeholder=images)\n        result_index.append(results)\n    print(result_index)\n\n"""
train_nets.py,40,"b'import tensorflow as tf\nimport tensorlayer as tl\nimport argparse\nfrom data.mx2tfrecords import parse_function\nimport os\n# from nets.L_Resnet_E_IR import get_resnet\n# from nets.L_Resnet_E_IR_GBN import get_resnet\nfrom nets.L_Resnet_E_IR_fix_issue9 import get_resnet\nfrom losses.face_losses import arcface_loss\nfrom tensorflow.core.protobuf import config_pb2\nimport time\nfrom data.eval_data_reader import load_bin\nfrom verification import ver_test\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\'parameters to train net\')\n    parser.add_argument(\'--net_depth\', default=100, help=\'resnet depth, default is 50\')\n    parser.add_argument(\'--epoch\', default=100000, help=\'epoch to train the network\')\n    parser.add_argument(\'--batch_size\', default=32, help=\'batch size to train network\')\n    parser.add_argument(\'--lr_steps\', default=[40000, 60000, 80000], help=\'learning rate to train network\')\n    parser.add_argument(\'--momentum\', default=0.9, help=\'learning alg momentum\')\n    parser.add_argument(\'--weight_deacy\', default=5e-4, help=\'learning alg momentum\')\n    # parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_ff\', \'cfp_fp\', \'agedb_30\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_datasets\', default=[\'lfw\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_db_path\', default=\'./datasets/faces_ms1m_112x112\', help=\'evluate datasets base path\')\n    parser.add_argument(\'--image_size\', default=[112, 112], help=\'the image size\')\n    parser.add_argument(\'--num_output\', default=85164, help=\'the image size\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'./datasets/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    parser.add_argument(\'--summary_path\', default=\'./output/summary\', help=\'the summary file save path\')\n    parser.add_argument(\'--ckpt_path\', default=\'./output/ckpt\', help=\'the ckpt file save path\')\n    parser.add_argument(\'--log_file_path\', default=\'./output/logs\', help=\'the ckpt file save path\')\n    parser.add_argument(\'--saver_maxkeep\', default=100, help=\'tf.train.Saver max keep ckpt files\')\n    parser.add_argument(\'--buffer_size\', default=10000, help=\'tf dataset api buffer size\')\n    parser.add_argument(\'--log_device_mapping\', default=False, help=\'show device placement log\')\n    parser.add_argument(\'--summary_interval\', default=300, help=\'interval to save summary\')\n    parser.add_argument(\'--ckpt_interval\', default=10000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--validate_interval\', default=2000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--show_info_interval\', default=20, help=\'intervals to save ckpt file\')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    # 1. define global parameters\n    args = get_parser()\n    global_step = tf.Variable(name=\'global_step\', initial_value=0, trainable=False)\n    inc_op = tf.assign_add(global_step, 1, name=\'increment_global_step\')\n    images = tf.placeholder(name=\'img_inputs\', shape=[None, *args.image_size, 3], dtype=tf.float32)\n    labels = tf.placeholder(name=\'img_labels\', shape=[None, ], dtype=tf.int64)\n    # trainable = tf.placeholder(name=\'trainable_bn\', dtype=tf.bool)\n    dropout_rate = tf.placeholder(name=\'dropout_rate\', dtype=tf.float32)\n    # 2 prepare train datasets and test datasets by using tensorflow dataset api\n    # 2.1 train datasets\n    # the image is substracted 127.5 and multiplied 1/128.\n    # random flip left right\n    tfrecords_f = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    dataset = tf.data.TFRecordDataset(tfrecords_f)\n    dataset = dataset.map(parse_function)\n    dataset = dataset.shuffle(buffer_size=args.buffer_size)\n    dataset = dataset.batch(args.batch_size)\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    # 2.2 prepare validate datasets\n    ver_list = []\n    ver_name_list = []\n    for db in args.eval_datasets:\n        print(\'begin db %s convert.\' % db)\n        data_set = load_bin(db, args.image_size, args)\n        ver_list.append(data_set)\n        ver_name_list.append(db)\n    # 3. define network, loss, optimize method, learning rate schedule, summary writer, saver\n    # 3.1 inference phase\n    w_init_method = tf.contrib.layers.xavier_initializer(uniform=False)\n    net = get_resnet(images, args.net_depth, type=\'ir\', w_init=w_init_method, trainable=True, keep_rate=dropout_rate)\n    # 3.2 get arcface loss\n    logit = arcface_loss(embedding=net.outputs, labels=labels, w_init=w_init_method, out_num=args.num_output)\n    # test net  because of batch normal layer\n    tl.layers.set_name_reuse(True)\n    test_net = get_resnet(images, args.net_depth, type=\'ir\', w_init=w_init_method, trainable=False, reuse=True, keep_rate=dropout_rate)\n    embedding_tensor = test_net.outputs\n    # 3.3 define the cross entropy\n    inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels))\n    # inference_loss_avg = tf.reduce_mean(inference_loss)\n    # 3.4 define weight deacy losses\n    # for var in tf.trainable_variables():\n    #     print(var.name)\n    # print(\'##########\'*30)\n    wd_loss = 0\n    for weights in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n        wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(weights)\n    for W in tl.layers.get_variables_with_name(\'resnet_v1_50/E_DenseLayer/W\', True, True):\n        wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(W)\n    for weights in tl.layers.get_variables_with_name(\'embedding_weights\', True, True):\n        wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(weights)\n    for gamma in tl.layers.get_variables_with_name(\'gamma\', True, True):\n        wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(gamma)\n    # for beta in tl.layers.get_variables_with_name(\'beta\', True, True):\n    #     wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(beta)\n    for alphas in tl.layers.get_variables_with_name(\'alphas\', True, True):\n        wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(alphas)\n    # for bias in tl.layers.get_variables_with_name(\'resnet_v1_50/E_DenseLayer/b\', True, True):\n    #     wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(bias)\n\n    # 3.5 total losses\n    total_loss = inference_loss + wd_loss\n    # 3.6 define the learning rate schedule\n    p = int(512.0/args.batch_size)\n    lr_steps = [p*val for val in args.lr_steps]\n    print(lr_steps)\n    lr = tf.train.piecewise_constant(global_step, boundaries=lr_steps, values=[0.001, 0.0005, 0.0003, 0.0001], name=\'lr_schedule\')\n    # 3.7 define the optimize method\n    opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=args.momentum)\n    # 3.8 get train op\n    grads = opt.compute_gradients(total_loss)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = opt.apply_gradients(grads, global_step=global_step)\n    # train_op = opt.minimize(total_loss, global_step=global_step)\n    # 3.9 define the inference accuracy used during validate or test\n    pred = tf.nn.softmax(logit)\n    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, axis=1), labels), dtype=tf.float32))\n    # 3.10 define sess\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=args.log_device_mapping)\n    config.gpu_options.allow_growth = True\n\n    sess = tf.Session(config=config)\n    # 3.11 summary writer\n    summary = tf.summary.FileWriter(args.summary_path, sess.graph)\n    summaries = []\n    # # 3.11.1 add grad histogram op\n    for grad, var in grads:\n        if grad is not None:\n            summaries.append(tf.summary.histogram(var.op.name + \'/gradients\', grad))\n    # 3.11.2 add trainabel variable gradients\n    for var in tf.trainable_variables():\n        summaries.append(tf.summary.histogram(var.op.name, var))\n    # 3.11.3 add loss summary\n    summaries.append(tf.summary.scalar(\'inference_loss\', inference_loss))\n    summaries.append(tf.summary.scalar(\'wd_loss\', wd_loss))\n    summaries.append(tf.summary.scalar(\'total_loss\', total_loss))\n    # 3.11.4 add learning rate\n    summaries.append(tf.summary.scalar(\'leraning_rate\', lr))\n    summary_op = tf.summary.merge(summaries)\n    # 3.12 saver\n    saver = tf.train.Saver(max_to_keep=args.saver_maxkeep)\n    # 3.13 init all variables\n    sess.run(tf.global_variables_initializer())\n\n    # restore_saver = tf.train.Saver()\n    # restore_saver.restore(sess, \'/home/aurora/workspaces2018/InsightFace_TF/output/ckpt/InsightFace_iter_1110000.ckpt\')\n    # 4 begin iteration\n    if not os.path.exists(args.log_file_path):\n        os.makedirs(args.log_file_path)\n    log_file_path = args.log_file_path + \'/train\' + time.strftime(\'_%Y-%m-%d-%H-%M\', time.localtime(time.time())) + \'.log\'\n    log_file = open(log_file_path, \'w\')\n    # 4 begin iteration\n    count = 0\n    total_accuracy = {}\n\n    for i in range(args.epoch):\n        sess.run(iterator.initializer)\n        while True:\n            try:\n                images_train, labels_train = sess.run(next_element)\n                feed_dict = {images: images_train, labels: labels_train, dropout_rate: 0.4}\n                feed_dict.update(net.all_drop)\n                start = time.time()\n                _, total_loss_val, inference_loss_val, wd_loss_val, _, acc_val = \\\n                    sess.run([train_op, total_loss, inference_loss, wd_loss, inc_op, acc],\n                              feed_dict=feed_dict,\n                              options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))\n                end = time.time()\n                pre_sec = args.batch_size/(end - start)\n                # print training information\n                if count > 0 and count % args.show_info_interval == 0:\n                    print(\'epoch %d, total_step %d, total loss is %.2f , inference loss is %.2f, weight deacy \'\n                          \'loss is %.2f, training accuracy is %.6f, time %.3f samples/sec\' %\n                          (i, count, total_loss_val, inference_loss_val, wd_loss_val, acc_val, pre_sec))\n                count += 1\n\n                # save summary\n                if count > 0 and count % args.summary_interval == 0:\n                    feed_dict = {images: images_train, labels: labels_train, dropout_rate: 0.4}\n                    feed_dict.update(net.all_drop)\n                    summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n                    summary.add_summary(summary_op_val, count)\n\n                # save ckpt files\n                if count > 0 and count % args.ckpt_interval == 0:\n                    filename = \'InsightFace_iter_{:d}\'.format(count) + \'.ckpt\'\n                    filename = os.path.join(args.ckpt_path, filename)\n                    saver.save(sess, filename)\n\n                # validate\n                if count > 0 and count % args.validate_interval == 0:\n                    feed_dict_test ={dropout_rate: 1.0}\n                    feed_dict_test.update(tl.utils.dict_to_one(net.all_drop))\n                    results = ver_test(ver_list=ver_list, ver_name_list=ver_name_list, nbatch=count, sess=sess,\n                             embedding_tensor=embedding_tensor, batch_size=args.batch_size, feed_dict=feed_dict_test,\n                             input_placeholder=images)\n                    print(\'test accuracy is: \', str(results[0]))\n                    total_accuracy[str(count)] = results[0]\n                    log_file.write(\'########\'*10+\'\\n\')\n                    log_file.write(\',\'.join(list(total_accuracy.keys())) + \'\\n\')\n                    log_file.write(\',\'.join([str(val) for val in list(total_accuracy.values())])+\'\\n\')\n                    log_file.flush()\n                    if max(results) > 0.996:\n                        print(\'best accuracy is %.5f\' % max(results))\n                        filename = \'InsightFace_iter_best_{:d}\'.format(count) + \'.ckpt\'\n                        filename = os.path.join(args.ckpt_path, filename)\n                        saver.save(sess, filename)\n                        log_file.write(\'######Best Accuracy######\'+\'\\n\')\n                        log_file.write(str(max(results))+\'\\n\')\n                        log_file.write(filename+\'\\n\')\n\n                        log_file.flush()\n            except tf.errors.OutOfRangeError:\n                print(""End of epoch %d"" % i)\n                break\n    log_file.close()\n    log_file.write(\'\\n\')'"
train_nets_mgpu.py,43,"b'import tensorflow as tf\nimport tensorlayer as tl\nimport argparse\nfrom data.mx2tfrecords import parse_function\nimport os\nfrom nets.L_Resnet_E_IR_MGPU import get_resnet\nfrom losses.face_losses import arcface_loss\nimport time\nfrom data.eval_data_reader import load_bin\nfrom verification import ver_test\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\'parameters to train net\')\n    parser.add_argument(\'--net_depth\', default=50, help=\'resnet depth, default is 50\')\n    parser.add_argument(\'--epoch\', default=100000, help=\'epoch to train the network\')\n    parser.add_argument(\'--batch_size\', default=32, help=\'batch size to train network\')\n    parser.add_argument(\'--lr_steps\', default=[40000, 60000, 80000], help=\'learning rate to train network\')\n    parser.add_argument(\'--momentum\', default=0.9, help=\'learning alg momentum\')\n    parser.add_argument(\'--weight_deacy\', default=5e-4, help=\'learning alg momentum\')\n    # parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_ff\', \'cfp_fp\', \'agedb_30\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_fp\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_db_path\', default=\'./datasets/faces_ms1m_112x112\', help=\'evluate datasets base path\')\n    parser.add_argument(\'--image_size\', default=[112, 112], help=\'the image size\')\n    parser.add_argument(\'--num_output\', default=85164, help=\'the image size\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'./datasets/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    parser.add_argument(\'--summary_path\', default=\'./output/summary\', help=\'the summary file save path\')\n    parser.add_argument(\'--ckpt_path\', default=\'./output/ckpt\', help=\'the ckpt file save path\')\n    parser.add_argument(\'--saver_maxkeep\', default=100, help=\'tf.train.Saver max keep ckpt files\')\n    parser.add_argument(\'--buffer_size\', default=50000, help=\'tf dataset api buffer size\')\n    parser.add_argument(\'--log_device_mapping\', default=False, help=\'show device placement log\')\n    parser.add_argument(\'--summary_interval\', default=300, help=\'interval to save summary\')\n    parser.add_argument(\'--ckpt_interval\', default=5000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--validate_interval\', default=2000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--show_info_interval\', default=20, help=\'intervals to show information\')\n    parser.add_argument(\'--num_gpus\', default=2, help=\'the num of gpus\')\n    parser.add_argument(\'--tower_name\', default=\'tower\', help=\'tower name\')\n    args = parser.parse_args()\n    return args\n\n\ndef average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\nif __name__ == \'__main__\':\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    # 1. define global parameters\n    args = get_parser()\n    global_step = tf.Variable(name=\'global_step\', initial_value=0, trainable=False)\n    inc_op = tf.assign_add(global_step, 1, name=\'increment_global_step\')\n    trainable = tf.placeholder(name=\'trainable_bn\', dtype=tf.bool)\n    images = tf.placeholder(name=\'img_inputs\', shape=[None, *args.image_size, 3], dtype=tf.float32)\n    labels = tf.placeholder(name=\'img_labels\', shape=[None, ], dtype=tf.int64)\n    # splits input to different gpu\n    images_s = tf.split(images, num_or_size_splits=args.num_gpus, axis=0)\n    labels_s = tf.split(labels, num_or_size_splits=args.num_gpus, axis=0)\n    # 2 prepare train datasets and test datasets by using tensorflow dataset api\n    # 2.1 train datasets\n    # the image is substracted 127.5 and multiplied 1/128.\n    # random flip left right\n    tfrecords_f = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    dataset = tf.data.TFRecordDataset(tfrecords_f)\n    dataset = dataset.map(parse_function)\n    dataset = dataset.shuffle(buffer_size=args.buffer_size)\n    dataset = dataset.batch(args.batch_size)\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    # 2.2 prepare validate datasets\n    ver_list = []\n    ver_name_list = []\n    for db in args.eval_datasets:\n        print(\'begin db %s convert.\' % db)\n        data_set = load_bin(db, args.image_size, args)\n        ver_list.append(data_set)\n        ver_name_list.append(db)\n\n    # 3. define network, loss, optimize method, learning rate schedule, summary writer, saver\n    # 3.1 inference phase\n    w_init_method = tf.contrib.layers.xavier_initializer(uniform=False)\n    # 3.2 define the learning rate schedule\n    p = int(512.0/args.batch_size)\n    lr_steps = [p*val for val in args.lr_steps]\n    print(\'learning rate steps: \', lr_steps)\n    lr = tf.train.piecewise_constant(global_step, boundaries=lr_steps, values=[0.001, 0.0001, 0.00005, 0.00001], name=\'lr_schedule\')\n    # 3.3 define the optimize method\n    opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=args.momentum)\n\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    tl.layers.set_name_reuse(True)\n    loss_dict = {}\n    drop_dict = {}\n    loss_keys = []\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in range(args.num_gpus):\n        with tf.device(\'/gpu:%d\' % i):\n          with tf.name_scope(\'%s_%d\' % (args.tower_name, i)) as scope:\n            net = get_resnet(images_s[i], args.net_depth, type=\'ir\', w_init=w_init_method, trainable=trainable)\n            logit = arcface_loss(embedding=net.outputs, labels=labels_s[i], w_init=w_init_method, out_num=args.num_output)\n            # Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n            # define the cross entropy\n            inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels_s[i]))\n            # define weight deacy losses\n            wd_loss = 0\n            for weights in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(weights)\n            for W in tl.layers.get_variables_with_name(\'resnet_v1_50/E_DenseLayer/W\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(W)\n            for weights in tl.layers.get_variables_with_name(\'embedding_weights\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(weights)\n            for gamma in tl.layers.get_variables_with_name(\'gamma\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(gamma)\n            for beta in tl.layers.get_variables_with_name(\'beta\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(beta)\n            for alphas in tl.layers.get_variables_with_name(\'alphas\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(alphas)\n            for bias in tl.layers.get_variables_with_name(\'resnet_v1_50/E_DenseLayer/b\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(bias)\n            total_loss = inference_loss + wd_loss\n\n            loss_dict[(\'inference_loss_%s_%d\' % (\'gpu\', i))] = inference_loss\n            loss_keys.append((\'inference_loss_%s_%d\' % (\'gpu\', i)))\n            loss_dict[(\'wd_loss_%s_%d\' % (\'gpu\', i))] = wd_loss\n            loss_keys.append((\'wd_loss_%s_%d\' % (\'gpu\', i)))\n            loss_dict[(\'total_loss_%s_%d\' % (\'gpu\', i))] = total_loss\n            loss_keys.append((\'total_loss_%s_%d\' % (\'gpu\', i)))\n            grads = opt.compute_gradients(total_loss)\n            tower_grads.append(grads)\n            drop_dict.update(net.all_drop)\n            if i == 0:\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                pred = tf.nn.softmax(logit)\n                acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, axis=1), labels_s[i]), dtype=tf.int64))\n                embedding_tensor_gpu0 = net.outputs\n\n    grads = average_gradients(tower_grads)\n    with tf.control_dependencies(update_ops):\n        # Apply the gradients to adjust the shared variables.\n        train_op = opt.apply_gradients(grads, global_step=global_step)\n\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=args.log_device_mapping)\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    # summary writer\n    summary = tf.summary.FileWriter(args.summary_path, sess.graph)\n    summaries = []\n    # add grad histogram op\n    for grad, var in grads:\n        if grad is not None:\n            summaries.append(tf.summary.histogram(var.op.name + \'/gradients\', grad))\n    # add trainabel variable gradients\n    for var in tf.trainable_variables():\n        summaries.append(tf.summary.histogram(var.op.name, var))\n    # add loss summary\n    for keys, val in loss_dict.items():\n        summaries.append(tf.summary.scalar(keys, val))\n    # add learning rate\n    summaries.append(tf.summary.scalar(\'leraning_rate\', lr))\n    summary_op = tf.summary.merge(summaries)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n    # init all variables\n    sess.run(tf.global_variables_initializer())\n\n    drop_dict_test = {keys: 1 for keys in drop_dict.keys()}\n    # begin iteration\n    count = 0\n    for i in range(args.epoch):\n        sess.run(iterator.initializer)\n        while True:\n            try:\n                images_train, labels_train = sess.run(next_element)\n                feed_dict = {images: images_train, labels: labels_train, trainable: True}\n                feed_dict.update(drop_dict)\n                start = time.time()\n                _, _, inference_loss_val_gpu_1, wd_loss_val_gpu_1, total_loss_gpu_1, inference_loss_val_gpu_2, \\\n                wd_loss_val_gpu_2, total_loss_gpu_2, acc_val = sess.run([train_op, inc_op, loss_dict[loss_keys[0]],\n                                                                         loss_dict[loss_keys[1]],\n                                                                         loss_dict[loss_keys[2]],\n                                                                         loss_dict[loss_keys[3]],\n                                                                         loss_dict[loss_keys[4]],\n                                                                         loss_dict[loss_keys[5]], acc],\n                                                                         feed_dict=feed_dict)\n                end = time.time()\n                pre_sec = args.batch_size/(end - start)\n                # print training information\n                if count > 0 and count % args.show_info_interval == 0:\n                    print(\'epoch %d, total_step %d, total loss gpu 1 is %.2f , inference loss gpu 1 is %.2f, weight deacy \'\n                          \'loss gpu 1 is %.2f, total loss gpu 2 is %.2f , inference loss gpu 2 is %.2f, weight deacy \'\n                          \'loss gpu 2 is %.2f, training accuracy is %.6f, time %.3f samples/sec\' %\n                          (i, count, total_loss_gpu_1, inference_loss_val_gpu_1, wd_loss_val_gpu_1, total_loss_gpu_2,\n                           inference_loss_val_gpu_2, wd_loss_val_gpu_2, acc_val, pre_sec))\n                count += 1\n\n                # save summary\n                if count > 0 and count % args.summary_interval == 0:\n                    feed_dict = {images: images_train, labels: labels_train, trainable: True}\n                    feed_dict.update(drop_dict)\n                    summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n                    summary.add_summary(summary_op_val, count)\n\n                # save ckpt files\n                if count > 0 and count % args.ckpt_interval == 0:\n                    filename = \'InsightFace_iter_{:d}\'.format(count) + \'.ckpt\'\n                    filename = os.path.join(args.ckpt_path, filename)\n                    saver.save(sess, filename)\n                # # validate\n                if count > 0 and count % args.validate_interval == 0:\n                    feed_dict_test ={trainable: False}\n                    feed_dict_test.update(drop_dict_test)\n                    results = ver_test(ver_list=ver_list, ver_name_list=ver_name_list, nbatch=count, sess=sess,\n                             embedding_tensor=embedding_tensor_gpu0, batch_size=args.batch_size//args.num_gpus, feed_dict=feed_dict_test,\n                             input_placeholder=images_s[0])\n                    if max(results) > 0.99:\n                        print(\'best accuracy is %.5f\' % max(results))\n                        filename = \'InsightFace_iter_best_{:d}\'.format(count) + \'.ckpt\'\n                        filename = os.path.join(args.ckpt_path, filename)\n                        saver.save(sess, filename)\n            except tf.errors.OutOfRangeError:\n                print(""End of epoch %d"" % i)\n                break\n'"
train_nets_mgpu_new.py,42,"b'import tensorflow as tf\nimport tensorlayer as tl\nimport argparse\nfrom data.mx2tfrecords import parse_function\nimport os\nfrom nets.L_Resnet_E_IR_MGPU import get_resnet\nfrom losses.face_losses import arcface_loss\nimport time\nfrom data.eval_data_reader import load_bin\nfrom verification import ver_test\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\'parameters to train net\')\n    parser.add_argument(\'--net_depth\', default=100, help=\'resnet depth, default is 50\')\n    parser.add_argument(\'--epoch\', default=100000, help=\'epoch to train the network\')\n    parser.add_argument(\'--batch_size\', default=64, help=\'batch size to train network\')\n    parser.add_argument(\'--lr_steps\', default=[40000, 60000, 80000], help=\'learning rate to train network\')\n    parser.add_argument(\'--momentum\', default=0.9, help=\'learning alg momentum\')\n    parser.add_argument(\'--weight_deacy\', default=5e-4, help=\'learning alg momentum\')\n    # parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_ff\', \'cfp_fp\', \'agedb_30\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_fp\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_db_path\', default=\'./datasets/faces_ms1m_112x112\', help=\'evluate datasets base path\')\n    parser.add_argument(\'--image_size\', default=[112, 112], help=\'the image size\')\n    parser.add_argument(\'--num_output\', default=85164, help=\'the image size\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'./datasets/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    parser.add_argument(\'--summary_path\', default=\'./output/summary\', help=\'the summary file save path\')\n    parser.add_argument(\'--ckpt_path\', default=\'./output/ckpt\', help=\'the ckpt file save path\')\n    parser.add_argument(\'--saver_maxkeep\', default=100, help=\'tf.train.Saver max keep ckpt files\')\n    parser.add_argument(\'--buffer_size\', default=100000, help=\'tf dataset api buffer size\')\n    parser.add_argument(\'--log_device_mapping\', default=False, help=\'show device placement log\')\n    parser.add_argument(\'--summary_interval\', default=300, help=\'interval to save summary\')\n    parser.add_argument(\'--ckpt_interval\', default=5000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--validate_interval\', default=2000, help=\'intervals to save ckpt file\')\n    parser.add_argument(\'--show_info_interval\', default=20, help=\'intervals to show information\')\n    parser.add_argument(\'--num_gpus\', default=2, help=\'the num of gpus\')\n    parser.add_argument(\'--tower_name\', default=\'tower\', help=\'tower name\')\n    args = parser.parse_args()\n    return args\n\n\ndef average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\nif __name__ == \'__main__\':\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    # 1. define global parameters\n    args = get_parser()\n    global_step = tf.Variable(name=\'global_step\', initial_value=0, trainable=False)\n    inc_op = tf.assign_add(global_step, 1, name=\'increment_global_step\')\n    images = tf.placeholder(name=\'img_inputs\', shape=[None, *args.image_size, 3], dtype=tf.float32)\n    images_test = tf.placeholder(name=\'img_inputs\', shape=[None, *args.image_size, 3], dtype=tf.float32)\n    labels = tf.placeholder(name=\'img_labels\', shape=[None, ], dtype=tf.int64)\n    dropout_rate = tf.placeholder(name=\'dropout_rate\', dtype=tf.float32)\n    # splits input to different gpu\n    images_s = tf.split(images, num_or_size_splits=args.num_gpus, axis=0)\n    labels_s = tf.split(labels, num_or_size_splits=args.num_gpus, axis=0)\n    # 2 prepare train datasets and test datasets by using tensorflow dataset api\n    # 2.1 train datasets\n    # the image is substracted 127.5 and multiplied 1/128.\n    # random flip left right\n    tfrecords_f = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    dataset = tf.data.TFRecordDataset(tfrecords_f)\n    dataset = dataset.map(parse_function)\n    dataset = dataset.shuffle(buffer_size=args.buffer_size)\n    dataset = dataset.batch(args.batch_size)\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    # 2.2 prepare validate datasets\n    ver_list = []\n    ver_name_list = []\n    for db in args.eval_datasets:\n        print(\'begin db %s convert.\' % db)\n        data_set = load_bin(db, args.image_size, args)\n        ver_list.append(data_set)\n        ver_name_list.append(db)\n    # 3. define network, loss, optimize method, learning rate schedule, summary writer, saver\n    # 3.1 inference phase\n    w_init_method = tf.contrib.layers.xavier_initializer(uniform=False)\n    # 3.2 define the learning rate schedule\n    p = int(512.0/args.batch_size)\n    lr_steps = [p*val for val in args.lr_steps]\n    print(\'learning rate steps: \', lr_steps)\n    lr = tf.train.piecewise_constant(global_step, boundaries=lr_steps, values=[0.001, 0.0005, 0.0003, 0.0001],\n                                     name=\'lr_schedule\')\n    # 3.3 define the optimize method\n    opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=args.momentum)\n\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    tl.layers.set_name_reuse(True)\n    loss_dict = {}\n    drop_dict = {}\n    loss_keys = []\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in range(args.num_gpus):\n        with tf.device(\'/gpu:%d\' % i):\n          with tf.name_scope(\'%s_%d\' % (args.tower_name, i)) as scope:\n            net = get_resnet(images_s[i], args.net_depth, type=\'ir\', w_init=w_init_method, trainable=True, keep_rate=dropout_rate)\n            logit = arcface_loss(embedding=net.outputs, labels=labels_s[i], w_init=w_init_method, out_num=args.num_output)\n            # Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n            # define the cross entropy\n            inference_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels_s[i]))\n            # define weight deacy losses\n            wd_loss = 0\n            for weights in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(weights)\n            for W in tl.layers.get_variables_with_name(\'resnet_v1_50/E_DenseLayer/W\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(W)\n            for weights in tl.layers.get_variables_with_name(\'embedding_weights\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(weights)\n            for gamma in tl.layers.get_variables_with_name(\'gamma\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(gamma)\n            for alphas in tl.layers.get_variables_with_name(\'alphas\', True, True):\n                wd_loss += tf.contrib.layers.l2_regularizer(args.weight_deacy)(alphas)\n            total_loss = inference_loss + wd_loss\n\n            loss_dict[(\'inference_loss_%s_%d\' % (\'gpu\', i))] = inference_loss\n            loss_keys.append((\'inference_loss_%s_%d\' % (\'gpu\', i)))\n            loss_dict[(\'wd_loss_%s_%d\' % (\'gpu\', i))] = wd_loss\n            loss_keys.append((\'wd_loss_%s_%d\' % (\'gpu\', i)))\n            loss_dict[(\'total_loss_%s_%d\' % (\'gpu\', i))] = total_loss\n            loss_keys.append((\'total_loss_%s_%d\' % (\'gpu\', i)))\n            grads = opt.compute_gradients(total_loss)\n            tower_grads.append(grads)\n            if i == 0:\n                test_net = get_resnet(images_test, args.net_depth, type=\'ir\', w_init=w_init_method, trainable=False, keep_rate=dropout_rate)\n                embedding_tensor = test_net.outputs\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                pred = tf.nn.softmax(logit)\n                acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, axis=1), labels_s[i]), dtype=tf.float32))\n\n    grads = average_gradients(tower_grads)\n    with tf.control_dependencies(update_ops):\n        # Apply the gradients to adjust the shared variables.\n        train_op = opt.apply_gradients(grads, global_step=global_step)\n\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=args.log_device_mapping)\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    # summary writer\n    summary = tf.summary.FileWriter(args.summary_path, sess.graph)\n    summaries = []\n    # add grad histogram op\n    for grad, var in grads:\n        if grad is not None:\n            summaries.append(tf.summary.histogram(var.op.name + \'/gradients\', grad))\n    # add trainabel variable gradients\n    for var in tf.trainable_variables():\n        summaries.append(tf.summary.histogram(var.op.name, var))\n    # add loss summary\n    for keys, val in loss_dict.items():\n        summaries.append(tf.summary.scalar(keys, val))\n    # add learning rate\n    summaries.append(tf.summary.scalar(\'leraning_rate\', lr))\n    summary_op = tf.summary.merge(summaries)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n    # init all variables\n    sess.run(tf.global_variables_initializer())\n    # begin iteration\n    count = 0\n    for i in range(args.epoch):\n        sess.run(iterator.initializer)\n        while True:\n            try:\n                images_train, labels_train = sess.run(next_element)\n                feed_dict = {images: images_train, labels: labels_train, dropout_rate: 0.4}\n                start = time.time()\n                _, _, inference_loss_val_gpu_1, wd_loss_val_gpu_1, total_loss_gpu_1, inference_loss_val_gpu_2, \\\n                wd_loss_val_gpu_2, total_loss_gpu_2, acc_val = sess.run([train_op, inc_op, loss_dict[loss_keys[0]],\n                                                                         loss_dict[loss_keys[1]],\n                                                                         loss_dict[loss_keys[2]],\n                                                                         loss_dict[loss_keys[3]],\n                                                                         loss_dict[loss_keys[4]],\n                                                                         loss_dict[loss_keys[5]], acc],\n                                                                         feed_dict=feed_dict)\n                end = time.time()\n                pre_sec = args.batch_size/(end - start)\n                # print training information\n                if count > 0 and count % args.show_info_interval == 0:\n                    # print(\'epoch %d, total_step %d, total loss gpu 1 is %.2f , inference loss gpu 1 is %.2f, weight deacy \'\n                    #       \'loss gpu 1 is %.2f, total loss gpu 2 is %.2f , inference loss gpu 2 is %.2f, weight deacy \'\n                    #       \'loss gpu 2 is %.2f, training accuracy is %.6f, time %.3f samples/sec\' %\n                    #       (i, count, total_loss_gpu_1, inference_loss_val_gpu_1, wd_loss_val_gpu_1, total_loss_gpu_2,\n                    #        inference_loss_val_gpu_2, wd_loss_val_gpu_2, acc_val, pre_sec))\n\n                    print(\'epoch %d, total_step %d, total loss: [%.2f, %.2f], inference loss: [%.2f, %.2f], weight deacy \'\n                          \'loss: [%.2f, %.2f], training accuracy is %.6f, time %.3f samples/sec\' %\n                          (i, count, total_loss_gpu_1, total_loss_gpu_2, inference_loss_val_gpu_1, inference_loss_val_gpu_2,\n                           wd_loss_val_gpu_1, wd_loss_val_gpu_2, acc_val, pre_sec))\n                count += 1\n\n                # save summary\n                if count > 0 and count % args.summary_interval == 0:\n                    feed_dict = {images: images_train, labels: labels_train, dropout_rate: 0.4}\n                    summary_op_val = sess.run(summary_op, feed_dict=feed_dict)\n                    summary.add_summary(summary_op_val, count)\n\n                # save ckpt files\n                if count > 0 and count % args.ckpt_interval == 0:\n                    filename = \'InsightFace_iter_{:d}\'.format(count) + \'.ckpt\'\n                    filename = os.path.join(args.ckpt_path, filename)\n                    saver.save(sess, filename)\n                # # validate\n                if count >= 0 and count % args.validate_interval == 0:\n                    feed_dict_test ={dropout_rate: 1.0}\n                    results = ver_test(ver_list=ver_list, ver_name_list=ver_name_list, nbatch=count, sess=sess,\n                             embedding_tensor=embedding_tensor, batch_size=args.batch_size//args.num_gpus, feed_dict=feed_dict_test,\n                             input_placeholder=images_test)\n                    if max(results) > 0.998:\n                        print(\'best accuracy is %.5f\' % max(results))\n                        filename = \'InsightFace_iter_best_{:d}\'.format(count) + \'.ckpt\'\n                        filename = os.path.join(args.ckpt_path, filename)\n                        saver.save(sess, filename)\n            except tf.errors.OutOfRangeError:\n                print(""End of epoch %d"" % i)\n                break\n'"
verification.py,0,"b'""""""Helper for evaluation on the Labeled Faces in the Wild dataset\n""""""\n\n# MIT License\n#\n# Copyright (c) 2016 David Sandberg\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nimport sklearn\nfrom scipy import interpolate\nimport datetime\nimport mxnet as mx\n\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n\n    tprs = np.zeros((nrof_folds, nrof_thresholds))\n    fprs = np.zeros((nrof_folds, nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    indices = np.arange(nrof_pairs)\n    # print(\'pca\', pca)\n\n    if pca == 0:\n        diff = np.subtract(embeddings1, embeddings2)\n        dist = np.sum(np.square(diff), 1)\n\n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        # print(\'train_set\', train_set)\n        # print(\'test_set\', test_set)\n        if pca > 0:\n            print(\'doing pca on\', fold_idx)\n            embed1_train = embeddings1[train_set]\n            embed2_train = embeddings2[train_set]\n            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\n            # print(_embed_train.shape)\n            pca_model = PCA(n_components=pca)\n            pca_model.fit(_embed_train)\n            embed1 = pca_model.transform(embeddings1)\n            embed2 = pca_model.transform(embeddings2)\n            embed1 = sklearn.preprocessing.normalize(embed1)\n            embed2 = sklearn.preprocessing.normalize(embed2)\n            # print(embed1.shape, embed2.shape)\n            diff = np.subtract(embed1, embed2)\n            dist = np.sum(np.square(diff), 1)\n\n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n        print(\'best_threshold_index\', best_threshold_index, acc_train[best_threshold_index])\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n                                                                                                 dist[test_set],\n                                                                                                 actual_issame[\n                                                                                                     test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\n                                                      actual_issame[test_set])\n\n    tpr = np.mean(tprs, 0)\n    fpr = np.mean(fprs, 0)\n    return tpr, fpr, accuracy\n\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n\n    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n    acc = float(tp + tn) / dist.size\n    return tpr, fpr, acc\n\n\ndef calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\n    \'\'\'\n    Copy from [insightface](https://github.com/deepinsight/insightface)\n    :param thresholds:\n    :param embeddings1:\n    :param embeddings2:\n    :param actual_issame:\n    :param far_target:\n    :param nrof_folds:\n    :return:\n    \'\'\'\n    assert (embeddings1.shape[0] == embeddings2.shape[0])\n    assert (embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n\n    val = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)\n\n    diff = np.subtract(embeddings1, embeddings2)\n    dist = np.sum(np.square(diff), 1)\n    indices = np.arange(nrof_pairs)\n\n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n\n        # Find the threshold that gives FAR = far_target\n        far_train = np.zeros(nrof_thresholds)\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n        if np.max(far_train) >= far_target:\n            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n            threshold = f(far_target)\n        else:\n            threshold = 0.0\n\n        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n\n    val_mean = np.mean(val)\n    far_mean = np.mean(far)\n    val_std = np.std(val)\n    return val_mean, val_std, far_mean\n\n\ndef calculate_val_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    val = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)\n    return val, far\n\n\ndef evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = calculate_roc(thresholds, embeddings1, embeddings2,\n                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\n    thresholds = np.arange(0, 4, 0.001)\n    val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n                                      np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n    return tpr, fpr, accuracy, val, val_std, far\n\n\ndef data_iter(datasets, batch_size):\n    data_num = datasets.shape[0]\n    for i in range(0, data_num, batch_size):\n        yield datasets[i:min(i+batch_size, data_num), ...]\n\n\ndef test(data_set, sess, embedding_tensor, batch_size, label_shape=None, feed_dict=None, input_placeholder=None):\n    \'\'\'\n    referenc official implementation [insightface](https://github.com/deepinsight/insightface)\n    :param data_set:\n    :param sess:\n    :param embedding_tensor:\n    :param batch_size:\n    :param label_shape:\n    :param feed_dict:\n    :param input_placeholder:\n    :return:\n    \'\'\'\n    print(\'testing verification..\')\n    data_list = data_set[0]\n    issame_list = data_set[1]\n    embeddings_list = []\n    time_consumed = 0.0\n    for i in range(len(data_list)):\n        datas = data_list[i]\n        embeddings = None\n        feed_dict.setdefault(input_placeholder, None)\n        for idx, data in enumerate(data_iter(datas, batch_size)):\n            data_tmp = data.copy()    # fix issues #4\n            data_tmp -= 127.5\n            data_tmp *= 0.0078125\n            feed_dict[input_placeholder] = data_tmp\n            time0 = datetime.datetime.now()\n            _embeddings = sess.run(embedding_tensor, feed_dict)\n            time_now = datetime.datetime.now()\n            diff = time_now - time0\n            time_consumed += diff.total_seconds()\n            if embeddings is None:\n                embeddings = np.zeros((datas.shape[0], _embeddings.shape[1]))\n            try:\n                embeddings[idx*batch_size:min((idx+1)*batch_size, datas.shape[0]), ...] = _embeddings\n            except ValueError:\n                print(\'idx*batch_size value is %d min((idx+1)*batch_size, datas.shape[0]) %d, batch_size %d, data.shape[0] %d\' %\n                      (idx*batch_size, min((idx+1)*batch_size, datas.shape[0]), batch_size, datas.shape[0]))\n                print(\'embedding shape is \', _embeddings.shape)\n        embeddings_list.append(embeddings)\n\n    _xnorm = 0.0\n    _xnorm_cnt = 0\n    for embed in embeddings_list:\n        for i in range(embed.shape[0]):\n            _em = embed[i]\n            _norm = np.linalg.norm(_em)\n            # print(_em.shape, _norm)\n            _xnorm += _norm\n            _xnorm_cnt += 1\n    _xnorm /= _xnorm_cnt\n\n    acc1 = 0.0\n    std1 = 0.0\n    embeddings = embeddings_list[0] + embeddings_list[1]\n    embeddings = sklearn.preprocessing.normalize(embeddings)\n    print(embeddings.shape)\n    print(\'infer time\', time_consumed)\n    _, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=10)\n    acc2, std2 = np.mean(accuracy), np.std(accuracy)\n    return acc1, std1, acc2, std2, _xnorm, embeddings_list\n\n\ndef ver_test(ver_list, ver_name_list, nbatch, sess, embedding_tensor, batch_size, feed_dict, input_placeholder):\n    results = []\n    for i in range(len(ver_list)):\n        acc1, std1, acc2, std2, xnorm, embeddings_list = test(data_set=ver_list[i], sess=sess, embedding_tensor=embedding_tensor,\n                                                              batch_size=batch_size, feed_dict=feed_dict,\n                                                              input_placeholder=input_placeholder)\n        print(\'[%s][%d]XNorm: %f\' % (ver_name_list[i], nbatch, xnorm))\n        print(\'[%s][%d]Accuracy-Flip: %1.5f+-%1.5f\' % (ver_name_list[i], nbatch, acc2, std2))\n        results.append(acc2)\n    return results\n'"
data/__init__.py,0,b''
data/eval_data_reader.py,14,"b'import tensorflow as tf\nimport numpy as np\nimport pickle\nimport argparse\nimport os\nimport mxnet as mx\nimport cv2\nimport io\nimport PIL.Image\nimport mxnet.ndarray as nd\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\'evluation data parser\')\n    parser.add_argument(\'--eval_datasets\', default=[\'lfw\', \'cfp_ff\', \'cfp_fp\', \'agedb_30\'], help=\'evluation datasets\')\n    # parser.add_argument(\'--eval_datasets\', default=[\'cfp_fp\'], help=\'evluation datasets\')\n    parser.add_argument(\'--eval_db_path\', default=\'../datasets/faces_ms1m_112x112\', help=\'evluate datasets base path\')\n    parser.add_argument(\'--image_size\', default=[112, 112], help=\'the image size\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'../datasets/tfrecords/eval\', help=\'the image size\')\n    parser.add_argument(\'--db_base_path\', default=\'../datasets/faces_ms1m_112x112\', help=\'the image size\')\n    args = parser.parse_args()\n    return args\n\n\ndef load_bin(path, image_size):\n    \'\'\'\n    :param path: the input file path\n    :param image_size: the input image size\n    :return: the returned datasets is opencv format BGR  [112, 112, 3]\n    \'\'\'\n    bins, issame_list = pickle.load(open(path, \'rb\'), encoding=\'bytes\')\n    issame_list_int = list(map(int, issame_list))\n    data_list = []\n    for _ in [0, 1]:\n        data = np.zeros(shape=[len(issame_list)*2, *image_size, 3])\n        data_list.append(data)\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]\n        tf_images = tf.image.decode_jpeg(_bin)\n        tf_images = tf.reshape(tf_images, shape=(112, 112, 3))\n        sess = tf.Session()\n        images = sess.run(tf_images)\n        img_cv = cv2.cvtColor(images, cv2.COLOR_RGB2BGR)\n        print(np.min(img_cv), np.max(img_cv), img_cv.dtype)\n        cv2.imshow(\'test\', img_cv)\n        cv2.waitKey(0)\n        for flip in [0,1]:\n            if flip == 1:\n                # print(i, flip)\n                img_cv = np.fliplr(img_cv)\n                # cv2.imshow(\'test\', img_cv)\n                # cv2.waitKey(0)\n            data_list[flip][i][:] = img_cv\n        i += 1\n        if i % 1000 == 0:\n            print(\'loading bin\', i)\n    print(data_list[0].shape)\n    return data_list, issame_list\n\n\ndef mx2tfrecords(imgidx, imgrec, args):\n    output_path = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for i in imgidx:\n        img_info = imgrec.read_idx(i)\n        header, img = mx.recordio.unpack(img_info)\n        encoded_jpg_io = io.BytesIO(img)\n        image = PIL.Image.open(encoded_jpg_io)\n        np_img = np.array(image)\n        img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n        img_raw = img.tobytes()\n        label = int(header.label)\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\n        if i % 10000 == 0:\n            print(\'%d num image processed\' % i)\n    writer.close()\n\n\ndef mx2tfrecords_eval_data(args, db_name):\n    \'\'\'\n    Change evaluation data to tfrecords\n    :param args:\n    :param type: lfw, ......\n    :return:\n    \'\'\'\n    bins, issame_list = pickle.load(open(os.path.join(args.db_base_path, db_name+\'.bin\'), \'rb\'), encoding=\'bytes\')\n    output_image_path = os.path.join(args.tfrecords_file_path, db_name+\'_eval_data.tfrecords\')\n    writer_img = tf.python_io.TFRecordWriter(output_image_path)\n    for i in range(len(bins)):\n        img_info = bins[i]\n        img = mx.image.imdecode(img_info).asnumpy()\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img_b = img.tobytes()\n        # # decode test\n        # sess = tf.Session()\n        # img_2 = tf.decode_raw(img_b, out_type=tf.uint8)\n        # img_2 = tf.reshape(img_2, shape=(112, 112, 3))\n        # img_2 = tf.image.flip_left_right(img_2)\n        # img_2_np = sess.run(img_2)\n        # print(img_2_np.shape)\n        # cv2.imshow(\'test\', img_2_np)\n        # cv2.waitKey(0)\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_b]))\n        }))\n        writer_img.write(example.SerializeToString())  # Serialize To String\n        if i % 1000 == 0:\n            print(\'%d num image processed\' % i)\n    writer_img.close()\n\n\ndef load_bin(db_name, image_size, args):\n    bins, issame_list = pickle.load(open(os.path.join(args.eval_db_path, db_name+\'.bin\'), \'rb\'), encoding=\'bytes\')\n    data_list = []\n    for _ in [0,1]:\n        data = np.empty((len(issame_list)*2, image_size[0], image_size[1], 3))\n        data_list.append(data)\n    for i in range(len(issame_list)*2):\n        _bin = bins[i]\n        img = mx.image.imdecode(_bin).asnumpy()\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        for flip in [0,1]:\n            if flip == 1:\n                img = np.fliplr(img)\n            data_list[flip][i, ...] = img\n        i += 1\n        if i % 1000 == 0:\n            print(\'loading bin\', i)\n    print(data_list[0].shape)\n    return data_list, issame_list\n\n\nif __name__ == \'__main__\':\n    args = get_parser()\n    ver_list = []\n    ver_name_list = []\n    for db in args.eval_datasets:\n        print(\'begin db %s convert.\' % db)\n        # mx2tfrecords_eval_data(args, db)\n        data_set = load_bin(db, args.image_size)'"
data/mx2tfrecords.py,24,"b'import mxnet as mx\nimport argparse\nimport PIL.Image\nimport io\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport os\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\'data path information\'\n    )\n    parser.add_argument(\'--bin_path\', default=\'../datasets/faces_ms1m_112x112/train.rec\', type=str,\n                        help=\'path to the binary image file\')\n    parser.add_argument(\'--idx_path\', default=\'../datasets/faces_ms1m_112x112/train.idx\', type=str,\n                        help=\'path to the image index path\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'../datasets/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    args = parser.parse_args()\n    return args\n\n\ndef mx2tfrecords_old(imgidx, imgrec, args):\n    output_path = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for i in imgidx:\n        img_info = imgrec.read_idx(i)\n        header, img = mx.recordio.unpack(img_info)\n        encoded_jpg_io = io.BytesIO(img)\n        image = PIL.Image.open(encoded_jpg_io)\n        np_img = np.array(image)\n        img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n        img_raw = img.tobytes()\n        label = int(header.label)\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\n        if i % 10000 == 0:\n            print(\'%d num image processed\' % i)\n    writer.close()\n\n\ndef mx2tfrecords(imgidx, imgrec, args):\n    output_path = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for i in imgidx:\n        img_info = imgrec.read_idx(i)\n        header, img = mx.recordio.unpack(img_info)\n        label = int(header.label)\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\n        if i % 10000 == 0:\n            print(\'%d num image processed\' % i)\n    writer.close()\n\n\ndef parse_function(example_proto):\n    features = {\'image_raw\': tf.FixedLenFeature([], tf.string),\n                \'label\': tf.FixedLenFeature([], tf.int64)}\n    features = tf.parse_single_example(example_proto, features)\n    # You can do more image distortion here for training data\n    img = tf.image.decode_jpeg(features[\'image_raw\'])\n    img = tf.reshape(img, shape=(112, 112, 3))\n    r, g, b = tf.split(img, num_or_size_splits=3, axis=-1)\n    img = tf.concat([b, g, r], axis=-1)\n    img = tf.cast(img, dtype=tf.float32)\n    img = tf.subtract(img, 127.5)\n    img = tf.multiply(img,  0.0078125)\n    img = tf.image.random_flip_left_right(img)\n    label = tf.cast(features[\'label\'], tf.int64)\n    return img, label\n\n\nif __name__ == \'__main__\':\n    # # define parameters\n    # id2range = {}\n    # data_shape = (3, 112, 112)\n    args = parse_args()\n    # imgrec = mx.recordio.MXIndexedRecordIO(args.idx_path, args.bin_path, \'r\')\n    # s = imgrec.read_idx(0)\n    # header, _ = mx.recordio.unpack(s)\n    # print(header.label)\n    # imgidx = list(range(1, int(header.label[0])))\n    # seq_identity = range(int(header.label[0]), int(header.label[1]))\n    # for identity in seq_identity:\n    #     s = imgrec.read_idx(identity)\n    #     header, _ = mx.recordio.unpack(s)\n    #     a, b = int(header.label[0]), int(header.label[1])\n    #     id2range[identity] = (a, b)\n    # print(\'id2range\', len(id2range))\n\n    # # generate tfrecords\n    # mx2tfrecords(imgidx, imgrec, args)\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    sess = tf.Session(config=config)\n    # training datasets api config\n    tfrecords_f = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    dataset = tf.data.TFRecordDataset(tfrecords_f)\n    dataset = dataset.map(parse_function)\n    dataset = dataset.shuffle(buffer_size=30000)\n    dataset = dataset.batch(32)\n    iterator = dataset.make_initializable_iterator()\n    next_element = iterator.get_next()\n    # begin iteration\n    for i in range(1000):\n        sess.run(iterator.initializer)\n        while True:\n            try:\n                images, labels = sess.run(next_element)\n                cv2.imshow(\'test\', images[1, ...])\n                cv2.waitKey(0)\n            except tf.errors.OutOfRangeError:\n                print(""End of dataset"")\n\n\n\n\n'"
losses/__init__.py,0,b''
losses/face_losses.py,50,"b""import tensorflow as tf\nimport math\n\n\ndef arcface_loss(embedding, labels, out_num, w_init=None, s=64., m=0.5):\n    '''\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value default is 64\n    :param out_num: output class num\n    :param m: the margin value, default is 0.5\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    '''\n    cos_m = math.cos(m)\n    sin_m = math.sin(m)\n    mm = sin_m * m  # issue 1\n    threshold = math.cos(math.pi - m)\n    with tf.variable_scope('arcface_loss'):\n        # inputs and weights norm\n        embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)\n        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n        weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n                                  initializer=w_init, dtype=tf.float32)\n        weights_norm = tf.norm(weights, axis=0, keep_dims=True)\n        weights = tf.div(weights, weights_norm, name='norm_weights')\n        # cos(theta+m)\n        cos_t = tf.matmul(embedding, weights, name='cos_t')\n        cos_t2 = tf.square(cos_t, name='cos_2')\n        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n        sin_t = tf.sqrt(sin_t2, name='sin_t')\n        cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n\n        # this condition controls the theta+m should in range [0, pi]\n        #      0<=theta+m<=pi\n        #     -m<=theta<=pi-m\n        cond_v = cos_t - threshold\n        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n\n        keep_val = s*(cos_t - mm)\n        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n\n        mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n        # mask = tf.squeeze(mask, 1)\n        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n\n        s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n\n        output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')\n    return output\n\n\ndef cosineface_losses(embedding, labels, out_num, w_init=None, s=30., m=0.4):\n    '''\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value, default is 30\n    :param out_num: output class num\n    :param m: the margin value, default is 0.4\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    '''\n    with tf.variable_scope('cosineface_loss'):\n        # inputs and weights norm\n        embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)\n        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')\n        weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n                                  initializer=w_init, dtype=tf.float32)\n        weights_norm = tf.norm(weights, axis=0, keep_dims=True)\n        weights = tf.div(weights, weights_norm, name='norm_weights')\n        # cos_theta - m\n        cos_t = tf.matmul(embedding, weights, name='cos_t')\n        cos_t_m = tf.subtract(cos_t, m, name='cos_t_m')\n\n        mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')\n        inv_mask = tf.subtract(1., mask, name='inverse_mask')\n\n        output = tf.add(s * tf.multiply(cos_t, inv_mask), s * tf.multiply(cos_t_m, mask), name='cosineface_loss_output')\n    return output\n\n\ndef combine_loss_val(embedding, labels, w_init, out_num, margin_a, margin_m, margin_b, s):\n    '''\n    This code is contributed by RogerLo. Thanks for you contribution.\n\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value default is 64\n    :param out_num: output class num\n    :param m: the margin value, default is 0.5\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    '''\n    weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),\n                              initializer=w_init, dtype=tf.float32)\n    weights_unit = tf.nn.l2_normalize(weights, axis=0)\n    embedding_unit = tf.nn.l2_normalize(embedding, axis=1)\n    cos_t = tf.matmul(embedding_unit, weights_unit)\n    ordinal = tf.constant(list(range(0, embedding.get_shape().as_list()[0])), tf.int64)\n    ordinal_y = tf.stack([ordinal, labels], axis=1)\n    zy = cos_t * s\n    sel_cos_t = tf.gather_nd(zy, ordinal_y)\n    if margin_a != 1.0 or margin_m != 0.0 or margin_b != 0.0:\n        if margin_a == 1.0 and margin_m == 0.0:\n            s_m = s * margin_b\n            new_zy = sel_cos_t - s_m\n        else:\n            cos_value = sel_cos_t / s\n            t = tf.acos(cos_value)\n            if margin_a != 1.0:\n                t = t * margin_a\n            if margin_m > 0.0:\n                t = t + margin_m\n            body = tf.cos(t)\n            if margin_b > 0.0:\n                body = body - margin_b\n            new_zy = body * s\n    updated_logits = tf.add(zy, tf.scatter_nd(ordinal_y, tf.subtract(new_zy, sel_cos_t), zy.get_shape()))\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=updated_logits))\n    predict_cls = tf.argmax(updated_logits, 1)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(predict_cls, tf.int64), tf.cast(labels, tf.int64)), 'float'))\n    predict_cls_s = tf.argmax(zy, 1)\n    accuracy_s = tf.reduce_mean(tf.cast(tf.equal(tf.cast(predict_cls_s, tf.int64), tf.cast(labels, tf.int64)), 'float'))\n    return zy, loss, accuracy, accuracy_s, predict_cls_s"""
nets/L_Resnet_E_IR.py,60,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorflow.contrib.layers.python.layers import utils\nimport collections\nfrom tensorlayer.layers import Layer, list_remove_repeat\n\n\nclass ElementwiseLayer(Layer):\n    """"""\n    The :class:`ElementwiseLayer` class combines multiple :class:`Layer` which have the same output shapes by a given elemwise-wise operation.\n\n    Parameters\n    ----------\n    layer : a list of :class:`Layer` instances\n        The `Layer` class feeding into this layer.\n    combine_fn : a TensorFlow elemwise-merge function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`_ .\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n        self,\n        layer = [],\n        combine_fn = tf.minimum,\n        name =\'elementwise_layer\',\n        act = None,\n    ):\n        Layer.__init__(self, name=name)\n\n        if act:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s, act:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__, act.__name__))\n        else:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__))\n\n        self.outputs = layer[0].outputs\n        # print(self.outputs._shape, type(self.outputs._shape))\n        for l in layer[1:]:\n            # assert str(self.outputs.get_shape()) == str(l.outputs.get_shape()), ""Hint: the input shapes should be the same. %s != %s"" %  (self.outputs.get_shape() , str(l.outputs.get_shape()))\n            self.outputs = combine_fn(self.outputs, l.outputs, name=name)\n        if act:\n            self.outputs = act(self.outputs)\n        self.all_layers = list(layer[0].all_layers)\n        self.all_params = list(layer[0].all_params)\n        self.all_drop = dict(layer[0].all_drop)\n\n        for i in range(1, len(layer)):\n            self.all_layers.extend(list(layer[i].all_layers))\n            self.all_params.extend(list(layer[i].all_params))\n            self.all_drop.update(dict(layer[i].all_drop))\n\n        self.all_layers = list_remove_repeat(self.all_layers)\n        self.all_params = list_remove_repeat(self.all_params)\n\n\nclass BatchNormLayer(Layer):\n    """"""\n    The :class:`BatchNormLayer` class is a normalization layer, see ``tf.nn.batch_normalization`` and ``tf.nn.moments``.\n\n    Batch normalization on fully-connected or convolutional maps.\n\n    ```\n        https://www.tensorflow.org/api_docs/python/tf/cond\n        If x < y, the tf.add operation will be executed and tf.square operation will not be executed.\n        Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally.\n    ```\n\n    Parameters\n    -----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    decay : float, default is 0.9.\n        A decay factor for ExponentialMovingAverage, use larger value for large dataset.\n    epsilon : float\n        A small float number to avoid dividing by 0.\n    act : activation function.\n    is_train : boolean\n        Whether train or inference.\n    beta_init : beta initializer\n        The initializer for initializing beta\n    gamma_init : gamma initializer\n        The initializer for initializing gamma\n    dtype : tf.float32 (default) or tf.float16\n    name : a string or None\n        An optional name to attach to this layer.\n\n    References\n    ----------\n    - `Source <https://github.com/ry/tensorflow-resnet/blob/master/resnet.py>`_\n    - `stackoverflow <http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow>`_\n\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            decay=0.9,\n            epsilon=2e-5,\n            act=tf.identity,\n            is_train=False,\n            fix_gamma=True,\n            beta_init=tf.zeros_initializer,\n            gamma_init=tf.random_normal_initializer(mean=1.0, stddev=0.002),  # tf.ones_initializer,\n            # dtype = tf.float32,\n            trainable=None,\n            name=\'batchnorm_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        print(""  [TL] BatchNormLayer %s: decay:%f epsilon:%f act:%s is_train:%s"" % (self.name, decay, epsilon, act.__name__, is_train))\n        x_shape = self.inputs.get_shape()\n        params_shape = x_shape[-1:]\n\n        from tensorflow.python.training import moving_averages\n        from tensorflow.python.ops import control_flow_ops\n\n        with tf.variable_scope(name) as vs:\n            axis = list(range(len(x_shape) - 1))\n\n            ## 1. beta, gamma\n            if tf.__version__ > \'0.12.1\' and beta_init == tf.zeros_initializer:\n                beta_init = beta_init()\n            beta = tf.get_variable(\'beta\', shape=params_shape, initializer=beta_init, dtype=tf.float32, trainable=is_train)  #, restore=restore)\n\n            gamma = tf.get_variable(\n                \'gamma\',\n                shape=params_shape,\n                initializer=gamma_init,\n                dtype=tf.float32,\n                trainable=fix_gamma,\n            )  #restore=restore)\n\n            ## 2.\n            if tf.__version__ > \'0.12.1\':\n                moving_mean_init = tf.zeros_initializer()\n            else:\n                moving_mean_init = tf.zeros_initializer\n            moving_mean = tf.get_variable(\'moving_mean\', params_shape, initializer=moving_mean_init, dtype=tf.float32, trainable=False)  #   restore=restore)\n            moving_variance = tf.get_variable(\n                \'moving_variance\',\n                params_shape,\n                initializer=tf.constant_initializer(1.),\n                dtype=tf.float32,\n                trainable=False,\n            )  #   restore=restore)\n\n            ## 3.\n            # These ops will only be preformed when training.\n            mean, variance = tf.nn.moments(self.inputs, axis)\n            try:  # TF12\n                update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay, zero_debias=False)  # if zero_debias=True, has bias\n                update_moving_variance = moving_averages.assign_moving_average(\n                    moving_variance, variance, decay, zero_debias=False)  # if zero_debias=True, has bias\n                # print(""TF12 moving"")\n            except Exception as e:  # TF11\n                update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay)\n                update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, decay)\n                # print(""TF11 moving"")\n\n            def mean_var_with_update():\n                with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n                    return tf.identity(mean), tf.identity(variance)\n            if trainable:\n                mean, var = mean_var_with_update()\n                print(mean)\n                print(var)\n                self.outputs = act(tf.nn.batch_normalization(self.inputs, mean, var, beta, gamma, epsilon))\n            else:\n                self.outputs = act(tf.nn.batch_normalization(self.inputs, moving_mean, moving_variance, beta, gamma, epsilon))\n            variables = [beta, gamma, moving_mean, moving_variance]\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend(variables)\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return tl.layers.MaxPool2d(inputs, [1, 1], strides=(factor, factor), name=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, strides, rate=1, w_init=None, scope=None, trainable=None):\n    \'\'\'\n    Reference slim resnet\n    :param inputs:\n    :param num_outputs:\n    :param kernel_size:\n    :param strides:\n    :param rate:\n    :param scope:\n    :return:\n    \'\'\'\n    if strides == 1:\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                   strides=(strides, strides), W_init=w_init, act=None, padding=\'SAME\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size),\n                                               rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        return nets\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tl.layers.PadLayer(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], name=\'padding_%s\' % scope)\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                    strides=(strides, strides), W_init=w_init, act=None, padding=\'VALID\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                              rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        return nets\n\n\ndef bottleneck_IR(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = BatchNormLayer(shortcut, act=tf.identity, is_train=True, trainable=trainable, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = BatchNormLayer(inputs, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = BatchNormLayer(residual, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=None)\n        return output\n\n\ndef bottleneck_IR_SE(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = BatchNormLayer(shortcut, act=tf.identity, is_train=True, trainable=trainable, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = BatchNormLayer(inputs, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = BatchNormLayer(residual, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        # squeeze\n        squeeze = tl.layers.InputLayer(tf.reduce_mean(residual.outputs, axis=[1, 2]), name=\'squeeze_layer\')\n        # excitation\n        excitation1 = tl.layers.DenseLayer(squeeze, n_units=int(depth/16.0), act=tf.nn.relu,\n                                           W_init=w_init, name=\'excitation_1\')\n        # excitation1 = tl.layers.PReluLayer(excitation1, name=\'excitation_prelu\')\n        excitation2 = tl.layers.DenseLayer(excitation1, n_units=depth, act=tf.nn.sigmoid,\n                                           W_init=w_init, name=\'excitation_2\')\n        # scale\n        scale = tl.layers.ReshapeLayer(excitation2, shape=[tf.shape(excitation2.outputs)[0], 1, 1, depth], name=\'excitation_reshape\')\n\n        residual_se = ElementwiseLayer(layer=[residual, scale],\n                                       combine_fn=tf.multiply,\n                                       name=\'scale_layer\',\n                                       act=None)\n\n        output = ElementwiseLayer(layer=[shortcut, residual_se],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef resnet(inputs, bottle_neck, blocks, w_init=None, trainable=None, reuse=False, keep_rate=None, scope=None):\n    with tf.variable_scope(scope, reuse=reuse):\n        # inputs = tf.subtract(inputs, 127.5)\n        # inputs = tf.multiply(inputs, 0.0078125)\n        net_inputs = tl.layers.InputLayer(inputs, name=\'input_layer\')\n        if bottle_neck:\n            net = tl.layers.Conv2d(net_inputs, n_filter=64, filter_size=(3, 3), strides=(1, 1),\n                                   act=None, W_init=w_init, b_init=None, name=\'conv1\', use_cudnn_on_gpu=True)\n            net = BatchNormLayer(net, act=tf.identity, name=\'bn0\', is_train=True, trainable=trainable)\n            net = tl.layers.PReluLayer(net, name=\'prelu0\')\n        else:\n            raise ValueError(\'The standard resnet must support the bottleneck layer\')\n        for block in blocks:\n            with tf.variable_scope(block.scope):\n                for i, var in enumerate(block.args):\n                    with tf.variable_scope(\'unit_%d\' % (i+1)):\n                        net = block.unit_fn(net, depth=var[\'depth\'], depth_bottleneck=var[\'depth_bottleneck\'],\n                                            w_init=w_init, stride=var[\'stride\'], rate=var[\'rate\'], scope=None,\n                                            trainable=trainable)\n        net = BatchNormLayer(net, act=tf.identity, is_train=True, name=\'E_BN1\', trainable=trainable)\n        # net = tl.layers.DropoutLayer(net, keep=0.4, name=\'E_Dropout\')\n        net.outputs = tf.nn.dropout(net.outputs, keep_prob=keep_rate, name=\'E_Dropout\')\n        net_shape = net.outputs.get_shape()\n        net = tl.layers.ReshapeLayer(net, shape=[-1, net_shape[1]*net_shape[2]*net_shape[3]], name=\'E_Reshapelayer\')\n        net = tl.layers.DenseLayer(net, n_units=512, W_init=w_init, name=\'E_DenseLayer\')\n        net = BatchNormLayer(net, act=tf.identity, is_train=True, fix_gamma=False, trainable=trainable, name=\'E_BN2\')\n        return net\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef resnetse_v1_block(scope, base_depth, num_units, stride, rate=1, unit_fn=None):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, unit_fn, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }] + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1))\n\n\ndef get_resnet(inputs, num_layers, type=None, w_init=None, trainable=None, sess=None, reuse=False, keep_rate=None):\n    if type == \'ir\':\n        unit_fn = bottleneck_IR\n    elif type == \'se_ir\':\n        unit_fn = bottleneck_IR_SE\n    else:\n        raise ValueError(\'the input fn is unknown\')\n\n    if num_layers == 50:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=14, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 101:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=13, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=30, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    else:\n        raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n    net = resnet(inputs=inputs,\n                 bottle_neck=True,\n                 blocks=blocks,\n                 w_init=w_init,\n                 trainable=trainable,\n                 reuse=reuse,\n                 keep_rate = keep_rate,\n                 scope=\'resnet_v1_%d\' % num_layers)\n    return net\n\n\nif __name__ == \'__main__\':\n        x = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3], name=\'input_place\')\n        sess = tf.Session()\n        # w_init = tf.truncated_normal_initializer(mean=10, stddev=5e-2)\n        w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n        # test resnetse\n        nets = get_resnet(x, 50, type=\'ir\', w_init=w_init, sess=sess)\n        tl.layers.initialize_global_variables(sess)\n\n        for p in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n            print(p.op.name)\n        print(\'##############\'*30)\n        with sess:\n            nets.print_params()\n'"
nets/L_Resnet_E_IR_GBN.py,39,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorflow.contrib.layers.python.layers import utils\nimport collections\nfrom tensorlayer.layers import Layer, list_remove_repeat\nfrom tl_layers_modify import GroupNormLayer\n\n\nclass ElementwiseLayer(Layer):\n    """"""\n    The :class:`ElementwiseLayer` class combines multiple :class:`Layer` which have the same output shapes by a given elemwise-wise operation.\n\n    Parameters\n    ----------\n    layer : a list of :class:`Layer` instances\n        The `Layer` class feeding into this layer.\n    combine_fn : a TensorFlow elemwise-merge function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`_ .\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n        self,\n        layer = [],\n        combine_fn = tf.minimum,\n        name =\'elementwise_layer\',\n        act = None,\n    ):\n        Layer.__init__(self, name=name)\n\n        if act:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s, act:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__, act.__name__))\n        else:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__))\n\n        self.outputs = layer[0].outputs\n        # print(self.outputs._shape, type(self.outputs._shape))\n        for l in layer[1:]:\n            # assert str(self.outputs.get_shape()) == str(l.outputs.get_shape()), ""Hint: the input shapes should be the same. %s != %s"" %  (self.outputs.get_shape() , str(l.outputs.get_shape()))\n            self.outputs = combine_fn(self.outputs, l.outputs, name=name)\n        if act:\n            self.outputs = act(self.outputs)\n        self.all_layers = list(layer[0].all_layers)\n        self.all_params = list(layer[0].all_params)\n        self.all_drop = dict(layer[0].all_drop)\n\n        for i in range(1, len(layer)):\n            self.all_layers.extend(list(layer[i].all_layers))\n            self.all_params.extend(list(layer[i].all_params))\n            self.all_drop.update(dict(layer[i].all_drop))\n\n        self.all_layers = list_remove_repeat(self.all_layers)\n        self.all_params = list_remove_repeat(self.all_params)\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return tl.layers.MaxPool2d(inputs, [1, 1], strides=(factor, factor), name=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, strides, rate=1, w_init=None, scope=None, trainable=None):\n    \'\'\'\n    Reference slim resnet\n    :param inputs:\n    :param num_outputs:\n    :param kernel_size:\n    :param strides:\n    :param rate:\n    :param scope:\n    :return:\n    \'\'\'\n    if strides == 1:\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                   strides=(strides, strides), W_init=w_init, act=None, padding=\'SAME\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = GroupNormLayer(layer=nets, act=tf.identity, name=scope+\'_bn/GroupNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size),\n                                               rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = GroupNormLayer(layer=nets, act=tf.identity, name=scope+\'_bn/GroupNorm\')\n        return nets\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tl.layers.PadLayer(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], name=\'padding_%s\' % scope)\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                    strides=(strides, strides), W_init=w_init, act=None, padding=\'VALID\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = GroupNormLayer(layer=nets, act=tf.identity, name=scope+\'_bn/GroupNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                              rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = GroupNormLayer(layer=nets, act=tf.identity, name=scope+\'_bn/GroupNorm\')\n        return nets\n\n\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1, scope=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        b_init=None, name=\'shortcut_conv\')\n            shortcut = GroupNormLayer(layer=shortcut, act=tf.identity, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = tl.layers.Conv2d(inputs, depth_bottleneck, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                    name=\'conv1\')\n        residual = GroupNormLayer(layer=residual, act=tf.nn.relu, name=\'conv1_bn/BatchNorm\')\n\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth_bottleneck, kernel_size=3, strides= stride, rate=rate, scope=\'conv2\')\n\n        # bottleneck layer 3\n        residual = tl.layers.Conv2d(residual, depth, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                    name=\'conv3\')\n        residual = GroupNormLayer(layer=residual, act=tf.identity, name=\'conv3_bn/BatchNorm\',\n                                  scale_init=tf.constant_initializer(0.0))\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef bottleneck_IR(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = GroupNormLayer(layer=shortcut, act=tf.identity, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = GroupNormLayer(layer=inputs, act=tf.identity, name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = GroupNormLayer(layer=residual, act=tf.identity, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=None)\n        return output\n\n\ndef bottleneck_IR_SE(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = GroupNormLayer(layer=shortcut, act=tf.identity, name=\'shortcut_bn/BatchNorm\')\n        residual = GroupNormLayer(layer=inputs, act=tf.identity, name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = GroupNormLayer(layer=residual, act=tf.identity, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        # squeeze\n        squeeze = tl.layers.InputLayer(tf.reduce_mean(residual.outputs, axis=[1, 2]), name=\'squeeze_layer\')\n        # excitation\n        excitation1 = tl.layers.DenseLayer(squeeze, n_units=int(depth/16.0), act=tf.nn.relu,\n                                           W_init=w_init, name=\'excitation_1\')\n        # excitation1 = tl.layers.PReluLayer(excitation1, name=\'excitation_prelu\')\n        excitation2 = tl.layers.DenseLayer(excitation1, n_units=depth, act=tf.nn.sigmoid,\n                                           W_init=w_init, name=\'excitation_2\')\n        # scale\n        scale = tl.layers.ReshapeLayer(excitation2, shape=[tf.shape(excitation2.outputs)[0], 1, 1, depth], name=\'excitation_reshape\')\n\n        residual_se = ElementwiseLayer(layer=[residual, scale],\n                                       combine_fn=tf.multiply,\n                                       name=\'scale_layer\',\n                                       act=None)\n\n        output = ElementwiseLayer(layer=[shortcut, residual_se],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef resnet(inputs, bottle_neck, blocks, w_init=None, trainable=None, scope=None):\n    with tf.variable_scope(scope):\n        net_inputs = tl.layers.InputLayer(inputs, name=\'input_layer\')\n        if bottle_neck:\n            net = tl.layers.Conv2d(net_inputs, n_filter=64, filter_size=(3, 3), strides=(1, 1),\n                                   act=None, W_init=w_init, b_init=None, name=\'conv1\', use_cudnn_on_gpu=True)\n            net = GroupNormLayer(layer=net, act=tf.identity, name=\'group_norm_0\')\n            net = tl.layers.PReluLayer(net, name=\'prelu0\')\n        else:\n            raise ValueError(\'The standard resnet must support the bottleneck layer\')\n        for block in blocks:\n            with tf.variable_scope(block.scope):\n                for i, var in enumerate(block.args):\n                    with tf.variable_scope(\'unit_%d\' % (i+1)):\n                        net = block.unit_fn(net, depth=var[\'depth\'], depth_bottleneck=var[\'depth_bottleneck\'],\n                                            w_init=w_init, stride=var[\'stride\'], rate=var[\'rate\'], scope=None,\n                                            trainable=trainable)\n        net = GroupNormLayer(layer=net, act=tf.identity, name=\'E_GN_0\')\n        net = tl.layers.DropoutLayer(net, keep=0.4, name=\'E_Dropout\')\n        net_shape = net.outputs.get_shape()\n        net = tl.layers.ReshapeLayer(net, shape=[-1, net_shape[1]*net_shape[2]*net_shape[3]], name=\'E_Reshapelayer\')\n        net = tl.layers.DenseLayer(net, n_units=512, W_init=w_init, name=\'E_DenseLayer\')\n        # net = GroupNormLayer(layer=net, act=tf.identity, name=\'E_GN_1\')\n        return net\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef resnetse_v1_block(scope, base_depth, num_units, stride, rate=1, unit_fn=None):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, unit_fn, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }] + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1))\n\n\ndef resnetse_v1_block_2(scope, base_depth, num_units, stride, rate=1, unit_fn=None):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, unit_fn, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }])\n\n\ndef get_resnet(inputs, num_layers, type=None, w_init=None, trainable=None, sess=None):\n    if type == \'ir\':\n        unit_fn = bottleneck_IR\n    elif type == \'se_ir\':\n        unit_fn = bottleneck_IR_SE\n    # elif type == \'resnet\':\n    #     unit_fn = bottleneck\n    #     blocks = [\n    #         resnetse_v1_block_2(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n    #         resnetse_v1_block_2(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, unit_fn=unit_fn),\n    #         resnetse_v1_block_2(\'block3\', base_depth=256, num_units=6, stride=2, rate=1, unit_fn=unit_fn),\n    #         resnetse_v1_block_2(\'block4\', base_depth=512, num_units=3, stride=1, rate=1, unit_fn=unit_fn)\n    #     ]\n    else:\n        raise ValueError(\'the input fn is unknown\')\n\n    if num_layers == 50:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=14, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 101:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=13, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=30, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    else:\n        raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n    net = resnet(inputs=inputs,\n                 bottle_neck=True,\n                 blocks=blocks,\n                 w_init=w_init,\n                 trainable=trainable,\n                 scope=\'resnet_v1_%d\' % num_layers)\n    return net\n\n\nif __name__ == \'__main__\':\n        x = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3], name=\'input_place\')\n        sess = tf.Session()\n        # w_init = tf.truncated_normal_initializer(mean=10, stddev=5e-2)\n        w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n        # test resnetse\n        nets = get_resnet(x, 50, type=\'ir\', w_init=w_init, sess=sess)\n        tl.layers.initialize_global_variables(sess)\n\n        for p in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n            print(p.op.name)\n        print(\'##############\'*30)\n        with sess:\n            nets.print_params()\n'"
nets/L_Resnet_E_IR_MGPU.py,30,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorflow.contrib.layers.python.layers import utils\nimport collections\nfrom tl_layers_modify import ElementwiseLayer, BatchNormLayer, Conv2d, PReluLayer, DenseLayer\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return tl.layers.MaxPool2d(inputs, [1, 1], strides=(factor, factor), name=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, strides, rate=1, w_init=None, scope=None, trainable=None):\n    \'\'\'\n    Reference slim resnet\n    :param inputs:\n    :param num_outputs:\n    :param kernel_size:\n    :param strides:\n    :param rate:\n    :param scope:\n    :return:\n    \'\'\'\n    if strides == 1:\n        if rate == 1:\n            nets = Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                   strides=(strides, strides), W_init=w_init, act=None, padding=\'SAME\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size),\n                                               rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        return nets\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tl.layers.PadLayer(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], name=\'padding_%s\' % scope)\n        if rate == 1:\n            nets = Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                    strides=(strides, strides), W_init=w_init, act=None, padding=\'VALID\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                              rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        return nets\n\n\ndef bottleneck_IR(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = BatchNormLayer(shortcut, act=tf.identity, is_train=True, trainable=trainable, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = BatchNormLayer(inputs, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn1\')\n        residual = Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = BatchNormLayer(residual, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=None)\n        return output\n\n\ndef bottleneck_IR_SE(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = BatchNormLayer(shortcut, act=tf.identity, is_train=True, trainable=trainable, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = BatchNormLayer(inputs, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn1\')\n        residual = Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = BatchNormLayer(residual, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        # squeeze\n        squeeze = tl.layers.InputLayer(tf.reduce_mean(residual.outputs, axis=[1, 2]), name=\'squeeze_layer\')\n        # excitation\n        excitation1 = DenseLayer(squeeze, n_units=int(depth/16.0), act=tf.nn.relu,\n                                           W_init=w_init, name=\'excitation_1\')\n        # excitation1 = tl.layers.PReluLayer(excitation1, name=\'excitation_prelu\')\n        excitation2 = DenseLayer(excitation1, n_units=depth, act=tf.nn.sigmoid,\n                                           W_init=w_init, name=\'excitation_2\')\n        # scale\n        scale = tl.layers.ReshapeLayer(excitation2, shape=[tf.shape(excitation2.outputs)[0], 1, 1, depth], name=\'excitation_reshape\')\n\n        residual_se = ElementwiseLayer(layer=[residual, scale],\n                                       combine_fn=tf.multiply,\n                                       name=\'scale_layer\',\n                                       act=None)\n\n        output = ElementwiseLayer(layer=[shortcut, residual_se],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef resnet(inputs, bottle_neck, blocks, w_init=None, trainable=None, keep_rate=None, scope=None):\n    with tf.variable_scope(scope):\n        net_inputs = tl.layers.InputLayer(inputs, name=\'input_layer\')\n        if bottle_neck:\n            net = Conv2d(net_inputs, n_filter=64, filter_size=(3, 3), strides=(1, 1),\n                                   act=None, W_init=w_init, b_init=None, name=\'conv1\', use_cudnn_on_gpu=True)\n            net = BatchNormLayer(net, act=tf.identity, name=\'bn0\', is_train=True, trainable=trainable)\n            net = PReluLayer(net, name=\'prelu0\')\n        else:\n            raise ValueError(\'The standard resnet must support the bottleneck layer\')\n        for block in blocks:\n            with tf.variable_scope(block.scope):\n                for i, var in enumerate(block.args):\n                    with tf.variable_scope(\'unit_%d\' % (i+1)):\n                        net = block.unit_fn(net, depth=var[\'depth\'], depth_bottleneck=var[\'depth_bottleneck\'],\n                                            w_init=w_init, stride=var[\'stride\'], rate=var[\'rate\'], scope=None,\n                                            trainable=trainable)\n        net = BatchNormLayer(net, act=tf.identity, is_train=True, name=\'E_BN1\', trainable=trainable)\n        net = tl.layers.DropoutLayer(net, keep=keep_rate, name=\'E_Dropout\')\n        net_shape = net.outputs.get_shape()\n        net = tl.layers.ReshapeLayer(net, shape=[-1, net_shape[1]*net_shape[2]*net_shape[3]], name=\'E_Reshapelayer\')\n        net = DenseLayer(net, n_units=512, W_init=w_init, name=\'E_DenseLayer\')\n        net = BatchNormLayer(net, act=tf.identity, is_train=True, fix_gamma=False, trainable=trainable, name=\'E_BN2\')\n        return net\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef resnetse_v1_block(scope, base_depth, num_units, stride, rate=1, unit_fn=None):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, unit_fn, [{\n      \'depth\': base_depth,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }] + [{\n      \'depth\': base_depth,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1))\n\n\ndef get_resnet(inputs, num_layers, type=None, w_init=None, trainable=None, keep_rate=None, sess=None):\n    if type == \'ir\':\n        unit_fn = bottleneck_IR\n    elif type == \'se_ir\':\n        unit_fn = bottleneck_IR_SE\n    else:\n        raise ValueError(\'the input fn is unknown\')\n\n    if num_layers == 50:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=14, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 100:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=13, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=30, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    else:\n        raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n    net = resnet(inputs=inputs,\n                 bottle_neck=True,\n                 blocks=blocks,\n                 w_init=w_init,\n                 trainable=trainable,\n                 keep_rate=keep_rate,\n                 scope=\'resnet_v1_%d\' % num_layers)\n    return net\n\n\nif __name__ == \'__main__\':\n        x = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3], name=\'input_place\')\n        sess = tf.Session()\n        # w_init = tf.truncated_normal_initializer(mean=10, stddev=5e-2)\n        w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n        # test resnetse\n        nets = get_resnet(x, 50, type=\'ir\', w_init=w_init, sess=sess)\n        tl.layers.initialize_global_variables(sess)\n\n        for p in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n            print(p.op.name)\n        print(\'##############\'*30)\n        with sess:\n            nets.print_params()\n'"
nets/L_Resnet_E_IR_RBN.py,23,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorflow.contrib.layers.python.layers import utils\nimport collections\nfrom tensorlayer.layers import Layer, list_remove_repeat\n\n\nclass ElementwiseLayer(Layer):\n    """"""\n    The :class:`ElementwiseLayer` class combines multiple :class:`Layer` which have the same output shapes by a given elemwise-wise operation.\n\n    Parameters\n    ----------\n    layer : a list of :class:`Layer` instances\n        The `Layer` class feeding into this layer.\n    combine_fn : a TensorFlow elemwise-merge function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`_ .\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n        self,\n        layer = [],\n        combine_fn = tf.minimum,\n        name =\'elementwise_layer\',\n        act = None,\n    ):\n        Layer.__init__(self, name=name)\n\n        if act:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s, act:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__, act.__name__))\n        else:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__))\n\n        self.outputs = layer[0].outputs\n        # print(self.outputs._shape, type(self.outputs._shape))\n        for l in layer[1:]:\n            # assert str(self.outputs.get_shape()) == str(l.outputs.get_shape()), ""Hint: the input shapes should be the same. %s != %s"" %  (self.outputs.get_shape() , str(l.outputs.get_shape()))\n            self.outputs = combine_fn(self.outputs, l.outputs, name=name)\n        if act:\n            self.outputs = act(self.outputs)\n        self.all_layers = list(layer[0].all_layers)\n        self.all_params = list(layer[0].all_params)\n        self.all_drop = dict(layer[0].all_drop)\n\n        for i in range(1, len(layer)):\n            self.all_layers.extend(list(layer[i].all_layers))\n            self.all_params.extend(list(layer[i].all_params))\n            self.all_drop.update(dict(layer[i].all_drop))\n\n        self.all_layers = list_remove_repeat(self.all_layers)\n        self.all_params = list_remove_repeat(self.all_params)\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return tl.layers.MaxPool2d(inputs, [1, 1], strides=(factor, factor), name=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, strides, rate=1, w_init=None, scope=None, trainable=None):\n    \'\'\'\n    Reference slim resnet\n    :param inputs:\n    :param num_outputs:\n    :param kernel_size:\n    :param strides:\n    :param rate:\n    :param scope:\n    :return:\n    \'\'\'\n    if strides == 1:\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                   strides=(strides, strides), W_init=w_init, act=None, padding=\'SAME\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets.outputs = tf.layers.batch_normalization(inputs=nets.outputs,\n                                                         momentum=0.9,\n                                                         training=trainable,\n                                                         renorm=True,\n                                                         renorm_clipping={\'rmax\':3, \'rmin\':0.3333,\n                                                                          \'dmax\':5},\n                                                         renorm_momentum=0.9,\n                                                         name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size),\n                                               rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets.outputs = tf.layers.batch_normalization(inputs=nets.outputs,\n                                                         momentum=0.9,\n                                                         training=trainable,\n                                                         renorm=True,\n                                                         renorm_clipping={\'rmax\':3, \'rmin\':0.3333,\n                                                                          \'dmax\':5},\n                                                         renorm_momentum=0.9,\n                                                         name=scope+\'_bn/BatchNorm\')\n        return nets\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tl.layers.PadLayer(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], name=\'padding_%s\' % scope)\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                    strides=(strides, strides), W_init=w_init, act=None, padding=\'VALID\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets.outputs = tf.layers.batch_normalization(inputs=nets.outputs,\n                                                         momentum=0.9,\n                                                         training=trainable,\n                                                         renorm=True,\n                                                         renorm_clipping={\'rmax\':3, \'rmin\':0.3333,\n                                                                          \'dmax\':5},\n                                                         renorm_momentum=0.9,\n                                                         name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                              rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets.outputs = tf.layers.batch_normalization(inputs=nets.outputs,\n                                                         momentum=0.9,\n                                                         training=trainable,\n                                                         renorm=True,\n                                                         renorm_clipping={\'rmax\':3, \'rmin\':0.3333,\n                                                                          \'dmax\':5},\n                                                         renorm_momentum=0.9,\n                                                         name=scope+\'_bn/BatchNorm\')\n        return nets\n\n\ndef bottleneck_IR(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut.outputs = tf.layers.batch_normalization(inputs=shortcut.outputs,\n                                                             momentum=0.9,\n                                                             training=trainable,\n                                                             renorm=True,\n                                                             renorm_clipping={\'rmax\': 3, \'rmin\': 0.3333,\n                                                                              \'dmax\': 5},\n                                                             renorm_momentum=0.9,\n                                                             name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        inputs.outputs = tf.layers.batch_normalization(inputs=inputs.outputs,\n                                                         momentum=0.9,\n                                                         training=trainable,\n                                                         renorm=True,\n                                                         renorm_clipping={\'rmax\': 3, \'rmin\': 0.3333,\n                                                                          \'dmax\': 5},\n                                                         renorm_momentum=0.9,\n                                                         name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(inputs, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual.outputs = tf.layers.batch_normalization(inputs=residual.outputs,\n                                                         momentum=0.9,\n                                                         training=trainable,\n                                                         renorm=True,\n                                                         renorm_clipping={\'rmax\': 3, \'rmin\': 0.3333,\n                                                                          \'dmax\': 5},\n                                                         renorm_momentum=0.9,\n                                                         name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=None)\n        return output\n\n\ndef resnet(inputs, bottle_neck, blocks, w_init=None, trainable=None, scope=None):\n    with tf.variable_scope(scope):\n        # inputs = tf.subtract(inputs, 127.5)\n        # inputs = tf.multiply(inputs, 0.0078125)\n        net_inputs = tl.layers.InputLayer(inputs, name=\'input_layer\')\n        if bottle_neck:\n            net = tl.layers.Conv2d(net_inputs, n_filter=64, filter_size=(3, 3), strides=(1, 1),\n                                   act=None, W_init=w_init, b_init=None, name=\'conv1\', use_cudnn_on_gpu=True)\n            net.outputs = tf.layers.batch_normalization(inputs=net.outputs,\n                                                             momentum=0.9,\n                                                             training=trainable,\n                                                             renorm=True,\n                                                             renorm_clipping={\'rmax\': 3, \'rmin\': 0.3333,\n                                                                              \'dmax\': 5},\n                                                             renorm_momentum=0.9,\n                                                             name=\'bn0\')\n            net = tl.layers.PReluLayer(net, name=\'prelu0\')\n        else:\n            raise ValueError(\'The standard resnet must support the bottleneck layer\')\n        for block in blocks:\n            with tf.variable_scope(block.scope):\n                for i, var in enumerate(block.args):\n                    with tf.variable_scope(\'unit_%d\' % (i+1)):\n                        net = block.unit_fn(net, depth=var[\'depth\'], depth_bottleneck=var[\'depth_bottleneck\'],\n                                            w_init=w_init, stride=var[\'stride\'], rate=var[\'rate\'], scope=None,\n                                            trainable=trainable)\n        net.outputs = tf.layers.batch_normalization(inputs=net.outputs,\n                                                    momentum=0.9,\n                                                    training=trainable,\n                                                    renorm=True,\n                                                    renorm_clipping={\'rmax\': 3, \'rmin\': 0.3333,\n                                                                     \'dmax\': 5},\n                                                    renorm_momentum=0.9,\n                                                    name=\'E_BN1\')\n        net = tl.layers.DropoutLayer(net, keep=0.4, name=\'E_Dropout\')\n        net_shape = net.outputs.get_shape()\n        net = tl.layers.ReshapeLayer(net, shape=[-1, net_shape[1]*net_shape[2]*net_shape[3]], name=\'E_Reshapelayer\')\n        net = tl.layers.DenseLayer(net, n_units=512, W_init=w_init, name=\'E_DenseLayer\')\n        net.outputs = tf.layers.batch_normalization(inputs=net.outputs,\n                                                    momentum=0.9,\n                                                    training=trainable,\n                                                    renorm=True,\n                                                    renorm_clipping={\'rmax\': 3, \'rmin\': 0.3333,\n                                                                     \'dmax\': 5},\n                                                    renorm_momentum=0.9,\n                                                    name=\'E_BN2\')\n        return net\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef resnetse_v1_block(scope, base_depth, num_units, stride, rate=1, unit_fn=None):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, unit_fn, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }] + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1))\n\n\ndef get_resnet(inputs, num_layers, type=None, w_init=None, trainable=None, sess=None):\n    if type == \'ir\':\n        unit_fn = bottleneck_IR\n    # elif type == \'se_ir\':\n    #     unit_fn = bottleneck_IR_SE\n    else:\n        raise ValueError(\'the input fn is unknown\')\n\n    if num_layers == 50:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=14, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 101:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=13, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=30, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    else:\n        raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n    net = resnet(inputs=inputs,\n                 bottle_neck=True,\n                 blocks=blocks,\n                 w_init=w_init,\n                 trainable=trainable,\n                 scope=\'resnet_v1_%d\' % num_layers)\n    return net\n\n\nif __name__ == \'__main__\':\n        x = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3], name=\'input_place\')\n        sess = tf.Session()\n        # w_init = tf.truncated_normal_initializer(mean=10, stddev=5e-2)\n        w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n        # test resnetse\n        nets = get_resnet(x, 50, type=\'ir\', w_init=w_init, sess=sess)\n        tl.layers.initialize_global_variables(sess)\n\n        for p in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n            print(p.op.name)\n        print(\'##############\'*30)\n        with sess:\n            nets.print_params()\n'"
nets/L_Resnet_E_IR_fix_issue9.py,60,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorflow.contrib.layers.python.layers import utils\nimport collections\nfrom tensorlayer.layers import Layer, list_remove_repeat\n\n\nclass ElementwiseLayer(Layer):\n    """"""\n    The :class:`ElementwiseLayer` class combines multiple :class:`Layer` which have the same output shapes by a given elemwise-wise operation.\n\n    Parameters\n    ----------\n    layer : a list of :class:`Layer` instances\n        The `Layer` class feeding into this layer.\n    combine_fn : a TensorFlow elemwise-merge function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`_ .\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n        self,\n        layer = [],\n        combine_fn = tf.minimum,\n        name =\'elementwise_layer\',\n        act = None,\n    ):\n        Layer.__init__(self, name=name)\n\n        if act:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s, act:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__, act.__name__))\n        else:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__))\n\n        self.outputs = layer[0].outputs\n        # print(self.outputs._shape, type(self.outputs._shape))\n        for l in layer[1:]:\n            # assert str(self.outputs.get_shape()) == str(l.outputs.get_shape()), ""Hint: the input shapes should be the same. %s != %s"" %  (self.outputs.get_shape() , str(l.outputs.get_shape()))\n            self.outputs = combine_fn(self.outputs, l.outputs, name=name)\n        if act:\n            self.outputs = act(self.outputs)\n        self.all_layers = list(layer[0].all_layers)\n        self.all_params = list(layer[0].all_params)\n        self.all_drop = dict(layer[0].all_drop)\n\n        for i in range(1, len(layer)):\n            self.all_layers.extend(list(layer[i].all_layers))\n            self.all_params.extend(list(layer[i].all_params))\n            self.all_drop.update(dict(layer[i].all_drop))\n\n        self.all_layers = list_remove_repeat(self.all_layers)\n        self.all_params = list_remove_repeat(self.all_params)\n\n\nclass BatchNormLayer(Layer):\n    """"""\n    The :class:`BatchNormLayer` class is a normalization layer, see ``tf.nn.batch_normalization`` and ``tf.nn.moments``.\n\n    Batch normalization on fully-connected or convolutional maps.\n\n    ```\n        https://www.tensorflow.org/api_docs/python/tf/cond\n        If x < y, the tf.add operation will be executed and tf.square operation will not be executed.\n        Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally.\n    ```\n\n    Parameters\n    -----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    decay : float, default is 0.9.\n        A decay factor for ExponentialMovingAverage, use larger value for large dataset.\n    epsilon : float\n        A small float number to avoid dividing by 0.\n    act : activation function.\n    is_train : boolean\n        Whether train or inference.\n    beta_init : beta initializer\n        The initializer for initializing beta\n    gamma_init : gamma initializer\n        The initializer for initializing gamma\n    dtype : tf.float32 (default) or tf.float16\n    name : a string or None\n        An optional name to attach to this layer.\n\n    References\n    ----------\n    - `Source <https://github.com/ry/tensorflow-resnet/blob/master/resnet.py>`_\n    - `stackoverflow <http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow>`_\n\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            decay=0.9,\n            epsilon=2e-5,\n            act=tf.identity,\n            is_train=False,\n            fix_gamma=True,\n            beta_init=tf.zeros_initializer,\n            gamma_init=tf.random_normal_initializer(mean=1.0, stddev=0.002),  # tf.ones_initializer,\n            # dtype = tf.float32,\n            trainable=None,\n            name=\'batchnorm_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        print(""  [TL] BatchNormLayer %s: decay:%f epsilon:%f act:%s is_train:%s"" % (self.name, decay, epsilon, act.__name__, is_train))\n        x_shape = self.inputs.get_shape()\n        params_shape = x_shape[-1:]\n\n        from tensorflow.python.training import moving_averages\n        from tensorflow.python.ops import control_flow_ops\n\n        with tf.variable_scope(name) as vs:\n            axis = list(range(len(x_shape) - 1))\n\n            ## 1. beta, gamma\n            if tf.__version__ > \'0.12.1\' and beta_init == tf.zeros_initializer:\n                beta_init = beta_init()\n            beta = tf.get_variable(\'beta\', shape=params_shape, initializer=beta_init, dtype=tf.float32, trainable=is_train)  #, restore=restore)\n\n            gamma = tf.get_variable(\n                \'gamma\',\n                shape=params_shape,\n                initializer=gamma_init,\n                dtype=tf.float32,\n                trainable=fix_gamma,\n            )  #restore=restore)\n\n            ## 2.\n            if tf.__version__ > \'0.12.1\':\n                moving_mean_init = tf.zeros_initializer()\n            else:\n                moving_mean_init = tf.zeros_initializer\n            moving_mean = tf.get_variable(\'moving_mean\', params_shape, initializer=moving_mean_init, dtype=tf.float32, trainable=False)  #   restore=restore)\n            moving_variance = tf.get_variable(\n                \'moving_variance\',\n                params_shape,\n                initializer=tf.constant_initializer(1.),\n                dtype=tf.float32,\n                trainable=False,\n            )  #   restore=restore)\n\n            ## 3.\n            # These ops will only be preformed when training.\n            mean, variance = tf.nn.moments(self.inputs, axis)\n            try:  # TF12\n                update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay, zero_debias=False)  # if zero_debias=True, has bias\n                update_moving_variance = moving_averages.assign_moving_average(\n                    moving_variance, variance, decay, zero_debias=False)  # if zero_debias=True, has bias\n                # print(""TF12 moving"")\n            except Exception as e:  # TF11\n                update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay)\n                update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, decay)\n                # print(""TF11 moving"")\n\n            def mean_var_with_update():\n                with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n                    return tf.identity(mean), tf.identity(variance)\n            if trainable:\n                mean, var = mean_var_with_update()\n                print(mean)\n                print(var)\n                self.outputs = act(tf.nn.batch_normalization(self.inputs, mean, var, beta, gamma, epsilon))\n            else:\n                self.outputs = act(tf.nn.batch_normalization(self.inputs, moving_mean, moving_variance, beta, gamma, epsilon))\n            variables = [beta, gamma, moving_mean, moving_variance]\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend(variables)\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return tl.layers.MaxPool2d(inputs, [1, 1], strides=(factor, factor), name=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, strides, rate=1, w_init=None, scope=None, trainable=None):\n    \'\'\'\n    Reference slim resnet\n    :param inputs:\n    :param num_outputs:\n    :param kernel_size:\n    :param strides:\n    :param rate:\n    :param scope:\n    :return:\n    \'\'\'\n    if strides == 1:\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                   strides=(strides, strides), W_init=w_init, act=None, padding=\'SAME\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size),\n                                               rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        return nets\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tl.layers.PadLayer(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], name=\'padding_%s\' % scope)\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                    strides=(strides, strides), W_init=w_init, act=None, padding=\'VALID\', name=scope,\n                                    use_cudnn_on_gpu=True)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                              rate=rate, act=None, W_init=w_init, padding=\'SAME\', name=scope)\n            nets = BatchNormLayer(nets, act=tf.identity, is_train=True, trainable=trainable, name=scope+\'_bn/BatchNorm\')\n        return nets\n\n\ndef bottleneck_IR(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = BatchNormLayer(shortcut, act=tf.identity, is_train=True, trainable=trainable, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = BatchNormLayer(inputs, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = BatchNormLayer(residual, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=None)\n        return output\n\n\ndef bottleneck_IR_SE(inputs, depth, depth_bottleneck, stride, rate=1, w_init=None, scope=None, trainable=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        W_init=w_init, b_init=None, name=\'shortcut_conv\', use_cudnn_on_gpu=True)\n            shortcut = BatchNormLayer(shortcut, act=tf.identity, is_train=True, trainable=trainable, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = BatchNormLayer(inputs, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn1\')\n        residual = tl.layers.Conv2d(residual, depth_bottleneck, filter_size=(3, 3), strides=(1, 1), act=None, b_init=None,\n                                    W_init=w_init, name=\'conv1\', use_cudnn_on_gpu=True)\n        residual = BatchNormLayer(residual, act=tf.identity, is_train=True, trainable=trainable, name=\'conv1_bn2\')\n        # bottleneck prelu\n        residual = tl.layers.PReluLayer(residual)\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth, kernel_size=3, strides=stride, rate=rate, w_init=w_init, scope=\'conv2\', trainable=trainable)\n        # squeeze\n        squeeze = tl.layers.InputLayer(tf.reduce_mean(residual.outputs, axis=[1, 2]), name=\'squeeze_layer\')\n        # excitation\n        excitation1 = tl.layers.DenseLayer(squeeze, n_units=int(depth/16.0), act=tf.nn.relu,\n                                           W_init=w_init, name=\'excitation_1\')\n        # excitation1 = tl.layers.PReluLayer(excitation1, name=\'excitation_prelu\')\n        excitation2 = tl.layers.DenseLayer(excitation1, n_units=depth, act=tf.nn.sigmoid,\n                                           W_init=w_init, name=\'excitation_2\')\n        # scale\n        scale = tl.layers.ReshapeLayer(excitation2, shape=[tf.shape(excitation2.outputs)[0], 1, 1, depth], name=\'excitation_reshape\')\n\n        residual_se = ElementwiseLayer(layer=[residual, scale],\n                                       combine_fn=tf.multiply,\n                                       name=\'scale_layer\',\n                                       act=None)\n\n        output = ElementwiseLayer(layer=[shortcut, residual_se],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef resnet(inputs, bottle_neck, blocks, w_init=None, trainable=None, reuse=False, keep_rate=None, scope=None):\n    with tf.variable_scope(scope, reuse=reuse):\n        # inputs = tf.subtract(inputs, 127.5)\n        # inputs = tf.multiply(inputs, 0.0078125)\n        net_inputs = tl.layers.InputLayer(inputs, name=\'input_layer\')\n        if bottle_neck:\n            net = tl.layers.Conv2d(net_inputs, n_filter=64, filter_size=(3, 3), strides=(1, 1),\n                                   act=None, W_init=w_init, b_init=None, name=\'conv1\', use_cudnn_on_gpu=True)\n            net = BatchNormLayer(net, act=tf.identity, name=\'bn0\', is_train=True, trainable=trainable)\n            net = tl.layers.PReluLayer(net, name=\'prelu0\')\n        else:\n            raise ValueError(\'The standard resnet must support the bottleneck layer\')\n        for block in blocks:\n            with tf.variable_scope(block.scope):\n                for i, var in enumerate(block.args):\n                    with tf.variable_scope(\'unit_%d\' % (i+1)):\n                        net = block.unit_fn(net, depth=var[\'depth\'], depth_bottleneck=var[\'depth_bottleneck\'],\n                                            w_init=w_init, stride=var[\'stride\'], rate=var[\'rate\'], scope=None,\n                                            trainable=trainable)\n        net = BatchNormLayer(net, act=tf.identity, is_train=True, name=\'E_BN1\', trainable=trainable)\n        # net = tl.layers.DropoutLayer(net, keep=0.4, name=\'E_Dropout\')\n        net.outputs = tf.nn.dropout(net.outputs, keep_prob=keep_rate, name=\'E_Dropout\')\n        net_shape = net.outputs.get_shape()\n        net = tl.layers.ReshapeLayer(net, shape=[-1, net_shape[1]*net_shape[2]*net_shape[3]], name=\'E_Reshapelayer\')\n        net = tl.layers.DenseLayer(net, n_units=512, W_init=w_init, name=\'E_DenseLayer\')\n        net = BatchNormLayer(net, act=tf.identity, is_train=True, fix_gamma=False, trainable=trainable, name=\'E_BN2\')\n        return net\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef resnetse_v1_block(scope, base_depth, num_units, stride, rate=1, unit_fn=None):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, unit_fn, [{\n      \'depth\': base_depth,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }] + [{\n      \'depth\': base_depth,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1))\n\n\ndef get_resnet(inputs, num_layers, type=None, w_init=None, trainable=None, sess=None, reuse=False, keep_rate=None):\n    if type == \'ir\':\n        unit_fn = bottleneck_IR\n    elif type == \'se_ir\':\n        unit_fn = bottleneck_IR_SE\n    else:\n        raise ValueError(\'the input fn is unknown\')\n\n    if num_layers == 50:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=14, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 100:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=13, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=30, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1, unit_fn=unit_fn),\n            resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=2, rate=1, unit_fn=unit_fn)\n        ]\n    else:\n        raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n    net = resnet(inputs=inputs,\n                 bottle_neck=True,\n                 blocks=blocks,\n                 w_init=w_init,\n                 trainable=trainable,\n                 reuse=reuse,\n                 keep_rate=keep_rate,\n                 scope=\'resnet_v1_%d\' % num_layers)\n    return net\n\n\nif __name__ == \'__main__\':\n        x = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3], name=\'input_place\')\n        sess = tf.Session()\n        # w_init = tf.truncated_normal_initializer(mean=10, stddev=5e-2)\n        w_init = tf.contrib.layers.xavier_initializer(uniform=False)\n        # test resnetse\n        nets = get_resnet(x, 50, type=\'ir\', w_init=w_init, sess=sess)\n        tl.layers.initialize_global_variables(sess)\n\n        for p in tl.layers.get_variables_with_name(\'W_conv2d\', True, True):\n            print(p.op.name)\n        print(\'##############\'*30)\n        with sess:\n            nets.print_params()\n'"
nets/__init__.py,0,b''
nets/imagenet_classes.py,0,"b'class_names = \'\'\'tench, Tinca tinca\ngoldfish, Carassius auratus\ngreat white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\ntiger shark, Galeocerdo cuvieri\nhammerhead, hammerhead shark\nelectric ray, crampfish, numbfish, torpedo\nstingray\ncock\nhen\nostrich, Struthio camelus\nbrambling, Fringilla montifringilla\ngoldfinch, Carduelis carduelis\nhouse finch, linnet, Carpodacus mexicanus\njunco, snowbird\nindigo bunting, indigo finch, indigo bird, Passerina cyanea\nrobin, American robin, Turdus migratorius\nbulbul\njay\nmagpie\nchickadee\nwater ouzel, dipper\nkite\nbald eagle, American eagle, Haliaeetus leucocephalus\nvulture\ngreat grey owl, great gray owl, Strix nebulosa\nEuropean fire salamander, Salamandra salamandra\ncommon newt, Triturus vulgaris\neft\nspotted salamander, Ambystoma maculatum\naxolotl, mud puppy, Ambystoma mexicanum\nbullfrog, Rana catesbeiana\ntree frog, tree-frog\ntailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\nloggerhead, loggerhead turtle, Caretta caretta\nleatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\nmud turtle\nterrapin\nbox turtle, box tortoise\nbanded gecko\ncommon iguana, iguana, Iguana iguana\nAmerican chameleon, anole, Anolis carolinensis\nwhiptail, whiptail lizard\nagama\nfrilled lizard, Chlamydosaurus kingi\nalligator lizard\nGila monster, Heloderma suspectum\ngreen lizard, Lacerta viridis\nAfrican chameleon, Chamaeleo chamaeleon\nKomodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\nAfrican crocodile, Nile crocodile, Crocodylus niloticus\nAmerican alligator, Alligator mississipiensis\ntriceratops\nthunder snake, worm snake, Carphophis amoenus\nringneck snake, ring-necked snake, ring snake\nhognose snake, puff adder, sand viper\ngreen snake, grass snake\nking snake, kingsnake\ngarter snake, grass snake\nwater snake\nvine snake\nnight snake, Hypsiglena torquata\nboa constrictor, Constrictor constrictor\nrock python, rock snake, Python sebae\nIndian cobra, Naja naja\ngreen mamba\nsea snake\nhorned viper, cerastes, sand viper, horned asp, Cerastes cornutus\ndiamondback, diamondback rattlesnake, Crotalus adamanteus\nsidewinder, horned rattlesnake, Crotalus cerastes\ntrilobite\nharvestman, daddy longlegs, Phalangium opilio\nscorpion\nblack and gold garden spider, Argiope aurantia\nbarn spider, Araneus cavaticus\ngarden spider, Aranea diademata\nblack widow, Latrodectus mactans\ntarantula\nwolf spider, hunting spider\ntick\ncentipede\nblack grouse\nptarmigan\nruffed grouse, partridge, Bonasa umbellus\nprairie chicken, prairie grouse, prairie fowl\npeacock\nquail\npartridge\nAfrican grey, African gray, Psittacus erithacus\nmacaw\nsulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\nlorikeet\ncoucal\nbee eater\nhornbill\nhummingbird\njacamar\ntoucan\ndrake\nred-breasted merganser, Mergus serrator\ngoose\nblack swan, Cygnus atratus\ntusker\nechidna, spiny anteater, anteater\nplatypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\nwallaby, brush kangaroo\nkoala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\nwombat\njellyfish\nsea anemone, anemone\nbrain coral\nflatworm, platyhelminth\nnematode, nematode worm, roundworm\nconch\nsnail\nslug\nsea slug, nudibranch\nchiton, coat-of-mail shell, sea cradle, polyplacophore\nchambered nautilus, pearly nautilus, nautilus\nDungeness crab, Cancer magister\nrock crab, Cancer irroratus\nfiddler crab\nking crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\nAmerican lobster, Northern lobster, Maine lobster, Homarus americanus\nspiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\ncrayfish, crawfish, crawdad, crawdaddy\nhermit crab\nisopod\nwhite stork, Ciconia ciconia\nblack stork, Ciconia nigra\nspoonbill\nflamingo\nlittle blue heron, Egretta caerulea\nAmerican egret, great white heron, Egretta albus\nbittern\ncrane\nlimpkin, Aramus pictus\nEuropean gallinule, Porphyrio porphyrio\nAmerican coot, marsh hen, mud hen, water hen, Fulica americana\nbustard\nruddy turnstone, Arenaria interpres\nred-backed sandpiper, dunlin, Erolia alpina\nredshank, Tringa totanus\ndowitcher\noystercatcher, oyster catcher\npelican\nking penguin, Aptenodytes patagonica\nalbatross, mollymawk\ngrey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\nkiller whale, killer, orca, grampus, sea wolf, Orcinus orca\ndugong, Dugong dugon\nsea lion\nChihuahua\nJapanese spaniel\nMaltese dog, Maltese terrier, Maltese\nPekinese, Pekingese, Peke\nShih-Tzu\nBlenheim spaniel\npapillon\ntoy terrier\nRhodesian ridgeback\nAfghan hound, Afghan\nbasset, basset hound\nbeagle\nbloodhound, sleuthhound\nbluetick\nblack-and-tan coonhound\nWalker hound, Walker foxhound\nEnglish foxhound\nredbone\nborzoi, Russian wolfhound\nIrish wolfhound\nItalian greyhound\nwhippet\nIbizan hound, Ibizan Podenco\nNorwegian elkhound, elkhound\notterhound, otter hound\nSaluki, gazelle hound\nScottish deerhound, deerhound\nWeimaraner\nStaffordshire bullterrier, Staffordshire bull terrier\nAmerican Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\nBedlington terrier\nBorder terrier\nKerry blue terrier\nIrish terrier\nNorfolk terrier\nNorwich terrier\nYorkshire terrier\nwire-haired fox terrier\nLakeland terrier\nSealyham terrier, Sealyham\nAiredale, Airedale terrier\ncairn, cairn terrier\nAustralian terrier\nDandie Dinmont, Dandie Dinmont terrier\nBoston bull, Boston terrier\nminiature schnauzer\ngiant schnauzer\nstandard schnauzer\nScotch terrier, Scottish terrier, Scottie\nTibetan terrier, chrysanthemum dog\nsilky terrier, Sydney silky\nsoft-coated wheaten terrier\nWest Highland white terrier\nLhasa, Lhasa apso\nflat-coated retriever\ncurly-coated retriever\ngolden retriever\nLabrador retriever\nChesapeake Bay retriever\nGerman short-haired pointer\nvizsla, Hungarian pointer\nEnglish setter\nIrish setter, red setter\nGordon setter\nBrittany spaniel\nclumber, clumber spaniel\nEnglish springer, English springer spaniel\nWelsh springer spaniel\ncocker spaniel, English cocker spaniel, cocker\nSussex spaniel\nIrish water spaniel\nkuvasz\nschipperke\ngroenendael\nmalinois\nbriard\nkelpie\nkomondor\nOld English sheepdog, bobtail\nShetland sheepdog, Shetland sheep dog, Shetland\ncollie\nBorder collie\nBouvier des Flandres, Bouviers des Flandres\nRottweiler\nGerman shepherd, German shepherd dog, German police dog, alsatian\nDoberman, Doberman pinscher\nminiature pinscher\nGreater Swiss Mountain dog\nBernese mountain dog\nAppenzeller\nEntleBucher\nboxer\nbull mastiff\nTibetan mastiff\nFrench bulldog\nGreat Dane\nSaint Bernard, St Bernard\nEskimo dog, husky\nmalamute, malemute, Alaskan malamute\nSiberian husky\ndalmatian, coach dog, carriage dog\naffenpinscher, monkey pinscher, monkey dog\nbasenji\npug, pug-dog\nLeonberg\nNewfoundland, Newfoundland dog\nGreat Pyrenees\nSamoyed, Samoyede\nPomeranian\nchow, chow chow\nkeeshond\nBrabancon griffon\nPembroke, Pembroke Welsh corgi\nCardigan, Cardigan Welsh corgi\ntoy poodle\nminiature poodle\nstandard poodle\nMexican hairless\ntimber wolf, grey wolf, gray wolf, Canis lupus\nwhite wolf, Arctic wolf, Canis lupus tundrarum\nred wolf, maned wolf, Canis rufus, Canis niger\ncoyote, prairie wolf, brush wolf, Canis latrans\ndingo, warrigal, warragal, Canis dingo\ndhole, Cuon alpinus\nAfrican hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\nhyena, hyaena\nred fox, Vulpes vulpes\nkit fox, Vulpes macrotis\nArctic fox, white fox, Alopex lagopus\ngrey fox, gray fox, Urocyon cinereoargenteus\ntabby, tabby cat\ntiger cat\nPersian cat\nSiamese cat, Siamese\nEgyptian cat\ncougar, puma, catamount, mountain lion, painter, panther, Felis concolor\nlynx, catamount\nleopard, Panthera pardus\nsnow leopard, ounce, Panthera uncia\njaguar, panther, Panthera onca, Felis onca\nlion, king of beasts, Panthera leo\ntiger, Panthera tigris\ncheetah, chetah, Acinonyx jubatus\nbrown bear, bruin, Ursus arctos\nAmerican black bear, black bear, Ursus americanus, Euarctos americanus\nice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\nsloth bear, Melursus ursinus, Ursus ursinus\nmongoose\nmeerkat, mierkat\ntiger beetle\nladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\nground beetle, carabid beetle\nlong-horned beetle, longicorn, longicorn beetle\nleaf beetle, chrysomelid\ndung beetle\nrhinoceros beetle\nweevil\nfly\nbee\nant, emmet, pismire\ngrasshopper, hopper\ncricket\nwalking stick, walkingstick, stick insect\ncockroach, roach\nmantis, mantid\ncicada, cicala\nleafhopper\nlacewing, lacewing fly\ndragonfly, darning needle, devil\'s darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\ndamselfly\nadmiral\nringlet, ringlet butterfly\nmonarch, monarch butterfly, milkweed butterfly, Danaus plexippus\ncabbage butterfly\nsulphur butterfly, sulfur butterfly\nlycaenid, lycaenid butterfly\nstarfish, sea star\nsea urchin\nsea cucumber, holothurian\nwood rabbit, cottontail, cottontail rabbit\nhare\nAngora, Angora rabbit\nhamster\nporcupine, hedgehog\nfox squirrel, eastern fox squirrel, Sciurus niger\nmarmot\nbeaver\nguinea pig, Cavia cobaya\nsorrel\nzebra\nhog, pig, grunter, squealer, Sus scrofa\nwild boar, boar, Sus scrofa\nwarthog\nhippopotamus, hippo, river horse, Hippopotamus amphibius\nox\nwater buffalo, water ox, Asiatic buffalo, Bubalus bubalis\nbison\nram, tup\nbighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\nibex, Capra ibex\nhartebeest\nimpala, Aepyceros melampus\ngazelle\nArabian camel, dromedary, Camelus dromedarius\nllama\nweasel\nmink\npolecat, fitch, foulmart, foumart, Mustela putorius\nblack-footed ferret, ferret, Mustela nigripes\notter\nskunk, polecat, wood pussy\nbadger\narmadillo\nthree-toed sloth, ai, Bradypus tridactylus\norangutan, orang, orangutang, Pongo pygmaeus\ngorilla, Gorilla gorilla\nchimpanzee, chimp, Pan troglodytes\ngibbon, Hylobates lar\nsiamang, Hylobates syndactylus, Symphalangus syndactylus\nguenon, guenon monkey\npatas, hussar monkey, Erythrocebus patas\nbaboon\nmacaque\nlangur\ncolobus, colobus monkey\nproboscis monkey, Nasalis larvatus\nmarmoset\ncapuchin, ringtail, Cebus capucinus\nhowler monkey, howler\ntiti, titi monkey\nspider monkey, Ateles geoffroyi\nsquirrel monkey, Saimiri sciureus\nMadagascar cat, ring-tailed lemur, Lemur catta\nindri, indris, Indri indri, Indri brevicaudatus\nIndian elephant, Elephas maximus\nAfrican elephant, Loxodonta africana\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\nbarracouta, snoek\neel\ncoho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\nrock beauty, Holocanthus tricolor\nanemone fish\nsturgeon\ngar, garfish, garpike, billfish, Lepisosteus osseus\nlionfish\npuffer, pufferfish, blowfish, globefish\nabacus\nabaya\nacademic gown, academic robe, judge\'s robe\naccordion, piano accordion, squeeze box\nacoustic guitar\naircraft carrier, carrier, flattop, attack aircraft carrier\nairliner\nairship, dirigible\naltar\nambulance\namphibian, amphibious vehicle\nanalog clock\napiary, bee house\napron\nashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\nassault rifle, assault gun\nbackpack, back pack, knapsack, packsack, rucksack, haversack\nbakery, bakeshop, bakehouse\nbalance beam, beam\nballoon\nballpoint, ballpoint pen, ballpen, Biro\nBand Aid\nbanjo\nbannister, banister, balustrade, balusters, handrail\nbarbell\nbarber chair\nbarbershop\nbarn\nbarometer\nbarrel, cask\nbarrow, garden cart, lawn cart, wheelbarrow\nbaseball\nbasketball\nbassinet\nbassoon\nbathing cap, swimming cap\nbath towel\nbathtub, bathing tub, bath, tub\nbeach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\nbeacon, lighthouse, beacon light, pharos\nbeaker\nbearskin, busby, shako\nbeer bottle\nbeer glass\nbell cote, bell cot\nbib\nbicycle-built-for-two, tandem bicycle, tandem\nbikini, two-piece\nbinder, ring-binder\nbinoculars, field glasses, opera glasses\nbirdhouse\nboathouse\nbobsled, bobsleigh, bob\nbolo tie, bolo, bola tie, bola\nbonnet, poke bonnet\nbookcase\nbookshop, bookstore, bookstall\nbottlecap\nbow\nbow tie, bow-tie, bowtie\nbrass, memorial tablet, plaque\nbrassiere, bra, bandeau\nbreakwater, groin, groyne, mole, bulwark, seawall, jetty\nbreastplate, aegis, egis\nbroom\nbucket, pail\nbuckle\nbulletproof vest\nbullet train, bullet\nbutcher shop, meat market\ncab, hack, taxi, taxicab\ncaldron, cauldron\ncandle, taper, wax light\ncannon\ncanoe\ncan opener, tin opener\ncardigan\ncar mirror\ncarousel, carrousel, merry-go-round, roundabout, whirligig\ncarpenter\'s kit, tool kit\ncarton\ncar wheel\ncash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\ncassette\ncassette player\ncastle\ncatamaran\nCD player\ncello, violoncello\ncellular telephone, cellular phone, cellphone, cell, mobile phone\nchain\nchainlink fence\nchain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\nchain saw, chainsaw\nchest\nchiffonier, commode\nchime, bell, gong\nchina cabinet, china closet\nChristmas stocking\nchurch, church building\ncinema, movie theater, movie theatre, movie house, picture palace\ncleaver, meat cleaver, chopper\ncliff dwelling\ncloak\nclog, geta, patten, sabot\ncocktail shaker\ncoffee mug\ncoffeepot\ncoil, spiral, volute, whorl, helix\ncombination lock\ncomputer keyboard, keypad\nconfectionery, confectionary, candy store\ncontainer ship, containership, container vessel\nconvertible\ncorkscrew, bottle screw\ncornet, horn, trumpet, trump\ncowboy boot\ncowboy hat, ten-gallon hat\ncradle\ncrane\ncrash helmet\ncrate\ncrib, cot\nCrock Pot\ncroquet ball\ncrutch\ncuirass\ndam, dike, dyke\ndesk\ndesktop computer\ndial telephone, dial phone\ndiaper, nappy, napkin\ndigital clock\ndigital watch\ndining table, board\ndishrag, dishcloth\ndishwasher, dish washer, dishwashing machine\ndisk brake, disc brake\ndock, dockage, docking facility\ndogsled, dog sled, dog sleigh\ndome\ndoormat, welcome mat\ndrilling platform, offshore rig\ndrum, membranophone, tympan\ndrumstick\ndumbbell\nDutch oven\nelectric fan, blower\nelectric guitar\nelectric locomotive\nentertainment center\nenvelope\nespresso maker\nface powder\nfeather boa, boa\nfile, file cabinet, filing cabinet\nfireboat\nfire engine, fire truck\nfire screen, fireguard\nflagpole, flagstaff\nflute, transverse flute\nfolding chair\nfootball helmet\nforklift\nfountain\nfountain pen\nfour-poster\nfreight car\nFrench horn, horn\nfrying pan, frypan, skillet\nfur coat\ngarbage truck, dustcart\ngasmask, respirator, gas helmet\ngas pump, gasoline pump, petrol pump, island dispenser\ngoblet\ngo-kart\ngolf ball\ngolfcart, golf cart\ngondola\ngong, tam-tam\ngown\ngrand piano, grand\ngreenhouse, nursery, glasshouse\ngrille, radiator grille\ngrocery store, grocery, food market, market\nguillotine\nhair slide\nhair spray\nhalf track\nhammer\nhamper\nhand blower, blow dryer, blow drier, hair dryer, hair drier\nhand-held computer, hand-held microcomputer\nhandkerchief, hankie, hanky, hankey\nhard disc, hard disk, fixed disk\nharmonica, mouth organ, harp, mouth harp\nharp\nharvester, reaper\nhatchet\nholster\nhome theater, home theatre\nhoneycomb\nhook, claw\nhoopskirt, crinoline\nhorizontal bar, high bar\nhorse cart, horse-cart\nhourglass\niPod\niron, smoothing iron\njack-o\'-lantern\njean, blue jean, denim\njeep, landrover\njersey, T-shirt, tee shirt\njigsaw puzzle\njinrikisha, ricksha, rickshaw\njoystick\nkimono\nknee pad\nknot\nlab coat, laboratory coat\nladle\nlampshade, lamp shade\nlaptop, laptop computer\nlawn mower, mower\nlens cap, lens cover\nletter opener, paper knife, paperknife\nlibrary\nlifeboat\nlighter, light, igniter, ignitor\nlimousine, limo\nliner, ocean liner\nlipstick, lip rouge\nLoafer\nlotion\nloudspeaker, speaker, speaker unit, loudspeaker system, speaker system\nloupe, jeweler\'s loupe\nlumbermill, sawmill\nmagnetic compass\nmailbag, postbag\nmailbox, letter box\nmaillot\nmaillot, tank suit\nmanhole cover\nmaraca\nmarimba, xylophone\nmask\nmatchstick\nmaypole\nmaze, labyrinth\nmeasuring cup\nmedicine chest, medicine cabinet\nmegalith, megalithic structure\nmicrophone, mike\nmicrowave, microwave oven\nmilitary uniform\nmilk can\nminibus\nminiskirt, mini\nminivan\nmissile\nmitten\nmixing bowl\nmobile home, manufactured home\nModel T\nmodem\nmonastery\nmonitor\nmoped\nmortar\nmortarboard\nmosque\nmosquito net\nmotor scooter, scooter\nmountain bike, all-terrain bike, off-roader\nmountain tent\nmouse, computer mouse\nmousetrap\nmoving van\nmuzzle\nnail\nneck brace\nnecklace\nnipple\nnotebook, notebook computer\nobelisk\noboe, hautboy, hautbois\nocarina, sweet potato\nodometer, hodometer, mileometer, milometer\noil filter\norgan, pipe organ\noscilloscope, scope, cathode-ray oscilloscope, CRO\noverskirt\noxcart\noxygen mask\npacket\npaddle, boat paddle\npaddlewheel, paddle wheel\npadlock\npaintbrush\npajama, pyjama, pj\'s, jammies\npalace\npanpipe, pandean pipe, syrinx\npaper towel\nparachute, chute\nparallel bars, bars\npark bench\nparking meter\npassenger car, coach, carriage\npatio, terrace\npay-phone, pay-station\npedestal, plinth, footstall\npencil box, pencil case\npencil sharpener\nperfume, essence\nPetri dish\nphotocopier\npick, plectrum, plectron\npickelhaube\npicket fence, paling\npickup, pickup truck\npier\npiggy bank, penny bank\npill bottle\npillow\nping-pong ball\npinwheel\npirate, pirate ship\npitcher, ewer\nplane, carpenter\'s plane, woodworking plane\nplanetarium\nplastic bag\nplate rack\nplow, plough\nplunger, plumber\'s helper\nPolaroid camera, Polaroid Land camera\npole\npolice van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\nponcho\npool table, billiard table, snooker table\npop bottle, soda bottle\npot, flowerpot\npotter\'s wheel\npower drill\nprayer rug, prayer mat\nprinter\nprison, prison house\nprojectile, missile\nprojector\npuck, hockey puck\npunching bag, punch bag, punching ball, punchball\npurse\nquill, quill pen\nquilt, comforter, comfort, puff\nracer, race car, racing car\nracket, racquet\nradiator\nradio, wireless\nradio telescope, radio reflector\nrain barrel\nrecreational vehicle, RV, R.V.\nreel\nreflex camera\nrefrigerator, icebox\nremote control, remote\nrestaurant, eating house, eating place, eatery\nrevolver, six-gun, six-shooter\nrifle\nrocking chair, rocker\nrotisserie\nrubber eraser, rubber, pencil eraser\nrugby ball\nrule, ruler\nrunning shoe\nsafe\nsafety pin\nsaltshaker, salt shaker\nsandal\nsarong\nsax, saxophone\nscabbard\nscale, weighing machine\nschool bus\nschooner\nscoreboard\nscreen, CRT screen\nscrew\nscrewdriver\nseat belt, seatbelt\nsewing machine\nshield, buckler\nshoe shop, shoe-shop, shoe store\nshoji\nshopping basket\nshopping cart\nshovel\nshower cap\nshower curtain\nski\nski mask\nsleeping bag\nslide rule, slipstick\nsliding door\nslot, one-armed bandit\nsnorkel\nsnowmobile\nsnowplow, snowplough\nsoap dispenser\nsoccer ball\nsock\nsolar dish, solar collector, solar furnace\nsombrero\nsoup bowl\nspace bar\nspace heater\nspace shuttle\nspatula\nspeedboat\nspider web, spider\'s web\nspindle\nsports car, sport car\nspotlight, spot\nstage\nsteam locomotive\nsteel arch bridge\nsteel drum\nstethoscope\nstole\nstone wall\nstopwatch, stop watch\nstove\nstrainer\nstreetcar, tram, tramcar, trolley, trolley car\nstretcher\nstudio couch, day bed\nstupa, tope\nsubmarine, pigboat, sub, U-boat\nsuit, suit of clothes\nsundial\nsunglass\nsunglasses, dark glasses, shades\nsunscreen, sunblock, sun blocker\nsuspension bridge\nswab, swob, mop\nsweatshirt\nswimming trunks, bathing trunks\nswing\nswitch, electric switch, electrical switch\nsyringe\ntable lamp\ntank, army tank, armored combat vehicle, armoured combat vehicle\ntape player\nteapot\nteddy, teddy bear\ntelevision, television system\ntennis ball\nthatch, thatched roof\ntheater curtain, theatre curtain\nthimble\nthresher, thrasher, threshing machine\nthrone\ntile roof\ntoaster\ntobacco shop, tobacconist shop, tobacconist\ntoilet seat\ntorch\ntotem pole\ntow truck, tow car, wrecker\ntoyshop\ntractor\ntrailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\ntray\ntrench coat\ntricycle, trike, velocipede\ntrimaran\ntripod\ntriumphal arch\ntrolleybus, trolley coach, trackless trolley\ntrombone\ntub, vat\nturnstile\ntypewriter keyboard\numbrella\nunicycle, monocycle\nupright, upright piano\nvacuum, vacuum cleaner\nvase\nvault\nvelvet\nvending machine\nvestment\nviaduct\nviolin, fiddle\nvolleyball\nwaffle iron\nwall clock\nwallet, billfold, notecase, pocketbook\nwardrobe, closet, press\nwarplane, military plane\nwashbasin, handbasin, washbowl, lavabo, wash-hand basin\nwasher, automatic washer, washing machine\nwater bottle\nwater jug\nwater tower\nwhiskey jug\nwhistle\nwig\nwindow screen\nwindow shade\nWindsor tie\nwine bottle\nwing\nwok\nwooden spoon\nwool, woolen, woollen\nworm fence, snake fence, snake-rail fence, Virginia fence\nwreck\nyawl\nyurt\nweb site, website, internet site, site\ncomic book\ncrossword puzzle, crossword\nstreet sign\ntraffic light, traffic signal, stoplight\nbook jacket, dust cover, dust jacket, dust wrapper\nmenu\nplate\nguacamole\nconsomme\nhot pot, hotpot\ntrifle\nice cream, icecream\nice lolly, lolly, lollipop, popsicle\nFrench loaf\nbagel, beigel\npretzel\ncheeseburger\nhotdog, hot dog, red hot\nmashed potato\nhead cabbage\nbroccoli\ncauliflower\nzucchini, courgette\nspaghetti squash\nacorn squash\nbutternut squash\ncucumber, cuke\nartichoke, globe artichoke\nbell pepper\ncardoon\nmushroom\nGranny Smith\nstrawberry\norange\nlemon\nfig\npineapple, ananas\nbanana\njackfruit, jak, jack\ncustard apple\npomegranate\nhay\ncarbonara\nchocolate sauce, chocolate syrup\ndough\nmeat loaf, meatloaf\npizza, pizza pie\npotpie\nburrito\nred wine\nespresso\ncup\neggnog\nalp\nbubble\ncliff, drop, drop-off\ncoral reef\ngeyser\nlakeside, lakeshore\npromontory, headland, head, foreland\nsandbar, sand bar\nseashore, coast, seacoast, sea-coast\nvalley, vale\nvolcano\nballplayer, baseball player\ngroom, bridegroom\nscuba diver\nrapeseed\ndaisy\nyellow lady\'s slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\ncorn\nacorn\nhip, rose hip, rosehip\nbuckeye, horse chestnut, conker\ncoral fungus\nagaric\ngyromitra\nstinkhorn, carrion fungus\nearthstar\nhen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\nbolete\near, spike, capitulum\ntoilet tissue, toilet paper, bathroom tissue\'\'\'.split(""\\n"")'"
nets/nets_utils.py,0,"b'from tensorflow.python import pywrap_tensorflow\nimport collections\nimport numpy as np\n\n\nvar_stat = collections.namedtuple(\'stats\', [\'mean\', \'median\', \'std\'])\n\n\ndef get_variables_in_checkpoint_file(file_name):\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        var_to_shape_map = reader.get_variable_to_shape_map()\n        return var_to_shape_map\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\n\ndef get_tensor_static_val(file_name, all_tensors, all_tensor_names):\n    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n    vars_dict = {}\n    if all_tensors or all_tensor_names:\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in sorted(var_to_shape_map):\n        if all_tensors:\n          vars_dict[key] = var_stat(np.mean(reader.get_tensor(key)), np.median(reader.get_tensor(key)),\n                                    np.std(reader.get_tensor(key)))\n    return vars_dict'"
nets/networks.py,3,"b""import tensorflow as tf\nfrom vgg16 import get_vgg16\nfrom vgg19 import get_vgg19\n\n\ndef get_model(inputs, sess, type, pretrained=True):\n    if type == 'vgg16':\n        return get_vgg16(inputs, sess, pretrained)\n    elif type == 'vgg19':\n        return get_vgg19(inputs, sess, pretrained)\n\n\nif __name__ == '__main__':\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    x = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3], name='inpust')\n    with tf.Session(config=tfconfig) as sess:\n        network = get_model(x, sess, type='vgg19', pretrained=True)\n        network.print_params()\n        network.print_layers()"""
nets/resnet.py,44,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorflow.contrib.layers.python.layers import utils\nimport collections\nfrom tensorlayer.layers import Layer, list_remove_repeat\nfrom nets_utils import get_variables_in_checkpoint_file\n\n\nclass ElementwiseLayer(Layer):\n    """"""\n    The :class:`ElementwiseLayer` class combines multiple :class:`Layer` which have the same output shapes by a given elemwise-wise operation.\n\n    Parameters\n    ----------\n    layer : a list of :class:`Layer` instances\n        The `Layer` class feeding into this layer.\n    combine_fn : a TensorFlow elemwise-merge function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`_ .\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n        self,\n        layer = [],\n        combine_fn = tf.minimum,\n        name =\'elementwise_layer\',\n        act = None,\n    ):\n        Layer.__init__(self, name=name)\n\n        if act:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s, act:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__, act.__name__))\n        else:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__))\n\n        self.outputs = layer[0].outputs\n        # print(self.outputs._shape, type(self.outputs._shape))\n        for l in layer[1:]:\n            # assert str(self.outputs.get_shape()) == str(l.outputs.get_shape()), ""Hint: the input shapes should be the same. %s != %s"" %  (self.outputs.get_shape() , str(l.outputs.get_shape()))\n            self.outputs = combine_fn(self.outputs, l.outputs, name=name)\n        if act:\n            self.outputs = act(self.outputs)\n        self.all_layers = list(layer[0].all_layers)\n        self.all_params = list(layer[0].all_params)\n        self.all_drop = dict(layer[0].all_drop)\n\n        for i in range(1, len(layer)):\n            self.all_layers.extend(list(layer[i].all_layers))\n            self.all_params.extend(list(layer[i].all_params))\n            self.all_drop.update(dict(layer[i].all_drop))\n\n        self.all_layers = list_remove_repeat(self.all_layers)\n        self.all_params = list_remove_repeat(self.all_params)\n\n\ndef subsample(inputs, factor, scope=None):\n    if factor == 1:\n        return inputs\n    else:\n        return tl.layers.MaxPool2d(inputs, [1, 1], strides=(factor, factor), name=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, strides, rate=1, scope=None):\n    \'\'\'\n    Reference slim resnet\n    :param inputs:\n    :param num_outputs:\n    :param kernel_size:\n    :param strides:\n    :param rate:\n    :param scope:\n    :return:\n    \'\'\'\n    if strides == 1:\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                   strides=(strides, strides), act=None, padding=\'SAME\', name=scope)\n            nets = tl.layers.BatchNormLayer(nets, act=tf.nn.relu, is_train=True, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size),\n                                               rate=rate, act=None, padding=\'SAME\', name=scope)\n            nets = tl.layers.BatchNormLayer(nets, act=tf.nn.relu, is_train=True, name=scope+\'_bn/BatchNorm\')\n        return nets\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        inputs = tl.layers.PadLayer(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], name=\'padding_%s\' % scope)\n        if rate == 1:\n            nets = tl.layers.Conv2d(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                    strides=(strides, strides), act=None, padding=\'VALID\', name=scope)\n            nets = tl.layers.BatchNormLayer(nets, act=tf.nn.relu, is_train=True, name=scope+\'_bn/BatchNorm\')\n        else:\n            nets = tl.layers.AtrousConv2dLayer(inputs, n_filter=num_outputs, filter_size=(kernel_size, kernel_size), b_init=None,\n                                              rate=rate, act=None, padding=\'SAME\', name=scope)\n            nets = tl.layers.BatchNormLayer(nets, act=tf.nn.relu, is_train=True, name=scope+\'_bn/BatchNorm\')\n        return nets\n\n\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1, scope=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        b_init=None, name=\'shortcut_conv\')\n            shortcut = tl.layers.BatchNormLayer(shortcut, act=tf.identity, is_train=True, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = tl.layers.Conv2d(inputs, depth_bottleneck, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                    name=\'conv1\')\n        residual = tl.layers.BatchNormLayer(residual, act=tf.nn.relu, is_train=True, name=\'conv1_bn/BatchNorm\')\n\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth_bottleneck, kernel_size=3, strides= stride, rate=rate, scope=\'conv2\')\n\n        # bottleneck layer 3\n        residual = tl.layers.Conv2d(residual, depth, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                    name=\'conv3\')\n        residual = tl.layers.BatchNormLayer(residual, act=tf.identity, is_train=True, name=\'conv3_bn/BatchNorm\')\n\n        output = ElementwiseLayer(layer=[shortcut, residual],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef bottleneck_SE(inputs, depth, depth_bottleneck, stride, rate=1, scope=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        b_init=None, name=\'shortcut_conv\')\n            shortcut = tl.layers.BatchNormLayer(shortcut, act=tf.identity, is_train=True, name=\'shortcut_bn/BatchNorm\')\n        # bottleneck layer 1\n        residual = tl.layers.Conv2d(inputs, depth_bottleneck, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                    name=\'conv1\')\n        residual = tl.layers.BatchNormLayer(residual, act=tf.nn.relu, is_train=True, name=\'conv1_bn/BatchNorm\')\n\n        # bottleneck layer 2\n        residual = conv2d_same(residual, depth_bottleneck, kernel_size=3, strides= stride, rate=rate, scope=\'conv2\')\n\n        # bottleneck layer 3\n        residual = tl.layers.Conv2d(residual, depth, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                    name=\'conv3\')\n        residual = tl.layers.BatchNormLayer(residual, act=tf.identity, is_train=True, name=\'conv3_bn/BatchNorm\')\n\n        # squeeze\n        squeeze = tl.layers.InputLayer(tf.reduce_mean(residual.outputs, axis=[1, 2]), name=\'squeeze_layer\')\n        # excitation\n        excitation1 = tl.layers.DenseLayer(squeeze, n_units=int(depth/16.0), act=tf.nn.relu, name=\'excitation_1\')\n        excitation2 = tl.layers.DenseLayer(excitation1, n_units=depth, act=tf.nn.sigmoid, name=\'excitation_2\')\n        # scale\n        scale = tl.layers.ReshapeLayer(excitation2, shape=[tf.shape(excitation2.outputs)[0], 1, 1, depth], name=\'excitation_reshape\')\n\n        residual_se = ElementwiseLayer(layer=[residual, scale],\n                                    combine_fn=tf.multiply,\n                                    name=\'scale_layer\',\n                                    act=None)\n\n        output = ElementwiseLayer(layer=[shortcut, residual_se],\n                                  combine_fn=tf.add,\n                                  name=\'combine_layer\',\n                                  act=tf.nn.relu)\n        return output\n\n\ndef bottleneck_Xt(inputs, depth, stride, cardinality, cardinality_dim, rate=1, scope=None):\n    with tf.variable_scope(scope, \'bottleneck_v1\') as sc:\n        depth_in = utils.last_dimension(inputs.outputs.get_shape(), min_rank=4)\n        if depth == depth_in:\n            shortcut = subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = tl.layers.Conv2d(inputs, depth, filter_size=(1, 1), strides=(stride, stride), act=None,\n                                        b_init=None, name=\'shortcut_conv\')\n            shortcut = tl.layers.BatchNormLayer(shortcut, act=tf.identity, is_train=True, name=\'shortcut_bn/BatchNorm\')\n\n        cardinality_layers = []\n        for i in range(cardinality):\n            # bottleneck layer 1\n            residual = tl.layers.Conv2d(inputs, cardinality_dim, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                        name=\'cardinality_%d/conv1\' % i)\n            residual = tl.layers.BatchNormLayer(residual, act=tf.nn.relu, is_train=True, name=\'cardinality_%d/conv1_bn/BatchNorm\' % i)\n\n            # bottleneck layer 2\n            residual = conv2d_same(residual, cardinality_dim, kernel_size=3, strides= stride, rate=rate, scope=\'cardinality_%d/conv2\' % i)\n\n            # bottleneck layer 3\n            residual = tl.layers.Conv2d(residual, depth, filter_size=(1, 1), strides=(1, 1), act=None, b_init=None,\n                                        name=\'cardinality_%d/conv3\' % i)\n            residual = tl.layers.BatchNormLayer(residual, act=tf.identity, is_train=True, name=\'cardinality_%d/conv3_bn/BatchNorm\' % i)\n            cardinality_layers.append(residual)\n\n        residual_total = ElementwiseLayer(layer=cardinality_layers,\n                                          combine_fn=tf.add,\n                                          name=\'cardinality_cmobine\',\n                                          act=None)\n        with tf.control_dependencies([residual_total.outputs]):\n            output = ElementwiseLayer(layer=[shortcut, residual_total],\n                                          combine_fn=tf.add,\n                                          name=\'combine_layer\',\n                                          act=tf.nn.relu)\n        return output\n\n\ndef resnet(inputs, bottle_neck, blocks, num_classes=1000, scope=None, type=None):\n    # mean_rgb_var = tf.Variable()\n    with tf.variable_scope(scope):\n        mean_rgb_var = tf.Variable(dtype=tf.float32, name=\'mean_rgb\', trainable=False, initial_value=[128.0, 128.0, 128.0])\n        rgb_mean_dims = tf.reshape(mean_rgb_var, shape=[1, 1, 1, 3])\n        inputs = tf.subtract(inputs, rgb_mean_dims)\n        net_inputs = tl.layers.InputLayer(inputs, name=\'input_layer\')\n        if bottle_neck:\n            net = conv2d_same(net_inputs, 64, 7, strides=2, rate=1, scope=\'conv1\')\n            net = tl.layers.MaxPool2d(net, (2, 2), padding=\'SAME\', name=\'pool1\')\n        else:\n            raise ValueError(\'The standard resnet must support the bottleneck layer\')\n        for block in blocks:\n            with tf.variable_scope(block.scope):\n                for i, var in enumerate(block.args):\n                    with tf.variable_scope(\'unit_%d\' % (i+1)):\n                        if type==\'resnext\':\n                            net = block.unit_fn(net, depth=var[\'depth\'], stride=var[\'stride\'],\n                                                cardinality=var[\'cardinality\'],\n                                                cardinality_dim=var[\'cardinality_dim\'], rate=1, scope=None)\n                        else:\n                            net = block.unit_fn(net, depth=var[\'depth\'], depth_bottleneck=var[\'depth_bottleneck\'],\n                                                stride=var[\'stride\'], rate=var[\'rate\'], scope=None)\n        net.outputs = tf.reduce_mean(net.outputs, [1, 2], keep_dims=True)\n        net = tl.layers.Conv2d(net, num_classes, filter_size=(1, 1), strides=(1, 1), act=None, name=\'logits\')\n        net.outputs = tf.squeeze(net.outputs, [1, 2], name=\'SpatialSqueeze\')\n        return net\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n    """"""A named tuple describing a ResNet block.\n\n    Its parts are:\n      scope: The scope of the `Block`.\n      unit_fn: The ResNet unit function which takes as input a `Tensor` and\n        returns another `Tensor` with the output of the ResNet unit.\n      args: A list of length equal to the number of units in the `Block`. The list\n        contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n        block to serve as argument to unit_fn.\n    """"""\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride, rate=1):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }])\n\n\ndef resnetse_v1_block(scope, base_depth, num_units, stride, rate=1):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, bottleneck_SE, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1,\n      \'rate\': rate\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride,\n      \'rate\': rate\n  }])\n\n\ndef resnext_v1_block(scope, base_depth, num_units, stride, cardinality, cardinality_dim, rate=1):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return Block(scope, bottleneck_Xt, [{\n      \'depth\': base_depth * 4,\n      \'stride\': 1,\n      \'rate\': rate,\n      \'cardinality\': cardinality,\n      \'cardinality_dim\': cardinality_dim\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'stride\': stride,\n      \'rate\': 1,\n      \'cardinality\': cardinality,\n      \'cardinality_dim\': cardinality_dim\n  }])\n\n\ndef get_resnet(inputs, num_classes, num_layers, type=\'resnet\', sess=None, pretrained=True):\n    \'\'\'\n    :param inputs: inputs is an tensorflow placeholder\n    :param num_classes:\n    :param num_layers:\n    :param type: choose weather using se xt or just oridinary resnet  [\'resnet\', \'resnetse\', \'resnext\']\n    :return:\n    \'\'\'\n    if type == \'resnet\':\n        if num_layers == 50:\n            blocks = [\n                resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1),\n                resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1),\n                resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2, rate=1),\n                resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1)\n            ]\n            ckpt_file_path = \'../model_weights/resnet_v1_50.ckpt\'\n        elif num_layers == 101:\n            blocks = [\n                resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1),\n                resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1),\n                resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2, rate=1),\n                resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1)\n            ]\n            ckpt_file_path = \'../model_weights/resnet_v1_101.ckpt\'\n        elif num_layers == 152:\n            blocks = [\n                resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1),\n                resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1),\n                resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1),\n                resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1)\n            ]\n            ckpt_file_path = \'../model_weights/resnet_v1_152.ckpt\'\n        else:\n            raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n        net = resnet(inputs=inputs,\n                     bottle_neck=True,\n                     blocks=blocks,\n                     num_classes=num_classes,\n                     scope=\'resnet_v1_%d\' % num_layers,\n                     type=type)\n        if pretrained:\n            var_ckpt = get_variables_in_checkpoint_file(ckpt_file_path)\n            vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n            vars_dict = {}\n            for var in vars:\n                var_name = var.op.name\n                var_name_new = var_name\n                if \'_bn\' in var_name:\n                    var_name_new = var_name_new.replace(\'_bn\', \'\')\n                if \'W_conv2d\' in var_name:\n                    var_name_new = var_name_new.replace(\'W_conv2d\', \'weights\')\n                if \'b_conv2d\' in var_name:\n                    var_name_new = var_name_new.replace(\'b_conv2d\', \'biases\')\n                if \'shortcut_conv\' in var_name:\n                    var_name_new = var_name_new.replace(\'shortcut_conv\', \'shortcut\')\n                print(var_name_new)\n                if var_name_new in var_ckpt:\n                    vars_dict[var_name_new] = var\n            tl.layers.initialize_global_variables(sess)\n            if len(vars_dict.keys()) > 0:\n                saver = tf.train.Saver(vars_dict)\n                saver.restore(sess, ckpt_file_path)\n            return net\n        else:\n            return net\n    elif type == \'resnetse\':\n        if pretrained:\n            raise ValueError(\'resnetse have no pretrained model!\')\n        if num_layers == 50:\n            blocks = [\n                resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1),\n                resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1),\n                resnetse_v1_block(\'block3\', base_depth=256, num_units=6, stride=2, rate=1),\n                resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1)\n            ]\n        elif num_layers == 101:\n            blocks = [\n                resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1),\n                resnetse_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1),\n                resnetse_v1_block(\'block3\', base_depth=256, num_units=23, stride=2, rate=1),\n                resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1)\n            ]\n        elif num_layers == 152:\n            blocks = [\n                resnetse_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1),\n                resnetse_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1),\n                resnetse_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1),\n                resnetse_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1)\n            ]\n        else:\n            raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n        net = resnet(inputs=inputs,\n                     bottle_neck=True,\n                     blocks=blocks,\n                     num_classes=num_classes,\n                     scope=\'resnet_v1_%d\' % num_layers,\n                     type=type)\n        return net\n    elif type == \'resnext\':\n        if pretrained:\n            raise ValueError(\'resnetse have no pretrained model!\')\n        if num_layers == 50:\n            blocks = [\n                resnext_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block3\', base_depth=256, num_units=6, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1, cardinality=32,\n                                 cardinality_dim=4)\n            ]\n        elif num_layers == 101:\n            blocks = [\n                resnext_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block2\', base_depth=128, num_units=4, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block3\', base_depth=256, num_units=23, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1, cardinality=32,\n                                 cardinality_dim=4)\n            ]\n        elif num_layers == 152:\n            blocks = [\n                resnext_v1_block(\'block1\', base_depth=64, num_units=3, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block2\', base_depth=128, num_units=8, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block3\', base_depth=256, num_units=36, stride=2, rate=1, cardinality=32,\n                                 cardinality_dim=4),\n                resnext_v1_block(\'block4\', base_depth=512, num_units=3, stride=1, rate=1, cardinality=32,\n                                 cardinality_dim=4)\n            ]\n        else:\n            raise ValueError(\'Resnet layer %d is not supported now.\' % num_layers)\n        net = resnet(inputs=inputs,\n                     bottle_neck=True,\n                     blocks=blocks,\n                     num_classes=num_classes,\n                     scope=\'resnet_v1_%d\' % num_layers,\n                     type=type)\n        return net\n    else:\n        raise ValueError(\'resnet type %s does not support now!\' % type)\n\n\nif __name__ == \'__main__\':\n        x = tf.placeholder(dtype=tf.float32, shape=[1, 224, 224, 3], name=\'input_place\')\n        sess = tf.Session()\n        # # test resnet\n        # nets = get_resnet(x, 1000, 50, type=\'resnet\', sess=sess, pretrained=True)\n        # with sess:\n        #     nets.print_params()\n\n        # # test resnetse\n        # nets = get_resnet(x, 1000, 50, type=\'resnetse\', sess=sess, pretrained=False)\n        # tl.layers.initialize_global_variables(sess)\n        # with sess:\n        #     nets.print_params()\n\n        # test resnext\n        nets = get_resnet(x, 1000, 50, type=\'resnext\', sess=sess, pretrained=False)\n        tl.layers.initialize_global_variables(sess)\n        with sess:\n            nets.print_params()'"
nets/tl_layers_modify.py,104,"b'import tensorflow as tf\nimport tensorlayer as tl\nfrom tensorlayer.layers import Layer, list_remove_repeat\n\n\nD_TYPE = tf.float32\nTF_GRAPHKEYS_VARIABLES = tf.GraphKeys.GLOBAL_VARIABLES\n\n\nclass ElementwiseLayer(Layer):\n    """"""\n    The :class:`ElementwiseLayer` class combines multiple :class:`Layer` which have the same output shapes by a given elemwise-wise operation.\n\n    Parameters\n    ----------\n    layer : a list of :class:`Layer` instances\n        The `Layer` class feeding into this layer.\n    combine_fn : a TensorFlow elemwise-merge function\n        e.g. AND is ``tf.minimum`` ;  OR is ``tf.maximum`` ; ADD is ``tf.add`` ; MUL is ``tf.multiply`` and so on.\n        See `TensorFlow Math API <https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#math>`_ .\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n        self,\n        layer = [],\n        combine_fn = tf.minimum,\n        name =\'elementwise_layer\',\n        act = None,\n    ):\n        Layer.__init__(self, name=name)\n\n        if act:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s, act:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__, act.__name__))\n        else:\n            print(""  [TL] ElementwiseLayer %s: size:%s fn:%s"" % (\n            self.name, layer[0].outputs.get_shape(), combine_fn.__name__))\n\n        self.outputs = layer[0].outputs\n        # print(self.outputs._shape, type(self.outputs._shape))\n        for l in layer[1:]:\n            # assert str(self.outputs.get_shape()) == str(l.outputs.get_shape()), ""Hint: the input shapes should be the same. %s != %s"" %  (self.outputs.get_shape() , str(l.outputs.get_shape()))\n            self.outputs = combine_fn(self.outputs, l.outputs, name=name)\n        if act:\n            self.outputs = act(self.outputs)\n        self.all_layers = list(layer[0].all_layers)\n        self.all_params = list(layer[0].all_params)\n        self.all_drop = dict(layer[0].all_drop)\n\n        for i in range(1, len(layer)):\n            self.all_layers.extend(list(layer[i].all_layers))\n            self.all_params.extend(list(layer[i].all_params))\n            self.all_drop.update(dict(layer[i].all_drop))\n\n        self.all_layers = list_remove_repeat(self.all_layers)\n        self.all_params = list_remove_repeat(self.all_params)\n\n\nclass BatchNormLayer(Layer):\n    """"""\n    The :class:`BatchNormLayer` class is a normalization layer, see ``tf.nn.batch_normalization`` and ``tf.nn.moments``.\n\n    Batch normalization on fully-connected or convolutional maps.\n\n    ```\n        https://www.tensorflow.org/api_docs/python/tf/cond\n        If x < y, the tf.add operation will be executed and tf.square operation will not be executed.\n        Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally.\n    ```\n\n    Parameters\n    -----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    decay : float, default is 0.9.\n        A decay factor for ExponentialMovingAverage, use larger value for large dataset.\n    epsilon : float\n        A small float number to avoid dividing by 0.\n    act : activation function.\n    is_train : boolean\n        Whether train or inference.\n    beta_init : beta initializer\n        The initializer for initializing beta\n    gamma_init : gamma initializer\n        The initializer for initializing gamma\n    dtype : tf.float32 (default) or tf.float16\n    name : a string or None\n        An optional name to attach to this layer.\n\n    References\n    ----------\n    - `Source <https://github.com/ry/tensorflow-resnet/blob/master/resnet.py>`_\n    - `stackoverflow <http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow>`_\n\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            decay=0.9,\n            epsilon=2e-5,\n            act=tf.identity,\n            is_train=False,\n            fix_gamma=True,\n            beta_init=tf.zeros_initializer,\n            gamma_init=tf.random_normal_initializer(mean=1.0, stddev=0.002),  # tf.ones_initializer,\n            # dtype = tf.float32,\n            trainable=None,\n            name=\'batchnorm_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        print(""  [TL] BatchNormLayer %s: decay:%f epsilon:%f act:%s is_train:%s"" % (self.name, decay, epsilon, act.__name__, is_train))\n        x_shape = self.inputs.get_shape()\n        params_shape = x_shape[-1:]\n\n        from tensorflow.python.training import moving_averages\n        from tensorflow.python.ops import control_flow_ops\n\n        with tf.variable_scope(name) as vs:\n            axis = list(range(len(x_shape) - 1))\n\n            ## 1. beta, gamma\n            if tf.__version__ > \'0.12.1\' and beta_init == tf.zeros_initializer:\n                beta_init = beta_init()\n            with tf.device(\'/cpu:0\'):\n                beta = tf.get_variable(\'beta\', shape=params_shape, initializer=beta_init, dtype=tf.float32, trainable=is_train)  #, restore=restore)\n\n                gamma = tf.get_variable(\n                    \'gamma\',\n                    shape=params_shape,\n                    initializer=gamma_init,\n                    dtype=tf.float32,\n                    trainable=fix_gamma,\n                )  #restore=restore)\n\n            ## 2.\n            if tf.__version__ > \'0.12.1\':\n                moving_mean_init = tf.zeros_initializer()\n            else:\n                moving_mean_init = tf.zeros_initializer\n            with tf.device(\'/cpu:0\'):\n                moving_mean = tf.get_variable(\'moving_mean\', params_shape, initializer=moving_mean_init, dtype=tf.float32, trainable=False)  #   restore=restore)\n                moving_variance = tf.get_variable(\n                    \'moving_variance\',\n                    params_shape,\n                    initializer=tf.constant_initializer(1.),\n                    dtype=tf.float32,\n                    trainable=False,\n                )  #   restore=restore)\n\n            ## 3.\n            # These ops will only be preformed when training.\n            mean, variance = tf.nn.moments(self.inputs, axis)\n            try:  # TF12\n                update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay, zero_debias=False)  # if zero_debias=True, has bias\n                update_moving_variance = moving_averages.assign_moving_average(\n                    moving_variance, variance, decay, zero_debias=False)  # if zero_debias=True, has bias\n                # print(""TF12 moving"")\n            except Exception as e:  # TF11\n                update_moving_mean = moving_averages.assign_moving_average(moving_mean, mean, decay)\n                update_moving_variance = moving_averages.assign_moving_average(moving_variance, variance, decay)\n                # print(""TF11 moving"")\n\n            def mean_var_with_update():\n                with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n                    return tf.identity(mean), tf.identity(variance)\n            if trainable:\n                mean, var = mean_var_with_update()\n                print(mean)\n                print(var)\n                self.outputs = act(tf.nn.batch_normalization(self.inputs, mean, var, beta, gamma, epsilon))\n            else:\n                self.outputs = act(tf.nn.batch_normalization(self.inputs, moving_mean, moving_variance, beta, gamma, epsilon))\n            variables = [beta, gamma, moving_mean, moving_variance]\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend(variables)\n\n\ndef Conv2d(\n        net,\n        n_filter=32,\n        filter_size=(3, 3),\n        strides=(1, 1),\n        act=None,\n        padding=\'SAME\',\n        W_init=tf.truncated_normal_initializer(stddev=0.02),\n        b_init=tf.constant_initializer(value=0.0),\n        W_init_args={},\n        b_init_args={},\n        use_cudnn_on_gpu=None,\n        data_format=None,\n        name=\'conv2d\',\n):\n    """"""Wrapper for :class:`Conv2dLayer`, if you don\'t understand how to use :class:`Conv2dLayer`, this function may be easier.\n\n    Parameters\n    ----------\n    net : TensorLayer layer.\n    n_filter : number of filter.\n    filter_size : tuple (height, width) for filter size.\n    strides : tuple (height, width) for strides.\n    act : None or activation function.\n    others : see :class:`Conv2dLayer`.\n\n    Examples\n    --------\n    >>> w_init = tf.truncated_normal_initializer(stddev=0.01)\n    >>> b_init = tf.constant_initializer(value=0.0)\n    >>> inputs = InputLayer(x, name=\'inputs\')\n    >>> conv1 = Conv2d(inputs, 64, (3, 3), act=tf.nn.relu, padding=\'SAME\', W_init=w_init, b_init=b_init, name=\'conv1_1\')\n    >>> conv1 = Conv2d(conv1, 64, (3, 3), act=tf.nn.relu, padding=\'SAME\', W_init=w_init, b_init=b_init, name=\'conv1_2\')\n    >>> pool1 = MaxPool2d(conv1, (2, 2), padding=\'SAME\', name=\'pool1\')\n    >>> conv2 = Conv2d(pool1, 128, (3, 3), act=tf.nn.relu, padding=\'SAME\', W_init=w_init, b_init=b_init, name=\'conv2_1\')\n    >>> conv2 = Conv2d(conv2, 128, (3, 3), act=tf.nn.relu, padding=\'SAME\', W_init=w_init, b_init=b_init, name=\'conv2_2\')\n    >>> pool2 = MaxPool2d(conv2, (2, 2), padding=\'SAME\', name=\'pool2\')\n    """"""\n    assert len(strides) == 2, ""len(strides) should be 2, Conv2d and Conv2dLayer are different.""\n    if act is None:\n        act = tf.identity\n\n    try:\n        pre_channel = int(net.outputs.get_shape()[-1])\n    except:  # if pre_channel is ?, it happens when using Spatial Transformer Net\n        pre_channel = 1\n        print(""[warnings] unknow input channels, set to 1"")\n    net = Conv2dLayer(\n        net,\n        act=act,\n        shape=[filter_size[0], filter_size[1], pre_channel, n_filter],  # 32 features for each 5x5 patch\n        strides=[1, strides[0], strides[1], 1],\n        padding=padding,\n        W_init=W_init,\n        W_init_args=W_init_args,\n        b_init=b_init,\n        b_init_args=b_init_args,\n        use_cudnn_on_gpu=use_cudnn_on_gpu,\n        data_format=data_format,\n        name=name)\n    return net\n\n\nclass Conv2dLayer(Layer):\n    """"""\n    The :class:`Conv2dLayer` class is a 2D CNN layer, see `tf.nn.conv2d <https://www.tensorflow.org/versions/master/api_docs/python/nn.html#conv2d>`_.\n\n    Parameters\n    ----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    act : activation function\n        The function that is applied to the layer activations.\n    shape : list of shape\n        shape of the filters, [filter_height, filter_width, in_channels, out_channels].\n    strides : a list of ints.\n        The stride of the sliding window for each dimension of input.\\n\n        It Must be in the same order as the dimension specified with format.\n    padding : a string from: ""SAME"", ""VALID"".\n        The type of padding algorithm to use.\n    W_init : weights initializer\n        The initializer for initializing the weight matrix.\n    b_init : biases initializer or None\n        The initializer for initializing the bias vector. If None, skip biases.\n    W_init_args : dictionary\n        The arguments for the weights tf.get_variable().\n    b_init_args : dictionary\n        The arguments for the biases tf.get_variable().\n    use_cudnn_on_gpu : bool, default is None.\n    data_format : string ""NHWC"" or ""NCHW"", default is ""NHWC""\n    name : a string or None\n        An optional name to attach to this layer.\n\n    Notes\n    ------\n    - shape = [h, w, the number of output channel of previous layer, the number of output channels]\n    - the number of output channel of a layer is its last dimension.\n\n    Examples\n    --------\n    >>> x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n    >>> network = tl.layers.InputLayer(x, name=\'input_layer\')\n    >>> network = tl.layers.Conv2dLayer(network,\n    ...                   act = tf.nn.relu,\n    ...                   shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch\n    ...                   strides=[1, 1, 1, 1],\n    ...                   padding=\'SAME\',\n    ...                   W_init=tf.truncated_normal_initializer(stddev=5e-2),\n    ...                   W_init_args={},\n    ...                   b_init = tf.constant_initializer(value=0.0),\n    ...                   b_init_args = {},\n    ...                   name =\'cnn_layer1\')     # output: (?, 28, 28, 32)\n    >>> network = tl.layers.PoolLayer(network,\n    ...                   ksize=[1, 2, 2, 1],\n    ...                   strides=[1, 2, 2, 1],\n    ...                   padding=\'SAME\',\n    ...                   pool = tf.nn.max_pool,\n    ...                   name =\'pool_layer1\',)   # output: (?, 14, 14, 32)\n\n    >>> Without TensorLayer, you can implement 2d convolution as follow.\n    >>> W = tf.Variable(W_init(shape=[5, 5, 1, 32], ), name=\'W_conv\')\n    >>> b = tf.Variable(b_init(shape=[32], ), name=\'b_conv\')\n    >>> outputs = tf.nn.relu( tf.nn.conv2d(inputs, W,\n    ...                       strides=[1, 1, 1, 1],\n    ...                       padding=\'SAME\') + b )\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            act=tf.identity,\n            shape=[5, 5, 1, 100],\n            strides=[1, 1, 1, 1],\n            padding=\'SAME\',\n            W_init=tf.truncated_normal_initializer(stddev=0.02),\n            b_init=tf.constant_initializer(value=0.0),\n            W_init_args={},\n            b_init_args={},\n            use_cudnn_on_gpu=None,\n            data_format=None,\n            name=\'cnn_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        print(""  [TL] Conv2dLayer %s: shape:%s strides:%s pad:%s act:%s"" % (self.name, str(shape), str(strides), padding, act.__name__))\n\n        with tf.variable_scope(name) as vs:\n            with tf.device(\'/cpu:0\'):\n                W = tf.get_variable(name=\'W_conv2d\', shape=shape, initializer=W_init, dtype=D_TYPE, **W_init_args)\n            if b_init:\n                with tf.device(\'/cpu:0\'):\n                    b = tf.get_variable(name=\'b_conv2d\', shape=(shape[-1]), initializer=b_init, dtype=D_TYPE, **b_init_args)\n                self.outputs = act(\n                    tf.nn.conv2d(self.inputs, W, strides=strides, padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format) + b)\n            else:\n                self.outputs = act(tf.nn.conv2d(self.inputs, W, strides=strides, padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format))\n\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        if b_init:\n            self.all_params.extend([W, b])\n        else:\n            self.all_params.extend([W])\n\n\n## Special activation\nclass PReluLayer(Layer):\n    """"""\n    The :class:`PReluLayer` class is Parametric Rectified Linear layer.\n\n    Parameters\n    ----------\n    x : A `Tensor` with type `float`, `double`, `int32`, `int64`, `uint8`,\n        `int16`, or `int8`.\n    channel_shared : `bool`. Single weight is shared by all channels\n    a_init : alpha initializer, default zero constant.\n        The initializer for initializing the alphas.\n    a_init_args : dictionary\n        The arguments for the weights initializer.\n    name : A name for this activation op (optional).\n\n    References\n    -----------\n    - `Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification <http://arxiv.org/pdf/1502.01852v1.pdf>`_\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            channel_shared=False,\n            a_init=tf.constant_initializer(value=0.0),\n            a_init_args={},\n            # restore = True,\n            name=""prelu_layer""):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        print(""  [TL] PReluLayer %s: channel_shared:%s"" % (self.name, channel_shared))\n        if channel_shared:\n            w_shape = (1, )\n        else:\n            w_shape = int(self.inputs.get_shape()[-1])\n\n        # with tf.name_scope(name) as scope:\n        with tf.variable_scope(name) as vs:\n            with tf.device(\'/cpu:0\'):\n                alphas = tf.get_variable(name=\'alphas\', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args)\n            try:  ## TF 1.0\n                self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5\n            except:  ## TF 0.12\n                self.outputs = tf.nn.relu(self.inputs) + tf.mul(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5\n\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend([alphas])\n\n\n## Dense layer\nclass DenseLayer(Layer):\n    """"""\n    The :class:`DenseLayer` class is a fully connected layer.\n\n    Parameters\n    ----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    n_units : int\n        The number of units of the layer.\n    act : activation function\n        The function that is applied to the layer activations.\n    W_init : weights initializer\n        The initializer for initializing the weight matrix.\n    b_init : biases initializer or None\n        The initializer for initializing the bias vector. If None, skip biases.\n    W_init_args : dictionary\n        The arguments for the weights tf.get_variable.\n    b_init_args : dictionary\n        The arguments for the biases tf.get_variable.\n    name : a string or None\n        An optional name to attach to this layer.\n\n    Examples\n    --------\n    >>> network = tl.layers.InputLayer(x, name=\'input_layer\')\n    >>> network = tl.layers.DenseLayer(\n    ...                 network,\n    ...                 n_units=800,\n    ...                 act = tf.nn.relu,\n    ...                 W_init=tf.truncated_normal_initializer(stddev=0.1),\n    ...                 name =\'relu_layer\'\n    ...                 )\n\n    >>> Without TensorLayer, you can do as follow.\n    >>> W = tf.Variable(\n    ...     tf.random_uniform([n_in, n_units], -1.0, 1.0), name=\'W\')\n    >>> b = tf.Variable(tf.zeros(shape=[n_units]), name=\'b\')\n    >>> y = tf.nn.relu(tf.matmul(inputs, W) + b)\n\n    Notes\n    -----\n    If the input to this layer has more than two axes, it need to flatten the\n    input by using :class:`FlattenLayer` in this case.\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            n_units=100,\n            act=tf.identity,\n            W_init=tf.truncated_normal_initializer(stddev=0.1),\n            b_init=tf.constant_initializer(value=0.0),\n            W_init_args={},\n            b_init_args={},\n            name=\'dense_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        if self.inputs.get_shape().ndims != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        n_in = int(self.inputs.get_shape()[-1])\n        self.n_units = n_units\n        print(""  [TL] DenseLayer  %s: %d %s"" % (self.name, self.n_units, act.__name__))\n        with tf.variable_scope(name) as vs:\n            with tf.device(\'/cpu:0\'):\n                W = tf.get_variable(name=\'W\', shape=(n_in, n_units), initializer=W_init, dtype=D_TYPE, **W_init_args)\n            if b_init is not None:\n                try:\n                    with tf.device(\'/cpu:0\'):\n                        b = tf.get_variable(name=\'b\', shape=(n_units), initializer=b_init, dtype=D_TYPE, **b_init_args)\n                except:  # If initializer is a constant, do not specify shape.\n                    with tf.device(\'/cpu:0\'):\n                        b = tf.get_variable(name=\'b\', initializer=b_init, dtype=D_TYPE, **b_init_args)\n                self.outputs = act(tf.matmul(self.inputs, W) + b)\n            else:\n                self.outputs = act(tf.matmul(self.inputs, W))\n\n        # Hint : list(), dict() is pass by value (shallow), without them, it is\n        # pass by reference.\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        if b_init is not None:\n            self.all_params.extend([W, b])\n        else:\n            self.all_params.extend([W])\n\n\ndef get_shape(input_tensor):\n    static_shape = input_tensor.get_shape().as_list()\n    dynamic_shape = tf.unstack(tf.shape(input_tensor))\n    final_shapes = [shapes[1] if shapes[0] is None else shapes[0] for shapes in zip(static_shape, dynamic_shape)]\n    return final_shapes\n\n\n## Group Batch Normalization layer\nclass GroupNormLayer(Layer):\n    """"""The :class:`GroupNormLayer` class is a for instance normalization.\n       The implementation is based on paper [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)\n    Parameters\n    -----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    act : activation function.\n    epsilon : float\n        A small float number.\n    scale_init : beta initializer\n        The initializer for initializing beta\n    offset_init : gamma initializer\n        The initializer for initializing gamma\n    G: the group number\n    name : a string or None\n        An optional name to attach to this layer.\n    """"""\n    def __init__(\n            self,\n            layer=None,\n            act=tf.identity,\n            epsilon=1e-5,\n            scale_init=tf.constant_initializer(1.0),\n            offset_init=tf.constant_initializer(0.0),\n            G=32,\n            name=\'group_norm\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        print(""  [TL] GroupNormLayer %s: epsilon:%f act:%s"" % (self.name, epsilon, act.__name__))\n        inputs_shape = get_shape(layer.outputs)\n        G = tf.minimum(G, inputs_shape[-1])\n        # [N, H, W, C] to [N, C, H, W]\n        temp_input = tf.transpose(self.inputs, [0, 3, 1, 2])\n        temp_input = tf.reshape(temp_input, [inputs_shape[0], G, inputs_shape[-1]//G, inputs_shape[1], inputs_shape[2]],\n                                name=\'group_reshape1\')\n        with tf.variable_scope(name) as vs:\n            mean, var = tf.nn.moments(temp_input, [2, 3, 4], keep_dims=True)\n            scale = tf.get_variable(\'scale\', shape=[1, inputs_shape[-1], 1, 1], initializer=scale_init, dtype=D_TYPE)\n            offset = tf.get_variable(\'offset\', shape=[1, inputs_shape[-1], 1, 1], initializer=offset_init, dtype=D_TYPE)\n            temp_input = (temp_input - mean) / tf.sqrt(var + epsilon)\n            temp_input = tf.reshape(temp_input, shape=[inputs_shape[0], inputs_shape[-1], inputs_shape[1], inputs_shape[2]],\n                                    name=\'group_reshape2\')\n            self.outputs = scale * temp_input + offset\n            self.outputs = tf.transpose(self.outputs, [0, 2, 3, 1])\n            self.outputs = act(self.outputs)\n            variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend(variables)\n\n\nif __name__ == \'__main__\':\n    import numpy as np\n    a = np.random.randn(5, 32, 32, 64)\n    b = tf.placeholder(dtype=tf.float32, name=\'placeholder_b\', shape=(None, 32, 32, 64))\n    inputs_tl = tl.layers.InputLayer(b, name=\'inputs_layer\')\n    gp = GroupNormLayer(layer=inputs_tl)\n'"
nets/vgg16.py,27,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nVGG-16 for ImageNet.\nIntroduction\n----------------\nVGG is a convolutional neural network model proposed by K. Simonyan and A. Zisserman\nfrom the University of Oxford in the paper \xe2\x80\x9cVery Deep Convolutional Networks for\nLarge-Scale Image Recognition\xe2\x80\x9d  . The model achieves 92.7% top-5 test accuracy in ImageNet,\nwhich is a dataset of over 14 million images belonging to 1000 classes.\nDownload Pre-trained Model\n----------------------------\n- Model weights in this example - vgg16_weights.npz : http://www.cs.toronto.edu/~frossard/post/vgg16/\n- Caffe VGG 16 model : https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n- Tool to convert the Caffe models to TensorFlow\'s : https://github.com/ethereon/caffe-tensorflow\nNote\n------\n- For simplified CNN layer see ""Convolutional layer (Simplified)""\nin read the docs website.\n- When feeding other images to the model be sure to properly resize or crop them\nbeforehand. Distorted images might end up being misclassified. One way of safely\nfeeding images of multiple sizes is by doing center cropping, as shown in the\n\nThe input image type is\n    from scipy.misc import imread, imresize\n    img1 = imread(\'data/laska.png\', mode=\'RGB\')  # test data in github\n    img1 = imresize(img1, (224, 224))\nSo the input image is three channels, and is RGB.\n\nfollowing snippet:\n#   >>> image_h, image_w, _ = np.shape(img)\n#   >>> shorter_side = min(image_h, image_w)\n#   >>> scale = 224. / shorter_side\n#   >>> image_h, image_w = np.ceil([scale * image_h, scale * image_w]).astype(\'int32\')\n#   >>> img = imresize(img, (image_h, image_w))\n#   >>> crop_x = (image_w - 224) / 2\n#   >>> crop_y = (image_h - 224) / 2\n#   >>> img = img[crop_y:crop_y+224,crop_x:crop_x+224,:]\n""""""\n\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom scipy.misc import imread, imresize\nfrom nets.imagenet_classes import *\nimport os\n\n\ndef _conv_layers(net_in):\n    with tf.name_scope(\'preprocess\'):\n        # Notice that we include a preprocessing layer that takes the RGB image\n        # with pixels values in the range of 0-255 and subtracts the mean image\n        # values (calculated over the entire ImageNet training set).\n        mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n        net_in.outputs = net_in.outputs - mean\n\n    # conv1\n    network = Conv2dLayer(\n        net_in,\n        act=tf.nn.relu,\n        shape=[3, 3, 3, 64],  # 64 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv1_1\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 64, 64],  # 64 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv1_2\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool1\')\n\n    # conv2\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 64, 128],  # 128 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv2_1\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 128, 128],  # 128 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv2_2\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool2\')\n\n    # conv3\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 128, 256],  # 256 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv3_1\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 256, 256],  # 256 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv3_2\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 256, 256],  # 256 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv3_3\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool3\')\n\n    # conv4\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 256, 512],  # 512 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv4_1\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 512, 512],  # 512 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv4_2\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 512, 512],  # 512 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv4_3\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool4\')\n\n    # conv5\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 512, 512],  # 512 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv5_1\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 512, 512],  # 512 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv5_2\')\n    network = Conv2dLayer(\n        network,\n        act=tf.nn.relu,\n        shape=[3, 3, 512, 512],  # 512 features for each 3x3 patch\n        strides=[1, 1, 1, 1],\n        padding=\'SAME\',\n        name=\'conv5_3\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool5\')\n    return network\n\n\ndef _fc_layers(net):\n    network = FlattenLayer(net, name=\'flatten\')\n    network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name=\'fc1_relu\')\n    network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name=\'fc2_relu\')\n    network = DenseLayer(network, n_units=1000, act=tf.identity, name=\'fc3_relu\')\n    return network\n\n\ndef get_vgg16(x, sess=None, pretrained=True):\n    net_in = InputLayer(x, name=\'input\')\n    net_cnn = _conv_layers(net_in)  # simplified CNN APIs\n    network = _fc_layers(net_cnn)\n\n    if pretrained:\n        npz = np.load(\'../model_weights/vgg16_weights.npz\')\n        params = []\n        for val in sorted(npz.items()):\n            print(""  Loading %s"" % str(val[1].shape))\n            params.append(val[1])\n        tl.files.assign_params(sess, params, network)\n        return network\n    else:\n        tl.layers.initialize_global_variables(sess)\n        return network\n\n\nif __name__ == \'__main__\':\n    DATA_PATH = \'/home/aurora/workspaces2/PycharmProjects/tensorflow/tensorlayer/example/data\'\n\n    x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Session(config=tfconfig) as sess:\n        network = get_vgg16(x, sess, pretrained=True)\n        y = network.outputs\n        network.print_params()\n        network.print_layers()\n        img1 = imread(os.path.join(DATA_PATH, \'laska.png\'), mode=\'RGB\')  # test data in github\n        img1 = imresize(img1, (224, 224))\n        probs = tf.nn.softmax(y)\n        start_time = time.time()\n        prob = sess.run(probs, feed_dict={x: [img1]})[0]\n        print(""  End time : %.5ss"" % (time.time() - start_time))\n        preds = (np.argsort(prob)[::-1])[0:5]\n        for p in preds:\n            print(class_names[p], prob[p])'"
nets/vgg19.py,58,"b'#! /usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\nVGG-19 for ImageNet.\nPre-trained model in this example - VGG19 NPZ and\ntrainable examples of VGG16/19 in TensorFlow can be found here:\nhttps://github.com/machrisaa/tensorflow-vgg\nFor simplified CNN layer see ""Convolutional layer (Simplified)""\nin read the docs website.\n""""""\n\nimport os\nimport time\n\nimport numpy as np\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport tensorflow as tf\nfrom scipy.misc import imread, imresize\nimport tensorlayer as tl\nfrom tensorlayer.layers import *\nfrom imagenet_classes import *\n\n\nDATA_PATH = \'/home/aurora/workspaces2/PycharmProjects/tensorflow/tensorlayer/example/data\'\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\ndef load_image(path):\n    # load image\n    img = skimage.io.imread(path)\n    img = img / 255.0\n    if ((0 <= img).all() and (img <= 1.0).all()) is False:\n        raise Exception(""image value should be [0, 1]"")\n    # print ""Original Image Shape: "", img.shape\n    # we crop image from center\n    short_edge = min(img.shape[:2])\n    yy = int((img.shape[0] - short_edge) / 2)\n    xx = int((img.shape[1] - short_edge) / 2)\n    crop_img = img[yy:yy + short_edge, xx:xx + short_edge]\n    # resize to 224, 224\n    resized_img = skimage.transform.resize(crop_img, (224, 224))\n    return resized_img\n\n\ndef print_prob(prob):\n    synset = class_names\n    # print prob\n    pred = np.argsort(prob)[::-1]\n    # Get top1 label\n    top1 = synset[pred[0]]\n    print(""Top1: "", top1, prob[pred[0]])\n    # Get top5 label\n    top5 = [(synset[pred[i]], prob[pred[i]]) for i in range(5)]\n    print(""Top5: "", top5)\n    return top1\n\n\ndef _Vgg19(rgb):\n    """"""\n    Build the VGG 19 Model\n    Parameters\n    -----------\n    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]\n    """"""\n    start_time = time.time()\n    print(""build model started"")\n    rgb_scaled = rgb * 255.0\n    # Convert RGB to BGR\n    if tf.__version__ <= \'0.11\':\n        red, green, blue = tf.split(3, 3, rgb_scaled)\n    else:  # TF 1.0\n        print(rgb_scaled)\n        red, green, blue = tf.split(rgb_scaled, 3, 3)\n    if red.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(""image size unmatch"")\n    if green.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(""image size unmatch"")\n    if blue.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(""image size unmatch"")\n    if tf.__version__ <= \'0.11\':\n        bgr = tf.concat(3, [\n            blue - VGG_MEAN[0],\n            green - VGG_MEAN[1],\n            red - VGG_MEAN[2],\n        ])\n    else:\n        bgr = tf.concat(\n            [\n                blue - VGG_MEAN[0],\n                green - VGG_MEAN[1],\n                red - VGG_MEAN[2],\n            ], axis=3)\n    if bgr.get_shape().as_list()[1:] != [224, 224, 3]:\n        raise Exception(""image size unmatch"")\n    # input layer\n    net_in = InputLayer(bgr, name=\'input\')\n    # conv1\n    network = Conv2dLayer(net_in, act=tf.nn.relu, shape=[3, 3, 3, 64], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv1_1\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 64, 64], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv1_2\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool1\')\n    # conv2\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 64, 128], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv2_1\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 128, 128], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv2_2\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool2\')\n    # conv3\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 128, 256], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv3_1\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 256, 256], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv3_2\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 256, 256], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv3_3\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 256, 256], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv3_4\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool3\')\n    # conv4\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 256, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv4_1\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv4_2\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv4_3\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv4_4\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool4\')\n    # conv5\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv5_1\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv5_2\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv5_3\')\n    network = Conv2dLayer(network, act=tf.nn.relu, shape=[3, 3, 512, 512], strides=[1, 1, 1, 1], padding=\'SAME\', name=\'conv5_4\')\n    network = PoolLayer(network, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', pool=tf.nn.max_pool, name=\'pool5\')\n    # fc 6~8\n    network = FlattenLayer(network, name=\'flatten\')\n    network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name=\'fc6\')\n    network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name=\'fc7\')\n    network = DenseLayer(network, n_units=1000, act=tf.identity, name=\'fc8\')\n    print(""build model finished: %fs"" % (time.time() - start_time))\n    return network\n\n\ndef _Vgg19_simple_api(rgb):\n    """"""\n    Build the VGG 19 Model\n    Parameters\n    -----------\n    rgb : rgb image placeholder [batch, height, width, 3] values scaled [0, 1]\n    """"""\n    start_time = time.time()\n    print(""build model started"")\n    rgb_scaled = rgb * 255.0\n    # Convert RGB to BGR\n    if tf.__version__ <= \'0.11\':\n        red, green, blue = tf.split(3, 3, rgb_scaled)\n    else:  # TF 1.0\n        print(rgb_scaled)\n        red, green, blue = tf.split(rgb_scaled, 3, 3)\n    if red.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(""image size unmatch"")\n    if green.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(""image size unmatch"")\n    if blue.get_shape().as_list()[1:] != [224, 224, 1]:\n        raise Exception(""image size unmatch"")\n    if tf.__version__ <= \'0.11\':\n        bgr = tf.concat(3, [\n            blue - VGG_MEAN[0],\n            green - VGG_MEAN[1],\n            red - VGG_MEAN[2],\n        ])\n    else:\n        bgr = tf.concat(\n            [\n                blue - VGG_MEAN[0],\n                green - VGG_MEAN[1],\n                red - VGG_MEAN[2],\n            ], axis=3)\n    if bgr.get_shape().as_list()[1:] != [224, 224, 3]:\n        raise Exception(""image size unmatch"")\n    # input layer\n    net_in = InputLayer(bgr, name=\'input\')\n    # conv1\n    network = Conv2d(net_in, n_filter=64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv1_1\')\n    network = Conv2d(network, n_filter=64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv1_2\')\n    network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool1\')\n    # conv2\n    network = Conv2d(network, n_filter=128, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv2_1\')\n    network = Conv2d(network, n_filter=128, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv2_2\')\n    network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool2\')\n    # conv3\n    network = Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv3_1\')\n    network = Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv3_2\')\n    network = Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv3_3\')\n    network = Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv3_4\')\n    network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool3\')\n    # conv4\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv4_1\')\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv4_2\')\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv4_3\')\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv4_4\')\n    network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool4\')\n    # conv5\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv5_1\')\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv5_2\')\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv5_3\')\n    network = Conv2d(network, n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding=\'SAME\', name=\'conv5_4\')\n    network = MaxPool2d(network, filter_size=(2, 2), strides=(2, 2), padding=\'SAME\', name=\'pool5\')\n    # fc 6~8\n    network = FlattenLayer(network, name=\'flatten\')\n    network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name=\'fc6\')\n    network = DenseLayer(network, n_units=4096, act=tf.nn.relu, name=\'fc7\')\n    network = DenseLayer(network, n_units=1000, act=tf.identity, name=\'fc8\')\n    print(""build model finished: %fs"" % (time.time() - start_time))\n    return network\n\n\ndef get_vgg19(inputs, sess=None, pretrained=True):\n    network = _Vgg19(inputs)\n    if pretrained:\n        vgg19_npy_path = ""../model_weights/vgg19.npy""\n        npz = np.load(vgg19_npy_path, encoding=\'latin1\').item()\n        params = []\n        for val in sorted(npz.items()):\n            W = np.asarray(val[1][0])\n            b = np.asarray(val[1][1])\n            print(""  Loading %s: %s, %s"" % (val[0], W.shape, b.shape))\n            params.extend([W, b])\n        print(""Restoring model from npz file"")\n        tl.files.assign_params(sess, params, network)\n        return network\n    else:\n        tl.layers.initialize_global_variables(sess)\n        return network\n\n\nif __name__ == \'__main__\':\n    sess = tf.InteractiveSession()\n    x = tf.placeholder(""float"", [None, 224, 224, 3])\n    network = get_vgg19(x, sess)\n    y = network.outputs\n    probs = tf.nn.softmax(y, name=""prob"")\n    img1 = load_image(os.path.join(DATA_PATH, ""tiger.jpeg""))  # test data in github\n    img1 = img1.reshape((1, 224, 224, 3))\n    start_time = time.time()\n    prob = sess.run(probs, feed_dict={x: img1})\n    print(""End time : %.5ss"" % (time.time() - start_time))\n\n    print_prob(prob[0])'"
test/memory_usage_test.py,10,"b'import mxnet as mx\nimport argparse\nimport PIL.Image\nimport io\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport os\nimport sys\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\'data path information\'\n    )\n    parser.add_argument(\'--bin_path\', default=\'../datasets/faces_ms1m_112x112/train.rec\', type=str,\n                        help=\'path to the binary image file\')\n    parser.add_argument(\'--idx_path\', default=\'../datasets/faces_ms1m_112x112/train.idx\', type=str,\n                        help=\'path to the image index path\')\n    parser.add_argument(\'--tfrecords_file_path\', default=\'../datasets/tfrecords\', type=str,\n                        help=\'path to the output of tfrecords file path\')\n    args = parser.parse_args()\n    return args\n\n\ndef mx2tfrecords_mem_test(imgidx, imgrec, args):\n    output_path = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for i in imgidx:\n        img_info = imgrec.read_idx(i)\n        header, img = mx.recordio.unpack(img_info)\n        print(type(img))\n        print(img)\n        print(sys.getsizeof(img))\n        print(\'#####################\')\n        img_mx = mx.image.imdecode(img)\n        print(type(img_mx))\n        print(sys.getsizeof(img_mx))\n        print(img_mx.size)\n        print(img_mx.dtype)\n        print(img_mx.context)\n        print(img_mx.stype)\n        print(img_mx)\n        print(\'#####################\')\n        img_mx_np = img_mx.asnumpy()\n        print(type(img_mx_np))\n        print(sys.getsizeof(img_mx_np))\n        print(\'#####################\')\n        back_mx_ndarray = mx.nd.array(img_mx_np)\n        print(type(back_mx_ndarray))\n        print(sys.getsizeof(back_mx_ndarray))\n        encoded_jpg_io = io.BytesIO(img)\n        print(sys.getsizeof(encoded_jpg_io))\n        image = PIL.Image.open(encoded_jpg_io)\n        np_img = np.array(image)\n        img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n        print(sys.getsizeof(img))\n        print(\'#####################\')\n        img_raw = img.tobytes()\n        print(sys.getsizeof(img))\n        print(\'#####################\')\n    writer.close()\n\n\ndef mx2tfrecords(imgidx, imgrec, args):\n    output_path = os.path.join(args.tfrecords_file_path, \'tran.tfrecords\')\n    writer = tf.python_io.TFRecordWriter(output_path)\n    for i in imgidx:\n        img_info = imgrec.read_idx(i)\n        header, img = mx.recordio.unpack(img_info)\n        # encoded_jpg_io = io.BytesIO(img)\n        # image = PIL.Image.open(encoded_jpg_io)\n        # np_img = np.array(image)\n        # img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n        # img_raw = img.tobytes()\n        # images = tf.image.decode_jpeg(img)\n        # images = tf.reshape(images, shape=(112, 112, 3))\n        # r, g, b = tf.split(images, num_or_size_splits=3, axis=-1)\n        # images = tf.concat([b, g, r], axis=-1)\n        # sess = tf.Session()\n        # np_images = sess.run(images)\n        # print(images.shape)\n        # print(type(np_images))\n        # print(sys.getsizeof(np_images))\n        # cv2.imshow(\'test\', np_images)\n        # cv2.waitKey(0)\n        label = int(header.label)\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),\n            ""label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\n        if i % 10000 == 0:\n            print(\'%d num image processed\' % i)\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    # define parameters\n    id2range = {}\n    data_shape = (3, 112, 112)\n    args = parse_args()\n    imgrec = mx.recordio.MXIndexedRecordIO(args.idx_path, args.bin_path, \'r\')\n    s = imgrec.read_idx(0)\n    header, _ = mx.recordio.unpack(s)\n    print(header.label)\n    imgidx = list(range(1, int(header.label[0])))\n    seq_identity = range(int(header.label[0]), int(header.label[1]))\n    for identity in seq_identity:\n        s = imgrec.read_idx(identity)\n        header, _ = mx.recordio.unpack(s)\n        a, b = int(header.label[0]), int(header.label[1])\n        id2range[identity] = (a, b)\n    print(\'id2range\', len(id2range))\n\n    # generate tfrecords\n    mx2tfrecords_mem_test(imgidx, imgrec, args)\n\n\n\n\n'"
test/resnet_test_static.py,5,"b""from resnet import get_resnet\nimport tensorflow as tf\nfrom nets_utils import get_tensor_static_val\nimport numpy as np\n\n\ndef resnet_diff_test(layers_num):\n    ckpt_file_path = '../model_weights/resnet_v1_'+str(layers_num)+'.ckpt'\n    x = tf.placeholder(dtype=tf.float32, shape=[1, 224, 224, 3], name='input_place')\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    sess = tf.Session(config=tfconfig)\n    nets = get_resnet(x, 1000, layers_num, sess)\n    ckpt_static = get_tensor_static_val(ckpt_file_path, all_tensors=True, all_tensor_names=True)\n\n    print('###########'*30)\n    vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n    total_count = 0\n    mean_avg = 0.0\n    median_avg = 0.0\n    std_avg = 0.0\n\n    for var in vars:\n        var_name = var.op.name\n        var_name_new = var_name\n        if '_bn' in var_name:\n            var_name_new = var_name_new.replace('_bn', '')\n        if 'W_conv2d' in var_name:\n            var_name_new = var_name_new.replace('W_conv2d', 'weights')\n        if 'b_conv2d' in var_name:\n            var_name_new = var_name_new.replace('b_conv2d', 'biases')\n        if 'shortcut_conv' in var_name:\n            var_name_new = var_name_new.replace('shortcut_conv', 'shortcut')\n\n        if var_name_new in ckpt_static:\n            print(var_name_new, end=',    ')\n            total_count += 1\n            ckpt_s = ckpt_static[var_name_new]\n            var_val = sess.run(var)\n            mean_diff = np.mean(var_val) - ckpt_s.mean\n            mean_avg += mean_diff\n            median_diff = np.median(var_val) - ckpt_s.median\n            median_avg += median_diff\n            std_diff = np.std(var_val) - ckpt_s.std\n            std_avg += std_diff\n            print('mean_diff: ', mean_diff, 'median_diff: ', median_diff, 'std_diff: ', std_diff)\n\n    print('total_mean_diff', mean_avg/total_count, 'total_mean_diff', median_avg/total_count,\n          'total_std_diff', std_avg/total_count)\n\n\nif __name__ == '__main__':\n    with tf.device('/device:GPU:1'):\n        resnet_diff_test(50)\n"""
test/test_losses.py,26,"b'import tensorflow as tf\nimport numpy as np\nfrom losses.face_losses import cosineface_losses\nimport mxnet as mx\nimport math\n\n\ndef arcface_loss_val(embedding, labels, weights, out_num, s=64., m=0.5):\n    \'\'\'\n    :param embedding: the input embedding vectors\n    :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n    :param s: scalar value default is 64\n    :param out_num: output class num\n    :param m: the margin value, default is 0.5\n    :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n    \'\'\'\n    cos_m = math.cos(m)\n    sin_m = math.sin(m)\n    mm = sin_m * m * s\n    threshold = math.cos(math.pi - m)\n    with tf.variable_scope(\'arcface_loss\'):\n        # inputs and weights norm\n        embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)\n        embedding = tf.div(embedding, embedding_norm, name=\'norm_embedding\')\n        weights_norm = tf.norm(weights, axis=0, keep_dims=True)\n        weights = tf.div(weights, weights_norm, name=\'norm_weights\')\n        # cos(theta+m)\n        cos_t = tf.matmul(embedding, weights, name=\'cos_t\')\n        cos_t2 = tf.square(cos_t, name=\'cos_2\')\n        sin_t2 = tf.subtract(1., cos_t2, name=\'sin_2\')\n        sin_t = tf.sqrt(sin_t2, name=\'sin_t\')\n        cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name=\'cos_mt\')\n\n        # this condition controls the theta+m should in range [0, pi]\n        #      0<=theta+m<=pi\n        #     -m<=theta<=pi-m\n        cond_v = cos_t - threshold\n        cond = tf.cast(tf.nn.relu(cond_v, name=\'if_else\'), dtype=tf.bool)\n\n        keep_val = s * (cos_t - mm)\n        cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n\n        mask = tf.one_hot(labels, depth=out_num, name=\'one_hot_mask\')\n        inv_mask = tf.subtract(1., mask, name=\'inverse_mask\')\n\n        s_cos_t = tf.multiply(s, cos_t, name=\'scalar_cos_t\')\n\n        output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name=\'arcface_loss_output\')\n    return output\n\n\ndef test_arcface_losses(np_embedding, np_weights):\n    tf_embedding = tf.constant(np_embedding, name=\'embedding\', dtype=tf.float32)\n    labels = tf.constant([1, 3, 2, 1, 1], name=\'input_labels\', dtype=tf.int64)\n    print(labels)\n    tf_weights = tf.constant(np_weights, name=\'weights\')\n    output = arcface_loss_val(embedding=tf_embedding, labels=labels, out_num=10, weights=tf_weights)\n    print(output)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    results1 = sess.run(output)\n    print(results1)\n    return results1\n\n\ndef test_cosineface_losses():\n    np_embedding = np.random.randn(5, 512).astype(dtype=np.float32)\n    tf_embedding = tf.constant(np_embedding, name=\'embedding\', dtype=tf.float32)\n    labels = tf.constant([1, 3, 2, 1, 1], name=\'input_labels\', dtype=tf.int64)\n    output = cosineface_losses(embedding=tf_embedding, labels=labels, out_num=10)\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(output))\n\n\ndef test_mxnet_losses(np_embedding, np_weights):\n    labels = np.array([1, 3, 2, 1, 1]).astype(dtype=np.float32)\n    return mxnet_arcface_val(np_embedding, labels, np_weights)\n\n\ndef mxnet_arcface_val(embedding, gt_label, weights):\n    s = 64\n    m = 0.5\n    _weight = mx.symbol.Variable(""fc7_weight"", shape=(10, 512), lr_mult=1.0)\n    _weight = mx.symbol.L2Normalization(_weight, mode=\'instance\')\n    _embedding = mx.symbol.Variable(\'mx_embedding\', shape=(5, 512), lr_mult=1.0)\n    nembedding = mx.symbol.L2Normalization(_embedding, mode=\'instance\', name=\'fc1n\')*s\n    fc7 = mx.sym.FullyConnected(data=nembedding, weight=_weight, no_bias=True, num_hidden=10, name=\'fc7\')\n\n    _labels = mx.symbol.Variable(\'labels\', shape=(5, ), lr_mult=1.0)\n    zy = mx.sym.pick(fc7, _labels, axis=1)\n    cos_t = zy/s\n\n    cos_m = math.cos(m)\n    sin_m = math.sin(m)\n    mm = math.sin(math.pi - m) * m\n    # threshold = 0.0\n    threshold = math.cos(math.pi - m)\n\n    cond_v = cos_t - threshold\n    cond = mx.symbol.Activation(data=cond_v, act_type=\'relu\')\n\n    body = cos_t * cos_t\n    body = 1.0 - body\n    sin_t = mx.sym.sqrt(body)\n    new_zy = cos_t * cos_m\n    b = sin_t * sin_m\n    new_zy = new_zy - b\n    new_zy = new_zy * s\n\n    zy_keep = zy - s * mm\n    new_zy = mx.sym.where(cond, new_zy, zy_keep)\n\n    diff = new_zy - zy\n    diff = mx.sym.expand_dims(diff, 1)\n    gt_one_hot = mx.sym.one_hot(_labels, depth = 10, on_value = 1.0, off_value = 0.0)\n    body = mx.sym.broadcast_mul(gt_one_hot, diff)\n    fc7 = fc7+body\n    executor = fc7.bind(mx.cpu(), {\'fc7_weight\': mx.nd.array(weights.T), \'mx_embedding\': mx.nd.array(embedding),\n                                   \'labels\': mx.nd.array(gt_label)})\n    output = executor.forward()\n    print(output)\n    return output\n\n\nif __name__ == \'__main__\':\n    np_embedding = np.random.randn(5, 512).astype(dtype=np.float32)\n    np_weights = np.random.randn(512, 10).astype(dtype=np.float32)\n    # test arcface_losses output\n    result1 = test_arcface_losses(np_embedding, np_weights)\n    # print(\'########\'*30)\n    print(\'################\')\n    result2 = test_mxnet_losses(np_embedding, np_weights)\n    print(len(result2[0]))\n    print(type(result1))\n    print(type(result2[0].asnumpy()))\n    print(np.mean(result1 - result2[0].asnumpy()))   # 1.26362e-07'"
test/benchmark/__init__.py,0,b''
test/benchmark/gluon_batchsize_test.py,0,"b'from mxnet import gluon\nimport mxnet as mx\nfrom mxnet import ndarray as nd\nimport utils_final as utils\nimport mxnet.gluon.nn as nn\nfrom mxnet import init\nimport os\nfrom mxnet import initializer\nfrom mxnet.gluon.block import HybridBlock\n\n\ndef prelu():\n    pass\n\n\ndef inference():\n    net = gluon.nn.Sequential()\n    with net.name_scope():\n        net.add(nn.Conv2D(channels=64, kernel_size=3, padding=1))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n        # net.add(mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'prelu1\'))\n        net.add(nn.Conv2D(channels=64, kernel_size=3, padding=1))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n        net.add(nn.Conv2D(channels=64, kernel_size=3, padding=1, strides=2))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n\n        net.add(nn.Conv2D(channels=128, kernel_size=3, padding=1))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n        net.add(nn.Conv2D(channels=128, kernel_size=3, padding=1))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n        net.add(nn.Conv2D(channels=128, kernel_size=3, padding=1, strides=2))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n\n        net.add(nn.Conv2D(channels=256, kernel_size=3, padding=1))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n        net.add(nn.Conv2D(channels=256, kernel_size=3, padding=1))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n        net.add(nn.Conv2D(channels=256, kernel_size=3, padding=1, strides=2))\n        net.add(nn.BatchNorm(axis=1, center=True, scale=True))\n\n        net.add(nn.Flatten())\n        net.add(nn.Dense(10))\n    return net\n\n\nif __name__ == \'__main__\':\n    # without prelu and bn    7000< max batch size <8000\n    # with bn only            3000< max batch size <4000\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    batch_size = 3000\n    train_data, test_data = utils.load_data_mnist(batch_size=batch_size)\n    ctx = utils.try_gpu()\n    net = inference()\n    print(net)\n    net.initialize(ctx=ctx, init=init.Xavier())\n    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n    trainer = gluon.Trainer(net.collect_params(), \'sgd\', {\'learning_rate\': 0.01})\n    utils.train(train_data, test_data, net, softmax_cross_entropy, trainer, ctx, num_epochs=10)\n\n\n'"
test/benchmark/mxnet_batchsize_test.py,0,"b'import mxnet as mx\nimport mxnet.ndarray as nd\nimport os\n\n\nif __name__ == \'__main__\':\n    # without bn and prelu  max batchsize (40000, 50000)\n    # with bn max batchsize (20000, 30000)\n    # with prelu batchsize (20000, 30000)\n    # with bn and prelu max batchsize (10000, 20000)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    batch_size = 10000\n    mnist = mx.test_utils.get_mnist()\n    print(mnist[\'train_data\'].shape)\n    train_iter = mx.io.NDArrayIter(mnist[\'train_data\'], mnist[\'train_label\'], batch_size, shuffle=True)\n\n    # inference\n    data = mx.sym.var(\'data\')\n    # first conv layer\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), num_filter=64)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn1\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul1\')\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), num_filter=64)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn2\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul2\')\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), stride=(2, 2), num_filter=64)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn3\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul3\')\n\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), num_filter=128)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn4\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul4\')\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), num_filter=128)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn5\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul5\')\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), stride=(2, 2), num_filter=128)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn6\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul6\')\n\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), num_filter=256)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn7\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul7\')\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), num_filter=256)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn8\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul8\')\n    net = mx.sym.Convolution(data=data, kernel=(3, 3), stride=(2, 2), num_filter=256)\n    net = mx.sym.BatchNorm(data=net, fix_gamma=False, eps=2e-5, name=\'_bn9\')\n    net = mx.sym.LeakyReLU(data=net, act_type=\'prelu\', name=\'_preul9\')\n\n    flatten = mx.sym.flatten(data=net)\n    # MNIST has 10 classes\n    fc3 = mx.sym.FullyConnected(data=flatten, num_hidden=10)\n    # Softmax with cross entropy loss\n    mlp = mx.sym.SoftmaxOutput(data=fc3, name=\'softmax\')\n\n    import logging\n\n    logging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n    # create a trainable module on GPU\n    mlp_model = mx.mod.Module(symbol=mlp, context=mx.gpu())\n    mlp_model.fit(train_iter,  # train data\n                  optimizer=\'sgd\',  # use SGD to train\n                  optimizer_params={\'learning_rate\': 0.1},  # use fixed learning rate\n                  eval_metric=\'acc\',  # report accuracy during training\n                  batch_end_callback=mx.callback.Speedometer(batch_size, 100),\n                  # output progress for each 100 data batches\n                  num_epoch=10)  # train for at most 10 dataset passes'"
test/benchmark/resnet_slim_benchmark.py,10,"b""import tensorflow as tf\nimport tensorflow.contrib.slim.nets as nets\nimport numpy as np\n\n\nslim = tf.contrib.slim\nresnet = nets.resnet_v1\n\nif __name__ == '__main__':\n    output_shape = 85164\n    batch_size = 64\n    image = tf.placeholder(name='input_x', shape=[None, 224, 224, 3], dtype=tf.float32)\n    labels = tf.placeholder(name='input_label', shape=[None, output_shape], dtype=tf.float32)\n    with slim.arg_scope(nets.resnet_utils.resnet_arg_scope()):\n        resnet_50, end_points = resnet.resnet_v1_50(inputs=image, num_classes=output_shape, scope='resnet_v1_50')\n        prob = tf.squeeze(resnet_50, axis=[1, 2])\n    probabilities = tf.reduce_mean(tf.nn.softmax(prob, dim=-1))\n    losses = tf.norm(tf.subtract(probabilities, labels))\n    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(losses)\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n    while True:\n        datasets = np.random.randn(batch_size, 224, 224, 3).astype(np.float32)\n        datasets_labels = np.random.randn(batch_size, output_shape).astype(np.float32)\n        losses_val, _ = sess.run([losses, train_op], feed_dict={image: datasets, labels: datasets_labels})\n        print(losses_val)"""
test/benchmark/resnet_tl_benchmark.py,9,"b""import tensorflow as tf\nimport tensorflow.contrib.slim.nets as nets\nimport numpy as np\nfrom nets.resnet import get_resnet\n\n\nslim = tf.contrib.slim\nresnet = nets.resnet_v1\n\nif __name__ == '__main__':\n    output_shape = 85164\n    batch_size = 128\n    image = tf.placeholder(name='input_x', shape=[None, 224, 224, 3], dtype=tf.float32)\n    labels = tf.placeholder(name='input_label', shape=[None, output_shape], dtype=tf.float32)\n    with slim.arg_scope(nets.resnet_utils.resnet_arg_scope()):\n        nets = get_resnet(image, output_shape, 50, type='resnet', sess=None, pretrained=False)\n    print(nets.outputs)\n    probabilities = tf.reduce_mean(tf.nn.softmax(nets.outputs, dim=-1))\n    print(probabilities)\n    losses = tf.norm(tf.subtract(probabilities, labels))\n    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(losses)\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n    while True:\n        datasets = np.random.randn(batch_size, 224, 224, 3).astype(np.float32)\n        datasets_labels = np.random.randn(batch_size, output_shape).astype(np.float32)\n        losses_val, _ = sess.run([losses, train_op], feed_dict={image: datasets, labels: datasets_labels})\n        print(losses_val)"""
test/benchmark/tensorlayer_batchsize_test.py,16,"b'import tensorflow as tf\nimport tensorlayer as tl\nimport os\n\n\ndef inference(x):\n    w_init_method = tf.contrib.layers.xavier_initializer(uniform=True)\n    # define the network\n    network = tl.layers.InputLayer(x, name=\'input\')\n    network = tl.layers.Conv2d(network, n_filter=64, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv1_1\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn1\')\n    network = tl.layers.PReluLayer(network, name=\'prelu1\')\n    network = tl.layers.Conv2d(network, n_filter=64, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv1_2\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn2\')\n    network = tl.layers.PReluLayer(network, name=\'prelu2\')\n    network = tl.layers.Conv2d(network, n_filter=64, filter_size=(3, 3), strides=(2, 2), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv1_3\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn3\')\n    network = tl.layers.PReluLayer(network, name=\'prelu3\')\n\n    network = tl.layers.Conv2d(network, n_filter=128, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv2_1\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn4\')\n    network = tl.layers.PReluLayer(network, name=\'prelu4\')\n\n    network = tl.layers.Conv2d(network, n_filter=128, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv2_2\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn5\')\n    network = tl.layers.PReluLayer(network, name=\'prelu5\')\n    network = tl.layers.Conv2d(network, n_filter=128, filter_size=(3, 3), strides=(2, 2), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv2_3\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn6\')\n    network = tl.layers.PReluLayer(network, name=\'prelu6\')\n\n    network = tl.layers.Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv3_1\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn7\')\n    network = tl.layers.PReluLayer(network, name=\'prelu7\')\n    network = tl.layers.Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(1, 1), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv3_2\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn8\')\n    network = tl.layers.PReluLayer(network, name=\'prelu8\')\n    network = tl.layers.Conv2d(network, n_filter=256, filter_size=(3, 3), strides=(2, 2), padding=\'SAME\', act=None,\n                               W_init=w_init_method, name=\'conv3_3\')\n    network = tl.layers.BatchNormLayer(network, act=tf.identity, is_train=True, name=\'bn9\')\n    network = tl.layers.PReluLayer(network, name=\'prelu9\')\n\n    network = tl.layers.FlattenLayer(network, name=\'flatten\')\n    network = tl.layers.DenseLayer(network, 10)\n\n    return network.outputs\n\n\nif __name__ == \'__main__\':\n    # without bn prelu     8000< max batch size <9000\n    # with bn only         5000< max batch size <6000\n    # with prelu only      3000< max batch size <4000\n    # with bn and prelu    2000< max batch size <3000\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n    batch_size = 2000\n    n_epoch = 10\n    # prepare data\n    X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))\n    # define placeholder\n    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\'x\')\n    y_ = tf.placeholder(tf.int64, shape=[None], name=\'y_\')\n\n    output = inference(x)\n    cost = tl.cost.cross_entropy(output, y_, \'cost\')\n    train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n    sess = tf.Session()\n    tl.layers.initialize_global_variables(sess)\n\n    correct_prediction = tf.equal(tf.argmax(output, 1), y_)\n    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    for epoch in range(n_epoch):\n        train_loss, train_acc, n_batch = 0, 0, 0\n        for X_train_a, y_train_a in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):\n            feed_dict = {x: X_train_a, y_: y_train_a}\n            _, err, ac = sess.run([train_op, cost, acc], feed_dict=feed_dict)\n            train_loss += err\n            train_acc += ac\n            n_batch += 1\n        print(""epoch %d, train acc: %f"" % (epoch, (train_acc / n_batch)))'"
test/benchmark/utils_final.py,0,"b'from math import exp\nfrom mxnet import gluon\nfrom mxnet import autograd\nfrom mxnet import nd\nfrom mxnet import image\nfrom mxnet.gluon import nn\nimport mxnet as mx\nimport numpy as np\nfrom time import time\nimport matplotlib.pyplot as plt\nimport random\n\n\nclass DataLoader(object):\n    """"""similiar to gluon.data.DataLoader, but might be faster.\n\n    The main difference this data loader tries to read more exmaples each\n    time. But the limits are 1) all examples in dataset have the same shape, 2)\n    data transfomer needs to process multiple examples at each time\n    """"""\n\n    def __init__(self, dataset, batch_size, shuffle, transform=None):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.transform = transform\n\n    def __iter__(self):\n        data = self.dataset[:]\n        X = data[0]\n        y = nd.array(data[1])\n        n = X.shape[0]\n        if self.shuffle:\n            idx = np.arange(n)\n            np.random.shuffle(idx)\n            X = nd.array(X.asnumpy()[idx])\n            y = nd.array(y.asnumpy()[idx])\n\n        for i in range(n // self.batch_size):\n            if self.transform is not None:\n                yield self.transform(X[i * self.batch_size:(i + 1) * self.batch_size],\n                                     y[i * self.batch_size:(i + 1) * self.batch_size])\n            else:\n                yield (X[i * self.batch_size:(i + 1) * self.batch_size],\n                       y[i * self.batch_size:(i + 1) * self.batch_size])\n\n    def __len__(self):\n        return len(self.dataset) // self.batch_size\n\n\ndef load_data_fashion_mnist(batch_size, resize=None, root=""~/.mxnet/datasets/fashion-mnist""):\n    """"""download the fashion mnist dataest and then load into memory""""""\n\n    def transform_mnist(data, label):\n        # Transform a batch of examples.\n        if resize:\n            n = data.shape[0]\n            new_data = nd.zeros((n, resize, resize, data.shape[3]))\n            for i in range(n):\n                new_data[i] = image.imresize(data[i], resize, resize)\n            data = new_data\n        # change data from batch x height x width x channel to batch x channel x height x width\n        return nd.transpose(data.astype(\'float32\'), (0, 3, 1, 2)) / 255, label.astype(\'float32\')\n\n    mnist_train = gluon.data.vision.FashionMNIST(root=root, train=True, transform=None)\n    mnist_test = gluon.data.vision.FashionMNIST(root=root, train=False, transform=None)\n    # Transform later to avoid memory explosion.\n    train_data = DataLoader(mnist_train, batch_size, shuffle=True, transform=transform_mnist)\n    test_data = DataLoader(mnist_test, batch_size, shuffle=False, transform=transform_mnist)\n    return (train_data, test_data)\n\n\ndef load_data_mnist(batch_size, resize=None, root=""~/.mxnet/datasets/mnist""):\n    """"""download the fashion mnist dataest and then load into memory""""""\n\n    def transform_mnist(data, label):\n        # Transform a batch of examples.\n        if resize:\n            n = data.shape[0]\n            new_data = nd.zeros((n, resize, resize, data.shape[3]))\n            for i in range(n):\n                new_data[i] = image.imresize(data[i], resize, resize)\n            data = new_data\n        # change data from batch x height x width x channel to batch x channel x height x width\n        return nd.transpose(data.astype(\'float32\'), (0, 3, 1, 2)) / 255, label.astype(\'float32\')\n\n    mnist_train = gluon.data.vision.MNIST(root=root, train=True, transform=None)\n    mnist_test = gluon.data.vision.MNIST(root=root, train=False, transform=None)\n    # Transform later to avoid memory explosion.\n    train_data = DataLoader(mnist_train, batch_size, shuffle=True, transform=transform_mnist)\n    test_data = DataLoader(mnist_test, batch_size, shuffle=False, transform=transform_mnist)\n    return (train_data, test_data)\n\n\ndef try_gpu():\n    """"""If GPU is available, return mx.gpu(0); else return mx.cpu()""""""\n    try:\n        ctx = mx.gpu()\n        _ = nd.array([0], ctx=ctx)\n    except:\n        ctx = mx.cpu()\n    return ctx\n\n\ndef try_all_gpus():\n    """"""Return all available GPUs, or [mx.gpu()] if there is no GPU""""""\n    ctx_list = []\n    try:\n        for i in range(16):\n            ctx = mx.gpu(i)\n            _ = nd.array([0], ctx=ctx)\n            ctx_list.append(ctx)\n    except:\n        pass\n    if not ctx_list:\n        ctx_list = [mx.cpu()]\n    return ctx_list\n\n\ndef SGD(params, lr):\n    for param in params:\n        param[:] = param - lr * param.grad\n\n\ndef accuracy(output, label):\n    return nd.mean(output.argmax(axis=1) == label).asscalar()\n\n\ndef _get_batch(batch, ctx):\n    """"""return data and label on ctx""""""\n    if isinstance(batch, mx.io.DataBatch):\n        data = batch.data[0]\n        label = batch.label[0]\n    else:\n        data, label = batch\n    return (gluon.utils.split_and_load(data, ctx),\n            gluon.utils.split_and_load(label, ctx),\n            data.shape[0])\n\n\ndef evaluate_accuracy(data_iterator, net, ctx=[mx.cpu()]):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    acc = nd.array([0])\n    n = 0.\n    if isinstance(data_iterator, mx.io.MXDataIter):\n        data_iterator.reset()\n    for batch in data_iterator:\n        data, label, batch_size = _get_batch(batch, ctx)\n        for X, y in zip(data, label):\n            acc += nd.sum(net(X).argmax(axis=1) == y).copyto(mx.cpu())\n            n += y.size\n        acc.wait_to_read()  # don\'t push too many operators into backend\n    return acc.asscalar() / n\n\n\ndef train(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None):\n    """"""Train a network""""""\n    print(""Start training on "", ctx)\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    for epoch in range(num_epochs):\n        train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0\n        if isinstance(train_data, mx.io.MXDataIter):\n            train_data.reset()\n        start = time()\n        for i, batch in enumerate(train_data):\n            data, label, batch_size = _get_batch(batch, ctx)\n            losses = []\n            with autograd.record():\n                outputs = [net(X) for X in data]\n                losses = [loss(yhat, y) for yhat, y in zip(outputs, label)]\n            for l in losses:\n                l.backward()\n            train_acc += sum([(yhat.argmax(axis=1) == y).sum().asscalar()\n                              for yhat, y in zip(outputs, label)])\n            train_loss += sum([l.sum().asscalar() for l in losses])\n            trainer.step(batch_size)\n            n += batch_size\n            m += sum([y.size for y in label])\n            if print_batches and (i + 1) % print_batches == 0:\n                print(""Batch %d. Loss: %f, Train acc %f"" % (\n                    n, train_loss / n, train_acc / m\n                ))\n\n        test_acc = evaluate_accuracy(test_data, net, ctx)\n        print(""Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Time %.1f sec"" % (\n            epoch, train_loss / n, train_acc / m, test_acc, time() - start\n        ))\n\n\nclass Residual(nn.HybridBlock):\n    def __init__(self, channels, same_shape=True, **kwargs):\n        super(Residual, self).__init__(**kwargs)\n        self.same_shape = same_shape\n        with self.name_scope():\n            strides = 1 if same_shape else 2\n            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n                                   strides=strides)\n            self.bn1 = nn.BatchNorm()\n            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n            self.bn2 = nn.BatchNorm()\n            if not same_shape:\n                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n                                       strides=strides)\n\n    def hybrid_forward(self, F, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(out + x)\n\n\ndef resnet18(num_classes):\n    net = nn.HybridSequential()\n    with net.name_scope():\n        net.add(\n            nn.BatchNorm(),\n            nn.Conv2D(64, kernel_size=3, strides=1),\n            nn.MaxPool2D(pool_size=3, strides=2),\n            Residual(64),\n            Residual(64),\n            Residual(128, same_shape=False),\n            Residual(128),\n            Residual(256, same_shape=False),\n            Residual(256),\n            nn.GlobalAvgPool2D(),\n            nn.Dense(num_classes)\n        )\n    return net\n\n\ndef show_images(imgs, nrows, ncols, figsize=None):\n    """"""plot a list of images""""""\n    if not figsize:\n        figsize = (ncols, nrows)\n    _, figs = plt.subplots(nrows, ncols, figsize=figsize)\n    for i in range(nrows):\n        for j in range(ncols):\n            figs[i][j].imshow(imgs[i * ncols + j].asnumpy())\n            figs[i][j].axes.get_xaxis().set_visible(False)\n            figs[i][j].axes.get_yaxis().set_visible(False)\n    plt.show()\n\n\ndef data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n    """"""Sample mini-batches in a random order from sequential data.""""""\n    # Subtract 1 because label indices are corresponding input indices + 1.\n    num_examples = (len(corpus_indices) - 1) // num_steps\n    epoch_size = num_examples // batch_size\n    # Randomize samples.\n    example_indices = list(range(num_examples))\n    random.shuffle(example_indices)\n\n    def _data(pos):\n        return corpus_indices[pos: pos + num_steps]\n\n    for i in range(epoch_size):\n        # Read batch_size random samples each time.\n        i = i * batch_size\n        batch_indices = example_indices[i: i + batch_size]\n        data = nd.array(\n            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n        label = nd.array(\n            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n        yield data, label\n\n\ndef data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n    """"""Sample mini-batches in a consecutive order from sequential data.""""""\n    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n    data_len = len(corpus_indices)\n    batch_len = data_len // batch_size\n\n    indices = corpus_indices[0: batch_size * batch_len].reshape((\n        batch_size, batch_len))\n    # Subtract 1 because label indices are corresponding input indices + 1.\n    epoch_size = (batch_len - 1) // num_steps\n\n    for i in range(epoch_size):\n        i = i * num_steps\n        data = indices[:, i: i + num_steps]\n        label = indices[:, i + 1: i + num_steps + 1]\n        yield data, label\n\n\ndef grad_clipping(params, clipping_norm, ctx):\n    """"""Gradient clipping.""""""\n    if clipping_norm is not None:\n        norm = nd.array([0.0], ctx)\n        for p in params:\n            norm += nd.sum(p.grad ** 2)\n        norm = nd.sqrt(norm).asscalar()\n        if norm > clipping_norm:\n            for p in params:\n                p.grad[:] *= clipping_norm / norm\n\n\ndef predict_rnn(rnn, prefix, num_chars, params, hidden_dim, ctx, idx_to_char,\n                char_to_idx, get_inputs, is_lstm=False):\n    """"""Predict the next chars given the prefix.""""""\n    prefix = prefix.lower()\n    state_h = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n    if is_lstm:\n        state_c = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n    output = [char_to_idx[prefix[0]]]\n    for i in range(num_chars + len(prefix)):\n        X = nd.array([output[-1]], ctx=ctx)\n        if is_lstm:\n            Y, state_h, state_c = rnn(get_inputs(X), state_h, state_c, *params)\n        else:\n            Y, state_h = rnn(get_inputs(X), state_h, *params)\n        if i < len(prefix) - 1:\n            next_input = char_to_idx[prefix[i + 1]]\n        else:\n            next_input = int(Y[0].argmax(axis=1).asscalar())\n        output.append(next_input)\n    return \'\'.join([idx_to_char[i] for i in output])\n\n\ndef train_and_predict_rnn(rnn, is_random_iter, epochs, num_steps, hidden_dim,\n                          learning_rate, clipping_norm, batch_size,\n                          pred_period, pred_len, seqs, get_params, get_inputs,\n                          ctx, corpus_indices, idx_to_char, char_to_idx,\n                          is_lstm=False):\n    """"""Train an RNN model and predict the next item in the sequence.""""""\n    if is_random_iter:\n        data_iter = data_iter_random\n    else:\n        data_iter = data_iter_consecutive\n    params = get_params()\n\n    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    for e in range(1, epochs + 1):\n        # If consecutive sampling is used, in the same epoch, the hidden state\n        # is initialized only at the beginning of the epoch.\n        if not is_random_iter:\n            state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n            if is_lstm:\n                state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n        train_loss, num_examples = 0, 0\n        for data, label in data_iter(corpus_indices, batch_size, num_steps,\n                                     ctx):\n            # If random sampling is used, the hidden state has to be\n            # initialized for each mini-batch.\n            if is_random_iter:\n                state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n                if is_lstm:\n                    state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n            with autograd.record():\n                # outputs shape: (batch_size, vocab_size)\n                if is_lstm:\n                    outputs, state_h, state_c = rnn(get_inputs(data), state_h,\n                                                    state_c, *params)\n                else:\n                    outputs, state_h = rnn(get_inputs(data), state_h, *params)\n                # Let t_ib_j be the j-th element of the mini-batch at time i.\n                # label shape: (batch_size * num_steps)\n                # label = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ].\n                label = label.T.reshape((-1,))\n                # Concatenate outputs:\n                # shape: (batch_size * num_steps, vocab_size).\n                outputs = nd.concat(*outputs, dim=0)\n                # Now outputs and label are aligned.\n                loss = softmax_cross_entropy(outputs, label)\n            loss.backward()\n\n            grad_clipping(params, clipping_norm, ctx)\n            SGD(params, learning_rate)\n\n            train_loss += nd.sum(loss).asscalar()\n            num_examples += loss.size\n\n        if e % pred_period == 0:\n            print(""Epoch %d. Training perplexity %f"" % (e,\n                                                        exp(train_loss / num_examples)))\n            for seq in seqs:\n                print(\' - \', predict_rnn(rnn, seq, pred_len, params,\n                                         hidden_dim, ctx, idx_to_char, char_to_idx, get_inputs,\n                                         is_lstm))\n            print()\n\n'"
test/benchmark/vgg19_slim_benchmark.py,9,"b""import tensorflow as tf\nimport tensorflow.contrib.slim.nets as nets\nimport numpy as np\n\nslim = tf.contrib.slim\n\nif __name__ == '__main__':\n    output_shape = 1000\n    batch_size = 128\n    image = tf.placeholder(name='input_x', shape=[None, 224, 224, 3], dtype=tf.float32)\n    labels = tf.placeholder(name='input_label', shape=[None, output_shape], dtype=tf.float32)\n    with slim.arg_scope(nets.vgg.vgg_arg_scope()):\n        vgg_19, end_points = nets.vgg.vgg_19(inputs=image, num_classes=output_shape, scope='vgg_19')\n    probabilities = tf.reduce_mean(tf.nn.softmax(vgg_19, dim=-1))\n    losses = tf.norm(tf.subtract(probabilities, labels))\n    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(losses)\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n    while True:\n        datasets = np.random.randn(batch_size, 224, 224, 3).astype(np.float32)\n        datasets_labels = np.random.randn(batch_size, output_shape).astype(np.float32)\n        losses_val, _ = sess.run([losses, train_op], feed_dict={image: datasets, labels: datasets_labels})\n        print(losses_val)"""
test/benchmark/vgg19_tl_benchmark.py,5,"b'import tensorflow as tf\nfrom nets.vgg19 import get_vgg19\nimport numpy as np\n\n\nif __name__ == \'__main__\':\n    sess = tf.Session()\n    x = tf.placeholder(name=""inputs_x"", shape=[None, 224, 224, 3], dtype=tf.float32)\n    y = tf.placeholder(name=\'inputs_y\', shape=[None, 1000], dtype=tf.float32)\n    network = get_vgg19(x, sess, pretrained=False)\n    outputs_y = network.outputs\n    probs = tf.nn.softmax(outputs_y, name=""prob"")\n    loss = tf.reduce_mean(tf.subtract(probs, y))\n\n    while True:\n        batch_size = 128\n        datasets_x = np.random.randn(batch_size, 224, 224, 3).astype(np.float32)\n        datasets_y = np.random.randn(batch_size, 1000).astype(np.float32)\n        feed_dict = {x: datasets_x, y: datasets_y}\n        loss_val = sess.run(loss, feed_dict=feed_dict)\n        print(\'batch size %d, loss value is %.2f\' % (batch_size, loss_val))\n'"
test/multiple_gpu_test/__init__.py,0,b''
test/multiple_gpu_test/test_mgpu_mnist.py,45,"b'import tensorflow as tf\nimport tensorlayer as tl\nimport os\n\nLayer = tl.layers.Layer\nD_TYPE = tf.float32\n\nclass DenseLayer(Layer):\n    """"""\n    The :class:`DenseLayer` class is a fully connected layer.\n\n    Parameters\n    ----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    n_units : int\n        The number of units of the layer.\n    act : activation function\n        The function that is applied to the layer activations.\n    W_init : weights initializer\n        The initializer for initializing the weight matrix.\n    b_init : biases initializer or None\n        The initializer for initializing the bias vector. If None, skip biases.\n    W_init_args : dictionary\n        The arguments for the weights tf.get_variable.\n    b_init_args : dictionary\n        The arguments for the biases tf.get_variable.\n    name : a string or None\n        An optional name to attach to this layer.\n\n    Examples\n    --------\n    >>> network = tl.layers.InputLayer(x, name=\'input_layer\')\n    >>> network = tl.layers.DenseLayer(\n    ...                 network,\n    ...                 n_units=800,\n    ...                 act = tf.nn.relu,\n    ...                 W_init=tf.truncated_normal_initializer(stddev=0.1),\n    ...                 name =\'relu_layer\'\n    ...                 )\n\n    >>> Without TensorLayer, you can do as follow.\n    >>> W = tf.Variable(\n    ...     tf.random_uniform([n_in, n_units], -1.0, 1.0), name=\'W\')\n    >>> b = tf.Variable(tf.zeros(shape=[n_units]), name=\'b\')\n    >>> y = tf.nn.relu(tf.matmul(inputs, W) + b)\n\n    Notes\n    -----\n    If the input to this layer has more than two axes, it need to flatten the\n    input by using :class:`FlattenLayer` in this case.\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            n_units=100,\n            act=tf.identity,\n            W_init=tf.truncated_normal_initializer(stddev=0.1),\n            b_init=tf.constant_initializer(value=0.0),\n            W_init_args={},\n            b_init_args={},\n            name=\'dense_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        if self.inputs.get_shape().ndims != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        n_in = int(self.inputs.get_shape()[-1])\n        self.n_units = n_units\n        print(""  [TL] DenseLayer  %s: %d %s"" % (self.name, self.n_units, act.__name__))\n        with tf.variable_scope(name) as vs:\n            with tf.device(\'/cpu:0\'):\n                W = tf.get_variable(name=\'W\', shape=(n_in, n_units), initializer=W_init, dtype=D_TYPE, **W_init_args)\n            if b_init is not None:\n                try:\n                    with tf.device(\'/cpu:0\'):\n                        b = tf.get_variable(name=\'b\', shape=(n_units), initializer=b_init, dtype=D_TYPE, **b_init_args)\n                except:  # If initializer is a constant, do not specify shape.\n                    with tf.device(\'/cpu:0\'):\n                        b = tf.get_variable(name=\'b\', initializer=b_init, dtype=D_TYPE, **b_init_args)\n                self.outputs = act(tf.matmul(self.inputs, W) + b)\n            else:\n                self.outputs = act(tf.matmul(self.inputs, W))\n\n        # Hint : list(), dict() is pass by value (shallow), without them, it is\n        # pass by reference.\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        if b_init is not None:\n            self.all_params.extend([W, b])\n        else:\n            self.all_params.extend([W])\n\n\ndef inference(x):\n    network = tl.layers.InputLayer(x, name=\'input\')\n    network = tl.layers.DropoutLayer(network, keep=0.8, name=\'drop1\')\n    network = DenseLayer(network, n_units=800, act=tf.nn.relu, name=\'relu1\')\n    network = tl.layers.DropoutLayer(network, keep=0.5, name=\'drop2\')\n    network = DenseLayer(network, n_units=800, act=tf.nn.relu, name=\'relu2\')\n    network = tl.layers.DropoutLayer(network, keep=0.5, name=\'drop3\')\n    network = DenseLayer(network, n_units=10, act=tf.identity, name=\'output\')\n    y = network.outputs\n    return y\n\n\ndef load_data():\n    X_train, y_train, X_val, y_val, X_test, y_test = \\\n        tl.files.load_mnist_dataset(shape=(-1, 784), path=\'/home/aurora/workspaces/data\')\n    print(\'X_train.shape\', X_train.shape)\n    print(\'y_train.shape\', y_train.shape)\n    print(\'X_val.shape\', X_val.shape)\n    print(\'y_val.shape\', y_val.shape)\n    print(\'X_test.shape\', X_test.shape)\n    print(\'y_test.shape\', y_test.shape)\n    print(\'X %s   y %s\' % (X_test.dtype, y_test.dtype))\n    return X_train, y_train\n\n\ndef tower_losses(inputs, labels):\n    logit = inference(inputs)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=labels, name=\'cross_entropy\')\n    return loss\n\n\ndef average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, g1 in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\ndef train():\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n        global_step = tf.get_variable(\n            \'global_step\', [],\n            initializer=tf.constant_initializer(0), trainable=False)\n        # Decay the learning rate exponentially based on the number of steps.\n        lr = tf.train.exponential_decay(0.01,\n                                        global_step,\n                                        10000,\n                                        0.99,\n                                        staircase=True)\n        # Create an optimizer that performs gradient descent.\n        opt = tf.train.GradientDescentOptimizer(lr)\n        tower_grads = []\n        x = tf.placeholder(tf.float32, shape=[None, 784], name=\'x\')\n        y_ = tf.placeholder(tf.int64, shape=[None, ], name=\'y_\')\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(1):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'%s_%d\' % (\'tower\', i)) as scope:\n                        tl.layers.set_name_reuse(True)\n                        # Dequeues one batch for the GPU\n                        # Calculate the loss for one tower of the CIFAR model. This function\n                        # constructs the entire CIFAR model but shares the variables across\n                        # all towers.\n                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                        loss = tower_losses(x, y_)\n                        # Reuse variables for the next tower.\n                        tf.get_variable_scope().reuse_variables()\n                        # Calculate the gradients for the batch of data on this CIFAR tower.\n                        grads = opt.compute_gradients(loss)\n                        # Keep track of the gradients across all towers.\n                        tower_grads.append(grads)\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers.\n        grads = average_gradients(tower_grads)\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n        # Track the moving averages of all trainable variables.\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # Build an initialization operation to run below.\n        init = tf.global_variables_initializer()\n        sess = tf.Session(config=tf.ConfigProto(\n            allow_soft_placement=True,\n            log_device_placement=True))\n        sess.run(init)\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\n    train()\n\n'"
test/multiple_gpu_test/test_tensorlayer.py,27,"b'import tensorflow as tf\nimport tensorlayer as tl\n\nLayer = tl.layers.Layer\nD_TYPE = tf.float32\n\n\nclass DenseLayer(Layer):\n    """"""\n    The :class:`DenseLayer` class is a fully connected layer.\n\n    Parameters\n    ----------\n    layer : a :class:`Layer` instance\n        The `Layer` class feeding into this layer.\n    n_units : int\n        The number of units of the layer.\n    act : activation function\n        The function that is applied to the layer activations.\n    W_init : weights initializer\n        The initializer for initializing the weight matrix.\n    b_init : biases initializer or None\n        The initializer for initializing the bias vector. If None, skip biases.\n    W_init_args : dictionary\n        The arguments for the weights tf.get_variable.\n    b_init_args : dictionary\n        The arguments for the biases tf.get_variable.\n    name : a string or None\n        An optional name to attach to this layer.\n\n    Examples\n    --------\n    >>> network = tl.layers.InputLayer(x, name=\'input_layer\')\n    >>> network = tl.layers.DenseLayer(\n    ...                 network,\n    ...                 n_units=800,\n    ...                 act = tf.nn.relu,\n    ...                 W_init=tf.truncated_normal_initializer(stddev=0.1),\n    ...                 name =\'relu_layer\'\n    ...                 )\n\n    >>> Without TensorLayer, you can do as follow.\n    >>> W = tf.Variable(\n    ...     tf.random_uniform([n_in, n_units], -1.0, 1.0), name=\'W\')\n    >>> b = tf.Variable(tf.zeros(shape=[n_units]), name=\'b\')\n    >>> y = tf.nn.relu(tf.matmul(inputs, W) + b)\n\n    Notes\n    -----\n    If the input to this layer has more than two axes, it need to flatten the\n    input by using :class:`FlattenLayer` in this case.\n    """"""\n\n    def __init__(\n            self,\n            layer=None,\n            n_units=100,\n            act=tf.identity,\n            W_init=tf.truncated_normal_initializer(stddev=0.1),\n            b_init=tf.constant_initializer(value=0.0),\n            W_init_args={},\n            b_init_args={},\n            name=\'dense_layer\',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        if self.inputs.get_shape().ndims != 2:\n            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")\n\n        n_in = int(self.inputs.get_shape()[-1])\n        self.n_units = n_units\n        print(""  [TL] DenseLayer  %s: %d %s"" % (self.name, self.n_units, act.__name__))\n        with tf.variable_scope(name) as vs:\n            with tf.device(\'/cpu:0\'):\n                W = tf.get_variable(name=\'W\', shape=(n_in, n_units), initializer=W_init, dtype=D_TYPE, **W_init_args)\n            if b_init is not None:\n                try:\n                    with tf.device(\'/cpu:0\'):\n                        b = tf.get_variable(name=\'b\', shape=(n_units), initializer=b_init, dtype=D_TYPE, **b_init_args)\n                except:  # If initializer is a constant, do not specify shape.\n                    with tf.device(\'/cpu:0\'):\n                        b = tf.get_variable(name=\'b\', initializer=b_init, dtype=D_TYPE, **b_init_args)\n                self.outputs = act(tf.matmul(self.inputs, W) + b)\n            else:\n                self.outputs = act(tf.matmul(self.inputs, W))\n\n        # Hint : list(), dict() is pass by value (shallow), without them, it is\n        # pass by reference.\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        if b_init is not None:\n            self.all_params.extend([W, b])\n        else:\n            self.all_params.extend([W])\n\n\ndef inference():\n        x = tf.placeholder(tf.float32, shape=[None, 784], name=\'x\')\n        network = tl.layers.InputLayer(x, name=\'input\')\n        network = tl.layers.DropoutLayer(network, keep=0.8, name=\'drop1\')\n        network = DenseLayer(network, n_units=800, act=tf.nn.relu, name=\'relu1\')\n        network = tl.layers.DropoutLayer(network, keep=0.5, name=\'drop2\')\n        network = DenseLayer(network, n_units=800, act=tf.nn.relu, name=\'relu2\')\n        network = tl.layers.DropoutLayer(network, keep=0.5, name=\'drop3\')\n        network = DenseLayer(network, n_units=10, act=tf.identity, name=\'output\')\n        return network\n\n\nif __name__ == \'__main__\':\n    with tf.device(\'/gpu:0\'):\n        network = inference()\n        network.print_layers()\n        sess = tf.Session(config=tf.ConfigProto(\n                allow_soft_placement=True,\n                log_device_placement=True))\n        tl.layers.initialize_global_variables(sess)'"
