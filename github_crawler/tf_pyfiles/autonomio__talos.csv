file_path,api_count,code
setup.py,0,"b'#! /usr/bin/env python\r\n#\r\n# Copyright (C) 2018 Mikko Kotila\r\n\r\nDESCRIPTION = ""Talos Hyperparameter Tuning for Keras""\r\nLONG_DESCRIPTION = """"""\\\r\nTalos radically changes the ordinary Keras workflow by\r\nfully automating hyperparameter tuning and model evaluation.\r\nTalos exposes Keras functionality entirely and there is no new\r\nsyntax or templates to learn.\r\n""""""\r\n\r\nDISTNAME = \'talos\'\r\nMAINTAINER = \'Mikko Kotila\'\r\nMAINTAINER_EMAIL = \'mailme@mikkokotila.com\'\r\nURL = \'http://autonom.io\'\r\nLICENSE = \'MIT\'\r\nDOWNLOAD_URL = \'https://github.com/autonomio/talos/\'\r\nVERSION = \'0.6.7\'\r\n\r\ntry:\r\n    from setuptools import setup\r\n    _has_setuptools = True\r\nexcept ImportError:\r\n    from distutils.core import setup\r\n\r\ninstall_requires = [\'statsmodels>=0.11.0\',\r\n                    \'wrangle>=0.6.7\',\r\n                    \'numpy\',\r\n                    \'pandas\',\r\n                    \'tensorflow==1.14.0\',\r\n                    \'keras==2.3.0\',\r\n                    \'astetik\',\r\n                    \'sklearn\',\r\n                    \'tqdm\',\r\n                    \'chances\',\r\n                    \'kerasplotlib\',\r\n                    \'requests\']\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    setup(name=DISTNAME,\r\n          author=MAINTAINER,\r\n          author_email=MAINTAINER_EMAIL,\r\n          maintainer=MAINTAINER,\r\n          maintainer_email=MAINTAINER_EMAIL,\r\n          description=DESCRIPTION,\r\n          long_description=LONG_DESCRIPTION,\r\n          license=LICENSE,\r\n          url=URL,\r\n          version=VERSION,\r\n          download_url=DOWNLOAD_URL,\r\n          install_requires=install_requires,\r\n          packages=[\'talos\',\r\n                    \'talos.scan\',\r\n                    \'talos.templates\',\r\n                    \'talos.utils\',\r\n                    \'talos.model\',\r\n                    \'talos.parameters\',\r\n                    \'talos.reducers\',\r\n                    \'talos.metrics\',\r\n                    \'talos.commands\',\r\n                    \'talos.logging\',\r\n                    \'talos.autom8\'],\r\n\r\n          classifiers=[\'Intended Audience :: Science/Research\',\r\n                       \'Programming Language :: Python :: 2.7\',\r\n                       \'Programming Language :: Python :: 3.5\',\r\n                       \'Programming Language :: Python :: 3.6\',\r\n                       \'License :: OSI Approved :: MIT License\',\r\n                       \'Topic :: Scientific/Engineering :: Human Machine Interfaces\',\r\n                       \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\r\n                       \'Topic :: Scientific/Engineering :: Mathematics\',\r\n                       \'Operating System :: POSIX\',\r\n                       \'Operating System :: Unix\',\r\n                       \'Operating System :: MacOS\',\r\n                       \'Operating System :: Microsoft :: Windows :: Windows 10\'])\r\n'"
test_script.py,0,"b'#!/usr/bin/env python\n\nif __name__ == \'__main__\':\n\n    from test.commands import *\n\n    test_latest()\n    test_random_methods()\n\n    test_autom8()\n    test_templates()\n    scan_object = test_scan()\n    test_analyze(scan_object)\n\n    test_lr_normalizer()\n    test_predict()\n    test_reducers()\n    test_rest(scan_object)\n\n    print(""\\n All tests successfully completed :) Good work. \\n "")\n'"
examples/iris.py,0,"b""# first import things as you would usually\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.losses import categorical_crossentropy, logcosh\nfrom keras.activations import relu, elu, softmax\n\n# import talos\nimport talos\n\n# load rthe iris dataset\nx, y = talos.datasets.iris()\n\n# then define the parameter boundaries\n\np = {'lr': (2, 10, 30),\n     'first_neuron': [4, 8, 16, 32, 64, 128],\n     'hidden_layers': [2, 3, 4, 5, 6],\n     'batch_size': [2, 3, 4],\n     'epochs': [300],\n     'dropout': (0, 0.40, 10),\n     'weight_regulizer': [None],\n     'emb_output_dims': [None],\n     'optimizer': ['adam', 'nadam'],\n     'losses': [categorical_crossentropy, logcosh],\n     'activation': [relu, elu],\n     'last_activation': [softmax]}\n\n\n# then define your Keras model\ndef iris_model(x_train, y_train, x_val, y_val, params):\n\n    model = Sequential()\n    model.add(Dense(params['first_neuron'],\n                    input_dim=x_train.shape[1],\n                    activation=params['activation']))\n    model.add(Dropout(params['dropout']))\n    model.add(Dense(y_train.shape[1], activation=params['last_activation']))\n\n    model.compile(optimizer=params['optimizer'],\n                  loss=params['losses'],\n                  metrics=['acc'])\n\n    out = model.fit(x_train, y_train,\n                    batch_size=params['batch_size'],\n                    epochs=params['epochs'],\n                    verbose=0,\n                    validation_data=[x_val, y_val])\n\n    return out, model\n\n\n# and run the scan\nh = talos.Scan(x, y,\n               params=p,\n               experiment_name='first_test',\n               model=iris_model,\n               fraction_limit=0.5)\n"""
talos/__init__.py,0,"b'import warnings\r\nwarnings.simplefilter(\'ignore\')\r\n\r\n# import commands\r\nfrom .scan.Scan import Scan\r\nfrom .commands.analyze import Analyze\r\nfrom .commands.analyze import Analyze as Reporting\r\nfrom .commands.predict import Predict\r\nfrom .commands.deploy import Deploy\r\nfrom .commands.evaluate import Evaluate\r\nfrom .commands.restore import Restore\r\n\r\n# import extras\r\nfrom . import utils\r\nfrom . import templates\r\nfrom . import autom8\r\n\r\n# the purpose of everything below is to keep the namespace completely clean\r\n\r\ntemplate_sub = [templates.datasets,\r\n                templates.models,\r\n                templates.params,\r\n                templates.pipelines]\r\n\r\nkeep_from_templates = [\'iris\', \'cervical_cancer\', \'titanic\', \'breast_cancer\',\r\n                       \'icu_mortality\', \'telco_churn\', \'mnist\']\r\n\r\nfor sub in template_sub:\r\n    for key in list(sub.__dict__):\r\n        if key.startswith(\'__\') is False:\r\n            if key not in keep_from_templates:\r\n                delattr(sub, key)\r\n\r\ndel commands, scan, model, metrics, key\r\ndel sub, keep_from_templates, template_sub, warnings\r\n\r\n__version__ = ""0.6.7""\r\n'"
test/__init__.py,0,b''
test/__main__.py,0,"b""import sys\n\nsys.path.append('../talos')\n"""
talos/autom8/__init__.py,0,"b'from .automodel import AutoModel\nfrom .autoparams import AutoParams\nfrom .autopredict import AutoPredict\nfrom .autoscan import AutoScan\n\ndel automodel, autoparams, autopredict, autoscan\n'"
talos/autom8/automodel.py,0,"b'class AutoModel:\n\n    def __init__(self, task, experiment_name, metric=None):\n\n        \'\'\'\n\n        Creates an input model for Scan(). Optimized for being used together\n        with Params(). For example:\n\n        p = talos.AutoParams().params\n        model = talos.AutoModel(task=\'binary\').model\n\n        talos.Scan(x, y, p, model)\n\n        NOTE: the parameter space from Params() is very large, so use limits\n        in or reducers in Scan() accordingly.\n\n        task : string or None\n            If \'continuous\' then mae is used for metric, if \'binary\',\n            \'multiclass\', or \'multilabel\', f1score is used. Accuracy is always\n            used.\n        experiment_name | str | Must be same as in `Scan()`\n        metric : None or list\n            You can also input a list with one or more custom metrics or names\n            of Keras or Talos metrics.\n        \'\'\'\n\n        from talos.utils.experiment_log_callback import ExperimentLogCallback\n\n        self.task = task\n        self.experiment_name = experiment_name\n        self.metric = metric\n\n        if self.task is not None:\n            self.metrics = self._set_metric()\n        elif self.metric is not None and isinstance(self.metric, list):\n            self.metrics = self.metric + [\'acc\']\n        else:\n            print(""Either pick task or provide list as input for metric."")\n\n        # create the model\n        self.model = self._create_input_model\n        self.callback = ExperimentLogCallback\n\n    def _set_metric(self):\n\n        """"""Sets the metric for the model based on the experiment type\n        or a list of metrics from user.""""""\n\n        import talos as ta\n\n        if self.task in [\'binary\', \'multiclass\', \'multilabel\']:\n            return [ta.utils.metrics.f1score, \'acc\']\n        elif self.task == \'continuous\':\n            return [ta.utils.metrics.mae, \'acc\']\n\n    def _create_input_model(self, x_train, y_train, x_val, y_val, params):\n\n        import wrangle as wr\n\n        from keras.models import Sequential\n        from keras.layers import Dropout, Flatten\n        from keras.layers import LSTM, Conv1D, SimpleRNN, Dense, Bidirectional\n\n        model = Sequential()\n\n        if params[\'network\'] != \'dense\':\n            x_train = wr.array_reshape_conv1d(x_train)\n            x_val = wr.array_reshape_conv1d(x_val)\n\n        if params[\'network\'] == \'conv1d\':\n            model.add(Conv1D(params[\'first_neuron\'], x_train.shape[1]))\n            model.add(Flatten())\n\n        elif params[\'network\'] == \'lstm\':\n            model.add(LSTM(params[\'first_neuron\']))\n\n        if params[\'network\'] == \'bidirectional_lstm\':\n            model.add(Bidirectional(LSTM(params[\'first_neuron\'])))\n\n        elif params[\'network\'] == \'simplernn\':\n            model.add(SimpleRNN(params[\'first_neuron\']))\n\n        elif params[\'network\'] == \'dense\':\n            model.add(Dense(params[\'first_neuron\'],\n                            input_dim=x_train.shape[1],\n                            activation=\'relu\',\n                            kernel_initializer=params[\'kernel_initializer\']))\n\n        model.add(Dropout(params[\'dropout\']))\n\n        # add hidden layers to the model\n        from talos.model.hidden_layers import hidden_layers\n        hidden_layers(model, params, 1)\n\n        # get the right activation and last_neuron based on task\n        from talos.model.output_layer import output_layer\n        activation, last_neuron = output_layer(self.task,\n                                               params[\'last_activation\'],\n                                               y_train,\n                                               y_val)\n\n        model.add(Dense(last_neuron,\n                        activation=activation,\n                        kernel_initializer=params[\'kernel_initializer\']))\n\n        # bundle the optimizer with learning rate changes\n        from talos.model.normalizers import lr_normalizer\n        optimizer = params[\'optimizer\'](lr=lr_normalizer(params[\'lr\'],\n                                                         params[\'optimizer\']))\n\n        # compile the model\n        model.compile(optimizer=optimizer,\n                      loss=params[\'losses\'],\n                      metrics=self.metrics)\n\n        # fit the model\n        out = model.fit(x_train, y_train,\n                        batch_size=params[\'batch_size\'],\n                        epochs=params[\'epochs\'],\n                        verbose=0,\n                        callbacks=[self.callback(self.experiment_name, params)],\n                        validation_data=[x_val, y_val])\n\n        # pass the output to Talos\n        return out, model\n'"
talos/autom8/autoparams.py,0,"b""import numpy as np\nfrom keras.optimizers import Adam, Nadam, Adadelta, SGD\n\n\nloss = {'binary': ['binary_crossentropy', 'logcosh'],\n        'multi_class': ['sparse_categorical_crossentropy'],\n        'multi_label': ['categorical_crossentropy'],\n        'continuous': ['mae']}\n\nlast_activation = {'binary': ['sigmoid'],\n                   'multi_class': ['softmax'],\n                   'multi_label': ['softmax'],\n                   'continuous': [None]}\n\n\nclass AutoParams:\n\n    def __init__(self,\n                 params=None,\n                 task='binary',\n                 replace=True,\n                 auto=True,\n                 network=True,\n                 resample_params=4):\n\n        '''A facility for generating or appending params dictionary.\n\n        params : dict or None\n        task : str\n             'binary', 'multi_class', 'multi_label', or 'continuous'\n        replace : bool\n             Replace current dictionary entries with new ones.\n        auto : bool\n             Automatically generate or append params dictionary with\n             all available parameters.\n        network : bool\n             Adds several network architectures as parameters. This is to be\n             used as an input together with KerasModel(). If False then only\n             'dense' will be added.\n        resample_params | int or False | The number of values per parameter\n        '''\n\n        self._task = task\n        self._replace = replace\n        self._network = network\n\n        if params is None:\n            self.params = {}\n        else:\n            self.params = params\n        if auto:\n            self._automated()\n\n        if resample_params is not False:\n            self.resample_params(resample_params)\n\n    def _automated(self, shapes='fixed'):\n\n        '''Automatically generate a comprehensive\n        parameter dict to be used in Scan()\n\n        shapes : string\n            Either 'fixed' or 'sloped'\n\n        '''\n\n        if shapes == 'fixed':\n            self.shapes()\n        else:\n            self.shapes_slope()\n        self.layers()\n        self.dropout()\n        self.optimizers()\n        self.activations()\n        self.neurons()\n        self.losses()\n        self.batch_size()\n        self.epochs()\n        self.kernel_initializers()\n        self.lr()\n        if self._network:\n            self.networks()\n        else:\n            self.params['network'] = ['dense']\n        self.last_activations()\n\n    def shapes(self, shapes='auto'):\n\n        '''Uses triangle, funnel, and brick shapes.'''\n\n        if shapes == 'auto':\n            self._append_params('shapes', ['triangle', 'funnel', 'brick'])\n        else:\n            self._append_params('shapes', shapes)\n\n    def shapes_slope(self, min_slope=0, max_slope=.6, steps=.1):\n\n        '''Uses a single decimal float for values below 0.5 to\n        reduce the width of the following layer.'''\n\n        self._append_params('shapes', np.arange(min_slope,\n                                                max_slope,\n                                                steps).tolist())\n\n    def layers(self, min_layers=0, max_layers=6, steps=1):\n\n        self._append_params('hidden_layers',\n                            list(range(min_layers, max_layers, steps)))\n\n    def dropout(self, min_dropout=0, max_dropout=.85, steps=0.1):\n\n        self._append_params('dropout',\n                            np.round(np.arange(min_dropout,\n                                               max_dropout,\n                                               steps), 2).tolist())\n\n    def optimizers(self, optimizers='auto'):\n\n        '''If `optimizers='auto'` then optimizers will be picked based on\n        automatically. Otherwise input a list with one or\n        more optimizers will be used.\n        '''\n\n        if optimizers == 'auto':\n            self._append_params('optimizer', [Adam, Nadam, Adadelta, SGD])\n        else:\n            self._append_params('optimizer', optimizers)\n\n    def activations(self, activations='auto'):\n\n        '''If `activations='auto'` then activations will be picked based on\n        automatically. Otherwise input a list with one or\n        more activations will be used.\n        '''\n\n        if activations == 'auto':\n            activations = ['relu', 'elu']\n\n        self._append_params('activation', activations)\n\n    def losses(self, losses='auto'):\n\n        '''If `losses='auto'` then losses will be picked based on\n        `AutoParam()` argument `task`. Otherwise input a list with one or\n        more losses will be used.\n        '''\n\n        if losses == 'auto':\n            self._append_params('losses', loss[self._task])\n        else:\n            self._append_params('losses', losses)\n\n    def neurons(self, min_neuron=8, max_neuron=None, steps=None):\n\n        '''`max` and `steps` has to be either `None` or\n        integer value at the same time.'''\n\n        if max_neuron is None and steps is None:\n            values = [int(np.exp2(i)) for i in range(3, 11)]\n        else:\n            values = list(range(min_neuron, max_neuron, steps))\n\n        self._append_params('first_neuron', values)\n\n    def batch_size(self, min_size=8, max_size=None, steps=None):\n\n        '''`max_size` and `steps` has to be either `None` or\n        integer value at the same time.'''\n\n        if max_size is None and steps is None:\n            values = [int(np.exp2(i/2)) for i in range(3, 15)]\n        else:\n            values = list(range(min_size, max_size, steps))\n\n        self._append_params('batch_size', values)\n\n    def epochs(self, min_epochs=50, max_epochs=None, steps=None):\n\n        '''`max_epochs` and `steps` has to be either `None` or\n        integer value at the same time.'''\n\n        if max_epochs is None and steps is None:\n            values = [int(np.exp2(i/2))+50 for i in range(3, 15)]\n        else:\n            values = list(range(min_epochs, max_epochs, steps))\n\n        self._append_params('epochs', values)\n\n    def kernel_initializers(self, kernel_inits='auto'):\n\n        '''\n        kernel_inits | list | one or more kernel initializers\n        '''\n\n        if kernel_inits == 'auto':\n            self._append_params('kernel_initializer',\n                                ['uniform', 'normal', 'he_normal',\n                                 'he_uniform', 'lecun_normal',\n                                 'glorot_uniform', 'glorot_normal',\n                                 'random_uniform', 'random_normal'])\n        else:\n            self._append_params('kernel_initializer', kernel_inits)\n\n    def lr(self, learning_rates='auto'):\n\n        '''If `learning_rates='auto'` then a very wide range of learning rates\n        will be added. Otherwise a list with one or more learning rates\n        is used.\n\n        NOTE: talos.utils.lr_normalizer should be used if more than one optimizer\n        is used in the experiment\n        '''\n\n        if learning_rates == 'auto':\n\n            a = np.round(np.arange(0.01, 0.2, 0.02), 3).tolist()\n            b = np.round(np.arange(0, 1, 0.2), 2).tolist()\n            c = list(range(0, 11))\n\n            self._append_params('lr', a + b + c)\n\n        else:\n\n            self._append_params('lr', learning_rates)\n\n    def networks(self, networks='auto'):\n\n        '''If `network='auto'` then dense, simplernn, lstm, conv1d, and\n        bidirectional_lstm are added. Otherwise a list with one or more\n        network architectures is used.\n        '''\n\n        if networks == 'auto':\n            self._append_params('network', ['dense',\n                                            'simplernn',\n                                            'lstm',\n                                            'bidirectional_lstm',\n                                            'conv1d'])\n        else:\n            self._append_params('network', networks)\n\n    def last_activations(self, last_activations='auto'):\n\n        '''If `last_activations='auto'` then activations will be picked\n        automatically based on `AutoParams` property `task`.\n        Otherwise input a list with one or more activations will be used.\n        '''\n\n        if last_activations == 'auto':\n            self._append_params('last_activation', last_activation[self._task])\n        else:\n            self._append_params('last_activation', last_activations)\n\n    def resample_params(self, n):\n\n        '''Resamples params dictionary so that `n` values are present for each\n        parameter.'''\n\n        from wrangle import dic_resample_values\n\n        self.params = dic_resample_values(self.params, n)\n\n    def _append_params(self, label, values):\n\n        if self._replace is False:\n            try:\n                self.params[label]\n            except KeyError:\n                self.params[label] = values\n\n        else:\n            self.params[label] = values\n"""
talos/autom8/autopredict.py,0,"b'def AutoPredict(scan_object,\n                x_val,\n                y_val,\n                x_pred,\n                task,\n                metric=\'val_acc\',\n                n_models=10,\n                folds=5,\n                shuffle=True,\n                asc=False):\n\n    \'\'\'Automatically handles the process of finding the best models from a\n    completed `Scan()` experiment, evaluates those models, and uses the winner\n    to make predictions on input data.\n\n    NOTE: the input data must be in same format as \'x\' that was\n    used in `Scan()`.\n\n    Parameters\n    ----------\n    scan_object : Scan() object\n        A Scan() process needs to be completed first, and then the resulting\n        object can be used as input here.\n    x_val : ndarray or list of ndarray\n        Data to be used for \'x\' in evaluation. Note that should be in the same\n        format as the data which was used in the Scan() but not the same data.\n    y_val : ndarray or list of ndarray\n        Data to be used for \'y\' in evaluation. Note that should be in the same\n        format as the data which was used in the Scan() but not the same data.\n    y_pred : ndarray or list of ndarray\n        Input data to be used for the actual predictions in evaluation. Note\n        it should be in the same format as the data which was used in the\n        Scan() but not the same data.\n    task : string\n        \'binary\', \'multi_class\', \'multi_label\', or \'continuous\'.\n    metric : str\n        The metric to be used for deciding which models are promising.\n        Basically the \'n\' argument and \'metric\' argument are combined to pick\n        \'n\' best performing models based on \'metric\'.\n    n_models : str\n        Number of promising models to be included in the evaluation process.\n        Time increase linearly with number of models.\n    folds : int\n        Number of folds to be used in cross-validation.\n    shuffle : bool\n        If the data should be shuffled before cross-validation.\n    average : str\n        This parameter is required for multiclass/multilabel targets. If None,\n        the scores for each class are returned. Otherwise, this determines\n        the type of averaging performed on the data:\n\n        \'binary\':\n        Only report results for the class specified by pos_label.\n        This is applicable only if targets (y_{true,pred}) are binary.\n\n        \'micro\':\n        Calculate metrics globally by counting the total true positives,\n        false negatives and false positives.\n\n        \'macro\':\n        Calculate metrics for each label, and find their unweighted mean.\n        This does not take label imbalance into account.\n\n        \'weighted\':\n        Calculate metrics for each label, and find their average weighted\n        by support (the number of true instances for each label). This alters\n        \'macro\' to account for label imbalance; it can result in an F-score\n        that is not between precision and recall.\n\n        \'samples\':\n        Calculate metrics for each instance, and find their average\n        (only meaningful for multilabel classification where this differs\n        from accuracy_score).\n    asc : bool\n        This needs to be True for evaluation metrics that need to be minimized,\n        and False when a metric needs to be maximized.\n\n    \'\'\'\n\n    # evaluate and add the evaluation scores\n    scan_object.evaluate_models(x_val,\n                                y_val,\n                                n_models=n_models,\n                                task=task,\n                                metric=metric,\n                                folds=folds,\n                                shuffle=shuffle,\n                                asc=False)\n\n    # get the best model based on evaluated score\n    scan_object.preds_model = scan_object.best_model(\'eval_f1score_mean\')\n\n    # make predictions with the model\n    scan_object.preds_probabilities = scan_object.preds_model.predict(x_pred)\n\n    # make (class) predictiosn with the model\n    scan_object.preds_classes = scan_object.preds_model.predict_classes(x_pred)\n\n    # get the hyperparameter for the model\n    scan_object.preds_parameters = scan_object.data.sort_values(\'eval_f1score_mean\',\n                                                           ascending=False).iloc[0]\n\n    print("">> Added model, probabilities, classes, and parameters to scan_object"")\n\n    return scan_object\n'"
talos/autom8/autoscan.py,0,"b""class AutoScan:\n\n    def __init__(self,\n                 task,\n                 experiment_name,\n                 max_param_values=None):\n\n        '''Configure the `AutoScan()` experiment and then use\n        the property `start` in the returned class object to start\n        the actual experiment.\n\n        `task` | str | 'binary', 'multi_class', 'multi_label', or 'continuous'\n        `max_param_values` | int | Number of parameter values to be included.\n                                   Note, this will only work when `params` is\n                                   not passed as kwargs in `AutoScan.start`.\n        '''\n\n        self.task = task\n        self.max_param_values = max_param_values\n        self.experiment_name = experiment_name\n\n    def start(self, x, y, **kwargs):\n\n        '''Start the scan. Note that you can use `Scan()` arguments as you\n        would otherwise directly interacting with `Scan()`.\n\n        `x` | array or list of arrays | prediction features\n        `y` | array or list of arrays | prediction outcome variable\n        `kwargs` | arguments | any `Scan()` argument can be passed here\n\n        '''\n\n        import talos\n\n        m = talos.autom8.AutoModel(self.task, self.experiment_name).model\n\n        try:\n            kwargs['params']\n            scan_object = talos.Scan(x, y,\n                                     model=m,\n                                     experiment_name=self.experiment_name,\n                                     **kwargs)\n        except KeyError:\n            p = talos.autom8.AutoParams(task=self.task)\n\n            if self.max_param_values is not None:\n                p.resample_params(self.max_param_values)\n            params = p.params\n            scan_object = talos.Scan(x=x,\n                                     y=y,\n                                     params=params,\n                                     model=m,\n                                     experiment_name=self.experiment_name,\n                                     **kwargs)\n\n        return scan_object\n"""
talos/commands/__init__.py,0,b''
talos/commands/analyze.py,0,"b""class Analyze:\n\n    '''A suite of commands that are useful for analyzing the results\n    of a completed scan, or during a scan.\n\n    filename :: the name of the experiment log from Scan()'''\n\n    def __init__(self, source=None):\n\n        '''Takes as input a filename to the experiment\n        log or the Scan object'''\n\n        import pandas as pd\n\n        if isinstance(source, str):\n            self.data = pd.read_csv(source)\n        else:\n            self.data = source.data\n\n    def high(self, metric):\n\n        '''Returns the highest value for a given metric'''\n\n        return max(self.data[metric])\n\n    def rounds(self):\n\n        '''Returns the number of rounds in the experiment'''\n\n        return len(self.data)\n\n    def rounds2high(self, metric):\n\n        '''Returns the number of rounds it took to get to the\n        highest value for a given metric.'''\n\n        return self.data[self.data[metric] == self.data[metric].max()].index[0]\n\n    def low(self, metric):\n\n        '''Returns the minimum value for a given metric'''\n\n        return min(self.data[metric])\n\n    def correlate(self, metric, exclude):\n\n        '''Returns a correlation table against a given metric. Drops\n        all other metrics and correlates against hyperparameters only.\n\n        metric | str | Column label for the metric to correlate with\n        exclude | list | Column label/s to be excluded from the correlation\n\n        NOTE: You should use `exclude` to avoid correlating with other metrics.\n\n        '''\n\n        columns = [c for c in self.data.columns if c not in exclude + [metric]]\n        out = self.data[columns]\n        out.insert(0, metric, self.data[metric])\n\n        out = out.corr()[metric]\n\n        return out[out != 1]\n\n    def plot_line(self, metric):\n\n        '''A line plot for a given metric where rounds is on x-axis\n\n        NOTE: remember to invoke %matplotlib inline if in notebook\n\n        metric | str | Column label for the metric to correlate with\n\n        '''\n        try:\n            import astetik as ast\n            return ast.line(self.data, metric)\n        except:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def plot_hist(self, metric, bins=10):\n\n        '''A histogram for a given metric\n\n        NOTE: remember to invoke %matplotlib inline if in notebook\n\n        metric | str | Column label for the metric to correlate with\n        bins | int | Number of bins to use in histogram\n\n        '''\n        try:\n            import astetik as ast\n            return ast.hist(self.data, metric, bins=bins)\n        except RuntimeError:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def plot_corr(self, metric, exclude, color_grades=5):\n\n        '''A heatmap with a single metric and hyperparameters.\n\n        NOTE: remember to invoke %matplotlib inline if in notebook\n\n        metric | str | Column label for the metric to correlate with\n        exclude | list | Column label/s to be excluded from the correlation\n        color_grades | int | Number of colors to use in heatmap\n\n        '''\n\n        try:\n            import astetik as ast\n            cols = self._cols(metric, exclude)\n            return ast.corr(self.data[cols], color_grades=color_grades)\n        except RuntimeError:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def plot_regs(self, x, y):\n\n        '''A regression plot with data on two axis\n\n        x = data for the x axis\n        y = data for the y axis\n        '''\n\n        try:\n            import astetik as ast\n            return ast.regs(self.data, x, y)\n        except RuntimeError:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def plot_box(self, x, y, hue=None):\n\n        '''A box plot with data on two axis\n\n        x = data for the x axis\n        y = data for the y axis\n        hue = data for the hue separation\n        '''\n        try:\n            import astetik as ast\n            return ast.box(self.data, x, y, hue)\n        except RuntimeError:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def plot_bars(self, x, y, hue, col):\n\n        '''A comparison plot with 4 axis'''\n\n        try:\n            import astetik as ast\n            return ast.bargrid(self.data,\n                               x=x,\n                               y=y,\n                               hue=hue,\n                               col=col,\n                               col_wrap=4)\n        except RuntimeError:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def plot_kde(self, x, y=None):\n\n        '''Kernel Destiny Estimation type histogram with\n        support for 1 or 2 axis of data'''\n\n        try:\n            import astetik as ast\n            return ast.kde(self.data, x, y)\n        except RuntimeError:\n            print('Matplotlib Runtime Error. Plots will not work.')\n\n    def table(self, metric, exclude=[], sort_by=None, ascending=False):\n\n        '''Shows a table with hyperparameters and a given metric\n\n        EXAMPLE USE:\n\n        ra1 = Reporting('diabetes_1.csv')\n        ra1.table(sort_by='fmeasure_acc', ascending=False)\n\n        PARAMS:\n\n        metric | str or list | Column labels for the metric to correlate with\n        exclude | list | Column label/s to be excluded from the correlation\n        sort_by | str | The colunm name sorting should be based on\n        ascending | bool | Set to True when `sort_by` is to be minimized eg. loss\n\n        '''\n\n        cols = self._cols(metric, exclude)\n\n        if sort_by is None:\n            sort_by = metric\n\n        out = self.data[cols].sort_values(sort_by, ascending=ascending)\n\n        return out\n\n    def best_params(self, metric, exclude, n=10, ascending=False):\n\n        '''Get the best parameters of the experiment based on a metric.\n        Returns a numpy array with the values in a format that can be used\n        with the talos backend in Scan(). Adds an index as the last column.\n\n        metric | str or list | Column labels for the metric to correlate with\n        exclude | list | Column label/s to be excluded from the correlation\n        n | int | Number of hyperparameter permutations to be returned\n        ascending | bool | Set to True when `metric` is to be minimized eg. loss\n\n        '''\n\n        cols = self._cols(metric, exclude)\n        out = self.data[cols].sort_values(metric, ascending=ascending)\n        out = out.drop(metric, axis=1).head(n)\n        out.insert(out.shape[1], 'index_num', range(len(out)))\n\n        return out.values\n\n    def _cols(self, metric, exclude):\n\n        '''Helper to remove other than desired metric from data table'''\n\n        cols = [col for col in self.data.columns if col not in exclude + [metric]]\n\n        if isinstance(metric, list) is False:\n            metric = [metric]\n        for i, metric in enumerate(metric):\n            cols.insert(i, metric)\n\n        # make sure only unique values in col list\n        cols = list(set(cols))\n\n        return cols\n"""
talos/commands/deploy.py,0,"b'class Deploy:\n\n    \'\'\'Functionality for deploying a model to a filename\'\'\'\n\n    def __init__(self, scan_object, model_name, metric, asc=False):\n\n        \'\'\'Deploy a model to be used later or in a different system.\n\n        NOTE: for a metric that is to be minimized, set asc=True or otherwise\n        you will end up with the model that has the highest loss.\n\n        Deploy() takes in the object from Scan() and creates a package locally\n        that can be later activated with Restore().\n\n        scan_object : object\n            The object that is returned from Scan() upon completion.\n        model_name : str\n            Name for the .zip file to be created.\n        metric : str\n            The metric to be used for picking the best model.\n        asc: bool\n            Make this True for metrics that are to be minimized (e.g. loss) ,\n            and False when the metric is to be maximized (e.g. acc)\n\n        \'\'\'\n\n        import os\n\n        self.scan_object = scan_object\n        os.mkdir(model_name)\n        self.path = model_name + \'/\' + model_name\n        self.model_name = model_name\n        self.metric = metric\n        self.asc = asc\n        self.data = scan_object.data\n\n        from ..utils.best_model import best_model, activate_model\n        self.best_model = best_model(scan_object, metric, asc)\n        self.model = activate_model(scan_object, self.best_model)\n\n        # runtime\n        self.save_model_as()\n        self.save_details()\n        self.save_data()\n        self.save_results()\n        self.save_params()\n        self.save_readme()\n        self.package()\n\n    def save_model_as(self):\n\n        \'\'\'Model Saver\n        WHAT: Saves a trained model so it can be loaded later\n        for predictions by predictor().\n        \'\'\'\n\n        model_json = self.model.to_json()\n        with open(self.path + ""_model.json"", ""w"") as json_file:\n            json_file.write(model_json)\n\n        self.model.save_weights(self.path + ""_model.h5"")\n        print(""Deploy package"" + "" "" + self.model_name + "" "" + ""have been saved."")\n\n    def save_details(self):\n\n        self.scan_object.details.to_csv(self.path + \'_details.txt\')\n\n    def save_data(self):\n\n        import pandas as pd\n\n        # input data is <= 2d\n        try:\n            x = pd.DataFrame(self.scan_object.x[:100])\n            y = pd.DataFrame(self.scan_object.y[:100])\n\n        # input data is > 2d\n        except ValueError:\n            x = pd.DataFrame()\n            y = pd.DataFrame()\n            print(""data is not 2d, dummy data written instead."")\n\n        x.to_csv(self.path + \'_x.csv\', header=None, index=None)\n        y.to_csv(self.path + \'_y.csv\', header=None, index=None)\n\n    def save_results(self):\n\n        self.scan_object.data.to_csv(self.path + \'_results.csv\')\n\n    def save_params(self):\n\n        import numpy as np\n\n        np.save(self.path + \'_params\', self.scan_object.params)\n\n    def save_readme(self):\n\n        txt = \'To activate the assets in the Talos deploy package: \\n\\n   from talos.commands.restore import Restore \\n   a = Restore(\\\'path_to_asset\\\')\\n\\nNow you will have an object similar to the Scan object, which can be used with other Talos commands as you would be able to with the Scan object\'\n\n        text_file = open(self.path.split(\'/\')[0] + \'/README.txt\', ""w"")\n        text_file.write(txt)\n        text_file.close()\n\n    def package(self):\n\n        import shutil\n\n        shutil.make_archive(self.model_name, \'zip\', self.model_name)\n        shutil.rmtree(self.model_name)\n'"
talos/commands/evaluate.py,0,"b'class Evaluate:\n\n    \'\'\'Class for evaluating models based on the Scan() object\'\'\'\n\n    def __init__(self, scan_object):\n\n        \'\'\'Takes in as input a Scan() object.\n        e = evaluate(scan_object) and see docstring\n        for e() for more information.\'\'\'\n\n        self.scan_object = scan_object\n        self.data = scan_object.data\n\n    def evaluate(self,\n                 x,\n                 y,\n                 task,\n                 metric,\n                 model_id=None,\n                 folds=5,\n                 shuffle=True,\n                 asc=False,\n                 print_out=False):\n\n        \'\'\'Evaluate a model based on f1_score (all except regression)\n        or mae (for regression). Supports \'binary\', \'multi_class\',\n        \'multi_label\', and \'regression\' evaluation.\n\n        x : array\n            The input data for making predictions\n        y : array\n            The ground truth for x\n        model_id : int\n            It\'s possible to evaluate a specific model based on ID.\n            Can be None.\n        folds : int\n            Number of folds to use for cross-validation\n        sort_metric : string\n            A column name referring to the metric that was used in the\n            scan_object as a performance metric. This is used for sorting\n            the results to pick for evaluation.\n        shuffle : bool\n            Data is shuffled before evaluation.\n        task : string\n            \'binary\', \'multi_class\', \'multi_label\', or \'continuous\'.\n        asc : bool\n            False if the metric is to be optimized upwards\n            (e.g. accuracy or f1_score)\n        print_out : bool\n            Print out the results.\n\n        TODO: add possibility to input custom metrics.\n\n        \'\'\'\n\n        import numpy as np\n        import sklearn as sk\n\n        out = []\n        if model_id is None:\n            from ..utils.best_model import best_model\n            model_id = best_model(self.scan_object, metric, asc)\n\n        from ..utils.best_model import activate_model\n        model = activate_model(self.scan_object, model_id)\n\n        from ..utils.validation_split import kfold\n        kx, ky = kfold(x, y, folds, shuffle)\n\n        for i in range(folds):\n\n            y_pred = model.predict(kx[i], verbose=0)\n\n            if task == \'binary\':\n                y_pred = np.array(y_pred) >= .5\n                scores = sk.metrics.f1_score(y_pred, ky[i], average=\'binary\')\n\n            elif task == \'multi_class\':\n                y_pred = y_pred.argmax(axis=-1)\n                scores = sk.metrics.f1_score(y_pred, ky[i], average=\'macro\')\n\n            if task == \'multi_label\':\n                y_pred = model.predict(kx[i]).argmax(axis=1)\n                scores = sk.metrics.f1_score(y_pred,\n                                             ky[i].argmax(axis=1),\n                                             average=\'macro\')\n\n            elif task == \'continuous\':\n                y_pred = model.predict(kx[i])\n                scores = sk.metrics.mean_absolute_error(y_pred, ky[i])\n\n            out.append(scores)\n\n        if print_out is True:\n            print(""mean : %.2f \\n std : %.2f"" % (np.mean(out), np.std(out)))\n\n        return out\n'"
talos/commands/predict.py,0,"b""class Predict:\n\n    '''Class for making predictions on the models that are stored\n    in the Scan() object'''\n\n    def __init__(self, scan_object):\n\n        '''Takes in as input a Scan() object and returns and object\n        with properties for `predict` and `predict_classes`'''\n\n        self.scan_object = scan_object\n        self.data = scan_object.data\n\n    def predict(self, x, metric, asc, model_id=None):\n\n        '''Makes a probability prediction from input x. If model_id\n        is not given, then best_model will be used.\n\n        x | array | data to be used for the predictions\n        model_id | int | the id of the model from the Scan() object\n        metric | str | the metric to be used for picking best model\n        asc | bool | True if `metric` is something to be minimized\n\n        '''\n\n        if model_id is None:\n            from ..utils.best_model import best_model\n            model_id = best_model(self.scan_object, metric, asc)\n\n        from ..utils.best_model import activate_model\n        model = activate_model(self.scan_object, model_id)\n\n        return model.predict(x)\n\n    def predict_classes(self, x, metric, asc, model_id=None):\n\n        '''Makes a class prediction from input x. If model_id\n        is not given, then best_model will be used.\n\n        x | array | data to be used for the predictions\n        model_id | int | the id of the model from the Scan() object\n        metric | str | the metric to be used for picking best model\n        asc | bool | True if `metric` is something to be minimized\n\n        '''\n\n        if model_id is None:\n            from ..utils.best_model import best_model\n            model_id = best_model(self.scan_object, metric, asc)\n\n        from ..utils.best_model import activate_model\n        model = activate_model(self.scan_object, model_id)\n\n        return model.predict_classes(x)\n"""
talos/commands/restore.py,0,"b""class Restore:\n\n    '''Restores the scan_object that had been stored locally as a result\n    of talos.Deploy(scan_object, 'example')\n\n    USE:\n\n    diabetes = ta.Scan(x, y, p, input_model)\n    ta.Deploy(diabetes, 'diabetes')\n    ta.Restore('diabetes.zip')\n\n    '''\n\n    def __init__(self, path_to_zip):\n\n        from zipfile import ZipFile\n\n        import pandas as pd\n        import numpy as np\n\n        # create paths\n        self.path_to_zip = path_to_zip\n        self.extract_to = path_to_zip.replace('.zip', '')\n        self.package_name = self.extract_to.split('/')[-1]\n        self.file_prefix = self.extract_to + '/' + self.package_name\n\n        # extract the zip\n        # unpack_archive(self.path_to_zip, self.extract_to)\n        z = ZipFile(self.path_to_zip, mode='r')\n        z.extractall(self.extract_to)\n\n        # add params dictionary\n        self.params = np.load(self.file_prefix + '_params.npy',\n                              allow_pickle=True).item()\n\n        # add experiment details\n        self.details = pd.read_csv(self.file_prefix + '_details.txt',\n                                   header=None)\n\n        # add x data sample\n        self.x = pd.read_csv(self.file_prefix + '_x.csv', header=None)\n\n        # add y data sample\n        self.y = pd.read_csv(self.file_prefix + '_y.csv', header=None)\n\n        # add model\n        from talos.utils.load_model import load_model\n        self.model = load_model(self.file_prefix + '_model')\n\n        # add results\n        self.results = pd.read_csv(self.file_prefix + '_results.csv')\n        self.results.drop('Unnamed: 0', axis=1, inplace=True)\n\n        # clean up\n        del self.extract_to, self.file_prefix\n        del self.package_name, self.path_to_zip\n"""
talos/logging/__init__.py,0,b''
talos/logging/logging_finish.py,0,b'def logging_finish(self):\n\n    from .results import result_todf\n\n    # save the results\n    self = result_todf(self)\n\n    return self\n'
talos/logging/logging_run.py,0,"b""def logging_run(self, round_start, start, model_history):\n\n    import time\n\n    # count the duration of the round\n    self._round_seconds = time.time() - start\n\n    # set end time and log\n    round_end = time.strftime('%D-%H%M%S')\n    self.round_times.append([round_start, round_end, self._round_seconds])\n\n    # handle first round only things\n    if self.first_round:\n\n        # capture the history keys for later\n        self._all_keys = list(model_history.history.keys())\n        self._metric_keys = [k for k in self._all_keys if 'val_' not in k]\n        self._val_keys = [k for k in self._all_keys if 'val_' in k]\n\n        # create a header column for output\n        _results_header = ['round_epochs'] + self._all_keys + self._param_dict_keys\n        self.result.append(_results_header)\n\n        # save the results\n        from .results import save_result\n        save_result(self)\n\n        # avoid doing this again\n        self.first_round = False\n\n    # create log and other stats\n    from ..metrics.entropy import epoch_entropy\n    self.epoch_entropy.append(epoch_entropy(self, model_history.history))\n\n    # get round results to the results table and save it\n    from .results import run_round_results\n    _round_results = run_round_results(self, model_history)\n\n    self.result.append(_round_results)\n\n    from .results import save_result\n    save_result(self)\n\n    # return the Scan() self\n    return self\n"""
talos/logging/results.py,0,"b""def run_round_results(self, out):\n\n    '''Called from logging/logging_run.py\n\n    THE MAIN FUNCTION FOR CREATING RESULTS FOR EACH ROUNDself.\n    Takes in the history object from model.fit() and handles it.\n\n    NOTE: The epoch level data will be dropped here each round.\n\n    '''\n\n    self._round_epochs = len(list(out.history.values())[0])\n\n    _round_result_out = [self._round_epochs]\n\n    # record the last epoch result\n    for key in out.history.keys():\n        _round_result_out.append(out.history[key][-1])\n\n    # record the round hyper-parameters\n    for key in self.round_params.keys():\n        _round_result_out.append(self.round_params[key])\n\n    return _round_result_out\n\n\ndef save_result(self):\n\n    '''SAVES THE RESULTS/PARAMETERS TO A CSV SPECIFIC TO THE EXPERIMENT'''\n\n    import numpy as np\n\n    np.savetxt(self._experiment_log,\n               self.result,\n               fmt='%s',\n               delimiter=',')\n\n\ndef result_todf(self):\n\n    '''ADDS A DATAFRAME VERSION OF THE RESULTS TO THE CLASS OBJECT'''\n\n    import pandas as pd\n\n    # create dataframe for results\n    cols = self.result[0]\n    self.result = pd.DataFrame(self.result[1:])\n    self.result.columns = cols\n\n    return self\n\n\ndef peak_epochs_todf(self):\n\n    import pandas as pd\n\n    return pd.DataFrame(self.peak_epochs, columns=self.peak_epochs[0]).drop(0)\n"""
talos/metrics/__init__.py,0,b''
talos/metrics/entropy.py,0,"b""def epoch_entropy(self, history):\n\n    '''Called from logging/logging_run.py\n\n    Computes the entropy for epoch metric\n    variation. If validation is on,\n    then returns KL divergence instead of\n    simple Shannon entropy. When Keras\n    validation_freq is on, Shannon entropy\n    is returned. Basically, all experiments\n    should use validation, so Shannon is\n    provided mearly as a fallback.\n\n    '''\n\n    import warnings\n    from scipy.stats import entropy\n\n    warnings.simplefilter('ignore')\n\n    out = []\n\n    # set the default entropy mode to shannon\n    mode = 'shannon'\n\n    # try to make sure each metric has validation\n    if len(self._metric_keys) == len(self._val_keys):\n        # make sure that the length of the arrays are same\n        for i in range(len(self._metric_keys)):\n            if len(history[self._metric_keys[i]]) == len(history[self._val_keys[i]]):\n                mode = 'kl_divergence'\n            else:\n                break\n\n    # handle the case where only shannon entropy can be used\n    if mode == 'shannon':\n        for i in range(len(self._metric_keys)):\n            out.append(entropy(history[self._metric_keys[i]]))\n\n    # handle the case where kl divergence can be used\n    elif mode == 'kl_divergence':\n        for i in range(len(self._metric_keys)):\n            out.append(entropy(history[self._val_keys[i]],\n                               history[self._metric_keys[i]]))\n\n    return out\n"""
talos/metrics/keras_metrics.py,0,"b""def mae(y_true, y_pred):\n    from keras import backend as K\n    return K.mean(K.abs(y_pred - y_true), axis=-1)\n\n\ndef mse(y_true, y_pred):\n    from keras import backend as K\n    return K.mean(K.square(y_pred - y_true), axis=-1)\n\n\ndef rmae(y_true, y_pred):\n    from keras import backend as K\n    return K.sqrt(K.mean(K.abs(y_pred - y_true), axis=-1))\n\n\ndef rmse(y_true, y_pred):\n    from keras import backend as K\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n\n\ndef mape(y_true, y_pred):\n    from keras import backend as K\n    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                            K.epsilon(),\n                                            None))\n    return 100. * K.mean(diff, axis=-1)\n\n\ndef msle(y_true, y_pred):\n    from keras import backend as K\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.mean(K.square(first_log - second_log), axis=-1)\n\n\ndef rmsle(y_true, y_pred):\n    from keras import backend as K\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\n\ndef matthews(y_true, y_pred):\n\n    from keras import backend as K\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator / (denominator + K.epsilon())\n\n\ndef precision(y_true, y_pred):\n\n    from keras import backend as K\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n\ndef recall(y_true, y_pred):\n\n    from keras import backend as K\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n\ndef fbeta(y_true, y_pred, beta=1):\n\n    from keras import backend as K\n    if beta < 0:\n        raise ValueError('The lowest choosable beta is zero (only precision).')\n\n    # If there are no true positives, fix the F score at 0 like sklearn.\n    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n        return 0\n\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    bb = beta ** 2\n    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n    return fbeta_score\n\n\ndef f1score(y_true, y_pred):\n\n    return fbeta(y_true, y_pred, beta=1)\n"""
talos/model/__init__.py,0,b'from .early_stopper import early_stopper\nfrom .hidden_layers import hidden_layers\nfrom .normalizers import lr_normalizer\n'
talos/model/early_stopper.py,0,"b""from keras.callbacks import EarlyStopping\n\n\ndef early_stopper(epochs=None,\n                  monitor='val_loss',\n                  mode='moderate',\n                  min_delta=None,\n                  patience=None):\n\n    '''EARLY STOP CALLBACK\n\n    Helps prevent wasting time when loss is not becoming\n    better. Offers two pre-determined settings 'moderate'\n    and 'strict' and allows input of list with two values:\n\n    `epochs` | int | The number of epochs for the permutation e.g. params['epochs']\n    `monitor` | int | The metric to monitor for change\n    `mode` | str | One of the presets `lazy`, `moderate`, `strict` or `None`\n    `min_delta` | float | The limit for change at which point flag is raised\n    `patience` | str | the number of epochs before termination from flag\n\n    '''\n    if mode == 'lazy':\n        _es_out = EarlyStopping(monitor=monitor,\n                                min_delta=0,\n                                patience=int(epochs / 3),\n                                verbose=0, mode='auto')\n\n    if mode == 'moderate':\n        _es_out = EarlyStopping(monitor=monitor,\n                                min_delta=0,\n                                patience=int(epochs / 10),\n                                verbose=0, mode='auto')\n    elif mode == 'strict':\n        _es_out = EarlyStopping(monitor=monitor,\n                                min_delta=0,\n                                patience=2,\n                                verbose=0, mode='auto')\n    else:\n        _es_out = EarlyStopping(monitor=monitor,\n                                min_delta=mode[0],\n                                patience=mode[1],\n                                verbose=0, mode='auto')\n    return _es_out\n"""
talos/model/hidden_layers.py,0,"b'def hidden_layers(model, params, last_neuron):\n    \'\'\'HIDDEN LAYER Generator\n\n    NOTE: \'shapes\', \'first_neuron\', \'dropout\', and \'hidden_layers\' need\n    to be present in the params dictionary.\n\n    Hidden layer generation for the cases where number\n    of layers is used as a variable in the optimization process.\n    Handles things in a way where any number of layers can be tried\n    with matching hyperparameters.\'\'\'\n\n    # check for the params that are required for hidden_layers\n\n    from keras.layers import Dense, Dropout\n    from .network_shape import network_shape\n    from ..utils.exceptions import TalosParamsError\n\n    required = [\'shapes\', \'first_neuron\', \'dropout\', \'hidden_layers\', \'activation\']\n    for param in required:\n        if param not in params:\n            message = ""hidden_layers requires \'"" + param + ""\' in params""\n            raise TalosParamsError(message)\n\n    layer_neurons = network_shape(params, last_neuron)\n\n    for i in range(params[\'hidden_layers\']):\n        model.add(Dense(\n            layer_neurons[i],\n            kernel_initializer=params.get(\n                \'kernel_initializer\',\n                \'glorot_uniform\'\n            ),\n            kernel_regularizer=params.get(\'kernel_regularizer\'),\n            bias_initializer=params.get(\'bias_initializer\', \'zeros\'),\n            bias_regularizer=params.get(\'bias_regularizer\'),\n            use_bias=params.get(\'use_bias\', True),\n            activity_regularizer=params.get(\'activity_regularizer\'),\n            kernel_constraint=params.get(\'kernel_constraint\'),\n            bias_constraint=params.get(\'bias_constraint\'),\n            activation=params.get(\'activation\')\n        ))\n        model.add(Dropout(params[\'dropout\']))\n'"
talos/model/ingest_model.py,0,"b""def ingest_model(self):\n\n    '''Ingests the model that is input by the user\n    through Scan() model paramater.'''\n\n    return self.model(self.x_train,\n                      self.y_train,\n                      self.x_val,\n                      self.y_val,\n                      self.round_params)\n"""
talos/model/network_shape.py,0,"b'def network_shape(params, last_neuron):\n\n    \'\'\'Provides the ability to include network shape in experiments. If params\n    dictionary for the round contains float value for params[\'shapes\'] then\n    a linear contraction towards the last_neuron value. The higher the value,\n    the fewer layers it takes to reach lesser than last_neuron.\n\n    Supports three inbuilt shapes \'brick\', \'funnel\', and \'triangle\'.\n\n\n    params : dict\n         Scan() params for a single roundself.\n    last_neuron : int\n         Number of neurons on the output layer in the Keras model.\n    \'\'\'\n    import numpy as np\n    from ..utils.exceptions import TalosParamsError\n\n    layers = params[\'hidden_layers\']\n    shape = params[\'shapes\']\n    first_neuron = params[\'first_neuron\']\n    out = []\n    n = first_neuron\n\n    # the case where hidden_layers is zero\n    if layers == 0:\n        return [0]\n\n    # the cases where an angle is applied\n    if isinstance(shape, float):\n\n        for i in range(layers):\n\n            n *= 1 - shape\n\n            if n > last_neuron:\n                out.append(int(n))\n            else:\n                out.append(last_neuron)\n\n    # the case where a rectantular shape is used\n    elif shape == \'brick\':\n        out = [first_neuron] * layers\n\n    elif shape == \'funnel\':\n        for i in range(layers + 1):\n            n -= int((first_neuron - last_neuron) / layers)\n            out.append(n)\n        out.pop(-1)\n\n    elif shape == \'triangle\':\n        out = np.linspace(first_neuron,\n                          last_neuron,\n                          layers+2,\n                          dtype=int).tolist()\n\n        out.pop(0)\n        out.pop(-1)\n        out.reverse()\n\n    else:\n        message = ""\'shapes\' must be float or in [\'funnel\', \'brick\', \'triangle\']""\n        raise TalosParamsError(message)\n\n    return out\n'"
talos/model/normalizers.py,0,"b'def lr_normalizer(lr, optimizer):\n    """"""Assuming a default learning rate 1, rescales the learning rate\n    such that learning rates amongst different optimizers are more or less\n    equivalent.\n\n    Parameters\n    ----------\n    lr : float\n        The learning rate.\n    optimizer : keras optimizer\n        The optimizer. For example, Adagrad, Adam, RMSprop.\n    """"""\n\n    from keras.optimizers import SGD, Adam, Adadelta, Adagrad, Adamax, RMSprop\n    from keras.optimizers import Nadam\n    from talos.utils.exceptions import TalosModelError\n\n    if optimizer == Adadelta:\n        pass\n    elif optimizer == SGD or optimizer == Adagrad:\n        lr /= 100.0\n    elif optimizer == Adam or optimizer == RMSprop:\n        lr /= 1000.0\n    elif optimizer == Adamax or optimizer == Nadam:\n        lr /= 500.0\n    else:\n        raise TalosModelError(str(optimizer) + "" is not supported by lr_normalizer"")\n\n    return lr\n'"
talos/model/output_layer.py,0,"b""def output_layer(task, last_activation, y_train, y_val):\n\n    import numpy as np\n\n    # output layer\n    if task == 'binary':\n        activation = last_activation\n        last_neuron = 1\n\n    elif task == 'multi_class':\n        activation = last_activation\n        last_neuron = len(np.unique(np.hstack((y_train, y_val))))\n\n    elif task == 'multi_label':\n        activation = last_activation\n        last_neuron = y_train.shape[1]\n\n    elif task == 'continuous':\n        activation = None\n        last_neuron = 1\n\n    return activation, last_neuron\n"""
talos/parameters/ParamSpace.py,0,"b'import inspect\n\nimport numpy as np\nimport itertools as it\nfrom datetime import datetime\n\n\nclass ParamSpace:\n\n    def __init__(self,\n                 params,\n                 param_keys,\n                 random_method=\'uniform_mersenne\',\n                 fraction_limit=None,\n                 round_limit=None,\n                 time_limit=None,\n                 boolean_limit=None):\n\n        # set all the arguments\n        self.params = params\n        self.param_keys = param_keys\n        self.fraction_limit = fraction_limit\n        self.round_limit = round_limit\n        self.time_limit = time_limit\n        self.boolean_limit = boolean_limit\n        self.random_method = random_method\n\n        # set a counter\n        self.round_counter = 0\n\n        # handle tuple conversion to discrete values\n        self.p = self._param_input_conversion()\n\n        # create list of list from the params dictionary\n        self._params_temp = [list(self.p[key]) for key in self.param_keys]\n\n        # establish max dimensions\n        self.dimensions = np.prod([len(l) for l in self._params_temp])\n\n        # apply all the set limits\n        self.param_index = self._param_apply_limits()\n\n        # create the parameter space\n        self.param_space = self._param_space_creation()\n\n        # handle the boolean limits separately\n        if self.boolean_limit is not None:\n            index = self._convert_lambda(self.boolean_limit)(self.param_space)\n            self.param_space = self.param_space[index]\n\n        # reset index\n        self.param_index = list(range(len(self.param_index)))\n\n    def _param_input_conversion(self):\n\n        \'\'\'Parameters may be input as lists of single or\n        multiple values (discrete values) or tuples\n        (range of values). This helper checks the format of\n        each input and handles it accordingly.\'\'\'\n\n        out = {}\n\n        # go through each parameter type\n        for param in self.param_keys:\n\n            # deal with range (tuple) values\n            if isinstance(self.params[param], tuple):\n                out[param] = self._param_range_expansion(self.params[param])\n\n            # deal with range (list) values\n            elif isinstance(self.params[param], list):\n                out[param] = self.params[param]\n\n        return out\n\n    def _param_apply_limits(self):\n\n        from talos.reducers.sample_reducer import sample_reducer\n\n        if self.boolean_limit is not None:\n            # NOTE: this is handled in __init__\n            pass\n\n        # a time limit is set\n        if self.time_limit is not None:\n            # NOTE: this is handled in _time_left\n            pass\n\n        # a fractional limit is set\n        if self.fraction_limit is not None:\n            return sample_reducer(self.fraction_limit,\n                                  self.dimensions,\n                                  self.random_method)\n\n        # a round limit is set\n        if self.round_limit is not None:\n            return sample_reducer(self.round_limit,\n                                  self.dimensions,\n                                  self.random_method)\n\n        # no limits are set\n        return list(range(self.dimensions))\n\n    def _param_range_expansion(self, param_values):\n\n        \'\'\'Expands a range (tuple) input into discrete\n        values. Helper for _param_input_conversion.\n        Expects to have a input as (start, end, steps).\n        \'\'\'\n\n        start = param_values[0]\n        end = param_values[1]\n        steps = param_values[2]\n\n        out = np.arange(start, end, (end - start) / steps, dtype=float)\n\n        # inputs are all ints\n        if isinstance(start, int) and isinstance(end, int):\n            out = out.astype(int)\n            out = np.unique(out)\n\n        return out\n\n    def _param_space_creation(self):\n\n        \'\'\'Expand params dictionary to permutations\n\n        Takes the input params dictionary and expands it to\n        actual parameter permutations for the experiment.\n        \'\'\'\n\n        # handle the cases where parameter space is still large\n        if len(self.param_index) > 100000:\n\n            final_grid = list(it.product(*self._params_temp))\n            out = np.array(final_grid, dtype=\'object\')\n\n        # handle the cases where parameter space is already smaller\n        else:\n            final_grid = []\n            for i in self.param_index:\n                p = []\n                for l in reversed(self._params_temp):\n                    i, s = divmod(int(i), len(l))\n                    p.insert(0, l[s])\n                final_grid.append(tuple(p))\n\n            out = np.array(final_grid, dtype=\'object\')\n\n        return out\n\n    def _check_time_limit(self):\n\n        if self.time_limit is None:\n            return True\n\n        stop = datetime.strptime(self.time_limit, ""%Y-%m-%d %H:%M"")\n\n        return stop > datetime.now()\n\n    def round_parameters(self):\n\n        # permutations remain in index\n        if len(self.param_index) > 0:\n\n            # time limit has not been met yet\n            if self._check_time_limit():\n                self.round_counter += 1\n\n                # get current index\n                index = self.param_index.pop(0)\n\n                # get the values based on the index\n                values = self.param_space[index]\n                round_parameters = self._round_parameters_todict(values)\n\n                # pass the parameters to Scan\n                return round_parameters\n\n        # the experiment is finished\n        return False\n\n    def _round_parameters_todict(self, values):\n\n        round_parameters = {}\n\n        for i, key in enumerate(self.param_keys):\n            round_parameters[key] = values[i]\n\n        return round_parameters\n\n    def _convert_lambda(self, fn):\n\n        \'\'\'Converts a lambda function into a format\n        where parameter labels are changed to the column\n        indexes in parameter space.\'\'\'\n\n        # get the source code for the lambda function\n        fn_string = inspect.getsource(fn)\n        fn_string = fn_string.replace(\'""\', \'\\\'\')\n\n        # look for column/label names\n        for i, name in enumerate(self.param_keys):\n            index = \':,\' + str(i)\n            fn_string = fn_string.replace(name, index)\n\n        # cleanup the string\n        fn_string = fn_string.split(\'lambda\')[1]\n        fn_string = fn_string.replace(\'[\\\':\', \'[:\')\n        fn_string = fn_string.replace(\'\\\']\', \']\')\n        fn_string = \'lambda \' + fn_string\n\n        # pass it back as a function\n        return eval(fn_string)\n\n    def remove_is_not(self, label, value):\n\n        \'\'\'Removes baesd on exact match but reversed\'\'\'\n\n        col = self.param_keys.index(label)\n        drop = np.where(self.param_space[:, col] != value)[0].tolist()\n        self.param_index = [x for x in self.param_index if x not in drop]\n\n    def remove_is(self, label, value):\n\n        \'\'\'Removes based on exact match\'\'\'\n\n        col = self.param_keys.index(label)\n        drop = np.where(self.param_space[:, col] == value)[0].tolist()\n        self.param_index = [x for x in self.param_index if x not in drop]\n\n    def remove_ge(self, label, value):\n\n        \'\'\'Removes based on greater-or-equal\'\'\'\n\n        col = self.param_keys.index(label)\n        drop = np.where(self.param_space[:, col] >= value)[0].tolist()\n        self.param_index = [x for x in self.param_index if x not in drop]\n\n    def remove_le(self, label, value):\n\n        \'\'\'Removes based on lesser-or-equal\'\'\'\n\n        col = self.param_keys.index(label)\n        drop = np.where(self.param_space[:, col] <= value)[0].tolist()\n        self.param_index = [x for x in self.param_index if x not in drop]\n\n    def remove_lambda(self, function):\n\n        \'\'\'Removes based on a lambda function\'\'\'\n\n        index = self._convert_lambda(function)(self.param_space)\n        self.param_space = self.param_space[index]\n        self.param_index = list(range(len(self.param_space)))\n'"
talos/parameters/__init__.py,0,b''
talos/reducers/GamifyMap.py,0,"b""class GamifyMap:\n\n    def __init__(self, scan_object):\n\n        '''GamifyMap handles the management of the\n        dictionary that contains the information about\n        hyperparameters, which is exchanged in and out\n        during the `Scan()` experiment.\n        '''\n\n        self.params = scan_object.param_object.params\n        self.scan_object = scan_object\n        self.generate_gamify_dict()\n        self.gamify_map = self.generate_gamify_dict_map()\n\n        # parse together the output file name\n        _folder = './' + scan_object.experiment_name + '/'\n        _id = scan_object._experiment_id\n        self._filename = _folder + _id\n\n    def run_updates(self):\n\n        for key in self.gamify_dict.keys():\n            for val in self.gamify_dict[key].keys():\n                if self.gamify_dict[key][val] != self.updated_dict[key][val]:\n\n                    label = list(self.params.keys())[int(key)]\n                    value = self.params[label][int(val)]\n\n                    self.gamify_dict[key][val] = self.updated_dict[key][val]\n                    self.scan_object.param_object.remove_is(label, value)\n\n        return self.scan_object\n\n    def back_to_original(self, gamify_from_json):\n\n        gamify_dict = {}\n        for key in gamify_from_json:\n            param_vals = {}\n            for val in gamify_from_json[key]:\n                param_vals[self.gamify_map[key][val][1]] = gamify_from_json[key][val][:2]\n            gamify_dict[self.gamify_map[key][val][0]] = param_vals\n\n        return gamify_dict\n\n    def generate_gamify_dict_map(self):\n\n        gamify_dict_map = {}\n\n        for i, key in enumerate(self.params.keys()):\n            param_vals = {}\n            for ii, val in enumerate(self.params[key]):\n                param_vals[str(ii)] = [key, val]\n            gamify_dict_map[str(i)] = param_vals\n\n        return gamify_dict_map\n\n    def generate_gamify_dict(self):\n\n        '''This is done once at the beginning\n        of the experiment.\n\n        NOTE: This will be all stringified, so an index\n        mapping system will be used to convert back to\n        actual forms later.'''\n\n        gamify_dict = {}\n\n        for i, key in enumerate(self.params.keys()):\n            param_vals = {}\n            for ii, val in enumerate(self.params[key]):\n                param_vals[str(ii)] = ['active', 0, str(key), str(val)]\n            gamify_dict[str(i)] = param_vals\n\n        self.gamify_dict = gamify_dict\n\n    def export_json(self):\n\n        import json\n\n        with open(self._filename + '.json', 'w') as fp:\n            json.dump(self.gamify_dict, fp)\n\n    def import_json(self):\n\n        import json\n\n        with open(self._filename + '.json', 'r') as fp:\n            out = json.load(fp)\n\n        self.updated_dict = out\n"""
talos/reducers/__init__.py,0,b''
talos/reducers/correlation.py,0,"b""def correlation(self, method):\n\n    '''This is called from reduce_run.py.\n\n    Performs a spearman rank order correlation\n    based reduction. First looks for a parameter\n    that correlates with reduction_metric and\n    correlation meets reduction_threshold and\n    then converts the match parameter into\n    a 2d multilabel shape. Then new correlation\n    against reduction_metric is performed to identify\n    which particular value is to be dropped.\n\n    '''\n\n    import numpy as np\n\n    # transform the data properly first\n    from .reduce_utils import cols_to_multilabel\n    data = cols_to_multilabel(self)\n\n    # get the correlations\n    corr_values = data.corr(method)[self.reduction_metric]\n\n    # drop the reduction metric row\n    corr_values.drop(self.reduction_metric, inplace=True)\n\n    # drop labels where value is NaN\n    corr_values.dropna(inplace=True)\n\n    # if all nans, then stop\n    if len(corr_values) <= 1:\n        return self\n\n    # sort based on the metric type\n    corr_values.sort_values(ascending=self.minimize_loss, inplace=True)\n\n    # if less than threshold, then stop\n    if abs(corr_values[-1]) < self.reduction_threshold:\n        return self\n\n    # get the strongest correlation\n    corr_values = corr_values.index[-1]\n\n    # get the label, value, and dtype from the column header\n    label, dtype, value = corr_values.split('~')\n\n    # convert things back to their original dtype\n    value = np.array([value]).astype(dtype)[0]\n\n    # this is where we modify the parameter space accordingly\n    self.param_object.remove_is(label, value)\n\n    return self\n"""
talos/reducers/forrest.py,0,"b""def forrest(self):\n\n    '''Random Forrest based reduction strategy. Somewhat more\n    aggressive than for example 'spearman' because there are no\n    negative values, but instead the highest positive correlation\n    is minused from all the values so that max value is 0, and then\n    values are turned into positive. The one with the highest positive\n    score in the end will be dropped. This means that anything with\n    0 originally, is a candidate for dropping. Because there are multiple\n    zeroes in many cases, there is an element of randomness on which one\n    is dropped.\n\n    '''\n\n    import wrangle\n    import numpy as np\n\n    # handle conversion to multi_labels\n    from .reduce_utils import cols_to_multilabel\n    data = cols_to_multilabel(self)\n\n    # get the correlations\n    corr_values = wrangle.df_corr_randomforest(data, self.reduction_metric)\n\n    # drop labels where value is NaN\n    corr_values.dropna(inplace=True)\n\n    # handle the turning around of values (see docstring for more info)\n    corr_values -= corr_values[0]\n    corr_values = corr_values.abs()\n\n    # get the strongest correlation\n    corr_values = corr_values.index[-1]\n\n    # get the label, value, and dtype from the column header\n    label, dtype, value = corr_values.split('~')\n\n    # convert things back to their original dtype\n    value = np.array([value]).astype(dtype)[0]\n\n    # this is where we modify the parameter space accordingly\n    self.param_object.remove_is(label, value)\n\n    return self\n"""
talos/reducers/gamify.py,0,"b""def gamify(self):\n\n    '''Will apply reduction changes based on edits on the\n    the produced .json file in the experiment folder'''\n\n    if self.param_object.round_counter == 1:\n\n        # create the gamify object\n        from .GamifyMap import GamifyMap\n        g = GamifyMap(self)\n\n        # keep in scan_object\n        self._gamify_object = g\n\n        # do the first export in the experiment folder\n        g.export_json()\n\n        return self\n\n    # for every round check if there are changes\n    self._gamify_object.import_json()\n    self = self._gamify_object.run_updates()\n    self._gamify_object.export_json()\n\n    return self\n"""
talos/reducers/limit_by_metric.py,0,"b""def limit_by_metric(self):\n\n    '''Takes as input metric, threshold, and loss and\n    and returs a True if metric threshold have been\n    met and False if not.\n\n    USE: space.check_metric(model_history)\n    '''\n\n    metric = self.performance_target[0]\n    threshold = self.performance_target[1]\n    loss = self.performance_target[2]\n\n    if loss is True:\n        return self.model_history.history[metric][-1] <= threshold\n    elif loss is False:\n        return self.model_history.history[metric][-1] >= threshold\n"""
talos/reducers/local_strategy.py,0,"b'def local_strategy(self):\n\n    try:\n        import importlib\n        importlib.reload(talos_strategy)\n        self = talos_strategy(self)\n    except NameError:\n        try:\n            from talos_strategy import talos_strategy\n            self = talos_strategy(self)\n        except ImportError:\n            print(""No talos_strategy.py found in pwd. Nothing is done."")\n\n    return self\n'"
talos/reducers/reduce_run.py,0,"b'def reduce_run(self):\n\n    \'\'\'The process run script for reduce\n    procedures; takes care of everything\n    related with reduction. When new\n    reduction methods are added, they need\n    to be added as options here.\n\n    To add new reducers, create a file in /reducers\n    which is where this file is located. In that file,\n    take as input self from Scan() and give as output\n    either False, which does nothing, or a tuple of\n    \'value\' and \'label\' where value is a parameter\n    value and label is parameter name. For example\n    batch_size and 128. Then add a reference to\n    reduce_run.py and make sure that you process\n    the self.param_object.param_index there before\n    wrapping up.\n\n    \'\'\'\n\n    from .correlation import correlation\n    from .forrest import forrest\n    from .trees import trees\n    from .gamify import gamify\n\n    from .local_strategy import local_strategy\n    from .limit_by_metric import limit_by_metric\n\n    # check if performance target is met\n    if self.performance_target is not None:\n        status = limit_by_metric(self)\n\n        # handle the case where performance target is met\n        if status == True:\n            self.param_object.param_index = []\n            print(""Target %.3f have been met."" % self.performance_target[1])\n\n    # stop here if no reduction method is set\n    if self.reduction_method is None:\n        return self\n\n    # setup what\'s required for updating progress bar\n    left = (self.param_object.round_counter + 1)\n    right = self.reduction_interval\n    len_before_reduce = len(self.param_object.param_index)\n\n    # check if monte carlo can do something\n    if self.reduction_method == \'gamify\':\n        self = gamify(self)\n\n    # apply window based reducers\n    if left % right == 0:\n\n        # check if correlation reducer can do something\n        if self.reduction_method in [\'pearson\', \'kendall\', \'spearman\']:\n            self = correlation(self, self.reduction_method)\n\n        # check if correlation reducer can do something\n        if self.reduction_method == \'correlation\':\n            self = correlation(self, \'spearman\')\n\n        # check if random forrest can do something\n        if self.reduction_method == \'forrest\':\n            self = forrest(self)\n\n        # check if random forrest can do something\n        if self.reduction_method == \'trees\':\n            self = trees(self)\n\n        if self.reduction_method == \'local_strategy\':\n            self = local_strategy(self)\n\n        # finish up by updating progress bar\n        total_reduced = len_before_reduce - len(self.param_object.param_index)\n        total_reduced = max(0, total_reduced)\n        self.pbar.update(total_reduced)\n\n        if total_reduced > 0:\n            # print out the the status\n            drop_share = total_reduced / len_before_reduce * 100\n            print(""Total %.1f%% permutations reduced"" % drop_share)\n\n    return self\n'"
talos/reducers/reduce_utils.py,0,"b""def cols_to_multilabel(self):\n\n    '''Utility function for correlation and other reducers\n    that require transforming hyperparameter values into\n    multilabel values before applying the reduction strategy.'''\n\n    import wrangle\n    import pandas as pd\n\n    # read in the experiment log\n    data = pd.read_csv(self._experiment_log)\n\n    # apply recuction window\n    data = data.tail(self.reduction_window)\n\n    # drop all other metric columns except reduction_metric\n    data = data[[self.reduction_metric] + self._param_dict_keys]\n\n    # convert all hyperparameter columns to multi label columns\n    for col in data.iloc[:, 1:].columns:\n\n        # get the dtype of the column data\n        col_dtype = data[col].dtype\n\n        # parse column name to contain label, value and dtype\n        data = wrangle.col_to_multilabel(data,\n                                         col,\n                                         extended_colname=True,\n                                         extended_separator='~' + str(col_dtype) + '~')\n\n    return data\n"""
talos/reducers/sample_reducer.py,0,"b'def sample_reducer(limit, max_value, random_method):\n\n    \'\'\'Sample Reducer (Helper)\n\n    NOTE: The Scan() object  is in self.main_self because\n    the object being passed here is ParamGrid() object where\n    the Scan() object is attached as self.main_self.\n\n    Utilize \'grid_downsample\', \'shuffle\', and \'random_method\'\n    to reduce the param_grid before starting the experiment.\n    This is the simplest method in Talos for dealing with curse\n    of dimensionality.\n\n    Options are uniform random, stratified random, latin hypercube\n    sampling, and latin hypercube with sudoku style constraint.\n\n    Returns the reduced param_grid as numpy array.\n\n    \'\'\'\n\n    import chances as ch\n\n    # calculate the size of the downsample\n    if isinstance(limit, float):\n        n = int(max_value * limit)\n    if isinstance(limit, int):\n        n = limit\n\n    max_value = int(max_value)\n\n    # throw an error if\n    from ..utils.exceptions import TalosDataError\n    if n < 1:\n        raise TalosDataError(""Limiters lead to < 1 permutations."")\n\n    # Initialize Randomizer()\n    r = ch.Randomizer(max_value, n)\n\n    # use the user selected method\n    if random_method == \'sobol\':\n        out = r.sobol()\n    elif random_method == \'quantum\':\n        out = r.quantum()\n    elif random_method == \'halton\':\n        out = r.halton()\n    elif random_method == \'korobov_matrix\':\n        out = r.korobov_matrix()\n    elif random_method == \'latin_sudoku\':\n        out = r.latin_sudoku()\n    elif random_method == \'latin_matrix\':\n        out = r.latin_matrix()\n    elif random_method == \'latin_improved\':\n        out = r.latin_improved()\n    elif random_method == \'uniform_mersenne\':\n        out = r.uniform_mersenne()\n    elif random_method == \'uniform_crypto\':\n        out = r.uniform_crypto()\n    elif random_method == \'ambience\':\n        out = r.ambience()\n    else:\n        print(\'No eligble random_method found. Using uniform_mersenne.\')\n        out = r.uniform_mersenne()\n\n    return out\n'"
talos/reducers/trees.py,0,"b""def trees(self, quantile=.8):\n\n    '''Extra Trees based reduction strategy. Like 'forrest', somewhat more\n    aggressive than for example 'spearman' because there are no\n    negative values, but instead the highest positive correlation\n    is minused from all the values so that max value is 0, and then\n    values are turned into positive. The one with the highest positive\n    score in the end will be dropped. This means that anything with\n    0 originally, is a candidate for dropping. Because there are multiple\n    zeroes in many cases, there is an element of randomness on which one\n    is dropped.\n\n    '''\n\n    import wrangle\n    import numpy as np\n\n    # handle conversion to multi_labels\n    from .reduce_utils import cols_to_multilabel\n    data = cols_to_multilabel(self)\n\n    # because extra trees wants label as 'y' we first transform with quantile\n    quantile_value = data[self.reduction_metric].quantile(quantile)\n    data[self.reduction_metric] = data[self.reduction_metric] > quantile_value\n\n    # get the correlations\n    corr_values = wrangle.df_corr_extratrees(data, self.reduction_metric)\n\n    # drop labels where value is NaN\n    corr_values.dropna(inplace=True)\n\n    # handle the turning around of values (see docstring for more info)\n    corr_values -= corr_values[0]\n    corr_values = corr_values.abs()\n\n    # get the strongest correlation\n    corr_values = corr_values.index[-1]\n\n    # get the label, value, and dtype from the column header\n    label, dtype, value = corr_values.split('~')\n\n    # convert things back to their original dtype\n    value = np.array([value]).astype(dtype)[0]\n\n    # this is where we modify the parameter space accordingly\n    self.param_object.remove_is(label, value)\n\n    return self\n"""
talos/scan/Scan.py,0,"b'class Scan:\r\n    """"""Hyperparamater scanning and optimization\r\n\r\n    USE: ta.Scan(x=x, y=y, params=params_dict, model=model)\r\n\r\n    Takes in a Keras model, and a dictionary with the parameter\r\n    boundaries for the experiment.\r\n\r\n        p = {\r\n            \'epochs\' : [50, 100, 200],\r\n            \'activation\' : [\'relu\'],\r\n            \'dropout\': (0, 0.1, 5)\r\n        }\r\n\r\n    Accepted input formats are [1] single value in a list, [0.1, 0.2]\r\n    multiple values in a list, and (0, 0.1, 5) a range of 5 values\r\n    from 0 to 0.1.\r\n\r\n    Here is an example of the input model:\r\n\r\n    def model():\r\n\r\n        # any Keras model\r\n\r\n        return out, model\r\n\r\n\r\n    You must replace the parameters in the model with references to\r\n    the dictionary, for example:\r\n\r\n    model.fit(epochs=params[\'epochs\'])\r\n\r\n    To learn more, start from the examples and documentation\r\n    available here: https://github.com/autonomio/talos\r\n\r\n\r\n    # CORE ARGUMENTS\r\n    ----------------\r\n    x : ndarray\r\n        1d or 2d array, or a list of arrays with features for the prediction\r\n        task.\r\n    y : ndarray\r\n        1d or 2d array, or a list of arrays with labels for the prediction\r\n        task.\r\n    params : dict\r\n        Lists all permutations of hyperparameters, a subset of which will be\r\n        selected at random for training and evaluation.\r\n    model : keras model\r\n        Any Keras model with relevant declrations like params[\'first_neuron\']\r\n    experiment_name : str\r\n        Experiment name will be used to produce a folder (unless already) it\'s\r\n        there from previous iterations of the experiment. Logs of the\r\n        experiment are saved in the folder with timestamp of start\r\n        time as filenames.\r\n    x_val : ndarray\r\n        User specified cross-validation data. (Default is None).\r\n    y_val : ndarray\r\n        User specified cross-validation labels. (Default is None).\r\n    val_split : float, optional\r\n        The proportion of the input `x` which is set aside as the\r\n        validation data. (Default is 0.3).\r\n\r\n    # RANDOMNESS ARGUMENTS\r\n    ----------------------\r\n\r\n    random_method : str\r\n        Determinines the way in which the grid_downsample is applied. The\r\n        default setting is \'uniform_mersenne\'.\r\n    seed : int\r\n        Sets numpy random seed.\r\n\r\n    # LIMITER ARGUMENTS\r\n    -------------------\r\n\r\n    performance_target : None or list [metric, threshold, loss or not]\r\n        Allows setting a threshold for a given metric, at which point the\r\n        experiment will be concluded as successful.\r\n        E.g. performance_target=[\'f1score\', 0.8, False]\r\n    fraction_limit : int\r\n        The fraction of `params` that will be tested (Default is None).\r\n        Previously grid_downsample.\r\n    round_limit : int\r\n        Limits the number of rounds (permutations) in the experiment.\r\n    time_limit : None or str\r\n        Allows setting a time when experiment will be completed. Use the format\r\n        ""%Y-%m-%d %H:%M"" here.\r\n    boolean_limit : None or lambda function\r\n        Allows setting a limit to accepted permutations as a lambda function.\r\n        E.g. example lambda p: p[\'first_neuron\'] * p[\'hidden_layers\'] < 220\r\n\r\n    # OPTIMIZER ARGUMENTS\r\n    ---------------------\r\n    reduction_method : None or string\r\n        If None, random search will be used as the optimization strategy.\r\n        Otherwise use the name of the specific strategy, e.g. \'correlation\'.\r\n    reduction_interval : None or int\r\n        The number of reduction method rounds that will be performed. (Default\r\n        is None).\r\n    reduction_window : None or int\r\n        The number of rounds of the reduction method before observing the\r\n        results. (Default is None).\r\n    reduction_threshold: None or float\r\n        The minimum value for reduction to be applied. For example, when\r\n        the \'correlation\' reducer finds correlation below the threshold,\r\n        nothing is reduced.\r\n    reduction_metric : None or str\r\n        Metric used to tune the reductions. minimize_loss has to be set to True\r\n        if this is a loss.\r\n    minimize_loss : bool\r\n        Must be set to True if a reduction_metric is a loss.\r\n\r\n    # OUTPUT ARGUMENTS\r\n    ------------------\r\n    disable_progress_bar : bool\r\n        Disable TQDM live progress bar.\r\n    print_params : bool\r\n        Print params for each round on screen (useful when using TrainingLog\r\n        callback for visualization)\r\n\r\n    # OTHER ARGUMENTS\r\n    -----------------\r\n    clear_session : bool\r\n        If the backend session is cleared between every permutation.\r\n    save_weights : bool\r\n        If set to False, then model weights will not be saved and best_model\r\n        and some other features will not work. Will reduce memory pressure\r\n        on very large models and high number of rounds/permutations.\r\n    """"""\r\n\r\n    def __init__(self,\r\n                 x,\r\n                 y,\r\n                 params,\r\n                 model,\r\n                 experiment_name,\r\n                 x_val=None,\r\n                 y_val=None,\r\n                 val_split=.3,\r\n                 random_method=\'uniform_mersenne\',\r\n                 seed=None,\r\n                 performance_target=None,\r\n                 fraction_limit=None,\r\n                 round_limit=None,\r\n                 time_limit=None,\r\n                 boolean_limit=None,\r\n                 reduction_method=None,\r\n                 reduction_interval=50,\r\n                 reduction_window=20,\r\n                 reduction_threshold=0.2,\r\n                 reduction_metric=\'val_acc\',\r\n                 minimize_loss=False,\r\n                 disable_progress_bar=False,\r\n                 print_params=False,\r\n                 clear_session=True,\r\n                 save_weights=True):\r\n\r\n        self.x = x\r\n        self.y = y\r\n        self.params = params\r\n        self.model = model\r\n        self.experiment_name = experiment_name\r\n        self.x_val = x_val\r\n        self.y_val = y_val\r\n        self.val_split = val_split\r\n\r\n        # randomness\r\n        self.random_method = random_method\r\n        self.seed = seed\r\n\r\n        # limiters\r\n        self.performance_target = performance_target\r\n        self.fraction_limit = fraction_limit\r\n        self.round_limit = round_limit\r\n        self.time_limit = time_limit\r\n        self.boolean_limit = boolean_limit\r\n\r\n        # optimization\r\n        self.reduction_method = reduction_method\r\n        self.reduction_interval = reduction_interval\r\n        self.reduction_window = reduction_window\r\n        self.reduction_threshold = reduction_threshold\r\n        self.reduction_metric = reduction_metric\r\n        self.minimize_loss = minimize_loss\r\n\r\n        # display\r\n        self.disable_progress_bar = disable_progress_bar\r\n        self.print_params = print_params\r\n\r\n        # performance\r\n        self.clear_session = clear_session\r\n        self.save_weights = save_weights\r\n        # input parameters section ends\r\n\r\n        # start runtime\r\n        from .scan_run import scan_run\r\n        scan_run(self)\r\n'"
talos/scan/__init__.py,0,b''
talos/scan/scan_addon.py,0,"b'def func_best_model(scan_object, metric=\'val_acc\', asc=False):\n\n    \'\'\'Picks the best model based on a given metric and\n    returns the index number for the model.\n\n    NOTE: for loss \'asc\' should be True\'\'\'\n\n    import warnings as warnings\n\n    warnings.simplefilter(\'ignore\')\n\n    from ..utils.best_model import best_model, activate_model\n    model_no = best_model(scan_object, metric, asc)\n    out = activate_model(scan_object, model_no)\n\n    return out\n\n\ndef func_evaluate(scan_object,\n                  x_val,\n                  y_val,\n                  task,\n                  n_models=10,\n                  metric=\'val_acc\',\n                  folds=5,\n                  shuffle=True,\n                  asc=False):\n\n    \'\'\'K-fold Cross Evaluator\n\n    For creating scores from kfold cross-evaluation and\n    adding them to the data frame.\n\n    scan_object : python class\n        The class object returned by Scan() upon completion of the experiment.\n    x_val : array or list of arrays\n        Input data (features) in the same format as used in Scan(), but should\n        not be the same data (or it will not be much of validation).\n    y_val : array or list of arrays\n        Input data (labels) in the same format as used in Scan(), but should\n        not be the same data (or it will not be much of validation).\n    task : string\n        \'binary\', \'multi_class\', \'multi_label\', or \'continuous\'.\n    n_models : int\n        The number of models to be evaluated. If set to 10, then 10 models\n        with the highest metric value are evaluated. See below.\n    metric : str\n        The metric to be used for picking the models to be evaluated.\n    folds : int\n        The number of folds to be used in the evaluation.\n    shuffle : bool\n        If the data is to be shuffled or not. Set always to False for\n        timeseries but keep in mind that you might get periodical/seasonal bias.\n    asc : bool\n        Set to True if the metric is to be minimized.\n\n    \'\'\'\n    import warnings as warnings\n    from tqdm import tqdm\n    import numpy as np\n\n    warnings.simplefilter(\'ignore\')\n\n    picks = scan_object.data.sort_values(metric,\n                                         ascending=asc).index.values[:n_models]\n\n    if n_models > len(scan_object.data):\n        data_len = len(scan_object.data)\n    else:\n        data_len = n_models\n\n    out = []\n\n    pbar = tqdm(total=data_len)\n\n    from ..commands.evaluate import Evaluate\n    for i in range(len(scan_object.data)):\n        if i in list(picks):\n            evaluate_object = Evaluate(scan_object)\n            temp = evaluate_object.evaluate(x_val, y_val,\n                                            task=task,\n                                            model_id=i,\n                                            metric=metric,\n                                            folds=folds,\n                                            shuffle=shuffle,\n                                            asc=asc)\n            out.append([np.mean(temp), np.std(temp)])\n            pbar.update(1)\n        else:\n            out.append([np.nan, np.nan])\n\n    pbar.close()\n\n    if task == \'continuous\':\n        heading = \'eval_\' + \'mae\'\n    else:\n        heading = \'eval_\' + \'f1score\'\n\n    scan_object.data[heading + \'_mean\'] = [i[0] for i in out]\n    scan_object.data[heading + \'_std\'] = [i[1] for i in out]\n\n    print("">> Added evaluation score columns to scan_object.data"")\n'"
talos/scan/scan_finish.py,0,"b""def scan_finish(self):\n\n    attrs_final = ['data', 'x', 'y', 'learning_entropy', 'round_times',\n                   'params', 'saved_models', 'saved_weights', 'round_history']\n\n    attrs_to_keep = attrs_final + ['random_method', 'grid_downsample',\n                                   'reduction_interval', 'reduce_loss',\n                                   'reduction_method', 'reduction_metric',\n                                   'reduction_threshold', 'reduction_window',\n                                   'experiment_name', 'round_history']\n\n    import time\n    import pandas as pd\n\n    # create a dataframe with permutation times\n    self.round_times = pd.DataFrame(self.round_times)\n    self.round_times.columns = ['start', 'end', 'duration']\n\n    # combine epoch entropy tables\n    self.learning_entropy = pd.DataFrame(self.epoch_entropy)\n    self.learning_entropy.columns = self._metric_keys\n\n    # clean the results into a dataframe\n    self.data = self.result\n\n    # remove redundant columns\n    keys = list(self.__dict__.keys())\n    for key in keys:\n        if key not in attrs_to_keep:\n            delattr(self, key)\n\n    # summarize single inputs in dictionary\n    out = {}\n\n    for key in list(self.__dict__.keys()):\n        if key not in attrs_final:\n            out[key] = self.__dict__[key]\n\n    out['complete_time'] = time.strftime('%D/%H:%M')\n    try:\n        out['x_shape'] = self.x.shape\n    # for the case when x is list\n    except AttributeError:\n        out['x_shape'] = 'list'\n\n    try:\n        out['y_shape'] = self.y.shape\n    except AttributeError:\n        out['y_shape'] = 'list'\n\n    # final cleanup\n    keys = list(self.__dict__.keys())\n    for key in keys:\n        if key not in attrs_final:\n            delattr(self, key)\n\n    # add details dictionary as series\n    self.details = pd.Series(out)\n\n    # add best_model\n\n    from ..scan.scan_addon import func_best_model, func_evaluate\n\n    self.best_model = func_best_model.__get__(self)\n    self.evaluate_models = func_evaluate.__get__(self)\n\n    # reset the index\n    self.data.index = range(len(self.data))\n\n    return self\n"""
talos/scan/scan_prepare.py,0,"b'def scan_prepare(self):\n\n    \'\'\'Includes all preparation procedures up until starting the first scan\n    through scan_run()\'\'\'\n\n    from .scan_utils import initialize_log\n\n    self._experiment_log = initialize_log(self)\n\n    # for the case where x_val or y_val is missing when other is present\n    self.custom_val_split = False\n    if (self.x_val is not None and self.y_val is None) or \\\n       (self.x_val is None and self.y_val is not None):\n        raise RuntimeError(""If x_val/y_val is inputted, other must as well."")\n\n    elif self.x_val is not None and self.y_val is not None:\n        self.custom_val_split = True\n\n    # create reference for parameter keys\n    self._param_dict_keys = sorted(list(self.params.keys()))\n\n    # create the parameter object and move to self\n    from ..parameters.ParamSpace import ParamSpace\n    self.param_object = ParamSpace(params=self.params,\n                                   param_keys=self._param_dict_keys,\n                                   random_method=self.random_method,\n                                   fraction_limit=self.fraction_limit,\n                                   round_limit=self.round_limit,\n                                   time_limit=self.time_limit,\n                                   boolean_limit=self.boolean_limit\n                                   )\n\n    # mark that it\'s a first round\n    self.first_round = True\n\n    # create various stores\n    self.round_history = []\n    self.peak_epochs = []\n    self.epoch_entropy = []\n    self.round_times = []\n    self.result = []\n    self.saved_models = []\n    self.saved_weights = []\n\n    # handle validation split\n    from ..utils.validation_split import validation_split\n    self = validation_split(self)\n\n    # set data and len\n    self._data_len = len(self.x)\n\n    return self\n'"
talos/scan/scan_round.py,0,"b'def scan_round(self):\n\n    \'\'\'The main operational function that manages the experiment\n    on the level of execution of each round.\'\'\'\n\n    import time\n    import gc\n\n    # print round params\n    if self.print_params is True:\n        print(self.round_params)\n\n    # set start time\n    round_start = time.strftime(\'%D-%H%M%S\')\n    start = time.time()\n\n    # fit the model\n    from ..model.ingest_model import ingest_model\n    self.model_history, self.round_model = ingest_model(self)\n    self.round_history.append(self.model_history.history)\n\n    # handle logging of results\n    from ..logging.logging_run import logging_run\n    self = logging_run(self, round_start, start, self.model_history)\n\n    # apply reductions\n    from ..reducers.reduce_run import reduce_run\n    self = reduce_run(self)\n\n    try:\n        # save model and weights\n        self.saved_models.append(self.round_model.to_json())\n\n        if self.save_weights:\n            self.saved_weights.append(self.round_model.get_weights())\n        else:\n            self.saved_weights.append(None)\n\n    except AttributeError as e:\n        # make sure that the error message is from torch\n        if str(e) == ""\'Model\' object has no attribute \'to_json\'"":\n            if self.save_weights:\n                self.saved_models.append(self.round_model.state_dict())\n            else:\n                self.saved_weights.append(None)\n\n    # clear tensorflow sessions\n    if self.clear_session is True:\n\n        del self.round_model\n        gc.collect()\n\n        # try TF specific and pass for everyone else\n        try:\n            from keras import backend as K\n            K.clear_session()\n        except ImportError:\n            pass\n\n    return self\n'"
talos/scan/scan_run.py,0,"b""def scan_run(self):\n\n    '''The high-level management of the scan procedures\n    onwards from preparation. Manages round_run()'''\n\n    from tqdm import tqdm\n\n    from .scan_prepare import scan_prepare\n    self = scan_prepare(self)\n\n    # initiate the progress bar\n    self.pbar = tqdm(total=len(self.param_object.param_index),\n                     disable=self.disable_progress_bar)\n\n    # the main cycle of the experiment\n    while True:\n\n        # get the parameters\n        self.round_params = self.param_object.round_parameters()\n\n        # break when there is no more permutations left\n        if self.round_params is False:\n            break\n        # otherwise proceed with next permutation\n        from .scan_round import scan_round\n        self = scan_round(self)\n        self.pbar.update(1)\n\n    # close progress bar before finishing\n    self.pbar.close()\n\n    # finish\n    from ..logging.logging_finish import logging_finish\n    self = logging_finish(self)\n\n    from .scan_finish import scan_finish\n    self = scan_finish(self)\n"""
talos/scan/scan_utils.py,0,"b""def initialize_log(self):\n\n    import time\n    import os\n\n    # create the experiment folder (unless one is already there)\n    try:\n        path = os.getcwd()\n        os.mkdir(path + '/' + self.experiment_name)\n    except FileExistsError:\n        pass\n\n    self._experiment_id = time.strftime('%D%H%M%S').replace('/', '')\n    _file_name = self._experiment_id + '.csv'\n    _experiment_log = './' + self.experiment_name + '/' + _file_name\n\n    f = open(_experiment_log, 'w')\n    f.write('')\n    f.close()\n\n    return _experiment_log\n"""
talos/templates/__init__.py,0,b'from . import datasets\nfrom . import models\nfrom . import params\nfrom . import pipelines\n'
talos/templates/datasets.py,0,"b'def telco_churn(quantile=.5):\r\n\r\n    \'\'\'Returns dataset in format x, [y1, y2]. This dataset\r\n    is useful for demonstrating multi-output model or for\r\n    experimenting with reduction strategy creation.\r\n\r\n    The data is from hyperparameter optimization experiment with\r\n    Kaggle telco churn dataset.\r\n\r\n    x: features\r\n    y1: val_loss\r\n    y2: val_f1score\r\n\r\n    quantile is for transforming the otherwise continuous y variables into\r\n    labels so that higher value is stronger. If set to 0 then original\r\n    continuous will be returned.\'\'\'\r\n\r\n    import wrangle\r\n    import pandas as pd\r\n\r\n    df = pd.read_csv(\'https://raw.githubusercontent.com/autonomio/examples/master/telco_churn/telco_churn_for_sensitivity.csv\')\r\n\r\n    df = df.drop([\'val_acc\', \'loss\', \'f1score\', \'acc\', \'round_epochs\'], 1)\r\n\r\n    for col in df.iloc[:, 2:].columns:\r\n        df = wrangle.col_to_multilabel(df, col)\r\n\r\n    df = wrangle.df_rename_cols(df)\r\n\r\n    if quantile > 0:\r\n        y1 = (df.C0 < df.C0.quantile(quantile)).astype(int).values\r\n        y2 = (df.C1 > df.C1.quantile(quantile)).astype(int).values\r\n    else:\r\n        y1 = df.C0.values\r\n        y2 = df.C1.values\r\n\r\n    x = df.drop([\'C0\', \'C1\'], 1).values\r\n\r\n    return x, [y1, y2]\r\n\r\n\r\ndef icu_mortality(samples=None):\r\n\r\n    import pandas as pd\r\n    base = \'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/\'\r\n    df = pd.read_csv(base + \'icu_mortality.csv\')\r\n    df = df.dropna(thresh=3580, axis=1)\r\n    df = df.dropna()\r\n    df = df.sample(frac=1).head(samples)\r\n    y = df[\'hospitalmortality\'].astype(int).values\r\n    x = df.drop(\'hospitalmortality\', axis=1).values\r\n\r\n    return x, y\r\n\r\n\r\ndef titanic():\r\n\r\n    import pandas as pd\r\n    base = \'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/\'\r\n    df = pd.read_csv(base + \'titanic.csv\')\r\n\r\n    y = df.survived.values\r\n\r\n    x = df[[\'age\', \'sibsp\', \'parch\']]\r\n    cols = [\'class\', \'embark_town\', \'who\', \'deck\', \'sex\']\r\n\r\n    for col in cols:\r\n        x = pd.merge(x,\r\n                     pd.get_dummies(df[col]),\r\n                     left_index=True,\r\n                     right_index=True)\r\n\r\n    x = x.values\r\n\r\n    print(\'BE CAREFUL, this dataset has nan values.\')\r\n\r\n    return x, y\r\n\r\n\r\ndef iris():\r\n\r\n    import pandas as pd\r\n    from keras.utils import to_categorical\r\n    base = \'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/\'\r\n    df = pd.read_csv(base + \'iris.csv\')\r\n    df[\'species\'] = df[\'species\'].factorize()[0]\r\n    df = df.sample(len(df))\r\n    y = to_categorical(df[\'species\'])\r\n    x = df.iloc[:, :-1].values\r\n\r\n    y = to_categorical(df[\'species\'])\r\n    x = df.iloc[:, :-1].values\r\n\r\n    return x, y\r\n\r\n\r\ndef cervical_cancer():\r\n\r\n    import pandas as pd\r\n    from numpy import nan\r\n    base = \'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/\'\r\n    df = pd.read_csv(base + \'cervical_cancer.csv\')\r\n    df = df.replace(\'?\', nan)\r\n    df = df.drop([\'citology\', \'hinselmann\', \'biopsy\'], axis=1)\r\n    df = df.drop([\'since_first_diagnosis\',\r\n                  \'since_last_diagnosis\'], axis=1).dropna()\r\n\r\n    df = df.astype(float)\r\n\r\n    y = df.schiller.values\r\n    x = df.drop(\'schiller\', axis=1).values\r\n\r\n    return x, y\r\n\r\n\r\ndef breast_cancer():\r\n\r\n    import pandas as pd\r\n    base = \'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/\'\r\n    df = pd.read_csv(base + \'breast_cancer.csv\')\r\n\r\n    # then some minimal data cleanup\r\n    df.drop(""Unnamed: 32"", axis=1, inplace=True)\r\n    df.drop(""id"", axis=1, inplace=True)\r\n\r\n    # separate to x and y\r\n    y = df.diagnosis.values\r\n    x = df.drop(\'diagnosis\', axis=1).values\r\n\r\n    # convert the string labels to binary\r\n    y = (y == \'M\').astype(int)\r\n\r\n    return x, y\r\n\r\n\r\ndef mnist():\r\n\r\n    \'\'\'Note that this dataset, unlike other Talos datasets,returns:\r\n\r\n    x_train, y_train, x_val, y_val\'\'\'\r\n\r\n    import keras\r\n    import numpy as np\r\n\r\n    # the data, split between train and test sets\r\n    (x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\r\n\r\n    # input image dimensions\r\n    img_rows, img_cols = 28, 28\r\n\r\n    if keras.backend.image_data_format() == \'channels_first\':\r\n\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\r\n        input_shape = (1, img_rows, img_cols)\r\n\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\r\n        input_shape = (img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype(\'float32\')\r\n    x_val = x_val.astype(\'float32\')\r\n    x_train /= 255\r\n    x_val /= 255\r\n\r\n    classes = len(np.unique(y_train))\r\n\r\n    # convert class vectors to binary class matrices\r\n    y_train = keras.utils.to_categorical(y_train, classes)\r\n    y_val = keras.utils.to_categorical(y_val, classes)\r\n\r\n    print(""Use input_shape %s"" % str(input_shape))\r\n\r\n    return x_train, y_train, x_val, y_val\r\n'"
talos/templates/models.py,0,"b""def breast_cancer(x_train, y_train, x_val, y_val, params):\n\n    from keras.models import Sequential\n    from keras.layers import Dropout, Dense\n    from talos.model import lr_normalizer, early_stopper, hidden_layers\n\n    from talos.metrics.keras_metrics import matthews, precision, recall, f1score\n\n    model = Sequential()\n    model.add(Dense(params['first_neuron'],\n                    input_dim=x_train.shape[1],\n                    activation='relu'))\n\n    model.add(Dropout(params['dropout']))\n\n    hidden_layers(model, params, 1)\n\n    model.add(Dense(1, activation=params['last_activation']))\n\n    model.compile(optimizer=params['optimizer']\n                  (lr=lr_normalizer(params['lr'],\n                                    params['optimizer'])),\n                  loss=params['losses'],\n                  metrics=['acc',\n                           f1score,\n                           recall,\n                           precision,\n                           matthews])\n\n    results = model.fit(x_train, y_train,\n                        batch_size=params['batch_size'],\n                        epochs=params['epochs'],\n                        verbose=0,\n                        validation_data=[x_val, y_val],\n                        callbacks=[early_stopper(params['epochs'],\n                                                 mode='moderate',\n                                                 monitor='val_f1score')])\n\n    return results, model\n\n\ndef cervical_cancer(x_train, y_train, x_val, y_val, params):\n\n    from keras.models import Sequential\n    from keras.layers import Dropout, Dense\n    from talos.model import lr_normalizer, early_stopper, hidden_layers\n\n    from talos.metrics.keras_metrics import matthews, precision, recall, f1score\n\n    model = Sequential()\n    model.add(Dense(params['first_neuron'],\n                    input_dim=x_train.shape[1],\n                    activation='relu'))\n\n    model.add(Dropout(params['dropout']))\n\n    hidden_layers(model, params, 1)\n\n    model.add(Dense(1, activation=params['last_activation']))\n\n    model.compile(optimizer=params['optimizer']\n                  (lr=lr_normalizer(params['lr'],\n                                    params['optimizer'])),\n                  loss=params['losses'],\n                  metrics=['acc',\n                           f1score,\n                           recall,\n                           precision,\n                           matthews])\n\n    results = model.fit(x_train, y_train,\n                        batch_size=params['batch_size'],\n                        epochs=params['epochs'],\n                        verbose=0,\n                        validation_data=[x_val, y_val],\n                        callbacks=[early_stopper(params['epochs'],\n                                                 mode='moderate',\n                                                 monitor='val_f1score')])\n\n    return results, model\n\n\ndef titanic(x_train, y_train, x_val, y_val, params):\n\n    from keras.models import Sequential\n    from keras.layers import Dropout, Dense\n\n    # note how instead of passing the value, we pass a dictionary entry\n    model = Sequential()\n    model.add(Dense(params['first_neuron'],\n                    input_dim=x_train.shape[1],\n                    activation='relu'))\n\n    # same here, just passing a dictionary entry\n    model.add(Dropout(params['dropout']))\n\n    # again, instead of the activation name, we have a dictionary entry\n    model.add(Dense(1, activation=params['last_activation']))\n\n    # here are using a learning rate boundary\n    model.compile(optimizer=params['optimizer'],\n                  loss=params['losses'],\n                  metrics=['acc'])\n\n    # here we are also using the early_stopper function for a callback\n    out = model.fit(x_train, y_train,\n                    batch_size=params['batch_size'],\n                    epochs=2,\n                    verbose=0,\n                    validation_data=[x_val, y_val])\n\n    return out, model\n\n\ndef iris(x_train, y_train, x_val, y_val, params):\n\n    from keras.models import Sequential\n    from keras.layers import Dropout, Dense\n    from talos.model import lr_normalizer, early_stopper, hidden_layers\n\n    # note how instead of passing the value, we pass a dictionary entry\n    model = Sequential()\n    model.add(Dense(params['first_neuron'],\n                    input_dim=x_train.shape[1],\n                    activation='relu'))\n\n    # same here, just passing a dictionary entry\n    model.add(Dropout(params['dropout']))\n\n    # with this call we can create any number of hidden layers\n    hidden_layers(model, params, y_train.shape[1])\n\n    # again, instead of the activation name, we have a dictionary entry\n    model.add(Dense(y_train.shape[1],\n                    activation=params['last_activation']))\n\n    # here are using a learning rate boundary\n    model.compile(optimizer=params['optimizer']\n                  (lr=lr_normalizer(params['lr'],\n                                    params['optimizer'])),\n                  loss=params['losses'],\n                  metrics=['acc'])\n\n    # here we are also using the early_stopper function for a callback\n    out = model.fit(x_train, y_train,\n                    batch_size=params['batch_size'],\n                    epochs=params['epochs'],\n                    verbose=0,\n                    validation_data=[x_val, y_val],\n                    callbacks=[early_stopper(params['epochs'], mode=[1, 1])])\n\n    return out, model\n"""
talos/templates/params.py,0,"b""def titanic():\n\n    # here use a standard 2d dictionary for inputting the param boundaries\n    p = {'lr': (0.5, 5, 10),\n         'first_neuron': [4, 8, 16],\n         'batch_size': [20, 30, 40],\n         'dropout': (0, 0.5, 5),\n         'optimizer': ['Adam', 'Nadam'],\n         'losses': ['logcosh', 'binary_crossentropy'],\n         'activation': ['relu', 'elu'],\n         'last_activation': ['sigmoid']}\n\n    return p\n\n\ndef iris():\n\n    from keras.optimizers import Adam, Nadam\n    from keras.losses import logcosh, categorical_crossentropy\n    from keras.activations import relu, elu, softmax\n\n    # here use a standard 2d dictionary for inputting the param boundaries\n    p = {'lr': (0.5, 5, 10),\n         'first_neuron': [4, 8, 16, 32, 64],\n         'hidden_layers': [0, 1, 2, 3, 4],\n         'batch_size': (2, 30, 10),\n         'epochs': [2],\n         'dropout': (0, 0.5, 5),\n         'weight_regulizer': [None],\n         'emb_output_dims':  [None],\n         'shapes': ['brick', 'triangle', 0.2],\n         'optimizer': [Adam, Nadam],\n         'losses': [logcosh, categorical_crossentropy],\n         'activation': [relu, elu],\n         'last_activation': [softmax]}\n\n    return p\n\n\ndef breast_cancer():\n\n    from keras.optimizers import Adam, Nadam, RMSprop\n    from keras.losses import logcosh, binary_crossentropy\n    from keras.activations import relu, elu, sigmoid\n\n    # then we can go ahead and set the parameter space\n    p = {'lr': (0.5, 5, 10),\n         'first_neuron': [4, 8, 16, 32, 64],\n         'hidden_layers': [0, 1, 2],\n         'batch_size': (2, 30, 10),\n         'epochs': [50, 100, 150],\n         'dropout': (0, 0.5, 5),\n         'shapes': ['brick', 'triangle', 'funnel'],\n         'optimizer': [Adam, Nadam, RMSprop],\n         'losses': [logcosh, binary_crossentropy],\n         'activation': [relu, elu],\n         'last_activation': [sigmoid]}\n\n    return p\n\n\ndef cervical_cancer():\n    return breast_cancer()\n"""
talos/templates/pipelines.py,0,"b""def breast_cancer(round_limit=2, random_method='uniform_mersenne'):\n\n    '''Performs a Scan with Iris dataset and simple dense net'''\n    import talos as ta\n    scan_object = ta.Scan(ta.templates.datasets.breast_cancer()[0],\n                          ta.templates.datasets.breast_cancer()[1],\n                          ta.templates.params.breast_cancer(),\n                          ta.templates.models.breast_cancer,\n                          'test',\n                          round_limit=round_limit)\n\n    return scan_object\n\n\ndef cervical_cancer(round_limit=2, random_method='uniform_mersenne'):\n\n    '''Performs a Scan with Iris dataset and simple dense net'''\n    import talos as ta\n    scan_object = ta.Scan(ta.templates.datasets.cervical_cancer()[0],\n                          ta.templates.datasets.cervical_cancer()[1],\n                          ta.templates.params.cervical_cancer(),\n                          ta.templates.models.cervical_cancer,\n                          'test',\n                          round_limit=round_limit)\n\n    return scan_object\n\n\ndef iris(round_limit=2, random_method='uniform_mersenne'):\n\n    '''Performs a Scan with Iris dataset and simple dense net'''\n    import talos as ta\n    scan_object = ta.Scan(ta.templates.datasets.iris()[0],\n                          ta.templates.datasets.iris()[1],\n                          ta.templates.params.iris(),\n                          ta.templates.models.iris,\n                          'test',\n                          round_limit=round_limit)\n\n    return scan_object\n\n\ndef titanic(round_limit=2, random_method='uniform_mersenne'):\n\n    '''Performs a Scan with Iris dataset and simple dense net'''\n    import talos as ta\n    scan_object = ta.Scan(ta.templates.datasets.titanic()[0][:50],\n                          ta.templates.datasets.titanic()[1][:50],\n                          ta.templates.params.titanic(),\n                          ta.templates.models.titanic,\n                          'test',\n                          random_method=random_method,\n                          round_limit=round_limit)\n\n    return scan_object\n"""
talos/utils/__init__.py,0,"b'# In this init we load everything under utils in the Talos namespace\n\nfrom kerasplotlib import TrainingLog as live\n\nfrom ..model.normalizers import lr_normalizer\nfrom ..model.hidden_layers import hidden_layers\nfrom ..model.early_stopper import early_stopper\nfrom .generator import generator\nfrom . import gpu_utils\nimport talos.metrics.keras_metrics as metrics\nfrom .sequence_generator import SequenceGenerator\nfrom .rescale_meanzero import rescale_meanzero\nfrom .experiment_log_callback import ExperimentLogCallback\nfrom .torch_history import TorchHistory\nfrom wrangle import array_split as val_split\nfrom .power_draw_callback import PowerDrawCallback\nfrom .power_draw_append import power_draw_append\nfrom .recover_best_model import recover_best_model\n\ndel experiment_log_callback, sequence_generator\n'"
talos/utils/best_model.py,0,"b""from keras.models import model_from_json\n\n\ndef best_model(self, metric, asc):\n\n    '''Picks the best model based on a given metric and\n    returns the index number for the model.\n\n    NOTE: for loss 'asc' should be True'''\n\n    best = self.data.sort_values(metric, ascending=asc).iloc[0].name\n\n    return best\n\n\ndef activate_model(self, model_id):\n\n    '''Loads the model from the json that is stored in the Scan object'''\n\n    model = model_from_json(self.saved_models[model_id])\n    model.set_weights(self.saved_weights[model_id])\n\n    return model\n"""
talos/utils/exceptions.py,0,b'class TalosReturnError(Exception):\n    pass\n\n\nclass TalosParamsError(Exception):\n    pass\n\n\nclass TalosTypeError(Exception):\n    pass\n\n\nclass TalosModelError(Exception):\n    pass\n\n\nclass TalosDataError(Exception):\n    pass\n'
talos/utils/experiment_log_callback.py,0,"b'from keras.callbacks import Callback\n\n\nclass ExperimentLogCallback(Callback):\n\n    def __init__(self,\n                 experiment_name,\n                 params):\n\n        \'\'\'Takes as input the name of the experiment which will be\n        used for creating a .log file with the outputs and the params\n        dictionary from the input model in `Scan()`\n\n        experiment_name | str | must match the experiment_name in `Scan()`\n\n        \'\'\'\n\n        super(ExperimentLogCallback, self).__init__()\n\n        import glob\n        import os\n\n        # get the experiment id first\n        list_of_files = glob.glob(\'./\' + experiment_name + \'/*.csv\')\n        try:\n            latest_file = max(list_of_files, key=os.path.getmtime)\n        except ValueError:\n            print(""`experiment_name` has to match `Scan(experiment_name)`"")\n\n        self.name = latest_file.replace(\'.csv\', \'\') + \'.log\'\n\n        # rest of the config variables\n        self.params = params\n        self.counter = 1\n        self.new_file = True\n\n    def on_train_begin(self, logs={}):\n\n        import random\n        self.hash = hex(abs(hash(str(random.random()))))\n        self.final_out = []\n\n    def on_train_end(self, logs={}):\n\n        f = open(self.name, \'a+\')\n        [f.write(\',\'.join(map(str, i)) + \'\\n\') for i in self.final_out]\n        f.close()\n\n    def on_epoch_begin(self, epoch, logs={}):\n\n        self.epoch_out = []\n\n    def on_epoch_end(self, epoch, logs={}):\n\n        if len(self.final_out) == 0:\n\n            try:\n                open(self.name, \'r\')\n            except FileNotFoundError:\n\n                self.epoch_out.append(\'id\')\n                self.epoch_out.append(\'epoch\')\n\n                for key in logs.keys():\n\n                    # add to the epoch out list\n                    self.epoch_out.append(key)\n\n                self.final_out.append(self.epoch_out)\n                self.epoch_out = []\n\n        self.epoch_out.append(self.hash)\n        self.epoch_out.append(epoch + 1)\n\n        for key in logs.keys():\n\n            # round the values\n            rounded = round(logs[key], 4)\n\n            # add to the epoch out list\n            self.epoch_out.append(rounded)\n\n        # add to the final out list\n        self.final_out.append(self.epoch_out)\n'"
talos/utils/generator.py,0,"b""def generator(x, y, batch_size):\n\n    '''Creates a data generator for Keras fit_generator(). '''\n\n    import numpy as np\n\n    samples_per_epoch = x.shape[0]\n    number_of_batches = samples_per_epoch / batch_size\n    counter = 0\n\n    while 1:\n\n        x_batch = np.array(x[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n        y_batch = np.array(y[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n        counter += 1\n\n        yield x_batch, y_batch\n\n        if counter >= number_of_batches:\n            counter = 0\n"""
talos/utils/gpu_utils.py,5,"b""def parallel_gpu_jobs(allow_growth=True, fraction=.5):\n\n    '''Sets the max used memory as a fraction for tensorflow\n    backend\n\n    allow_growth :: True of False\n\n    fraction :: a float value (e.g. 0.5 means 4gb out of 8gb)\n\n    '''\n\n    import keras.backend as K\n    import tensorflow as tf\n\n    gpu_options = tf.GPUOptions(allow_growth=allow_growth,\n                                  per_process_gpu_memory_fraction=fraction)\n    config = tf.ConfigProto(gpu_options=gpu_options)\n    session = tf.Session(config=config)\n    K.set_session(session)\n\n\ndef multi_gpu(model, gpus=None, cpu_merge=True, cpu_relocation=False):\n\n    '''Takes as input the model, and returns a model\n    based on the number of GPUs available on the machine\n    or alternatively the 'gpus' user input.\n\n    NOTE: this needs to be used before model.compile() in the\n    model inputted to Scan in the form:\n\n    from talos.utils.gpu_utils import multi_gpu\n    model = multi_gpu(model)\n\n    '''\n\n    from keras.utils import multi_gpu_model\n\n    return multi_gpu_model(model,\n                           gpus=gpus,\n                           cpu_merge=cpu_merge,\n                           cpu_relocation=cpu_relocation)\n\n\ndef force_cpu():\n\n    '''Force CPU on a GPU system\n    '''\n\n    import keras.backend as K\n    import tensorflow as tf\n\n    config = tf.ConfigProto(device_count={'GPU': 0})\n    session = tf.Session(config=config)\n    K.set_session(session)\n"""
talos/utils/load_model.py,0,"b'from keras.models import model_from_json\n\n\ndef load_model(saved_model):\n\n    \'\'\'Load a Model from local disk\n\n    Takes as input .json and .h5 file with model\n    and weights and returns a model that can be then\n    used for predictions.\n\n    saved_model :: name of the saved model without\n    suffix (e.g. \'iris_model\' and not \'iris_model.json\')\n\n    \'\'\'\n\n    json_file = open(saved_model + "".json"", \'r\')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    model = model_from_json(loaded_model_json)\n    model.load_weights(saved_model + \'.h5\')\n\n    return model\n'"
talos/utils/power_draw_append.py,0,"b""def power_draw_append(history, power_draw):\n\n    '''For appending the data from PowerDrawCallback to the history object\n    and allowing the data to be captured in the experiment log in Talos.'''\n\n    import numpy as np\n\n    joined = power_draw.log['epoch_begin'] + power_draw.log['epoch_end']\n    avg_watts = (np.array(joined)) / 2\n\n    history.history['watts_min'] = [min(joined)]\n    history.history['watts_max'] = [max(joined)]\n    history.history['seconds'] = [sum(power_draw.log['seconds'])]\n\n    watt_seconds = round(sum(avg_watts * np.array(power_draw.log['seconds'])), 2)\n    history.history['Ws'] = [watt_seconds]\n\n    return history\n"""
talos/utils/power_draw_callback.py,0,"b'from keras.callbacks import Callback\n\n\nclass PowerDrawCallback(Callback):\n\n    \'\'\'A callback for recording GPU power draw (watts) on epoch begin and end.\n\n    Example use:\n\n    power_draw = PowerDrawCallback()\n\n    model.fit(...callbacks=[power_draw]...)\n\n    print(power_draw.logs)\n\n    \'\'\'\n\n    def __init__(self):\n\n        super(PowerDrawCallback, self).__init__()\n\n        import os\n        import time\n\n        self.os = os\n        self.time = time.time\n        self.command = ""nvidia-smi -i 0 -q | grep -i \'power draw\' | tr -s \' \' | cut -d \' \' -f5""\n\n    def on_train_begin(self, logs={}):\n        self.log = {}\n        self.log[\'epoch_begin\'] = []\n        self.log[\'epoch_end\'] = []\n        self.log[\'seconds\'] = []\n\n    def on_epoch_begin(self, batch, logs=None):\n        self.epoch_start_time = self.time()\n        temp = self.os.popen(self.command).read()\n        temp = float(temp.strip())\n        self.log[\'epoch_begin\'].append(temp)\n\n    def on_epoch_end(self, batch, logs=None):\n        temp = self.os.popen(self.command).read()\n        temp = float(temp.strip())\n        self.log[\'epoch_end\'].append(temp)\n        seconds = round(self.time() - self.epoch_start_time, 3)\n        self.log[\'seconds\'].append(seconds)\n'"
talos/utils/recover_best_model.py,0,"b""\n\ndef recover_best_model(x_train,\n                       y_train,\n                       x_val,\n                       y_val,\n                       experiment_log,\n                       input_model,\n                       x_cross=None,\n                       y_cross=None,\n                       n_models=5,\n                       task='multi_label'):\n\n    '''Recover best models from Talos experiment log.\n\n    x_train | array | same as was used in the experiment\n    y_train | array | same as was used in the experiment\n    x_val | array | same as was used in the experiment\n    y_val | array | same as was used in the experiment\n    x_cross | array | data for the cross-validation or None for use x_val\n    y_cross | array | data for the cross-validation or None for use y_val\n    experiment_log | str | path to the Talos experiment log\n    input_model | function | model used in the experiment\n    n_models | int | number of models to cross-validate\n    task | str | binary, multi_class, multi_label or continuous\n\n    Returns a pandas dataframe with the cross-validation results\n    and the models.\n\n    '''\n\n    import pandas as pd\n    import sklearn as sk\n    import numpy as np\n\n    from talos.utils.validation_split import kfold\n\n    # read the experiment log into a dataframe\n    df = pd.read_csv(experiment_log)\n\n    # handle input data scenarios\n\n    if x_cross is None or y_cross is None:\n        x_cross = x_val\n        y_cross = y_val\n\n    # for final output\n    results = []\n    models = []\n\n    for i in range(n_models):\n\n        # get the params for the model and train it\n        params = df.sort_values('val_acc', ascending=False).drop('val_acc', 1).iloc[i].to_dict()\n        history, model = input_model(x_train, y_train, x_val, y_val, params)\n\n        # start kfold cross-validation\n        out = []\n        folds = 5\n        kx, ky = kfold(x_cross, y_cross, folds, True)\n\n        for i in range(folds):\n\n            y_pred = model.predict(kx[i]).argmax(axis=1)\n\n            if task == 'binary':\n                y_pred = y_pred >= .5\n                scores = sk.metrics.f1_score(y_pred, ky[i], average='binary')\n\n            elif task == 'multi_class':\n                y_pred = y_pred.argmax(axis=-1)\n                scores = sk.metrics.f1_score(y_pred, ky[i], average='macro')\n\n            if task == 'multi_label':\n                y_pred = model.predict(kx[i]).argmax(axis=1)\n                scores = sk.metrics.f1_score(y_pred,\n                                             ky[i].argmax(axis=1),\n                                             average='macro')\n\n            elif task == 'continuous':\n                y_pred = model.predict(kx[i])\n                scores = sk.metrics.mean_absolute_error(y_pred, ky[i])\n\n            out.append(scores)\n\n        results.append(np.mean(out))\n        models.append(model)\n\n    out = df.sort_values('val_acc', ascending=False).head(n_models)\n    out['crossval_mean_f1score'] = results\n\n    return out, models\n"""
talos/utils/rescale_meanzero.py,0,"b""def rescale_meanzero(x):\n\n    '''Rescales an array to mean-zero.\n\n    x | array | the dataset to be rescaled\n    '''\n\n    import wrangle\n    import pandas as pd\n\n    return wrangle.df_rescale_meanzero(pd.DataFrame(x)).values\n"""
talos/utils/sequence_generator.py,0,"b'from keras.utils import Sequence\n\n\nclass SequenceGenerator(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n\n        import numpy as np\n\n        return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return batch_x, batch_y\n'"
talos/utils/test_utils.py,0,"b""def create_param_space(data, no_of_metrics=2):\n\n    '''Takes as input experiment log dataframe and returns\n    ParamSpace object\n\n    data | DataFrame | Talos experiment log as pandas dataframe\n    no_of_metrics | int | number of metrics in the dataframe\n\n    '''\n\n    from talos.parameters.ParamSpace import ParamSpace\n\n    params = {}\n\n    for col in data.iloc[:, no_of_metrics:].columns:\n        params[col] = data[col].unique().tolist()\n\n    param_keys = sorted(list(params.keys()))\n\n    return ParamSpace(params, param_keys)\n"""
talos/utils/torch_history.py,0,"b""class TorchHistory:\n\n    '''This is a helper for replicating the history object\n    behavior of Keras to make Talos Scan() API consistent between\n    the two backends.'''\n\n    def __init__(self):\n\n        self.history = {}\n\n    def init_history(self):\n        self.history = {}\n\n    def append_history(self, history_data, label):\n        if label not in self.history.keys():\n            self.history[label] = []\n        self.history[label].append(history_data)\n\n    def append_loss(self, _loss):\n        self.append_history(_loss, 'loss')\n\n    def append_metric(self, _metric):\n        self.append_history(_metric, 'metric')\n\n    def append_val_loss(self, _loss):\n        self.append_history(_loss, 'val_loss')\n\n    def append_val_metric(self, _loss):\n        self.append_history(_loss, 'val_metric')\n"""
talos/utils/validation_split.py,0,"b'def validation_split(self):\n    """"""Defines the attributes `x_train`, `y_train`, `x_val` and `y_val`.\n    The validation (cross-validation, aka development) sets are determined\n    by the attribute val_split, which is a number in (0, 1) which determines\n    the proportion of the input data to be allocated for cross-validation.""""""\n\n    if self.custom_val_split:\n        self.x_train = self.x\n        self.y_train = self.y\n        # self.x/y_val are already set\n\n    else:\n        # shuffle the data before splitting\n        random_shuffle(self)\n\n        # deduce the midway point for input data\n        limit = int(len(self.x) * (1 - self.val_split))\n\n        self.x_train = self.x[:limit]\n        self.y_train = self.y[:limit]\n\n        self.x_val = self.x[limit:]\n        self.y_val = self.y[limit:]\n\n    return self\n\n\ndef random_shuffle(self):\n\n    """"""Randomly shuffles the datasets.\n    If self.seed is set, seed the generator\n    to ensure that the results are reproducible.""""""\n\n    def randomize(x):\n\n        \'\'\'\n        Helper function to support the case\n        where x consist of a list of arrays.\n        \'\'\'\n\n        import numpy as np\n\n        if self.seed is not None:\n            np.random.seed(self.seed)\n\n        ix = np.arange(len(x))\n        np.random.shuffle(ix)\n\n        return ix\n\n    if isinstance(self.x, list):\n\n        ix = randomize(self.x[0])\n        out = []\n\n        for a in self.x:\n            out.append(a[ix])\n        self.x = out\n\n    else:\n        ix = randomize(self.x)\n        self.x = self.x[ix]\n\n    self.y = self.y[ix]\n\n\ndef kfold(x, y, folds=10, shuffled=True):\n\n    import wrangle as wr\n\n    if shuffled is True:\n        x, y = wr.array_random_shuffle(x, y)\n\n    out_x = []\n    out_y = []\n\n    x_len = len(x)\n    step = int(x_len / folds)\n\n    lo = 0\n    hi = step\n\n    for _i in range(folds):\n        out_x.append(x[lo:hi])\n        out_y.append(y[lo:hi])\n\n        lo += step\n        hi += step\n\n    return out_x, out_y\n'"
test/commands/__init__.py,0,b'from .test_analyze import test_analyze\nfrom .test_autom8 import test_autom8\nfrom .test_latest import test_latest\nfrom .test_lr_normalizer import test_lr_normalizer\nfrom .test_predict import test_predict\nfrom .test_random_methods import test_random_methods\nfrom .test_reducers import test_reducers\nfrom .test_rest import test_rest\nfrom .test_scan import test_scan\nfrom .test_templates import test_templates\n'
test/commands/test_analyze.py,0,"b""def test_analyze(scan_object):\n\n    '''Tests all the attributes available in the Reporting() object'''\n\n    print('\\n >>> Start Analyze()... \\n')\n\n    import talos\n    import glob\n\n    # for now test with old name\n    r = talos.Reporting(scan_object)\n\n    # read from file\n    list_of_files = glob.glob('./testing_latest/' + '/*.csv')\n\n    r = talos.Reporting(list_of_files[-1])\n\n    # and then from scan object\n    r = talos.Analyze(scan_object)\n\n    # test the object properties\n    r.best_params('val_loss', ['val_acc'])\n    r.correlate('val_loss', ['val_acc'])\n    r.data\n    r.high('val_acc')\n    r.low('val_acc')\n\n    # r.plot_bars('first_neuron', 'val_acc', 'dropout', 'hidden_layers')\n    r.plot_box('first_neuron', 'val_acc')\n    r.plot_corr('val_loss', ['val_acc'])\n    r.plot_hist('val_acc')\n    r.plot_kde('val_acc')\n    r.plot_line('val_acc')\n    r.plot_regs('val_acc', 'val_loss')\n    r.rounds()\n    r.rounds2high('val_acc')\n    r.table('val_loss', ['val_acc'])\n\n    print('finish Analyze() \\n')\n"""
test/commands/test_autom8.py,0,"b""def test_autom8():\n\n    import talos\n    import wrangle\n\n    from keras.optimizers import Adam\n\n    print('\\n >>> start AutoParams()... \\n')\n\n    p = talos.autom8.AutoParams()\n    p.params\n    p = talos.autom8.AutoParams(p.params)\n    p.resample_params(5)\n\n    p.activations(['relu'])\n    p.batch_size(20, 50, 2)\n    p.dropout(0, 0.22, 0.04)\n    p.epochs(5, 10, 1)\n    p.kernel_initializers(['zeros'])\n    p.last_activations(['softmax'])\n    p.layers(0, 2, 1)\n    p.losses([talos.utils.metrics.f1score])\n    p.lr([0.01])\n    p.networks(['dense'])\n    p.neurons(1, 5, 1)\n    p.optimizers([Adam])\n    p.shapes(['brick'])\n    p.shapes_slope(0, .2, .01)\n\n    p.resample_params(1)\n\n    print('finised AutoParams() \\n')\n\n    # # # # # # # # # # # # # # # #\n\n    print('\\n >>> start AutoModel(), AutoScan() and AutoPredict()... \\n')\n\n    x, y = wrangle.utils.create_synth_data('binary', 50, 10, 1)\n    p.losses(['binary_crossentropy'])\n    auto = talos.autom8.AutoScan('binary', 'testinga', 1)\n    scan_object = auto.start(x, y, params=p.params)\n    talos.autom8.AutoPredict(scan_object, x, y, x, 'binary')\n\n    x, y = wrangle.utils.create_synth_data('multi_label', 50, 10, 4)\n    p.losses(['categorical_crossentropy'])\n    auto = talos.autom8.AutoScan('multi_label', 'testingb', 1)\n    auto.start(x, y)\n    talos.autom8.AutoPredict(scan_object, x, y, x, 'multi_label')\n\n    x, y = wrangle.utils.create_synth_data('multi_class', 50, 10, 3)\n    p.losses(['sparse_categorical_crossentropy'])\n    auto = talos.autom8.AutoScan('multi_class', 'testingc', 1)\n    auto.start(x, y, params=p.params)\n    talos.autom8.AutoPredict(scan_object, x, y, x, 'multi_class')\n\n    x, y = wrangle.utils.create_synth_data('continuous', 50, 10, 1)\n    p.losses(['mae'])\n    auto = talos.autom8.AutoScan('continuous', 'testingd', 1)\n    auto.start(x, y, params=p.params)\n    talos.autom8.AutoPredict(scan_object, x, y, x, 'continuous')\n\n    print('finised AutoModel(), AutoScan() and AutoPredict() \\n')\n\n    # # # # # # # # # # # # # # # #\n"""
test/commands/test_latest.py,0,"b""def test_latest():\n\n    print('\\n >>> start Latest Features... \\n')\n\n    import talos\n    from keras.models import Sequential\n    from keras.layers import Dense\n\n    x, y = talos.templates.datasets.iris()\n\n    p = {'activation': ['relu', 'elu'],\n         'optimizer': ['Nadam', 'Adam'],\n         'losses': ['logcosh'],\n         'shapes': ['brick'],\n         'first_neuron': [16, 32, 64, 128],\n         'hidden_layers': [0, 1, 2, 3],\n         'dropout': [.2, .3, .4],\n         'batch_size': [20, 30, 40, 50],\n         'epochs': [10]}\n\n    def iris_model(x_train, y_train, x_val, y_val, params):\n\n        model = Sequential()\n        model.add(Dense(params['first_neuron'],\n                        input_dim=4,\n                        activation=params['activation']))\n\n        talos.utils.hidden_layers(model, params, 3)\n\n        model.add(Dense(3, activation='softmax'))\n        model.compile(optimizer=params['optimizer'],\n                      loss=params['losses'], metrics=['acc'])\n\n        out = model.fit(x_train,\n                        y_train,\n                        callbacks=[talos.utils.ExperimentLogCallback('testing_latest', params)],\n                        batch_size=params['batch_size'],\n                        epochs=params['epochs'],\n                        validation_data=[x_val, y_val],\n                        verbose=0)\n\n        return out, model\n\n    scan_object = talos.Scan(x, y,\n                             model=iris_model,\n                             params=p,\n                             experiment_name='testing_latest',\n                             round_limit=5,\n                             reduction_method='gamify',\n                             save_weights=False)\n\n    print('finised Latest Features \\n')\n"""
test/commands/test_lr_normalizer.py,0,"b""def test_lr_normalizer():\n    '''Test learning rate normalizer to confirm an invalid type is\n    recognized and throws TalosModelError.'''\n\n    from talos.model.normalizers import lr_normalizer\n    from talos.utils.exceptions import TalosModelError\n\n    print('Testing lr_normalizer() and invalid optimizer type...')\n\n    # Using string as proxy for any invalid class\n    # (ex., tensorflow-sourced optimizer)\n    bad_optimizer = 'test'\n\n    try:\n        lr_normalizer(1, bad_optimizer)\n    except TalosModelError:\n        print('Invalid model optimizer caught successfully!')\n    else:\n        print('Invalid (string) model optimizer type not caught.')\n"""
test/commands/test_predict.py,0,"b'def test_predict():\n\n    print(""\\n >>> start Predict()..."")\n\n    import sys\n    sys.path.insert(0, \'/Users/mikko/Documents/GitHub/talos\')\n    import talos\n\n    x, y = talos.templates.datasets.iris()\n    p = talos.templates.params.iris()\n    model = talos.templates.models.iris\n\n    x = x[:50]\n    y = y[:50]\n\n    scan_object = talos.Scan(x=x,\n                             y=y,\n                             params=p,\n                             model=model,\n                             experiment_name=\'test_iris\', round_limit=2)\n\n    predict = talos.Predict(scan_object)\n\n    _preds = predict.predict(x, \'val_acc\', False)\n    _preds = predict.predict_classes(x, \'val_acc\', False)\n\n    print(\'finised Predict() \\n\')\n\n    # # # # # # # # # # # # # # # # # #\n'"
test/commands/test_random_methods.py,0,"b""def test_random_methods():\n\n    '''Tests all the available random methods\n    in reducers/sample_reducer.py that are invoked\n    that are invoked through Scan(random_method)'''\n\n    print('\\n >>> start Random Methods... \\n')\n\n    import talos\n\n    random_methods = ['sobol',\n                      'quantum',\n                      'halton',\n                      'korobov_matrix',\n                      'latin_sudoku',\n                      'latin_matrix',\n                      'latin_improved',\n                      'uniform_mersenne',\n                      'uniform_crypto',\n                      'ambience'\n                      ]\n\n    for method in random_methods:\n        talos.templates.pipelines.titanic(random_method=method)\n\n    print('finish Random Methods \\n')\n"""
test/commands/test_reducers.py,0,"b'def test_reducers():\n\n    print(""\\n >>> start reducers..."")\n\n    import talos\n\n    x, y = talos.templates.datasets.iris()\n    p = talos.templates.params.iris()\n    model = talos.templates.models.iris\n\n    x = x[:50]\n    y = y[:50]\n\n    for strategy in [\'trees\',\n                     \'forrest\',\n                     \'correlation\',\n                     \'gamify\',\n                     \'local_strategy\']:\n\n        talos.Scan(x=x,\n                   y=y,\n                   params=p,\n                   model=model,\n                   experiment_name=\'test_iris\',\n                   round_limit=2,\n                   reduction_method=strategy,\n                   reduction_interval=1)\n\n    print(\'finised reducers \\n\')\n\n    # # # # # # # # # # # # # # # # # #\n'"
test/commands/test_rest.py,0,"b""def test_rest(scan_object):\n\n    print('\\n >>> start testing the rest... \\n')\n\n    import talos\n\n    import random\n\n    deploy_filename = 'test' + str(random.randint(1, 20000000000))\n\n    print('\\n ...Deploy()... \\n')\n    talos.Deploy(scan_object, deploy_filename, 'val_acc')\n\n    print('\\n ...Restore()... \\n')\n    restored = talos.Restore(deploy_filename + '.zip')\n\n    x, y = talos.templates.datasets.breast_cancer()\n    x = x[:50]\n    y = y[:50]\n\n    x_train, y_train, x_val, y_val = talos.utils.val_split(x, y, .2)\n    x = talos.utils.rescale_meanzero(x)\n\n    callbacks = [\n                 talos.utils.early_stopper(10),\n                 talos.utils.ExperimentLogCallback('test', {})]\n\n    metrics = [talos.utils.metrics.f1score,\n               talos.utils.metrics.fbeta,\n               talos.utils.metrics.mae,\n               talos.utils.metrics.mape,\n               talos.utils.metrics.matthews,\n               talos.utils.metrics.mse,\n               talos.utils.metrics.msle,\n               talos.utils.metrics.precision,\n               talos.utils.metrics.recall,\n               talos.utils.metrics.rmae,\n               talos.utils.metrics.rmse,\n               talos.utils.metrics.rmsle]\n\n    from keras.models import Sequential\n    from keras.layers import Dense\n\n    print('\\n ...callbacks and metrics... \\n')\n\n    model1 = Sequential()\n    model1.add(Dense(10, input_dim=x.shape[1]))\n    model1.add(Dense(1))\n    model1.compile('adam', 'logcosh', metrics=metrics)\n    model1.fit(x, y, callbacks=callbacks)\n\n    print('\\n ...generator... \\n')\n\n    model2 = Sequential()\n    model2.add(Dense(10, input_dim=x.shape[1]))\n    model2.add(Dense(1))\n    model2.compile('adam', 'logcosh')\n    model2.fit_generator(talos.utils.generator(x, y, 10), 5)\n\n    print('\\n ...SequenceGenerator... \\n')\n\n    model3 = Sequential()\n    model3.add(Dense(10, input_dim=x.shape[1]))\n    model3.add(Dense(1))\n    model3.compile('adam', 'logcosh')\n    model3.fit_generator(talos.utils.SequenceGenerator(x, y, 10))\n\n    print('\\n ...gpu_utils... \\n')\n\n    talos.utils.gpu_utils.force_cpu()\n    talos.utils.gpu_utils.parallel_gpu_jobs()\n\n    print('\\n ...gpu_utils... \\n')\n\n    from talos.utils.test_utils import create_param_space\n    create_param_space(restored.results, 5)\n\n    print('finished testing the rest \\n')\n"""
test/commands/test_scan.py,0,"b'def test_scan():\n\n    print(""\\n >>> start Scan()..."")\n\n    import talos\n\n    from keras.losses import binary_crossentropy\n    from keras.optimizers import Adam\n    from keras.activations import relu, elu\n    from keras.layers import Dense\n    from keras.models import Sequential\n\n    p = {\'activation\': [relu, elu],\n         \'optimizer\': [\'Nadam\', Adam],\n         \'losses\': [\'logcosh\', binary_crossentropy],\n         \'shapes\': [\'brick\', \'funnel\', \'triangle\'],\n         \'first_neuron\': [16],\n         \'hidden_layers\': ([0, 1, 2, 3]),\n         \'dropout\': (.05, .35, .1),\n         \'epochs\': [50]}\n\n    def iris_model(x_train, y_train, x_val, y_val, params):\n\n        model = Sequential()\n        model.add(Dense(params[\'first_neuron\'],\n                        input_dim=4,\n                        activation=params[\'activation\']))\n\n        talos.utils.hidden_layers(model, params, 3)\n\n        model.add(Dense(3, activation=\'softmax\'))\n\n        if isinstance(params[\'optimizer\'], str):\n            opt = params[\'optimizer\']\n        else:\n            opt = params[\'optimizer\']()\n\n        model.compile(optimizer=opt,\n                      loss=params[\'losses\'],\n                      metrics=[\'acc\', talos.utils.metrics.f1score])\n\n        out = model.fit(x_train, y_train,\n                        batch_size=25,\n                        epochs=params[\'epochs\'],\n                        validation_data=[x_val, y_val],\n                        verbose=0)\n\n        return out, model\n\n    x, y = talos.templates.datasets.iris()\n\n    scan_object = talos.Scan(x=x,\n                             y=y,\n                             params=p,\n                             model=iris_model,\n                             experiment_name=\'testingq\',\n                             val_split=0.3,\n                             random_method=\'uniform_mersenne\',\n                             round_limit=15,\n                             reduction_method=\'spearman\',\n                             reduction_interval=10,\n                             reduction_window=9,\n                             reduction_threshold=0.01,\n                             reduction_metric=\'val_acc\',\n                             minimize_loss=False,\n                             boolean_limit=lambda p: p[\'first_neuron\'] * p[\'hidden_layers\'] < 220\n                             )\n\n    x = x[:50]\n    y = y[:50]\n\n    p[\'epochs\'] = [5]\n\n    # minimal settings\n    talos.Scan(x=x,\n               y=y,\n               x_val=x,\n               y_val=y,\n               params=p,\n               model=iris_model,\n               experiment_name=\'iris_test\',\n               fraction_limit=.05)\n\n    # config invoked\n    talos.Scan(x=x,\n               y=y,\n               params=p,\n               model=iris_model,\n               experiment_name=""testing2"",\n               x_val=x,\n               y_val=y,\n               random_method=\'latin_suduko\',\n               seed=3,\n               performance_target=[\'acc\', 0.01, False],\n               round_limit=3,\n               disable_progress_bar=True,\n               print_params=True,\n               clear_session=False)\n\n    talos.Scan(x=x,\n               y=y,\n               params=p,\n               model=iris_model,\n               experiment_name=""testing3"",\n               x_val=None,\n               y_val=None,\n               val_split=0.3,\n               random_method=\'sobol\',\n               seed=5,\n               performance_target=[\'val_acc\', 0.1, False],\n               fraction_limit=None,\n               time_limit=""2099-09-09 09:09"",\n               boolean_limit=None,\n               reduction_method=\'spearman\',\n               reduction_interval=2,\n               reduction_window=2,\n               reduction_threshold=0.2,\n               reduction_metric=\'loss\',\n               minimize_loss=True,\n               clear_session=False)\n\n    print(\'finised Scan() \\n\')\n\n    # # # # # # # # # # # # # # # # # #\n\n    print(""\\n >>> start Scan() object ..."")\n\n    # the create the test based on it\n\n    _keras_model = scan_object.best_model()\n    _keras_model = scan_object.best_model(\'loss\', True)\n\n    scan_object.evaluate_models(x_val=scan_object.x,\n                                y_val=scan_object.y,\n                                task=\'multi_label\')\n\n    scan_object.evaluate_models(x_val=scan_object.x,\n                                y_val=scan_object.y,\n                                task=\'multi_label\',\n                                n_models=3,\n                                metric=\'val_loss\',\n                                folds=3,\n                                shuffle=False,\n                                asc=True)\n\n    print(\'finised Scan() object \\n\')\n\n    # # # # # # # # # # # # # # # # # #\n\n\n    return scan_object\n'"
test/commands/test_templates.py,0,"b'def test_templates():\n\n    print(""\\n >>> start templates ..."")\n\n    import talos\n\n    x, y = talos.templates.datasets.titanic()\n    x = x[:50]\n    y = y[:50]\n    model = talos.templates.models.titanic\n    p = talos.templates.params.titanic()\n    talos.Scan(x, y, p, model, \'test\', round_limit=2)\n\n    x, y = talos.templates.datasets.iris()\n    x = x[:50]\n    y = y[:50]\n    model = talos.templates.models.iris\n    p = talos.templates.params.iris()\n    talos.Scan(x, y, p, model, \'test\', round_limit=2)\n\n    x, y = talos.templates.datasets.cervical_cancer()\n    x = x[:50]\n    y = y[:50]\n    model = talos.templates.models.cervical_cancer\n    p = talos.templates.params.cervical_cancer()\n    talos.Scan(x, y, p, model, \'test\', round_limit=2)\n\n    x, y = talos.templates.datasets.breast_cancer()\n    x = x[:50]\n    y = y[:50]\n    model = talos.templates.models.breast_cancer\n    p = talos.templates.params.breast_cancer()\n    talos.Scan(x, y, p, model, \'test\', round_limit=2)\n\n    x, y = talos.templates.datasets.icu_mortality(50)\n    x, y = talos.templates.datasets.telco_churn(.3)\n    x, y, x1, y1 = talos.templates.datasets.mnist()\n    x, y = talos.templates.datasets.breast_cancer()\n    x, y = talos.templates.datasets.cervical_cancer()\n    x, y = talos.templates.datasets.titanic()\n\n    talos.templates.pipelines.breast_cancer(random_method=\'quantum\')\n    talos.templates.pipelines.cervical_cancer(random_method=\'sobol\')\n    talos.templates.pipelines.iris(random_method=\'uniform_crypto\')\n    talos.templates.pipelines.titanic(random_method=\'korobov_matrix\')\n\n    print(""finish templates \\n"")\n'"
test/performance/memory_pressure.py,0,"b'import talos\nfrom talos.utils import SequenceGenerator\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D\n\np = {\'activation\': [\'relu\'],\n     \'optimizer\': [\'AdaDelta\'],\n     \'losses\': [\'categorical_crossentropy\'],\n     \'dropout\': [.2],\n     \'batch_size\': [256],\n     \'epochs\': [1, 1, 1, 1, 1]}\n\nx_train, y_train, x_val, y_val = talos.templates.datasets.mnist()\n\n@profile\ndef talos_version():\n\n    def mnist_model(x_train, y_train, x_val, y_val, params):\n\n        model = Sequential()\n        model.add(Conv2D(32, kernel_size=(3, 3), activation=params[\'activation\'], input_shape=(28, 28, 1)))\n        model.add(Flatten())\n        model.add(Dense(128, activation=params[\'activation\']))\n        model.add(Dropout(params[\'dropout\']))\n        model.add(Dense(10, activation=\'softmax\'))\n\n        model.compile(optimizer=params[\'optimizer\'],\n                      loss=params[\'losses\'],\n                      metrics=[\'acc\', talos.utils.metrics.f1score])\n\n        out = model.fit_generator(SequenceGenerator(x_train,\n                                                    y_train,\n                                                    batch_size=params[\'batch_size\']),\n                                                    epochs=params[\'epochs\'],\n                                                    validation_data=[x_val, y_val],\n                                                    callbacks=[],\n                                                    workers=4,\n                                                    verbose=0)\n\n        return out, model\n\n    scan_object = talos.Scan(x=x_train,\n                             y=y_train,\n                             x_val=x_val,\n                             y_val=y_val,\n                             params=p,\n                             model=mnist_model,\n                             experiment_name=\'mnist\',\n\t\t\t                 save_weights=False)\n\n\nif __name__ == ""__main__"":\n\n    talos_version()\n'"
test/performance/memory_pressure_check.py,0,"b'if __name__ == \'__main__\':\n\n    import numpy as np\n    import pandas as pd\n    import os\n\n    print(\'\\n Memory Pressure Test Starts...\\n\')\n\n    for i in os.listdir():\n        if \'mprofile_\' in i:\n            df = pd.read_csv(i, sep=\' \', error_bad_lines=False)\n\n    df.columns = [\'null\', \'memory\', \'time\']\n    df.drop(\'null\', 1, inplace=True)\n\n    std_limit = 5\n    highest_limit = 800\n\n    std = np.std(np.array(df.memory.values[1500:]))\n    highest = df.memory.max()\n\n    if std > std_limit:\n        raise Exception(\'MEMORY TEST FAILED: Standard deviation of memory pressure is %d which is above the %d limit\' % (std, std_limit))\n\n    if highest > highest_limit:\n        raise Exception(\'MEMORY TEST FAILED: Max memory is %d which is above the %d limit\' % (highest, highest_limit))\n\n    print(""\\n Memory Pressure Test Passed \\n"")\n'"
