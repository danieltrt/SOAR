file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nimport os\n\n#include the non python files\ndef package_files(directory, strip_leading):\n    paths = []\n    for (path, directories, filenames) in os.walk(directory):\n        for filename in filenames:\n            package_file = os.path.join(path, filename)\n            paths.append(package_file[len(strip_leading):])\n    return paths\n\ncar_templates=[\'templates/*\']\nweb_controller_html = package_files(\'donkeycar/parts/controllers/templates\', \'donkeycar/\')\n\n\nextra_files = car_templates + web_controller_html\nprint(\'extra_files\', extra_files)\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\n\nsetup(name=\'donkeycar\',\n    version=\'3.1.2\',\n    long_description = long_description,\n    description=\'Self driving library for python.\',\n    url=\'https://github.com/autorope/donkeycar\',\n    author=\'Will Roscoe, Adam Conway, Tawn Kramer\',\n    author_email=\'wroscoe@gmail.com, adam@casaconway.com, tawnkramer@gmail.com\',\n    license=\'MIT\',\n    entry_points={\n        \'console_scripts\': [\n            \'donkey=donkeycar.management.base:execute_from_command_line\',\n        ],\n    },\n    install_requires=[\'numpy\',\n                      \'pillow\',\n                      \'docopt\',\n                      \'tornado\',\n                      \'requests\',\n                      \'h5py\',\n                      \'moviepy\',\n                      \'pandas\',\n                      \'PrettyTable\',\n                      \'paho-mqtt\'\n                     ],\n\n    extras_require={\n                    \'pi\': [\n                        \'picamera\',\n                        \'Adafruit_PCA9685\',\n                        \'Adafruit_SSD1306\',\n                        \'RPi.GPIO\',\n                        \'pyserial\',\n                        ],\n                    \'nano\': [\n                        \'Adafruit_PCA9685\',\n                        \'Adafruit_SSD1306\',\n                        ],\n                    \'pc\': [\n                        \'matplotlib\',\n                        ],\n                    \'dev\' : [\n                        \'pytest\',\n                        \'pytest-cov\',\n                        \'responses\',\n                        ],\n                    \'ci\': [\'codecov\'],\n                    \'tf\': [\'tensorflow==1.13.1\'],\n                    \'tf_gpu\': [\'tensorflow-gpu==1.13.1\'],\n                    \'mm1\': [\'pyserial\']\n                    },\n    package_data={\n        \'donkeycar\': extra_files,\n        },\n\n      include_package_data=True,\n\n      classifiers=[\n          # How mature is this project? Common values are\n          #   3 - Alpha\n          #   4 - Beta\n          #   5 - Production/Stable\n          \'Development Status :: 3 - Alpha\',\n\n          # Indicate who your project is intended for\n          \'Intended Audience :: Developers\',\n          \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n\n          # Pick your license as you wish (should match ""license"" above)\n          \'License :: OSI Approved :: MIT License\',\n\n          # Specify the Python versions you support here. In particular, ensure\n          # that you indicate whether you support Python 2, Python 3 or both.\n\n          \'Programming Language :: Python :: 3.6\',\n          \'Programming Language :: Python :: 3.7\',\n      ],\n      keywords=\'selfdriving cars donkeycar diyrobocars\',\n\n      packages=find_packages(exclude=([\'tests\', \'docs\', \'site\', \'env\'])),\n      )\n'"
donkeycar/__init__.py,0,"b""__version__ = '3.1.2'\n\nprint('using donkey v{} ...'.format(__version__))\n\nimport sys\n\nif sys.version_info.major < 3:\n    msg = 'Donkey Requires Python 3.4 or greater. You are using {}'.format(sys.version)\n    raise ValueError(msg)\n\nfrom . import parts\nfrom .vehicle import Vehicle\nfrom .memory import Memory\nfrom . import utils\nfrom . import config\nfrom . import contrib\nfrom .config import load_config\n"""
donkeycar/config.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Sep 13 21:27:44 2017\n\n@author: wroscoe\n""""""\nimport os\nimport types\n    \nclass Config:\n    \n    def from_pyfile(self, filename, silent=False):\n        #filename = os.path.join(self.root_path, filename)\n        d = types.ModuleType(\'config\')\n        d.__file__ = filename\n        try:\n            with open(filename, mode=\'rb\') as config_file:\n                exec(compile(config_file.read(), filename, \'exec\'), d.__dict__)\n        except IOError as e:\n            e.strerror = \'Unable to load configuration file (%s)\' % e.strerror\n            raise\n        self.from_object(d)\n        return True\n    \n    def from_object(self, obj):\n        for key in dir(obj):\n            if key.isupper():\n                #self[key] = getattr(obj, key)\n                setattr(self, key, getattr(obj, key))\n                \n    def __str__(self):\n        result = []\n        for key in dir(self):\n            if key.isupper():\n                result.append((key, getattr(self,key)))\n        return str(result)\n\n    def show(self):\n        for attr in dir(self):\n            if attr.isupper():\n                print(attr, "":"", getattr(self, attr))\n\n\n\ndef load_config(config_path=None, myconfig=""myconfig.py""):\n    \n    if config_path is None:\n        import __main__ as main\n        main_path = os.path.dirname(os.path.realpath(main.__file__))\n        config_path = os.path.join(main_path, \'config.py\')\n        if not os.path.exists(config_path):\n            local_config = os.path.join(os.path.curdir, \'config.py\')\n            if os.path.exists(local_config):\n                config_path = local_config\n    \n    print(\'loading config file: {}\'.format(config_path))\n    cfg = Config()\n    cfg.from_pyfile(config_path)\n\n    #look for the optional myconfig.py in the same path.\n    print(""myconfig"", myconfig)\n    personal_cfg_path = config_path.replace(""config.py"", myconfig)\n    if os.path.exists(personal_cfg_path):\n        print(""loading personal config over-rides from"", myconfig)\n        personal_cfg = Config()\n        personal_cfg.from_pyfile(personal_cfg_path)\n        #personal_cfg.show()\n\n        cfg.from_object(personal_cfg)\n        #print(""final settings:"")\n        #cfg.show()\n    else:\n        print(""personal config: file not found "", personal_cfg_path)\n        \n    \n    #derivative settings\n    if hasattr(cfg, \'IMAGE_H\') and hasattr(cfg, \'IMAGE_W\'): \n        cfg.TARGET_H = cfg.IMAGE_H - cfg.ROI_CROP_TOP - cfg.ROI_CROP_BOTTOM\n        cfg.TARGET_W = cfg.IMAGE_W\n        cfg.TARGET_D = cfg.IMAGE_DEPTH\n\n    print()\n\n    print(\'config loaded\')\n    return cfg\n'"
donkeycar/geom.py,0,"b""'''\nGeometry\nAuthor: Tawn Kramer\nDate: Nov 11, 2014\n'''\nfrom .la import Vec2\n\nclass LineSeg2d(object):\n\n    def __init__(self, x1, y1, x2, y2):\n        a = Vec2(x1, y1)\n        b = Vec2(x2, y2)\n        self.point = a\n        self.end = b\n        self.ray = a - b\n        self.ray.normalize()\n\n    def closest_vec_to(self, vec2_pt):\n        '''\n        produces a vector normal to this line passing through the given point vec2_pt\n        '''\n        delta_pt = self.point - vec2_pt\n        dp = delta_pt.dot(self.ray)\n        return self.ray * dp - delta_pt\n\n    def cross_track_error(self, vec2_pt):\n        '''\n        a signed magnitude of distance from line segment\n        '''\n        err_vec = self.closest_vec_to(vec2_pt)\n        mag = err_vec.mag()\n        err_vec.scale(1.0 / mag)\n        sign = 1.\n        if err_vec.cross(self.ray) < 0.0:\n            sign = -1.\n        return mag * sign\n\n"""
donkeycar/la.py,0,"b'\'\'\'\nLinear Algebra\nAuthor: Tawn Kramer\nDate: Nov 11, 2014\n\'\'\'\nimport math\n\nclass Vec2(object):\n    def __init__(self, x=0.0, y=0.0):\n        self.x = x\n        self.y = y\n\n    def __add__(self, other):\n        return self.add(other)\n\n    def __sub__(self, other):\n        return self.subtract(other)\n\n    def __mul__(self, other):\n        return self.multiply(other)\n\n    def __div__(self, other):\n        return self.multiply(other.reciprocal())\n\n    def __neg__(self):\n        return self.scaled(-1.0)\n\n    def __iadd__(self, other): #+= other\n        self = self.add(other)\n        return self\n\n    def mag_squared(self):\n        return self.x * self.x + self.y * self.y\n\n    def mag(self):\n        return math.sqrt(self.x * self.x + self.y * self.y)\n        \n    def scale(self, s):\n        self.x *= s\n        self.y *= s\n        return self\n\n    def scaled(self, s):\n        r = Vec2()\n        r.x = self.x * s\n        r.y = self.y * s\n        return r\n        \n    def normalize(self):\n        m = self.mag()\n        self.scale(1.0 / m)\n        return self\n        \n    def subtract(self, v):\n        r = Vec2()\n        r.x = self.x - v.x\n        r.y = self.y - v.y\n        return r\n        \n    def add(self, v):\n        r = Vec2()\n        r.x = self.x + v.x\n        r.y = self.y + v.y\n        return r\n        \n    def multiply(self, v):\n        r = Vec2()\n        r.x = self.x * v.x\n        r.y = self.y * v.y\n        return r\n        \n    def dot(self, v):\n        return self.x * v.x + self.y * v.y\n        \n    def cross(self, v):\n        #the sign tells you which side the other vector lies\n        return self.x * v.y - self.y * v.x\t\n\n    def dist(self, v):\n        r = self.subtract(v)\n        return r.mag()\n\n    def reciprocal(self):\n        r = Vec2()\n        if(self.x != 0.0):\n            r.x = 1.0 / self.x\n        if(self.y != 0.0):\n            r.y = 1.0 / self.y\n        return r\n        \n    def unit_angle(self, v):\n        #note! requires normalized vectors as input\n        #returns radian angle\n        return math.acos(self.dot(v))\n\n\nclass Vec3(object):\n    def __init__(self, x=0.0, y=0.0, z=0.0):\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def __add__(self, other):\n        return self.add(other)\n\n    def __sub__(self, other):\n        return self.subtract(other)\n\n    def __mul__(self, other):\n        return self.multiply(other)\n\n    def __div__(self, other):\n        return self.multiply(other.reciprocal())\n\n    def __neg__(self):\n        return self.scaled(-1.0)\n\n    def __iadd__(self, other): #+= other\n        self = self.add(other)\n        return self\n\n    def mag(self):\n        return math.sqrt(self.x * self.x + self.y * self.y + self.z * self.z)\n        \n    def scale(self, s):\n        self.x *= s\n        self.y *= s\n        self.z *= s\n        return self\n\n    def scaled(self, s):\n        r = Vec3()\n        r.x = self.x * s\n        r.y = self.y * s\n        r.z = self.z * s\n        return r\n        \n    def normalize(self):\n        m = self.mag()\n        self.scale(1.0 / m)\n        return self\n\n    def normalized(self):\n        m = self.mag()\n        v = Vec3(self.x, self.y, self.z)\n        v.scale(1.0 / m)\n        return v\n\n    def subtract(self, v):\n        r = Vec3()\n        r.x = self.x - v.x\n        r.y = self.y - v.y\n        r.z = self.z - v.z\n        return r\n        \n    def add(self, v):\n        r = Vec3()\n        r.x = self.x + v.x\n        r.y = self.y + v.y\n        r.z = self.z + v.z\n        return r\n        \n    def multiply(self, v):\n        r = Vec3()\n        r.x = self.x * v.x\n        r.y = self.y * v.y\n        r.z = self.z * v.z\n        return r\n        \n    def dot(self, v):\n        return (self.x * v.x + self.y * v.y + self.z * v.z)\n        \n    def cross(self, v):\n        r = Vec3()\n        r.x = (self.y * v.z) - (self.z * v.y)\n        r.y = (self.z * v.x) - (self.x * v.z)\n        r.z = (self.x * v.y) - (self.y * v.x)\n        return r\n    \n    def dist(self, v):\n        r = self.subtract(v)\n        return r.mag()\n\n    def reciprocal(self):\n        r = Vec3()\n        if(self.x != 0.0):\n            r.x = 1.0 / self.x\n        if(self.y != 0.0):\n            r.y = 1.0 / self.y\n        if(self.z != 0.0):\n            r.z = 1.0 / self.z\n        return r\n        \n    def unit_angle(self, v):\n        #note! requires normalized vectors as input\n        return math.acos(self.dot(v))\n \ndef Quat_RotY( radians ):\n    halfAngle = radians * 0.5\n    sinHalf = math.sin(halfAngle)\n    cosHalf = math.cos(halfAngle)\n    return Quat(0.0, sinHalf, 0.0, cosHalf)\n\nclass Quat(object):\n    def __init__(self, x=0.0, y=0.0, z=0.0, w=1.0):\n        self.x = x\n        self.y = y\n        self.z = z\n        self.w = w\n\n    def rot_x(self, angle):\n        #make this quat a rotation about the X axis of radian angle\n        halfa = angle * 0.5\n        self.x = math.sin(halfa)\n        self.y = 0.\n        self.z = 0.\n        self.w = math.cos(halfa)\n\n    def rot_y(self, angle):\n        #make this quat a rotation about the Y axis of radian angle\n        halfa = angle * 0.5\n        self.y = math.sin(halfa)\n        self.x = 0.\n        self.z = 0.\n        self.w = math.cos(halfa)\n\n    def rot_z(self, angle):\n        #make this quat a rotation about the Z axis of radian angle\n        halfa = angle * 0.5\n        self.z = math.sin(halfa)\n        self.y = 0.\n        self.x = 0.\n        self.w = math.cos(halfa)\n\n    def __mul__(self, other):\n        q = Quat()\n        q.multiply(self, other)\n        return q\n\n    def mag(self):\n        return math.sqrt(self.x * self.x + self.y * self.y + self.z * self.z + self.w * self.w)\n\n    def normalize(self):\n        m = self.mag()\n        invM = 1.0 / m\n        self.scale(invM)\n        return self\n\n    def normalized(self):\n        return self.scaled(1.0 / self.mag())        \n    \n    def scale(self, s):\n        self.x *= s\n        self.y *= s\n        self.z *= s\n        self.w *= s\n        return self\n\n    def scaled(self, s):\n        r = Vec4()\n        r.x = self.x * s\n        r.y = self.y * s\n        r.z = self.z * s\n        r.w = self.w * s\n        return r\n\n    def conjugate(self):\n        return Quat(-self.x, -self.y, -self.y, self.w)\n\n    def inverse(self):\n        q0 = self.normalized()\n        return q0.scale(-1.0)\n        \n    def multiply(self, q1, q2):\n        self.x = q2.w * q1.x + q2.x * q1.w + q2.y * q1.z - q2.z * q1.y\n        self.y = q2.w * q1.y + q2.y * q1.w + q2.z * q1.x - q2.x * q1.z\n        self.z = q2.w * q1.z + q2.z * q1.w + q2.x * q1.y - q2.y * q1.x\n        self.w = q2.w * q1.w - q2.x * q1.x - q2.y * q1.y - q2.z * q1.z\n        \n    def vector_transform(self, v):\n        qxyz = Vec3(self.x, self.y, self.z)\n        cross_v = qxyz.cross(v)\n        vw = v.scale(self.w)\n        halfV = qxyz.cross ( cross_v.add(vw) )\n        return v.add( halfV.scale(2.0) )\n\n    def from_axis_angle(self, axis, angle):\n        \'\'\'\n        construct a quat from an normalized axis vector and radian rotation about that axis\n        \'\'\'\n        sinha = math.sin(angle * 0.5)\n        cosha = math.cos(angle * 0.5)\n        self.w = cosha\n        self.x = sinha * axis.x\n        self.y = sinha * axis.y\n        self.z = sinha * axis.z\n\n    def to_axis_angle(self):\n        \'\'\'\n        returns a normalized axis vector and radian rotation about that axis\n        \'\'\'\n        halfa = math.acos(self.w)\n        sinha = math.sin(halfa)\n        axis = Vec3()        \n        if sinha != 0.0:\n            axis.x = self.x / sinha\n            axis.y = self.y / sinha\n            axis.z = self.z / sinha\n        else:\n            axis.z = 1.0\n        angle = 2.0 * halfa\n        return axis, angle\n\n        \n    def getYAxisRot(self):\n        c = Vec3()\n        x2 = self.x + self.x\n        y2 = self.y + self.y\n        z2 = self.z + self.z\n        xx = self.x * x2\n        xz = self.x * z2\n        yy = self.y * y2\n        wy = self.w * y2\n        \n        c.x = xz + wy\n        c.y = 0.0\n        c.z = 1.0 - (xx + yy)\n        cx2cz2 = c.x * c.x + c.z * c.z\n        \n        if cx2cz2 > 0.0:\n            factor = 1.0 / math.sqrt(cx2cz2)\n            c.x = c.x * factor\n            c.z = c.z * factor\n        else:\n            return 0.0\n        \n        if c.z <= -0.9999999:\n            return math.pi\n        \n        if c.z >= 0.9999999:\n            return 0.0\n        \n        return math.atan2(c.x, c.z)\n        \n    def slerp(self, tval, low, high):\n        lHigh = Quat()\n        cosom = low.x*high.x + low.y*high.y + low.z*high.z + low.w*high.w\n        if cosom < 0.0:\n            cosom = -cosom\n            lHigh.x = -high.x\n            lHigh.y = -high.y\n            lHigh.z = -high.z\n            lHigh.w = -high.w\n        else:\n            lHigh.x = high.x\n            lHigh.y = high.y\n            lHigh.z = high.z\n            lHigh.w = high.w\n        \n        FLOAT_EPSILON = 0.0000001\n        if ( (1.0 - cosom) > FLOAT_EPSILON ):\n            #standard case (slerp)\n            omega = math.acos(cosom)\n            sinom = math.sin(omega)\n            fOneOverSinom = 1.0/sinom\n            scalar0 = math.sin( ((1.0 - tval) * omega) ) * fOneOverSinom\n            scalar1 = math.sin( (tval * omega) ) * fOneOverSinom\n        else:\n            # ""from"" and ""to"" Quaternions are very close \n            #  ... so we can do a linear interpolation\n            scalar0 = 1.0 - tval\n            scalar1 = tval\n            \n        # calculate final values\n        self.x = scalar0 * low.x + scalar1 * lHigh.x\n        self.y = scalar0 * low.y + scalar1 * lHigh.y\n        self.z = scalar0 * low.z + scalar1 * lHigh.z\n        self.w = scalar0 * low.w + scalar1 * lHigh.w \n        \n\nclass Vec4(object):\n    def __init__(self, x=0.0, y=0.0, z=0.0, w=0.0):\n        self.x = x\n        self.y = y\n        self.z = z\n        self.w = w\n\n    def __add__(self, other):\n        return self.add(other)\n\n    def __sub__(self, other):\n        return self.subtract(other)\n\n    def __mul__(self, other):\n        return self.multiply(other)\n\n    def __div__(self, other):\n        return self.multiply(other.reciprocal())\n\n    def __neg__(self):\n        return self.scaled(-1.0)\n\n    def __iadd__(self, other): #+= other\n        self = self.add(other)\n        return self\n\n    def mag(self):\n        return math.sqrt(self.x * self.x + self.y * self.y + self.z * self.z + self.w * self.w)\n        \n    def scale(self, s):\n        self.x *= s\n        self.y *= s\n        self.z *= s\n        self.w *= s\n        return self\n\n    def scaled(self, s):\n        r = Vec4()\n        r.x = self.x * s\n        r.y = self.y * s\n        r.z = self.z * s\n        r.w = self.w * s\n        return r\n        \n    def normalize(self):\n        m = self.mag()\n        self.scale(1.0 / m)\n        return self\n\n    def normalized(self):\n        m = self.mag()\n        return self.scaled(1.0 / m)        \n        \n    def subtract(self, v):\n        r = Vec4()\n        r.x = self.x - v.x\n        r.y = self.y - v.y\n        r.z = self.z - v.z\n        r.w = self.w - v.w\n        return r\n        \n    def add(self, v):\n        r = Vec4()\n        r.x = self.x + v.x\n        r.y = self.y + v.y\n        r.z = self.z + v.z\n        r.w = self.w + v.w\n        return r\n        \n    def multiply(self, v):\n        r = Vec4()\n        r.x = self.x * v.x\n        r.y = self.y * v.y\n        r.z = self.z * v.z\n        return r\n        \n    def dot(self, v):\n        return (self.x * v.x + self.y * v.y + self.z * v.z + self.w * v.w)\n           \n    def dist(self, v):\n        r = self.subtract(v)\n        return r.mag()\n\n    def reciprocal(self):\n        r = Vec4()\n        if(self.x != 0.0):\n            r.x = 1.0 / self.x\n        if(self.y != 0.0):\n            r.y = 1.0 / self.y\n        if(self.z != 0.0):\n            r.z = 1.0 / self.z\n        if(self.w != 0.0):\n            r.w = 1.0 / self.w\n        return r\n\ndef Det2x2(a, b, c, d):\n    return(a * d - b * c)\n\ndef Det3x3(a1, a2, a3, b1, b2, b3, c1, c2, c3):\n    return(\n        a1 * Det2x2(b2, b3, c2, c3) -\n        b1 * Det2x2(a2, a3, c2, c3) +\n        c1 * Det2x2(a2, a3, b2, b3))\n\nclass Mat44(object):\n    def __init__(self, a=Vec4(), b=Vec4(), c=Vec4(), d=Vec4()):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n\n    def indentity(self):\n        self.a = Vec4(1.0, 0.0, 0.0, 0.0)\n        self.b = Vec4(0.0, 1.0, 0.0, 0.0)\n        self.c = Vec4(0.0, 0.0, 1.0, 0.0)\n        self.d = Vec4(0.0, 0.0, 0.0, 1.0)\n        \n    def fromQuat(self, q):\n        #calculate coefficients\n        x2 = q.x + q.x\n        y2 = q.y + q.y\n        z2 = q.z + q.z\n        xx = q.x * x2\n        xy = q.x * y2\n        xz = q.x * z2\n        yy = q.y * y2\n        yz = q.y * z2\n        zz = q.z * z2\n        wx = q.w * x2\n        wy = q.w * y2\n        wz = q.w * z2\n        \n        self.a.x = 1.0 - (yy + zz)\n        self.a.y = xy + wz\n        self.a.z = xz - wy\n        self.a.w = 0.0\n        \n        self.b.x = xy - wz\n        self.b.y = 1.0 - (xx + zz)\n        self.b.z = yz + wx\n        self.b.w = 0.0\n        \n        self.c.x = xz + wy\n        self.c.y = yz - wx\n        self.c.z = 1.0 - (xx + yy)\n        self.c.w = 0.0\n        \n        self.d.x = 0.0 \n        self.d.y = 0.0\n        self.d.z = 0.0\n        self.d.w = 1.0\n        \n    def setTranslation(self, trans):\n        self.d.x = trans.x\n        self.d.y = trans.y\n        self.d.z = trans.z\n        \n    def affineTransform(self, v):\n        x = self.a.x*v.x + self.b.x*v.y + self.c.x*v.z + self.d.x\n        y = self.a.y*v.x + self.b.y*v.y + self.c.y*v.z + self.d.y\n        z = self.a.z*v.x + self.b.z*v.y + self.c.z*v.z + self.d.z\n        return Vec3(x, y, z)\n\n    def vectorTransform(self, v):\n        x = self.a.x*v.x + self.b.x*v.y + self.c.x*v.z\n        y = self.a.y*v.x + self.b.y*v.y + self.c.y*v.z\n        z = self.a.z*v.x + self.b.z*v.y + self.c.z*v.z\n        return Vec3(x, y, z)\n\n    def multiply_vec4(self, v):\n        return Vec4(\n\t\t    self.a.x*v.x + self.b.x*v.y + self.c.x*v.z + self.d.x*v.w,\n\t\t    self.a.y*v.x + self.b.y*v.y + self.c.y*v.z + self.d.y*v.w,\n\t\t    self.a.z*v.x + self.b.z*v.y + self.c.z*v.z + self.d.z*v.w,\n\t\t    self.a.w*v.x + self.b.w*v.y + self.c.w*v.z + self.d.w*v.w)\n\n    def multiply_mat44(self, src2):\n        mtxOut = Mat44()\n\n        mtxOut.a.x = self.a.x*src2.a.x + self.a.y*src2.b.x + self.a.z*src2.c.x + self.a.w*src2.d.x;\n        mtxOut.a.y = self.a.x*src2.a.y + self.a.y*src2.b.y + self.a.z*src2.c.y + self.a.w*src2.d.y;\n        mtxOut.a.z = self.a.x*src2.a.z + self.a.y*src2.b.z + self.a.z*src2.c.z + self.a.w*src2.d.z;\n        mtxOut.a.w = self.a.x*src2.a.w + self.a.y*src2.b.w + self.a.z*src2.c.w + self.a.w*src2.d.w;\n        \n        mtxOut.b.x = self.b.x*src2.a.x + self.b.y*src2.b.x + self.b.z*src2.c.x + self.b.w*src2.d.x;\n        mtxOut.b.y = self.b.x*src2.a.y + self.b.y*src2.b.y + self.b.z*src2.c.y + self.b.w*src2.d.y;\n        mtxOut.b.z = self.b.x*src2.a.z + self.b.y*src2.b.z + self.b.z*src2.c.z + self.b.w*src2.d.z;\n        mtxOut.b.w = self.b.x*src2.a.w + self.b.y*src2.b.w + self.b.z*src2.c.w + self.b.w*src2.d.w;\n        \n        mtxOut.c.x = self.c.x*src2.a.x + self.c.y*src2.b.x + self.c.z*src2.c.x + self.c.w*src2.d.x;\n        mtxOut.c.y = self.c.x*src2.a.y + self.c.y*src2.b.y + self.c.z*src2.c.y + self.c.w*src2.d.y;\n        mtxOut.c.z = self.c.x*src2.a.z + self.c.y*src2.b.z + self.c.z*src2.c.z + self.c.w*src2.d.z;\n        mtxOut.c.w = self.c.x*src2.a.w + self.c.y*src2.b.w + self.c.z*src2.c.w + self.c.w*src2.d.w;\n        \n        mtxOut.d.x = self.d.x*src2.a.x + self.d.y*src2.b.x + self.d.z*src2.c.x + self.d.w*src2.d.x;\n        mtxOut.d.y = self.d.x*src2.a.y + self.d.y*src2.b.y + self.d.z*src2.c.y + self.d.w*src2.d.y;\n        mtxOut.d.z = self.d.x*src2.a.z + self.d.y*src2.b.z + self.d.z*src2.c.z + self.d.w*src2.d.z;\n        mtxOut.d.w = self.d.x*src2.a.w + self.d.y*src2.b.w + self.d.z*src2.c.w + self.d.w*src2.d.w;\n\n        return mtxOut\n\n    def inverse(self):\n        inv = Mat44()\n        inv.indentity()\n\n        det = Det3x3(self.a.x, self.b.x, self.c.x,  self.a.y, self.b.y, self.c.y,  self.a.z, self.b.z, self.c.z)\n        if det < 0.000000001:\n            return inv\n\n        # inverse(A) = adjunct(A) / det(A)\n        oodet = 1.0 / det\n        inv.a.x =  Det2x2(self.b.y, self.b.z, self.c.y, self.c.z) * oodet\n        inv.b.x = -Det2x2(self.b.x, self.b.z, self.c.x, self.c.z) * oodet\n        inv.c.x =  Det2x2(self.b.x, self.b.y, self.c.x, self.c.y) * oodet\n        \n        inv.a.y = -Det2x2(self.a.y, self.a.z, self.c.y, self.c.z) * oodet\n        inv.b.y =  Det2x2(self.a.x, self.a.z, self.c.x, self.c.z) * oodet\n        inv.c.y = -Det2x2(self.a.x, self.a.y, self.c.x, self.c.y) * oodet\n        \n        inv.a.z =  Det2x2(self.a.y, self.a.z, self.b.y, self.b.z) * oodet\n        inv.b.z = -Det2x2(self.a.x, self.a.z, self.b.x, self.b.z) * oodet\n        inv.c.z =  Det2x2(self.a.x, self.a.y, self.b.x, self.b.y) * oodet\n\n        # inverse(C) = -C * inverse(A)\n        inv.d.x = -(self.d.x*inv.a.x+self.d.y*inv.b.x+self.d.z*inv.c.x)\n        inv.d.y = -(self.d.x*inv.a.y+self.d.y*inv.b.y+self.d.z*inv.c.y)\n        inv.d.z = -(self.d.x*inv.a.z+self.d.y*inv.b.z+self.d.z*inv.c.z)\n\n        return(inv)\n\n\nclass Line3D(object):\n\n    def __init__(self, a, b):\n        self.origin = a\n        self.dir = a - b\n        self.dir.normalize()\n\n    def vector_to(self, p):\n        delta = self.origin - p\n        dot = delta.dot(self.dir)\n        return self.dir.scaled(dot) - delta'"
donkeycar/memory.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Jun 25 11:07:48 2017\n\n@author: wroscoe\n""""""\n\nclass Memory:\n    """"""\n    A convenience class to save key/value pairs.\n    """"""\n    def __init__(self, *args, **kw):\n        self.d = {}\n    \n    def __setitem__(self, key, value):\n        if type(key) is not tuple:\n            print(\'tuples\')\n            key = (key,)\n            value=(value,)\n        \n        for i, k in enumerate(key):\n            self.d[k] = value[i]\n        \n    def __getitem__(self, key):\n        if type(key) is tuple:\n            return [self.d[k] for k in key]\n        else:\n            return self.d[key]\n        \n    def update(self, new_d):\n        self.d.update(new_d)\n        \n    def put(self, keys, inputs):\n        if len(keys) > 1:\n            for i, key in enumerate(keys):\n                try:\n                    self.d[key] = inputs[i]\n                except IndexError as e:\n                    error = str(e) + \' issue with keys: \' + str(key)\n                    raise IndexError(error)\n        \n        else:\n            self.d[keys[0]] = inputs\n\n            \n            \n    def get(self, keys):\n        result = [self.d.get(k) for k in keys]\n        return result\n    \n    def keys(self):\n        return self.d.keys()\n    \n    def values(self):\n        return self.d.values()\n    \n    def items(self):\n        return self.d.items()\n        '"
donkeycar/utils.py,0,"b'\'\'\'\nutils.py\n\nFunctions that don\'t fit anywhere else.\n\n\'\'\'\nfrom io import BytesIO\nimport os\nimport glob\nimport socket\nimport zipfile\nimport sys\nimport itertools\nimport subprocess\nimport math\nimport random\nimport time\nimport signal\n\n\nfrom PIL import Image\nimport numpy as np\n\n\'\'\'\nIMAGES\n\'\'\'\none_byte_scale = 1.0 / 255.0\n\n\ndef scale(im, size=128):\n    \'\'\'\n    accepts: PIL image, size of square sides\n    returns: PIL image scaled so sides lenght = size\n    \'\'\'\n    size = (size,size)\n    im.thumbnail(size, Image.ANTIALIAS)\n    return im\n\n\ndef img_to_binary(img, format=\'jpeg\'):\n    \'\'\'\n    accepts: PIL image\n    returns: binary stream (used to save to database)\n    \'\'\'\n    f = BytesIO()\n    try:\n        img.save(f, format=format)\n    except Exception as e:\n        raise e\n    return f.getvalue()\n\n\ndef arr_to_binary(arr):\n    \'\'\'\n    accepts: numpy array with shape (Hight, Width, Channels)\n    returns: binary stream (used to save to database)\n    \'\'\'\n    img = arr_to_img(arr)\n    return img_to_binary(img)\n\n\ndef arr_to_img(arr):\n    \'\'\'\n    accepts: numpy array with shape (Height, Width, Channels)\n    returns: binary stream (used to save to database)\n    \'\'\'\n    arr = np.uint8(arr)\n    img = Image.fromarray(arr)\n    return img\n\n\ndef img_to_arr(img):\n    \'\'\'\n    accepts: numpy array with shape (Height, Width, Channels)\n    returns: binary stream (used to save to database)\n    \'\'\'\n    return np.array(img)\n\n\ndef binary_to_img(binary):\n    \'\'\'\n    accepts: binary file object from BytesIO\n    returns: PIL image\n    \'\'\'\n    if binary is None or len(binary) == 0:\n        return None\n\n    img = BytesIO(binary)\n    try:\n        img = Image.open(img)\n        return img\n    except:\n        return None\n\n\ndef norm_img(img):\n    return (img - img.mean() / np.std(img)) * one_byte_scale\n\n\ndef create_video(img_dir_path, output_video_path):\n    import envoy\n    # Setup path to the images with telemetry.\n    full_path = os.path.join(img_dir_path, \'frame_*.png\')\n\n    # Run ffmpeg.\n    command = (""""""ffmpeg\n               -framerate 30/1\n               -pattern_type glob -i \'%s\'\n               -c:v libx264\n               -r 15\n               -pix_fmt yuv420p\n               -y\n               %s"""""" % (full_path, output_video_path))\n    response = envoy.run(command)\n\n\ndef rgb2gray(rgb):\n    \'\'\'\n    take a numpy rgb image return a new single channel image converted to greyscale\n    \'\'\'\n    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n\n\ndef img_crop(img_arr, top, bottom):\n\n    if bottom is 0:\n        end = img_arr.shape[0]\n    else:\n        end = -bottom\n    return img_arr[top:end, ...]\n\n\ndef normalize_and_crop(img_arr, cfg):\n    img_arr = img_arr.astype(np.float32) * one_byte_scale\n    if cfg.ROI_CROP_TOP or cfg.ROI_CROP_BOTTOM:\n        img_arr = img_crop(img_arr, cfg.ROI_CROP_TOP, cfg.ROI_CROP_BOTTOM)\n        if len(img_arr.shape) == 2:\n            img_arrH = img_arr.shape[0]\n            img_arrW = img_arr.shape[1]\n            img_arr = img_arr.reshape(img_arrH, img_arrW, 1)\n    return img_arr\n\n\ndef load_scaled_image_arr(filename, cfg):\n    \'\'\'\n    load an image from the filename, and use the cfg to resize if needed\n    also apply cropping and normalize\n    \'\'\'\n    import donkeycar as dk\n    try:\n        img = Image.open(filename)\n        if img.height != cfg.IMAGE_H or img.width != cfg.IMAGE_W:\n            img = img.resize((cfg.IMAGE_W, cfg.IMAGE_H))\n        img_arr = np.array(img)\n        img_arr = normalize_and_crop(img_arr, cfg)\n        croppedImgH = img_arr.shape[0]\n        croppedImgW = img_arr.shape[1]\n        if img_arr.shape[2] == 3 and cfg.IMAGE_DEPTH == 1:\n            img_arr = dk.utils.rgb2gray(img_arr).reshape(croppedImgH, croppedImgW, 1)\n    except Exception as e:\n        print(e)\n        print(\'failed to load image:\', filename)\n        img_arr = None\n    return img_arr\n\n\n\'\'\'\nFILES\n\'\'\'\n\n\ndef most_recent_file(dir_path, ext=\'\'):\n    \'\'\'\n    return the most recent file given a directory path and extension\n    \'\'\'\n    query = dir_path + \'/*\' + ext\n    newest = min(glob.iglob(query), key=os.path.getctime)\n    return newest\n\n\ndef make_dir(path):\n    real_path = os.path.expanduser(path)\n    if not os.path.exists(real_path):\n        os.makedirs(real_path)\n    return real_path\n\n\ndef zip_dir(dir_path, zip_path):\n    """"""\n    Create and save a zipfile of a one level directory\n    """"""\n    file_paths = glob.glob(dir_path + ""/*"") #create path to search for files.\n\n    zf = zipfile.ZipFile(zip_path, \'w\')\n    dir_name = os.path.basename(dir_path)\n    for p in file_paths:\n        file_name = os.path.basename(p)\n        zf.write(p, arcname=os.path.join(dir_name, file_name))\n    zf.close()\n    return zip_path\n\n\n\n\'\'\'\nBINNING\nfunctions to help converte between floating point numbers and categories.\n\'\'\'\n\n\ndef clamp(n, min, max):\n    if n < min:\n        return min\n    if n > max:\n        return max\n    return n\n\n\ndef linear_bin(a, N=15, offset=1, R=2.0):\n    \'\'\'\n    create a bin of length N\n    map val A to range R\n    offset one hot bin by offset, commonly R/2\n    \'\'\'\n    a = a + offset\n    b = round(a / (R/(N-offset)))\n    arr = np.zeros(N)\n    b = clamp(b, 0, N - 1)\n    arr[int(b)] = 1\n    return arr\n\n\ndef linear_unbin(arr, N=15, offset=-1, R=2.0):\n    \'\'\'\n    preform inverse linear_bin, taking\n    one hot encoded arr, and get max value\n    rescale given R range and offset\n    \'\'\'\n    b = np.argmax(arr)\n    a = b *(R/(N + offset)) + offset\n    return a\n\n\ndef map_range(x, X_min, X_max, Y_min, Y_max):\n    \'\'\'\n    Linear mapping between two ranges of values\n    \'\'\'\n    X_range = X_max - X_min\n    Y_range = Y_max - Y_min\n    XY_ratio = X_range/Y_range\n\n    y = ((x-X_min) / XY_ratio + Y_min) // 1\n\n    return int(y)\n\n\ndef map_range_float(x, X_min, X_max, Y_min, Y_max):\n    \'\'\'\n    Same as map_range but supports floats return, rounded to 2 decimal places\n    \'\'\'\n    X_range = X_max - X_min\n    Y_range = Y_max - Y_min\n    XY_ratio = X_range/Y_range\n\n    y = ((x-X_min) / XY_ratio + Y_min)\n\n    # print(""y= {}"".format(y))\n\n    return round(y,2)\n\n\'\'\'\nANGLES\n\'\'\'\n\n\ndef norm_deg(theta):\n    while theta > 360:\n        theta -= 360\n    while theta < 0:\n        theta += 360\n    return theta\n\n\nDEG_TO_RAD = math.pi / 180.0\n\n\ndef deg2rad(theta):\n    return theta * DEG_TO_RAD\n\n\'\'\'\nVECTORS\n\'\'\'\n\n\ndef dist(x1, y1, x2, y2):\n    return math.sqrt(math.pow(x2 - x1, 2) + math.pow(y2 - y1, 2))\n\n\n\'\'\'\nNETWORKING\n\'\'\'\n\ndef my_ip():\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.connect((\'192.0.0.8\', 1027))\n    return s.getsockname()[0]\n\n\n\'\'\'\nOTHER\n\'\'\'\n\ndef map_frange(x, X_min, X_max, Y_min, Y_max):\n    \'\'\'\n    Linear mapping between two ranges of values\n    \'\'\'\n    X_range = X_max - X_min\n    Y_range = Y_max - Y_min\n    XY_ratio = X_range/Y_range\n\n    y = ((x-X_min) / XY_ratio + Y_min)\n\n    return y\n\n\ndef merge_two_dicts(x, y):\n    """"""Given two dicts, merge them into a new dict as a shallow copy.""""""\n    z = x.copy()\n    z.update(y)\n    return z\n\n\n\ndef param_gen(params):\n    \'\'\'\n    Accepts a dictionary of parameter options and returns\n    a list of dictionary with the permutations of the parameters.\n    \'\'\'\n    for p in itertools.product(*params.values()):\n        yield dict(zip(params.keys(), p ))\n\n\ndef run_shell_command(cmd, cwd=None, timeout=15):\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n    out = []\n    err = []\n\n    try:\n        proc.wait(timeout=timeout)\n    except subprocess.TimeoutExpired:\n        kill(proc.pid)\n\n    for line in proc.stdout.readlines():\n        out.append(line.decode())\n\n    for line in proc.stderr.readlines():\n        err.append(line)\n    return out, err, proc.pid\n\n\n\ndef kill(proc_id):\n    os.kill(proc_id, signal.SIGINT)\n\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)\n\n\n""""""\nTub management\n""""""\n\n\ndef expand_path_masks(paths):\n    \'\'\'\n    take a list of paths and expand any wildcards\n    returns a new list of paths fully expanded\n    \'\'\'\n    import glob\n    expanded_paths = []\n    for path in paths:\n        if \'*\' in path or \'?\' in path:\n            mask_paths = glob.glob(path)\n            expanded_paths += mask_paths\n        else:\n            expanded_paths.append(path)\n\n    return expanded_paths\n\n\ndef gather_tub_paths(cfg, tub_names=None):\n    \'\'\'\n    takes as input the configuration, and the comma seperated list of tub paths\n    returns a list of Tub paths\n    \'\'\'\n    if tub_names:\n        if type(tub_names) == list:\n            tub_paths = [os.path.expanduser(n) for n in tub_names]\n        else:\n            tub_paths = [os.path.expanduser(n) for n in tub_names.split(\',\')]\n        return expand_path_masks(tub_paths)\n    else:\n        paths = [os.path.join(cfg.DATA_PATH, n) for n in os.listdir(cfg.DATA_PATH)]\n        dir_paths = []\n        for p in paths:\n            if os.path.isdir(p):\n                dir_paths.append(p)\n        return dir_paths\n\n\ndef gather_tubs(cfg, tub_names):\n    \'\'\'\n    takes as input the configuration, and the comma seperated list of tub paths\n    returns a list of Tub objects initialized to each path\n    \'\'\'\n    from donkeycar.parts.datastore import Tub\n\n    tub_paths = gather_tub_paths(cfg, tub_names)\n    tubs = [Tub(p) for p in tub_paths]\n\n    return tubs\n\n""""""\nTraining helpers\n""""""\n\ndef get_image_index(fnm):\n    sl = os.path.basename(fnm).split(\'_\')\n    return int(sl[0])\n\n\ndef get_record_index(fnm):\n    sl = os.path.basename(fnm).split(\'_\')\n    return int(sl[1].split(\'.\')[0])\n\n\ndef gather_records(cfg, tub_names, opts=None, verbose=False):\n\n    tubs = gather_tubs(cfg, tub_names)\n\n    records = []\n\n    for tub in tubs:\n        if verbose:\n            print(tub.path)\n        record_paths = tub.gather_records()\n        records += record_paths\n\n    return records\n\n\ndef get_model_by_type(model_type, cfg):\n    \'\'\'\n    given the string model_type and the configuration settings in cfg\n    create a Keras model and return it.\n    \'\'\'\n    from donkeycar.parts.keras import KerasRNN_LSTM, KerasBehavioral, \\\n        KerasCategorical, KerasIMU, KerasLinear, Keras3D_CNN, \\\n        KerasLocalizer, KerasLatent\n    from donkeycar.parts.tflite import TFLitePilot\n\n    if model_type is None:\n        model_type = cfg.DEFAULT_MODEL_TYPE\n    print(""\\""get_model_by_type\\"" model Type is: {}"".format(model_type))\n\n    input_shape = (cfg.IMAGE_H, cfg.IMAGE_W, cfg.IMAGE_DEPTH)\n    roi_crop = (cfg.ROI_CROP_TOP, cfg.ROI_CROP_BOTTOM)\n\n    if model_type == ""tflite_linear"":\n        kl = TFLitePilot()\n    elif model_type == ""localizer"" or cfg.TRAIN_LOCALIZER:\n        kl = KerasLocalizer(num_locations=cfg.NUM_LOCATIONS, input_shape=input_shape)\n    elif model_type == ""behavior"" or cfg.TRAIN_BEHAVIORS:\n        kl = KerasBehavioral(num_outputs=2, num_behavior_inputs=len(cfg.BEHAVIOR_LIST), input_shape=input_shape)\n    elif model_type == ""imu"":\n        kl = KerasIMU(num_outputs=2, num_imu_inputs=6, input_shape=input_shape, roi_crop=roi_crop)\n    elif model_type == ""linear"":\n        kl = KerasLinear(input_shape=input_shape, roi_crop=roi_crop)\n    elif model_type == ""tensorrt_linear"":\n        # Aggressively lazy load this. This module imports pycuda.autoinit which causes a lot of unexpected things\n        # to happen when using TF-GPU for training.\n        from donkeycar.parts.tensorrt import TensorRTLinear\n        kl = TensorRTLinear(cfg=cfg)\n    elif model_type == ""coral_tflite_linear"":\n        from donkeycar.parts.coral import CoralLinearPilot\n        kl = CoralLinearPilot()\n    elif model_type == ""3d"":\n        kl = Keras3D_CNN(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, seq_length=cfg.SEQUENCE_LENGTH, roi_crop=roi_crop)\n    elif model_type == ""rnn"":\n        kl = KerasRNN_LSTM(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, seq_length=cfg.SEQUENCE_LENGTH, roi_crop=roi_crop)\n    elif model_type == ""categorical"":\n        kl = KerasCategorical(input_shape=input_shape, throttle_range=cfg.MODEL_CATEGORICAL_MAX_THROTTLE_RANGE, roi_crop=roi_crop)\n    elif model_type == ""latent"":\n        kl = KerasLatent(input_shape=input_shape)\n    elif model_type == ""fastai"":\n        from donkeycar.parts.fastai import FastAiPilot\n        kl = FastAiPilot()\n    else:\n        raise Exception(""unknown model type: %s"" % model_type)\n\n    return kl\n\n\ndef get_test_img(model):\n    \'\'\'\n    query the input to see what it likes\n    make an image capable of using with that test model\n    \'\'\'\n    assert(len(model.inputs) > 0)\n    try:\n        count, h, w, ch = model.inputs[0].get_shape()\n        seq_len = 0\n    except Exception as e:\n        count, seq_len, h, w, ch = model.inputs[0].get_shape()\n\n    # generate random array in the right shape\n    img = np.random.rand(int(h), int(w), int(ch))\n\n    return img\n\n\ndef train_test_split(data_list, shuffle=True, test_size=0.2):\n    \'\'\'\n    take a list, split it into two sets while selecting a\n    random element in order to shuffle the results.\n    use the test_size to choose the split percent.\n    shuffle is always True, left there to be backwards compatible\n    \'\'\'\n    assert shuffle\n    train_data = []\n\n    target_train_size = len(data_list) * (1. - test_size)\n\n    i_sample = 0\n\n    while i_sample < target_train_size and len(data_list) > 1:\n        i_choice = random.randint(0, len(data_list) - 1)\n        train_data.append(data_list.pop(i_choice))\n        i_sample += 1\n\n    # remainder of the original list is the validation set\n    val_data = data_list\n\n    return train_data, val_data\n\n\n""""""\nTimers\n""""""\n\n\nclass FPSTimer(object):\n    def __init__(self):\n        self.t = time.time()\n        self.iter = 0\n\n    def reset(self):\n        self.t = time.time()\n        self.iter = 0\n\n    def on_frame(self):\n        self.iter += 1\n        if self.iter == 100:\n            e = time.time()\n            print(\'fps\', 100.0 / (e - self.t))\n            self.t = time.time()\n            self.iter = 0'"
donkeycar/vehicle.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Jun 25 10:44:24 2017\n\n@author: wroscoe\n""""""\n\nimport time\nimport numpy as np\nfrom threading import Thread\nfrom .memory import Memory\nfrom prettytable import PrettyTable\nimport traceback\n\n\nclass PartProfiler:\n    def __init__(self):\n        self.records = {}\n\n    def profile_part(self, p):\n        self.records[p] = { ""times"" : [] }\n\n    def on_part_start(self, p):\n        self.records[p][\'times\'].append(time.time())\n\n    def on_part_finished(self, p):\n        now = time.time()\n        prev = self.records[p][\'times\'][-1]\n        delta = now - prev\n        thresh = 0.000001\n        if delta < thresh or delta > 100000.0:\n            delta = thresh\n        self.records[p][\'times\'][-1] = delta\n\n    def report(self):\n        print(""Part Profile Summary: (times in ms)"")\n        pt = PrettyTable()\n        field_names = [""part"", ""max"", ""min"", ""avg""]\n        pctile = [50, 90, 99, 99.9]\n        pt.field_names = field_names + [str(p) + \'%\' for p in pctile]\n        for p, val in self.records.items():\n            # remove first and last entry because you there could be one-off\n            # time spent in initialisations, and the latest diff could be\n            # incomplete because of user keyboard interrupt\n            arr = val[\'times\'][1:-1]\n            if len(arr) == 0:\n                continue\n            row = [p.__class__.__name__,\n                   ""%.2f"" % (max(arr) * 1000),\n                   ""%.2f"" % (min(arr) * 1000),\n                   ""%.2f"" % (sum(arr) / len(arr) * 1000)]\n            row += [""%.2f"" % (np.percentile(arr, p) * 1000) for p in pctile]\n            pt.add_row(row)\n        print(pt)\n\n\nclass Vehicle:\n    def __init__(self, mem=None):\n\n        if not mem:\n            mem = Memory()\n        self.mem = mem\n        self.parts = []\n        self.on = True\n        self.threads = []\n        self.profiler = PartProfiler()\n\n    def add(self, part, inputs=[], outputs=[],\n            threaded=False, run_condition=None):\n        """"""\n        Method to add a part to the vehicle drive loop.\n\n        Parameters\n        ----------\n            part: class\n                donkey vehicle part has run() attribute\n            inputs : list\n                Channel names to get from memory.\n            outputs : list\n                Channel names to save to memory.\n            threaded : boolean\n                If a part should be run in a separate thread.\n            run_condition : boolean\n                If a part should be run or not\n        """"""\n        assert type(inputs) is list, ""inputs is not a list: %r"" % inputs\n        assert type(outputs) is list, ""outputs is not a list: %r"" % outputs\n        assert type(threaded) is bool, ""threaded is not a boolean: %r"" % threaded\n\n        p = part\n        print(\'Adding part {}.\'.format(p.__class__.__name__))\n        entry = {}\n        entry[\'part\'] = p\n        entry[\'inputs\'] = inputs\n        entry[\'outputs\'] = outputs\n        entry[\'run_condition\'] = run_condition\n\n        if threaded:\n            t = Thread(target=part.update, args=())\n            t.daemon = True\n            entry[\'thread\'] = t\n\n        self.parts.append(entry)\n        self.profiler.profile_part(part)\n\n    def remove(self, part):\n        """"""\n        remove part form list\n        """"""\n        self.parts.remove(part)\n\n    def start(self, rate_hz=10, max_loop_count=None, verbose=False):\n        """"""\n        Start vehicle\'s main drive loop.\n\n        This is the main thread of the vehicle. It starts all the new\n        threads for the threaded parts then starts an infinite loop\n        that runs each part and updates the memory.\n\n        Parameters\n        ----------\n\n        rate_hz : int\n            The max frequency that the drive loop should run. The actual\n            frequency may be less than this if there are many blocking parts.\n        max_loop_count : int\n            Maximum number of loops the drive loop should execute. This is\n            used for testing that all the parts of the vehicle work.\n        verbose: bool\n            If debug output should be printed into shell\n        """"""\n\n        try:\n\n            self.on = True\n\n            for entry in self.parts:\n                if entry.get(\'thread\'):\n                    # start the update thread\n                    entry.get(\'thread\').start()\n\n            # wait until the parts warm up.\n            print(\'Starting vehicle at {} Hz\'.format(rate_hz))\n\n            loop_count = 0\n            while self.on:\n                start_time = time.time()\n                loop_count += 1\n\n                self.update_parts()\n\n                # stop drive loop if loop_count exceeds max_loopcount\n                if max_loop_count and loop_count > max_loop_count:\n                    self.on = False\n\n                sleep_time = 1.0 / rate_hz - (time.time() - start_time)\n                if sleep_time > 0.0:\n                    time.sleep(sleep_time)\n                else:\n                    # print a message when could not maintain loop rate.\n                    if verbose:\n                        print(\'WARN::Vehicle: jitter violation in vehicle loop \'\n                              \'with {0:4.0f}ms\'.format(abs(1000 * sleep_time)))\n\n                if verbose and loop_count % 200 == 0:\n                    self.profiler.report()\n\n        except KeyboardInterrupt:\n            pass\n        except Exception as e:\n            traceback.print_exc()\n        finally:\n            self.stop()\n\n    def update_parts(self):\n        \'\'\'\n        loop over all parts\n        \'\'\'\n        for entry in self.parts:\n\n            run = True\n            # check run condition, if it exists\n            if entry.get(\'run_condition\'):\n                run_condition = entry.get(\'run_condition\')\n                run = self.mem.get([run_condition])[0]\n            \n            if run:\n                # get part\n                p = entry[\'part\']\n                # start timing part run\n                self.profiler.on_part_start(p)\n                # get inputs from memory\n                inputs = self.mem.get(entry[\'inputs\'])\n                # run the part\n                if entry.get(\'thread\'):\n                    outputs = p.run_threaded(*inputs)\n                else:\n                    outputs = p.run(*inputs)\n\n                # save the output to memory\n                if outputs is not None:\n                    self.mem.put(entry[\'outputs\'], outputs)\n                # finish timing part run\n                self.profiler.on_part_finished(p)\n\n    def stop(self):        \n        print(\'Shutting down vehicle and its parts...\')\n        for entry in self.parts:\n            try:\n                entry[\'part\'].shutdown()\n            except AttributeError:\n                # usually from missing shutdown method, which should be optional\n                pass\n            except Exception as e:\n                print(e)\n\n        self.profiler.report()\n'"
scripts/cull_tub.py,0,"b'\'\'\'\nUsage:\n    cull_tub.py --tub=<path> --count=<int>\n\nNote:\n    This script will remove records in a given tub until it reaches a desired count.\n    It will try to sample from the records to maintain a even steering distribution\n    as it selects records to remove.\n\'\'\'\n\nimport os\n\nfrom docopt import docopt\nimport json\n\nimport donkeycar as dk\nfrom donkeycar.utils import *\nfrom donkeycar.parts.datastore import TubGroup\n\ndef main(tub_path, count):\n    cfg = dk.load_config(\'config.py\')\n    records_paths = gather_records(cfg, tub_path)\n\n    record_name = ""user/angle""\n    num_bins = 20\n    half_bins = num_bins // 2\n    bins = {}\n    records = []\n\n    for i in range(num_bins):\n        bins[i] = []\n\n\n    if len(records_paths) <= count:\n        print(""we have fewer records than target count."")\n        return\n\n    #put the record in a bin based on angle. expecting -1 to 1\n    for record_path in records_paths:\n        with open(record_path, \'r\') as fp:\n            record = json.load(fp)\n        user_angle = float(record[""user/angle""])\n        record[""filename""] = record_path\n        iBin = round((user_angle * half_bins)  + (half_bins - 1)) % num_bins\n        bins[iBin].append(record)\n        records.append(record)\n\n\n    for i in range(num_bins):\n        print(""iBin:"", len(bins[i]))\n\n\n    keep = []\n    iBin = 0\n    while len(keep) < count:\n        iBin += 1\n        if iBin >= num_bins:\n            iBin = 0\n        num_elem = len(bins[iBin]) \n        if num_elem > 0:\n            iRec = random.randint(0, num_elem - 1)\n            rec = bins[iBin].pop(iRec)\n            keep.append(rec)\n\n    print(""have"", count, ""records selected. removing the rest..."")\n\n    \n    for record in records:\n        if record in keep:\n            continue\n        img_filename = os.path.join(tub_path, record[\'cam/image_array\'])\n        os.unlink(img_filename)\n        os.unlink(record[""filename""])\n\n    print(\'done\')\n    \nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n\n    count = int(args[""--count""])\n    main(args[""--tub""], count)\n'"
scripts/cull_tubs.py,0,"b'#Cull many tubs\nimport os\nimport sys\n\nfrom cull_tub import main\n\ntubs = os.listdir(sys.argv[1])\ncount = int(sys.argv[2])\nprint(""found"", len(tubs), ""tubs"")\nfor tub in tubs:\n    if os.path.isdir(tub):\n        print(""working on"", tub)\n        main(tub, count)'"
scripts/freeze_model.py,6,"b'\'\'\'\nUsage:\n    freeze_model.py --model=""mymodel.h5"" --output=""frozen_model.pb""\n\nNote:\n    This requires that TensorRT is setup correctly. For more instructions, take a look at\n    https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html\n\'\'\'\nimport os\n\nfrom docopt import docopt\nimport json\nfrom pathlib import Path\nimport tensorflow as tf\n\nargs = docopt(__doc__)\nin_model = os.path.expanduser(args[\'--model\'])\noutput = os.path.expanduser(args[\'--output\'])\noutput_path = Path(output)\noutput_meta = Path(\'%s/%s.metadata\' % (output_path.parent.as_posix(), output_path.stem))\n\n\n# Reset session\ntf.keras.backend.clear_session()\ntf.keras.backend.set_learning_phase(0)\n\nmodel = tf.keras.models.load_model(in_model, compile=False)\nsession = tf.keras.backend.get_session()\n\ninput_names = sorted([layer.op.name for layer in model.inputs])\noutput_names = sorted([layer.op.name for layer in model.outputs])\n\n# Store additional information in metadata, useful for infrencing\nmeta = {\'input_names\': input_names, \'output_names\': output_names}\n\ngraph = session.graph\n\n# Freeze Graph\nwith graph.as_default():\n    # Convert variables to constants\n    graph_frozen = tf.compat.v1.graph_util.convert_variables_to_constants(session, graph.as_graph_def(), output_names)\n    # Remote training nodes\n    graph_frozen = tf.compat.v1.graph_util.remove_training_nodes(graph_frozen)\n    with open(output, \'wb\') as output_file, open(output_meta.as_posix(), \'w\') as meta_file:\n        output_file.write(graph_frozen.SerializeToString())\n        meta_file.write(json.dumps(meta))\n\n    print (\'Inputs = [%s], Outputs = [%s]\' % (input_names, output_names))\n    print (\'Writing metadata to %s\' % output_meta.as_posix())\n    print (\'To convert use: \\n   `convert-to-uff %s`\' % (output))\n\n'"
scripts/graph_listener.py,0,"b'\nimport os\nimport time\nimport math\nfrom docopt import docopt\nimport donkeycar as dk\n\nfrom donkeycar.parts.cv import CvImageView\nfrom donkeycar.parts.graph import Graph\nfrom donkeycar.parts.network import ZMQValueSub\nfrom donkeycar.parts.transform import Lambda\n\nV = dk.vehicle.Vehicle()\nip = ""localhost""\nw = 640\nh = 480\nd = 3\n\ndef condition_values(obj):\n    if obj is None:\n        return None\n    \'\'\'\n    This expects a tuple of 4 values.\n    The first value is time (x), and the rest are y values\n    from -2, +2\n    This will work with the network publisher test:\n    python ~/projects/donkey_tkramer/donkeycar/parts/network.py\n    \'\'\'\n\n    vals = obj[1:]\n    x = round(obj[0] * 30.0)\n    ret = []\n\n    i = 0\n    for val in vals:\n        coord = (x, val * (h / 4.) + (h / 2.))\n        color = [0, 0, 0]\n        color[i] = 1\n        i += 1\n        ret.append( (coord, color) )\n\n    #a solid white center line.\n    coord = (x, h / 2.0)\n    color = (1.0, 1.0, 1.0)\n    ret.append( (coord, color) )\n\n    return ret\n\nl = Lambda(condition_values)\n\nV.add(ZMQValueSub(name=""test"", ip=ip), outputs=[""obj""])\nV.add(l, inputs=[""obj""], outputs=[""values""])\nV.add(Graph(res=(h, w, d)), inputs=[""values""], outputs=[""graph/img""])\nV.add(CvImageView(), inputs=[""graph/img""])\n\nV.start(rate_hz=10)\n'"
scripts/multi_train.py,0,"b'\'\'\'\r\nmulti_train.py\r\n\r\nThis script can be dropped into your car dir next to manage.py.\r\nThis will invoke a number of sub processes to drive some ai clients\r\nand have them log into and drive on the SDSandbox donkey sim server.\r\nCheck: https://docs.donkeycar.com/guide/simulator/\r\n\'\'\'\r\nimport os\r\nimport time\r\nimport random\r\nimport subprocess\r\n\r\nnum_clients = 4\r\nmodel_file = ""lane_keeper.h5"" #or any model in ~/mycar/models/\r\nbody_styles = [""donkey"", ""bare"", ""car01""]\r\nhost = \'127.0.0.1\'\r\nprocs = []\r\n\r\nfor i in range(num_clients):\r\n    conf_file = ""client%d.py"" % i\r\n    with open(conf_file, ""wt"") as outfile:\r\n        outfile.write(\'WEB_CONTROL_PORT = 888%d\\n\' % i)\r\n        outfile.write(\'WEB_INIT_MODE = ""local""\\n\')\r\n        outfile.write(\'DONKEY_GYM = True\\n\')\r\n        outfile.write(\'DONKEY_SIM_PATH = ""remote""\\n\')\r\n        outfile.write(\'SIM_HOST = ""%s""\\n\' % host)\r\n        iStyle = random.randint(0, len(body_styles) - 1)\r\n        body_style = body_styles[iStyle]\r\n        r = random.randint(0, 255)\r\n        g = random.randint(0, 255)\r\n        b = random.randint(0, 255)\r\n        outfile.write(\'GYM_CONF = { ""body_style"" : ""%s"", ""body_rgb"" : (%d, %d, %d), ""car_name"" : ""ai%d"", ""font_size"" : 100}\\n\' % (body_style, r, g, b, i+1))\r\n        outfile.close()\r\n\r\n    command = ""python manage.py drive --model=models/%s --myconfig=%s"" % (model_file, conf_file)\r\n    com_list = command.split("" "")\r\n    print(com_list)\r\n    proc = subprocess.Popen(com_list)\r\n    procs.append(proc)\r\n    time.sleep(1)\r\n\r\n\r\nprint(""running for 5 min..."")\r\ntry:\r\n    time.sleep(60 * 5)\r\nexcept:\r\n    pass\r\n\r\nprint(""stopping ai"")\r\nfor proc in procs:\r\n    proc.kill()\r\nprint(\'done\')\r\n'"
scripts/pigpio_donkey.py,0,"b""'''\n# pigpio_donkey.py\n# author: Tawn Kramer\n# date: 3/11/2018\n#\n# Use the pigpio python module and daemon to get hardware pwm controls from\n# a raspberrypi gpio pins and no additional hardware.\n#\n# Install and setup:\n# sudo update && sudo apt install pigpio python3-pigpio& sudo systemctl start pigpiod\n'''\nimport os\nimport donkeycar as dk\nfrom donkeycar.parts.controller import PS3JoystickController\nfrom donkeycar.parts.actuator import PWMSteering, PWMThrottle\n\nimport pigpio\n\nclass PiGPIO_PWM():\n    def __init__(self, pin, pgio, freq=75):\n        self.pin = pin\n        self.pgio = pgio\n        self.freq = freq\n        self.pgio.set_mode(self.pin, pigpio.OUTPUT)\n\n    def __del__(self):\n        self.pgio.stop()\n\n    def set_pulse(self, pulse):\n        self.pgio.hardware_PWM(self.pin, self.freq, pulse)\n\n    def run(self, pulse):\n        self.set_pulse(pulse)\n\n\ncfg = dk.load_config()\n\np = pigpio.pi()\n\nV = dk.Vehicle()\n\ncfg.STEERING_CHANNEL = 12\ncfg.THROTTLE_CHANNEL = 13\n\nPULSE_MULT = 1000\n\ncfg.STEERING_LEFT_PWM = 40 * PULSE_MULT\ncfg.STEERING_RIGHT_PWM = 170 * PULSE_MULT\n\ncfg.THROTTLE_FORWARD_PWM = 170 * PULSE_MULT\ncfg.THROTTLE_STOPPED_PWM = 105 * PULSE_MULT\ncfg.THROTTLE_REVERSE_PWM = 40 * PULSE_MULT\n\nV.add(PS3JoystickController(), inputs=['camera/arr'],\n            outputs=['angle', 'throttle', 'mode', 'recording'],\n            threaded=True)\n\nsteering_controller = PiGPIO_PWM(cfg.STEERING_CHANNEL, p)\nsteering = PWMSteering(controller=steering_controller,\n                            left_pulse=cfg.STEERING_LEFT_PWM, \n                            right_pulse=cfg.STEERING_RIGHT_PWM)\n\nthrottle_controller = PiGPIO_PWM(cfg.THROTTLE_CHANNEL, p)\nthrottle = PWMThrottle(controller=throttle_controller,\n                            max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                            zero_pulse=cfg.THROTTLE_STOPPED_PWM, \n                            min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\nV.add(steering, inputs=['angle'])\nV.add(throttle, inputs=['throttle'])\n\nV.start()\n\n\n"""
scripts/profile.py,0,"b'""""""\nScript to drive a TF model as fast as possible\n\nUsage:\n    profile.py (--model=<model>) (--type=<linear|categorical|etc>)\n    \nOptions:\n    -h --help        Show this screen.\n""""""\nimport os\nfrom docopt import docopt\nimport donkeycar as dk\nimport numpy as np\nimport time\nfrom donkeycar.utils import FPSTimer\n\n\ndef profile(model_path, model_type):\n    cfg = dk.load_config(\'config.py\')\n    model_path = os.path.expanduser(model_path)\n    model = dk.utils.get_model_by_type(model_type, cfg)\n    model.load(model_path)\n    \n    count, h, w, ch = 1, cfg.TARGET_H, cfg.TARGET_W, cfg.TARGET_D\n    seq_len = 0\n\n    if ""rnn"" in model_type or ""3d"" in model_type:\n        seq_len = cfg.SEQUENCE_LENGTH\n\n    # generate random array in the right shape in [0,1)\n    img = np.random.rand(int(h), int(w), int(ch))\n\n    if seq_len:\n        img_arr = []\n        for i in range(seq_len):\n            img_arr.append(img)\n        img_arr = np.array(img_arr)\n\n    # make a timer obj\n    timer = FPSTimer()\n\n    try:\n        while True:\n\n            \'\'\'\n            run forward pass on model\n            \'\'\'\n            if seq_len:\n                model.run(img_arr)\n            else:\n                model.run(img)\n\n            \'\'\'\n            keep track of iterations and give feed back on iter/sec\n            \'\'\'\n            timer.on_frame()\n\n    except KeyboardInterrupt:\n        pass\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    profile(model_path = args[\'--model\'], model_type = args[\'--type\'])\n'"
scripts/profile_coral.py,0,"b'import argparse\nfrom donkeycar.parts.coral import InferenceEngine\nfrom PIL import Image\nfrom donkeycar.utils import FPSTimer\nimport numpy as np\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--model\', help=\'File path of Tflite model.\', required=True)\n  parser.add_argument(\n      \'--image\', help=\'File path of the image to be recognized.\', required=True)\n  args = parser.parse_args()\n  # Initialize engine.\n  engine = InferenceEngine(args.model)\n  # Run inference.\n  img = Image.open(args.image)\n  result = engine.Inference(np.array(img))\n  print(""inference result"", result)\n\n  timer = FPSTimer()\n  while True:\n    engine.Inference(np.array(img))\n    timer.on_frame()\n\nif __name__ == \'__main__\':\n  main()'"
scripts/remote_cam_view.py,0,"b'""""""\nScripts to drive a donkey car remotely\n\nUsage:\n    remote_cam_view.py --name=<robot_name> --broker=""localhost"" [--record=<path>]\n\n\nOptions:\n    -h --help     Show this screen.\n""""""\nimport os\nimport time\nimport math\nfrom docopt import docopt\nimport donkeycar as dk\nimport cv2\n\nfrom donkeycar.parts.cv import CvImageView, ImgBGR2RGB, ImgRGB2BGR, ImageScale, ImgWriter, ArrowKeyboardControls\nfrom donkeycar.parts.salient import SalientVis\nfrom donkeycar.parts.network import MQTTValuePub, MQTTValueSub\nfrom donkeycar.parts.transform import Lambda\nfrom donkeycar.parts.image import JpgToImgArr\n\nV = dk.vehicle.Vehicle()\nargs = docopt(__doc__)\nprint(args)\n\nV.add(MQTTValueSub(name=""donkey/%s/camera"" % args[""--name""], broker=args[""--broker""]), outputs=[""jpg""])\nV.add(JpgToImgArr(), inputs=[""jpg""], outputs=[""img_arr""]) \nV.add(ImgBGR2RGB(), inputs=[""img_arr""], outputs=[""rgb""])\nV.add(ImageScale(4.0), inputs=[""rgb""], outputs=[""lg_img""])\nV.add(CvImageView(), inputs=[""lg_img""])\n\nV.add(ArrowKeyboardControls(), outputs=[""control""])\nV.add(MQTTValuePub(name=""donkey/%s/controls"" % args[""--name""]), inputs=[""control""])\n\nrecord_path = args[""--record""]\nif record_path is not None:\n    class ImageSaver:\n        def __init__(self, path):\n            self.index = 0\n            self.path = path\n        \n        def run(self, img_arr):\n            if img_arr is None:\n                return\n            dest_path = os.path.join(self.path, ""img_%d.jpg"" % self.index)\n            self.index += 1\n            cv2.imwrite(dest_path, img_arr)\n    \n    V.add(ImageSaver(record_path), inputs=[""rgb""])\n\n\nV.start(rate_hz=20)\n\n'"
scripts/remote_cam_view_tcp.py,0,"b'""""""\nScript to view a donkeycar camera remotely (when published using TcpServer)\n\nUsage:\n    remote_cam_view_tcp.py (--ip=<ip_address>) [--record=<path>]\n\nOptions:\n    -h --help     Show this screen.\n    --record=<path>      If data should be recorded (locally) specify the path\n    \n""""""\nfrom docopt import docopt\nimport donkeycar as dk\nfrom donkeycar.parts.cv import CvImageView, ImgBGR2RGB, ImageScale\nfrom donkeycar.parts.network import TCPClientValue\nfrom donkeycar.parts.image import JpgToImgArr\n\nargs = docopt(__doc__)\nprint(args)\n\nV = dk.vehicle.Vehicle()\nV.add(TCPClientValue(""camera"", args[""--ip""]), outputs=[""jpg""])\nV.add(JpgToImgArr(), inputs=[""jpg""], outputs=[""img_arr""])\nV.add(ImgBGR2RGB(), inputs=[""img_arr""], outputs=[""rgb""])\nV.add(ImageScale(4.0), inputs=[""rgb""], outputs=[""lg_img""])\nV.add(CvImageView(), inputs=[""lg_img""])\n\n# Local saving of images?\nrecord_path = args[""--record""]\nif record_path is not None:\n    class ImageSaver:\n        def __init__(self, path):\n            self.index = 0\n            self.path = path\n        \n        def run(self, img_arr):\n            if img_arr is None:\n                return\n            dest_path = os.path.join(self.path, ""img_%d.jpg"" % self.index)\n            self.index += 1\n            cv2.imwrite(dest_path, img_arr)\n    \n    V.add(ImageSaver(record_path), inputs=[""rgb""])\n    \nV.start(rate_hz=20)\n'"
scripts/salient_vis_listener.py,0,"b'""""""\nScripts to drive a donkey 2 car\n\nUsage:\n    salient_vis_listener.py [--ip=""localhost""] [--model=<model>] [--type=(linear|categorical|rnn|imu|behavior|3d|localizer)] [--config=""config.py""] \n\n\nOptions:\n    -h --help     Show this screen.\n""""""\nimport os\nimport time\nimport math\nfrom docopt import docopt\nimport donkeycar as dk\n\nfrom donkeycar.parts.cv import CvImageView, ImgBGR2RGB, ImgRGB2BGR, ImageScale, ImgWriter\nfrom donkeycar.parts.salient import SalientVis\nfrom donkeycar.parts.network import ZMQValueSub, UDPValueSub, TCPClientValue\nfrom donkeycar.parts.transform import Lambda\nfrom donkeycar.parts.image import JpgToImgArr\n\nV = dk.vehicle.Vehicle()\nargs = docopt(__doc__)\ncfg = dk.load_config(args[\'--config\'])\n\nmodel_path = args[\'--model\']\nmodel_type = args[\'--type\']\nip = args[\'--ip\']\n\nif model_type is None:\n    model_type = ""categorical""\n\nmodel = dk.utils.get_model_by_type(model_type, cfg)\nmodel.load(model_path)\n\nV.add(TCPClientValue(name=""camera"", host=ip), outputs=[""packet""])\nV.add(JpgToImgArr(), inputs=[""packet""], outputs=[""img""]) \nV.add(ImgBGR2RGB(), inputs=[""img""], outputs=[""img""])\nV.add(SalientVis(model), inputs=[""img""], outputs=[""img""])\nV.add(ImageScale(4.0), inputs=[""img""], outputs=[""lg_img""])\nV.add(CvImageView(), inputs=[""lg_img""])\n\nV.start(rate_hz=1)\n\n'"
scripts/tflite_convert.py,0,"b'\'\'\'\nUsage: \n    tflite_convert.py --model=""mymodel.h5"" --out=""mymodel.tflite""\n\nNote:\n    may require tensorflow > 1.11 or\n    pip install tf-nightly\n\'\'\'\nimport os\n\nfrom docopt import docopt\nfrom donkeycar.parts.tflite import keras_model_to_tflite\n\nargs = docopt(__doc__)\n\nin_model = os.path.expanduser(args[\'--model\'])\nout_model = os.path.expanduser(args[\'--out\']) \nkeras_model_to_tflite(in_model, out_model)\n\n'"
scripts/tflite_profile.py,1,"b'\'\'\'\nUsage: \n    tflite_test.py --model=""mymodel.tflite""\n\nNote:\n    may require tensorflow > 1.11 or\n    pip install tf-nightly\n\'\'\'\nimport os\n\nfrom docopt import docopt\nimport tensorflow as tf\nimport numpy as np\n\nfrom donkeycar.utils import FPSTimer\n\nargs = docopt(__doc__)\n\nin_model = os.path.expanduser(args[\'--model\'])\n\n# Load TFLite model and allocate tensors.\ninterpreter = tf.lite.Interpreter(model_path=in_model)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Test model on random input data.\ninput_shape = input_details[0][\'shape\']\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n\ninterpreter.set_tensor(input_details[0][\'index\'], input_data)\ninterpreter.invoke()\n\n#sample output\nfor tensor in output_details:\n    output_data = interpreter.get_tensor(tensor[\'index\'])\n    print(output_data)\n\n#run in a loop to test performance.\nprint(""test performance: hit CTRL+C to break"")\ntimer = FPSTimer()\nwhile True:\n    interpreter.set_tensor(input_details[0][\'index\'], input_data)\n    interpreter.invoke()\n    timer.on_frame()\n\n'"
donkeycar/contrib/__init__.py,0,b''
donkeycar/gym/__init__.py,0,b''
donkeycar/gym/gym_real.py,0,"b'\'\'\'\nfile: gym_real.py\nauthor: Tawn Kramer\ndate: 2019-01-24\ndesc: Control a real donkey robot via the gym interface\n\'\'\'\nimport os\nimport time\n\nimport gym\nimport numpy as np\nfrom gym import error, spaces, utils\n\nfrom .remote_controller import DonkeyRemoteContoller\n\n\nclass DonkeyRealEnv(gym.Env):\n    """"""\n    OpenAI Gym Environment for a real Donkey\n    """"""\n\n    metadata = {\n        ""render.modes"": [""human"", ""rgb_array""],\n    }\n\n    ACTION_NAMES = [""steer"", ""throttle""]\n    STEER_LIMIT_LEFT = -1.0\n    STEER_LIMIT_RIGHT = 1.0\n    THROTTLE_MIN = 0.0\n    THROTTLE_MAX = 5.0\n    VAL_PER_PIXEL = 255\n\n    def __init__(self, time_step=0.05, frame_skip=2):\n\n        print(""starting DonkeyGym env"")\n        \n        try:\n            donkey_name = str(os.environ[\'DONKEY_NAME\'])\n        except:\n            donkey_name = \'my_robot1234\'\n            print(""No DONKEY_NAME environment var. Using default:"", donkey_name)\n\n        try:\n            mqtt_broker = str(os.environ[\'DONKEY_MQTT_BROKER\'])\n        except:\n            mqtt_broker = ""iot.eclipse.org""\n            print(""No DONKEY_MQTT_BROKER environment var. Using default:"", mqtt_broker)\n            \n        # start controller\n        self.controller = DonkeyRemoteContoller(donkey_name=donkey_name, mqtt_broker=mqtt_broker)\n        \n        # steering and throttle\n        self.action_space = spaces.Box(low=np.array([self.STEER_LIMIT_LEFT, self.THROTTLE_MIN]),\n            high=np.array([self.STEER_LIMIT_RIGHT, self.THROTTLE_MAX]), dtype=np.float32 )\n\n        # camera sensor data\n        self.observation_space = spaces.Box(0, self.VAL_PER_PIXEL, self.controller.get_sensor_size(), dtype=np.uint8)\n\n        # Frame Skipping\n        self.frame_skip = frame_skip\n\n        # wait until loaded\n        self.controller.wait_until_connected()\n        \n\n    def close(self):\n        self.controller.quit()        \n\n    def step(self, action):\n        for i in range(self.frame_skip):\n            self.controller.take_action(action)\n            time.sleep(0.05)\n            observation = self.controller.observe()\n            reward, done, info = 0.1, False, None\n        return observation, reward, done, info\n\n    def reset(self):\n        observation = self.controller.observe()\n        reward, done, info = 0.1, False, None\n        return observation\n\n    def render(self, mode=""human"", close=False):\n        if close:\n            self.controller.quit()\n\n        return self.controller.observe()\n\n    def is_game_over(self):\n        return False\n'"
donkeycar/gym/remote_controller.py,0,"b'\'\'\'\nfile: remote_controller.py\nauthor: Tawn Kramer\ndate: 2019-01-24\ndesc: Control a remote donkey robot over network\n\'\'\'\n\nimport time\n\nfrom donkeycar.parts.network import MQTTValueSub, MQTTValuePub\nfrom donkeycar.parts.image import JpgToImgArr\n\nclass DonkeyRemoteContoller:\n    def __init__(self, donkey_name, mqtt_broker, sensor_size=(120, 160, 3)):\n        self.camera_sub = MQTTValueSub(""donkey/%s/camera"" % donkey_name, broker=mqtt_broker)\n        self.controller_pub = MQTTValuePub(""donkey/%s/controls"" % donkey_name, broker=mqtt_broker)\n        self.jpgToImg = JpgToImgArr()\n        self.sensor_size = sensor_size\n\n    def get_sensor_size(self):\n        return self.sensor_size\n\n    def wait_until_connected(self):\n        pass\n\n    def take_action(self, action):\n        self.controller_pub.run(action)\n\n    def quit(self):\n        self.camera_sub.shutdown()\n        self.controller_pub.shutdown()\n\n    def get_original_image(self):\n        return self.img\n\n    def observe(self):\n        jpg = self.camera_sub.run()\n        self.img = self.jpgToImg.run(jpg)\n        return self.img\n\n\n    \n'"
donkeycar/management/base.py,0,"b'\nimport shutil\nimport argparse\nimport json\n\nfrom socket import *\nimport os\nfrom threading import Thread\n\nimport donkeycar as dk\nfrom donkeycar.parts.datastore import Tub\nfrom donkeycar.utils import *\nfrom donkeycar.management.tub import TubManager\nfrom donkeycar.management.joystick_creator import CreateJoystick\nimport numpy as np\n\nPACKAGE_PATH = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\nTEMPLATES_PATH = os.path.join(PACKAGE_PATH, \'templates\')\n\ndef make_dir(path):\n    real_path = os.path.expanduser(path)\n    print(\'making dir \', real_path)\n    if not os.path.exists(real_path):\n        os.makedirs(real_path)\n    return real_path\n\n\ndef load_config(config_path):\n\n    \'\'\'\n    load a config from the given path\n    \'\'\'\n    conf = os.path.expanduser(config_path)\n\n    if not os.path.exists(conf):\n        print(""No config file at location: %s. Add --config to specify\\\n                location or run from dir containing config.py."" % conf)\n        return None\n\n    try:\n        cfg = dk.load_config(conf)\n    except:\n        print(""Exception while loading config from"", conf)\n        return None\n\n    return cfg\n\n\nclass BaseCommand(object):\n    pass\n\n\nclass CreateCar(BaseCommand):\n    \n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'createcar\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--path\', default=None, help=\'path where to create car folder\')\n        parser.add_argument(\'--template\', default=None, help=\'name of car template to use\')\n        parser.add_argument(\'--overwrite\', action=\'store_true\', help=\'should replace existing files\')\n        \n        parsed_args = parser.parse_args(args)\n        return parsed_args\n        \n    def run(self, args):\n        args = self.parse_args(args)\n        self.create_car(path=args.path, template=args.template, overwrite=args.overwrite)\n    \n    def create_car(self, path, template=\'complete\', overwrite=False):\n        """"""\n        This script sets up the folder structure for donkey to work.\n        It must run without donkey installed so that people installing with\n        docker can build the folder structure for docker to mount to.\n        """"""\n\n        #these are neeeded incase None is passed as path\n        path = path or \'~/mycar\'\n        template = template or \'complete\'\n\n\n        print(""Creating car folder: {}"".format(path))\n        path = make_dir(path)\n        \n        print(""Creating data & model folders."")\n        folders = [\'models\', \'data\', \'logs\']\n        folder_paths = [os.path.join(path, f) for f in folders]   \n        for fp in folder_paths:\n            make_dir(fp)\n            \n        #add car application and config files if they don\'t exist\n        app_template_path = os.path.join(TEMPLATES_PATH, template+\'.py\')\n        config_template_path = os.path.join(TEMPLATES_PATH, \'cfg_\' + template + \'.py\')\n        myconfig_template_path = os.path.join(TEMPLATES_PATH, \'myconfig.py\')\n        train_template_path = os.path.join(TEMPLATES_PATH, \'train.py\')\n        calibrate_template_path = os.path.join(TEMPLATES_PATH, \'calibrate.py\')\n        car_app_path = os.path.join(path, \'manage.py\')\n        car_config_path = os.path.join(path, \'config.py\')\n        mycar_config_path = os.path.join(path, \'myconfig.py\')\n        train_app_path = os.path.join(path, \'train.py\')\n        calibrate_app_path = os.path.join(path, \'calibrate.py\')        \n        \n        if os.path.exists(car_app_path) and not overwrite:\n            print(\'Car app already exists. Delete it and rerun createcar to replace.\')\n        else:\n            print(""Copying car application template: {}"".format(template))\n            shutil.copyfile(app_template_path, car_app_path)\n            \n        if os.path.exists(car_config_path) and not overwrite:\n            print(\'Car config already exists. Delete it and rerun createcar to replace.\')\n        else:\n            print(""Copying car config defaults. Adjust these before starting your car."")\n            shutil.copyfile(config_template_path, car_config_path)\n \n        if os.path.exists(train_app_path) and not overwrite:\n            print(\'Train already exists. Delete it and rerun createcar to replace.\')\n        else:\n            print(""Copying train script. Adjust these before starting your car."")\n            shutil.copyfile(train_template_path, train_app_path)\n            \n        if os.path.exists(calibrate_app_path) and not overwrite:\n            print(\'Calibrate already exists. Delete it and rerun createcar to replace.\')\n        else:\n            print(""Copying calibrate script. Adjust these before starting your car."")\n            shutil.copyfile(calibrate_template_path, calibrate_app_path)\n\n        if not os.path.exists(mycar_config_path):\n            print(""Copying my car config overrides"")\n            shutil.copyfile(myconfig_template_path, mycar_config_path)\n            #now copy file contents from config to myconfig, with all lines commented out.\n            cfg = open(car_config_path, ""rt"")\n            mcfg = open(mycar_config_path, ""at"")\n            copy = False\n            for line in cfg:\n                if ""import os"" in line:\n                    copy = True\n                if copy: \n                    mcfg.write(""# "" + line)\n            cfg.close()\n            mcfg.close()\n\n \n        print(""Donkey setup complete."")\n\n\nclass UpdateCar(BaseCommand):\n    \'\'\'\n    always run in the base ~/mycar dir to get latest\n    \'\'\'\n\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'update\', usage=\'%(prog)s [options]\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n        \n    def run(self, args):\n        cc = CreateCar()\n        cc.create_car(path=""."", overwrite=True)\n        \n\nclass FindCar(BaseCommand):\n    def parse_args(self, args):\n        pass        \n\n        \n    def run(self, args):\n        print(\'Looking up your computer IP address...\')\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((""8.8.8.8"",80))\n        ip = s.getsockname()[0] \n        print(\'Your IP address: %s \' %s.getsockname()[0])\n        s.close()\n        \n        print(""Finding your car\'s IP address..."")\n        cmd = ""sudo nmap -sP "" + ip + ""/24 | awk \'/^Nmap/{ip=$NF}/B8:27:EB/{print ip}\'""\n        print(""Your car\'s ip address is:"" )\n        os.system(cmd)\n        \n        \n        \nclass CalibrateCar(BaseCommand):    \n    \n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'calibrate\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--channel\', help=""The channel you\'d like to calibrate [0-15]"")\n        parser.add_argument(\'--address\', default=\'0x40\', help=""The i2c address you\'d like to calibrate [default 0x40]"")\n        parser.add_argument(\'--bus\', default=None, help=""The i2c bus you\'d like to calibrate [default autodetect]"")\n        parser.add_argument(\'--pwmFreq\', default=60, help=""The frequency to use for the PWM"")\n        parser.add_argument(\'--arduino\', dest=\'arduino\', action=\'store_true\', help=\'Use arduino pin for PWM (calibrate pin=<channel>)\')\n        parser.set_defaults(arduino=False)\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def run(self, args):\n        args = self.parse_args(args)\n        channel = int(args.channel)\n\n        if args.arduino == True:\n            from donkeycar.parts.actuator import ArduinoFirmata\n\n            arduino_controller = ArduinoFirmata(servo_pin=channel)\n            print(\'init Arduino PWM on pin %d\' %(channel))\n            input_prompt = ""Enter a PWM setting to test (\'q\' for quit) (0-180): ""\n        else:\n            from donkeycar.parts.actuator import PCA9685\n            from donkeycar.parts.sombrero import Sombrero\n\n            s = Sombrero()\n\n            busnum = None\n            if args.bus:\n                busnum = int(args.bus)\n            address = int(args.address, 16)\n            print(\'init PCA9685 on channel %d address %s bus %s\' %(channel, str(hex(address)), str(busnum)))\n            freq = int(args.pwmFreq)\n            print(""Using PWM freq: {}"".format(freq))\n            c = PCA9685(channel, address=address, busnum=busnum, frequency=freq)\n            input_prompt = ""Enter a PWM setting to test (\'q\' for quit) (0-1500): ""\n            print()\n        while True:\n            try:\n                val = input(input_prompt)\n                if val == \'q\' or val == \'Q\':\n                    break\n                pmw = int(val)\n                if args.arduino == True:\n                    arduino_controller.set_pulse(channel,pmw)\n                else:\n                    c.run(pmw)\n            except KeyboardInterrupt:\n                print(""\\nKeyboardInterrupt received, exit."")\n                break\n            except Exception as ex:\n                print(""Oops, {}"".format(ex))\n\n\nclass MakeMovieShell(BaseCommand):\n    \'\'\'\n    take the make movie args and then call make movie command\n    with lazy imports\n    \'\'\'\n    def __init__(self):\n        self.deg_to_rad = math.pi / 180.0\n\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'makemovie\')\n        parser.add_argument(\'--tub\', help=\'The tub to make movie from\')\n        parser.add_argument(\'--out\', default=\'tub_movie.mp4\', help=\'The movie filename to create. default: tub_movie.mp4\')\n        parser.add_argument(\'--config\', default=\'./config.py\', help=\'location of config file to use. default: ./config.py\')\n        parser.add_argument(\'--model\', default=None, help=\'the model to use to show control outputs\')\n        parser.add_argument(\'--type\', default=None, help=\'the model type to load\')\n        parser.add_argument(\'--salient\', action=""store_true"", help=\'should we overlay salient map showing activations\')\n        parser.add_argument(\'--start\', type=int, default=0, help=\'first frame to process\')\n        parser.add_argument(\'--end\', type=int, default=-1, help=\'last frame to process\')\n        parser.add_argument(\'--scale\', type=int, default=2, help=\'make image frame output larger by X mult\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args, parser\n\n    def run(self, args):\n        \'\'\'\n        Load the images from a tub and create a movie from them.\n        Movie\n        \'\'\'\n        args, parser = self.parse_args(args)\n\n        from donkeycar.management.makemovie import MakeMovie\n\n        mm = MakeMovie()\n        mm.run(args, parser)\n\n\nclass TubCheck(BaseCommand):\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'tubcheck\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'tubs\', nargs=\'+\', help=\'paths to tubs\')\n        parser.add_argument(\'--fix\', action=\'store_true\', help=\'remove problem records\')\n        parser.add_argument(\'--delete_empty\', action=\'store_true\', help=\'delete tub dir with no records\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def check(self, tub_paths, fix=False, delete_empty=False):\n        \'\'\'\n        Check for any problems. Looks at tubs and find problems in any records or images that won\'t open.\n        If fix is True, then delete images and records that cause problems.\n        \'\'\'\n        cfg = load_config(\'config.py\')\n        tubs = gather_tubs(cfg, tub_paths)\n\n        for tub in tubs:\n            tub.check(fix=fix)\n            if delete_empty and tub.get_num_records() == 0:\n                import shutil\n                print(""removing empty tub"", tub.path)\n                shutil.rmtree(tub.path)\n\n    def run(self, args):\n        args = self.parse_args(args)\n        self.check(args.tubs, args.fix, args.delete_empty)\n\n\nclass ShowHistogram(BaseCommand):\n\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'tubhist\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--tub\', nargs=\'+\', help=\'paths to tubs\')\n        parser.add_argument(\'--record\', default=None, help=\'name of record to create histogram\')\n        parser.add_argument(\'--out\', default=None, help=\'path where to save histogram end with .png\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def show_histogram(self, tub_paths, record_name, out):\n        \'\'\'\n        Produce a histogram of record type frequency in the given tub\n        \'\'\'\n        from matplotlib import pyplot as plt\n        from donkeycar.parts.datastore import TubGroup\n\n        output = out or os.path.basename(tub_paths)\n        tg = TubGroup(tub_paths=tub_paths)\n\n        if record_name is not None:\n            tg.df[record_name].hist(bins=50)\n        else:\n            tg.df.hist(bins=50)\n  \n        try:\n            if out is not None:\n                filename = output\n            else:\n                if record_name is not None:\n                    filename = output + \'_hist_%s.png\' % record_name.replace(\'/\', \'_\')\n                else:\n                    filename = output + \'_hist.png\'\n            plt.savefig(filename)\n            print(\'saving image to:\', filename)\n        except Exception as e:\n            print(e)\n        plt.show()\n\n    def run(self, args):\n        args = self.parse_args(args)\n        args.tub = \',\'.join(args.tub)\n        self.show_histogram(args.tub, args.record, args.out)\n\n\nclass ConSync(BaseCommand):\n    \'\'\'\n    continuously rsync data\n    \'\'\'\n    \n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'consync\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--dir\', default=\'./cont_data/\', help=\'paths to tubs\')\n        parser.add_argument(\'--delete\', default=\'y\', help=\'remove files locally that were deleted remotely y=yes n=no\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def run(self, args):\n        args = self.parse_args(args)\n        cfg = load_config(\'config.py\')\n        dest_dir = args.dir\n        del_arg = """"\n\n        if args.delete == \'y\':\n            reply = input(\'WARNING:this rsync operation will delete data in the target dir: %s. ok to proceeed? [y/N]: \' % dest_dir)\n\n            if reply != \'y\' and reply != ""Y"":\n                return\n            del_arg = ""--delete""\n\n        if not dest_dir[-1] == \'/\' and not dest_dir[-1] == \'\\\\\':\n            print(""Desination dir should end with a /"")\n            return\n\n        try:\n            os.mkdir(dest_dir)\n        except:\n            pass\n\n        while True:\n            command = ""rsync -aW --progress %s@%s:%s/data/ %s %s"" %\\\n                (cfg.PI_USERNAME, cfg.PI_HOSTNAME, cfg.PI_DONKEY_ROOT, dest_dir, del_arg)\n\n            os.system(command)\n            time.sleep(5)\n\n\nclass ConTrain(BaseCommand):\n    \'\'\'\n    continuously train data\n    \'\'\'\n    \n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'contrain\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--tub\', default=\'./cont_data/*\', help=\'paths to tubs\')\n        parser.add_argument(\'--model\', default=\'./models/drive.h5\', help=\'path to model\')\n        parser.add_argument(\'--transfer\', default=None, help=\'path to transfer model\')\n        parser.add_argument(\'--type\', default=\'categorical\', help=\'type of model (linear|categorical|rnn|imu|behavior|3d)\')\n        parser.add_argument(\'--aug\', action=""store_true"", help=\'perform image augmentation\')        \n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def run(self, args):\n        args = self.parse_args(args)\n        cfg = load_config(\'config.py\')\n        import sys\n        sys.path.append(\'.\')\n        from train import multi_train\n        continuous = True\n        multi_train(cfg, args.tub, args.model, args.transfer, args.type, continuous, args.aug)\n\n\nclass ShowCnnActivations(BaseCommand):\n\n    def __init__(self):\n        import matplotlib.pyplot as plt\n        self.plt = plt\n\n    def get_activations(self, image_path, model_path, cfg):\n        \'\'\'\n        Extracts features from an image\n\n        returns activations/features\n        \'\'\'\n        from tensorflow.python.keras.models import load_model, Model\n\n        model_path = os.path.expanduser(model_path)\n        image_path = os.path.expanduser(image_path)\n\n        model = load_model(model_path, compile=False)\n        image = load_scaled_image_arr(image_path, cfg)[None, ...]\n\n        conv_layer_names = self.get_conv_layers(model)\n        input_layer = model.get_layer(name=\'img_in\').input\n        activations = []      \n        for conv_layer_name in conv_layer_names:\n            output_layer = model.get_layer(name=conv_layer_name).output\n\n            layer_model = Model(inputs=[input_layer], outputs=[output_layer])\n            activations.append(layer_model.predict(image)[0])\n        return activations\n\n    def create_figure(self, activations):\n        import math\n        cols = 6\n\n        for i, layer in enumerate(activations):\n            fig = self.plt.figure()\n            fig.suptitle(\'Layer {}\'.format(i+1))\n\n            print(\'layer {} shape: {}\'.format(i+1, layer.shape))\n            feature_maps = layer.shape[2]\n            rows = math.ceil(feature_maps / cols)\n\n            for j in range(feature_maps):\n                self.plt.subplot(rows, cols, j + 1)\n\n                self.plt.imshow(layer[:, :, j])\n        \n        self.plt.show()\n\n    def get_conv_layers(self, model):\n        conv_layers = []\n        for layer in model.layers:\n            if layer.__class__.__name__ == \'Conv2D\':\n                conv_layers.append(layer.name)\n        return conv_layers\n\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'cnnactivations\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--image\', help=\'path to image\')\n        parser.add_argument(\'--model\', default=None, help=\'path to model\')\n        parser.add_argument(\'--config\', default=\'./config.py\', help=\'location of config file to use. default: ./config.py\')\n        \n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def run(self, args):\n        args = self.parse_args(args)\n        cfg = load_config(args.config)\n        activations = self.get_activations(args.image, args.model, cfg)\n        self.create_figure(activations)\n\n\nclass ShowPredictionPlots(BaseCommand):\n\n    def plot_predictions(self, cfg, tub_paths, model_path, limit, model_type):\n        \'\'\'\n        Plot model predictions for angle and throttle against data from tubs.\n\n        \'\'\'\n        import matplotlib.pyplot as plt\n        import pandas as pd\n\n        model_path = os.path.expanduser(model_path)\n        model = dk.utils.get_model_by_type(model_type, cfg)\n        # This just gets us the text for the plot title:\n        if model_type is None:\n            model_type = cfg.DEFAULT_MODEL_TYPE\n        model.load(model_path)\n\n        records = gather_records(cfg, tub_paths)\n        user_angles = []\n        user_throttles = []\n        pilot_angles = []\n        pilot_throttles = []       \n\n        records = records[:limit]\n        num_records = len(records)\n        print(\'processing %d records:\' % num_records)\n\n        for record_path in records:\n            with open(record_path, \'r\') as fp:\n                record = json.load(fp)\n            img_filename = os.path.join(tub_paths, record[\'cam/image_array\'])\n            img = load_scaled_image_arr(img_filename, cfg)\n            user_angle = float(record[""user/angle""])\n            user_throttle = float(record[""user/throttle""])\n            pilot_angle, pilot_throttle = model.run(img)\n\n            user_angles.append(user_angle)\n            user_throttles.append(user_throttle)\n            pilot_angles.append(pilot_angle)\n            pilot_throttles.append(pilot_throttle)\n\n        angles_df = pd.DataFrame({\'user_angle\': user_angles, \'pilot_angle\': pilot_angles})\n        throttles_df = pd.DataFrame({\'user_throttle\': user_throttles, \'pilot_throttle\': pilot_throttles})\n\n        fig = plt.figure()\n\n        title = ""Model Predictions\\nTubs: "" + tub_paths + ""\\nModel: "" + model_path + ""\\nType: "" + model_type\n        fig.suptitle(title)\n\n        ax1 = fig.add_subplot(211)\n        ax2 = fig.add_subplot(212)\n\n        angles_df.plot(ax=ax1)\n        throttles_df.plot(ax=ax2)\n\n        ax1.legend(loc=4)\n        ax2.legend(loc=4)\n\n        plt.savefig(model_path + \'_pred.png\')\n        plt.show()\n\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'tubplot\', usage=\'%(prog)s [options]\')\n        parser.add_argument(\'--tub\', nargs=\'+\', help=\'The tub to make plot from\')\n        parser.add_argument(\'--model\', default=None, help=\'name of record to create histogram\')\n        parser.add_argument(\'--limit\', type=int, default=1000, help=\'how many records to process\')\n        parser.add_argument(\'--type\', default=None, help=\'model type\')\n        parser.add_argument(\'--config\', default=\'./config.py\', help=\'location of config file to use. default: ./config.py\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def run(self, args):\n        args = self.parse_args(args)\n        args.tub = \',\'.join(args.tub)\n        cfg = load_config(args.config)\n        self.plot_predictions(cfg, args.tub, args.model, args.limit, args.type)\n        \n\nclass TubAugment(BaseCommand):\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'tubaugment\',\n                                         usage=\'%(prog)s [options]\')\n        parser.add_argument(\'tubs\', nargs=\'+\', help=\'paths to tubs\')\n        parser.add_argument(\'--inplace\', dest=\'inplace\', action=\'store_true\',\n                            help=\'If tub should be changed in place or new \'\n                                 \'tub will be created\')\n        parser.set_defaults(inplace=False)\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def augment(self, tub_paths, inplace=False):\n        """"""\n        :param tub_paths:   path list to tubs\n        :param inplace:     if tub should be changed or copied\n        :return:            None\n        """"""\n        cfg = load_config(\'config.py\')\n        tubs = gather_tubs(cfg, tub_paths)\n\n        for tub in tubs:\n            if inplace:\n                tub.augment_images()\n            else:\n                tub_path = tub.path\n                # remove trailing slash if exits\n                if tub_path[-1] == \'/\':\n                    tub_path = tub_path[:-1]\n                # create new tub path by inserting \'_aug\' after \'tub_XY\'\n                head, tail = os.path.split(tub_path)\n                tail_list = tail.split(\'_\')\n                tail_list.insert(2, \'aug\')\n                new_tail = \'_\'.join(tail_list)\n                new_path = os.path.join(head, new_tail)\n                # copy whole tub to new location and run augmentation\n                shutil.copytree(tub.path, new_path)\n                new_tub = Tub(new_path)\n                new_tub.augment_images()\n\n    def run(self, args):\n        args = self.parse_args(args)\n        self.augment(args.tubs, args.inplace)\n\n\ndef execute_from_command_line():\n    """"""\n    This is the function linked to the ""donkey"" terminal command.\n    """"""\n    commands = {\n            \'createcar\': CreateCar,\n            \'findcar\': FindCar,\n            \'calibrate\': CalibrateCar,\n            \'tubclean\': TubManager,\n            \'tubhist\': ShowHistogram,\n            \'tubplot\': ShowPredictionPlots,\n            \'tubcheck\': TubCheck,\n            \'tubaugment\': TubAugment,\n            \'makemovie\': MakeMovieShell,            \n            \'createjs\': CreateJoystick,\n            \'consync\': ConSync,\n            \'contrain\': ConTrain,\n            \'cnnactivations\': ShowCnnActivations,\n            \'update\': UpdateCar\n                }\n    \n    args = sys.argv[:]\n\n    if len(args) > 1 and args[1] in commands.keys():\n        command = commands[args[1]]\n        c = command()\n        c.run(args[2:])\n    else:\n        dk.utils.eprint(\'Usage: The available commands are:\')\n        dk.utils.eprint(list(commands.keys()))\n        \n    \nif __name__ == ""__main__"":\n    execute_from_command_line()\n'"
donkeycar/management/joystick_creator.py,0,"b'import sys\nimport os\nimport argparse\nimport json\nimport time\nimport math\n\nfrom donkeycar.parts.datastore import Tub\nfrom donkeycar.utils import *\nfrom donkeycar.parts.controller import JoystickCreatorController\n\ntry:\n    from prettytable import PrettyTable\nexcept:\n    print(""need: pip install PrettyTable"")\n\nclass CreateJoystick(object):\n\n    def __init__(self):\n        self.last_button = None\n        self.last_axis = None\n        self.axis_val = 0\n        self.running = False\n        self.thread = None\n        self.gyro_axis = []\n        self.axis_map = []\n        self.ignore_axis = False\n        self.mapped_controls = []\n\n    def poll(self):\n        while self.running:\n            button, button_state, axis, axis_val = self.js.poll()\n\n            if button is not None:\n                self.last_button = button\n                self.last_axis = None\n                self.axis_val = 0.0\n            elif axis is not None and not self.ignore_axis:\n                if not axis in self.gyro_axis:\n                    self.last_axis = axis\n                    self.last_button = None\n                    self.axis_val = axis_val\n\n    def get_button_press(self, duration=10.0):\n        self.last_button = None\n\n        start = time.time()\n\n        while self.last_button is None and time.time() - start < duration:\n            time.sleep(0.1)\n\n        return self.last_button\n\n    def get_axis_move(self, duration=2.0):\n        self.last_axis = None\n        axis_samples = {}\n\n        start = time.time()\n\n        while time.time() - start < duration:\n            if self.last_axis:\n                if self.last_axis in axis_samples:\n                    try:\n                        axis_samples[self.last_axis] = axis_samples[self.last_axis] + math.fabs(self.axis_val)\n                    except:\n                        try:\n                            axis_samples[self.last_axis] = math.fabs(self.axis_val)\n                        except:\n                            pass\n                else:\n                    axis_samples[self.last_axis] = math.fabs(self.axis_val)\n            \n        most_movement = None\n        most_val = 0\n        for key, value in axis_samples.items():\n            if value > most_val:\n                most_movement = key\n                most_val = value\n\n        return most_movement\n\n    def clear_scr(self):\n        print(chr(27) + ""[2J"")\n\n    def create_joystick(self, args):\n        \n        self.clear_scr()\n        print(""##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##"")\n        print(""## Welcome to Joystick Creator Wizard. ##"")\n        print(""##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##"")\n        print(""This will generate code to use your joystick with a Donkey car."")\n        print()\n        print(""Overview:"")\n        print()\n        print(""First we name each button, then each axis control."")\n        print(""Next we map names to actions."")\n        print(""Finally we output a python file you can use in your project."")\n        print()\n        input(\'Hit Enter to continue\')\n        self.clear_scr()\n        print(""Please plug-in your controller via USB or bluetooth. Make sure status lights are on and device is mapped."")\n        input(\'Enter to continue \')\n        self.clear_scr()\n        \n        self.init_js_device()\n        print()\n        \n        self.init_polling_js()\n        self.clear_scr()\n\n        self.find_gyro()\n        self.clear_scr()\n\n        self.explain_config()\n        self.clear_scr()\n\n        self.name_buttons()\n        self.clear_scr()\n\n        self.name_axes()\n        self.clear_scr()\n\n        self.map_steering_throttle()\n        self.clear_scr()\n\n        self.map_button_controls()\n        self.clear_scr()\n\n        self.revisit_topic()\n        self.clear_scr()\n\n        self.write_python_class_file()\n\n        print(""Check your new python file to see the controller implementation. Import this in manage.py and use for control."")\n\n        self.shutdown()\n\n    def init_js_device(self):\n        from donkeycar.parts.controller import JoystickCreatorController\n\n        js_cr = None\n\n        #Get device file and create js creator helper class\n        while js_cr is None:\n            print(""Where can we find the device file for your joystick?"")\n            dev_fn = input(""Hit Enter for default: /dev/input/js0 or type alternate path: "")\n            if len(dev_fn) is 0:\n                dev_fn = \'/dev/input/js0\'\n\n            print()\n            print(""Attempting to open device at that file..."")\n            try:\n                js_cr = JoystickCreatorController(dev_fn=dev_fn)\n                res = js_cr.init_js()\n                if res:\n                    print(""Found and accessed input device."")\n                else:\n                    js_cr = None\n            except Exception as e:\n                print(""threw exception:"" + str(e))\n                js_cr = None\n\n            if js_cr is None:\n                ret = input(""Failed to open device. try again? [Y/n] : "")\n                if ret.upper() == ""N"":\n                    exit(0)\n\n        self.js = js_cr.js\n        input(""Hit Enter to continue"")\n\n    def init_polling_js(self):\n        self.running = True\n        import threading\n        self.thread = threading.Thread(target=self.poll)\n        self.thread.daemon = True\n        self.thread.start()\n\n    def find_gyro(self):\n        print(""Next we are going to look for gyroscope data."")\n        input(""For 5 seconds, move controller and rotate on each axis. Hit Enter then start moving: "")\n        start = time.time()\n        while time.time() - start < 5.0:\n            if self.last_axis is not None and not self.last_axis in self.gyro_axis:\n                self.gyro_axis.append(self.last_axis)\n\n        print()\n        if len(self.gyro_axis) > 0:\n            print(""Ok, we found %d axes that stream gyroscope data. We will ignore those during labelling and mapping."" % len(self.gyro_axis))\n        else:\n            print(""Ok, we didn\'t see any events. So perhaps your controller doesn\'t emit gyroscope data. No problem."")\n        \n        input(""Hit Enter to continue "")\n\n    def get_code_from_button(self, button):\n        code = button\n        if \'unknown\' in button:\n            try:\n                code_str = button.split(\'(\')[1][:-1]\n                code = int(code_str, 16)\n            except Exception as e:\n                code = None\n                print(""failed to parse code"", str(e))\n        return code\n\n    def explain_config(self):\n        print(""We will display the current progress in this set of tables:"")\n        print()\n        self.print_config()\n        print(""\\nAs you name buttons and map them to controls this table will be updated."")\n        input(""Hit enter to continue"")\n\n\n    def name_buttons(self):\n        done = False\n        self.ignore_axis = True\n\n        self.print_config()\n\n        print(\'Next we will give every button a name. Not analog yet. We will do that next.\')\n\n        while not done:\n\n            print(\'Tap a button to name it.\')\n            \n            self.get_button_press()\n\n            if self.last_button is None:\n                print(""No button was pressed in last 10 seconds. It\'s possible that your buttons all generate axis commands."")\n                ret = input(""Keep mapping buttons? [Y, n]"")\n                if ret == \'n\':\n                    break\n            elif \'unknown\' in self.last_button:\n                code = self.get_code_from_button(self.last_button)\n\n                if code is not None:\n                    if code in self.js.button_names:\n                        ret = input(""This button has a name: %s. Are you done naming? (y/N) "" % self.js.button_names[code])\n                        if ret.upper() == ""Y"":\n                            done = True\n                            break\n                    label = input(""What name to give to this button:"")\n                    if len(label) == 0:\n                        print(""No name given. skipping."")\n                    else:\n                        self.clear_scr()\n                        self.js.button_names[code] = label\n                        self.print_config()\n            else:\n                print(\'got press: \', self.last_button)\n\n            self.clear_scr()\n            self.print_config()\n            \n\n    def print_config(self):\n        pt = PrettyTable()\n        pt.field_names = [""button code"", ""button name""]\n        for key, value in self.js.button_names.items():\n            pt.add_row([str(hex(key)), str(value)])\n        print(""Button Map:"")\n        print(pt)\n\n        pt = PrettyTable()\n        pt.field_names = [""axis code"", ""axis name""]\n        for key, value in self.js.axis_names.items():\n            pt.add_row([str(hex(key)), str(value)])\n        print(""Axis Map:"")\n        print(pt)\n\n        pt = PrettyTable()\n        pt.field_names = [""control"", ""action""]\n        for button, control in self.mapped_controls:\n            pt.add_row([button, control])\n        for axis, control in self.axis_map:\n            pt.add_row([axis, control])\n        print(""Control Map:"")\n        print(pt)\n\n    def name_axes(self):\n        self.print_config()\n        print()\n        print(\'Next we are going to name all the axis you would like to use.\')\n\n        done = False\n        self.ignore_axis = False\n\n        while not done:\n            print(\'Prepare to move one axis on the controller for 2 sec.\')\n            ret = input(""Hit Enter to begin. D when done. "")\n            if ret.upper() == \'D\':\n                break\n            \n            most_movement = self.get_axis_move()\n\n            if most_movement is None:\n                print(""Didn\'t detect any movement."")\n                res = input(""Try again? [Y/n]: "")\n                if res == ""n"":\n                    done = True\n                    break\n                else:\n                    continue\n\n            if \'unknown\' in most_movement:\n                code_str = most_movement.split(\'(\')[1][:-1]\n                print(\'Most movement on axis code:\', code_str)\n                try:\n                    code = int(code_str, 16)\n                except Exception as e:\n                    code = None\n                    print(""Failed to parse code"", str(e))\n\n                if code is not None:\n                    label = input(""What name to give to this axis: (D when done) "")\n                    if len(label) == 0:\n                        print(""No name given. skipping."")\n                    elif label.upper() == \'D\':\n                        done = True\n                    else:\n                        self.js.axis_names[code] = label\n                        self.clear_scr()\n                        self.print_config()\n            else:\n                print(\'Got axis: \', self.last_axis)\n            print()\n\n    def write_python_class_file(self):\n        pyth_filename = None\n        outfile = None\n        while pyth_filename is None:\n            print(""Now we will write these values to a new python file."")\n            pyth_filename = input(""What is the name of python file to create joystick code? [default: my_joystick.py]"")\n            if len(pyth_filename) == 0:\n                pyth_filename = \'my_joystick.py\'\n            print(\'using filename:\', pyth_filename)\n            print()\n            try:\n                outfile = open(pyth_filename, ""wt"")\n            except:\n                ret = input(""failed to open filename. Enter another filename? [Y,n]"")\n                if ret == ""n"":\n                    break\n                pyth_filename = None\n            print()\n            \n        if outfile is not None:\n            classname = input(""What is the name of joystick class? [default: MyJoystick] "")\n            if len(classname) == 0:\n                classname = ""MyJoystick""\n            file_header = \\\n            \'\'\'\nfrom donkeycar.parts.controller import Joystick, JoystickController\n\n\nclass %s(Joystick):\n    #An interface to a physical joystick available at /dev/input/js0\n    def __init__(self, *args, **kwargs):\n        super(%s, self).__init__(*args, **kwargs)\n\n            \\n\'\'\' % (classname, classname )\n\n            outfile.write(file_header)\n\n            outfile.write(\'        self.button_names = {\\n\')\n            for key, value in self.js.button_names.items():\n                outfile.write(""            %s : \'%s\',\\n"" % (str(hex(key)), str(value)))\n            outfile.write(\'        }\\n\\n\\n\')\n            \n            outfile.write(\'        self.axis_names = {\\n\')\n\n            for key, value in self.js.axis_names.items():\n                outfile.write(""            %s : \'%s\',\\n"" % (str(hex(key)), str(value)))\n            outfile.write(\'        }\\n\\n\\n\')\n\n            js_controller = \\\n            \'\'\'\nclass %sController(JoystickController):\n    #A Controller object that maps inputs to actions\n    def __init__(self, *args, **kwargs):\n        super(%sController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        #attempt to init joystick\n        try:\n            self.js = %s(self.dev_fn)\n            self.js.init()\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        #init set of mapping from buttons to function calls\n            \\n\'\'\' % (classname, classname, classname)\n\n            outfile.write(js_controller)\n\n            outfile.write(\'        self.button_down_trigger_map = {\\n\')\n            for button, control in self.mapped_controls:\n                outfile.write(""            \'%s\' : self.%s,\\n"" % (str(button), str(control)))\n            outfile.write(\'        }\\n\\n\\n\')\n            \n            outfile.write(\'        self.axis_trigger_map = {\\n\')\n            for axis, control in self.axis_map:\n                outfile.write(""            \'%s\' : self.%s,\\n"" % (str(axis), str(control)))\n            outfile.write(\'        }\\n\\n\\n\')\n\n            outfile.close()\n            print(pyth_filename, ""written."")\n\n    def map_control_axis(self, control_name, control_fn):\n        while True:\n            axis = self.get_axis_action(\'Move the controller axis you wish to use for %s. Continue moving for 2 seconds.\' % control_name)\n            \n            mapped = False\n\n            if axis is None:\n                print(""No mapping for %s."" % control_name)\n            else:\n                #print(""axis"", axis)\n                code = self.get_code_from_button(axis)\n                for key, value in self.js.axis_names.items():\n                    #print(\'key\', key, \'value\', value)\n                    if key == code or value == code:\n                        print(\'Mapping %s to %s.\\n\' % (value, control_name))\n                        mapped = value\n                        break\n            if mapped:\n                ret = input(\'Is this mapping ok? (y, N) \')\n                if ret.upper() == \'Y\':\n                    self.axis_map.append((mapped, control_fn))\n                    return\n            else:\n                ret = input(\'axis not recognized. try again? (Y, n) \')\n                if ret.upper() == \'N\':\n                    return\n\n\n    def map_steering_throttle(self):\n\n        self.axis_map = []\n\n        self.print_config()\n        print()\n        print(\'Now we will create a mapping of controls to actions.\\n\')\n\n        print(""First steering."")\n        self.map_control_axis(""steering"", ""set_steering"")\n\n        self.clear_scr()\n        self.print_config()\n        print()\n        print(""Next throttle."")\n        self.map_control_axis(""throttle"", ""set_throttle"")\n\n\n    def map_button_controls(self):\n        unmapped_controls = [\\\n            (\'toggle_mode\',\'changes the drive mode between user, local, and local_angle\'),\n            (\'erase_last_N_records\',\'erases the last 100 records while driving\'),\n            (\'emergency_stop\',\'executes a full back throttle to bring car to a quick stop\'),\n            (\'increase_max_throttle\',\'increases the max throttle, also used for constant throttle val\'),\n            (\'decrease_max_throttle\',\'decreases the max throttle, also used for constant throttle val\'),\n            (\'toggle_constant_throttle\', \'toggle the mode of supplying constant throttle\'),\n            (\'toggle_manual_recording\',\'toggles recording records on and off\')\n        ]\n        \n        self.mapped_controls = []\n        self.print_config()\n        print()\n        print(""Next we are going to assign button presses to controls."")\n        print()\n\n        while len(unmapped_controls) > 0:\n\n            pt = PrettyTable()\n            pt.field_names = [\'Num\', \'Control\', \'Help\']\n            print(""Unmapped Controls:"")\n            for i, td in enumerate(unmapped_controls):\n                control, help = td\n                pt.add_row([i + 1, control, help])\n            print(pt)\n\n            print()\n            try:\n                ret = "" ""\n                while (not ret.isdigit() and ret.upper() != \'D\') or (ret.isdigit() and (int(ret) < 1 or int(ret) > len(unmapped_controls))):\n                    ret = input(""Press the number of control to map (1-%d). D when done. "" % len(unmapped_controls))\n\n                if ret.upper() == \'D\':\n                    break\n\n                iControl = int(ret) - 1\n            except:\n                continue\n\n            \n            print(\'Press the button to map to control:\', unmapped_controls[iControl][0])\n            self.get_button_press()\n\n            if self.last_button is None:\n                print(""No button was pressed in last 10 seconds."")\n                ret = input(""Keep mapping commands? [Y, n]"")\n                if ret == \'n\':\n                    break\n            else:\n                code = self.get_code_from_button(self.last_button)\n                if code in self.js.button_names: \n                    button_name = self.js.button_names[code]\n                else:\n                    button_name = self.last_button\n                self.mapped_controls.append((button_name, unmapped_controls[iControl][0]))\n                unmapped_controls.pop(iControl)\n                self.clear_scr()\n                self.print_config()\n                print()\n\n        print(\'done mapping controls\')\n        print()\n\n    def revisit_topic(self):\n        done = False\n        while not done:\n            self.clear_scr()\n            self.print_config()\n            print(""Now we are nearly done! Are you happy with this config or would you like to revisit a topic?"")\n            print(""H)appy, please continue to write out python file."")\n            print(""B)uttons need renaming."")\n            print(""A)xes need renaming."")\n            print(""T)hrottle and steering need remap."")\n            print(""R)emap buttons to controls."")\n            \n            ret = input(""Select option "").upper()\n            if ret == \'H\':\n                done = True\n            elif ret == \'B\':\n                self.name_buttons()\n            elif ret == \'A\':\n                self.name_axes()\n            elif ret == \'T\':\n                self.map_steering_throttle()\n            elif ret == \'R\':\n                self.map_button_controls()          \n\n\n    def get_axis_action(self, prompt):\n        done = False        \n        while not done:\n            print(prompt)\n            ret = input(""Hit Enter to begin. D when done. "")\n            if ret.upper() == \'D\':\n                return None\n\n            most_movement = self.get_axis_move()\n\n            if most_movement is None:\n                print(""Didn\'t detect any movement."")\n                res = input(""Try again? [Y/n]: "")\n                if res == ""n"":\n                    return None\n                else:\n                    continue\n            else:\n                return most_movement\n\n\n    def shutdown(self):\n        self.running = False\n        if self.thread:\n            self.thread = None\n\n    def parse_args(self, args):\n        parser = argparse.ArgumentParser(prog=\'createjs\', usage=\'%(prog)s [options]\')\n        parsed_args = parser.parse_args(args)\n        return parsed_args\n\n    def run(self, args):\n        args = self.parse_args(args)\n        try:\n            self.create_joystick(args)\n        except KeyboardInterrupt:\n            self.shutdown()\n\n'"
donkeycar/management/makemovie.py,0,"b'import moviepy.editor as mpy\nfrom tensorflow.python.keras import activations\nfrom tensorflow.python.keras import backend as K\n\ntry:\n    from vis.visualization import visualize_saliency, overlay\n    from vis.utils import utils\n    from vis.backprop_modifiers import get\n    from vis.losses import ActivationMaximization\n    from vis.optimizer import Optimizer\nexcept:\n    raise Exception(""Please install keras-vis: pip install git+https://github.com/autorope/keras-vis.git"")\n\nimport donkeycar as dk\nfrom donkeycar.parts.datastore import Tub\nfrom donkeycar.utils import *\n\n\nclass MakeMovie(object):\n    def __init__(self):\n        self.deg_to_rad = math.pi / 180.0\n\n    def run(self, args, parser):\n        \'\'\'\n        Load the images from a tub and create a movie from them.\n        Movie\n        \'\'\'\n\n        if args.tub is None:\n            print(""ERR>> --tub argument missing."")\n            parser.print_help()\n            return\n\n        if args.type is None and args.model is not None:\n            print(""ERR>> --type argument missing. Required when providing a model."")\n            parser.print_help()\n            return\n\n        if args.salient:\n            if args.model is None:\n                print(""ERR>> salient visualization requires a model. Pass with the --model arg."")\n                parser.print_help()\n\n        conf = os.path.expanduser(args.config)\n        if not os.path.exists(conf):\n            print(""No config file at location: %s. Add --config to specify\\\n                 location or run from dir containing config.py."" % conf)\n            return\n\n        self.cfg = dk.load_config(conf)\n        self.tub = Tub(args.tub)\n        self.index = self.tub.get_index(shuffled=False)\n        start = args.start\n        self.end = args.end if args.end != -1 else len(self.index)\n        if self.end >= len(self.index):\n            self.end = len(self.index) - 1\n        num_frames = self.end - start\n        self.iRec = start\n        self.scale = args.scale\n        self.keras_part = None\n        self.do_salient = False\n        if args.model is not None:\n            self.keras_part = get_model_by_type(args.type, cfg=self.cfg)\n            self.keras_part.load(args.model)\n            self.keras_part.compile()\n            if args.salient:\n                self.do_salient = self.init_salient(self.keras_part.model)\n\n        print(\'making movie\', args.out, \'from\', num_frames, \'images\')\n        clip = mpy.VideoClip(self.make_frame,\n                             duration=((num_frames - 1) / self.cfg.DRIVE_LOOP_HZ))\n        clip.write_videofile(args.out, fps=self.cfg.DRIVE_LOOP_HZ)\n\n    def draw_user_input(self, record, img):\n        \'\'\'\n        Draw the user input as a green line on the image\n        \'\'\'\n\n        import cv2\n\n        user_angle = float(record[""user/angle""])\n        user_throttle = float(record[""user/throttle""])\n\n        height, width, _ = img.shape\n\n        length = height\n        a1 = user_angle * 45.0\n        l1 = user_throttle * length\n\n        mid = width // 2 - 1\n\n        p1 = tuple((mid - 2, height - 1))\n        p11 = tuple((int(p1[0] + l1 * math.cos((a1 + 270.0) * self.deg_to_rad)),\n                     int(p1[1] + l1 * math.sin((a1 + 270.0) * self.deg_to_rad))))\n\n        # user is green, pilot is blue\n        cv2.line(img, p1, p11, (0, 255, 0), 2)\n        \n    def draw_model_prediction(self, record, img):\n        \'\'\'\n        query the model for it\'s prediction, draw the predictions\n        as a blue line on the image\n        \'\'\'\n        if self.keras_part is None:\n            return\n\n        import cv2\n         \n        expected = self.keras_part.model.inputs[0].shape[1:]\n        actual = img.shape\n\n        # normalize image before prediction\n        pred_img = img.astype(np.float32) / 255.0\n\n        # check input depth\n        if expected[2] == 1 and actual[2] == 3:\n            pred_img = rgb2gray(pred_img)\n            pred_img = pred_img.reshape(pred_img.shape + (1,))\n            actual = pred_img.shape\n\n        if expected != actual:\n            print(""expected input dim"", expected, ""didn\'t match actual dim"", actual)\n            return\n\n        pilot_angle, pilot_throttle = self.keras_part.run(pred_img)\n        height, width, _ = pred_img.shape\n\n        length = height\n        a2 = pilot_angle * 45.0\n        l2 = pilot_throttle * length\n\n        mid = width // 2 - 1\n\n        p2 = tuple((mid + 2, height - 1))\n        p22 = tuple((int(p2[0] + l2 * math.cos((a2 + 270.0) * self.deg_to_rad)),\n                     int(p2[1] + l2 * math.sin((a2 + 270.0) * self.deg_to_rad))))\n\n        # user is green, pilot is blue\n        cv2.line(img, p2, p22, (0, 0, 255), 2)\n\n    def draw_steering_distribution(self, record, img):\n        \'\'\'\n        query the model for it\'s prediction, draw the distribution of steering choices\n        \'\'\'\n        from donkeycar.parts.keras import KerasCategorical\n\n        if self.keras_part is None or type(self.keras_part) is not KerasCategorical:\n            return\n\n        import cv2\n\n        pred_img = img.reshape((1,) + img.shape)\n        angle_binned, _ = self.keras_part.model.predict(pred_img)\n\n        x = 4\n        dx = 4\n        y = 120 - 4\n        iArgMax = np.argmax(angle_binned)\n        for i in range(15):\n            p1 = (x, y)\n            p2 = (x, y - int(angle_binned[0][i] * 100.0))\n            if i == iArgMax:\n                cv2.line(img, p1, p2, (255, 0, 0), 2)\n            else:\n                cv2.line(img, p1, p2, (200, 200, 200), 2)\n            x += dx\n\n    def init_salient(self, model):\n        # Utility to search for layer index by name. \n        # Alternatively we can specify this as -1 since it corresponds to the last layer.\n        first_output_name = None\n        for i, layer in enumerate(model.layers):\n            if first_output_name is None and ""dropout"" not in layer.name.lower() and ""out"" in layer.name.lower():\n                first_output_name = layer.name\n                layer_idx = i\n\n        if first_output_name is None:\n            print(""Failed to find the model layer named with \'out\'. Skipping salient."")\n            return False\n\n        print(""####################"")\n        print(""Visualizing activations on layer:"", first_output_name)\n        print(""####################"")\n        \n        # ensure we have linear activation\n        model.layers[layer_idx].activation = activations.linear\n        # build salient model and optimizer\n        sal_model = utils.apply_modifications(model)\n        modifier_fn = get(\'guided\')\n        sal_model_mod = modifier_fn(sal_model)\n        losses = [\n            (ActivationMaximization(sal_model_mod.layers[layer_idx], None), -1)\n        ]\n        self.opt = Optimizer(sal_model_mod.input, losses, norm_grads=False)\n        return True\n\n    def compute_visualisation_mask(self, img):\n        grad_modifier = \'absolute\'\n        grads = self.opt.minimize(seed_input=img, max_iter=1, grad_modifier=grad_modifier, verbose=False)[1]\n        channel_idx = 1 if K.image_data_format() == \'channels_first\' else -1\n        grads = np.max(grads, axis=channel_idx)\n        res = utils.normalize(grads)[0]\n        return res\n\n    def draw_salient(self, img):\n        import cv2\n        alpha = 0.004\n        beta = 1.0 - alpha\n\n        expected = self.keras_part.model.inputs[0].shape[1:]\n        actual = img.shape\n        pred_img = img.astype(np.float32) / 255.0\n\n        # check input depth\n        if expected[2] == 1 and actual[2] == 3:\n            pred_img = rgb2gray(pred_img)\n            pred_img = pred_img.reshape(pred_img.shape + (1,))\n\n        salient_mask = self.compute_visualisation_mask(pred_img)\n        z = np.zeros_like(salient_mask)\n        salient_mask_stacked = np.dstack((z, z))\n        salient_mask_stacked = np.dstack((salient_mask_stacked, salient_mask))\n        blend = cv2.addWeighted(img.astype(\'float32\'), alpha, salient_mask_stacked, beta, 0.0)\n        return blend\n\n    def make_frame(self, t):\n        \'\'\'\n        Callback to return an image from from our tub records.\n        This is called from the VideoClip as it references a time.\n        We don\'t use t to reference the frame, but instead increment\n        a frame counter. This assumes sequential access.\n        \'\'\'\n\n        if self.iRec >= self.end or self.iRec >= len(self.index):\n            return None\n\n        rec_ix = self.index[self.iRec]\n        rec = self.tub.get_record(rec_ix)\n        image = rec[\'cam/image_array\']\n\n        if self.cfg.ROI_CROP_TOP != 0 or self.cfg.ROI_CROP_BOTTOM != 0:\n            image = img_crop(image, self.cfg.ROI_CROP_TOP, self.cfg.ROI_CROP_BOTTOM)\n\n        if self.do_salient:\n            image = self.draw_salient(image)\n            image = image * 255\n            image = image.astype(\'uint8\')\n        \n        self.draw_user_input(rec, image)\n        if self.keras_part is not None:\n            self.draw_model_prediction(rec, image)\n            self.draw_steering_distribution(rec, image)\n\n        if self.scale != 1:\n            import cv2\n            h, w, d = image.shape\n            dsize = (w * self.scale, h * self.scale)\n            image = cv2.resize(image, dsize=dsize, interpolation=cv2.INTER_CUBIC)\n\n        self.iRec += 1\n        # returns a 8-bit RGB array\n        return image\n'"
donkeycar/management/tub.py,0,"b'\'\'\'\ntub.py\n\nManage tubs\n\'\'\'\n\nimport os, sys, time\nimport json\nimport tornado.web\nfrom stat import S_ISREG, ST_MTIME, ST_MODE, ST_CTIME, ST_ATIME\n\n\nclass TubManager:\n\n    def run(self, args):\n        WebServer(args[0]).start()\n\n\nclass WebServer(tornado.web.Application):\n\n    def __init__(self, data_path):\n        if not os.path.exists(data_path):\n            raise ValueError(\'The path {} does not exist.\'.format(data_path))\n\n        this_dir = os.path.dirname(os.path.realpath(__file__))\n        static_file_path = os.path.join(this_dir, \'tub_web\', \'static\')\n\n\n\n        handlers = [\n            (r""/"", tornado.web.RedirectHandler, dict(url=""/tubs"")),\n            (r""/tubs"", TubsView, dict(data_path=data_path)),\n            (r""/tubs/?(?P<tub_id>[^/]+)?"", TubView),\n            (r""/api/tubs/?(?P<tub_id>[^/]+)?"", TubApi, dict(data_path=data_path)),\n            (r""/static/(.*)"", tornado.web.StaticFileHandler, {""path"": static_file_path}),\n            (r""/tub_data/(.*)"", tornado.web.StaticFileHandler, {""path"": data_path}),\n            ]\n\n        settings = {\'debug\': True}\n\n        super().__init__(handlers, **settings)\n\n    def start(self, port=8886):\n        self.port = int(port)\n        self.listen(self.port)\n        print(\'Listening on {}...\'.format(port))\n        tornado.ioloop.IOLoop.instance().start()\n\n\nclass TubsView(tornado.web.RequestHandler):\n\n    def initialize(self, data_path):\n        self.data_path = data_path\n\n    def get(self):\n        import fnmatch\n        dir_list = fnmatch.filter(os.listdir(self.data_path), \'*\')\n        dir_list.sort()\n        data = {""tubs"": dir_list}\n        self.render(""tub_web/tubs.html"", **data)\n\n\nclass TubView(tornado.web.RequestHandler):\n\n    def get(self, tub_id):\n        data = {}\n        self.render(""tub_web/tub.html"", **data)\n\n\nclass TubApi(tornado.web.RequestHandler):\n\n    def initialize(self, data_path):\n        self.data_path = data_path\n\n    def image_path(self, tub_path, frame_id):\n        return os.path.join(tub_path, str(frame_id) + ""_cam-image_array_.jpg"")\n\n    def record_path(self, tub_path, frame_id):\n        return os.path.join(tub_path, ""record_"" + frame_id + "".json"")\n\n    def clips_of_tub(self, tub_path):\n        seqs = [ int(f.split(""_"")[0]) for f in os.listdir(tub_path) if f.endswith(\'.jpg\') ]\n        seqs.sort()\n\n        entries = ((os.stat(self.image_path(tub_path, seq))[ST_ATIME], seq) for seq in seqs)\n\n        (last_ts, seq) = next(entries)\n        clips = [[seq]]\n        for next_ts, next_seq in entries:\n            #if next_ts - last_ts > 100:  #greater than 1s apart\n            #    clips.append([next_seq])\n            #else:\n            #    clips[-1].append(next_seq)\n            clips[-1].append(next_seq)\n            last_ts = next_ts\n\n        return clips\n\n    def get(self, tub_id):\n        clips = self.clips_of_tub(os.path.join(self.data_path, tub_id))\n\n        self.set_header(""Content-Type"", ""application/json; charset=UTF-8"")\n        self.write(json.dumps({\'clips\': clips}))\n\n    def post(self, tub_id):\n        tub_path = os.path.join(self.data_path, tub_id)\n        old_clips = self.clips_of_tub(tub_path)\n        new_clips = tornado.escape.json_decode(self.request.body)\n\n        import itertools\n        old_frames = list(itertools.chain(*old_clips))\n        new_frames = list(itertools.chain(*new_clips[\'clips\']))\n        frames_to_delete = [str(item) for item in old_frames if item not in new_frames]\n        for frm in frames_to_delete:\n            os.remove(self.record_path(tub_path, frm))\n            os.remove(self.image_path(tub_path, frm))\n'"
donkeycar/parts/actuator.py,0,"b'""""""\nactuators.py\nClasses to control the motors and servos. These classes \nare wrapped in a mixer class before being used in the drive loop.\n""""""\n\nimport time\n\nimport donkeycar as dk\n\n        \nclass PCA9685:\n    \'\'\' \n    PWM motor controler using PCA9685 boards. \n    This is used for most RC Cars\n    \'\'\'\n    def __init__(self, channel, address=0x40, frequency=60, busnum=None, init_delay=0.1):\n\n        self.default_freq = 60\n        self.pwm_scale = frequency / self.default_freq\n\n        import Adafruit_PCA9685\n        # Initialise the PCA9685 using the default address (0x40).\n        if busnum is not None:\n            from Adafruit_GPIO import I2C\n            # replace the get_bus function with our own\n            def get_bus():\n                return busnum\n            I2C.get_default_bus = get_bus\n        self.pwm = Adafruit_PCA9685.PCA9685(address=address)\n        self.pwm.set_pwm_freq(frequency)\n        self.channel = channel\n        time.sleep(init_delay) # ""Tamiya TBLE-02"" makes a little leap otherwise\n\n    def set_pulse(self, pulse):\n        try:\n            self.pwm.set_pwm(self.channel, 0, int(pulse * self.pwm_scale))\n        except:\n            self.pwm.set_pwm(self.channel, 0, int(pulse * self.pwm_scale))\n\n    def run(self, pulse):\n        self.set_pulse(pulse)\n\n\nclass PiGPIO_PWM():\n    \'\'\'\n    # Use the pigpio python module and daemon to get hardware pwm controls from\n    # a raspberrypi gpio pins and no additional hardware. Can serve as a replacement\n    # for PCA9685.\n    #\n    # Install and setup:\n    # sudo update && sudo apt install pigpio python3-pigpio\n    # sudo systemctl start pigpiod\n    #\n    # Note: the range of pulses will differ from those required for PCA9685\n    # and can range from 12K to 170K\n    #\n    # If you use a control circuit that inverts the steering signal, set inverted to True\n    # Default multipler for pulses from config etc is 100\n    \'\'\'\n\n    def __init__(self, pin, pgio=None, freq=75, inverted=False):\n        import pigpio\n\n        self.pin = pin\n        self.pgio = pgio or pigpio.pi()\n        self.freq = freq\n        self.inverted = inverted\n        self.pgio.set_mode(self.pin, pigpio.OUTPUT)\n\n    def __del__(self):\n        self.pgio.stop()\n\n    def set_pulse(self, pulse):\n        self.pgio.hardware_PWM(self.pin, self.freq, int(pulse if self.inverted == False else 1e6 - pulse))\n\n    def run(self, pulse):\n        self.set_pulse(pulse)\n\n\nclass JHat:\n    \'\'\' \n    PWM motor controler using Teensy emulating PCA9685. \n    \'\'\'\n    def __init__(self, channel, address=0x40, frequency=60, busnum=None):\n        print(""Firing up the Hat"")\n        import Adafruit_PCA9685\n        LED0_OFF_L = 0x08\n        # Initialise the PCA9685 using the default address (0x40).\n        if busnum is not None:\n            from Adafruit_GPIO import I2C\n            # replace the get_bus function with our own\n            def get_bus():\n                return busnum\n            I2C.get_default_bus = get_bus\n        self.pwm = Adafruit_PCA9685.PCA9685(address=address)\n        self.pwm.set_pwm_freq(frequency)\n        self.channel = channel\n        self.register = LED0_OFF_L+4*channel\n\n        # we install our own write that is more efficient use of interrupts\n        self.pwm.set_pwm = self.set_pwm\n        \n    def set_pulse(self, pulse):\n        self.set_pwm(self.channel, 0, pulse) \n\n    def set_pwm(self, channel, on, off):\n        # sets a single PWM channel\n        self.pwm._device.writeList(self.register, [off & 0xFF, off >> 8])\n        \n    def run(self, pulse):\n        self.set_pulse(pulse)\n\nclass JHatReader:\n    \'\'\' \n    Read RC controls from teensy \n    \'\'\'\n    def __init__(self, channel, address=0x40, frequency=60, busnum=None):\n        import Adafruit_PCA9685\n        self.pwm = Adafruit_PCA9685.PCA9685(address=address)\n        self.pwm.set_pwm_freq(frequency)\n        self.register = 0 #i2c read doesn\'t take an address\n        self.steering = 0\n        self.throttle = 0\n        self.running = True\n        #send a reset\n        self.pwm._device.writeRaw8(0x06)\n\n    def read_pwm(self):\n        \'\'\'\n        send read requests via i2c bus to Teensy to get\n        pwm control values from last RC input  \n        \'\'\'\n        h1 = self.pwm._device.readU8(self.register)\n        # first byte of header must be 100, otherwize we might be reading\n        # in the wrong byte offset\n        while h1 != 100:\n            print(""skipping to start of header"")\n            h1 = self.pwm._device.readU8(self.register)\n        \n        h2 = self.pwm._device.readU8(self.register)\n        # h2 ignored now\n\n        val_a = self.pwm._device.readU8(self.register)\n        val_b = self.pwm._device.readU8(self.register)\n        self.steering = (val_b << 8) + val_a\n        \n        val_c = self.pwm._device.readU8(self.register)\n        val_d = self.pwm._device.readU8(self.register)\n        self.throttle = (val_d << 8) + val_c\n\n        # scale the values from -1 to 1\n        self.steering = (((float)(self.steering)) - 1500.0) / 500.0  + 0.158\n        self.throttle = (((float)(self.throttle)) - 1500.0) / 500.0  + 0.136\n\n    def update(self):\n        while(self.running):\n            self.read_pwm()\n            time.sleep(0.015)\n        \n    def run_threaded(self):\n        return self.steering, self.throttle\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(0.1)\n\n\nclass PWMSteering:\n    """"""\n    Wrapper over a PWM motor controller to convert angles to PWM pulses.\n    """"""\n    LEFT_ANGLE = -1\n    RIGHT_ANGLE = 1\n\n    def __init__(self,\n                 controller=None,\n                 left_pulse=290,\n                 right_pulse=490):\n\n        self.controller = controller\n        self.left_pulse = left_pulse\n        self.right_pulse = right_pulse\n        self.pulse = dk.utils.map_range(0, self.LEFT_ANGLE, self.RIGHT_ANGLE,\n                                        self.left_pulse, self.right_pulse)\n        self.running = True\n        print(\'PWM Steering created\')\n\n    def update(self):\n        while self.running:\n            self.controller.set_pulse(self.pulse)\n\n    def run_threaded(self, angle):\n        # map absolute angle to angle that vehicle can implement.\n        self.pulse = dk.utils.map_range(angle,\n                                        self.LEFT_ANGLE, self.RIGHT_ANGLE,\n                                        self.left_pulse, self.right_pulse)\n\n    def run(self, angle):\n        self.run_threaded(angle)\n        self.controller.set_pulse(self.pulse)\n\n    def shutdown(self):\n        # set steering straight\n        self.pulse = 0\n        time.sleep(0.3)\n        self.running = False\n\n\nclass PWMThrottle:\n    """"""\n    Wrapper over a PWM motor controller to convert -1 to 1 throttle\n    values to PWM pulses.\n    """"""\n    MIN_THROTTLE = -1\n    MAX_THROTTLE = 1\n\n    def __init__(self,\n                 controller=None,\n                 max_pulse=300,\n                 min_pulse=490,\n                 zero_pulse=350):\n\n        self.controller = controller\n        self.max_pulse = max_pulse\n        self.min_pulse = min_pulse\n        self.zero_pulse = zero_pulse\n        self.pulse = zero_pulse\n\n        # send zero pulse to calibrate ESC\n        print(""Init ESC"")\n        self.controller.set_pulse(self.max_pulse)\n        time.sleep(0.01)\n        self.controller.set_pulse(self.min_pulse)\n        time.sleep(0.01)\n        self.controller.set_pulse(self.zero_pulse)\n        time.sleep(1)\n        self.running = True\n        print(\'PWM Throttle created\')\n\n    def update(self):\n        while self.running:\n            self.controller.set_pulse(self.pulse)\n\n    def run_threaded(self, throttle):\n        if throttle > 0:\n            self.pulse = dk.utils.map_range(throttle, 0, self.MAX_THROTTLE,\n                                            self.zero_pulse, self.max_pulse)\n        else:\n            self.pulse = dk.utils.map_range(throttle, self.MIN_THROTTLE, 0,\n                                            self.min_pulse, self.zero_pulse)\n\n    def run(self, throttle):\n        self.run_threaded(throttle)\n        self.controller.set_pulse(self.pulse)\n\n    def shutdown(self):\n        # stop vehicle\n        self.run(0)\n        self.running = False\n\n\nclass Adafruit_DCMotor_Hat:\n    \'\'\' \n    Adafruit DC Motor Controller \n    Used for each motor on a differential drive car.\n    \'\'\'\n    def __init__(self, motor_num):\n        from Adafruit_MotorHAT import Adafruit_MotorHAT, Adafruit_DCMotor\n        import atexit\n        \n        self.FORWARD = Adafruit_MotorHAT.FORWARD\n        self.BACKWARD = Adafruit_MotorHAT.BACKWARD\n        self.mh = Adafruit_MotorHAT(addr=0x60) \n        \n        self.motor = self.mh.getMotor(motor_num)\n        self.motor_num = motor_num\n        \n        atexit.register(self.turn_off_motors)\n        self.speed = 0\n        self.throttle = 0\n    \n        \n    def run(self, speed):\n        \'\'\'\n        Update the speed of the motor where 1 is full forward and\n        -1 is full backwards.\n        \'\'\'\n        if speed > 1 or speed < -1:\n            raise ValueError( ""Speed must be between 1(forward) and -1(reverse)"")\n        \n        self.speed = speed\n        self.throttle = int(dk.utils.map_range(abs(speed), -1, 1, -255, 255))\n        \n        if speed > 0:            \n            self.motor.run(self.FORWARD)\n        else:\n            self.motor.run(self.BACKWARD)\n            \n        self.motor.setSpeed(self.throttle)\n        \n\n    def shutdown(self):\n        self.mh.getMotor(self.motor_num).run(Adafruit_MotorHAT.RELEASE)\n\n\nclass Maestro:\n    \'\'\'\n    Pololu Maestro Servo controller\n    Use the MaestroControlCenter to set the speed & acceleration values to 0!\n    \'\'\'\n    import threading\n\n    maestro_device = None\n    astar_device = None\n    maestro_lock = threading.Lock()\n    astar_lock = threading.Lock()\n\n    def __init__(self, channel, frequency = 60):\n        import serial\n\n        if Maestro.maestro_device == None:\n            Maestro.maestro_device = serial.Serial(\'/dev/ttyACM0\', 115200)\n\n        self.channel = channel\n        self.frequency = frequency\n        self.lturn = False\n        self.rturn = False\n        self.headlights = False\n        self.brakelights = False\n\n        if Maestro.astar_device == None:\n            Maestro.astar_device = serial.Serial(\'/dev/ttyACM2\', 115200, timeout= 0.01)\n\n    def set_pulse(self, pulse):\n        # Recalculate pulse width from the Adafruit values\n        w = pulse * (1 / (self.frequency * 4096)) # in seconds\n        w *= 1000 * 1000  # in microseconds\n        w *= 4  # in quarter microsenconds the maestro wants\n        w = int(w)\n\n        with Maestro.maestro_lock:\n            Maestro.maestro_device.write(bytearray([ 0x84,\n                                                     self.channel,\n                                                     (w & 0x7F),\n                                                     ((w >> 7) & 0x7F)]))\n\n    def set_turn_left(self, v):\n        if self.lturn != v:\n            self.lturn = v\n            b = bytearray(\'L\' if v else \'l\', \'ascii\')\n            with Maestro.astar_lock:\n                Maestro.astar_device.write(b)\n\n    def set_turn_right(self, v):\n        if self.rturn != v:\n            self.rturn = v\n            b = bytearray(\'R\' if v else \'r\', \'ascii\')\n            with Maestro.astar_lock:\n                Maestro.astar_device.write(b)\n\n    def set_headlight(self, v):\n        if self.headlights != v:\n            self.headlights = v\n            b = bytearray(\'H\' if v else \'h\', \'ascii\')\n            with Maestro.astar_lock:\n                Maestro.astar_device.write(b)\n\n    def set_brake(self, v):\n        if self.brakelights != v:\n            self.brakelights = v\n            b = bytearray(\'B\' if v else \'b\', \'ascii\')\n            with Maestro.astar_lock:\n                Maestro.astar_device.write(b)\n\n    def readline(self):\n        ret = None\n        with Maestro.astar_lock:\n            # expecting lines like\n            # E n nnn n\n            if Maestro.astar_device.inWaiting() > 8:\n                ret = Maestro.astar_device.readline()\n\n        if ret is not None:\n            ret = ret.rstrip()\n\n        return ret\n\n\nclass Teensy:\n    \'\'\'\n    Teensy Servo controller\n    \'\'\'\n    import threading\n\n    teensy_device = None\n    astar_device = None\n    teensy_lock = threading.Lock()\n    astar_lock = threading.Lock()\n\n    def __init__(self, channel, frequency = 60):\n        import serial\n\n        if Teensy.teensy_device == None:\n            Teensy.teensy_device = serial.Serial(\'/dev/teensy\', 115200, timeout = 0.01)\n\n        self.channel = channel\n        self.frequency = frequency\n        self.lturn = False\n        self.rturn = False\n        self.headlights = False\n        self.brakelights = False\n\n        if Teensy.astar_device == None:\n            Teensy.astar_device = serial.Serial(\'/dev/astar\', 115200, timeout = 0.01)\n\n    def set_pulse(self, pulse):\n        # Recalculate pulse width from the Adafruit values\n        w = pulse * (1 / (self.frequency * 4096)) # in seconds\n        w *= 1000 * 1000  # in microseconds\n\n        with Teensy.teensy_lock:\n            Teensy.teensy_device.write((""%c %.1f\\n"" % (self.channel, w)).encode(\'ascii\'))\n\n    def set_turn_left(self, v):\n        if self.lturn != v:\n            self.lturn = v\n            b = bytearray(\'L\' if v else \'l\', \'ascii\')\n            with Teensy.astar_lock:\n                Teensy.astar_device.write(b)\n\n    def set_turn_right(self, v):\n        if self.rturn != v:\n            self.rturn = v\n            b = bytearray(\'R\' if v else \'r\', \'ascii\')\n            with Teensy.astar_lock:\n                Teensy.astar_device.write(b)\n\n    def set_headlight(self, v):\n        if self.headlights != v:\n            self.headlights = v\n            b = bytearray(\'H\' if v else \'h\', \'ascii\')\n            with Teensy.astar_lock:\n                Teensy.astar_device.write(b)\n\n    def set_brake(self, v):\n        if self.brakelights != v:\n            self.brakelights = v\n            b = bytearray(\'B\' if v else \'b\', \'ascii\')\n            with Teensy.astar_lock:\n                Teensy.astar_device.write(b)\n\n    def teensy_readline(self):\n        ret = None\n        with Teensy.teensy_lock:\n            # expecting lines like\n            # E n nnn n\n            if Teensy.teensy_device.inWaiting() > 8:\n                ret = Teensy.teensy_device.readline()\n\n        if ret != None:\n            ret = ret.rstrip()\n\n        return ret\n\n    def astar_readline(self):\n        ret = None\n        with Teensy.astar_lock:\n            # expecting lines like\n            # E n nnn n\n            if Teensy.astar_device.inWaiting() > 8:\n                ret = Teensy.astar_device.readline()\n\n        if ret != None:\n            ret = ret.rstrip()\n\n        return ret\n\nclass MockController(object):\n    def __init__(self):\n        pass\n\n    def run(self, pulse):\n        pass\n\n    def shutdown(self):\n        pass\n\n\nclass L298N_HBridge_DC_Motor(object):\n    \'\'\'\n    Motor controlled with an L298N hbridge from the gpio pins on Rpi\n    \'\'\'\n    def __init__(self, pin_forward, pin_backward, pwm_pin, freq = 50):\n        import RPi.GPIO as GPIO\n        self.pin_forward = pin_forward\n        self.pin_backward = pin_backward\n        self.pwm_pin = pwm_pin\n\n        GPIO.setmode(GPIO.BOARD)\n        GPIO.setup(self.pin_forward, GPIO.OUT)\n        GPIO.setup(self.pin_backward, GPIO.OUT)\n        GPIO.setup(self.pwm_pin, GPIO.OUT)\n        \n        self.pwm = GPIO.PWM(self.pwm_pin, freq)\n        self.pwm.start(0)\n\n    def run(self, speed):\n        import RPi.GPIO as GPIO\n        \'\'\'\n        Update the speed of the motor where 1 is full forward and\n        -1 is full backwards.\n        \'\'\'\n        if speed > 1 or speed < -1:\n            raise ValueError( ""Speed must be between 1(forward) and -1(reverse)"")\n        \n        self.speed = speed\n        max_duty = 90 #I\'ve read 90 is a good max\n        self.throttle = int(dk.utils.map_range(speed, -1, 1, -max_duty, max_duty))\n        \n        if self.throttle > 0:\n            self.pwm.ChangeDutyCycle(self.throttle)\n            GPIO.output(self.pin_forward, GPIO.HIGH)\n            GPIO.output(self.pin_backward, GPIO.LOW)\n        elif self.throttle < 0:\n            self.pwm.ChangeDutyCycle(-self.throttle)\n            GPIO.output(self.pin_forward, GPIO.LOW)\n            GPIO.output(self.pin_backward, GPIO.HIGH)\n        else:\n            self.pwm.ChangeDutyCycle(self.throttle)\n            GPIO.output(self.pin_forward, GPIO.LOW)\n            GPIO.output(self.pin_backward, GPIO.LOW)\n\n\n    def shutdown(self):\n        import RPi.GPIO as GPIO\n        self.pwm.stop()\n        GPIO.cleanup()\n\n\nclass TwoWheelSteeringThrottle(object):\n\n    def run(self, throttle, steering):\n        if throttle > 1 or throttle < -1:\n            raise ValueError( ""throttle must be between 1(forward) and -1(reverse)"")\n \n        if steering > 1 or steering < -1:\n            raise ValueError( ""steering must be between 1(right) and -1(left)"")\n\n        left_motor_speed = throttle\n        right_motor_speed = throttle\n \n        if steering < 0:\n            left_motor_speed *= (1.0 - (-steering))\n        elif steering > 0:\n            right_motor_speed *= (1.0 - steering)\n\n        return left_motor_speed, right_motor_speed\n\n    def shutdown(self):\n        pass\n\n\nclass Mini_HBridge_DC_Motor_PWM(object):\n    \'\'\'\n    Motor controlled with an mini hbridge from the gpio pins on Rpi\n    This can be using the L298N as above, but wired differently with only\n    two inputs and no enable line.\n    https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dtoys-and-games&field-keywords=Mini+Dual+DC+Motor+H-Bridge+Driver\n    https://www.aliexpress.com/item/5-pc-2-DC-Motor-Drive-Module-Reversing-PWM-Speed-Dual-H-Bridge-Stepper-Motor-Mini\n    \'\'\'\n    def __init__(self, pin_forward, pin_backward, freq = 50, max_duty = 90):\n        \'\'\'\n        max_duy is from 0 to 100. I\'ve read 90 is a good max.\n        \'\'\'\n        import RPi.GPIO as GPIO\n        self.pin_forward = pin_forward\n        self.pin_backward = pin_backward\n        self.max_duty = max_duty\n        \n        GPIO.setmode(GPIO.BOARD)\n        GPIO.setup(self.pin_forward, GPIO.OUT)\n        GPIO.setup(self.pin_backward, GPIO.OUT)\n        \n        self.pwm_f = GPIO.PWM(self.pin_forward, freq)\n        self.pwm_f.start(0)\n        self.pwm_b = GPIO.PWM(self.pin_backward, freq)\n        self.pwm_b.start(0)\n\n    def run(self, speed):\n        import RPi.GPIO as GPIO\n        \'\'\'\n        Update the speed of the motor where 1 is full forward and\n        -1 is full backwards.\n        \'\'\'\n        if speed is None:\n            return\n        \n        if speed > 1 or speed < -1:\n            raise ValueError( ""Speed must be between 1(forward) and -1(reverse)"")\n        \n        self.speed = speed\n        self.throttle = int(dk.utils.map_range(speed, -1, 1, -self.max_duty, self.max_duty))\n        \n        if self.throttle > 0:\n            self.pwm_f.ChangeDutyCycle(self.throttle)\n            self.pwm_b.ChangeDutyCycle(0)\n        elif self.throttle < 0:\n            self.pwm_f.ChangeDutyCycle(0)\n            self.pwm_b.ChangeDutyCycle(-self.throttle)\n        else:\n            self.pwm_f.ChangeDutyCycle(0)\n            self.pwm_b.ChangeDutyCycle(0)\n\n\n    def shutdown(self):\n        import RPi.GPIO as GPIO\n        self.pwm_f.ChangeDutyCycle(0)\n        self.pwm_b.ChangeDutyCycle(0)\n        self.pwm_f.stop()\n        self.pwm_b.stop()\n        GPIO.cleanup()\n\n    \nclass RPi_GPIO_Servo(object):\n    \'\'\'\n    Servo controlled from the gpio pins on Rpi\n    \'\'\'\n    def __init__(self, pin, freq = 50, min=5.0, max=7.8):\n        import RPi.GPIO as GPIO\n        self.pin = pin\n        GPIO.setmode(GPIO.BOARD)\n        GPIO.setup(self.pin, GPIO.OUT)\n        \n        self.pwm = GPIO.PWM(self.pin, freq)\n        self.pwm.start(0)\n        self.min = min\n        self.max = max\n\n    def run(self, pulse):\n        import RPi.GPIO as GPIO\n        \'\'\'\n        Update the speed of the motor where 1 is full forward and\n        -1 is full backwards.\n        \'\'\'\n        #I\'ve read 90 is a good max\n        self.throttle = dk.map_frange(pulse, -1.0, 1.0, self.min, self.max)\n        #print(pulse, self.throttle)\n        self.pwm.ChangeDutyCycle(self.throttle)\n\n\n    def shutdown(self):\n        import RPi.GPIO as GPIO\n        self.pwm.stop()\n        GPIO.cleanup()\n\n\nclass ServoBlaster(object):\n    \'\'\'\n    Servo controlled from the gpio pins on Rpi\n    This uses a user space service to generate more efficient PWM via DMA control blocks.\n    Check readme and install here:\n    https://github.com/richardghirst/PiBits/tree/master/ServoBlaster\n    cd PiBits/ServoBlaster/user\n    make\n    sudo ./servod\n    will start the daemon and create the needed device file:\n    /dev/servoblaster\n\n    to test this from the command line:\n    echo P1-16=120 > /dev/servoblaster\n\n    will send 1200us PWM pulse to physical pin 16 on the pi.\n\n    If you want it to start on boot:\n    sudo make install\n    \'\'\'\n    def __init__(self, pin):\n        self.pin = pin\n        self.servoblaster = open(\'/dev/servoblaster\', \'w\')\n        self.min = min\n        self.max = max\n\n    def set_pulse(self, pulse):\n        s = \'P1-%d=%d\\n\' % (self.pin, pulse)\n        self.servoblaster.write(s)\n        self.servoblaster.flush()\n\n    def run(self, pulse):\n        self.set_pulse(pulse)\n\n    def shutdown(self):\n        self.run((self.max + self.min) / 2)\n        self.servoblaster.close()\n\n\nclass ArduinoFirmata:\n    \'\'\'\n    PWM controller using Arduino board.\n    This is particularly useful for boards like Latte Panda with built it Arduino.\n    Standard Firmata sketch needs to be loaded on Arduino side.\n    Refer to docs/parts/actuators.md for more details\n    \'\'\'\n\n    def __init__(self, servo_pin = 6, esc_pin = 5):\n        from pymata_aio.pymata3 import PyMata3\n        self.board = PyMata3()\n        self.board.sleep(0.015)\n        self.servo_pin = servo_pin\n        self.esc_pin = esc_pin\n        self.board.servo_config(servo_pin)\n        self.board.servo_config(esc_pin)\n\n    def set_pulse(self, pin, angle):\n        try:\n            self.board.analog_write(pin, int(angle))\n        except:\n            self.board.analog_write(pin, int(angle))\n\n    def set_servo_pulse(self, angle):\n        self.set_pulse(self.servo_pin, int(angle))\n\n    def set_esc_pulse(self, angle):\n        self.set_pulse(self.esc_pin, int(angle))\n\n\n\nclass ArdPWMSteering:\n    """"""\n    Wrapper over a Arduino Firmata controller to convert angles to PWM pulses.\n    """"""\n    LEFT_ANGLE = -1\n    RIGHT_ANGLE = 1\n\n    def __init__(self,\n                 controller=None,\n                 left_pulse=60,\n                 right_pulse=120):\n\n        self.controller = controller\n        self.left_pulse = left_pulse\n        self.right_pulse = right_pulse\n        self.pulse = dk.utils.map_range(0, self.LEFT_ANGLE, self.RIGHT_ANGLE,\n                                        self.left_pulse, self.right_pulse)\n        self.running = True\n        print(\'Arduino PWM Steering created\')\n\n    def run(self, angle):\n        # map absolute angle to angle that vehicle can implement.\n        self.pulse = dk.utils.map_range(angle,\n                                        self.LEFT_ANGLE, self.RIGHT_ANGLE,\n                                        self.left_pulse, self.right_pulse)\n        self.controller.set_servo_pulse(self.pulse)\n\n    def shutdown(self):\n        # set steering straight\n        self.pulse = dk.utils.map_range(0, self.LEFT_ANGLE, self.RIGHT_ANGLE,\n                                        self.left_pulse, self.right_pulse)\n        time.sleep(0.3)\n        self.running = False\n\n\nclass ArdPWMThrottle:\n\n    """"""\n    Wrapper over Arduino Firmata controller to convert -1 to 1 throttle\n    values to PWM pulses.\n    """"""\n    MIN_THROTTLE = -1\n    MAX_THROTTLE = 1\n\n    def __init__(self,\n                 controller=None,\n                 max_pulse=105,\n                 min_pulse=75,\n                 zero_pulse=90):\n\n        self.controller = controller\n        self.max_pulse = max_pulse\n        self.min_pulse = min_pulse\n        self.zero_pulse = zero_pulse\n        self.pulse = zero_pulse\n\n        # send zero pulse to calibrate ESC\n        print(""Init ESC"")\n        self.controller.set_esc_pulse(self.max_pulse)\n        time.sleep(0.01)\n        self.controller.set_esc_pulse(self.min_pulse)\n        time.sleep(0.01)\n        self.controller.set_esc_pulse(self.zero_pulse)\n        time.sleep(1)\n        self.running = True\n        print(\'Arduino PWM Throttle created\')\n\n    def run(self, throttle):\n        if throttle > 0:\n            self.pulse = dk.utils.map_range(throttle, 0, self.MAX_THROTTLE,\n                                            self.zero_pulse, self.max_pulse)\n        else:\n            self.pulse = dk.utils.map_range(throttle, self.MIN_THROTTLE, 0,\n                                            self.min_pulse, self.zero_pulse)\n        self.controller.set_esc_pulse(self.pulse)\n\n    def shutdown(self):\n        # stop vehicle\n        self.run(0)\n        self.running = False\n'"
donkeycar/parts/augment.py,0,"b'\'\'\'\n    File: augment.py\n    Author : Tawn Kramer\n    Date : July 2017\n\'\'\'\nimport random\nfrom PIL import Image\nfrom PIL import ImageEnhance\nimport glob\nimport numpy as np\nimport math\n\n\'\'\'\n    find_coeffs and persp_transform borrowed from:\n    https://stackoverflow.com/questions/14177744/how-does-perspective-transformation-work-in-pil\n\'\'\'\nONE_BY_255 = 1.0 / 255.0\n\n\ndef find_coeffs(pa, pb):\n    matrix = []\n    for p1, p2 in zip(pa, pb):\n        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n\n    A = np.matrix(matrix, dtype=np.float)\n    B = np.array(pb).reshape(8)\n\n    res = np.dot(np.linalg.inv(A.T * A) * A.T, B)\n    return np.array(res).reshape(8)\n\n\ndef rand_persp_transform(img):\n    width, height = img.size\n    new_width = math.floor(float(width) * random.uniform(0.9, 1.1))\n    xshift = math.floor(float(width) * random.uniform(-0.2, 0.2))\n    coeffs = find_coeffs(\n        [(0, 0), (256, 0), (256, 256), (0, 256)],\n        [(0, 0), (256, 0), (new_width, height), (xshift, height)])\n\n    return img.transform((width, height), Image.PERSPECTIVE, coeffs, Image.BICUBIC)\n\n\ndef augment_image(np_img, shadow_images=None, do_warp_persp=False):\n    """"""\n    :param np_img: numpy image\n        input image in numpy normalised format\n    :param shadow_images: list of 2-tuples of PIL images\n        shadow vector as prepared by load_shadow_images\n    :param do_warp_persp: bool\n        apply warping\n    :return: numpy image\n        output image in numpy normalised format\n    """"""\n    # denormalise image to 8int\n    conv_img = np_img * 255.0\n    conv_img = conv_img.astype(np.uint8)\n    # convert to PIL and apply transformation\n    img = Image.fromarray(conv_img)\n    img = augment_pil_image(img, shadow_images, do_warp_persp)\n    # transform back to normalised numpy format\n    img_out = np.array(img).astype(np.float) * ONE_BY_255\n    return img_out\n\n\ndef augment_pil_image(img, shadow_images=None, do_warp_persp=False):\n    """"""\n    :param img: PIL image\n        input image in PIL format\n    :param do_warp_persp: bool\n        apply warping\n    :param shadow_images: list of 2-tuples of PIL images\n        shadow vector as prepared by load_shadow_images\n    :return: PIL image\n        augmented image\n    """"""\n    # change the coloration, sharpness, and composite a shadow\n    factor = random.uniform(0.5, 2.0)\n    img = ImageEnhance.Brightness(img).enhance(factor)\n    factor = random.uniform(0.5, 1.0)\n    img = ImageEnhance.Contrast(img).enhance(factor)\n    factor = random.uniform(0.5, 1.5)\n    img = ImageEnhance.Sharpness(img).enhance(factor)\n    factor = random.uniform(0.0, 2.0)\n    img = ImageEnhance.Color(img).enhance(factor)\n    # optionally composite a shadow, prepared from load_shadow_images\n    if shadow_images is not None:\n        iShad = random.randrange(0, len(shadow_images))\n        top, mask = shadow_images[iShad]\n        theta = random.randrange(-35, 35)\n        mask.rotate(theta)\n        top.rotate(theta)\n        mask = ImageEnhance.Brightness(mask).enhance(random.uniform(0.3, 1.0))\n        offset = (random.randrange(-128, 128), random.randrange(-128, 128))\n        img.paste(top, offset, mask)\n    # optionally warp perspective\n    if do_warp_persp:\n        img = rand_persp_transform(img)\n    return img\n\n\ndef load_shadow_images(path_mask):\n    shadow_images = []\n    filenames = glob.glob(path_mask)\n    for filename in filenames:\n        shadow = Image.open(filename)\n        shadow.thumbnail((256, 256))\n        channels = shadow.split()\n        if len(channels) != 4:\n            continue\n        r, g, b, a = channels\n        top = Image.merge(""RGB"", (r, g, b))\n        mask = Image.merge(""L"", (a,))\n        shadow_images.append((top, mask))\n    return shadow_images\n\n\n'"
donkeycar/parts/behavior.py,0,"b'class BehaviorPart(object):\n    \'\'\'\n    Keep a list of states, and an active state. Keep track of switching.\n    And return active state information.\n    \'\'\'\n    def __init__(self, states):\n        \'\'\'\n        expects a list of strings to enumerate state\n        \'\'\'\n        print(""bvh states:"", states)\n        self.states = states\n        self.active_state = 0\n        self.one_hot_state_array = []\n        for i in range(len(states)):\n            self.one_hot_state_array.append(0.0)\n        self.one_hot_state_array[0] = 1.0\n\n    def increment_state(self):\n        self.one_hot_state_array[self.active_state] = 0.0\n        self.active_state += 1\n        if self.active_state >= len(self.states):\n            self.active_state = 0\n        self.one_hot_state_array[self.active_state] = 1.0\n        print(""In State:"", self.states[self.active_state])\n\n    def decrement_state(self):\n        self.one_hot_state_array[self.active_state] = 0.0\n        self.active_state -= 1\n        if self.active_state < 0:\n            self.active_state = len(self.states) - 1\n        self.one_hot_state_array[self.active_state] = 1.0\n        print(""In State:"", self.states[self.active_state])\n\n    def set_state(self, iState):\n        self.one_hot_state_array[self.active_state] = 0.0\n        self.active_state = iState\n        self.one_hot_state_array[self.active_state] = 1.0\n        print(""In State:"", self.states[self.active_state])\n\n    def run(self):\n        return self.active_state, self.states[self.active_state], self.one_hot_state_array\n\n    def shutdown(self):\n        pass\n        '"
donkeycar/parts/camera.py,0,"b'import os\nimport time\nimport numpy as np\nfrom PIL import Image\nimport glob\nfrom donkeycar.utils import rgb2gray\n\nclass BaseCamera:\n\n    def run_threaded(self):\n        return self.frame\n\nclass PiCamera(BaseCamera):\n    def __init__(self, image_w=160, image_h=120, image_d=3, framerate=20, vflip=False, hflip=False):\n        from picamera.array import PiRGBArray\n        from picamera import PiCamera\n        \n        resolution = (image_w, image_h)\n        # initialize the camera and stream\n        self.camera = PiCamera() #PiCamera gets resolution (height, width)\n        self.camera.resolution = resolution\n        self.camera.framerate = framerate\n        self.camera.vflip = vflip\n        self.camera.hflip = hflip\n        self.rawCapture = PiRGBArray(self.camera, size=resolution)\n        self.stream = self.camera.capture_continuous(self.rawCapture,\n            format=""rgb"", use_video_port=True)\n\n        # initialize the frame and the variable used to indicate\n        # if the thread should be stopped\n        self.frame = None\n        self.on = True\n        self.image_d = image_d\n\n        print(\'PiCamera loaded.. .warming camera\')\n        time.sleep(2)\n\n\n    def run(self):\n        f = next(self.stream)\n        frame = f.array\n        self.rawCapture.truncate(0)\n        if self.image_d == 1:\n            frame = rgb2gray(frame)\n        return frame\n\n    def update(self):\n        # keep looping infinitely until the thread is stopped\n        for f in self.stream:\n            # grab the frame from the stream and clear the stream in\n            # preparation for the next frame\n            self.frame = f.array\n            self.rawCapture.truncate(0)\n\n            if self.image_d == 1:\n                self.frame = rgb2gray(self.frame)\n\n            # if the thread indicator variable is set, stop the thread\n            if not self.on:\n                break\n\n    def shutdown(self):\n        # indicate that the thread should be stopped\n        self.on = False\n        print(\'Stopping PiCamera\')\n        time.sleep(.5)\n        self.stream.close()\n        self.rawCapture.close()\n        self.camera.close()\n\nclass Webcam(BaseCamera):\n    def __init__(self, image_w=160, image_h=120, image_d=3, framerate = 20, iCam = 0):\n        import pygame\n        import pygame.camera\n\n        super().__init__()\n        resolution = (image_w, image_h)\n        pygame.init()\n        pygame.camera.init()\n        l = pygame.camera.list_cameras()\n        print(\'cameras\', l)\n        self.cam = pygame.camera.Camera(l[iCam], resolution, ""RGB"")\n        self.resolution = resolution\n        self.cam.start()\n        self.framerate = framerate\n\n        # initialize variable used to indicate\n        # if the thread should be stopped\n        self.frame = None\n        self.on = True\n        self.image_d = image_d\n\n        print(\'WebcamVideoStream loaded.. .warming camera\')\n\n        time.sleep(2)\n\n    def update(self):\n        from datetime import datetime, timedelta\n        import pygame.image\n        while self.on:\n            start = datetime.now()\n\n            if self.cam.query_image():\n                # snapshot = self.cam.get_image()\n                # self.frame = list(pygame.image.tostring(snapshot, ""RGB"", False))\n                snapshot = self.cam.get_image()\n                snapshot1 = pygame.transform.scale(snapshot, self.resolution)\n                self.frame = pygame.surfarray.pixels3d(pygame.transform.rotate(pygame.transform.flip(snapshot1, True, False), 90))\n                if self.image_d == 1:\n                    self.frame = rgb2gray(self.frame)\n\n            stop = datetime.now()\n            s = 1 / self.framerate - (stop - start).total_seconds()\n            if s > 0:\n                time.sleep(s)\n\n        self.cam.stop()\n\n    def run_threaded(self):\n        return self.frame\n\n    def shutdown(self):\n        # indicate that the thread should be stopped\n        self.on = False\n        print(\'stoping Webcam\')\n        time.sleep(.5)\n\n\nclass CSICamera(BaseCamera):\n    \'\'\'\n    Camera for Jetson Nano IMX219 based camera\n    Credit: https://github.com/feicccccccc/donkeycar/blob/dev/donkeycar/parts/camera.py\n    gstreamer init string from https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/jetbot/camera.py\n    \'\'\'\n    def gstreamer_pipeline(self, capture_width=3280, capture_height=2464, output_width=224, output_height=224, framerate=21, flip_method=0) :   \n        return \'nvarguscamerasrc ! video/x-raw(memory:NVMM), width=%d, height=%d, format=(string)NV12, framerate=(fraction)%d/1 ! nvvidconv flip-method=%d ! nvvidconv ! video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! videoconvert ! appsink\' % (\n                capture_width, capture_height, framerate, flip_method, output_width, output_height)\n    \n    def __init__(self, image_w=160, image_h=120, image_d=3, capture_width=3280, capture_height=2464, framerate=60, gstreamer_flip=0):\n        \'\'\'\n        gstreamer_flip = 0 - no flip\n        gstreamer_flip = 1 - rotate CCW 90\n        gstreamer_flip = 2 - flip vertically\n        gstreamer_flip = 3 - rotate CW 90\n        \'\'\'\n        self.w = image_w\n        self.h = image_h\n        self.running = True\n        self.frame = None\n        self.flip_method = gstreamer_flip\n        self.capture_width = capture_width\n        self.capture_height = capture_height\n        self.framerate = framerate\n\n    def init_camera(self):\n        import cv2\n\n        # initialize the camera and stream\n        self.camera = cv2.VideoCapture(\n            self.gstreamer_pipeline(\n                capture_width =self.capture_width,\n                capture_height =self.capture_height,\n                output_width=self.w,\n                output_height=self.h,\n                framerate=self.framerate,\n                flip_method=self.flip_method),\n            cv2.CAP_GSTREAMER)\n\n        self.poll_camera()\n        print(\'CSICamera loaded.. .warming camera\')\n        time.sleep(2)\n        \n    def update(self):\n        self.init_camera()\n        while self.running:\n            self.poll_camera()\n\n    def poll_camera(self):\n        import cv2\n        self.ret , frame = self.camera.read()\n        self.frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    def run(self):\n        self.poll_camera()\n        return self.frame\n\n    def run_threaded(self):\n        return self.frame\n    \n    def shutdown(self):\n        self.running = False\n        print(\'stoping CSICamera\')\n        time.sleep(.5)\n        del(self.camera)\n\nclass V4LCamera(BaseCamera):\n    \'\'\'\n    uses the v4l2capture library from this fork for python3 support: https://github.com/atareao/python3-v4l2capture\n    sudo apt-get install libv4l-dev\n    cd python3-v4l2capture\n    python setup.py build\n    pip install -e .\n    \'\'\'\n    def __init__(self, image_w=160, image_h=120, image_d=3, framerate=20, dev_fn=""/dev/video0"", fourcc=\'MJPG\'):\n\n        self.running = True\n        self.frame = None\n        self.image_w = image_w\n        self.image_h = image_h\n        self.dev_fn = dev_fn\n        self.fourcc = fourcc\n\n    def init_video(self):\n        import v4l2capture\n\n        self.video = v4l2capture.Video_device(self.dev_fn)\n\n        # Suggest an image size to the device. The device may choose and\n        # return another size if it doesn\'t support the suggested one.\n        self.size_x, self.size_y = self.video.set_format(self.image_w, self.image_h, fourcc=self.fourcc)\n\n        print(""V4L camera granted %d, %d resolution."" % (self.size_x, self.size_y))\n\n        # Create a buffer to store image data in. This must be done before\n        # calling \'start\' if v4l2capture is compiled with libv4l2. Otherwise\n        # raises IOError.\n        self.video.create_buffers(30)\n\n        # Send the buffer to the device. Some devices require this to be done\n        # before calling \'start\'.\n        self.video.queue_all_buffers()\n\n        # Start the device. This lights the LED if it\'s a camera that has one.\n        self.video.start()\n\n\n    def update(self):\n        import select\n        from donkeycar.parts.image import JpgToImgArr\n\n        self.init_video()\n        jpg_conv = JpgToImgArr()\n\n        while self.running:\n            # Wait for the device to fill the buffer.\n            select.select((self.video,), (), ())\n            image_data = self.video.read_and_queue()\n            self.frame = jpg_conv.run(image_data)\n\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(0.5)\n\n\n\nclass MockCamera(BaseCamera):\n    \'\'\'\n    Fake camera. Returns only a single static frame\n    \'\'\'\n    def __init__(self, image_w=160, image_h=120, image_d=3, image=None):\n        if image is not None:\n            self.frame = image\n        else:\n            self.frame = np.array(Image.new(\'RGB\', (image_w, image_h)))\n\n    def update(self):\n        pass\n\n    def shutdown(self):\n        pass\n\nclass ImageListCamera(BaseCamera):\n    \'\'\'\n    Use the images from a tub as a fake camera output\n    \'\'\'\n    def __init__(self, path_mask=\'~/mycar/data/**/*.jpg\'):\n        self.image_filenames = glob.glob(os.path.expanduser(path_mask), recursive=True)\n    \n        def get_image_index(fnm):\n            sl = os.path.basename(fnm).split(\'_\')\n            return int(sl[0])\n\n        \'\'\'\n        I feel like sorting by modified time is almost always\n        what you want. but if you tared and moved your data around,\n        sometimes it doesn\'t preserve a nice modified time.\n        so, sorting by image index works better, but only with one path.\n        \'\'\'\n        self.image_filenames.sort(key=get_image_index)\n        #self.image_filenames.sort(key=os.path.getmtime)\n        self.num_images = len(self.image_filenames)\n        print(\'%d images loaded.\' % self.num_images)\n        print( self.image_filenames[:10])\n        self.i_frame = 0\n        self.frame = None\n        self.update()\n\n    def update(self):\n        pass\n\n    def run_threaded(self):        \n        if self.num_images > 0:\n            self.i_frame = (self.i_frame + 1) % self.num_images\n            self.frame = Image.open(self.image_filenames[self.i_frame]) \n\n        return np.asarray(self.frame)\n\n    def shutdown(self):\n        pass\n'"
donkeycar/parts/controller.py,0,"b'\nimport os\nimport array\nimport time\nimport struct\nimport random\nfrom threading import Thread\nimport logging\n\nfrom prettytable import PrettyTable\n\n#import for syntactical ease\nfrom donkeycar.parts.web_controller.web import LocalWebController\nfrom donkeycar.parts.web_controller.web import WebFpv\n\nclass Joystick(object):\n    \'\'\'\n    An interface to a physical joystick\n    \'\'\'\n    def __init__(self, dev_fn=\'/dev/input/js0\'):\n        self.axis_states = {}\n        self.button_states = {}\n        self.axis_names = {}\n        self.button_names = {}\n        self.axis_map = []\n        self.button_map = []\n        self.jsdev = None\n        self.dev_fn = dev_fn\n\n\n    def init(self):\n        try:\n            from fcntl import ioctl\n        except ModuleNotFoundError:\n            self.num_axes = 0\n            self.num_buttons = 0\n            print(""no support for fnctl module. joystick not enabled."")\n            return False\n\n        if not os.path.exists(self.dev_fn):\n            print(self.dev_fn, ""is missing"")\n            return False\n\n        \'\'\'\n        call once to setup connection to device and map buttons\n        \'\'\'\n        # Open the joystick device.\n        print(\'Opening %s...\' % self.dev_fn)\n        self.jsdev = open(self.dev_fn, \'rb\')\n\n        # Get the device name.\n        buf = array.array(\'B\', [0] * 64)\n        ioctl(self.jsdev, 0x80006a13 + (0x10000 * len(buf)), buf) # JSIOCGNAME(len)\n        self.js_name = buf.tobytes().decode(\'utf-8\')\n        print(\'Device name: %s\' % self.js_name)\n\n        # Get number of axes and buttons.\n        buf = array.array(\'B\', [0])\n        ioctl(self.jsdev, 0x80016a11, buf) # JSIOCGAXES\n        self.num_axes = buf[0]\n\n        buf = array.array(\'B\', [0])\n        ioctl(self.jsdev, 0x80016a12, buf) # JSIOCGBUTTONS\n        self.num_buttons = buf[0]\n\n        # Get the axis map.\n        buf = array.array(\'B\', [0] * 0x40)\n        ioctl(self.jsdev, 0x80406a32, buf) # JSIOCGAXMAP\n\n        for axis in buf[:self.num_axes]:\n            axis_name = self.axis_names.get(axis, \'unknown(0x%02x)\' % axis)\n            self.axis_map.append(axis_name)\n            self.axis_states[axis_name] = 0.0\n\n        # Get the button map.\n        buf = array.array(\'H\', [0] * 200)\n        ioctl(self.jsdev, 0x80406a34, buf) # JSIOCGBTNMAP\n\n        for btn in buf[:self.num_buttons]:\n            btn_name = self.button_names.get(btn, \'unknown(0x%03x)\' % btn)\n            self.button_map.append(btn_name)\n            self.button_states[btn_name] = 0\n            #print(\'btn\', \'0x%03x\' % btn, \'name\', btn_name)\n\n        return True\n\n\n    def show_map(self):\n        \'\'\'\n        list the buttons and axis found on this joystick\n        \'\'\'\n        print (\'%d axes found: %s\' % (self.num_axes, \', \'.join(self.axis_map)))\n        print (\'%d buttons found: %s\' % (self.num_buttons, \', \'.join(self.button_map)))\n\n\n    def poll(self):\n        \'\'\'\n        query the state of the joystick, returns button which was pressed, if any,\n        and axis which was moved, if any. button_state will be None, 1, or 0 if no changes,\n        pressed, or released. axis_val will be a float from -1 to +1. button and axis will\n        be the string label determined by the axis map in init.\n        \'\'\'\n        button = None\n        button_state = None\n        axis = None\n        axis_val = None\n\n        if self.jsdev is None:\n            return button, button_state, axis, axis_val\n\n        # Main event loop\n        evbuf = self.jsdev.read(8)\n\n        if evbuf:\n            tval, value, typev, number = struct.unpack(\'IhBB\', evbuf)\n\n            if typev & 0x80:\n                #ignore initialization event\n                return button, button_state, axis, axis_val\n\n            if typev & 0x01:\n                button = self.button_map[number]\n                #print(tval, value, typev, number, button, \'pressed\')\n                if button:\n                    self.button_states[button] = value\n                    button_state = value\n                    logging.info(""button: %s state: %d"" % (button, value))\n\n            if typev & 0x02:\n                axis = self.axis_map[number]\n                if axis:\n                    fvalue = value / 32767.0\n                    self.axis_states[axis] = fvalue\n                    axis_val = fvalue\n                    logging.debug(""axis: %s val: %f"" % (axis, fvalue))\n\n        return button, button_state, axis, axis_val\n\n\nclass PyGameJoystick(object):\n    def __init__( self,\n                  poll_delay=0.0,\n                  throttle_scale=1.0,\n                  steering_scale=1.0,\n                  throttle_dir=-1.0,\n                  dev_fn=\'/dev/input/js0\',\n                  auto_record_on_throttle=True,\n                  which_js=0):\n\n        import pygame\n\n        pygame.init()\n\n        # Initialize the joysticks\n        pygame.joystick.init()\n\n        self.joystick = pygame.joystick.Joystick(which_js)\n        self.joystick.init()\n        name = self.joystick.get_name()\n        print(""detected joystick device:"", name)\n\n        self.axis_states = [ 0.0 for i in range(self.joystick.get_numaxes())]\n        self.button_states = [ 0 for i in range(self.joystick.get_numbuttons() + self.joystick.get_numhats() * 4)]\n        self.axis_names = {}\n        self.button_names = {}\n        self.dead_zone = 0.07\n        for i in range(self.joystick.get_numaxes()):\n            self.axis_names[i] = i\n        for i in range(self.joystick.get_numbuttons() + self.joystick.get_numhats() * 4):\n            self.button_names[i] = i\n\n    def poll(self):\n        import pygame\n\n        button = None\n        button_state = None\n        axis = None\n        axis_val = None\n\n        pygame.event.get()\n\n\n        for i in range( self.joystick.get_numaxes() ):\n            val = self.joystick.get_axis( i )\n            if abs(val) < self.dead_zone:\n                val = 0.0\n            if self.axis_states[i] != val and i in self.axis_names:\n                axis = self.axis_names[i]\n                axis_val = val\n                self.axis_states[i] = val\n                logging.debug(""axis: %s val: %f"" % (axis, val))\n                #print(""axis: %s val: %f"" % (axis, val))\n\n\n        for i in range( self.joystick.get_numbuttons() ):\n            state = self.joystick.get_button( i )\n            if self.button_states[i] != state:\n                if not i in self.button_names:\n                    print(\'button:\', i)\n                    continue\n                button = self.button_names[i]\n                button_state = state\n                self.button_states[i] = state\n                logging.info(""button: %s state: %d"" % (button, state))\n                #print(""button: %s state: %d"" % (button, state))\n\n        for i in range( self.joystick.get_numhats() ):\n            hat = self.joystick.get_hat( i )\n            horz, vert = hat\n            iBtn = self.joystick.get_numbuttons() + (i * 4)\n            states = (horz == -1, horz == 1, vert == -1, vert == 1)\n            for state in states:\n                state = int(state)\n                if self.button_states[iBtn] != state:\n                    if not iBtn in self.button_names:\n                        print(""button:"", iBtn)\n                        continue\n                    button = self.button_names[iBtn]\n                    button_state = state\n                    self.button_states[iBtn] = state\n                    logging.info(""button: %s state: %d"" % (button, state))\n                    #print(""button: %s state: %d"" % (button, state))\n\n                iBtn += 1\n\n        return button, button_state, axis, axis_val\n        \n    def set_deadzone(self, val):\n        self.dead_zone = val\n\n\nclass JoystickCreator(Joystick):\n    \'\'\'\n    A Helper class to create a new joystick mapping\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(JoystickCreator, self).__init__(*args, **kwargs)\n\n        self.axis_names = {}\n        self.button_names = {}\n\n    def poll(self):\n\n        button, button_state, axis, axis_val = super(JoystickCreator, self).poll()\n\n        return button, button_state, axis, axis_val\n\n\nclass PS3JoystickOld(Joystick):\n    \'\'\'\n    An interface to a physical PS3 joystick available at /dev/input/js0\n    Contains mapping that worked for Raspian Jessie drivers\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PS3JoystickOld, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00 : \'left_stick_horz\',\n            0x01 : \'left_stick_vert\',\n            0x02 : \'right_stick_horz\',\n            0x05 : \'right_stick_vert\',\n\n            0x1a : \'tilt_x\',\n            0x1b : \'tilt_y\',\n            0x3d : \'tilt_a\',\n            0x3c : \'tilt_b\',\n\n            0x32 : \'L1_pressure\',\n            0x33 : \'R1_pressure\',\n            0x31 : \'R2_pressure\',\n            0x30 : \'L2_pressure\',\n\n            0x36 : \'cross_pressure\',\n            0x35 : \'circle_pressure\',\n            0x37 : \'square_pressure\',\n            0x34 : \'triangle_pressure\',\n\n            0x2d : \'dpad_r_pressure\',\n            0x2e : \'dpad_d_pressure\',\n            0x2c : \'dpad_u_pressure\',\n        }\n\n        self.button_names = {\n            0x120 : \'select\',\n            0x123 : \'start\',\n            0x2c0 : \'PS\',\n\n            0x12a : \'L1\',\n            0x12b : \'R1\',\n            0x128 : \'L2\',\n            0x129 : \'R2\',\n            0x121 : \'L3\',\n            0x122 : \'R3\',\n\n            0x12c : ""triangle"", \n            0x12d : ""circle"",\n            0x12e : ""cross"",\n            0x12f : \'square\',\n\n            0x124 : \'dpad_up\',\n            0x126 : \'dpad_down\',\n            0x127 : \'dpad_left\',\n            0x125 : \'dpad_right\',\n        }\n\n\nclass PS3Joystick(Joystick):\n    \'\'\'\n    An interface to a physical PS3 joystick available at /dev/input/js0\n    Contains mapping that work for Raspian Stretch drivers\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PS3Joystick, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00 : \'left_stick_horz\',\n            0x01 : \'left_stick_vert\',\n            0x03 : \'right_stick_horz\',\n            0x04 : \'right_stick_vert\',\n\n            0x02 : \'L2_pressure\',\n            0x05 : \'R2_pressure\',\n        }\n\n        self.button_names = {\n           0x13a : \'select\', #8 314\n           0x13b : \'start\', #9 315\n           0x13c : \'PS\', #a  316\n\n           0x136 : \'L1\', #4 310\n           0x137 : \'R1\', #5 311\n           0x138 : \'L2\', #6 312\n           0x139 : \'R2\', #7 313\n           0x13d : \'L3\', #b 317\n           0x13e : \'R3\', #c 318\n\n           0x133 : ""triangle"",  #2 307\n           0x131 : ""circle"",    #1 305\n           0x130 : ""cross"",    #0 304\n           0x134 : \'square\',    #3 308\n\n           0x220 : \'dpad_up\', #d 544\n           0x221 : \'dpad_down\', #e 545\n           0x222 : \'dpad_left\', #f 546\n           0x223 : \'dpad_right\', #10 547\n       }\n\n\nclass PS4Joystick(Joystick):\n    \'\'\'\n    An interface to a physical PS4 joystick available at /dev/input/js0\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PS4Joystick, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00 : \'left_stick_horz\',\n            0x01 : \'left_stick_vert\',\n            0x02 : \'right_stick_horz\',\n            0x05 : \'right_stick_vert\',\n\n            0x03 : \'left_trigger_axis\',\n            0x04 : \'right_trigger_axis\',\n\n            0x10 : \'dpad_leftright\',\n            0x11 : \'dpad_updown\',\n\n            0x19 : \'tilt_a\',\n            0x1a : \'tilt_b\',\n            0x1b : \'tilt_c\',\n\n            0x06 : \'motion_a\',\n            0x07 : \'motion_b\',\n            0x08 : \'motion_c\',\n        }\n\n        self.button_names = {\n\n            0x130 : \'square\',\n            0x131 : \'cross\',\n            0x132 : \'circle\',\n            0x133 : \'triangle\',\n\n            0x134 : \'L1\',\n            0x135 : \'R1\',\n            0x136 : \'L2\',\n            0x137 : \'R2\',\n            0x13a : \'L3\',\n            0x13b : \'R3\',\n\n            0x13d : \'pad\',\n            0x138 : \'share\',\n            0x139 : \'options\',\n            0x13c : \'PS\',\n        }\n\n\nclass PS3JoystickPC(Joystick):\n    \'\'\'\n    An interface to a physical PS3 joystick available at /dev/input/js1\n    Seems to exhibit slightly different codes because driver is different?\n    when running from ubuntu 16.04, it will interfere w mouse until:\n    xinput set-prop ""Sony PLAYSTATION(R)3 Controller"" ""Device Enabled"" 0\n    It also wants /dev/input/js1 device filename, not js0\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PS3JoystickPC, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00 : \'left_stick_horz\',\n            0x01 : \'left_stick_vert\',\n            0x03 : \'right_stick_horz\',\n            0x04 : \'right_stick_vert\',\n\n            0x1a : \'tilt_x\',\n            0x1b : \'tilt_y\',\n            0x3d : \'tilt_a\',\n            0x3c : \'tilt_b\',\n\n            0x32 : \'L1_pressure\',\n            0x33 : \'R1_pressure\',\n            0x05 : \'R2_pressure\',\n            0x02 : \'L2_pressure\',\n\n            0x36 : \'cross_pressure\',\n            0x35 : \'circle_pressure\',\n            0x37 : \'square_pressure\',\n            0x34 : \'triangle_pressure\',\n\n            0x2d : \'dpad_r_pressure\',\n            0x2e : \'dpad_d_pressure\',\n            0x2c : \'dpad_u_pressure\',\n        }\n\n        self.button_names = {\n            0x13a : \'select\',\n            0x13b : \'start\',\n            0x13c : \'PS\',\n\n            0x136 : \'L1\',\n            0x137 : \'R1\',\n            0x138 : \'L2\',\n            0x139 : \'R2\',\n            0x13d : \'L3\',\n            0x13e : \'R3\',\n\n            0x133 : ""triangle"",\n            0x131 : ""circle"",\n            0x130 : ""cross"",\n            0x134 : \'square\',\n\n            0x220 : \'dpad_up\',\n            0x221 : \'dpad_down\',\n            0x222 : \'dpad_left\',\n            0x223 : \'dpad_right\',\n        }\n\n\nclass PyGamePS4Joystick(PyGameJoystick):\n    \'\'\'\n    An interface to a physical PS4 joystick available via pygame\n    Windows setup: https://github.com/nefarius/ScpToolkit/releases/tag/v1.6.238.16010\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PyGamePS4Joystick, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00 : \'left_stick_horz\',\n            0x01 : \'left_stick_vert\',\n            0x03 : \'right_stick_vert\',\n            0x02 : \'right_stick_horz\',\n        }\n\n        self.button_names = {\n            2 : ""circle"",\n            1 : ""cross"",\n            0 : \'square\',\n            3 : ""triangle"",\n\n            8 : \'share\',\n            9 : \'options\',\n            13 : \'pad\',\n\n            4 : \'L1\',\n            5 : \'R1\',\n            6 : \'L2\',\n            7 : \'R2\',\n            10 : \'L3\',\n            11 : \'R3\',\n            14 : \'dpad_left\',\n            15 : \'dpad_right\',\n            16 : \'dpad_down\',\n            17 : \'dpad_up\',\n        }\n\n\nclass XboxOneJoystick(Joystick):\n    \'\'\'\n    An interface to a physical joystick \'Xbox Wireless Controller\' controller.\n    This will generally show up on /dev/input/js0.\n    - Note that this code presumes the built-in linux driver for \'Xbox Wireless Controller\'.\n      There is another user land driver called xboxdrv; this code has not been tested\n      with that driver.\n    - Note that this controller requires that the bluetooth disable_ertm parameter\n      be set to true; to do this:\n      - edit /etc/modprobe.d/xbox_bt.conf\n      - add the line: options bluetooth disable_ertm=1\n      - reboot to tha this take affect.\n      - after reboot you can vertify that disable_ertm is set to true entering this\n        command oin a terminal: cat /sys/module/bluetooth/parameters/disable_ertm\n      - the result should print \'Y\'.  If not, make sure the above steps have been done corretly.\n\n    credit:\n    https://github.com/Ezward/donkeypart_ps3_controller/blob/master/donkeypart_ps3_controller/part.py\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(XboxOneJoystick, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00: \'left_stick_horz\',\n            0x01: \'left_stick_vert\',\n            0x02: \'right_stick_horz\',\n            0x05: \'right_stick_vert\',\n            0x09: \'right_trigger\',\n            0x0a: \'left_trigger\',\n            0x10: \'dpad_horz\',\n            0x11: \'dpad_vert\',\n        }\n\n        self.button_names = {\n            0x130: \'a_button\',\n            0x131: \'b_button\',\n            0x133: \'x_button\',\n            0x134: \'y_button\',\n            0x136: \'left_shoulder\',\n            0x137: \'right_shoulder\',\n            0x13b: \'options\',\n        }\n\n\nclass LogitechJoystick(Joystick):\n    \'\'\'\n    An interface to a physical Logitech joystick available at /dev/input/js0\n    Contains mapping that work for Raspian Stretch drivers\n    Tested with Logitech Gamepad F710\n    https://www.amazon.com/Logitech-940-000117-Gamepad-F710/dp/B0041RR0TW\n    credit:\n    https://github.com/kevkruemp/donkeypart_logitech_controller/blob/master/donkeypart_logitech_controller/part.py\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(LogitechJoystick, self).__init__(*args, **kwargs)\n\n        self.axis_names = {\n            0x00: \'left_stick_horz\',\n            0x01: \'left_stick_vert\',\n            0x03: \'right_stick_horz\',\n            0x04: \'right_stick_vert\',\n\n            0x02: \'L2_pressure\',\n            0x05: \'R2_pressure\',\n\n            0x10: \'dpad_leftright\', # 1 is right, -1 is left\n            0x11: \'dpad_up_down\', # 1 is down, -1 is up\n        }\n\n        self.button_names = {\n            0x13a: \'back\',  # 8 314\n            0x13b: \'start\',  # 9 315\n            0x13c: \'Logitech\',  # a  316\n\n            0x130: \'A\',\n            0x131: \'B\',\n            0x133: \'X\',\n            0x134: \'Y\',\n\n            0x136: \'L1\',\n            0x137: \'R1\',\n\n            0x13d: \'left_stick_press\',\n            0x13e: \'right_stick_press\',\n        }\n\n\nclass Nimbus(Joystick):\n    #An interface to a physical joystick available at /dev/input/js0\n    #contains mappings that work for the SteelNimbus joystick\n    #on Jetson TX2, JetPack 4.2, Ubuntu 18.04\n    def __init__(self, *args, **kwargs):\n        super(Nimbus, self).__init__(*args, **kwargs)\n\n        self.button_names = {\n            0x130 : \'a\',\n            0x131 : \'b\',\n            0x132 : \'x\',\n            0x133 : \'y\',\n            0x135 : \'R1\',\n            0x137 : \'R2\',\n            0x134 : \'L1\',\n            0x136 : \'L2\',\n        }\n\n        self.axis_names = {\n            0x0 : \'lx\',\n            0x1 : \'ly\',\n            0x2 : \'rx\',\n            0x5 : \'ry\',\n            0x11 : \'hmm\',\n            0x10 : \'what\',\n        }\n\n\nclass WiiU(Joystick):\n    #An interface to a physical joystick available at /dev/input/js0\n    #contains mappings may work for the WiiUPro joystick\n    #This was taken from\n    #https://github.com/autorope/donkeypart_bluetooth_game_controller/blob/master/donkeypart_bluetooth_game_controller/wiiu_config.yml\n    #and need testing!\n    def __init__(self, *args, **kwargs):\n        super(WiiU, self).__init__(*args, **kwargs)\n\n        self.button_names = {\n            305: \'A\',\n            304: \'B\',\n            307: \'X\',\n            308: \'Y\',\n            312: \'LEFT_BOTTOM_TRIGGER\',\n            310: \'LEFT_TOP_TRIGGER\',\n            313: \'RIGHT_BOTTOM_TRIGGER\',\n            311: \'RIGHT_TOP_TRIGGER\',\n            317: \'LEFT_STICK_PRESS\',\n            318: \'RIGHT_STICK_PRESS\',\n            314: \'SELECT\',\n            315: \'START\',\n            547: \'PAD_RIGHT\',\n            546: \'PAD_LEFT\',\n            544: \'PAD_UP\',\n            548: \'PAD_DOWN,\',\n        }\n\n        self.axis_names = {\n            0: \'LEFT_STICK_X\',\n            1: \'LEFT_STICK_Y\',\n            3: \'RIGHT_STICK_X\',\n            4: \'RIGHT_STICK_Y\',\n        }\n\n\nclass RC3ChanJoystick(Joystick):\n    #An interface to a physical joystick available at /dev/input/js0\n    def __init__(self, *args, **kwargs):\n        super(RC3ChanJoystick, self).__init__(*args, **kwargs)\n\n\n        self.button_names = {\n            0x120 : \'Switch-up\',\n            0x121 : \'Switch-down\',\n        }\n\n\n        self.axis_names = {\n            0x1 : \'Throttle\',\n            0x0 : \'Steering\',\n        }\n\n\nclass JoystickController(object):\n    \'\'\'\n    JoystickController is a base class. You will not use this class directly,\n    but instantiate a flavor based on your joystick type. See classes following this.\n\n    Joystick client using access to local physical input. Maps button\n    presses into actions and takes action. Interacts with the Donkey part\n    framework.\n    \'\'\'\n\n    ES_IDLE = -1\n    ES_START = 0\n    ES_THROTTLE_NEG_ONE = 1\n    ES_THROTTLE_POS_ONE = 2\n    ES_THROTTLE_NEG_TWO = 3\n\n\n    def __init__(self, poll_delay=0.0,\n                 throttle_scale=1.0,\n                 steering_scale=1.0,\n                 throttle_dir=-1.0,\n                 dev_fn=\'/dev/input/js0\',\n                 auto_record_on_throttle=True):\n\n        self.angle = 0.0\n        self.throttle = 0.0\n        self.mode = \'user\'\n        self.poll_delay = poll_delay\n        self.running = True\n        self.last_throttle_axis_val = 0\n        self.throttle_scale = throttle_scale\n        self.steering_scale = steering_scale\n        self.throttle_dir = throttle_dir\n        self.recording = False\n        self.constant_throttle = False\n        self.auto_record_on_throttle = auto_record_on_throttle\n        self.dev_fn = dev_fn\n        self.js = None\n        self.tub = None\n        self.num_records_to_erase = 100\n        self.estop_state = self.ES_IDLE\n        self.chaos_monkey_steering = None\n        self.dead_zone = 0.0\n\n        self.button_down_trigger_map = {}\n        self.button_up_trigger_map = {}\n        self.axis_trigger_map = {}\n        self.init_trigger_maps()\n\n\n    def init_js(self):\n        \'\'\'\n        Attempt to init joystick. Should be definied by derived class\n        Should return true on successfully created joystick object\n        \'\'\'\n        raise(Exception(""Subclass needs to define init_js""))\n\n\n    def init_trigger_maps(self):\n        \'\'\'\n        Creating mapping of buttons to functions.\n        Should be definied by derived class\n        \'\'\'\n        raise(Exception(""init_trigger_maps""))\n\n\n    def set_deadzone(self, val):\n        \'\'\'\n        sets the minimim throttle for recording\n        \'\'\'\n        self.dead_zone = val\n\n\n    def print_controls(self):\n        \'\'\'\n        print the mapping of buttons and axis to functions\n        \'\'\'\n        pt = PrettyTable()\n        pt.field_names = [""control"", ""action""]\n        for button, control in self.button_down_trigger_map.items():\n            pt.add_row([button, control.__name__])\n        for axis, control in self.axis_trigger_map.items():\n            pt.add_row([axis, control.__name__])\n        print(""Joystick Controls:"")\n        print(pt)\n\n        # print(""Joystick Controls:"")\n        # print(""On Button Down:"")\n        # print(self.button_down_trigger_map)\n        # print(""On Button Up:"")\n        # print(self.button_up_trigger_map)\n        # print(""On Axis Move:"")\n        # print(self.axis_trigger_map)\n\n\n    def set_button_down_trigger(self, button, func):\n        \'\'\'\n        assign a string button descriptor to a given function call\n        \'\'\'\n        self.button_down_trigger_map[button] = func\n\n\n    def set_button_up_trigger(self, button, func):\n        \'\'\'\n        assign a string button descriptor to a given function call\n        \'\'\'\n        self.button_up_trigger_map[button] = func\n\n\n    def set_axis_trigger(self, axis, func):\n        \'\'\'\n        assign a string axis descriptor to a given function call\n        \'\'\'\n        self.axis_trigger_map[axis] = func\n\n\n    def set_tub(self, tub):\n        self.tub = tub\n\n\n    def erase_last_N_records(self):\n        if self.tub is not None:\n            try:\n                self.tub.erase_last_n_records(self.num_records_to_erase)\n                print(\'erased last %d records.\' % self.num_records_to_erase)\n            except:\n                print(\'failed to erase\')\n\n\n    def on_throttle_changes(self):\n        \'\'\'\n        turn on recording when non zero throttle in the user mode.\n        \'\'\'\n        if self.auto_record_on_throttle:\n            self.recording = (abs(self.throttle) > self.dead_zone and self.mode == \'user\')\n\n\n    def emergency_stop(self):\n        \'\'\'\n        initiate a series of steps to try to stop the vehicle as quickly as possible\n        \'\'\'\n        print(\'E-Stop!!!\')\n        self.mode = ""user""\n        self.recording = False\n        self.constant_throttle = False\n        self.estop_state = self.ES_START\n        self.throttle = 0.0\n\n\n    def update(self):\n        \'\'\'\n        poll a joystick for input events\n        \'\'\'\n\n        #wait for joystick to be online\n        while self.running and self.js is None and not self.init_js():\n            time.sleep(3)\n\n        while self.running:\n            button, button_state, axis, axis_val = self.js.poll()\n\n            if axis is not None and axis in self.axis_trigger_map:\n                \'\'\'\n                then invoke the function attached to that axis\n                \'\'\'\n                self.axis_trigger_map[axis](axis_val)\n\n            if button and button_state >= 1 and button in self.button_down_trigger_map:\n                \'\'\'\n                then invoke the function attached to that button\n                \'\'\'\n                self.button_down_trigger_map[button]()\n\n            if button and button_state == 0 and button in self.button_up_trigger_map:\n                \'\'\'\n                then invoke the function attached to that button\n                \'\'\'\n                self.button_up_trigger_map[button]()\n\n            time.sleep(self.poll_delay)\n\n    def do_nothing(self, param):\n        \'\'\'assign no action to the given axis\n        this is useful to unmap certain axes, for example when swapping sticks\n        \'\'\'\n        pass\n\n\n\n    def set_steering(self, axis_val):\n        self.angle = self.steering_scale * axis_val\n        #print(""angle"", self.angle)\n\n\n    def set_throttle(self, axis_val):\n        #this value is often reversed, with positive value when pulling down\n        self.last_throttle_axis_val = axis_val\n        self.throttle = (self.throttle_dir * axis_val * self.throttle_scale)\n        #print(""throttle"", self.throttle)\n        self.on_throttle_changes()\n\n\n    def toggle_manual_recording(self):\n        \'\'\'\n        toggle recording on/off\n        \'\'\'\n        if self.auto_record_on_throttle:\n            print(\'auto record on throttle is enabled.\')\n        elif self.recording:\n            self.recording = False\n        else:\n            self.recording = True\n\n        print(\'recording:\', self.recording)\n\n\n    def increase_max_throttle(self):\n        \'\'\'\n        increase throttle scale setting\n        \'\'\'\n        self.throttle_scale = round(min(1.0, self.throttle_scale + 0.01), 2)\n        if self.constant_throttle:\n            self.throttle = self.throttle_scale\n            self.on_throttle_changes()\n        else:\n            self.throttle = (self.throttle_dir * self.last_throttle_axis_val * self.throttle_scale)\n\n        print(\'throttle_scale:\', self.throttle_scale)\n\n\n    def decrease_max_throttle(self):\n        \'\'\'\n        decrease throttle scale setting\n        \'\'\'\n        self.throttle_scale = round(max(0.0, self.throttle_scale - 0.01), 2)\n        if self.constant_throttle:\n            self.throttle = self.throttle_scale\n            self.on_throttle_changes()\n        else:\n            self.throttle = (self.throttle_dir * self.last_throttle_axis_val * self.throttle_scale)\n\n        print(\'throttle_scale:\', self.throttle_scale)\n\n\n    def toggle_constant_throttle(self):\n        \'\'\'\n        toggle constant throttle\n        \'\'\'\n        if self.constant_throttle:\n            self.constant_throttle = False\n            self.throttle = 0\n            self.on_throttle_changes()\n        else:\n            self.constant_throttle = True\n            self.throttle = self.throttle_scale\n            self.on_throttle_changes()\n        print(\'constant_throttle:\', self.constant_throttle)\n\n\n    def toggle_mode(self):\n        \'\'\'\n        switch modes from:\n        user: human controlled steer and throttle\n        local_angle: ai steering, human throttle\n        local: ai steering, ai throttle\n        \'\'\'\n        if self.mode == \'user\':\n            self.mode = \'local_angle\'\n        elif self.mode == \'local_angle\':\n            self.mode = \'local\'\n        else:\n            self.mode = \'user\'\n        print(\'new mode:\', self.mode)\n\n\n    def chaos_monkey_on_left(self):\n        self.chaos_monkey_steering = -0.2\n\n\n    def chaos_monkey_on_right(self):\n        self.chaos_monkey_steering = 0.2\n\n\n    def chaos_monkey_off(self):\n        self.chaos_monkey_steering = None\n\n\n    def run_threaded(self, img_arr=None):\n        self.img_arr = img_arr\n\n        \'\'\'\n        process E-Stop state machine\n        \'\'\'\n        if self.estop_state > self.ES_IDLE:\n            if self.estop_state == self.ES_START:\n                self.estop_state = self.ES_THROTTLE_NEG_ONE\n                return 0.0, -1.0 * self.throttle_scale, self.mode, False\n            elif self.estop_state == self.ES_THROTTLE_NEG_ONE:\n                self.estop_state = self.ES_THROTTLE_POS_ONE\n                return 0.0, 0.01, self.mode, False\n            elif self.estop_state == self.ES_THROTTLE_POS_ONE:\n                self.estop_state = self.ES_THROTTLE_NEG_TWO\n                self.throttle = -1.0 * self.throttle_scale\n                return 0.0, self.throttle, self.mode, False\n            elif self.estop_state == self.ES_THROTTLE_NEG_TWO:\n                self.throttle += 0.05\n                if self.throttle >= 0.0:\n                    self.throttle = 0.0\n                    self.estop_state = self.ES_IDLE\n                return 0.0, self.throttle, self.mode, False\n\n        if self.chaos_monkey_steering is not None:\n            return self.chaos_monkey_steering, self.throttle, self.mode, False\n\n        return self.angle, self.throttle, self.mode, self.recording\n\n\n    def run(self, img_arr=None):\n        raise Exception(""We expect for this part to be run with the threaded=True argument."")\n        return None, None, None, None\n\n\n    def shutdown(self):\n        #set flag to exit polling thread, then wait a sec for it to leave\n        self.running = False\n        time.sleep(0.5)\n\n\nclass JoystickCreatorController(JoystickController):\n    \'\'\'\n    A Controller object helps create a new controller object and mapping\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(JoystickCreatorController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        \'\'\'\n        attempt to init joystick\n        \'\'\'\n        try:\n            self.js = JoystickCreator(self.dev_fn)\n            if not self.js.init():\n                self.js = None\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        \'\'\'\n        init set of mapping from buttons to function calls\n        \'\'\'\n        pass\n\n\nclass PS3JoystickController(JoystickController):\n    \'\'\'\n    A Controller object that maps inputs to actions\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PS3JoystickController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        \'\'\'\n        attempt to init joystick\n        \'\'\'\n        try:\n            self.js = PS3Joystick(self.dev_fn)\n            if not self.js.init():\n                self.js = None\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        \'\'\'\n        init set of mapping from buttons to function calls\n        \'\'\'\n\n        self.button_down_trigger_map = {\n            \'select\' : self.toggle_mode,\n            \'circle\' : self.toggle_manual_recording,\n            \'triangle\' : self.erase_last_N_records,\n            \'cross\' : self.emergency_stop,\n            \'dpad_up\' : self.increase_max_throttle,\n            \'dpad_down\' : self.decrease_max_throttle,\n            \'start\' : self.toggle_constant_throttle,\n            ""R1"" : self.chaos_monkey_on_right,\n            ""L1"" : self.chaos_monkey_on_left,\n        }\n\n        self.button_up_trigger_map = {\n            ""R1"" : self.chaos_monkey_off,\n            ""L1"" : self.chaos_monkey_off,\n        }\n\n        self.axis_trigger_map = {\n            \'left_stick_horz\' : self.set_steering,\n            \'right_stick_vert\' : self.set_throttle,\n        }\n\n\nclass PS4JoystickController(JoystickController):\n    \'\'\'\n    A Controller object that maps inputs to actions\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(PS4JoystickController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        \'\'\'\n        attempt to init joystick\n        \'\'\'\n        try:\n            self.js = PS4Joystick(self.dev_fn)\n            if not self.js.init():\n                self.js = None\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        \'\'\'\n        init set of mapping from buttons to function calls for ps4\n        \'\'\'\n\n        self.button_down_trigger_map = {\n            \'share\' : self.toggle_mode,\n            \'circle\' : self.toggle_manual_recording,\n            \'triangle\' : self.erase_last_N_records,\n            \'cross\' : self.emergency_stop,\n            \'L1\' : self.increase_max_throttle,\n            \'R1\' : self.decrease_max_throttle,\n            \'options\' : self.toggle_constant_throttle,\n        }\n\n        self.axis_trigger_map = {\n            \'left_stick_horz\' : self.set_steering,\n            \'right_stick_vert\' : self.set_throttle,\n        }\n\n\nclass PyGamePS4JoystickController(PS4JoystickController):\n    \'\'\'\n    A Controller object that maps inputs to actions\n    \'\'\'\n    def __init__(self, which_js=0, *args, **kwargs):\n        super(PyGamePS4JoystickController, self).__init__(*args, **kwargs)\n        self.which_js=which_js\n\n\n    def init_js(self):\n        \'\'\'\n        attempt to init joystick\n        \'\'\'\n        try:\n            self.js = PyGamePS4Joystick(which_js=self.which_js)\n        except Exception as e:\n            print(e)\n            self.js = None\n        return self.js is not None\n\n\n\nclass XboxOneJoystickController(JoystickController):\n    \'\'\'\n    A Controller object that maps inputs to actions\n    credit:\n    https://github.com/Ezward/donkeypart_ps3_controller/blob/master/donkeypart_ps3_controller/part.py\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(XboxOneJoystickController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        \'\'\'\n        attempt to init joystick\n        \'\'\'\n        try:\n            self.js = XboxOneJoystick(self.dev_fn)\n            self.js.init()\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def magnitude(self, reversed = False):\n        def set_magnitude(axis_val):\n            \'\'\'\n            Maps raw axis values to magnitude.\n            \'\'\'\n            # Axis values range from -1. to 1.\n            minimum = -1.\n            maximum = 1.\n            # Magnitude is now normalized in the range of 0 - 1.\n            magnitude = (axis_val - minimum) / (maximum - minimum)\n            if reversed:\n                magnitude *= -1\n            self.set_throttle(magnitude)\n        return set_magnitude\n\n\n    def init_trigger_maps(self):\n        \'\'\'\n        init set of mapping from buttons to function calls\n        \'\'\'\n\n        self.button_down_trigger_map = {\n            \'a_button\': self.toggle_mode,\n            \'b_button\': self.toggle_manual_recording,\n            \'x_button\': self.erase_last_N_records,\n            \'y_button\': self.emergency_stop,\n            \'right_shoulder\': self.increase_max_throttle,\n            \'left_shoulder\': self.decrease_max_throttle,\n            \'options\': self.toggle_constant_throttle,\n        }\n\n        self.axis_trigger_map = {\n            \'left_stick_horz\': self.set_steering,\n            \'right_stick_vert\': self.set_throttle,\n            # Forza Mode\n            \'right_trigger\': self.magnitude(),\n            \'left_trigger\': self.magnitude(reversed = True),\n        }\n\nclass XboxOneSwappedJoystickController(XboxOneJoystickController):\n    \'\'\'\n    Swap steering and throttle controls from std XBox one controller\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(XboxOneSwappedJoystickController, self).__init__(*args, **kwargs)\n\n    def init_trigger_maps(self):\n        \'\'\'\n        init set of mapping from buttons to function calls\n        \'\'\'\n        super(XboxOneSwappedJoystickController, self).init_trigger_maps()\n\n        # make the actual swap of the sticks\n        self.set_axis_trigger(\'right_stick_horz\', self.set_steering)\n        self.set_axis_trigger(\'left_stick_vert\', self.set_throttle)\n\n        # unmap default assinments to the axes\n        self.set_axis_trigger(\'left_stick_horz\', self.do_nothing)\n        self.set_axis_trigger(\'right_stick_vert\', self.do_nothing)\n\n\nclass LogitechJoystickController(JoystickController):\n    \'\'\'\n    A Controller object that maps inputs to actions\n    credit:\n    https://github.com/kevkruemp/donkeypart_logitech_controller/blob/master/donkeypart_logitech_controller/part.py\n    \'\'\'\n    def __init__(self, *args, **kwargs):\n        super(LogitechJoystickController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        \'\'\'\n        attempt to init joystick\n        \'\'\'\n        try:\n            self.js = LogitechJoystick(self.dev_fn)\n            self.js.init()\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        \'\'\'\n        init set of mapping from buttons to function calls\n        \'\'\'\n\n        self.button_down_trigger_map = {\n            \'start\': self.toggle_mode,\n            \'B\': self.toggle_manual_recording,\n            \'Y\': self.erase_last_N_records,\n            \'A\': self.emergency_stop,\n            \'back\': self.toggle_constant_throttle,\n            ""R1"" : self.chaos_monkey_on_right,\n            ""L1"" : self.chaos_monkey_on_left,\n        }\n\n        self.button_up_trigger_map = {\n            ""R1"" : self.chaos_monkey_off,\n            ""L1"" : self.chaos_monkey_off,\n        }\n\n        self.axis_trigger_map = {\n            \'left_stick_horz\': self.set_steering,\n            \'right_stick_vert\': self.set_throttle,\n            \'dpad_leftright\' : self.on_axis_dpad_LR,\n            \'dpad_up_down\' : self.on_axis_dpad_UD,\n        }\n\n    def on_axis_dpad_LR(self, val):\n        if val == -1.0:\n            self.on_dpad_left()\n        elif val == 1.0:\n            self.on_dpad_right()\n\n    def on_axis_dpad_UD(self, val):\n        if val == -1.0:\n            self.on_dpad_up()\n        elif val == 1.0:\n            self.on_dpad_down()\n\n    def on_dpad_up(self):\n        self.increase_max_throttle()\n\n    def on_dpad_down(self):\n        self.decrease_max_throttle()\n\n    def on_dpad_left(self):\n        print(""dpad left un-mapped"")\n\n    def on_dpad_right(self):\n        print(""dpad right un-mapped"")\n\n\nclass NimbusController(JoystickController):\n    #A Controller object that maps inputs to actions\n    def __init__(self, *args, **kwargs):\n        super(NimbusController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        #attempt to init joystick\n        try:\n            self.js = Nimbus(self.dev_fn)\n            self.js.init()\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        #init set of mapping from buttons to function calls\n\n        self.button_down_trigger_map = {\n            \'y\' : self.erase_last_N_records,\n            \'b\' : self.toggle_mode,\n            \'a\' : self.emergency_stop,\n        }\n\n        self.axis_trigger_map = {\n            \'lx\' : self.set_steering,\n            \'ry\' : self.set_throttle,\n        }\n\n\nclass WiiUController(JoystickController):\n    #A Controller object that maps inputs to actions\n    def __init__(self, *args, **kwargs):\n        super(WiiUController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        #attempt to init joystick\n        try:\n            self.js = WiiU(self.dev_fn)\n            self.js.init()\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n\n    def init_trigger_maps(self):\n        #init set of mapping from buttons to function calls\n\n        self.button_down_trigger_map = {\n            \'Y\' : self.erase_last_N_records,\n            \'B\' : self.toggle_mode,\n            \'A\' : self.emergency_stop,\n        }\n\n        self.axis_trigger_map = {\n            \'LEFT_STICK_X\' : self.set_steering,\n            \'RIGHT_STICK_Y\' : self.set_throttle,\n        }\n\n\n\nclass RC3ChanJoystickController(JoystickController):\n    #A Controller object that maps inputs to actions\n    def __init__(self, *args, **kwargs):\n        super(RC3ChanJoystickController, self).__init__(*args, **kwargs)\n\n\n    def init_js(self):\n        #attempt to init joystick\n        try:\n            self.js = RC3ChanJoystick(self.dev_fn)\n            self.js.init()\n        except FileNotFoundError:\n            print(self.dev_fn, ""not found."")\n            self.js = None\n        return self.js is not None\n\n    def on_steering(self, val, reverse = True):\n        if reversed:\n            val *= -1\n        self.set_steering(val)\n\n    def on_throttle(self, val, reverse = True):\n        if reversed:\n            val *= -1\n        self.set_throttle(val)\n\n    def on_switch_up(self):\n        if self.mode == \'user\':\n            self.erase_last_N_records()\n        else:\n            self.emergency_stop()\n\n    def on_switch_down(self):\n        self.toggle_mode()\n\n    def init_trigger_maps(self):\n        #init set of mapping from buttons to function calls\n\n        self.button_down_trigger_map = {\n            \'Switch-down\' : self.on_switch_down,\n            \'Switch-up\' : self.on_switch_up,\n        }\n\n\n        self.axis_trigger_map = {\n            \'Steering\' : self.on_steering,\n            \'Throttle\' : self.on_throttle,\n        }\n\n\nclass JoyStickPub(object):\n    \'\'\'\n    Use Zero Message Queue (zmq) to publish the control messages from a local joystick\n    \'\'\'\n    def __init__(self, port = 5556, dev_fn=\'/dev/input/js1\'):\n        import zmq\n        self.dev_fn = dev_fn\n        self.js = PS3JoystickPC(self.dev_fn)\n        self.js.init()\n        context = zmq.Context()\n        self.socket = context.socket(zmq.PUB)\n        self.socket.bind(""tcp://*:%d"" % port)\n\n\n    def run(self):\n        while True:\n            button, button_state, axis, axis_val = self.js.poll()\n            if axis is not None or button is not None:\n                if button is None:\n                    button  = ""0""\n                    button_state = 0\n                if axis is None:\n                    axis = ""0""\n                    axis_val = 0\n                message_data = (button, button_state, axis, axis_val)\n                self.socket.send_string( ""%s %d %s %f"" % message_data)\n                print(""SENT"", message_data)\n\n\nclass JoyStickSub(object):\n    \'\'\'\n    Use Zero Message Queue (zmq) to subscribe to control messages from a remote joystick\n    \'\'\'\n    def __init__(self, ip, port = 5556):\n        import zmq\n        context = zmq.Context()\n        self.socket = context.socket(zmq.SUB)\n        self.socket.connect(""tcp://%s:%d"" % (ip, port))\n        self.socket.setsockopt_string(zmq.SUBSCRIBE, \'\')\n        self.button = None\n        self.button_state = 0\n        self.axis = None\n        self.axis_val = 0.0\n        self.running = True\n\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(0.1)\n\n\n    def update(self):\n        while self.running:\n            payload = self.socket.recv().decode(""utf-8"")\n            #print(""got"", payload)\n            button, button_state, axis, axis_val = payload.split(\' \')\n            self.button = button\n            self.button_state = (int)(button_state)\n            self.axis = axis\n            self.axis_val = (float)(axis_val)\n            if self.button == ""0"":\n                self.button = None\n            if self.axis == ""0"":\n                self.axis = None\n\n\n    def run_threaded(self):\n        pass\n\n\n    def poll(self):\n        ret = (self.button, self.button_state, self.axis, self.axis_val)\n        self.button = None\n        self.axis = None\n        return ret\n\n\ndef get_js_controller(cfg):\n    cont_class = None\n    if cfg.CONTROLLER_TYPE == ""ps3"":\n        cont_class = PS3JoystickController\n    elif cfg.CONTROLLER_TYPE == ""ps4"":\n        cont_class = PS4JoystickController\n    elif cfg.CONTROLLER_TYPE == ""nimbus"":\n        cont_class = NimbusController\n    elif cfg.CONTROLLER_TYPE == ""xbox"":\n        cont_class = XboxOneJoystickController\n    elif cfg.CONTROLLER_TYPE == ""xboxswapped"":\n        cont_class = XboxOneSwappedJoystickController\n    elif cfg.CONTROLLER_TYPE == ""wiiu"":\n        cont_class = WiiUController\n    elif cfg.CONTROLLER_TYPE == ""F710"":\n        cont_class = LogitechJoystickController\n    elif cfg.CONTROLLER_TYPE == ""rc3"":\n        cont_class = RC3ChanJoystickController\n    elif cfg.CONTROLLER_TYPE == ""pygame"":\n        cont_class = PyGamePS4JoystickController\n    else:\n        raise( Exception(""Unknown controller type: "" + cfg.CONTROLLER_TYPE))\n\n    ctr = cont_class(throttle_dir=cfg.JOYSTICK_THROTTLE_DIR,\n                                throttle_scale=cfg.JOYSTICK_MAX_THROTTLE,\n                                steering_scale=cfg.JOYSTICK_STEERING_SCALE,\n                                auto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE,\n                                dev_fn=cfg.JOYSTICK_DEVICE_FILE)\n\n    ctr.set_deadzone(cfg.JOYSTICK_DEADZONE)\n    return ctr\n\nif __name__ == ""__main__"":\n    \'\'\'\n    publish ps3 controller\n    when running from ubuntu 16.04, it will interfere w mouse until:\n    xinput set-prop ""Sony PLAYSTATION(R)3 Controller"" ""Device Enabled"" 0\n    \'\'\'\n    print(""You may need:"")\n    print(\'xinput set-prop ""Sony PLAYSTATION(R)3 Controller"" ""Device Enabled"" 0\')\n    #p = JoyStickPub()\n\n    \n    #Ps4 pygame controller test.\n    import donkeycar\n    v = donkeycar.vehicle.Vehicle()\n    p = PyGamePS4JoystickController()\n    v.add(p, inputs=[\'cam/image_array\'],\n          outputs=[\'user/angle\', \'user/throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n    v.start(max_loop_count = 100)\n    \n    \'\'\'\n    j = PyGamePS4Joystick(which_js=0)\n    i = 0\n    while i < 100:\n        j.poll()\n        time.sleep(0.1)\n        i += 1\n    \'\'\'\n'"
donkeycar/parts/coral.py,0,"b'""""""Inference Engine used for inference tasks.""""""\n\nfrom edgetpu.basic.basic_engine import BasicEngine\nimport numpy\nfrom PIL import Image\n\n\nclass InferenceEngine(BasicEngine):\n  """"""Engine used for inference task.""""""\n\n  def __init__(self, model_path, device_path=None):\n    """"""Creates a BasicEngine with given model.\n\n    Args:\n      model_path: String, path to TF-Lite Flatbuffer file.\n      device_path: String, if specified, bind engine with Edge TPU at device_path.\n\n    Raises:\n      ValueError: An error occurred when the output format of model is invalid.\n    """"""\n    if device_path:\n      super().__init__(model_path, device_path)\n    else:\n      super().__init__(model_path)\n    output_tensors_sizes = self.get_all_output_tensors_sizes()\n    if output_tensors_sizes.size > 2:\n      raise ValueError(\n          (\'Inference model should have 2 output tensors or less!\'\n           \'This model has {}.\'.format(output_tensors_sizes.size)))\n\n  def Inference(self, img):\n    """"""Inference image with np array image object.\n\n    This interface assumes the loaded model is trained for image\n    classification.\n\n    Args:\n      img: numpy.array image object.\n\n    Returns:\n      List of (float) which represents inference results.\n\n    Raises:\n      RuntimeError: when tensor not a single 3 channel image.\n      Asserts: when image incorrect size.\n    """"""\n    input_tensor_shape = self.get_input_tensor_shape()\n    if (input_tensor_shape.size != 4 or input_tensor_shape[3] != 3 or\n        input_tensor_shape[0] != 1):\n      raise RuntimeError(\n          \'Invalid input tensor shape! Expected: [1, height, width, 3]\')\n    _, height, width, _ = input_tensor_shape\n    assert(height == img.shape[0])\n    assert(width == img.shape[1])\n    input_tensor = img.flatten()\n    return self.RunInferenceWithInputTensor(input_tensor)\n\n  def RunInferenceWithInputTensor(self, input_tensor):\n    """"""Run inference with raw input tensor.\n\n    This interface requires user to process input data themselves and convert\n    it to formatted input tensor.\n\n    Args:\n      input_tensor: numpy.array represents the input tensor.\n\n    Returns:\n      List of (float) which represents inference.\n\n    Raises:\n      ValueError: when input param is invalid.\n    """"""\n    _, self._raw_result = self.RunInference(\n        input_tensor)\n    return [self._raw_result]\n\n\n\nclass CoralLinearPilot(object):\n  \'\'\'\n  Base class for TFlite models that will provide steering and throttle to guide a car.\n  \'\'\'\n  def __init__(self):\n      self.model = None\n      self.engine = None\n\n  def load(self, model_path):\n      # Load Coral edgetpu TFLite model and allocate tensors.\n      self.engine = InferenceEngine(model_path)\n\n  def run(self, image):\n      steering, throttle = self.engine.Inference(image)[0]\n      return steering, throttle\n'"
donkeycar/parts/cv.py,0,"b""import time\nimport cv2\nimport numpy as np\n\nclass ImgGreyscale():\n\n    def run(self, img_arr):\n        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_RGB2GRAY)\n        return img_arr\n\n    def shutdown(self):\n        pass\n\nclass ImgWriter():\n\n    def __init__(self, filename):\n        self.filename = filename\n\n    def run(self, img_arr):\n        cv2.imwrite(self.filename, img_arr)\n\n    def shutdown(self):\n        pass\n\nclass ImgBGR2RGB():\n\n    def run(self, img_arr):\n        if img_arr is None:\n            return None\n        try:\n            img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n            return img_arr\n        except:\n            return None\n\n    def shutdown(self):\n        pass\n\nclass ImgRGB2BGR():\n\n    def run(self, img_arr):\n        if img_arr is None:\n            return None\n        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_RGB2BGR)\n        return img_arr\n\n    def shutdown(self):\n        pass\n\nclass ImageScale():\n\n    def __init__(self, scale):\n        self.scale = scale\n\n    def run(self, img_arr):\n        if img_arr is None:\n            return None\n        try:\n            return cv2.resize(img_arr, (0,0), fx=self.scale, fy=self.scale)\n        except:\n            return None\n\n    def shutdown(self):\n        pass\n\nclass ImageRotateBound():\n    '''\n    credit:\n    https://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/\n    '''\n\n    def __init__(self, rot_deg):\n        self.rot_deg = rot_deg\n\n    def run(self, image):\n        if image is None:\n            return None\n\n        # grab the dimensions of the image and then determine the\n        # center\n        (h, w) = image.shape[:2]\n        (cX, cY) = (w // 2, h // 2)\n    \n        # grab the rotation matrix (applying the negative of the\n        # angle to rotate clockwise), then grab the sine and cosine\n        # (i.e., the rotation components of the matrix)\n        M = cv2.getRotationMatrix2D((cX, cY), -self.rot_deg, 1.0)\n        cos = np.abs(M[0, 0])\n        sin = np.abs(M[0, 1])\n    \n        # compute the new bounding dimensions of the image\n        nW = int((h * sin) + (w * cos))\n        nH = int((h * cos) + (w * sin))\n    \n        # adjust the rotation matrix to take into account translation\n        M[0, 2] += (nW / 2) - cX\n        M[1, 2] += (nH / 2) - cY\n    \n        # perform the actual rotation and return the image\n        return cv2.warpAffine(image, M, (nW, nH))\n\n    def shutdown(self):\n        pass\n\nclass ImgCanny():\n\n    def __init__(self, low_threshold=60, high_threshold=110):\n        self.low_threshold = low_threshold\n        self.high_threshold = high_threshold\n        \n        \n    def run(self, img_arr):\n        return cv2.Canny(img_arr, \n                         self.low_threshold, \n                         self.high_threshold)\n\n    def shutdown(self):\n        pass\n    \n\nclass ImgGaussianBlur():\n\n    def __init__(self, kernal_size=5):\n        self.kernal_size = kernal_size\n        \n    def run(self, img_arr):\n        return cv2.GaussianBlur(img_arr, \n                                (self.kernel_size, self.kernel_size), 0)\n\n    def shutdown(self):\n        pass\n\n\nclass ArrowKeyboardControls:\n    '''\n    kind of sucky control, only one press active at a time. \n    good enough for a little testing.\n    requires that you have an CvImageView open and it has focus.\n    '''\n    def __init__(self):\n        self.left = 2424832\n        self.right = 2555904\n        self.up = 2490368\n        self.down = 2621440\n        self.codes = [self.left, self.right, self.down, self.up]\n        self.vec = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    def run(self):\n        code = cv2.waitKeyEx(delay=100)\n        for iCode, keyCode in enumerate(self.codes):\n            if keyCode == code:\n                return self.vec[iCode]\n        return (0., 0.)\n        \n        \n        \nclass Pipeline():\n    def __init__(self, steps):\n        self.steps = steps\n    \n    def run(self, val):\n        for step in self.steps:\n            f = step['f']\n            args = step['args']\n            kwargs = step['kwargs']\n            \n            val = f(val, *args, **kwargs)\n        return val\n    \nclass CvCam(object):\n    def __init__(self, image_w=160, image_h=120, image_d=3, iCam=0):\n\n        self.frame = None\n        self.cap = cv2.VideoCapture(iCam)\n        self.running = True\n        self.cap.set(3, image_w)\n        self.cap.set(4, image_h)\n\n    def poll(self):\n        if self.cap.isOpened():\n            ret, self.frame = self.cap.read()\n\n    def update(self):\n        '''\n        poll the camera for a frame\n        '''\n        while(self.running):\n            self.poll()\n\n    def run_threaded(self):\n        return self.frame\n\n    def run(self):\n        self.poll()\n        return self.frame\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(0.2)\n        self.cap.release()\n\n\nclass CvImageView(object):\n\n    def run(self, image):\n        if image is None:\n            return\n        try:\n            cv2.imshow('frame', image)\n            cv2.waitKey(1)\n        except:\n            pass\n\n    def shutdown(self):\n        cv2.destroyAllWindows()\n"""
donkeycar/parts/datastore.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Jul  4 12:32:53 2017\n\n@author: wroscoe\n""""""\nimport os\nimport sys\nimport time\nimport json\nimport datetime\nimport random\nimport glob\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\n\nfrom donkeycar.parts.augment import augment_pil_image\nfrom donkeycar.utils import arr_to_img\n\n\nclass Tub(object):\n    """"""\n    A datastore to store sensor data in a key, value format.\n\n    Accepts str, int, float, image_array, image, and array data types.\n\n    For example:\n\n    #Create a tub to store speed values.\n    >>> path = \'~/mydonkey/test_tub\'\n    >>> inputs = [\'user/speed\', \'cam/image\']\n    >>> types = [\'float\', \'image\']\n    >>> t=Tub(path=path, inputs=inputs, types=types)\n\n    """"""\n\n    def __init__(self, path, inputs=None, types=None, user_meta=[]):\n\n        self.path = os.path.expanduser(path)\n        #print(\'path_in_tub:\', self.path)\n        self.meta_path = os.path.join(self.path, \'meta.json\')\n        self.exclude_path = os.path.join(self.path, ""exclude.json"")\n        self.df = None\n\n        exists = os.path.exists(self.path)\n\n        if exists:\n            #load log and meta\n            #print(""Tub exists: {}"".format(self.path))\n            try:\n                with open(self.meta_path, \'r\') as f:\n                    self.meta = json.load(f)\n            except FileNotFoundError:\n                self.meta = {\'inputs\': [], \'types\': []}\n\n            try:\n                with open(self.exclude_path,\'r\') as f:\n                    excl = json.load(f) # stored as a list\n                    self.exclude = set(excl)\n            except FileNotFoundError:\n                self.exclude = set()\n\n            try:\n                self.current_ix = self.get_last_ix() + 1\n            except ValueError:\n                self.current_ix = 0\n\n            if \'start\' in self.meta:\n                self.start_time = self.meta[\'start\']\n            else:\n                self.start_time = time.time()\n                self.meta[\'start\'] = self.start_time\n\n        elif not exists and inputs:\n            print(\'Tub does NOT exist. Creating new tub...\')\n            self.start_time = time.time()\n            #create log and save meta\n            os.makedirs(self.path)\n            self.meta = {\'inputs\': inputs, \'types\': types, \'start\': self.start_time}\n            for kv in user_meta:\n                kvs = kv.split("":"")\n                if len(kvs) == 2:\n                    self.meta[kvs[0]] = kvs[1]\n                # else exception? print message?\n            with open(self.meta_path, \'w\') as f:\n                json.dump(self.meta, f)\n            self.current_ix = 0\n            self.exclude = set()\n            print(\'New tub created at: {}\'.format(self.path))\n        else:\n            msg = ""The tub path you provided doesn\'t exist and you didnt pass any meta info (inputs & types)"" + \\\n                  ""to create a new tub. Please check your tub path or provide meta info to create a new tub.""\n\n            raise AttributeError(msg)\n\n    def get_last_ix(self):\n        index = self.get_index()           \n        return max(index)\n\n    def update_df(self):\n        df = pd.DataFrame([self.get_json_record(i) for i in self.get_index(shuffled=False)])\n        self.df = df\n\n    def get_df(self):\n        if self.df is None:\n            self.update_df()\n        return self.df\n\n    def get_index(self, shuffled=True):\n        files = next(os.walk(self.path))[2]\n        record_files = [f for f in files if f[:6] == \'record\']\n\n        def get_file_ix(file_name):\n            try:\n                name = file_name.split(\'.\')[0]\n                num = int(name.split(\'_\')[1])\n            except:\n                num = 0\n            return num\n\n        nums = [get_file_ix(f) for f in record_files]\n        \n        if shuffled:\n            random.shuffle(nums)\n        else:\n            nums = sorted(nums)\n            \n        return nums \n\n    @property\n    def inputs(self):\n        return list(self.meta[\'inputs\'])\n\n    @property\n    def types(self):\n        return list(self.meta[\'types\'])\n\n    def get_input_type(self, key):\n        input_types = dict(zip(self.inputs, self.types))\n        return input_types.get(key)\n\n    def write_json_record(self, json_data):\n        path = self.get_json_record_path(self.current_ix)\n        try:\n            with open(path, \'w\') as fp:\n                json.dump(json_data, fp)\n\n        except TypeError:\n            print(\'troubles with record:\', json_data)\n        except FileNotFoundError:\n            raise\n        except:\n            print(""Unexpected error:"", sys.exc_info()[0])\n            raise\n\n    def get_num_records(self):\n        import glob\n        files = glob.glob(os.path.join(self.path, \'record_*.json\'))\n        return len(files)\n\n    def make_record_paths_absolute(self, record_dict):\n        # make paths absolute\n        d = {}\n        for k, v in record_dict.items():\n            if type(v) == str: #filename\n                if \'.\' in v:\n                    v = os.path.join(self.path, v)\n            d[k] = v\n\n        return d\n\n    def check(self, fix=False):\n        """"""\n        Iterate over all records and make sure we can load them.\n        Optionally remove records that cause a problem.\n        """"""\n        print(\'Checking tub:%s.\' % self.path)\n        print(\'Found: %d records.\' % self.get_num_records())\n        problems = False\n        for ix in self.get_index(shuffled=False):\n            try:\n                self.get_record(ix)\n            except:\n                problems = True\n                if fix == False:\n                    print(\'problems with record:\', self.path, ix)\n                else:\n                    print(\'problems with record, removing:\', self.path, ix)\n                    self.remove_record(ix)\n        if not problems:\n            print(""No problems found."")\n\n    def remove_record(self, ix):\n        \'\'\'\n        remove data associate with a record\n        \'\'\'\n        record = self.get_json_record_path(ix)\n        os.unlink(record)\n\n    def put_record(self, data):\n        """"""\n        Save values like images that can\'t be saved in the csv log and\n        return a record with references to the saved values that can\n        be saved in a csv.\n        """"""\n        json_data = {}\n        self.current_ix += 1\n        \n        for key, val in data.items():\n            typ = self.get_input_type(key)\n\n            if (val is not None) and (typ == \'float\'):\n                # in case val is a numpy.float32, which json doesn\'t like\n                json_data[key] = float(val)\n\n            elif typ in [\'str\', \'float\', \'int\', \'boolean\', \'vector\']:\n                json_data[key] = val\n\n            elif typ is \'image\':\n                path = self.make_file_path(key)\n                val.save(path)\n                json_data[key]=path\n\n            elif typ == \'image_array\':\n                img = Image.fromarray(np.uint8(val))\n                name = self.make_file_name(key, ext=\'.jpg\')\n                img.save(os.path.join(self.path, name))\n                json_data[key]=name\n\n            elif typ == \'gray16_array\':\n                # save np.uint16 as a 16bit png\n                img = Image.fromarray(np.uint16(val))\n                name = self.make_file_name(key, ext=\'.png\')\n                img.save(os.path.join(self.path, name))\n                json_data[key]=name\n\n            else:\n                msg = \'Tub does not know what to do with this type {}\'.format(typ)\n                raise TypeError(msg)\n\n        json_data[\'milliseconds\'] = int((time.time() - self.start_time) * 1000)\n\n        self.write_json_record(json_data)\n        return self.current_ix\n\n    def erase_last_n_records(self, num_erase):\n        """"""\n        erase N records from the disc and move current back accordingly\n        """"""\n        last_erase = self.current_ix\n        first_erase = last_erase - num_erase\n        self.current_ix = first_erase - 1\n        if self.current_ix < 0:\n            self.current_ix = 0\n\n        for i in range(first_erase, last_erase):\n            if i < 0:\n                continue\n            self.erase_record(i)\n\n    def erase_record(self, i):\n        json_path = self.get_json_record_path(i)\n        if os.path.exists(json_path):\n            os.unlink(json_path)\n        img_filename = \'%d_cam-image_array_.jpg\' % i\n        img_path = os.path.join(self.path, img_filename)\n        if os.path.exists(img_path):\n            os.unlink(img_path)\n\n    def get_json_record_path(self, ix):\n        return os.path.join(self.path, \'record_\' + str(ix) + \'.json\')\n\n    def get_json_record(self, ix):\n        path = self.get_json_record_path(ix)\n        try:\n            with open(path, \'r\') as fp:\n                json_data = json.load(fp)\n        except UnicodeDecodeError:\n            raise Exception(\'bad record: %d. You may want to run `python manage.py check --fix`\' % ix)\n        except FileNotFoundError:\n            raise\n        except:\n            print(""Unexpected error:"", sys.exc_info()[0])\n            raise\n\n        record_dict = self.make_record_paths_absolute(json_data)\n        return record_dict\n\n    def get_record(self, ix):\n        json_data = self.get_json_record(ix)\n        data = self.read_record(json_data)\n        return data\n\n    def read_record(self, record_dict):\n        data = {}\n        for key, val in record_dict.items():\n            typ = self.get_input_type(key)\n            # load objects that were saved as separate files\n            if typ == \'image_array\':\n                img = Image.open((val))\n                val = np.array(img)\n            data[key] = val\n        return data\n\n    def gather_records(self):\n        ri = lambda fnm: int(os.path.basename(fnm).split(\'_\')[1].split(\'.\')[0])\n        record_paths = glob.glob(os.path.join(self.path, \'record_*.json\'))\n        if len(self.exclude) > 0:\n            record_paths = [f for f in record_paths if ri(f) not in self.exclude]\n        record_paths.sort(key=ri)\n        return record_paths\n\n    def make_file_name(self, key, ext=\'.png\', ix=None):\n        this_ix = ix\n        if this_ix is None:\n            this_ix = self.current_ix\n        name = \'_\'.join([str(this_ix), key, ext])\n        name = name.replace(\'/\', \'-\')\n        return name\n\n    def delete(self):\n        """""" Delete the folder and files for this tub. """"""\n        import shutil\n        shutil.rmtree(self.path)\n\n    def shutdown(self):\n        pass\n\n    def excluded(self, index):\n        return index in self.exclude\n\n    def exclude_index(self, index):\n        self.exclude.add(index)\n\n    def include_index(self, index):\n        try:\n            self.exclude.remove(index)\n        except:\n            pass\n\n    def augment_images(self):\n        # Get all record\'s index\n        index = self.get_index(shuffled=False)\n        # Go through index\n        count = 0\n        for ix in index:\n            data = self.get_record(ix)\n            for key, val in data.items():\n                typ = self.get_input_type(key)\n                if typ == \'image_array\':\n                    # here val is an img_arr\n                    img = arr_to_img(val)\n                    # then augment and denormalise\n                    img_aug = augment_pil_image(img)\n                    name = self.make_file_name(key, ext=\'.jpg\', ix=ix)\n                    try:\n                        img_aug.save(os.path.join(self.path, name))\n                        count += 1\n                    except IOError as err:\n                        print(err)\n        print(\'Augmenting\', count, \'images in\', self.path)\n\n    def write_exclude(self):\n        if 0 == len(self.exclude):\n            # If the exclude set is empty don\'t leave an empty file around.\n            if os.path.exists(self.exclude_path):\n                os.unlink(self.exclude_path)\n        else:\n            with open(self.exclude_path, \'w\') as f:\n                json.dump(list(self.exclude), f)\n\n    def get_record_gen(self, record_transform=None, shuffle=True, df=None):\n\n        if df is None:\n            df = self.get_df()\n\n        while True:\n            for _, row in self.df.iterrows():\n                if shuffle:\n                    record_dict = df.sample(n=1).to_dict(orient=\'record\')[0]\n                else:\n                    record_dict = row\n\n                if record_transform:\n                    record_dict = record_transform(record_dict)\n\n                record_dict = self.read_record(record_dict)\n                yield record_dict\n\n    def get_batch_gen(self, keys, record_transform=None, batch_size=128, shuffle=True, df=None):\n\n        record_gen = self.get_record_gen(record_transform, shuffle=shuffle, df=df)\n\n        if keys is None:\n            keys = list(self.df.columns)\n\n        while True:\n            record_list = []\n            for _ in range(batch_size):\n                record_list.append(next(record_gen))\n\n            batch_arrays = {}\n            for i, k in enumerate(keys):\n                arr = np.array([r[k] for r in record_list])\n                batch_arrays[k] = arr\n\n            yield batch_arrays\n\n    def get_train_gen(self, X_keys, Y_keys, batch_size=128, record_transform=None, df=None):\n\n        batch_gen = self.get_batch_gen(X_keys + Y_keys,\n                                       batch_size=batch_size, record_transform=record_transform, df=df)\n\n        while True:\n            batch = next(batch_gen)\n            X = [batch[k] for k in X_keys]\n            Y = [batch[k] for k in Y_keys]\n            yield X, Y\n\n    def get_train_val_gen(self, X_keys, Y_keys, batch_size=128, record_transform=None, train_frac=.8):\n        train_df = train=self.df.sample(frac=train_frac,random_state=200)\n        val_df = self.df.drop(train_df.index)\n\n        train_gen = self.get_train_gen(X_keys=X_keys, Y_keys=Y_keys, batch_size=batch_size,\n                                       record_transform=record_transform, df=train_df)\n\n        val_gen = self.get_train_gen(X_keys=X_keys, Y_keys=Y_keys, batch_size=batch_size,\n                                       record_transform=record_transform, df=val_df)\n\n        return train_gen, val_gen\n\n\nclass TubWriter(Tub):\n    def __init__(self, *args, **kwargs):\n        super(TubWriter, self).__init__(*args, **kwargs)\n\n    def run(self, *args):\n        """"""\n        API function needed to use as a Donkey part. Accepts values,\n        pairs them with their inputs keys and saves them to disk.\n        """"""\n        assert len(self.inputs) == len(args)\n        record = dict(zip(self.inputs, args))\n        self.put_record(record)\n        return self.current_ix\n\n\nclass TubReader(Tub):\n    def __init__(self, path, *args, **kwargs):\n        super(TubReader, self).__init__(*args, **kwargs)\n\n    def run(self, *args):\n        """"""\n        API function needed to use as a Donkey part.\n        Accepts keys to read from the tub and retrieves them sequentially.\n        """"""\n        record_dict = self.get_record()\n        record = [record_dict[key] for key in args]\n        return record\n\n\nclass TubHandler:\n    def __init__(self, path):\n        self.path = os.path.expanduser(path)\n\n    def get_tub_list(self, path):\n        folders = next(os.walk(path))[1]\n        return folders\n\n    def next_tub_number(self, path):\n        def get_tub_num(tub_name):\n            try:\n                num = int(tub_name.split(\'_\')[1])\n            except:\n                num = 0\n            return num\n\n        folders = self.get_tub_list(path)\n        numbers = [get_tub_num(x) for x in folders]\n        next_number = max(numbers+[0]) + 1\n        return next_number\n\n    def create_tub_path(self):\n        tub_num = self.next_tub_number(self.path)\n        date = datetime.datetime.now().strftime(\'%y-%m-%d\')\n        name = \'_\'.join([\'tub\', str(tub_num), date])\n        tub_path = os.path.join(self.path, name)\n        return tub_path\n\n    def new_tub_writer(self, inputs, types, user_meta=[]):\n        tub_path = self.create_tub_path()\n        tw = TubWriter(path=tub_path, inputs=inputs, types=types, user_meta=user_meta)\n        return tw\n\n\nclass TubImageStacker(Tub):\n    \'\'\'\n    A Tub for training a NN with images that are the last three records stacked \n    togther as 3 channels of a single image. The idea is to give a simple feedforward\n    NN some chance of building a model based on motion.\n    If you drive with the ImageFIFO part, then you don\'t need this.\n    Just make sure your inference pass uses the ImageFIFO that the NN will now expect.\n    \'\'\'\n    \n    def rgb2gray(self, rgb):\n        \'\'\'\n        take a numpy rgb image return a new single channel image converted to greyscale\n        \'\'\'\n        return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n\n    def stack3Images(self, img_a, img_b, img_c):\n        \'\'\'\n        convert 3 rgb images into grayscale and put them into the 3 channels of\n        a single output image\n        \'\'\'\n        width, height, _ = img_a.shape\n\n        gray_a = self.rgb2gray(img_a)\n        gray_b = self.rgb2gray(img_b)\n        gray_c = self.rgb2gray(img_c)\n        \n        img_arr = np.zeros([width, height, 3], dtype=np.dtype(\'B\'))\n\n        img_arr[...,0] = np.reshape(gray_a, (width, height))\n        img_arr[...,1] = np.reshape(gray_b, (width, height))\n        img_arr[...,2] = np.reshape(gray_c, (width, height))\n\n        return img_arr\n\n    def get_record(self, ix):\n        \'\'\'\n        get the current record and two previous.\n        stack the 3 images into a single image.\n        \'\'\'\n        data = super(TubImageStacker, self).get_record(ix)\n\n        if ix > 1:\n            data_ch1 = super(TubImageStacker, self).get_record(ix - 1)\n            data_ch0 = super(TubImageStacker, self).get_record(ix - 2)\n\n            json_data = self.get_json_record(ix)\n            for key, val in json_data.items():\n                typ = self.get_input_type(key)\n\n                #load objects that were saved as separate files\n                if typ == \'image\':\n                    val = self.stack3Images(data_ch0[key], data_ch1[key], data[key])\n                    data[key] = val\n                elif typ == \'image_array\':\n                    img = self.stack3Images(data_ch0[key], data_ch1[key], data[key])\n                    val = np.array(img)\n\n        return data\n\n\nclass TubTimeStacker(TubImageStacker):\n    \'\'\'\n    A Tub for training N with records stacked through time. \n    The idea here is to force the network to learn to look ahead in time.\n    Init with an array of time offsets from the current time.\n    \'\'\'\n\n    def __init__(self, frame_list, *args, **kwargs):\n        \'\'\'\n        frame_list of [0, 10] would stack the current and 10 frames from now records togther in a single record\n        with just the current image returned.\n        [5, 90, 200] would return 3 frames of records, ofset 5, 90, and 200 frames in the future.\n\n        \'\'\'\n        super(TubTimeStacker, self).__init__(*args, **kwargs)\n        self.frame_list = frame_list\n  \n    def get_record(self, ix):\n        \'\'\'\n        stack the N records into a single record.\n        Each key value has the record index with a suffix of _N where N is\n        the frame offset into the data.\n        \'\'\'\n        data = {}\n        for i, iOffset in enumerate(self.frame_list):\n            iRec = ix + iOffset\n            \n            try:\n                json_data = self.get_json_record(iRec)\n            except FileNotFoundError:\n                pass\n            except:\n                pass\n\n            for key, val in json_data.items():\n                typ = self.get_input_type(key)\n\n                # load only the first image saved as separate files\n                if typ == \'image\' and i == 0:\n                    val = Image.open(os.path.join(self.path, val))\n                    data[key] = val                    \n                elif typ == \'image_array\' and i == 0:\n                    d = super(TubTimeStacker, self).get_record(ix)\n                    data[key] = d[key]\n                else:\n                    \'\'\'\n                    we append a _offset to the key\n                    so user/angle out now be user/angle_0\n                    \'\'\'\n                    new_key = key + ""_"" + str(iOffset)\n                    data[new_key] = val\n        return data\n\n\nclass TubGroup(Tub):\n    def __init__(self, tub_paths):\n        tub_paths = self.resolve_tub_paths(tub_paths)\n        print(\'TubGroup:tubpaths:\', tub_paths)\n        tubs = [Tub(path) for path in tub_paths]\n        self.input_types = {}\n\n        record_count = 0\n        for t in tubs:\n            t.update_df()\n            record_count += len(t.df)\n            self.input_types.update(dict(zip(t.inputs, t.types)))\n\n        print(\'joining the tubs {} records together. This could take {} minutes.\'\n              .format(record_count, int(record_count / 300000)))\n\n        self.meta = {\'inputs\': list(self.input_types.keys()),\n                     \'types\': list(self.input_types.values())}\n\n        self.df = pd.concat([t.df for t in tubs], axis=0, join=\'inner\')\n\n    def find_tub_paths(self, path):\n        matches = []\n        path = os.path.expanduser(path)\n        for file in glob.glob(path):\n            if os.path.isdir(file):\n                matches.append(os.path.join(os.path.abspath(file)))\n        return matches\n\n    def resolve_tub_paths(self, path_list):\n        path_list = path_list.split("","")\n        resolved_paths = []\n        for path in path_list:\n            paths = self.find_tub_paths(path)\n            resolved_paths += paths\n        return resolved_paths\n'"
donkeycar/parts/dgym.py,0,"b'import os\nimport os\nimport time\nimport gym\nimport gym_donkeycar\n\ndef is_exe(fpath):\n    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\nclass DonkeyGymEnv(object):\n\n    def __init__(self, sim_path, host=""127.0.0.1"", port=9091, headless=0, env_name=""donkey-generated-track-v0"", sync=""asynchronous"", conf={}, delay=0):\n        os.environ[\'DONKEY_SIM_PATH\'] = sim_path\n        os.environ[\'DONKEY_SIM_PORT\'] = str(port)\n        os.environ[\'DONKEY_SIM_HEADLESS\'] = str(headless)\n        os.environ[\'DONKEY_SIM_SYNC\'] = str(sync)\n\n        if sim_path != ""remote"":\n            if not os.path.exists(sim_path):\n                raise Exception(""The path you provided for the sim does not exist."") \n\n            if not is_exe(sim_path):\n                raise Exception(""The path you provided is not an executable."") \n\n        self.env = gym.make(env_name, exe_path=sim_path, host=host, port=port)\n        self.frame = self.env.reset()\n        self.action = [0.0, 0.0]\n        self.running = True\n        self.info = { \'pos\' : (0., 0., 0.)}\n        self.delay = float(delay)\n\n        if ""body_style"" in conf:\n            self.env.viewer.set_car_config(conf[""body_style""], conf[""body_rgb""], conf[""car_name""], conf[""font_size""])\n            #without this small delay, we seem to miss packets\n            time.sleep(0.1)\n\n    def update(self):\n        while self.running:\n            self.frame, _, _, self.info = self.env.step(self.action)\n\n    def run_threaded(self, steering, throttle):\n        if steering is None or throttle is None:\n            steering = 0.0\n            throttle = 0.0\n        if self.delay > 0.0:\n            time.sleep(self.delay / 1000.0)\n        self.action = [steering, throttle]\n        return self.frame\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(0.2)\n        self.env.close()\n\n\n    \n'"
donkeycar/parts/encoder.py,0,"b'""""""\nRotary Encoder\n""""""\n\nfrom datetime import datetime\nfrom donkeycar.parts.teensy import TeensyRCin\nimport re\nimport time\n\nclass AStarSpeed:\n    def __init__(self):\n        self.speed = 0\n        self.linaccel = None\n\n        self.sensor = TeensyRCin(0)\n\n        self.on = True\n\n    def update(self):\n        encoder_pattern = re.compile(\'^E ([-0-9]+)( ([-0-9]+))?( ([-0-9]+))?$\')\n        linaccel_pattern = re.compile(\'^L ([-.0-9]+) ([-.0-9]+) ([-.0-9]+) ([-0-9]+)$\')\n\n        while self.on:\n            start = datetime.now()\n\n            l = self.sensor.astar_readline()\n            while l:\n                m = encoder_pattern.match(l.decode(\'utf-8\'))\n\n                if m:\n                    value = int(m.group(1))\n                    # rospy.loginfo(""%s: Receiver E got %d"" % (self.node_name, value))\n                    # Speed\n                    # 40 ticks/wheel rotation,\n                    # circumfence 0.377m\n                    # every 0.1 seconds\n                    if len(m.group(3)) > 0:\n                        period = 0.001 * int(m.group(3))\n                    else:\n                        period = 0.1\n\n                    self.speed = 0.377 * (float(value) / 40) / period   # now in m/s\n                else:\n                    m = linaccel_pattern.match(l.decode(\'utf-8\'))\n\n                    if m:\n                        la = { \'x\': float(m.group(1)), \'y\': float(m.group(2)), \'z\': float(m.group(3)) }\n\n                        self.linaccel = la\n                        print(""mw linaccel= "" + str(self.linaccel))\n\n                l = self.sensor.astar_readline()\n\n            stop = datetime.now()\n            s = 0.1 - (stop - start).total_seconds()\n            if s > 0:\n                time.sleep(s)\n\n    def run_threaded(self):\n        return self.speed # , self.linaccel\n\n    def shutdown(self):\n        # indicate that the thread should be stopped\n        self.on = False\n        print(\'stopping AStarSpeed\')\n        time.sleep(.5)\n\n\nclass RotaryEncoder():\n    def __init__(self, mm_per_tick=0.306096, pin=13, poll_delay=0.0166, debug=False):\n        import RPi.GPIO as GPIO\n        GPIO.setmode(GPIO.BOARD)\n        GPIO.setup(pin, GPIO.IN)\n        GPIO.add_event_detect(pin, GPIO.RISING, callback=self.isr)\n\n        # initialize the odometer values\n        self.m_per_tick = mm_per_tick / 1000.0\n        self.poll_delay = poll_delay\n        self.meters = 0\n        self.last_time = time.time()\n        self.meters_per_second = 0\n        self.counter = 0\n        self.on = True\n        self.debug = debug\n        self.top_speed = 0\n        self.prev_dist = 0.\n    \n    def isr(self, channel):\n        self.counter += 1\n        \n    def update(self):\n        # keep looping infinitely until the thread is stopped\n        while(self.on):\n                \n            #save the ticks and reset the counter\n            ticks = self.counter\n            self.counter = 0\n            \n            #save off the last time interval and reset the timer\n            start_time = self.last_time\n            end_time = time.time()\n            self.last_time = end_time\n            \n            #calculate elapsed time and distance traveled\n            seconds = end_time - start_time\n            distance = ticks * self.m_per_tick\n            velocity = distance / seconds\n            \n            #update the odometer values\n            self.meters += distance\n            self.meters_per_second = velocity\n            if(self.meters_per_second > self.top_speed):\n                self.top_speed = self.meters_per_second\n\n            #console output for debugging\n            if(self.debug):\n                print(\'seconds:\', seconds)\n                print(\'distance:\', distance)\n                print(\'velocity:\', velocity)\n\n                print(\'distance (m):\', round(self.meters, 4))\n                print(\'velocity (m/s):\', self.meters_per_second)\n\n            time.sleep(self.poll_delay)\n\n    def run_threaded(self):\n        delta = self.meters - self.prev_dist\n        self.prev_dist = self.meters\n        return self.meters, self.meters_per_second, delta\n\n    def shutdown(self):\n        # indicate that the thread should be stopped\n        self.on = False\n        print(\'Stopping Rotary Encoder\')\n        print(""\\tDistance Travelled: {} meters"".format(round(self.meters, 4)))\n        print(""\\tTop Speed: {} meters/second"".format(round(self.top_speed, 4)))\n        time.sleep(.5)\n        \n        import RPi.GPIO as GPIO\n        GPIO.cleanup()\n'"
donkeycar/parts/fastai.py,0,"b'import os\nfrom fastai.vision import *\nimport torch\n\nclass FastAiPilot(object):\n\n    def __init__(self):\n        self.learn = None\n\n    def load(self, model_path):\n        if torch.cuda.is_available():\n            print(""using cuda for torch inference"")\n            defaults.device = torch.device(\'cuda\')\n        else:\n            print(""cuda not available for torch inference"")\n\n\n        path = os.path.dirname(model_path)\n        fname = os.path.basename(model_path)\n        self.learn = load_learner(path=path, file=fname)\n\n    def run(self, img):\n        t = pil2tensor(img, dtype=np.float32) # converts to tensor\n        im = Image(t) # Convert to fastAi Image - this class has ""apply_tfms""\n\n        pred = self.learn.predict(im)\n        steering = float(pred[0].data[0])\n        throttle = float(pred[0].data[1])\n\n        return steering, throttle\n    '"
donkeycar/parts/file_watcher.py,0,"b'import os\n\nclass FileWatcher(object):\n    \'\'\'\n    Watch a specific file and give a signal when it\'s modified\n    \'\'\'\n\n    def __init__(self, filename, verbose=False):\n        self.modified_time = os.path.getmtime(filename)\n        self.filename = filename\n        self.verbose = verbose\n\n    def run(self):\n        \'\'\'\n        return True when file changed. Keep in mind that this does not mean that the \n        file is finished with modification.\n        \'\'\'\n        m_time = os.path.getmtime(self.filename)\n\n        if m_time != self.modified_time:\n            self.modified_time = m_time\n            if self.verbose:\n                print(self.filename, ""changed."")\n            return True\n            \n        return False\n\n\n'"
donkeycar/parts/graph.py,0,"b""import numpy as np\nimport cv2\n\n\nclass Graph(object):\n    '''\n    Take input values and plot them on an image.\n    Takes a list of (x, y) (b, g, r) pairs and\n    plots the color at the given coordinate.\n    When the x value exceeds the width, the graph is erased\n    and begins with an offset to x values such that drawing\n    begins again at the left edge.\n    This assumes x is monotonically increasing, like a time value.\n    '''\n    def __init__(self, res=(200, 200, 3)):\n        self.img = np.zeros(res)\n        self.prev = 0\n\n    def clamp(self, val, lo, hi):\n        if val < lo:\n            val = lo\n        elif val > hi:\n            val = hi\n        return int(val)\n\n    def run(self, values):\n        if values is None:\n            return self.img\n\n        for coord, col in values:\n            x = coord[0] % self.img.shape[1]\n            y = self.clamp(coord[1], 0, self.img.shape[0] - 1)\n            self.img[y, x] = col\n\n        if abs(self.prev - x) > self.img.shape[1] / 2:\n            self.img = np.zeros_like(self.img)\n\n        self.prev = x\n            \n        return self.img\n\n    def shutdown(self):\n        pass\n"""
donkeycar/parts/image.py,0,"b'import os\nimport io\nfrom PIL import Image\nimport numpy as np\nfrom donkeycar.utils import img_to_binary, binary_to_img, arr_to_img, img_to_arr\n\nclass ImgArrToJpg():\n\n    def run(self, img_arr):\n        if img_arr is None:\n            return None\n        try:\n            image = arr_to_img(img_arr)\n            jpg = img_to_binary(image)\n            return jpg\n        except:\n            return None\n\nclass JpgToImgArr():\n\n    def run(self, jpg):\n        if jpg is None:\n            return None\n        image = binary_to_img(jpg)\n        img_arr = img_to_arr(image)\n        return img_arr\n\nclass StereoPair:\n    \'\'\'\n    take two images and put together in a single image\n    \'\'\'\n    def run(self, image_a, image_b):\n        \'\'\'\n        This will take the two images and combine them into a single image\n        One in red, the other in green, and diff in blue channel.\n        \'\'\'\n        if image_a is not None and image_b is not None:\n            width, height, _ = image_a.shape\n            grey_a = dk.utils.rgb2gray(image_a)\n            grey_b = dk.utils.rgb2gray(image_b)\n            grey_c = grey_a - grey_b\n            \n            stereo_image = np.zeros([width, height, 3], dtype=np.dtype(\'B\'))\n            stereo_image[...,0] = np.reshape(grey_a, (width, height))\n            stereo_image[...,1] = np.reshape(grey_b, (width, height))\n            stereo_image[...,2] = np.reshape(grey_c, (width, height))\n        else:\n            stereo_image = []\n\n        return np.array(stereo_image)\n\n\nclass ImgCrop:\n    """"""\n    Crop an image to an area of interest. \n    """"""\n    def __init__(self, top=0, bottom=0, left=0, right=0):\n        self.top = top\n        self.bottom = bottom\n        self.left = left\n        self.right = right\n        \n    def run(self, img_arr):\n        if img_arr is None:\n            return None\n        width, height, _ = img_arr.shape\n        img_arr = img_arr[self.top:height-self.bottom, \n                          self.left: width-self.right]\n        return img_arr\n\n    def shutdown(self):\n        pass\n        \n\n\nclass ImgStack:\n    """"""\n    Stack N previous images into a single N channel image, after converting each to grayscale.\n    The most recent image is the last channel, and pushes previous images towards the front.\n    """"""\n    def __init__(self, num_channels=3):\n        self.img_arr = None\n        self.num_channels = num_channels\n\n    def rgb2gray(self, rgb):\n        \'\'\'\n        take a numpy rgb image return a new single channel image converted to greyscale\n        \'\'\'\n        return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n        \n    def run(self, img_arr):\n        width, height, _ = img_arr.shape        \n        gray = self.rgb2gray(img_arr)\n        \n        if self.img_arr is None:\n            self.img_arr = np.zeros([width, height, self.num_channels], dtype=np.dtype(\'B\'))\n\n        for ch in range(self.num_channels - 1):\n            self.img_arr[...,ch] = self.img_arr[...,ch+1]\n\n        self.img_arr[...,self.num_channels - 1:] = np.reshape(gray, (width, height, 1))\n\n        return self.img_arr\n\n    def shutdown(self):\n        pass\n'"
donkeycar/parts/imu.py,0,"b'#!/usr/bin/env python3\nimport time\nSENSOR_MPU6050 = \'mpu6050\'\nSENSOR_MPU9250 = \'mpu9250\'\n\nDLP_SETTING_DISABLED = 0\nCONFIG_REGISTER = 0x1A\n\nclass IMU:\n    \'\'\'\n    Installation:\n    \n    - MPU6050\n    sudo apt install python3-smbus\n    or\n    sudo apt-get install i2c-tools libi2c-dev python-dev python3-dev\n    git clone https://github.com/pimoroni/py-smbus.git\n    cd py-smbus/library\n    python setup.py build\n    sudo python setup.py install\n\n    pip install mpu6050-raspberrypi\n    \n    - MPU9250\n    pip install mpu9250-jmdev\n    \n    \'\'\'\n\n    def __init__(self, addr=0x68, poll_delay=0.0166, sensor=SENSOR_MPU6050, dlp_setting=DLP_SETTING_DISABLED):\n        self.sensortype = sensor\n        if self.sensortype == SENSOR_MPU6050:\n            from mpu6050 import mpu6050 as MPU6050\n            self.sensor = MPU6050(addr)\n        \n            if(dlp_setting > 0):\n                self.sensor.bus.write_byte_data(self.sensor.address, CONFIG_REGISTER, dlp_setting)\n        \n        else:\n            from mpu9250_jmdev.registers import AK8963_ADDRESS, GFS_1000, AFS_4G, AK8963_BIT_16, AK8963_MODE_C100HZ\n            from mpu9250_jmdev.mpu_9250 import MPU9250\n\n            self.sensor = MPU9250(\n                address_ak=AK8963_ADDRESS,\n                address_mpu_master=addr,  # In 0x68 Address\n                address_mpu_slave=None,\n                bus=1,\n                gfs=GFS_1000,\n                afs=AFS_4G,\n                mfs=AK8963_BIT_16,\n                mode=AK8963_MODE_C100HZ)\n            \n            if(dlp_setting > 0):\n                self.sensor.writeSlave(CONFIG_REGISTER, dlp_setting)\n            self.sensor.calibrateMPU6500()\n            self.sensor.configure()\n\n        \n        self.accel = { \'x\' : 0., \'y\' : 0., \'z\' : 0. }\n        self.gyro = { \'x\' : 0., \'y\' : 0., \'z\' : 0. }\n        self.mag = {\'x\': 0., \'y\': 0., \'z\': 0.}\n        self.temp = 0.\n        self.poll_delay = poll_delay\n        self.on = True\n\n    def update(self):\n        while self.on:\n            self.poll()\n            time.sleep(self.poll_delay)\n                \n    def poll(self):\n        try:\n            if self.sensortype == SENSOR_MPU6050:\n                self.accel, self.gyro, self.temp = self.sensor.get_all_data()\n            else:\n                from mpu9250_jmdev.registers import GRAVITY\n                ret = self.sensor.getAllData()\n                self.accel = { \'x\' : ret[1] * GRAVITY, \'y\' : ret[2] * GRAVITY, \'z\' : ret[3] * GRAVITY }\n                self.gyro = { \'x\' : ret[4], \'y\' : ret[5], \'z\' : ret[6] }\n                self.mag = { \'x\' : ret[13], \'y\' : ret[14], \'z\' : ret[15] }\n                self.temp = ret[16]\n        except:\n            print(\'failed to read imu!!\')\n            \n    def run_threaded(self):\n        return self.accel[\'x\'], self.accel[\'y\'], self.accel[\'z\'], self.gyro[\'x\'], self.gyro[\'y\'], self.gyro[\'z\'], self.temp\n\n    def run(self):\n        self.poll()\n        return self.accel[\'x\'], self.accel[\'y\'], self.accel[\'z\'], self.gyro[\'x\'], self.gyro[\'y\'], self.gyro[\'z\'], self.temp\n\n    def shutdown(self):\n        self.on = False\n\n\nif __name__ == ""__main__"":\n    iter = 0\n    import sys\n    sensor_type = SENSOR_MPU6050 \n    dlp_setting = DLP_SETTING_DISABLED\n    if len(sys.argv) > 1:\n        sensor_type = sys.argv[1]\n    if len(sys.argv) > 2:\n        dlp_setting = int(sys.argv[2])\n\n    p = IMU(sensor=sensor_type)\n    while iter < 100:\n        data = p.run()\n        print(data)\n        time.sleep(0.1)\n        iter += 1\n     \n'"
donkeycar/parts/keras.py,1,"b'\'\'\'\n\npilots.py\n\nMethods to create, use, save and load pilots. Pilots \ncontain the highlevel logic used to determine the angle\nand throttle of a vehicle. Pilots can include one or more \nmodels to help direct the vehicles motion. \n\n\'\'\'\n\n\n\n\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.layers import Input, Dense\nfrom tensorflow.python.keras.models import Model, Sequential\nfrom tensorflow.python.keras.layers import Convolution2D, MaxPooling2D, Reshape, BatchNormalization\nfrom tensorflow.python.keras.layers import Activation, Dropout, Flatten, Cropping2D, Lambda\nfrom tensorflow.python.keras.layers.merge import concatenate\nfrom tensorflow.python.keras.layers import LSTM\nfrom tensorflow.python.keras.layers.wrappers import TimeDistributed as TD\nfrom tensorflow.python.keras.layers import Conv3D, MaxPooling3D, Cropping3D, Conv2DTranspose\n\nimport donkeycar as dk\n\nif tf.__version__ == \'1.13.1\':\n    from tensorflow import ConfigProto, Session\n\n    # Override keras session to work around a bug in TF 1.13.1\n    # Remove after we upgrade to TF 1.14 / TF 2.x.\n    config = ConfigProto()\n    config.gpu_options.allow_growth = True\n    session = Session(config=config)\n    keras.backend.set_session(session)\n\n\nclass KerasPilot(object):\n    \'\'\'\n    Base class for Keras models that will provide steering and throttle to guide a car.\n    \'\'\'\n    def __init__(self):\n        self.model = None\n        self.optimizer = ""adam""\n \n    def load(self, model_path):\n        self.model = keras.models.load_model(model_path, compile=False)\n\n    def load_weights(self, model_path, by_name=True):\n        self.model.load_weights(model_path, by_name=by_name)\n\n    def shutdown(self):\n        pass\n\n    def compile(self):\n        pass\n\n    def set_optimizer(self, optimizer_type, rate, decay):\n        if optimizer_type == ""adam"":\n            self.model.optimizer = keras.optimizers.Adam(lr=rate, decay=decay)\n        elif optimizer_type == ""sgd"":\n            self.model.optimizer = keras.optimizers.SGD(lr=rate, decay=decay)\n        elif optimizer_type == ""rmsprop"":\n            self.model.optimizer = keras.optimizers.RMSprop(lr=rate, decay=decay)\n        else:\n            raise Exception(""unknown optimizer type: %s"" % optimizer_type)\n    \n    def train(self, train_gen, val_gen, \n              saved_model_path, epochs=100, steps=100, train_split=0.8,\n              verbose=1, min_delta=.0005, patience=5, use_early_stop=True):\n        \n        """"""\n        train_gen: generator that yields an array of images an array of \n        \n        """"""\n\n        #checkpoint to save model after each epoch\n        save_best = keras.callbacks.ModelCheckpoint(saved_model_path, \n                                                    monitor=\'val_loss\', \n                                                    verbose=verbose, \n                                                    save_best_only=True, \n                                                    mode=\'min\')\n        \n        #stop training if the validation error stops improving.\n        early_stop = keras.callbacks.EarlyStopping(monitor=\'val_loss\', \n                                                   min_delta=min_delta, \n                                                   patience=patience, \n                                                   verbose=verbose, \n                                                   mode=\'auto\')\n        \n        callbacks_list = [save_best]\n\n        if use_early_stop:\n            callbacks_list.append(early_stop)\n        \n        hist = self.model.fit_generator(\n                        train_gen, \n                        steps_per_epoch=steps, \n                        epochs=epochs, \n                        verbose=1, \n                        validation_data=val_gen,\n                        callbacks=callbacks_list, \n                        validation_steps=steps*(1.0 - train_split))\n        return hist\n\n\nclass KerasCategorical(KerasPilot):\n    \'\'\'\n    The KerasCategorical pilot breaks the steering and throttle decisions into discreet\n    angles and then uses categorical cross entropy to train the network to activate a single\n    neuron for each steering and throttle choice. This can be interesting because we\n    get the confidence value as a distribution over all choices.\n    This uses the dk.utils.linear_bin and dk.utils.linear_unbin to transform continuous\n    real numbers into a range of discreet values for training and runtime.\n    The input and output are therefore bounded and must be chosen wisely to match the data.\n    The default ranges work for the default setup. But cars which go faster may want to\n    enable a higher throttle range. And cars with larger steering throw may want more bins.\n    \'\'\'\n    def __init__(self, input_shape=(120, 160, 3), throttle_range=0.5, roi_crop=(0, 0), *args, **kwargs):\n        super(KerasCategorical, self).__init__(*args, **kwargs)\n        self.model = default_categorical(input_shape, roi_crop)\n        self.compile()\n        self.throttle_range = throttle_range\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer, metrics=[\'acc\'],\n                  loss={\'angle_out\': \'categorical_crossentropy\', \n                        \'throttle_out\': \'categorical_crossentropy\'},\n                  loss_weights={\'angle_out\': 0.5, \'throttle_out\': 1.0})\n        \n    def run(self, img_arr):\n        if img_arr is None:\n            print(\'no image\')\n            return 0.0, 0.0\n\n        img_arr = img_arr.reshape((1,) + img_arr.shape)\n        angle_binned, throttle = self.model.predict(img_arr)\n        N = len(throttle[0])\n        throttle = dk.utils.linear_unbin(throttle, N=N, offset=0.0, R=self.throttle_range)\n        angle_unbinned = dk.utils.linear_unbin(angle_binned)\n        return angle_unbinned, throttle\n    \n    \n    \nclass KerasLinear(KerasPilot):\n    \'\'\'\n    The KerasLinear pilot uses one neuron to output a continous value via the \n    Keras Dense layer with linear activation. One each for steering and throttle.\n    The output is not bounded.\n    \'\'\'\n    def __init__(self, num_outputs=2, input_shape=(120, 160, 3), roi_crop=(0, 0), *args, **kwargs):\n        super(KerasLinear, self).__init__(*args, **kwargs)\n        self.model = default_n_linear(num_outputs, input_shape, roi_crop)\n        self.compile()\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer,\n                loss=\'mse\')\n\n    def run(self, img_arr):\n        img_arr = img_arr.reshape((1,) + img_arr.shape)\n        outputs = self.model.predict(img_arr)\n        steering = outputs[0]\n        throttle = outputs[1]\n        return steering[0][0], throttle[0][0]\n\n\n\nclass KerasIMU(KerasPilot):\n    \'\'\'\n    A Keras part that take an image and IMU vector as input,\n    outputs steering and throttle\n\n    Note: When training, you will need to vectorize the input from the IMU.\n    Depending on the names you use for imu records, something like this will work:\n\n    X_keys = [\'cam/image_array\',\'imu_array\']\n    y_keys = [\'user/angle\', \'user/throttle\']\n    \n    def rt(rec):\n        rec[\'imu_array\'] = np.array([ rec[\'imu/acl_x\'], rec[\'imu/acl_y\'], rec[\'imu/acl_z\'],\n            rec[\'imu/gyr_x\'], rec[\'imu/gyr_y\'], rec[\'imu/gyr_z\'] ])\n        return rec\n\n    kl = KerasIMU()\n\n    tubgroup = TubGroup(tub_names)\n    train_gen, val_gen = tubgroup.get_train_val_gen(X_keys, y_keys, record_transform=rt,\n                                                    batch_size=cfg.BATCH_SIZE,\n                                                    train_frac=cfg.TRAIN_TEST_SPLIT)\n\n    \'\'\'\n    def __init__(self, model=None, num_outputs=2, num_imu_inputs=6, input_shape=(120, 160, 3), roi_crop=(0,0), *args, **kwargs):\n        super(KerasIMU, self).__init__(*args, **kwargs)\n        self.num_imu_inputs = num_imu_inputs\n        self.model = default_imu(num_outputs = num_outputs, num_imu_inputs = num_imu_inputs, input_shape=input_shape, roi_crop=roi_crop)\n        self.compile()\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer,\n                  loss=\'mse\')\n        \n    def run(self, img_arr, accel_x, accel_y, accel_z, gyr_x, gyr_y, gyr_z):\n        #TODO: would be nice to take a vector input array.\n        img_arr = img_arr.reshape((1,) + img_arr.shape)\n        imu_arr = np.array([accel_x, accel_y, accel_z, gyr_x, gyr_y, gyr_z]).reshape(1,self.num_imu_inputs)\n        outputs = self.model.predict([img_arr, imu_arr])\n        steering = outputs[0]\n        throttle = outputs[1]\n        return steering[0][0], throttle[0][0]\n\n\nclass KerasBehavioral(KerasPilot):\n    \'\'\'\n    A Keras part that take an image and Behavior vector as input,\n    outputs steering and throttle\n    \'\'\'\n    def __init__(self, model=None, num_outputs=2, num_behavior_inputs=2, input_shape=(120, 160, 3), *args, **kwargs):\n        super(KerasBehavioral, self).__init__(*args, **kwargs)\n        self.model = default_bhv(num_outputs = num_outputs, num_bvh_inputs = num_behavior_inputs, input_shape=input_shape)\n        self.compile()\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer,\n                  loss=\'mse\')\n        \n    def run(self, img_arr, state_array):        \n        img_arr = img_arr.reshape((1,) + img_arr.shape)\n        bhv_arr = np.array(state_array).reshape(1,len(state_array))\n        angle_binned, throttle = self.model.predict([img_arr, bhv_arr])\n        #in order to support older models with linear throttle,\n        #we will test for shape of throttle to see if it\'s the newer\n        #binned version.\n        N = len(throttle[0])\n        \n        if N > 0:\n            throttle = dk.utils.linear_unbin(throttle, N=N, offset=0.0, R=0.5)\n        else:\n            throttle = throttle[0][0]\n        angle_unbinned = dk.utils.linear_unbin(angle_binned)\n        return angle_unbinned, throttle\n\n\nclass KerasLocalizer(KerasPilot):\n    \'\'\'\n    A Keras part that take an image as input,\n    outputs steering and throttle, and localisation category\n    \'\'\'\n    def __init__(self, model=None, num_locations=8, input_shape=(120, 160, 3), *args, **kwargs):\n        super(KerasLocalizer, self).__init__(*args, **kwargs)\n        self.model = default_loc(num_locations=num_locations, input_shape=input_shape)\n        self.compile()\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer, metrics=[\'acc\'],\n                  loss=\'mse\')\n        \n    def run(self, img_arr):        \n        img_arr = img_arr.reshape((1,) + img_arr.shape)\n        angle, throttle, track_loc = self.model.predict([img_arr])\n        loc = np.argmax(track_loc[0])\n\n        return angle, throttle, loc\n\ndef adjust_input_shape(input_shape, roi_crop):\n    height = input_shape[0]\n    new_height = height - roi_crop[0] - roi_crop[1]\n    return (new_height, input_shape[1], input_shape[2])\n\n\ndef default_categorical(input_shape=(120, 160, 3), roi_crop=(0, 0)):\n\n    opt = keras.optimizers.Adam()\n    drop = 0.2\n\n    #we now expect that cropping done elsewhere. we will adjust our expeected image size here:\n    input_shape = adjust_input_shape(input_shape, roi_crop)\n\n    img_in = Input(shape=input_shape, name=\'img_in\')                      # First layer, input layer, Shape comes from camera.py resolution, RGB\n    x = img_in\n    x = Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_1"")(x)       # 24 features, 5 pixel x 5 pixel kernel (convolution, feauture) window, 2wx2h stride, relu activation\n    x = Dropout(drop)(x)                                                      # Randomly drop out (turn off) 10% of the neurons (Prevent overfitting)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_2"")(x)       # 32 features, 5px5p kernel window, 2wx2h stride, relu activatiion\n    x = Dropout(drop)(x)                                                      # Randomly drop out (turn off) 10% of the neurons (Prevent overfitting)\n    if input_shape[0] > 32 :\n        x = Convolution2D(64, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_3"")(x)       # 64 features, 5px5p kernal window, 2wx2h stride, relu\n    else:\n        x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_3"")(x)       # 64 features, 5px5p kernal window, 2wx2h stride, relu\n    if input_shape[0] > 64 :\n        x = Convolution2D(64, (3,3), strides=(2,2), activation=\'relu\', name=""conv2d_4"")(x)       # 64 features, 3px3p kernal window, 2wx2h stride, relu\n    elif input_shape[0] > 32 :\n        x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_4"")(x)       # 64 features, 3px3p kernal window, 2wx2h stride, relu\n    x = Dropout(drop)(x)                                                      # Randomly drop out (turn off) 10% of the neurons (Prevent overfitting)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_5"")(x)       # 64 features, 3px3p kernal window, 1wx1h stride, relu\n    x = Dropout(drop)(x)                                                      # Randomly drop out (turn off) 10% of the neurons (Prevent overfitting)\n    # Possibly add MaxPooling (will make it less sensitive to position in image).  Camera angle fixed, so may not to be needed\n\n    x = Flatten(name=\'flattened\')(x)                                        # Flatten to 1D (Fully connected)\n    x = Dense(100, activation=\'relu\', name=""fc_1"")(x)                                    # Classify the data into 100 features, make all negatives 0\n    x = Dropout(drop)(x)                                                      # Randomly drop out (turn off) 10% of the neurons (Prevent overfitting)\n    x = Dense(50, activation=\'relu\', name=""fc_2"")(x)                                     # Classify the data into 50 features, make all negatives 0\n    x = Dropout(drop)(x)                                                      # Randomly drop out 10% of the neurons (Prevent overfitting)\n    #categorical output of the angle\n    angle_out = Dense(15, activation=\'softmax\', name=\'angle_out\')(x)        # Connect every input with every output and output 15 hidden units. Use Softmax to give percentage. 15 categories and find best one based off percentage 0.0-1.0\n    \n    #continous output of throttle\n    throttle_out = Dense(20, activation=\'softmax\', name=\'throttle_out\')(x)      # Reduce to 1 number, Positive number only\n    \n    model = Model(inputs=[img_in], outputs=[angle_out, throttle_out])\n    return model\n\n\n\ndef default_n_linear(num_outputs, input_shape=(120, 160, 3), roi_crop=(0, 0)):\n\n    drop = 0.1\n\n    #we now expect that cropping done elsewhere. we will adjust our expeected image size here:\n    input_shape = adjust_input_shape(input_shape, roi_crop)\n    \n    img_in = Input(shape=input_shape, name=\'img_in\')\n    x = img_in\n    x = Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_1"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_2"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_3"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_4"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_5"")(x)\n    x = Dropout(drop)(x)\n    \n    x = Flatten(name=\'flattened\')(x)\n    x = Dense(100, activation=\'relu\')(x)\n    x = Dropout(drop)(x)\n    x = Dense(50, activation=\'relu\')(x)\n    x = Dropout(drop)(x)\n\n    outputs = []\n    \n    for i in range(num_outputs):\n        outputs.append(Dense(1, activation=\'linear\', name=\'n_outputs\' + str(i))(x))\n        \n    model = Model(inputs=[img_in], outputs=outputs)\n    \n    return model\n\n\n\ndef default_imu(num_outputs, num_imu_inputs, input_shape, roi_crop=(0, 0)):\n\n    #we now expect that cropping done elsewhere. we will adjust our expeected image size here:\n    input_shape = adjust_input_shape(input_shape, roi_crop)\n\n    img_in = Input(shape=input_shape, name=\'img_in\')\n    imu_in = Input(shape=(num_imu_inputs,), name=""imu_in"")\n    \n    x = img_in\n    x = Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\')(x)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\')(x)\n    x = Convolution2D(64, (3,3), strides=(2,2), activation=\'relu\')(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\')(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\')(x)\n    x = Flatten(name=\'flattened\')(x)\n    x = Dense(100, activation=\'relu\')(x)\n    x = Dropout(.1)(x)\n    \n    y = imu_in\n    y = Dense(14, activation=\'relu\')(y)\n    y = Dense(14, activation=\'relu\')(y)\n    y = Dense(14, activation=\'relu\')(y)\n    \n    z = concatenate([x, y])\n    z = Dense(50, activation=\'relu\')(z)\n    z = Dropout(.1)(z)\n    z = Dense(50, activation=\'relu\')(z)\n    z = Dropout(.1)(z)\n\n    outputs = [] \n    \n    for i in range(num_outputs):\n        outputs.append(Dense(1, activation=\'linear\', name=\'out_\' + str(i))(z))\n        \n    model = Model(inputs=[img_in, imu_in], outputs=outputs)\n    \n    return model\n\n\ndef default_bhv(num_outputs, num_bvh_inputs, input_shape):\n    \'\'\'\n    Notes: this model depends on concatenate which failed on keras < 2.0.8\n    \'\'\'\n\n    img_in = Input(shape=input_shape, name=\'img_in\')\n    bvh_in = Input(shape=(num_bvh_inputs,), name=""behavior_in"")\n    \n    x = img_in\n    #x = Cropping2D(cropping=((60,0), (0,0)))(x) #trim 60 pixels off top\n    x = Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\')(x)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\')(x)\n    x = Convolution2D(64, (5,5), strides=(2,2), activation=\'relu\')(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\')(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\')(x)\n    x = Flatten(name=\'flattened\')(x)\n    x = Dense(100, activation=\'relu\')(x)\n    x = Dropout(.1)(x)\n    \n    y = bvh_in\n    y = Dense(num_bvh_inputs * 2, activation=\'relu\')(y)\n    y = Dense(num_bvh_inputs * 2, activation=\'relu\')(y)\n    y = Dense(num_bvh_inputs * 2, activation=\'relu\')(y)\n    \n    z = concatenate([x, y])\n    z = Dense(100, activation=\'relu\')(z)\n    z = Dropout(.1)(z)\n    z = Dense(50, activation=\'relu\')(z)\n    z = Dropout(.1)(z)\n    \n    #categorical output of the angle\n    angle_out = Dense(15, activation=\'softmax\', name=\'angle_out\')(z)        # Connect every input with every output and output 15 hidden units. Use Softmax to give percentage. 15 categories and find best one based off percentage 0.0-1.0\n    \n    #continous output of throttle\n    throttle_out = Dense(20, activation=\'softmax\', name=\'throttle_out\')(z)      # Reduce to 1 number, Positive number only\n        \n    model = Model(inputs=[img_in, bvh_in], outputs=[angle_out, throttle_out])\n    \n    return model\n\n\ndef default_loc(num_locations, input_shape):\n    drop = 0.2\n\n    img_in = Input(shape=input_shape, name=\'img_in\')\n    \n    x = img_in\n    x = Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_1"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_2"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_3"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (3,3), strides=(2,2), activation=\'relu\', name=""conv2d_4"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_5"")(x)\n    x = Dropout(drop)(x)\n    x = Flatten(name=\'flattened\')(x)\n    x = Dense(100, activation=\'relu\')(x)\n    x = Dropout(drop)(x)\n    \n    z = Dense(50, activation=\'relu\')(x)\n    z = Dropout(drop)(z)\n    \n    \n    #linear output of the angle\n    angle_out = Dense(1, activation=\'linear\', name=\'angle\')(z)\n    \n    #linear output of throttle\n    throttle_out = Dense(1, activation=\'linear\', name=\'throttle\')(z)\n\n    #categorical output of location\n    loc_out = Dense(num_locations, activation=\'softmax\', name=\'loc\')(z)\n\n    model = Model(inputs=[img_in], outputs=[angle_out, throttle_out, loc_out])\n    \n    return model\n\n\nclass KerasRNN_LSTM(KerasPilot):\n    def __init__(self, image_w =160, image_h=120, image_d=3, seq_length=3, roi_crop=(0,0), num_outputs=2, *args, **kwargs):\n        super(KerasRNN_LSTM, self).__init__(*args, **kwargs)\n        input_shape = (image_h, image_w, image_d)\n        self.model = rnn_lstm(seq_length=seq_length,\n            num_outputs=num_outputs,\n            input_shape=input_shape,\n            roi_crop=roi_crop)\n        self.seq_length = seq_length\n        self.image_d = image_d\n        self.image_w = image_w\n        self.image_h = image_h\n        self.img_seq = []\n        self.compile()\n        self.optimizer = ""rmsprop""\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer,\n                  loss=\'mse\')\n\n    def run(self, img_arr):\n        if img_arr.shape[2] == 3 and self.image_d == 1:\n            img_arr = dk.utils.rgb2gray(img_arr)\n\n        while len(self.img_seq) < self.seq_length:\n            self.img_seq.append(img_arr)\n\n        self.img_seq = self.img_seq[1:]\n        self.img_seq.append(img_arr)\n        \n        img_arr = np.array(self.img_seq).reshape(1, self.seq_length, self.image_h, self.image_w, self.image_d )\n        outputs = self.model.predict([img_arr])\n        steering = outputs[0][0]\n        throttle = outputs[0][1]\n        return steering, throttle\n  \n\ndef rnn_lstm(seq_length=3, num_outputs=2, input_shape=(120,160,3), roi_crop=(0, 0)):\n\n    #we now expect that cropping done elsewhere. we will adjust our expeected image size here:\n    input_shape = adjust_input_shape(input_shape, roi_crop)\n\n    img_seq_shape = (seq_length,) + input_shape   \n    img_in = Input(batch_shape = img_seq_shape, name=\'img_in\')\n    drop_out = 0.3\n\n    x = Sequential()\n    x.add(TD(Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\'), input_shape=img_seq_shape))\n    x.add(TD(Dropout(drop_out)))\n    x.add(TD(Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\')))\n    x.add(TD(Dropout(drop_out)))\n    x.add(TD(Convolution2D(32, (3,3), strides=(2,2), activation=\'relu\')))\n    x.add(TD(Dropout(drop_out)))\n    x.add(TD(Convolution2D(32, (3,3), strides=(1,1), activation=\'relu\')))\n    x.add(TD(Dropout(drop_out)))\n    x.add(TD(MaxPooling2D(pool_size=(2, 2))))\n    x.add(TD(Flatten(name=\'flattened\')))\n    x.add(TD(Dense(100, activation=\'relu\')))\n    x.add(TD(Dropout(drop_out)))\n      \n    x.add(LSTM(128, return_sequences=True, name=""LSTM_seq""))\n    x.add(Dropout(.1))\n    x.add(LSTM(128, return_sequences=False, name=""LSTM_fin""))\n    x.add(Dropout(.1))\n    x.add(Dense(128, activation=\'relu\'))\n    x.add(Dropout(.1))\n    x.add(Dense(64, activation=\'relu\'))\n    x.add(Dense(10, activation=\'relu\'))\n    x.add(Dense(num_outputs, activation=\'linear\', name=\'model_outputs\'))\n    \n    return x\n\n\nclass Keras3D_CNN(KerasPilot):\n    def __init__(self, image_w =160, image_h=120, image_d=3, seq_length=20, num_outputs=2, roi_crop=(0, 0), *args, **kwargs):\n        super(Keras3D_CNN, self).__init__(*args, **kwargs)\n\n        #we now expect that cropping done elsewhere. we will adjust our expeected image size here:\n        input_shape = adjust_input_shape((image_h, image_w, image_d), roi_crop)\n        image_h = input_shape[0]\n        image_w = input_shape[1]\n        image_d = input_shape[2]\n\n\n        self.model = build_3d_cnn(w=image_w, h=image_h, d=image_d, s=seq_length, num_outputs=num_outputs)\n        self.seq_length = seq_length\n        self.image_d = image_d\n        self.image_w = image_w\n        self.image_h = image_h\n        self.img_seq = []\n        self.compile()\n\n    def compile(self):\n        self.model.compile(loss=\'mean_squared_error\', optimizer=self.optimizer, metrics=[\'accuracy\'])\n\n    def run(self, img_arr):\n\n        if img_arr.shape[2] == 3 and self.image_d == 1:\n            img_arr = dk.utils.rgb2gray(img_arr)\n\n        while len(self.img_seq) < self.seq_length:\n            self.img_seq.append(img_arr)\n\n        self.img_seq = self.img_seq[1:]\n        self.img_seq.append(img_arr)\n        \n        img_arr = np.array(self.img_seq).reshape(1, self.seq_length, self.image_h, self.image_w, self.image_d )\n        outputs = self.model.predict([img_arr])\n        steering = outputs[0][0]\n        throttle = outputs[0][1]\n        return steering, throttle\n\n\ndef build_3d_cnn(w, h, d, s, num_outputs):\n    #Credit: https://github.com/jessecha/DNRacing/blob/master/3D_CNN_Model/model.py\n    \'\'\'\n        w : width\n        h : height\n        d : depth\n        s : n_stacked\n    \'\'\'\n    input_shape=(s, h, w, d)\n\n    model = Sequential()\n    #First layer\n    #model.add(Cropping3D(cropping=((0,0), (50,10), (0,0)), input_shape=input_shape) ) #trim pixels off top\n    \n    # Second layer\n    model.add(Conv3D(\n        filters=16, kernel_size=(3,3,3), strides=(1,3,3),\n        data_format=\'channels_last\', padding=\'same\', input_shape=input_shape)\n    )\n    model.add(Activation(\'relu\'))\n    model.add(MaxPooling3D(\n        pool_size=(1,2,2), strides=(1,2,2), padding=\'valid\', data_format=None)\n    )\n    # Third layer\n    model.add(Conv3D(\n        filters=32, kernel_size=(3,3,3), strides=(1,1,1),\n        data_format=\'channels_last\', padding=\'same\')\n    )\n    model.add(Activation(\'relu\'))\n    model.add(MaxPooling3D(\n        pool_size=(1, 2, 2), strides=(1,2,2), padding=\'valid\', data_format=None)\n    )\n    # Fourth layer\n    model.add(Conv3D(\n        filters=64, kernel_size=(3,3,3), strides=(1,1,1),\n        data_format=\'channels_last\', padding=\'same\')\n    )\n    model.add(Activation(\'relu\'))\n    model.add(MaxPooling3D(\n        pool_size=(1,2,2), strides=(1,2,2), padding=\'valid\', data_format=None)\n    )\n    # Fifth layer\n    model.add(Conv3D(\n        filters=128, kernel_size=(3,3,3), strides=(1,1,1),\n        data_format=\'channels_last\', padding=\'same\')\n    )\n    model.add(Activation(\'relu\'))\n    model.add(MaxPooling3D(\n        pool_size=(1,2,2), strides=(1,2,2), padding=\'valid\', data_format=None)\n    )\n    # Fully connected layer\n    model.add(Flatten())\n\n    model.add(Dense(256))\n    model.add(BatchNormalization())\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(256))\n    model.add(BatchNormalization())\n    model.add(Activation(\'relu\'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(num_outputs))\n    #model.add(Activation(\'tanh\'))\n\n    return model\n\nclass KerasLatent(KerasPilot):\n    def __init__(self, num_outputs=2, input_shape=(120, 160, 3), *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model = default_latent(num_outputs, input_shape)\n        self.compile()\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer, loss={\n            ""img_out"" : ""mse"", ""n_outputs0"" : ""mse"", ""n_outputs1"" : ""mse""\n        }, loss_weights={\n            ""img_out"" : 100.0, ""n_outputs0"" : 2.0, ""n_outputs1"" : 1.0\n        })\n\n    def run(self, img_arr):\n        img_arr = img_arr.reshape((1,) + img_arr.shape)\n        outputs = self.model.predict(img_arr)\n        steering = outputs[1]\n        throttle = outputs[2]\n        return steering[0][0], throttle[0][0]\n\n\ndef default_latent(num_outputs, input_shape):\n    \n    drop = 0.2\n    \n    img_in = Input(shape=input_shape, name=\'img_in\')\n    x = img_in\n    x = Convolution2D(24, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_1"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_2"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(32, (5,5), strides=(2,2), activation=\'relu\', name=""conv2d_3"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(32, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_4"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(32, (3,3), strides=(1,1), activation=\'relu\', name=""conv2d_5"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (3,3), strides=(2,2), activation=\'relu\', name=""conv2d_6"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (3,3), strides=(2,2), activation=\'relu\', name=""conv2d_7"")(x)\n    x = Dropout(drop)(x)\n    x = Convolution2D(64, (1,1), strides=(2,2), activation=\'relu\', name=""latent"")(x)\n    \n    y = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=2, name=""deconv2d_1"")(x)\n    y = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=2, name=""deconv2d_2"")(y)\n    y = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=2, name=""deconv2d_3"")(y)\n    y = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=2, name=""deconv2d_4"")(y)\n    y = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=2, name=""deconv2d_5"")(y)\n    y = Conv2DTranspose(filters=1, kernel_size=(3,3), strides=2, name=""img_out"")(y)\n    \n    x = Flatten(name=\'flattened\')(x)\n    x = Dense(256, activation=\'relu\')(x)\n    x = Dropout(drop)(x)\n    x = Dense(100, activation=\'relu\')(x)\n    x = Dropout(drop)(x)\n    x = Dense(50, activation=\'relu\')(x)\n    x = Dropout(drop)(x)\n\n    outputs = [y]\n    \n    for i in range(num_outputs):\n        outputs.append(Dense(1, activation=\'linear\', name=\'n_outputs\' + str(i))(x))\n        \n    model = Model(inputs=[img_in], outputs=outputs)\n    \n    return model\n'"
donkeycar/parts/launch.py,0,"b'import time\n\nclass AiLaunch():\n    \'\'\'\n    This part will apply a large thrust on initial activation. This is to help\n    in racing to start fast and then the ai will take over quickly when it\'s\n    up to speed.\n    \'\'\'\n\n    def __init__(self, launch_duration=1.0, launch_throttle=1.0, keep_enabled=False):\n        self.active = False\n        self.enabled = False\n        self.timer_start = None\n        self.timer_duration = launch_duration\n        self.launch_throttle = launch_throttle\n        self.prev_mode = None\n        self.trigger_on_switch = keep_enabled\n        \n    def enable_ai_launch(self):\n        self.enabled = True\n        print(\'AiLauncher is enabled.\')\n\n    def run(self, mode, ai_throttle):\n        new_throttle = ai_throttle\n\n        if mode != self.prev_mode:\n            self.prev_mode = mode\n            if mode == ""local"" and self.trigger_on_switch:\n                self.enabled = True\n\n        if mode == ""local"" and self.enabled:\n            if not self.active:\n                self.active = True\n                self.timer_start = time.time()\n            else:\n                duration = time.time() - self.timer_start\n                if duration > self.timer_duration:\n                    self.active = False\n                    self.enabled = False\n        else:\n            self.active = False\n\n        if self.active:\n            print(\'AiLauncher is active!!!\')\n            new_throttle = self.launch_throttle\n\n        return new_throttle\n\n'"
donkeycar/parts/led_status.py,0,"b'import time\nimport RPi.GPIO as GPIO\n\nclass LED:\n    \'\'\' \n    Toggle a GPIO pin for led control\n    \'\'\'\n    def __init__(self, pin):\n        self.pin = pin\n\n        GPIO.setmode(GPIO.BOARD)\n        GPIO.setup(self.pin, GPIO.OUT)\n        self.blink_changed = 0\n        self.on = False\n\n    def toggle(self, condition):\n        if condition:\n            GPIO.output(self.pin, GPIO.HIGH)\n            self.on = True\n        else:\n            GPIO.output(self.pin, GPIO.LOW)\n            self.on = False            \n\n    def blink(self, rate):\n        if time.time() - self.blink_changed > rate:\n            self.toggle(not self.on)\n            self.blink_changed = time.time()\n\n    def run(self, blink_rate):\n        if blink_rate == 0:\n            self.toggle(False)\n        elif blink_rate > 0:\n            self.blink(blink_rate)\n        else:\n            self.toggle(True)\n\n    def shutdown(self):\n        self.toggle(False)        \n        GPIO.cleanup()\n\n\nclass RGB_LED:\n    \'\'\' \n    Toggle a GPIO pin on at max_duty pwm if condition is true, off if condition is false.\n    Good for LED pwm modulated\n    \'\'\'\n    def __init__(self, pin_r, pin_g, pin_b, invert_flag=False):\n        self.pin_r = pin_r\n        self.pin_g = pin_g\n        self.pin_b = pin_b\n        self.invert = invert_flag\n        print(\'setting up gpio in board mode\')\n        GPIO.setwarnings(False)\n        GPIO.setmode(GPIO.BOARD)\n        GPIO.setup(self.pin_r, GPIO.OUT)\n        GPIO.setup(self.pin_g, GPIO.OUT)\n        GPIO.setup(self.pin_b, GPIO.OUT)\n        freq = 50\n        self.pwm_r = GPIO.PWM(self.pin_r, freq)\n        self.pwm_g = GPIO.PWM(self.pin_g, freq)\n        self.pwm_b = GPIO.PWM(self.pin_b, freq)\n        self.pwm_r.start(0)\n        self.pwm_g.start(0)\n        self.pwm_b.start(0)\n        self.zero = 0\n        if( self.invert ):\n            self.zero = 100\n\n        self.rgb = (50, self.zero, self.zero)\n\n        self.blink_changed = 0\n        self.on = False\n\n    def toggle(self, condition):\n        if condition:\n            r, g, b = self.rgb\n            self.set_rgb_duty(r, g, b)\n            self.on = True\n        else:\n            self.set_rgb_duty(self.zero, self.zero, self.zero)\n            self.on = False\n\n    def blink(self, rate):\n        if time.time() - self.blink_changed > rate:\n            self.toggle(not self.on)\n            self.blink_changed = time.time()\n\n    def run(self, blink_rate):\n        if blink_rate == 0:\n            self.toggle(False)\n        elif blink_rate > 0:\n            self.blink(blink_rate)\n        else:\n            self.toggle(True)\n\n    def set_rgb(self, r, g, b):\n        r = r if not self.invert else 100-r\n        g = g if not self.invert else 100-g\n        b = b if not self.invert else 100-b\n        self.rgb = (r, g, b)\n        self.set_rgb_duty(r, g, b)\n\n    def set_rgb_duty(self, r, g, b):\n        self.pwm_r.ChangeDutyCycle(r)\n        self.pwm_g.ChangeDutyCycle(g)\n        self.pwm_b.ChangeDutyCycle(b)\n\n    def shutdown(self):\n        self.toggle(False)\n        GPIO.cleanup()\n\n\nif __name__ == ""__main__"":\n    import time\n    import sys\n    pin_r = int(sys.argv[1])\n    pin_g = int(sys.argv[2])\n    pin_b = int(sys.argv[3])\n    rate = float(sys.argv[4])\n    print(\'output pin\', pin_r, pin_g, pin_b, \'rate\', rate)\n\n    p = RGB_LED(pin_r, pin_g, pin_b)\n    \n    iter = 0\n    while iter < 50:\n        p.run(rate)\n        time.sleep(0.1)\n        iter += 1\n    \n    delay = 0.1\n\n    iter = 0\n    while iter < 100:\n        p.set_rgb(iter, 100-iter, 0)\n        time.sleep(delay)\n        iter += 1\n    \n    iter = 0\n    while iter < 100:\n        p.set_rgb(100 - iter, 0, iter)\n        time.sleep(delay)\n        iter += 1\n\n    p.shutdown()\n\n'"
donkeycar/parts/lidar.py,0,"b'""""""\nLidar\n""""""\n\nimport time\nimport math\nimport pickle\nimport serial\nimport numpy as np\nfrom donkeycar.utils import norm_deg, dist, deg2rad, arr_to_img\nfrom PIL import Image, ImageDraw\n\nclass RPLidar(object):\n    \'\'\'\n    https://github.com/SkoltechRobotics/rplidar\n    \'\'\'\n    def __init__(self, port=\'/dev/ttyUSB0\'):\n        from rplidar import RPLidar\n        self.port = port\n        self.distances = [] #a list of distance measurements \n        self.angles = [] # a list of angles corresponding to dist meas above\n        self.lidar = RPLidar(self.port)\n        self.lidar.clear_input()\n        time.sleep(1)\n        self.on = True\n        #print(self.lidar.get_info())\n        #print(self.lidar.get_health())\n\n\n    def update(self):\n        scans = self.lidar.iter_scans(550)\n        while self.on:\n            try:\n                for scan in scans:\n                    self.distances = [item[2] for item in scan]\n                    self.angles = [item[1] for item in scan]\n            except serial.serialutil.SerialException:\n                print(\'serial.serialutil.SerialException from Lidar. common when shutting down.\')\n\n    def run_threaded(self):\n        return self.distances, self.angles\n\n    def shutdown(self):\n        self.on = False\n        time.sleep(2)\n        self.lidar.stop()\n        self.lidar.stop_motor()\n        self.lidar.disconnect()\n\n\nclass LidarPlot(object):\n    \'\'\'\n    takes the raw lidar measurements and plots it to an image\n    \'\'\'\n    PLOT_TYPE_LINE = 0\n    PLOT_TYPE_CIRC = 1\n    def __init__(self, resolution=(500,500),\n        max_dist=1000, #mm\n        radius_plot=3,\n        plot_type=PLOT_TYPE_CIRC):\n        self.frame = Image.new(\'RGB\', resolution)\n        self.max_dist = max_dist\n        self.rad = radius_plot\n        self.resolution = resolution\n        if plot_type == self.PLOT_TYPE_CIRC:\n            self.plot_fn = self.plot_circ\n        else:\n            self.plot_fn = self.plot_line\n            \n\n    def plot_line(self, img, dist, theta, max_dist, draw):\n        \'\'\'\n        scale dist so that max_dist is edge of img (mm)\n        and img is PIL Image, draw the line using the draw ImageDraw object\n        \'\'\'\n        center = (img.width / 2, img.height / 2)\n        max_pixel = min(center[0], center[1])\n        dist = dist / max_dist * max_pixel\n        if dist < 0 :\n            dist = 0\n        elif dist > max_pixel:\n            dist = max_pixel\n        theta = np.radians(theta)\n        sx = math.cos(theta) * dist + center[0]\n        sy = math.sin(theta) * dist + center[1]\n        ex = math.cos(theta) * (dist + self.rad) + center[0]\n        ey = math.sin(theta) * (dist + self.rad) + center[1]\n        fill = 128\n        draw.line((sx,sy, ex, ey), fill=(fill, fill, fill), width=1)\n        \n    def plot_circ(self, img, dist, theta, max_dist, draw):\n        \'\'\'\n        scale dist so that max_dist is edge of img (mm)\n        and img is PIL Image, draw the circle using the draw ImageDraw object\n        \'\'\'\n        center = (img.width / 2, img.height / 2)\n        max_pixel = min(center[0], center[1])\n        dist = dist / max_dist * max_pixel\n        if dist < 0 :\n            dist = 0\n        elif dist > max_pixel:\n            dist = max_pixel\n        theta = np.radians(theta)\n        sx = int(math.cos(theta) * dist + center[0])\n        sy = int(math.sin(theta) * dist + center[1])\n        ex = int(math.cos(theta) * (dist + 2 * self.rad) + center[0])\n        ey = int(math.sin(theta) * (dist + 2 * self.rad) + center[1])\n        fill = 128\n\n        draw.ellipse((min(sx, ex), min(sy, ey), max(sx, ex), max(sy, ey)), fill=(fill, fill, fill))\n\n    def plot_scan(self, img, distances, angles, max_dist, draw):\n        for dist, angle in zip(distances, angles):\n            self.plot_fn(img, dist, angle, max_dist, draw)\n            \n    def run(self, distances, angles):\n        \'\'\'\n        takes two lists of equal length, one of distance values, the other of angles corresponding to the dist meas \n        \'\'\'\n        self.frame = Image.new(\'RGB\', self.resolution, (255, 255, 255))\n        draw = ImageDraw.Draw(self.frame)\n        self.plot_scan(self.frame, distances, angles, self.max_dist, draw)\n        return self.frame\n\n    def shutdown(self):\n        pass\n\n\nclass BreezySLAM(object):\n    \'\'\'\n    https://github.com/simondlevy/BreezySLAM\n    \'\'\'\n    def __init__(self, MAP_SIZE_PIXELS=500, MAP_SIZE_METERS=10):\n        from breezyslam.algorithms import RMHC_SLAM\n        from breezyslam.sensors import Laser\n\n        laser_model = Laser(scan_size=360, scan_rate_hz=10., detection_angle_degrees=360, distance_no_detection_mm=12000)\n        MAP_QUALITY=5\n        self.slam = RMHC_SLAM(laser_model, MAP_SIZE_PIXELS, MAP_SIZE_METERS, MAP_QUALITY)\n    \n    def run(self, distances, angles, map_bytes):\n        \n        self.slam.update(distances, scan_angles_degrees=angles)\n        x, y, theta = self.slam.getpos()\n\n        if map_bytes is not None:\n            self.slam.getmap(map_bytes)\n\n        #print(\'x\', x, \'y\', y, \'theta\', norm_deg(theta))\n        return x, y, deg2rad(norm_deg(theta))\n\n    def shutdown(self):\n        pass\n\n\n\nclass BreezyMap(object):\n    \'\'\'\n    bitmap that may optionally be constructed by BreezySLAM\n    \'\'\'\n    def __init__(self, MAP_SIZE_PIXELS=500):\n        self.mapbytes = bytearray(MAP_SIZE_PIXELS * MAP_SIZE_PIXELS)\n\n    def run(self):\n        return self.mapbytes\n\n    def shutdown(self):\n        pass\n\nclass MapToImage(object):\n\n    def __init__(self, resolution=(500, 500)):\n        self.resolution = resolution\n\n    def run(self, map_bytes):\n        np_arr = np.array(map_bytes).reshape(self.resolution)\n        return arr_to_img(np_arr)\n\n    def shutdown(self):\n        pass\n\n'"
donkeycar/parts/network.py,0,"b'import socket\nimport zlib, pickle\nimport zmq\nimport time\n\nclass ZMQValuePub(object):\n    \'\'\'\n    Use Zero Message Queue (zmq) to publish values\n    \'\'\'\n    def __init__(self, name, port = 5556, hwm=10):\n        context = zmq.Context()\n        self.name = name\n        self.socket = context.socket(zmq.PUB)\n        self.socket.set_hwm(hwm)\n        self.socket.bind(""tcp://*:%d"" % port)\n    \n    def run(self, values):\n        packet = { ""name"": self.name, ""val"" : values }\n        p = pickle.dumps(packet)\n        z = zlib.compress(p)\n        self.socket.send(z)\n\n    def shutdown(self):\n        print(""shutting down zmq"")\n        #self.socket.close()\n        context = zmq.Context()\n        context.destroy()\n\nclass ZMQValueSub(object):\n    \'\'\'\n    Use Zero Message Queue (zmq) to subscribe to value messages from a remote publisher\n    \'\'\'\n    def __init__(self, name, ip, port = 5556, hwm=10, return_last=True):\n        context = zmq.Context()\n        self.socket = context.socket(zmq.SUB)\n        self.socket.set_hwm(hwm)\n        self.socket.connect(""tcp://%s:%d"" % (ip, port))\n        self.socket.setsockopt_string(zmq.SUBSCRIBE, \'\')\n        self.name = name\n        self.return_last = return_last\n        self.last = None\n\n    def run(self):\n        \'\'\'\n        poll socket for input. returns None when nothing was recieved\n        otherwize returns packet data\n        \'\'\'\n        try:\n            z = self.socket.recv(flags=zmq.NOBLOCK)\n        except zmq.Again as e:\n            if self.return_last:\n                return self.last\n            return None\n\n        #print(""got"", len(z), ""bytes"")\n        p = zlib.decompress(z)\n        obj = pickle.loads(p)\n\n        if self.name == obj[\'name\']:\n            self.last = obj[\'val\'] \n            return obj[\'val\']\n\n        if self.return_last:\n            return self.last\n        return None\n\n    def shutdown(self):\n        self.socket.close()\n        context = zmq.Context()\n        context.destroy()\n\nclass UDPValuePub(object):\n    \'\'\'\n    Use udp to broadcast values on local network\n    \'\'\'\n    def __init__(self, name, port = 37021):\n        self.name = name\n        self.port = port\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)    \n        self.sock.settimeout(0.2)\n        self.sock.bind(("""", 44444))\n\n    def run(self, values):\n        packet = { ""name"": self.name, ""val"" : values }\n        p = pickle.dumps(packet)\n        z = zlib.compress(p)\n        #print(""broadcast"", len(z), ""bytes to port"", self.port)\n        self.sock.sendto(z, (\'<broadcast>\', self.port))\n\n    def shutdown(self):\n        self.sock.close()\n\nclass UDPValueSub(object):\n    \'\'\'\n    Use UDP to listen for broadcase packets\n    \'\'\'\n    def __init__(self, name, port = 37021, def_value=None):\n        self.client = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # UDP\n        self.client.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        self.client.bind(("""", port))\n        print(""listening for UDP broadcasts on port"", port)\n        self.name = name\n        self.last = def_value\n        self.running = True\n\n    def run(self):\n        self.poll()\n        return self.last\n\n    def run_threaded(self):\n        return self.last\n\n    def update(self):\n        while self.running:\n            self.poll()\n\n    def poll(self):\n        data, addr = self.client.recvfrom(1024 * 65)\n        #print(""got"", len(data), ""bytes"")\n        if len(data) > 0:\n            p = zlib.decompress(data)\n            obj = pickle.loads(p)\n\n            if self.name == obj[\'name\']:\n                self.last = obj[\'val\']\n\n\n    def shutdown(self):\n        self.running = False\n        self.client.close()\n\nimport select\n\nclass TCPServeValue(object):\n    \'\'\'\n    Use tcp to serve values on local network\n    \'\'\'\n    def __init__(self, name, port = 3233):\n        self.name = name\n        self.port = port\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        self.sock.setblocking(False)\n        self.sock.bind((""0.0.0.0"", port))\n        self.sock.listen(3)\n        print(""serving value:"", name, ""on port:"", port)\n        self.clients = []\n\n    def send(self, sock, msg):\n        try:\n            sock.sendall(msg)\n        except ConnectionResetError:\n            print(""client dropped connection"")\n            self.clients.remove(sock)\n\n        #print(""sent"", len(msg), ""bytes"")\n\n    def run(self, values):\n        timeout = 0.05\n        ready_to_read, ready_to_write, in_error = \\\n               select.select(\n                  [self.sock],\n                  self.clients,\n                  [],\n                  timeout)\n            \n        if len(ready_to_write) > 0:\n            packet = { ""name"": self.name, ""val"" : values }\n            p = pickle.dumps(packet)\n            z = zlib.compress(p)\n            for client in ready_to_write:\n                try:\n                    self.send(client, z)\n                except BrokenPipeError or ConnectionResetError:\n                    print(""client dropped connection"")\n                    self.clients.remove(client)\n        \n        if self.sock in ready_to_read:\n            client, addr = self.sock.accept()\n            print(""got connection from"", addr)\n            self.clients.append(client)\n\n        if len(in_error) > 0:\n            print(""clients gone"")\n            for sock in in_error:\n                self.clients.remove(sock)\n\n    def shutdown(self):\n        self.sock.close()\n\n\nclass TCPClientValue(object):\n    \'\'\'\n    Use tcp to get values on local network\n    \'\'\'\n    def __init__(self, name, host, port=3233):\n        self.name = name\n        self.port = port\n        self.addr = (host, port)\n        self.sock = None\n        self.connect()\n        self.timeout = 0.05\n        self.lastread = time.time()\n\n    def connect(self):\n        print(""attempting connect to"", self.addr)\n        try:\n            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.sock.connect(self.addr)\n        except ConnectionRefusedError:\n            print(\'server down\')\n            time.sleep(3.0)\n            self.sock = None\n            return False\n        print(""connected!"")\n        self.sock.setblocking(False)\n        return True\n\n    def is_connected(self):\n        return self.sock is not None\n\n    def read(self, sock):\n        data = self.sock.recv(64 * 1024)\n\n        ready_to_read, ready_to_write, in_error = \\\n        select.select(\n            [self.sock],\n            [],\n            [],\n            self.timeout)\n\n        while len(ready_to_read) == 1:\n            more_data = self.sock.recv(64 * 1024)\n            if len(more_data) == 0:\n                break\n            data = data + more_data\n\n            ready_to_read, ready_to_write, in_error = \\\n            select.select(\n                [self.sock],\n                [],\n                [],\n                self.timeout)\n\n        return data\n\n    def reset(self):\n        self.sock.close()\n        self.sock = None\n        self.lastread = time.time()\n            \n \n    def run(self):\n\n        time_since_last_read = abs(time.time() - self.lastread)\n        \n        if self.sock is None:\n            if not self.connect():\n                return None\n        elif time_since_last_read > 5.0: \n            print(""error: no data from server. may have died"")\n            self.reset()\n            return None\n\n        ready_to_read, ready_to_write, in_error = \\\n               select.select(\n                  [self.sock],\n                  [self.sock],\n                  [],\n                  self.timeout)\n\n        if len(in_error) > 0:\n            print(""error: server may have died"")\n            self.reset()\n            return None\n\n        if len(ready_to_read) == 1:\n            try:\n                data = self.read(self.sock)\n                #print(""got"", len(data), ""bytes"")\n                self.lastread = time.time()\n                p = zlib.decompress(data)\n                obj = pickle.loads(p)\n            except Exception as e:\n                print(e)\n                print(""error: server may have died"")\n                self.reset()\n                return None\n\n            if self.name == obj[\'name\']:\n                self.last = obj[\'val\'] \n                return obj[\'val\']\n\n        if len(in_error) > 0:\n            print(""connection closed"")\n            self.reset()\n\n        return None\n\n    def shutdown(self):\n        self.sock.close()\n\nclass MQTTValuePub(object):\n    \'\'\'\n    Use MQTT to send values on network\n    pip install paho-mqtt\n    \'\'\'\n    def __init__(self, name, broker=""iot.eclipse.org""):\n        from paho.mqtt.client import Client\n\n        self.name = name\n        self.message = None\n        self.client = Client()\n        print(""connecting to broker"", broker)\n        self.client.connect(broker)\n        self.client.loop_start()\n        print(""connected."")\n\n    def run(self, values):\n        packet = { ""name"": self.name, ""val"" : values }\n        p = pickle.dumps(packet)\n        z = zlib.compress(p)\n        self.client.publish(self.name, z)\n\n    def shutdown(self):\n        self.client.disconnect()\n        self.client.loop_stop()\n\n\nclass MQTTValueSub(object):\n    \'\'\'\n    Use MQTT to recv values on network\n    pip install paho-mqtt\n    \'\'\'\n    def __init__(self, name, broker=""iot.eclipse.org"", def_value=None):\n        from paho.mqtt.client import Client\n\n        self.name = name\n        self.data = None\n        self.client = Client(clean_session=True)\n        self.client.on_message = self.on_message\n        print(""(clean_session) connecting to broker"", broker)\n        self.client.connect(broker)\n        self.client.loop_start()\n        self.client.subscribe(self.name)\n        self.def_value = def_value\n        print(""connected."")\n\n    def on_message(self, client, userdata, message):\n        self.data = message.payload\n        \n    def run(self):\n        if self.data is None:\n            return self.def_value\n\n        p = zlib.decompress(self.data)\n        obj = pickle.loads(p)\n\n        if self.name == obj[\'name\']:\n            self.last = obj[\'val\']\n            #print(""steering, throttle"", obj[\'val\'])\n            return obj[\'val\']\n            \n        return self.def_value\n\n    def shutdown(self):\n        self.client.disconnect()\n        self.client.loop_stop()\n\n\ndef test_pub_sub(ip):\n    \n    if ip is None:\n        print(""publishing test.."")\n        p = ZMQValuePub(\'test\')\n        import math\n        theta = 0.0\n        s = time.time()\n\n        while True:\n            v = (time.time() - s, math.sin(theta), math.cos(theta), math.tan(theta))\n            theta += 0.1\n            p.run(v)\n            time.sleep(0.1)\n\n    else:\n        print(""subscribing test.."", ip)\n        s = ZMQValueSub(\'test\', ip=ip)\n\n        while True:\n            res = s.run()\n            print(""got:"", res)\n            time.sleep(1)\n\ndef test_udp_broadcast(ip):\n    \n    if ip is None:\n        print(""udp broadcast test.."")\n        p = UDPValuePub(\'camera\')\n        from donkeycar.parts.camera import PiCamera\n        from donkeycar.parts.image import ImgArrToJpg\n        cam = PiCamera(160, 120, 3, framerate=4)\n        img_conv = ImgArrToJpg()\n        time.sleep(1)\n        \n        while True:\n            cam_img = cam.run()\n            jpg = img_conv.run(cam_img)\n            print(""sending"", len(jpg), ""bytes"")\n            p.run(jpg)\n            time.sleep(0.5)\n\n    else:\n        print(""udp listen test.."", ip)\n        s = UDPValueSub(\'camera\')\n\n        while True:\n            res = s.run()\n            time.sleep(0.1)\n\ndef test_tcp_client_server(ip):\n\n    if ip is None:\n        p = TCPServeValue(""camera"")\n        from donkeycar.parts.camera import PiCamera\n        from donkeycar.parts.image import ImgArrToJpg\n        cam = PiCamera(160, 120, 3, framerate=4)\n        img_conv = ImgArrToJpg()\n        while True:\n            cam_img = cam.run()\n            jpg = img_conv.run(cam_img)\n            p.run(jpg)\n            time.sleep(0.1)\n    else:\n        c = TCPClientValue(""camera"", ip)\n        while True:\n            c.run()\n            time.sleep(0.01)\n\ndef test_mqtt_pub_sub(ip):\n    \n    if ip is None:\n        print(""publishing test.."")\n        p = MQTTValuePub(\'donkey/camera\')\n        from donkeycar.parts.camera import PiCamera\n        from donkeycar.parts.image import ImgArrToJpg\n        cam = PiCamera(160, 120, 3, framerate=4)\n        img_conv = ImgArrToJpg()\n        while True:\n            cam_img = cam.run()\n            jpg = img_conv.run(cam_img)\n            p.run(jpg)\n            time.sleep(0.1)\n\n    else:\n        print(""subscribing test.."")\n        s = MQTTValueSub(\'donkey/camera\')\n\n        while True:\n            res = s.run()\n            print(""got:"", res)\n            time.sleep(0.1)\n\nif __name__ == ""__main__"":\n    import time\n    import sys\n\n    #usage:\n    #  for subscriber test, pass ip arg like:\n    # python network.py ip=localhost\n    #\n    #  for publisher test, pass no args\n    # python network.py\n\n    ip = None\n\n    for arg in sys.argv:\n        if ""ip="" in arg:\n            ip = arg[3:]\n\n    #test_pub_sub(ip)\n    #test_udp_broadcast(ip)\n    #test_mqtt_pub_sub(ip)\n    test_tcp_client_server(ip)\n    \n\n'"
donkeycar/parts/oled.py,0,"b'import Adafruit_SSD1306\n\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport subprocess\nimport time\n\nclass OLEDDisplay(object):\n    \'\'\'\n    Manages drawing of text on the OLED display.\n    \'\'\'\n    def __init__(self, bus_number=1):\n        # Placeholder\n        self._EMPTY = \'\'\n        # Total number of lines of text\n        self._SLOT_COUNT = 4\n        self.bus_number = bus_number\n        self.slots = [self._EMPTY] * self._SLOT_COUNT\n        self.display = None\n\n    def init_display(self):\n        \'\'\'\n        Initializes the OLED display.\n        \'\'\'\n        if self.display is None:\n            # Use gpio = 1 to prevent platform auto-detection.\n            self.display = Adafruit_SSD1306.SSD1306_128_32(rst=None, i2c_bus=self.bus_number, gpio=1)\n            # Initialize Library\n            self.display.begin()\n            # Clear Display\n            self.display.clear()\n            self.display.display()\n            # Display Metrics\n            self.width = self.display.width\n            self.height = self.display.height\n            # Create Image in 1-bit mode\n            self.image = Image.new(\'1\', (self.width, self.height))\n            # Create a Drawing object to draw into the image\n            self.draw = ImageDraw.Draw(self.image)\n            # Load Fonts\n            self.font = ImageFont.load_default()\n            self.clear_display()\n\n    def clear_display(self):\n        if self.draw is not None:\n            self.draw.rectangle((0, 0, self.width, self.height), outline=0, fill=0)\n\n    def update_slot(self, index, text):\n        if index < len(self.slots):\n            self.slots[index] = text\n\n    def clear_slot(self, index):\n        if index < len(self.slots):\n            self.slots[index] = self._EMPTY\n\n    def update(self):\n        \'\'\'Display text\'\'\'\n        x = 0\n        top = -2\n        self.clear_display()\n        for i in range(self._SLOT_COUNT):\n            text = self.slots[i]\n            if len(text) > 0:\n                self.draw.text((x, top), text, font=self.font, fill=255)\n                top += 8\n\n        # Update\n        self.display.image(self.image)\n        self.display.display()\n\n\nclass OLEDPart(object):\n    \'\'\'\n    The part that updates status on the oled display.\n    \'\'\'\n    def __init__(self, bus_number, auto_record_on_throttle=False):\n        self.bus_number = bus_number\n        self.oled = OLEDDisplay(self.bus_number)\n        self.oled.init_display()\n        self.on = False\n        if auto_record_on_throttle:\n            self.recording = \'AUTO\'\n        else:\n            self.recording = \'NO\'\n        self.num_records = 0\n        self.user_mode = None\n        eth0 = OLEDPart.get_ip_address(\'eth0\')\n        wlan0 = OLEDPart.get_ip_address(\'wlan0\')\n        if eth0 is not None:\n            self.eth0 = \'eth0 : %s\' % (eth0)\n        else:\n            self.eth0 = None\n        if wlan0 is not None:\n            self.wlan0 = \'wlan0 : %s\' % (wlan0)\n        else:\n            self.wlan0 = None\n\n    def run(self):\n        if not self.on:\n            self.on = True\n\n    def run_threaded(self, recording, num_records, user_mode):\n        if num_records is not None and num_records > 0:\n            self.num_records = num_records\n\n        if recording:\n            self.recording = \'YES (Records = %s)\' % (self.num_records)\n        else:\n            self.recording = \'NO (Records = %s)\' % (self.num_records)\n\n        self.user_mode = \'User Mode (%s)\' % (user_mode)\n        self.update()\n\n    def update_slots(self):\n        updates = [self.eth0, self.wlan0, self.recording, self.user_mode]\n        index = 0\n        # Update slots\n        for update in updates:\n            if update is not None:\n                self.oled.update_slot(index, update)\n                index += 1\n\n        # Update display\n        self.oled.update()\n\n    def update(self):\n        self.update_slots()\n\n    def shutdown(self):\n        self.oled.clear_display()\n        self.on = False\n\n    # https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/jetbot/utils/utils.py\n\n    @classmethod\n    def get_ip_address(cls, interface):\n        if OLEDPart.get_network_interface_state(interface) == \'down\':\n            return None\n        cmd = ""ifconfig %s | grep -Eo \'inet (addr:)?([0-9]*\\.){3}[0-9]*\' | grep -Eo \'([0-9]*\\.){3}[0-9]*\' | grep -v \'127.0.0.1\'"" % interface\n        return subprocess.check_output(cmd, shell=True).decode(\'ascii\')[:-1]\n\n    @classmethod\n    def get_network_interface_state(cls, interface):\n        return subprocess.check_output(\'cat /sys/class/net/%s/operstate\' % interface, shell=True).decode(\'ascii\')[:-1]\n'"
donkeycar/parts/path.py,0,"b'import pickle\nimport math\nimport logging\n\nimport numpy\nfrom PIL import Image, ImageDraw\n\nfrom donkeycar.utils import norm_deg, dist, deg2rad, arr_to_img\n\n\nclass Path(object):\n    def __init__(self, min_dist = 1.):\n        self.path = []\n        self.min_dist = min_dist\n        self.x = math.inf\n        self.y = math.inf\n        self.recording = True\n\n    def run(self, x, y):\n        d = dist(x, y, self.x, self.y)\n        if self.recording and d > self.min_dist:\n            self.path.append((x, y))\n            logging.info(""path point (%f, %f)"" % ( x, y))\n            self.x = x\n            self.y = y\n        return self.path\n\n    def save(self, filename):\n        outfile = open(filename, \'wb\')\n        pickle.dump(self.path, outfile)\n    \n    def load(self, filename):\n        infile = open(filename, \'rb\')\n        self.path = pickle.load(infile)\n        self.recording = False\n\nclass PImage(object):\n    def __init__(self, resolution=(500, 500), color=""white"", clear_each_frame=False):\n        self.resolution = resolution\n        self.color = color\n        self.img = Image.new(\'RGB\', resolution, color=color)\n        self.clear_each_frame = clear_each_frame\n\n    def run(self):\n        if self.clear_each_frame:\n            self.img = Image.new(\'RGB\', self.resolution, color=self.color)\n\n        return self.img\n\n\nclass OriginOffset(object):\n    \'\'\'\n    Use this to set the car back to the origin without restarting it.\n    \'\'\'\n\n    def __init__(self):\n        self.ox = 0.0\n        self.oy = 0.0\n        self.last_x = 0.\n        self.last_y = 0.\n\n    def run(self, x, y):\n        self.last_x = x\n        self.last_y = y\n\n        return x + self.ox, y + self.oy\n\n    def init_to_last(self):\n        self.ox = -self.last_x\n        self.oy = -self.last_y\n\n\nclass PathPlot(object):\n    \'\'\'\n    draw a path plot to an image\n    \'\'\'\n    def __init__(self, scale=1.0, offset=(0., 0.0)):\n        self.scale = scale\n        self.offset = offset\n\n    def plot_line(self, sx, sy, ex, ey, draw, color):\n        \'\'\'\n        scale dist so that max_dist is edge of img (mm)\n        and img is PIL Image, draw the line using the draw ImageDraw object\n        \'\'\'\n        draw.line((sx,sy, ex, ey), fill=color, width=1)\n\n    def run(self, img, path):\n        \n        if type(img) is numpy.ndarray:\n            stacked_img = numpy.stack((img,)*3, axis=-1)\n            img = arr_to_img(stacked_img)\n\n        draw = ImageDraw.Draw(img)\n        color = (255, 0, 0)\n        for iP in range(0, len(path) - 1):\n            ax, ay = path[iP]\n            bx, by = path[iP + 1]\n            self.plot_line(ax * self.scale + self.offset[0],\n                        ay * self.scale + self.offset[1], \n                        bx * self.scale + self.offset[0], \n                        by * self.scale + self.offset[1], \n                        draw, \n                        color)\n\n        return img\n\n\nclass PlotCircle(object):\n    \'\'\'\n    draw a circle plot to an image\n    \'\'\'\n    def __init__(self,  scale=1.0, offset=(0., 0.0), radius=4, color = (0, 255, 0)):\n        self.scale = scale\n        self.offset = offset\n        self.radius = radius\n        self.color = color\n\n    def plot_circle(self, x, y, rad, draw, color, width=1):\n        \'\'\'\n        scale dist so that max_dist is edge of img (mm)\n        and img is PIL Image, draw the circle using the draw ImageDraw object\n        \'\'\'\n        sx = x - rad\n        sy = y - rad\n        ex = x + rad\n        ey = y + rad\n\n        draw.ellipse([(sx, sy), (ex, ey)], fill=color)\n\n\n    def run(self, img, x, y):\n        draw = ImageDraw.Draw(img)\n        self.plot_circle(x * self.scale + self.offset[0],\n                        y * self.scale + self.offset[1], \n                        self.radius,\n                        draw, \n                        self.color)\n\n        return img\n\nfrom donkeycar.la import Line3D, Vec3\n\nclass CTE(object):\n\n    def nearest_two_pts(self, path, x, y):\n        if len(path) < 2:\n            return None, None\n\n        distances = []\n        for iP, p in enumerate(path):\n            d = dist(p[0], p[1], x, y)\n            distances.append((d, iP, p))\n        distances.sort(key=lambda elem : elem[0])\n        iA = (distances[0][1] - 1) % len(path)\n        a = path[iA]\n        #iB is the next element in the path, wrapping around..\n        iB = (iA + 2) % len(path)\n        b = path[iB]\n        \n        return a, b\n\n    def run(self, path, x, y):\n        cte = 0.\n\n        a, b = self.nearest_two_pts(path, x, y)\n        \n        if a and b:\n            #logging.info(""nearest: (%f, %f) to (%f, %f)"" % ( a[0], a[1], x, y))\n            a_v = Vec3(a[0], 0., a[1])\n            b_v = Vec3(b[0], 0., b[1])\n            p_v = Vec3(x, 0., y)\n            line = Line3D(a_v, b_v)\n            err = line.vector_to(p_v)\n            sign = 1.0\n            cp = line.dir.cross(err.normalized())\n            if cp.y > 0.0 :\n                sign = -1.0\n            cte = err.mag() * sign            \n\n        return cte\n\n\nclass PID_Pilot(object):\n\n    def __init__(self, pid, throttle):\n        self.pid = pid\n        self.throttle = throttle\n\n    def run(self, cte):\n        steer = self.pid.run(cte)\n        logging.info(""CTE: %f steer: %f"" % (cte, steer))\n        return steer, self.throttle\n'"
donkeycar/parts/pigpio_enc.py,0,"b'import pigpio\nimport time\n\nclass OdomDist(object):\n    """"""\n    Take a tick input from odometry and compute the distance travelled\n    """"""\n    def __init__(self, mm_per_tick, debug=False):\n        self.mm_per_tick = mm_per_tick\n        self.m_per_tick = mm_per_tick / 1000.0\n        self.meters = 0\n        self.last_time = time.time()\n        self.meters_per_second = 0\n        self.debug = debug\n        self.prev_ticks = 0\n\n    def run(self, ticks, throttle):\n        """"""\n        inputs => total ticks since start\n        inputs => throttle, used to determine positive or negative vel\n        return => total dist (m), current vel (m/s), delta dist (m)\n        """"""\n        new_ticks = ticks - self.prev_ticks\n        self.prev_ticks = ticks\n\n        #save off the last time interval and reset the timer\n        start_time = self.last_time\n        end_time = time.time()\n        self.last_time = end_time\n        \n        #calculate elapsed time and distance traveled\n        seconds = end_time - start_time\n        distance = new_ticks * self.m_per_tick\n        if throttle < 0.0:\n            distance = distance * -1.0\n        velocity = distance / seconds\n        \n        #update the odometer values\n        self.meters += distance\n        self.meters_per_second = velocity\n\n        #console output for debugging\n        if(self.debug):\n            print(\'seconds:\', seconds)\n            print(\'delta:\', distance)\n            print(\'velocity:\', velocity)\n\n            print(\'distance (m):\', self.meters)\n            print(\'velocity (m/s):\', self.meters_per_second)\n\n        return self.meters, self.meters_per_second, distance\n\n\nclass PiPGIOEncoder():\n    def __init__(self, pin, pi):\n        self.pin = pin\n        self.pi = pi\n        self.pi.set_mode(pin, pigpio.INPUT)\n        self.pi.set_pull_up_down(pin, pigpio.PUD_UP)\n        self.cb = pi.callback(self.pin, pigpio.FALLING_EDGE, self._cb)\n        self.count = 0\n\n    def _cb(self, pin, level, tick):\n        self.count += 1\n\n    def run(self):\n        return self.count\n\n    def shutdown(self):\n        if self.cb != None:\n            self.cb.cancel()\n            self.cb = None\n\n        self.pi.stop()\n\n\nif __name__ == ""__main__"":\n    pi = pigpio.pi()\n    e = PiPGIOEncoder(4, pi)\n    while True:\n        time.sleep(0.1)\n        e.run()\n\n\n'"
donkeycar/parts/realsense2.py,0,"b'\'\'\'\nAuthor: Tawn Kramer\nFile: realsense2.py\nDate: April 14 2019\nNotes: Parts to input data from Intel Realsense 2 cameras\n\'\'\'\nimport time\nimport logging\n\nimport numpy as np\nimport pyrealsense2 as rs\n\nclass RS_T265(object):\n    \'\'\'\n    The Intel Realsense T265 camera is a device which uses an imu, twin fisheye cameras,\n    and an Movidius chip to do sensor fusion and emit a world space coordinate frame that \n    is remarkably consistent.\n    \'\'\'\n\n    def __init__(self, image_output=False, calib_filename=None):\n        # Using the image_output will grab two image streams from the fisheye cameras but return only one.\n        # This can be a bit much for USB2, but you can try it. Docs recommend USB3 connection for this.\n        self.image_output = image_output\n\n        # When we have and encoder, this will be the last vel measured. \n        self.enc_vel_ms = 0.0\n        self.wheel_odometer = None\n\n        # Declare RealSense pipeline, encapsulating the actual device and sensors\n        print(""starting T265"")\n        self.pipe = rs.pipeline()\n        cfg = rs.config()\n        cfg.enable_stream(rs.stream.pose)\n        profile = cfg.resolve(self.pipe)\n        dev = profile.get_device()\n        tm2 = dev.as_tm2()\n        \n\n        if self.image_output:\n            #right now it\'s required for both streams to be enabled\n            cfg.enable_stream(rs.stream.fisheye, 1) # Left camera\n            cfg.enable_stream(rs.stream.fisheye, 2) # Right camera\n\n        if calib_filename is not None:\n            pose_sensor = tm2.first_pose_sensor()\n            self.wheel_odometer = pose_sensor.as_wheel_odometer() \n\n            # calibration to list of uint8\n            f = open(calib_filename)\n            chars = []\n            for line in f:\n                for c in line:\n                    chars.append(ord(c))  # char to uint8\n\n            # load/configure wheel odometer\n            print(""loading wheel config"", calib_filename)\n            self.wheel_odometer.load_wheel_odometery_config(chars)   \n\n\n        # Start streaming with requested config\n        self.pipe.start(cfg)\n        self.running = True\n        print(""Warning: T265 needs a warmup period of a few seconds before it will emit tracking data."")\n        \n        zero_vec = (0.0, 0.0, 0.0)\n        self.pos = zero_vec\n        self.vel = zero_vec\n        self.acc = zero_vec\n        self.img = None\n\n    def poll(self):\n\n        if self.wheel_odometer:\n            wo_sensor_id = 0  # indexed from 0, match to order in calibration file\n            frame_num = 0  # not used\n            v = rs.vector()\n            v.x = -1.0 * self.enc_vel_ms  # m/s\n            #v.z = -1.0 * self.enc_vel_ms  # m/s\n            self.wheel_odometer.send_wheel_odometry(wo_sensor_id, frame_num, v)\n\n        try:\n            frames = self.pipe.wait_for_frames()\n        except Exception as e:\n            logging.error(e)\n            return\n\n        if self.image_output:\n            #We will just get one image for now.\n            # Left fisheye camera frame\n            left = frames.get_fisheye_frame(1)\n            self.img = np.asanyarray(left.get_data())\n\n\n        # Fetch pose frame\n        pose = frames.get_pose_frame()\n\n        if pose:\n            data = pose.get_pose_data()\n            self.pos = data.translation\n            self.vel = data.velocity\n            self.acc = data.acceleration\n            logging.debug(\'realsense pos(%f, %f, %f)\' % (self.pos.x, self.pos.y, self.pos.z))\n\n\n    def update(self):\n        while self.running:\n            self.poll()\n\n    def run_threaded(self, enc_vel_ms):\n        self.enc_vel_ms = enc_vel_ms\n        return self.pos, self.vel, self.acc, self.img\n\n    def run(self, enc_vel_ms):\n        self.enc_vel_ms = enc_vel_ms\n        self.poll()\n        return self.run_threaded()\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(0.1)\n        self.pipe.stop()\n\n\n\nif __name__ == ""__main__"":\n    c = RS_T265()\n    while True:\n        pos, vel, acc = c.run()\n        print(pos)\n        time.sleep(0.1)\n    c.shutdown()\n'"
donkeycar/parts/realsense435i.py,0,"b'""""""\nAuthor: Ed Murphy\nFile: realsense435i.py\nDate: April 14 2019\nNotes: Donkeycar part for the Intel Realsense depth cameras D435 and D435i.\n""""""\nimport time\nimport logging\n\nimport numpy as np\nimport pyrealsense2 as rs\n\n#\n# NOTE: Jetson Nano users should clone the Jetson Hacks project\n#       https://github.com/JetsonHacksNano/installLibrealsense\n#       and _build librealsense from source_ in order to get the python bindings.\n#       ./buildLibrealsense.sh\n#\n\n#\n# The Realsense D435 will not output images with arbitrarily chosen dimensions.\n# The dimensions must be from the allowed list.  I\'ve chosen a reasonable\n# resolution that I know is supported by the camera.\n# If the part is initialized with different sizes, then opencv\n# will be used to resize the image before it is returned.\n#\nWIDTH = 424\nHEIGHT = 240\nCHANNELS = 3\n\nclass RealSense435i(object):\n    """"""\n    Donkeycar part for the Intel Realsense depth cameras D435 and D435i.\n    The Intel Realsense D435i camera is a device which uses an imu, twin fisheye cameras,\n    and an Movidius chip to stream a depth map along with an rgb image and optionally,\n    accelerometer and gyro data (the \'i\' variant has an IMU, the non-i variant does not)\n    NOTE: this ALWAYS captures 424 pixels wide x 240 pixels high x RGB at 60fps.\n          If an image width, height or depth are passed with different values,\n          the the outputs will be scaled to the requested size on the way out.\n    """"""\n\n    def __init__(self, width = WIDTH, height = HEIGHT, channels = CHANNELS, enable_rgb=True, enable_depth=True, enable_imu=False, device_id = None):\n        self.device_id = device_id  # ""923322071108"" # serial number of device to use or None to use default\n        self.enable_imu = enable_imu\n        self.enable_rgb = enable_rgb\n        self.enable_depth = enable_depth\n\n        self.width = width\n        self.height = height\n        self.channels = channels\n        self.resize = (width != WIDTH) or (height != height) or (channels != CHANNELS)\n        if self.resize:\n            print(""The output images will be resized from {} to {}.  This requires opencv."".format((WIDTH, HEIGHT, CHANNELS), (self.width, self.height, self.channels)))\n\n        # Configure streams\n        self.imu_pipeline = None\n        if self.enable_imu:\n            self.imu_pipeline = rs.pipeline()\n            imu_config = rs.config()\n            if None != self.device_id:\n                imu_config.enable_device(self.device_id)\n            imu_config.enable_stream(rs.stream.accel, rs.format.motion_xyz32f, 63)  # acceleration\n            imu_config.enable_stream(rs.stream.gyro, rs.format.motion_xyz32f, 200)  # gyroscope\n            imu_profile = self.imu_pipeline.start(imu_config)\n            # eat some frames to allow imu to settle\n            for i in range(0, 5):\n                self.imu_pipeline.wait_for_frames()\n\n        self.pipeline = None\n        if self.enable_depth or self.enable_rgb:\n            self.pipeline = rs.pipeline()\n            config = rs.config()\n\n            # if we are provided with a specific device, then enable it\n            if None != self.device_id:\n                config.enable_device(self.device_id)\n\n            if self.enable_depth:\n                config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)  # depth\n\n            if self.enable_rgb:\n                config.enable_stream(rs.stream.color, 424, 240, rs.format.rgb8, 60)  # rgb\n\n            # Start streaming\n            profile = self.pipeline.start(config)\n\n            # Getting the depth sensor\'s depth scale (see rs-align example for explanation)\n            if self.enable_depth:\n                depth_sensor = profile.get_device().first_depth_sensor()\n                depth_scale = depth_sensor.get_depth_scale()\n                print(""Depth Scale is: "", depth_scale)\n                if self.enable_rgb:\n                    # Create an align object\n                    # rs.align allows us to perform alignment of depth frames to others frames\n                    # The ""align_to"" is the stream type to which we plan to align depth frames.\n                    align_to = rs.stream.color\n                    self.align = rs.align(align_to)\n\n            # eat some frames to allow auto-exposure to settle\n            for i in range(0, 5):\n                self.pipeline.wait_for_frames()\n\n        time.sleep(2)   # let camera warm up\n\n        # initialize frame state\n        self.color_image = None\n        self.depth_image = None\n        self.acceleration_x = None\n        self.acceleration_y = None\n        self.acceleration_z = None\n        self.gyroscope_x = None\n        self.gyroscope_y = None\n        self.gyroscope_z = None\n        self.frame_count = 0\n        self.start_time = time.time()\n        self.frame_time = self.start_time\n\n        self.running = True\n\n    def _stop_pipeline(self):\n        if self.imu_pipeline is not None:\n            self.imu_pipeline.stop()\n            self.imu_pipeline = None\n        if self.pipeline is not None:\n            self.pipeline.stop()\n            self.pipeline = None\n\n    def _poll(self):\n        last_time = self.frame_time\n        self.frame_time = time.time() - self.start_time\n        self.frame_count += 1\n\n        #\n        # get the frames\n        #\n        try:\n            if self.enable_imu:\n                imu_frames = self.imu_pipeline.wait_for_frames()\n\n            if self.enable_rgb or self.enable_depth:\n                frames = self.pipeline.wait_for_frames()\n        except Exception as e:\n            logging.error(e)\n            return\n\n        #\n        # convert camera frames to images\n        #\n        if self.enable_rgb or self.enable_depth:\n            # Align the depth frame to color frame\n            aligned_frames = self.align.process(frames) if self.enable_depth and self.enable_rgb else None\n            depth_frame = aligned_frames.get_depth_frame() if aligned_frames is not None else frames.get_depth_frame()\n            color_frame = aligned_frames.get_color_frame() if aligned_frames is not None else frames.get_color_frame()\n\n            # Convert depth to 16bit array, RGB into 8bit planar array\n            self.depth_image = np.asanyarray(depth_frame.get_data(), dtype=np.uint16) if self.enable_depth else None\n            self.color_image = np.asanyarray(color_frame.get_data(), dtype=np.uint8) if self.enable_rgb else None\n\n            if self.resize:\n                import cv2\n                if self.width != WIDTH or self.height != HEIGHT:\n                    self.color_image = cv2.resize(self.color_image, (self.width, self.height), cv2.INTER_NEAREST) if self.enable_rgb else None\n                    self.depth_image = cv2.resize(self.depth_image, (self.width, self.height), cv2.INTER_NEAREST) if self.enable_depth else None\n                if self.channels != CHANNELS:\n                    self.color_image = cv2.cvtColor(self.color_image, cv2.COLOR_RGB2GRAY) if self.enable_rgb else None\n\n        #\n        # output imu data as discrete values in the same order as the Mpu6050 code\n        # so we can be compatible with any other parts that consume imu data.\n        #\n        if self.enable_imu:\n            acceleration = imu_frames.first_or_default(rs.stream.accel, rs.format.motion_xyz32f).as_motion_frame().get_motion_data()\n            self.acceleration_x = acceleration.x\n            self.acceleration_y = acceleration.y\n            self.acceleration_z = acceleration.z\n            gyroscope = imu_frames.first_or_default(rs.stream.gyro, rs.format.motion_xyz32f).as_motion_frame().get_motion_data()\n            self.gyroscope_x = gyroscope.x\n            self.gyroscope_y = gyroscope.y\n            self.gyroscope_z = gyroscope.z\n            logging.debug(""imu frame {} in {} seconds: \\n\\taccel = {}, \\n\\tgyro = {}"".format(\n                str(self.frame_count),\n                str(self.frame_time - last_time),\n                str((self.acceleration_x, self.acceleration_y, self.acceleration_z)),\n                str((self.gyroscope_x, self.gyroscope_y, self.gyroscope_z))))\n\n    def update(self):\n        """"""\n        When running threaded, update() is called from the background thread\n        to update the state.  run_threaded() is called to return the latest state.\n        """"""\n        while self.running:\n            self._poll()\n\n    def run_threaded(self):\n        """"""\n        Return the lastest state read by update().  This will not block.\n        All 4 states are returned, but may be None if the feature is not enabled when the camera part is constructed.\n        For gyroscope, x is pitch, y is yaw and z is roll.\n        :return: (rbg_image: nparray, depth_image: nparray, acceleration: (x:float, y:float, z:float), gyroscope: (x:float, y:float, z:float))\n        """"""\n        return self.color_image, self.depth_image, self.acceleration_x, self.acceleration_y, self.acceleration_z, self.gyroscope_x, self.gyroscope_y, self.gyroscope_z\n\n    def run(self):\n        """"""\n        Read and return frame from camera.  This will block while reading the frame.\n        see run_threaded() for return types.\n        """"""\n        self._poll()\n        return self.run_threaded()\n\n    def shutdown(self):\n        self.running = False\n        time.sleep(2) # give thread enough time to shutdown\n\n        # done running\n        self._stop_pipeline()\n\n\n#\n# self test\n#\nif __name__ == ""__main__"":\n\n    show_opencv_window = False # True to show images in opencv window: note that default donkeycar environment is not configured for this.\n    if show_opencv_window:\n        import cv2\n\n    enable_rgb = True\n    enable_depth = True\n    enable_imu = True\n    device_id = None\n\n    width = 212\n    height = 120\n    channels = 3\n\n    profile_frames = 0 # set to non-zero to calculate the max frame rate using given number of frames\n\n    try:\n        #\n        # for D435i, enable_imu can be True, for D435 enable_imu should be false\n        #\n        camera = RealSense435i(\n            width=width, height=height, channels=channels,\n            enable_rgb=enable_rgb, enable_depth=enable_depth, enable_imu=enable_imu, device_id=device_id)\n\n        frame_count = 0\n        start_time = time.time()\n        frame_time = start_time\n        while True:\n            #\n            # read data from camera\n            #\n            color_image, depth_image, acceleration_x, acceleration_y, acceleration_z, gyroscope_x, gyroscope_y, gyroscope_z = camera.run()\n\n            # maintain frame timing\n            frame_count += 1\n            last_time = frame_time\n            frame_time = time.time()\n\n            if enable_imu and not profile_frames:\n                print(""imu frame {} in {} seconds: \\n\\taccel = {}, \\n\\tgyro = {}"".format(\n                    str(frame_count),\n                    str(frame_time - last_time),\n                    str((acceleration_x, acceleration_y, acceleration_z)),\n                    str((gyroscope_x, gyroscope_y, gyroscope_z))))\n\n            # Show images\n            if show_opencv_window and not profile_frames:\n                cv2.namedWindow(\'RealSense\', cv2.WINDOW_AUTOSIZE)\n                if enable_rgb or enable_depth:\n                    # make sure depth and color images have same number of channels so we can show them together in the window\n                    if 3 == channels:\n                        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET) if enable_depth else None\n                    else:\n                        depth_colormap = cv2.cvtColor(cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET), cv2.COLOR_RGB2GRAY) if enable_depth else None\n\n\n                    # Stack both images horizontally\n                    images = None\n                    if enable_rgb:\n                        images = np.hstack((color_image, depth_colormap)) if enable_depth else color_image\n                    elif enable_depth:\n                        images = depth_colormap\n\n                    if images is not None:\n                        cv2.imshow(\'RealSense\', images)\n\n                # Press esc or \'q\' to close the image window\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\') or key == 27:\n                    cv2.destroyAllWindows()\n                    break\n            if profile_frames > 0:\n                if frame_count == profile_frames:\n                    print(""Aquired {} frames in {} seconds for {} fps"".format(str(frame_count), str(frame_time - start_time), str(frame_count / (frame_time - start_time))))\n                    break\n            else:\n                time.sleep(0.05)\n    finally:\n        camera.shutdown()\n'"
donkeycar/parts/robohat.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts for operating the RoboHAT MM1 by Robotics Masters with the Donkeycar\n\nauthor: @wallarug (Cian Byrne) 2019\ncontrib: @peterpanstechland 2019\ncontrib: @sctse999 2020\n\nNote: To be used with code.py bundled in this repo. See donkeycar/contrib/robohat/code.py\n""""""\n\nimport time\nimport donkeycar as dk\n\ntry:\n    import serial\nexcept ImportError:\n    print(""PySerial not found.  Please install: pip install pyserial"")\n\n\nclass RoboHATController:\n    \'\'\'\n    Driver to read signals from SAMD51 and convert into steering and throttle outputs\n    Input input signal range: 1000 to 2000\n    Output range: -1.00 to 1.00\n    \'\'\'\n\n    def __init__(self, cfg, debug=False):\n        # standard variables\n        self.angle = 0.0\n        self.throttle = 0.0\n        self.mode = \'user\'\n        self.recording = False\n        self.STEERING_MID = cfg.MM1_STEERING_MID\n        self.MAX_FORWARD = cfg.MM1_MAX_FORWARD\n        self.STOPPED_PWM = cfg.MM1_STOPPED_PWM\n        self.MAX_REVERSE = cfg.MM1_MAX_REVERSE\n        self.SHOW_STEERING_VALUE = cfg.MM1_SHOW_STEERING_VALUE\n        self.DEAD_ZONE = cfg.JOYSTICK_DEADZONE\n        self.debug = debug\n        \n        try:\n            self.serial = serial.Serial(cfg.MM1_SERIAL_PORT, 115200, timeout=1)\n        except serial.SerialException:\n            print(""Serial port not found!  Please enable: sudo raspi-config"")\n        except serial.SerialTimeoutException:\n            print(""Serial connection timed out!"")\n\n    def shutdown(self):\n        try:\n            self.serial.close()\n        except:\n            pass\n\n    def read_serial(self):\n        \'\'\'\n        Read the rc controller value from serial port. Map the value into\n        steering and throttle\n\n        Format ####,#### whereas the 1st number is steering and 2nd is throttle\n        \'\'\'\n        line = str(self.serial.readline().decode()).strip(\'\\n\').strip(\'\\r\')\n\n        output = line.split("", "")\n        if len(output) == 2:\n            if self.SHOW_STEERING_VALUE:\n                print(""MM1: steering={}"".format(output[0]))\n\n            if output[0].isnumeric() and output[1].isnumeric():\n                angle_pwm = float(output[0])\n                throttle_pwm = float(output[1])\n\n                if self.debug:\n                    print(""angle_pwm = {}, throttle_pwm= {}"".format(angle_pwm, throttle_pwm))\n\n\n                if throttle_pwm >= self.STOPPED_PWM:\n                    # Scale down the input pwm (1500 - 2000) to our max forward\n                    throttle_pwm = dk.utils.map_range_float(throttle_pwm,\n                                                                1500, 2000,\n                                                                self.STOPPED_PWM,\n                                                                self.MAX_FORWARD )\n                    # print(""remapping throttle_pwm from {} to {}"".format(output[1], throttle_pwm))\n\n                    # Go forward\n                    self.throttle = dk.utils.map_range_float(throttle_pwm,\n                                                             self.STOPPED_PWM,\n                                                             self.MAX_FORWARD,\n                                                             0, 1.0)\n                else:\n                    throttle_pwm = dk.utils.map_range_float(throttle_pwm,\n                                                                1000, 1500,\n                                                                self.MAX_REVERSE,\n                                                                self.STOPPED_PWM)\n\n\n                    # Go backward\n                    self.throttle = dk.utils.map_range_float(throttle_pwm,\n                                                             self.MAX_REVERSE,\n                                                             self.STOPPED_PWM,\n                                                             -1.0, 0)\n\n                if angle_pwm >= self.STEERING_MID:\n                    # Turn Left\n                    self.angle = dk.utils.map_range_float(angle_pwm,\n                                                          2000, self.STEERING_MID,\n                                                          -1, 0)\n                else:\n                    # Turn Right\n                    self.angle = dk.utils.map_range_float(angle_pwm,\n                                                          self.STEERING_MID, 1000,\n                                                          0, 1)\n\n                if self.debug:\n                    print(""angle = {}, throttle = {}"".format(self.angle, self.throttle))\n\n                if self.throttle > self.DEAD_ZONE:\n                    self.recording = True\n                else:\n                    self.recording = False\n\n                time.sleep(0.01)\n\n    def update(self):\n        # delay on startup to avoid crashing\n        print(""Warming serial port..."")\n        time.sleep(3)\n\n        while True:\n            try:\n                self.read_serial()\n            except:\n                print(""MM1: Error reading serial input!"")\n                break\n\n    def run(self, img_arr=None):\n        return self.run_threaded()\n\n    def run_threaded(self, img_arr=None):\n        return self.angle, self.throttle, self.mode, self.recording\n\n\nclass RoboHATDriver:\n    """"""\n    PWM motor controller using Robo HAT MM1 boards.\n    This is developed by Robotics Masters\n    """"""\n\n    def __init__(self, cfg, debug=False):\n        # Initialise the Robo HAT using the serial port\n        self.pwm = serial.Serial(cfg.MM1_SERIAL_PORT, 115200, timeout=1)\n        self.MAX_FORWARD = cfg.MM1_MAX_FORWARD\n        self.MAX_REVERSE = cfg.MM1_MAX_REVERSE\n        self.STOPPED_PWM = cfg.MM1_STOPPED_PWM\n        self.STEERING_MID = cfg.MM1_STEERING_MID\n        self.debug = debug\n\n    """"""\n    Steering and throttle should range between -1.0 to 1.0. This function will\n    trim value great than 1.0 or less than 1.0\n    """"""\n\n    def trim_out_of_bound_value(self, value):\n        if value > 1:\n            print(""MM1: Warning, value out of bound. Value = {}"".format(value))\n            return 1.0\n        elif value < -1:\n            print(""MM1: Warning, value out of bound. Value = {}"".format(value))\n            return -1.0\n        else:\n            return value\n\n    def set_pulse(self, steering, throttle):\n        try:\n            steering = self.trim_out_of_bound_value(steering)\n            throttle = self.trim_out_of_bound_value(throttle)\n\n            if throttle > 0:\n                output_throttle = dk.utils.map_range(throttle,\n                                                     0, 1.0,\n                                                     self.STOPPED_PWM, self.MAX_FORWARD)\n            else:\n                output_throttle = dk.utils.map_range(throttle,\n                                                     -1, 0,\n                                                     self.MAX_REVERSE, self.STOPPED_PWM)\n\n            if steering > 0:\n                output_steering = dk.utils.map_range(steering,\n                                                     0, 1.0,\n                                                     self.STEERING_MID, 1000)\n            else:\n                output_steering = dk.utils.map_range(steering,\n                                                     -1, 0,\n                                                     2000, self.STEERING_MID)\n\n            packet = ""{0},{1}"".format(str(output_steering).zfill(4),\n                                      str(output_throttle).zfill(4),)\n            self.write_pwm(packet)\n\n            if self.debug:\n                print(""output_steering=%d, output_throttle=%d\\r"" %\n                      (eval(packet)))\n        except OSError as err:\n            print(\n                ""Unexpected issue setting PWM (check wires to motor board): {0}"".format(err))\n\n    def write_pwm(self, data):\n        self.pwm.write(b""%d, %d\\r"" % (eval(data)))\n\n    def run(self, throttle, steering):\n        self.set_pulse(throttle, steering)\n\n    def shutdown(self):\n        try:\n            self.serial.close()\n        except:\n            pass\n'"
donkeycar/parts/ros.py,0,"b'import rospy\nfrom std_msgs.msg import String, Int32, Float32\n\n\'\'\'\nsudo apt-get install python3-catkin-pkg\n\nROS issues w python3:\nhttps://discourse.ros.org/t/should-we-warn-new-users-about-difficulties-with-python-3-and-alternative-python-interpreters/3874/3\n\'\'\'\n\nclass RosPubisher(object):\n    \'\'\'\n    A ROS node to pubish to a data stream\n    \'\'\'\n    def __init__(self, node_name, channel_name, stream_type=String, anonymous=True):\n        self.data = """"\n        self.pub = rospy.Publisher(channel_name, stream_type)\n        rospy.init_node(node_name, anonymous=anonymous)\n\n    def run(self, data):\n        \'\'\'\n        only publish when data stream changes.\n        \'\'\'\n        if data != self.data and not rospy.is_shutdown():\n            self.data = data\n            self.pub.publish(data)\n    \n\nclass RosSubscriber(object):\n    \'\'\'\n    A ROS node to subscribe to a data stream\n    \'\'\'\n\n    def __init__(self, node_name, channel_name, stream_type=String, anonymous=True):\n        self.data = """"\n        rospy.init_node(node_name, anonymous=anonymous)\n        self.pub = rospy.Subscriber(channel_name, stream_type, self.on_data_recv)        \n\n    def on_data_recv(self, data):\n        self.data = data.data\n\n    def run(self):\n        return self.data\n\n'"
donkeycar/parts/salient.py,8,"b""#from https://github.com/ermolenkodev/keras-salient-object-visualisation\nimport os\nfrom keras import backend as K\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom keras.layers import Input, Dense, merge\nfrom keras.models import Model\nfrom keras.layers import Convolution2D, MaxPooling2D, Reshape, BatchNormalization\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nimport cv2\nimport numpy as np\n\nclass SalientVis():\n    '''\n    Note, this part is quite tuned ust for the image dimensions and layers\n    in our standard models. It will not reflect cropping or additional layers\n    that might be in your model.\n    '''\n\n    def __init__(self, kerasPart):\n        self.model = kerasPart.model\n        self.init_salient(self.model)\n\n    def run(self, image):\n        if image is None:\n            return\n        image = self.draw_salient(image)\n        image = image * 255\n        image = image.astype('uint8')\n        return image\n\n    def init_salient(self, model):\n        img_in = Input(shape=(120, 160, 3), name='img_in')\n        x = img_in\n        x = Convolution2D(24, (5,5), strides=(2,2), activation='relu', name='conv1')(x)\n        x = Convolution2D(32, (5,5), strides=(2,2), activation='relu', name='conv2')(x)\n        x = Convolution2D(64, (5,5), strides=(2,2), activation='relu', name='conv3')(x)\n        x = Convolution2D(64, (3,3), strides=(2,2), activation='relu', name='conv4')(x)\n        conv_5 = Convolution2D(64, (3,3), strides=(1,1), activation='relu', name='conv5')(x)\n        self.convolution_part = Model(inputs=[img_in], outputs=[conv_5])\n\n        for layer_num in ('1', '2', '3', '4', '5'):\n            self.convolution_part.get_layer('conv' + layer_num).set_weights(model.get_layer('conv2d_' + layer_num).get_weights())\n        \n        self.inp = self.convolution_part.input                                           # input placeholder\n        self.outputs = [layer.output for layer in self.convolution_part.layers[1:]]          # all layer outputs\n        self.functor = K.function([self.inp], self.outputs)\n\n        kernel_3x3 = tf.constant(np.array([\n        [[[1]], [[1]], [[1]]], \n        [[[1]], [[1]], [[1]]], \n        [[[1]], [[1]], [[1]]]\n        ]), tf.float32)\n\n        kernel_5x5 = tf.constant(np.array([\n                [[[1]], [[1]], [[1]], [[1]], [[1]]], \n                [[[1]], [[1]], [[1]], [[1]], [[1]]], \n                [[[1]], [[1]], [[1]], [[1]], [[1]]],\n                [[[1]], [[1]], [[1]], [[1]], [[1]]],\n                [[[1]], [[1]], [[1]], [[1]], [[1]]]\n        ]), tf.float32)\n\n        self.layers_kernels = {5: kernel_3x3, 4: kernel_3x3, 3: kernel_5x5, 2: kernel_5x5, 1: kernel_5x5}\n\n        self.layers_strides = {5: [1, 1, 1, 1], 4: [1, 2, 2, 1], 3: [1, 2, 2, 1], 2: [1, 2, 2, 1], 1: [1, 2, 2, 1]}\n\n        \n    def compute_visualisation_mask(self, img):\n        #from https://github.com/ermolenkodev/keras-salient-object-visualisation\n        \n        activations = self.functor([np.array([img])])\n        activations = [np.reshape(img, (1, img.shape[0], img.shape[1], img.shape[2]))] + activations\n        upscaled_activation = np.ones((3, 6))\n        for layer in [5, 4, 3, 2, 1]:\n            averaged_activation = np.mean(activations[layer], axis=3).squeeze(axis=0) * upscaled_activation\n            output_shape = (activations[layer - 1].shape[1], activations[layer - 1].shape[2])\n            x = tf.constant(\n                np.reshape(averaged_activation, (1,averaged_activation.shape[0],averaged_activation.shape[1],1)),\n                tf.float32\n            )\n            conv = tf.nn.conv2d_transpose(\n                x, self.layers_kernels[layer],\n                output_shape=(1,output_shape[0],output_shape[1], 1), \n                strides=self.layers_strides[layer], \n                padding='VALID'\n            )\n            with tf.Session() as session:\n                result = session.run(conv)\n            upscaled_activation = np.reshape(result, output_shape)\n        final_visualisation_mask = upscaled_activation\n        return (final_visualisation_mask - np.min(final_visualisation_mask))/(np.max(final_visualisation_mask) - np.min(final_visualisation_mask))\n\n    def draw_salient(self, img):\n        #from https://github.com/ermolenkodev/keras-salient-object-visualisation\n        alpha = 0.004\n        beta = 1.0 - alpha\n\n        salient_mask = self.compute_visualisation_mask(img)\n        salient_mask_stacked = np.dstack((salient_mask,salient_mask))\n        salient_mask_stacked = np.dstack((salient_mask_stacked,salient_mask))\n        blend = cv2.addWeighted(img.astype('float32'), alpha, salient_mask_stacked, beta, 0.0)\n        return blend\n\n    def shutdown(self):\n        pass\n        \n"""
donkeycar/parts/serial_controller.py,0,"b'#!/usr/bin/env python3\n""""""\nScrits to read signals from Arduino and convert into steering and throttle outputs\nArduino input signal range: 0 to 200\nOutput range: -1.00 to 1.00\n""""""\n\nimport serial\nimport time\n\nclass SerialController:\n    def __init__(self):\n        print(""Starting Serial Controller"")\n\n        self.angle = 0.0\n        self.throttle = 0.0\n        self.mode = \'user\'\n        self.recording = False\n        self.serial = serial.Serial(\'/dev/ttyS0\', 115200, timeout=1) #Serial port - laptop: \'COM3\', Arduino: \'/dev/ttyACM0\'\n\n\n    def update(self):\n        # delay on startup to avoid crashing\n        print(""Warming Serial Controller"")\n        time.sleep(3)\n\n        while True:\n            line = str(self.serial.readline().decode()).strip(\'\\n\').strip(\'\\r\')\n            output = line.split("", "")\n            if len(output) == 2:\n                if output[0].isnumeric() and output[1].isnumeric():\n                    self.angle = (float(output[0])-1500)/500\n                    self.throttle = (float(output[1])-1500)/500\n                    if self.throttle > 0.01:\n                        self.recording = True\n                        print(""Recording"")\n                    else:\n                        self.recording = False\n                    time.sleep(0.01)\n\n    def run(self, img_arr=None):\n        return self.run_threaded()\n\n    def run_threaded(self, img_arr=None):\n        #print(""Signal:"", self.angle, self.throttle)\n        return self.angle, self.throttle, self.mode, self.recording\n'"
donkeycar/parts/simulation.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nParts to try donkeycar without a physical car.\n""""""\n\nimport random\nimport numpy as np\n\n\nclass MovingSquareTelemetry:\n    """"""\n    Generator of cordinates of a bouncing moving square for simulations.\n    """"""\n    def __init__(self, max_velocity=29,\n                 x_min=10, x_max=150,\n                 y_min=10, y_max=110):\n\n        self.velocity = random.random() * max_velocity\n\n        self.x_min, self.x_max = x_min, x_max\n        self.y_min, self.y_max = y_min, y_max\n\n        self.x_direction = random.random() * 2 - 1\n        self.y_direction = random.random() * 2 - 1\n\n        self.x = random.random() * x_max\n        self.y = random.random() * y_max\n\n        self.tel = self.x, self.y\n\n    def run(self):\n        # move\n        self.x += self.x_direction * self.velocity\n        self.y += self.y_direction * self.velocity\n\n        # make square bounce off walls\n        if self.y < self.y_min or self.y > self.y_max:\n            self.y_direction *= -1\n        if self.x < self.x_min or self.x > self.x_max:\n            self.x_direction *= -1\n\n        return int(self.x), int(self.y)\n\n    def update(self):\n        self.tel = self.run()\n\n    def run_threaded(self):\n        return self.tel\n\n\nclass SquareBoxCamera:\n    """"""\n    Fake camera that returns an image with a square box.\n\n    This can be used to test if a learning algorithm can learn.\n    """"""\n\n    def __init__(self, resolution=(120, 160), box_size=4, color=(255, 0, 0)):\n        self.resolution = resolution\n        self.box_size = box_size\n        self.color = color\n\n    def run(self, x, y, box_size=None, color=None):\n        """"""\n        Create an image of a square box at a given coordinates.\n        """"""\n        radius = int((box_size or self.box_size)/2)\n        color = color or self.color\n        frame = np.zeros(shape=self.resolution + (3,))\n        frame[y - radius: y + radius,\n              x - radius: x + radius, :] = color\n        return frame'"
donkeycar/parts/sombrero.py,0,"b'class Sombrero:\n    \'\'\'\n    A pi hat developed by Adam Conway to manage power, pwm for a Donkeycar\n    This requires that GPIO 26 is brought low to enable the pwm out.\n    Because all GPIO modes have to be the same accross code, we use BOARD\n    mode, which is physical pin 37.\n    \'\'\'\n\n    def __init__(self):\n        try:\n            import RPi.GPIO as GPIO\n\n            GPIO.setmode(GPIO.BOARD)\n            GPIO.setup(37, GPIO.OUT)\n            GPIO.output(37, GPIO.LOW)\n            print(""sombrero enabled"")\n        except:\n            pass\n\n    def __del__(self):\n        try:\n            import RPi.GPIO as GPIO\n\n            GPIO.cleanup()\n            print(""sombrero disabled"")\n        except:\n            pass\n'"
donkeycar/parts/teensy.py,0,"b'from datetime import datetime\nimport donkeycar as dk\nimport re\nimport time\n\nclass TeensyRCin:\n    def __init__(self):\n        self.inSteering = 0.0\n        self.inThrottle = 0.0\n\n        self.sensor = dk.parts.actuator.Teensy(0)\n\n        TeensyRCin.LEFT_ANGLE = -1.0\n        TeensyRCin.RIGHT_ANGLE = 1.0\n        TeensyRCin.MIN_THROTTLE = -1.0\n        TeensyRCin.MAX_THROTTLE =  1.0\n\n        TeensyRCin.LEFT_PULSE = 496.0\n        TeensyRCin.RIGHT_PULSE = 242.0\n        TeensyRCin.MAX_PULSE = 496.0\n        TeensyRCin.MIN_PULSE = 242.0\n\n\n        self.on = True\n\n    def map_range(self, x, X_min, X_max, Y_min, Y_max):\n        \'\'\'\n        Linear mapping between two ranges of values\n        \'\'\'\n        X_range = X_max - X_min\n        Y_range = Y_max - Y_min\n        XY_ratio = X_range/Y_range\n\n        return ((x-X_min) / XY_ratio + Y_min)\n\n    def update(self):\n        rcin_pattern = re.compile(\'^I +([.0-9]+) +([.0-9]+).*$\')\n\n        while self.on:\n            start = datetime.now()\n\n            l = self.sensor.teensy_readline()\n\n            while l:\n                # print(""mw TeensyRCin line= "" + l.decode(\'utf-8\'))\n                m = rcin_pattern.match(l.decode(\'utf-8\'))\n\n                if m:\n                    i = float(m.group(1))\n                    if i == 0.0:\n                        self.inSteering = 0.0\n                    else:\n                        i = i / (1000.0 * 1000.0) # in seconds\n                        i *= self.sensor.frequency * 4096.0\n                        self.inSteering = self.map_range(i,\n                                                         TeensyRCin.LEFT_PULSE, TeensyRCin.RIGHT_PULSE,\n                                                         TeensyRCin.LEFT_ANGLE, TeensyRCin.RIGHT_ANGLE)\n\n                    k = float(m.group(2))\n                    if k == 0.0:\n                        self.inThrottle = 0.0\n                    else:\n                        k = k / (1000.0 * 1000.0) # in seconds\n                        k *= self.sensor.frequency * 4096.0\n                        self.inThrottle = self.map_range(k,\n                                                         TeensyRCin.MIN_PULSE, TeensyRCin.MAX_PULSE,\n                                                         TeensyRCin.MIN_THROTTLE, TeensyRCin.MAX_THROTTLE)\n\n                    # print(""matched %.1f  %.1f  %.1f  %.1f"" % (i, self.inSteering, k, self.inThrottle))\n                l = self.sensor.teensy_readline()\n\n            stop = datetime.now()\n            s = 0.01 - (stop - start).total_seconds()\n            if s > 0:\n                time.sleep(s)\n\n    def run_threaded(self):\n        return self.inSteering, self.inThrottle\n\n    def shutdown(self):\n        # indicate that the thread should be stopped\n        self.on = False\n        print(\'stopping TeensyRCin\')\n        time.sleep(.5)\n\n'"
donkeycar/parts/tensorrt.py,0,"b""from collections import namedtuple\nfrom donkeycar.parts.keras import KerasPilot\nimport json\nimport numpy as np\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pathlib import Path\nimport tensorflow as tf\nimport tensorrt as trt\n\nHostDeviceMemory = namedtuple('HostDeviceMemory', 'host_memory device_memory')\n\nclass TensorRTLinear(KerasPilot):\n    '''\n    Uses TensorRT to do the inference.\n    '''\n    def __init__(self, cfg, *args, **kwargs):\n        super(TensorRTLinear, self).__init__(*args, **kwargs)\n        self.logger = trt.Logger(trt.Logger.WARNING)\n        self.cfg = cfg\n        self.engine = None\n        self.inputs = None\n        self.outputs = None\n        self.bindings = None\n        self.stream = None\n\n    def compile(self):\n        print('Nothing to compile')\n\n    def load(self, model_path):\n        uff_model = Path(model_path)\n        metadata_path = Path('%s/%s.metadata' % (uff_model.parent.as_posix(), uff_model.stem))\n        with open(metadata_path.as_posix(), 'r') as metadata, trt.Builder(self.logger) as builder, builder.create_network() as network, trt.UffParser() as parser:\n            \n            # Without this max_workspace_size setting, I was getting:\n            # Building CUDA Engine\n            # [TensorRT] ERROR: Internal error: could not find any implementation for node 2-layer MLP, try increasing the workspace size with IBuilder::setMaxWorkspaceSize()\n            # [TensorRT] ERROR: ../builder/tacticOptimizer.cpp (1230) - OutOfMemory Error in computeCosts: 0\n            builder.max_workspace_size = 1 << 20 #common.GiB(1)\n            builder.max_batch_size = 1\n\n            metadata = json.loads(metadata.read())\n            # Configure inputs and outputs\n            print('Configuring I/O')\n            input_names = metadata['input_names']\n            output_names = metadata['output_names']\n            for name in input_names:\n                parser.register_input(name, (self.cfg.TARGET_D, self.cfg.TARGET_H, self.cfg.TARGET_W))\n\n            for name in output_names:\n                parser.register_output(name)\n            # Parse network\n            print('Parsing TensorRT Network')\n            parser.parse(uff_model.as_posix(), network)\n            print('Building CUDA Engine')\n            self.engine = builder.build_cuda_engine(network)\n            # Allocate buffers\n            print('Allocating Buffers')\n            self.inputs, self.outputs, self.bindings, self.stream = TensorRTLinear.allocate_buffers(self.engine)\n            print('Ready')\n\n    def run(self, image):\n        # Channel first image format\n        image = image.transpose((2,0,1))\n        # Flatten it to a 1D array.\n        image = image.ravel()\n        # The first input is the image. Copy to host memory.\n        image_input = self.inputs[0] \n        np.copyto(image_input.host_memory, image)\n        with self.engine.create_execution_context() as context:\n            [throttle, steering] = TensorRTLinear.infer(context=context, bindings=self.bindings, inputs=self.inputs, outputs=self.outputs, stream=self.stream)\n            return steering[0], throttle[0]\n\n    @classmethod\n    def allocate_buffers(cls, engine):\n        inputs = []\n        outputs = []\n        bindings = []\n        stream = cuda.Stream()\n        for binding in engine:\n            size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n            dtype = trt.nptype(engine.get_binding_dtype(binding))\n            # Allocate host and device buffers\n            host_memory = cuda.pagelocked_empty(size, dtype)\n            device_memory = cuda.mem_alloc(host_memory.nbytes)\n            bindings.append(int(device_memory))\n            if engine.binding_is_input(binding):\n                inputs.append(HostDeviceMemory(host_memory, device_memory))\n            else:\n                outputs.append(HostDeviceMemory(host_memory, device_memory))\n\n        return inputs, outputs, bindings, stream\n\n    @classmethod\n    def infer(cls, context, bindings, inputs, outputs, stream, batch_size=1):\n        # Transfer input data to the GPU.\n        [cuda.memcpy_htod_async(inp.device_memory, inp.host_memory, stream) for inp in inputs]\n        # Run inference.\n        context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n        # Transfer predictions back from the GPU.\n        [cuda.memcpy_dtoh_async(out.host_memory, out.device_memory, stream) for out in outputs]\n        # Synchronize the stream\n        stream.synchronize()\n        # Return only the host outputs.\n        return [out.host_memory for out in outputs]\n"""
donkeycar/parts/tflite.py,12,"b'import tensorflow as tf\n\ndef keras_model_to_tflite(in_filename, out_filename, data_gen=None):\n    verStr = tf.__version__\n    if verStr.find(\'1.1\')  == 0: # found MAJOR.MINOR match for version 1.1x.x\n        converter = tf.lite.TFLiteConverter.from_keras_model_file(in_filename)\n    if verStr.find(\'2.\')  == 0: # found MAJOR.MINOR match for version 2.x.x\n        new_model= tf.keras.models.load_model(in_filename) #filepath=""keras_model.h5"")\n        converter = tf.lite.TFLiteConverter.from_keras_model(new_model)\n    if data_gen is not None:\n        #when we have a data_gen that is the trigger to use it to \n        #create integer weights and calibrate them. Warning: this model will\n        #no longer run with the standard tflite engine. That uses only float.\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.representative_dataset = data_gen\n        try:\n            converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        except:\n            pass\n        try:\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        except:\n            pass\n        converter.inference_input_type = tf.uint8\n        converter.inference_output_type = tf.uint8\n        print(""----- using data generator to create int optimized weights for Coral TPU -----"")\n    tflite_model = converter.convert()\n    open(out_filename, ""wb"").write(tflite_model)\n\ndef keras_session_to_tflite(model, out_filename):\n    inputs = model.inputs\n    outputs = model.outputs\n    with tf.keras.backend.get_session() as sess:        \n        converter = tf.lite.TFLiteConverter.from_session(sess, inputs, outputs)\n        tflite_model = converter.convert()\n        open(out_filename, ""wb"").write(tflite_model)\n\n\nclass TFLitePilot(object):\n    \'\'\'\n    Base class for TFlite models that will provide steering and throttle to guide a car.\n    \'\'\'\n    def __init__(self):\n        self.model = None\n \n    \n    def load(self, model_path):\n        # Load TFLite model and allocate tensors.\n        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n        self.interpreter.allocate_tensors()\n\n        # Get input and output tensors.\n        self.input_details = self.interpreter.get_input_details()\n        self.output_details = self.interpreter.get_output_details()\n\n        # Get Input shape\n        self.input_shape = self.input_details[0][\'shape\']\n\n    \n    def run(self, image):\n        input_data = image.reshape(self.input_shape).astype(\'float32\') \n\n        self.interpreter.set_tensor(self.input_details[0][\'index\'], input_data)\n        self.interpreter.invoke()\n\n        steering = 0.0\n        throttle = 0.0\n        outputs = []\n\n        for tensor in self.output_details:\n            output_data = self.interpreter.get_tensor(tensor[\'index\'])\n            outputs.append(output_data[0][0])\n\n        if len(outputs) > 1:\n            steering = outputs[0]\n            throttle = outputs[1]\n\n        return steering, throttle\n\n\n'"
donkeycar/parts/throttle_filter.py,0,"b""\nclass ThrottleFilter(object):\n    '''\n    allow reverse to trigger automatic reverse throttle\n    '''\n\n    def __init__(self):\n        self.reverse_triggered = False\n        self.last_throttle = 0.0\n\n    def run(self, throttle_in):\n        throttle_out = throttle_in\n\n        if throttle_out < 0.0:\n            if not self.reverse_triggered and self.last_throttle < 0.0:\n                throttle_out = 0.0\n                self.reverse_triggered = True                \n        else:\n            self.reverse_triggered = False   \n\n        self.last_throttle = throttle_out\n        return throttle_out\n\n    def shutdown(self):\n        pass\n"""
donkeycar/parts/transform.py,0,"b'# -*- coding: utf-8 -*-\n\nimport time\n\nclass Lambda:\n    """"""\n    Wraps a function into a donkey part.\n    """"""\n    def __init__(self, f):\n        """"""\n        Accepts the function to use.\n        """"""\n        self.f = f\n        \n    def run(self, *args, **kwargs):\n        return self.f(*args, **kwargs)\n    \n    def shutdown(self):\n        return\n\nclass TriggeredCallback:\n    def __init__(self, args, func_cb):\n        self.args = args\n        self.func_cb = func_cb\n\n    def run(self, trigger):\n        if trigger:\n            self.func_cb(self.args)\n\n    def shutdown(self):\n        return\n\nclass DelayedTrigger:\n    def __init__(self, delay):\n        self.ticks = 0\n        self.delay = delay\n\n    def run(self, trigger):\n        if self.ticks > 0:\n            self.ticks -= 1\n            if self.ticks == 0:\n                return True\n\n        if trigger:\n            self.ticks = self.delay\n\n        return False\n\n    def shutdown(self):\n        return\n\n\nclass PIDController:\n    """""" Performs a PID computation and returns a control value.\n        This is based on the elapsed time (dt) and the current value of the process variable\n        (i.e. the thing we\'re measuring and trying to change).\n        https://github.com/chrisspen/pid_controller/blob/master/pid_controller/pid.py\n    """"""\n\n    def __init__(self, p=0, i=0, d=0, debug=False):\n\n        # initialize gains\n        self.Kp = p\n        self.Ki = i\n        self.Kd = d\n\n        # The value the controller is trying to get the system to achieve.\n        self.target = 0\n\n        # initialize delta t variables\n        self.prev_tm = time.time()\n        self.prev_err = 0\n        self.error = None\n        self.totalError = 0\n\n        # initialize the output\n        self.alpha = 0\n\n        # debug flag (set to True for console output)\n        self.debug = debug\n\n    def run(self, err):\n        curr_tm = time.time()\n\n        self.difError = err - self.prev_err\n\n        # Calculate time differential.\n        dt = curr_tm - self.prev_tm\n\n        # Initialize output variable.\n        curr_alpha = 0\n\n        # Add proportional component.\n        curr_alpha += -self.Kp * err\n\n        # Add integral component.\n        curr_alpha += -self.Ki * (self.totalError * dt)\n\n        # Add differential component (avoiding divide-by-zero).\n        if dt > 0:\n            curr_alpha += -self.Kd * ((self.difError) / float(dt))\n\n        # Maintain memory for next loop.\n        self.prev_tm = curr_tm\n        self.prev_err = err\n        self.totalError += err\n\n        # Update the output\n        self.alpha = curr_alpha\n\n        if (self.debug):\n            print(\'PID err value:\', round(err, 4))\n            print(\'PID output:\', round(curr_alpha, 4))\n\n        return curr_alpha\n\n\ndef twiddle(evaluator, tol=0.001, params=3, error_cmp=None, initial_guess=None):\n    """"""\n    A coordinate descent parameter tuning algorithm.\n    https://github.com/chrisspen/pid_controller/blob/master/pid_controller/pid.py\n    \n    https://en.wikipedia.org/wiki/Coordinate_descent\n    \n    Params:\n    \n        evaluator := callable that will be passed a series of number parameters, which will return\n            an error measure\n            \n        tol := tolerance threshold, the smaller the value, the greater the tuning\n        \n        params := the number of parameters to tune\n        \n        error_cmp := a callable that takes two error measures (the current and last best)\n            and returns true if the first is less than the second\n            \n        initial_guess := parameters to begin tuning with\n    """"""\n\n    def _error_cmp(a, b):\n        # Returns true if a is closer to zero than b.\n        return abs(a) < abs(b)\n        \n    if error_cmp is None:\n        error_cmp = _error_cmp\n\n    if initial_guess is None:\n        p = [0]*params\n    else:\n        p = list(initial_guess)\n    dp = [1]*params\n    best_err = evaluator(*p)\n    steps = 0\n    while sum(dp) > tol:\n        steps += 1\n        print(\'steps:\', steps, \'tol:\', tol, \'best error:\', best_err)\n        for i, _ in enumerate(p):\n            \n            # first try to increase param\n            p[i] += dp[i]\n            err = evaluator(*p)\n            \n            if error_cmp(err, best_err):\n                # Increasing param reduced error, so record and continue to increase dp range.\n                best_err = err\n                dp[i] *= 1.1\n            else:\n                # Otherwise, increased error, so undo and try decreasing dp\n                p[i] -= 2.*dp[i]\n                err = evaluator(*p)\n                \n                if error_cmp(err, best_err):\n                    # Decreasing param reduced error, so record and continue to increase dp range.\n                    best_err = err\n                    dp[i] *= 1.1\n                    \n                else:\n                    # Otherwise, reset param and reduce dp range.\n                    p[i] += dp[i]\n                    dp[i] *= 0.9\n                \n    return p\n'"
donkeycar/templates/arduino_drive.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey 2 car\nShows how to use an implement the drive-loop for a car with Arduino as its\ndrive train. Further it shows how to control the car with a joystick for the\nsake of providing a functional demo.\n\nUsage:\n    manage.py (drive)\n\nOptions:\n    -h --help          Show this screen.\n""""""\nimport os\nimport time\n\nfrom docopt import docopt\n\nimport donkeycar as dk\nfrom donkeycar.parts.actuator import ArduinoFirmata, ArdPWMSteering, ArdPWMThrottle\nfrom donkeycar.parts.controller import get_js_controller\n\n\ndef drive(cfg):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n\n    #Initialize car\n    V = dk.vehicle.Vehicle()\n    ctr = get_js_controller(cfg)\n    V.add(ctr,\n          outputs=[\'user/angle\', \'user/throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n\n    #Drive train setup\n    arduino_controller = ArduinoFirmata(\n        servo_pin=cfg.STEERING_ARDUINO_PIN, esc_pin=cfg.THROTTLE_ARDUINO_PIN)\n    steering = ArdPWMSteering(controller=arduino_controller,\n                              left_pulse=cfg.STEERING_ARDUINO_LEFT_PWM,\n                              right_pulse=cfg.STEERING_ARDUINO_RIGHT_PWM)\n\n    throttle = ArdPWMThrottle(controller=arduino_controller,\n                              max_pulse=cfg.THROTTLE_ARDUINO_FORWARD_PWM,\n                              zero_pulse=cfg.THROTTLE_ARDUINO_STOPPED_PWM,\n                              min_pulse=cfg.THROTTLE_ARDUINO_REVERSE_PWM)\n\n    V.add(steering, inputs=[\'user/angle\'])\n    V.add(throttle, inputs=[\'user/throttle\'])\n\n    #run the vehicle\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ,\n            max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n\n    if args[\'drive\']:\n        drive(cfg)\n'"
donkeycar/templates/basic_web.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey 2 car\n\nUsage:\n    manage.py (drive) [--model=<model>] [--type=(linear|categorical|rnn|imu|behavior|3d|localizer|latent)]\n    manage.py (train) [--tub=<tub1,tub2,..tubn>] [--file=<file> ...] (--model=<model>) [--transfer=<model>] [--type=(linear|categorical|rnn|imu|behavior|3d|localizer)] [--continuous] [--aug]\n\n\nOptions:\n    -h --help          Show this screen.    \n    -f --file=<file>   A text file containing paths to tub files, one per line. Option may be used more than once.\n""""""\nimport os\nimport time\n\nfrom docopt import docopt\nimport numpy as np\n\nimport donkeycar as dk\nfrom donkeycar.parts.datastore import TubHandler\nfrom donkeycar.parts.controller import LocalWebController\nfrom donkeycar.parts.camera import PiCamera\nfrom donkeycar.utils import *\n\n\ndef drive(cfg, model_path=None, model_type=None):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n\n    if model_type is None:\n        model_type = cfg.DEFAULT_MODEL_TYPE\n    \n    #Initialize car\n    V = dk.vehicle.Vehicle()\n    \n    cam = PiCamera(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH)\n    V.add(cam, outputs=[\'cam/image_array\'], threaded=True)\n        \n   \n    V.add(LocalWebController(), \n          inputs=[\'cam/image_array\'],\n          outputs=[\'user/angle\', \'user/throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n\n   \n    #See if we should even run the pilot module. \n    #This is only needed because the part run_condition only accepts boolean\n    class PilotCondition:\n        def run(self, mode):\n            if mode == \'user\':\n                return False\n            else:\n                return True       \n\n    V.add(PilotCondition(), inputs=[\'user/mode\'], outputs=[\'run_pilot\'])\n    \n    #Sombrero\n    if cfg.HAVE_SOMBRERO:\n        from donkeycar.parts.sombrero import Sombrero\n        s = Sombrero()\n\n    class ImgPrecondition():\n        \'\'\'\n        precondition camera image for inference\n        \'\'\'\n        def __init__(self, cfg):\n            self.cfg = cfg\n\n        def run(self, img_arr):\n            return normalize_and_crop(img_arr, self.cfg)\n\n    V.add(ImgPrecondition(cfg),\n        inputs=[\'cam/image_array\'],\n        outputs=[\'cam/normalized/cropped\'],\n        run_condition=\'run_pilot\')\n\n    inputs=[\'cam/normalized/cropped\']\n\n    def load_model(kl, model_path):\n        start = time.time()\n        try:\n            print(\'loading model\', model_path)\n            kl.load(model_path, compile=False)\n            print(\'finished loading in %s sec.\' % (str(time.time() - start)) )\n        except Exception as e:\n            print(e)\n            print(\'ERR>> problems loading model\', model_path)\n\n    def load_weights(kl, weights_path):\n        start = time.time()\n        try:\n            print(\'loading model weights\', weights_path)\n            kl.model.load_weights(weights_path)\n            print(\'finished loading in %s sec.\' % (str(time.time() - start)) )\n        except Exception as e:\n            print(e)\n            print(\'ERR>> problems loading weights\', weights_path)\n\n    def load_model_json(kl, json_fnm):\n        start = time.time()\n        print(\'loading model json\', json_fnm)\n        from tensorflow.python import keras\n        try:\n            with open(json_fnm, \'r\') as handle:\n                contents = handle.read()\n                kl.model = keras.models.model_from_json(contents)\n            print(\'finished loading json in %s sec.\' % (str(time.time() - start)) )\n        except Exception as e:\n            print(e)\n            print(""ERR>> problems loading model json"", json_fnm)\n\n    if model_path:\n        #When we have a model, first create an appropriate Keras part\n        kl = dk.utils.get_model_by_type(model_type, cfg)\n\n        if \'.h5\' in model_path:\n            #when we have a .h5 extension\n            #load everything from the model file\n            load_model(kl, model_path)\n\n        elif \'.json\' in model_path:\n            #when we have a .json extension\n            #load the model from there and look for a matching\n            #.wts file with just weights\n            load_model_json(kl, model_path)\n            weights_path = model_path.replace(\'.json\', \'.weights\')\n            load_weights(kl, weights_path)\n\n        else:\n            print(""ERR>> Unknown extension type on model file!!"")\n            return\n\n        outputs=[\'pilot/angle\', \'pilot/throttle\']\n   \n        V.add(kl, inputs=inputs, \n            outputs=outputs,\n            run_condition=\'run_pilot\')\n    \n    #Choose what inputs should change the car.\n    class DriveMode:\n        def run(self, mode, \n                    user_angle, user_throttle,\n                    pilot_angle, pilot_throttle):\n            if mode == \'user\': \n                return user_angle, user_throttle\n            \n            elif mode == \'local_angle\':\n                return pilot_angle if pilot_angle else 0.0, user_throttle\n            \n            else: \n                return pilot_angle if pilot_angle else 0.0, pilot_throttle * cfg.AI_THROTTLE_MULT if pilot_throttle else 0.0\n        \n    V.add(DriveMode(), \n          inputs=[\'user/mode\', \'user/angle\', \'user/throttle\',\n                  \'pilot/angle\', \'pilot/throttle\'], \n          outputs=[\'angle\', \'throttle\'])\n\n    #Drive train setup\n\n    from donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\n\n    steering_controller = PCA9685(cfg.STEERING_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n    steering = PWMSteering(controller=steering_controller,\n                                    left_pulse=cfg.STEERING_LEFT_PWM, \n                                    right_pulse=cfg.STEERING_RIGHT_PWM)\n    \n    throttle_controller = PCA9685(cfg.THROTTLE_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n    throttle = PWMThrottle(controller=throttle_controller,\n                                    max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                    zero_pulse=cfg.THROTTLE_STOPPED_PWM, \n                                    min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\n    V.add(steering, inputs=[\'angle\'])\n    V.add(throttle, inputs=[\'throttle\'])\n    \n    #add tub to save data\n\n    inputs=[\'cam/image_array\',\n            \'user/angle\', \'user/throttle\', \n            \'user/mode\']\n\n    types=[\'image_array\',\n           \'float\', \'float\',\n           \'str\']\n\n    th = TubHandler(path=cfg.DATA_PATH)\n    tub = th.new_tub_writer(inputs=inputs, types=types)\n    V.add(tub, inputs=inputs, outputs=[""tub/num_records""], run_condition=\'recording\')\n\n    print(""You can now go to <your pis hostname.local>:8887 to drive your car."")\n\n    #run the vehicle for 20 seconds\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ, \n            max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n    \n    if args[\'drive\']:\n        model_type = args[\'--type\']\n        drive(cfg, model_path = args[\'--model\'], model_type=model_type)\n    \n    if args[\'train\']:\n        from train import multi_train, preprocessFileList\n        \n        tub = args[\'--tub\']\n        model = args[\'--model\']\n        transfer = args[\'--transfer\']\n        model_type = args[\'--type\']\n        continuous = args[\'--continuous\']\n        aug = args[\'--aug\']     \n\n        dirs = preprocessFileList( args[\'--file\'] )\n        if tub is not None:\n            tub_paths = [os.path.expanduser(n) for n in tub.split(\',\')]\n            dirs.extend( tub_paths )\n\n        multi_train(cfg, dirs, model, transfer, model_type, continuous, aug)\n\n'"
donkeycar/templates/calibrate.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey 2 car\n\nUsage:\n    manage.py (drive)\n\n\nOptions:\n    -h --help          Show this screen.\n""""""\nimport os\nimport time\n\nfrom docopt import docopt\n\nimport donkeycar as dk\n\n#import parts\nfrom donkeycar.parts.controller import LocalWebController, \\\n    JoystickController, WebFpv\nfrom donkeycar.parts.throttle_filter import ThrottleFilter\nfrom donkeycar.utils import *\n\nfrom socket import gethostname\n\ndef drive(cfg ):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n\n    #Initialize car\n    V = dk.vehicle.Vehicle()\n\n    ctr = LocalWebController()\n    V.add(ctr,\n          inputs=[\'cam/image_array\', \'tub/num_records\'],\n          outputs=[\'angle\', \'throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n\n    #this throttle filter will allow one tap back for esc reverse\n    th_filter = ThrottleFilter()\n    V.add(th_filter, inputs=[\'throttle\'], outputs=[\'throttle\'])\n\n    drive_train = None\n\n    #Drive train setup\n    if cfg.DONKEY_GYM or cfg.DRIVE_TRAIN_TYPE == ""MOCK"":\n        pass\n\n    elif cfg.DRIVE_TRAIN_TYPE == ""SERVO_ESC"":\n\n        from donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\n\n        steering_controller = PCA9685(cfg.STEERING_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n        steering = PWMSteering(controller=steering_controller,\n                                        left_pulse=cfg.STEERING_LEFT_PWM,\n                                        right_pulse=cfg.STEERING_RIGHT_PWM)\n\n        throttle_controller = PCA9685(cfg.THROTTLE_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n        throttle = PWMThrottle(controller=throttle_controller,\n                                        max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                        zero_pulse=cfg.THROTTLE_STOPPED_PWM,\n                                        min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\n        drive_train = dict()\n        drive_train[\'steering\'] = steering\n        drive_train[\'throttle\'] = throttle\n\n        V.add(steering, inputs=[\'angle\'], threaded=True)\n        V.add(throttle, inputs=[\'throttle\'], threaded=True)\n\n    elif cfg.DRIVE_TRAIN_TYPE == ""MM1"":\n        from donkeycar.parts.robohat import RoboHATDriver\n        drive_train = RoboHATDriver(cfg)\n        V.add(drive_train, inputs=[\'angle\', \'throttle\'])\n\n\n    ctr.drive_train = drive_train\n    ctr.drive_train_type = cfg.DRIVE_TRAIN_TYPE\n    \n    class ShowHowTo:\n        def __init__(self):\n            print(f""Go to http://{gethostname()}.local:8887/calibrate to calibrate "")\n            \n        def run(self):\n            pass\n        \n    V.add(ShowHowTo())\n\n    #run the vehicle for 20 seconds\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ,\n            max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n\n    if args[\'drive\']:\n        drive(cfg)\n'"
donkeycar/templates/cfg_arduino_drive.py,0,"b'"""""" \nCAR CONFIG \n\nThis file is read by your car application\'s manage.py script to change the car\nperformance. \n\nEXMAPLE\n-----------\nimport dk\ncfg = dk.load_config(config_path=\'~/mycar/config.py\')\nprint(cfg.CAMERA_RESOLUTION)\n\n""""""\n\n\nimport os\n\n#PATHS\nCAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(CAR_PATH, \'data\')\nMODELS_PATH = os.path.join(CAR_PATH, \'models\')\n\n#VEHICLE\nDRIVE_LOOP_HZ = 20\nMAX_LOOPS = None\n\n#STEERING\nSTEERING_ARDUINO_PIN = 6\nSTEERING_ARDUINO_LEFT_PWM = 120\nSTEERING_ARDUINO_RIGHT_PWM = 40\n\n#THROTTLE\nTHROTTLE_ARDUINO_PIN = 5\nTHROTTLE_ARDUINO_FORWARD_PWM = 105\nTHROTTLE_ARDUINO_STOPPED_PWM = 90\nTHROTTLE_ARDUINO_REVERSE_PWM = 75\n\n#JOYSTICK\nUSE_JOYSTICK_AS_DEFAULT = False     #when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 0.8         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don\'t need the full speed available.\nJOYSTICK_STEERING_SCALE = 1.0       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nCONTROLLER_TYPE=\'F710\'               #(ps3|ps4|xbox|nimbus|wiiu|F710|rc3)\nUSE_NETWORKED_JS = False            #should we listen for remote joystick control over the network?\nNETWORK_JS_SERVER_IP = ""192.168.0.1""#when listening for network joystick control, which ip is serving this information\nJOYSTICK_DEADZONE = 0.0             # when non zero, this is the smallest throttle before recording triggered.\nJOYSTICK_THROTTLE_DIR = -1.0        # use -1.0 to flip forward/backward, use 1.0 to use joystick\'s natural forward/backward\nUSE_FPV = False                     # send camera data to FPV webserver\n'"
donkeycar/templates/cfg_basic_web.py,0,"b'"""""" \nCAR CONFIG \n\nThis file is read by your car application\'s manage.py script to change the car\nperformance. \n\nEXMAPLE\n-----------\nimport dk\ncfg = dk.load_config(config_path=\'~/mycar/config.py\')\nprint(cfg.CAMERA_RESOLUTION)\n\n""""""\n\n\nimport os\n\n#PATHS\nCAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(CAR_PATH, \'data\')\nMODELS_PATH = os.path.join(CAR_PATH, \'models\')\n\n#VEHICLE\nDRIVE_LOOP_HZ = 20\nMAX_LOOPS = 100000\n\n#CAMERA\nIMAGE_W = 160\nIMAGE_H = 120\nIMAGE_DEPTH = 3         # default RGB=3, make 1 for mono\n\n#9865, over rides only if needed, ie. TX2..\nPCA9685_I2C_ADDR = 0x40\nPCA9685_I2C_BUSNUM = None\n\n#STEERING\nSTEERING_CHANNEL = 1\nSTEERING_LEFT_PWM = 460\nSTEERING_RIGHT_PWM = 290\n\n#THROTTLE\nTHROTTLE_CHANNEL = 0\nTHROTTLE_FORWARD_PWM = 500\nTHROTTLE_STOPPED_PWM = 370\nTHROTTLE_REVERSE_PWM = 220\n\n#TRAINING\nDEFAULT_MODEL_TYPE = \'linear\' #(linear|categorical|rnn|imu|behavior|3d|localizer|latent)\nBATCH_SIZE = 128\nTRAIN_TEST_SPLIT = 0.8\nMAX_EPOCHS = 100\nSHOW_PLOT = True\nVERBOSE_TRAIN = True\nUSE_EARLY_STOP = True\nEARLY_STOP_PATIENCE = 5\nMIN_DELTA = .0005\nPRINT_MODEL_SUMMARY = True      #print layers and weights to stdout\nOPTIMIZER = None                #adam, sgd, rmsprop, etc.. None accepts default\nLEARNING_RATE = 0.001           #only used when OPTIMIZER specified\nLEARNING_RATE_DECAY = 0.0       #only used when OPTIMIZER specified\nCACHE_IMAGES = True             #keep images in memory. will speed succesive epochs, but crater if not enough mem.\nPRUNE_CNN = False\nPRUNE_PERCENT_TARGET = 75 # The desired percentage of pruning.\nPRUNE_PERCENT_PER_ITERATION = 20 # Percenge of pruning that is perform per iteration.\nPRUNE_VAL_LOSS_DEGRADATION_LIMIT = 0.2 # The max amout of validation loss that is permitted during pruning.\nPRUNE_EVAL_PERCENT_OF_DATASET = .05  # percent of dataset used to perform evaluation of model.\n\n# Region of interst cropping\n# only supported in Categorical and Linear models.\nROI_CROP_TOP = 0\nROI_CROP_BOTTOM = 0\n\n#model transfer options\nFREEZE_LAYERS = False\nNUM_LAST_LAYERS_TO_TRAIN = 7\n\n#For the categorical model, this limits the upper bound of the learned throttle\n#it\'s very IMPORTANT that this value is matched from the training PC config.py and the robot.py\n#and ideally wouldn\'t change once set.\nMODEL_CATEGORICAL_MAX_THROTTLE_RANGE = 0.5\n\n#RNN or 3D\nSEQUENCE_LENGTH = 3\n\n#SOMBRERO\nHAVE_SOMBRERO = False\n\n#RECORD OPTIONS\nRECORD_DURING_AI = False\n'"
donkeycar/templates/cfg_complete.py,0,"b'""""""\nCAR CONFIG\n\nThis file is read by your car application\'s manage.py script to change the car\nperformance.\n\nEXMAPLE\n-----------\nimport dk\ncfg = dk.load_config(config_path=\'~/mycar/config.py\')\nprint(cfg.CAMERA_RESOLUTION)\n\n""""""\n\n\nimport os\n\n#PATHS\nCAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(CAR_PATH, \'data\')\nMODELS_PATH = os.path.join(CAR_PATH, \'models\')\n\n#VEHICLE\nDRIVE_LOOP_HZ = 20      # the vehicle loop will pause if faster than this speed.\nMAX_LOOPS = None        # the vehicle loop can abort after this many iterations, when given a positive integer.\n\n#CAMERA\nCAMERA_TYPE = ""PICAM""   # (PICAM|WEBCAM|CVCAM|CSIC|V4L|D435|MOCK|IMAGE_LIST)\nIMAGE_W = 160\nIMAGE_H = 120\nIMAGE_DEPTH = 3         # default RGB=3, make 1 for mono\nCAMERA_FRAMERATE = DRIVE_LOOP_HZ\nCAMERA_VFLIP = False\nCAMERA_HFLIP = False\n# For CSIC camera - If the camera is mounted in a rotated position, changing the below parameter will correct the output frame orientation\nCSIC_CAM_GSTREAMER_FLIP_PARM = 0 # (0 => none , 4 => Flip horizontally, 6 => Flip vertically)\n\n# For IMAGE_LIST camera\n# PATH_MASK = ""~/mycar/data/tub_1_20-03-12/*.jpg""\n\n#9865, over rides only if needed, ie. TX2..\nPCA9685_I2C_ADDR = 0x40     #I2C address, use i2cdetect to validate this number\nPCA9685_I2C_BUSNUM = None   #None will auto detect, which is fine on the pi. But other platforms should specify the bus num.\n\n#SSD1306_128_32\nUSE_SSD1306_128_32 = False    # Enable the SSD_1306 OLED Display\nSSD1306_128_32_I2C_BUSNUM = 1 # I2C bus number\n\n#DRIVETRAIN\n#These options specify which chasis and motor setup you are using. Most are using SERVO_ESC.\n#DC_STEER_THROTTLE uses HBridge pwm to control one steering dc motor, and one drive wheel motor\n#DC_TWO_WHEEL uses HBridge pwm to control two drive motors, one on the left, and one on the right.\n#SERVO_HBRIDGE_PWM use ServoBlaster to output pwm control from the PiZero directly to control steering, and HBridge for a drive motor.\n#PIGPIO_PWM uses Raspberrys internal PWM\nDRIVE_TRAIN_TYPE = ""SERVO_ESC"" # SERVO_ESC|DC_STEER_THROTTLE|DC_TWO_WHEEL|SERVO_HBRIDGE_PWM|PIGPIO_PWM|MM1|MOCK\n\n#STEERING\nSTEERING_CHANNEL = 1            #channel on the 9685 pwm board 0-15\nSTEERING_LEFT_PWM = 460         #pwm value for full left steering\nSTEERING_RIGHT_PWM = 290        #pwm value for full right steering\n\n#STEERING FOR PIGPIO_PWM\nSTEERING_PWM_PIN = 13           #Pin numbering according to Broadcom numbers\nSTEERING_PWM_FREQ = 50          #Frequency for PWM\nSTEERING_PWM_INVERTED = False   #If PWM needs to be inverted\n\n#THROTTLE\nTHROTTLE_CHANNEL = 0            #channel on the 9685 pwm board 0-15\nTHROTTLE_FORWARD_PWM = 500      #pwm value for max forward throttle\nTHROTTLE_STOPPED_PWM = 370      #pwm value for no movement\nTHROTTLE_REVERSE_PWM = 220      #pwm value for max reverse throttle\n\n#THROTTLE FOR PIGPIO_PWM\nTHROTTLE_PWM_PIN = 18           #Pin numbering according to Broadcom numbers\nTHROTTLE_PWM_FREQ = 50          #Frequency for PWM\nTHROTTLE_PWM_INVERTED = False   #If PWM needs to be inverted\n\n#DC_STEER_THROTTLE with one motor as steering, one as drive\n#these GPIO pinouts are only used for the DRIVE_TRAIN_TYPE=DC_STEER_THROTTLE\nHBRIDGE_PIN_LEFT = 18\nHBRIDGE_PIN_RIGHT = 16\nHBRIDGE_PIN_FWD = 15\nHBRIDGE_PIN_BWD = 13\n\n#DC_TWO_WHEEL - with two wheels as drive, left and right.\n#these GPIO pinouts are only used for the DRIVE_TRAIN_TYPE=DC_TWO_WHEEL\nHBRIDGE_PIN_LEFT_FWD = 18\nHBRIDGE_PIN_LEFT_BWD = 16\nHBRIDGE_PIN_RIGHT_FWD = 15\nHBRIDGE_PIN_RIGHT_BWD = 13\n\n\n#TRAINING\n#The DEFAULT_MODEL_TYPE will choose which model will be created at training time. This chooses\n#between different neural network designs. You can override this setting by passing the command\n#line parameter --type to the python manage.py train and drive commands.\nDEFAULT_MODEL_TYPE = \'linear\'   #(linear|categorical|rnn|imu|behavior|3d|localizer|latent)\nBATCH_SIZE = 128                #how many records to use when doing one pass of gradient decent. Use a smaller number if your gpu is running out of memory.\nTRAIN_TEST_SPLIT = 0.8          #what percent of records to use for training. the remaining used for validation.\nMAX_EPOCHS = 100                #how many times to visit all records of your data\nSHOW_PLOT = True                #would you like to see a pop up display of final loss?\nVERBOSE_TRAIN = True             #would you like to see a progress bar with text during training?\nUSE_EARLY_STOP = True           #would you like to stop the training if we see it\'s not improving fit?\nEARLY_STOP_PATIENCE = 5         #how many epochs to wait before no improvement\nMIN_DELTA = .0005               #early stop will want this much loss change before calling it improved.\nPRINT_MODEL_SUMMARY = True      #print layers and weights to stdout\nOPTIMIZER = None                #adam, sgd, rmsprop, etc.. None accepts default\nLEARNING_RATE = 0.001           #only used when OPTIMIZER specified\nLEARNING_RATE_DECAY = 0.0       #only used when OPTIMIZER specified\nSEND_BEST_MODEL_TO_PI = False   #change to true to automatically send best model during training\nCACHE_IMAGES = True             #keep images in memory. will speed succesive epochs, but crater if not enough mem.\n\nPRUNE_CNN = False               #This will remove weights from your model. The primary goal is to increase performance.\nPRUNE_PERCENT_TARGET = 75       # The desired percentage of pruning.\nPRUNE_PERCENT_PER_ITERATION = 20 # Percenge of pruning that is perform per iteration.\nPRUNE_VAL_LOSS_DEGRADATION_LIMIT = 0.2 # The max amout of validation loss that is permitted during pruning.\nPRUNE_EVAL_PERCENT_OF_DATASET = .05  # percent of dataset used to perform evaluation of model.\n\n#Pi login information\n#When using the continuous train option, these credentials will\n#be used to copy the final model to your vehicle. If not using this option, no need to set these.\nPI_USERNAME = ""pi""                  # username on pi\nPI_PASSWD = ""raspberry""             # password is optional. Only used from Windows machine. Ubuntu and mac users should copy their public keys to the pi. `ssh-copy-id username@hostname`\nPI_HOSTNAME = ""raspberrypi.local""   # the network hostname or ip address\nPI_DONKEY_ROOT = ""/home/pi/mycar""   # the location of the mycar dir on the pi. this will be used to help locate the final model destination.\n\n# Region of interst cropping\n# only supported in Categorical and Linear models.\n# If these crops values are too large, they will cause the stride values to become negative and the model with not be valid.\nROI_CROP_TOP = 0                    #the number of rows of pixels to ignore on the top of the image\nROI_CROP_BOTTOM = 0                 #the number of rows of pixels to ignore on the bottom of the image\n\n#Model transfer options\n#When copying weights during a model transfer operation, should we freeze a certain number of layers\n#to the incoming weights and not allow them to change during training?\nFREEZE_LAYERS = False               #default False will allow all layers to be modified by training\nNUM_LAST_LAYERS_TO_TRAIN = 7        #when freezing layers, how many layers from the last should be allowed to train?\n\n#WEB CONTROL\nWEB_CONTROL_PORT = 8887             # which port to listen on when making a web controller\nWEB_INIT_MODE = ""user""              # which control mode to start in. one of user|local_angle|local. Setting local will start in ai mode.\n\n#JOYSTICK\nUSE_JOYSTICK_AS_DEFAULT = False     #when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 0.5         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don\'t need the full speed available.\nJOYSTICK_STEERING_SCALE = 1.0       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nCONTROLLER_TYPE=\'ps3\'               #(ps3|ps4|xbox|nimbus|wiiu|F710|rc3|MM1|custom) custom will run the my_joystick.py controller written by the `donkey createjs` command\nUSE_NETWORKED_JS = False            #should we listen for remote joystick control over the network?\nNETWORK_JS_SERVER_IP = ""192.168.0.1""#when listening for network joystick control, which ip is serving this information\nJOYSTICK_DEADZONE = 0.0             # when non zero, this is the smallest throttle before recording triggered.\nJOYSTICK_THROTTLE_DIR = -1.0        # use -1.0 to flip forward/backward, use 1.0 to use joystick\'s natural forward/backward\nUSE_FPV = False                     # send camera data to FPV webserver\nJOYSTICK_DEVICE_FILE = ""/dev/input/js0"" # this is the unix file use to access the joystick.\n\n#For the categorical model, this limits the upper bound of the learned throttle\n#it\'s very IMPORTANT that this value is matched from the training PC config.py and the robot.py\n#and ideally wouldn\'t change once set.\nMODEL_CATEGORICAL_MAX_THROTTLE_RANGE = 0.5\n\n#RNN or 3D\nSEQUENCE_LENGTH = 3             #some models use a number of images over time. This controls how many.\n\n#IMU\nHAVE_IMU = False                #when true, this add a Mpu6050 part and records the data. Can be used with a\nIMU_SENSOR = \'mpu6050\'          # (mpu6050|mpu9250)\nIMU_DLP_CONFIG = 0              # Digital Lowpass Filter setting (0:250Hz, 1:184Hz, 2:92Hz, 3:41Hz, 4:20Hz, 5:10Hz, 6:5Hz)\n\n#SOMBRERO\nHAVE_SOMBRERO = False           #set to true when using the sombrero hat from the Donkeycar store. This will enable pwm on the hat.\n\n#ROBOHAT MM1\nHAVE_ROBOHAT = False            # set to true when using the Robo HAT MM1 from Robotics Masters.  This will change to RC Control.\nMM1_STEERING_MID = 1500         # Adjust this value if your car cannot run in a straight line\nMM1_MAX_FORWARD = 2000          # Max throttle to go fowrward. The bigger the faster\nMM1_STOPPED_PWM = 1500\nMM1_MAX_REVERSE = 1000          # Max throttle to go reverse. The smaller the faster\nMM1_SHOW_STEERING_VALUE = False\n# Serial port \n# -- Default Pi: \'/dev/ttyS0\'\n# -- Jetson Nano: \'/dev/ttyTHS1\'\n# -- Google coral: \'/dev/ttymxc0\'\n# -- Windows: \'COM3\', Arduino: \'/dev/ttyACM0\'\n# -- MacOS/Linux:please use \'ls /dev/tty.*\' to find the correct serial port for mm1 \n#  eg.\'/dev/tty.usbmodemXXXXXX\' and replace the port accordingly\nMM1_SERIAL_PORT = \'/dev/ttyS0\'  # Serial Port for reading and sending MM1 data.\n\n#RECORD OPTIONS\nRECORD_DURING_AI = False        #normally we do not record during ai mode. Set this to true to get image and steering records for your Ai. Be careful not to use them to train.\n\n#LED\nHAVE_RGB_LED = False            #do you have an RGB LED like https://www.amazon.com/dp/B07BNRZWNF\nLED_INVERT = False              #COMMON ANODE? Some RGB LED use common anode. like https://www.amazon.com/Xia-Fly-Tri-Color-Emitting-Diffused/dp/B07MYJQP8B\n\n#LED board pin number for pwm outputs\n#These are physical pinouts. See: https://www.raspberrypi-spy.co.uk/2012/06/simple-guide-to-the-rpi-gpio-header-and-pins/\nLED_PIN_R = 12\nLED_PIN_G = 10\nLED_PIN_B = 16\n\n#LED status color, 0-100\nLED_R = 0\nLED_G = 0\nLED_B = 1\n\n#LED Color for record count indicator\nREC_COUNT_ALERT = 1000          #how many records before blinking alert\nREC_COUNT_ALERT_CYC = 15        #how many cycles of 1/20 of a second to blink per REC_COUNT_ALERT records\nREC_COUNT_ALERT_BLINK_RATE = 0.4 #how fast to blink the led in seconds on/off\n\n#first number is record count, second tuple is color ( r, g, b) (0-100)\n#when record count exceeds that number, the color will be used\nRECORD_ALERT_COLOR_ARR = [ (0, (1, 1, 1)),\n            (3000, (5, 5, 5)),\n            (5000, (5, 2, 0)),\n            (10000, (0, 5, 0)),\n            (15000, (0, 5, 5)),\n            (20000, (0, 0, 5)), ]\n\n\n#LED status color, 0-100, for model reloaded alert\nMODEL_RELOADED_LED_R = 100\nMODEL_RELOADED_LED_G = 0\nMODEL_RELOADED_LED_B = 0\n\n\n#BEHAVIORS\n#When training the Behavioral Neural Network model, make a list of the behaviors,\n#Set the TRAIN_BEHAVIORS = True, and use the BEHAVIOR_LED_COLORS to give each behavior a color\nTRAIN_BEHAVIORS = False\nBEHAVIOR_LIST = [\'Left_Lane\', ""Right_Lane""]\nBEHAVIOR_LED_COLORS =[ (0, 10, 0), (10, 0, 0) ] #RGB tuples 0-100 per chanel\n\n#Localizer\n#The localizer is a neural network that can learn to predice it\'s location on the track.\n#This is an experimental feature that needs more developement. But it can currently be used\n#to predict the segement of the course, where the course is divided into NUM_LOCATIONS segments.\nTRAIN_LOCALIZER = False\nNUM_LOCATIONS = 10\nBUTTON_PRESS_NEW_TUB = False #when enabled, makes it easier to divide our data into one tub per track length if we make a new tub on each X button press.\n\n#DonkeyGym\n#Only on Ubuntu linux, you can use the simulator as a virtual donkey and\n#issue the same python manage.py drive command as usual, but have them control a virtual car.\n#This enables that, and sets the path to the simualator and the environment.\n#You will want to download the simulator binary from: https://github.com/tawnkramer/donkey_gym/releases/download/v18.9/DonkeySimLinux.zip\n#then extract that and modify DONKEY_SIM_PATH.\nDONKEY_GYM = False\nDONKEY_SIM_PATH = ""path to sim"" #""/home/tkramer/projects/sdsandbox/sdsim/build/DonkeySimLinux/donkey_sim.x86_64"" when racing on virtual-race-league use ""remote"", or user ""remote"" when you want to start the sim manually first.\nDONKEY_GYM_ENV_NAME = ""donkey-mountain-track-v0"" # (""donkey-generated-track-v0""|""donkey-generated-roads-v0""|""donkey-warehouse-v0""|""donkey-avc-sparkfun-v0"")\nGYM_CONF = { ""body_style"" : ""donkey"", ""body_rgb"" : (128, 128, 128), ""car_name"" : ""me"", ""font_size"" : 100} # body style(donkey|bare|car01) body rgb 0-255\nSIM_HOST = ""127.0.0.1""              # when racing on virtual-race-league use host ""trainmydonkey.com""\nSIM_ARTIFICIAL_LATENCY = 0          # this is the millisecond latency in controls. Can use useful in emulating the delay when useing a remote server. values of 100 to 400 probably reasonable.\n\n#publish camera over network\n#This is used to create a tcp service to pushlish the camera feed\nPUB_CAMERA_IMAGES = False\n\n#When racing, to give the ai a boost, configure these values.\nAI_LAUNCH_DURATION = 0.0            # the ai will output throttle for this many seconds\nAI_LAUNCH_THROTTLE = 0.0            # the ai will output this throttle value\nAI_LAUNCH_ENABLE_BUTTON = \'R2\'      # this keypress will enable this boost. It must be enabled before each use to prevent accidental trigger.\nAI_LAUNCH_KEEP_ENABLED = False      # when False ( default) you will need to hit the AI_LAUNCH_ENABLE_BUTTON for each use. This is safest. When this True, is active on each trip into ""local"" ai mode.\n\n#Scale the output of the throttle of the ai pilot for all model types.\nAI_THROTTLE_MULT = 1.0              # this multiplier will scale every throttle value for all output from NN models\n\n#Path following\nPATH_FILENAME = ""donkey_path.pkl""   # the path will be saved to this filename\nPATH_SCALE = 5.0                    # the path display will be scaled by this factor in the web page\nPATH_OFFSET = (0, 0)                # 255, 255 is the center of the map. This offset controls where the origin is displayed.\nPATH_MIN_DIST = 0.3                 # after travelling this distance (m), save a path point\nPID_P = -10.0                       # proportional mult for PID path follower\nPID_I = 0.000                       # integral mult for PID path follower\nPID_D = -0.2                        # differential mult for PID path follower\nPID_THROTTLE = 0.2                  # constant throttle value during path following\nSAVE_PATH_BTN = ""cross""             # joystick button to save path\nRESET_ORIGIN_BTN = ""triangle""       # joystick button to press to move car back to origin\n\n# Intel Realsense D435 and D435i depth sensing camera\nREALSENSE_D435_RGB = True       # True to capture RGB image\nREALSENSE_D435_DEPTH = True     # True to capture depth as image array\nREALSENSE_D435_IMU = False      # True to capture IMU data (D435i only)\nREALSENSE_D435_ID = None        # serial number of camera or None if you only have one camera (it will autodetect)\n\n\n'"
donkeycar/templates/cfg_cv_control.py,0,"b'"""""" \nCAR CONFIG \n\nThis file is read by your car application\'s manage.py script to change the car\nperformance. \n\nEXMAPLE\n-----------\nimport dk\ncfg = dk.load_config(config_path=\'~/mycar/config.py\')\nprint(cfg.CAMERA_RESOLUTION)\n\n""""""\n\n\nimport os\n\n#PATHS\nCAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(CAR_PATH, \'data\')\nMODELS_PATH = os.path.join(CAR_PATH, \'models\')\n\n#VEHICLE\nDRIVE_LOOP_HZ = 20\nMAX_LOOPS = -1\n\n#CAMERA\nIMAGE_W = 160\nIMAGE_H = 120\nIMAGE_DEPTH = 3         # default RGB=3, make 1 for mono\n\n#9865, over rides only if needed, ie. TX2..\nPCA9685_I2C_ADDR = 0x40\nPCA9685_I2C_BUSNUM = None\n\n#STEERING\nSTEERING_CHANNEL = 1\nSTEERING_LEFT_PWM = 460\nSTEERING_RIGHT_PWM = 290\n\n#THROTTLE\nTHROTTLE_CHANNEL = 0\nTHROTTLE_FORWARD_PWM = 500\nTHROTTLE_STOPPED_PWM = 370\nTHROTTLE_REVERSE_PWM = 220\n\n#SOMBRERO\nHAVE_SOMBRERO = False\n\n'"
donkeycar/templates/cfg_path_follow.py,0,"b'""""""\nCAR CONFIG\n\nThis file is read by your car application\'s manage.py script to change the car\nperformance.\n\nEXMAPLE\n-----------\nimport dk\ncfg = dk.load_config(config_path=\'~/mycar/config.py\')\nprint(cfg.CAMERA_RESOLUTION)\n\n""""""\n\n\nimport os\n\n#PATHS\nCAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(CAR_PATH, \'data\')\nMODELS_PATH = os.path.join(CAR_PATH, \'models\')\n\n#VEHICLE\nDRIVE_LOOP_HZ = 20      # the vehicle loop will pause if faster than this speed.\nMAX_LOOPS = None        # the vehicle loop can abort after this many iterations, when given a positive integer.\n\n\n#9865, over rides only if needed, ie. TX2..\nPCA9685_I2C_ADDR = 0x40     #I2C address, use i2cdetect to validate this number\nPCA9685_I2C_BUSNUM = None   #None will auto detect, which is fine on the pi. But other platforms should specify the bus num.\n\n#DRIVETRAIN\n#These options specify which chasis and motor setup you are using. Most are using SERVO_ESC.\n#DC_STEER_THROTTLE uses HBridge pwm to control one steering dc motor, and one drive wheel motor\n#DC_TWO_WHEEL uses HBridge pwm to control two drive motors, one on the left, and one on the right.\n#SERVO_HBRIDGE_PWM use ServoBlaster to output pwm control from the PiZero directly to control steering, and HBridge for a drive motor.\nDRIVE_TRAIN_TYPE = ""SERVO_ESC"" # SERVO_ESC|DC_STEER_THROTTLE|DC_TWO_WHEEL|SERVO_HBRIDGE_PWM\n\n#STEERING\nSTEERING_CHANNEL = 1            #channel on the 9685 pwm board 0-15\nSTEERING_LEFT_PWM = 460         #pwm value for full left steering\nSTEERING_RIGHT_PWM = 290        #pwm value for full right steering\n\n#THROTTLE\nTHROTTLE_CHANNEL = 0            #channel on the 9685 pwm board 0-15\nTHROTTLE_FORWARD_PWM = 500      #pwm value for max forward throttle\nTHROTTLE_STOPPED_PWM = 370      #pwm value for no movement\nTHROTTLE_REVERSE_PWM = 220      #pwm value for max reverse throttle\n\n#DC_STEER_THROTTLE with one motor as steering, one as drive\n#these GPIO pinouts are only used for the DRIVE_TRAIN_TYPE=DC_STEER_THROTTLE\nHBRIDGE_PIN_LEFT = 18\nHBRIDGE_PIN_RIGHT = 16\nHBRIDGE_PIN_FWD = 15\nHBRIDGE_PIN_BWD = 13\n\n#DC_TWO_WHEEL - with two wheels as drive, left and right.\n#these GPIO pinouts are only used for the DRIVE_TRAIN_TYPE=DC_TWO_WHEEL\nHBRIDGE_PIN_LEFT_FWD = 18\nHBRIDGE_PIN_LEFT_BWD = 16\nHBRIDGE_PIN_RIGHT_FWD = 15\nHBRIDGE_PIN_RIGHT_BWD = 13\n\n\n\n#JOYSTICK\nUSE_JOYSTICK_AS_DEFAULT = False     #when starting the manage.py, when True, will not require a --js option to use the joystick\nJOYSTICK_MAX_THROTTLE = 0.5         #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don\'t need the full speed available.\nJOYSTICK_STEERING_SCALE = 1.0       #some people want a steering that is less sensitve. This scalar is multiplied with the steering -1 to 1. It can be negative to reverse dir.\nAUTO_RECORD_ON_THROTTLE = True      #if true, we will record whenever throttle is not zero. if false, you must manually toggle recording with some other trigger. Usually circle button on joystick.\nCONTROLLER_TYPE=\'ps3\'               #(ps3|ps4|xbox|nimbus|wiiu|F710|rc3)\nUSE_NETWORKED_JS = False            #should we listen for remote joystick control over the network?\nNETWORK_JS_SERVER_IP = ""192.168.0.1""#when listening for network joystick control, which ip is serving this information\nJOYSTICK_DEADZONE = 0.0             # when non zero, this is the smallest throttle before recording triggered.\nJOYSTICK_THROTTLE_DIR = -1.0        # use -1.0 to flip forward/backward, use 1.0 to use joystick\'s natural forward/backward\n\n\n#SOMBRERO\nHAVE_SOMBRERO = False               #set to true when using the sombrero hat from the Donkeycar store. This will enable pwm on the hat.\n\n\n#Path following\nPATH_FILENAME = ""donkey_path.pkl""   # the path will be saved to this filename\nPATH_SCALE = 10.0                   # the path display will be scaled by this factor in the web page\nPATH_OFFSET = (255, 255)            # 255, 255 is the center of the map. This offset controls where the origin is displayed.\nPATH_MIN_DIST = 0.3                 # after travelling this distance (m), save a path point\nPID_P = -3.0                        # proportional mult for PID path follower\nPID_I = 0.000                       # integral mult for PID path follower\nPID_D = -0.2                        # differential mult for PID path follower\nPID_THROTTLE = 0.25                 # constant throttle value during path following\nSAVE_PATH_BTN = ""cross""             # joystick button to save path\nRESET_ORIGIN_BTN = ""triangle""       # joystick button to press to move car back to origin\n\n#Odometry\nHAVE_ODOM = False                   # Do you have an odometer? Uses pigpio \nMM_PER_TICK = 12.7625               # How much travel with a single tick, in mm\nODOM_PIN = 4                        # Which GPIO board mode pin to use as input\nODOM_DEBUG = False                  # Write out values on vel and distance as it runs\n\n#Intel T265\nWHEEL_ODOM_CALIB = ""calibration_odometry.json""\n\n#DonkeyGym\n#Only on Ubuntu linux, you can use the simulator as a virtual donkey and\n#issue the same python manage.py drive command as usual, but have them control a virtual car.\n#This enables that, and sets the path to the simualator and the environment.\n#You will want to download the simulator binary from: https://github.com/tawnkramer/donkey_gym/releases/download/v18.9/DonkeySimLinux.zip\n#then extract that and modify DONKEY_SIM_PATH.\nDONKEY_GYM = False\nDONKEY_SIM_PATH = ""path to sim"" #""/home/tkramer/projects/sdsandbox/sdsim/build/DonkeySimLinux/donkey_sim.x86_64""\nDONKEY_GYM_ENV_NAME = ""donkey-generated-track-v0"" # (""donkey-generated-track-v0""|""donkey-generated-roads-v0""|""donkey-warehouse-v0""|""donkey-avc-sparkfun-v0"")\n'"
donkeycar/templates/cfg_square.py,0,"b'"""""" \nCAR CONFIG \n\nThis file is read by your car application\'s manage.py script to change the car\nperformance. \n\nEXMAPLE\n-----------\nimport dk\ncfg = dk.load_config(config_path=\'~/mycar/config.py\')\nprint(cfg.CAMERA_RESOLUTION)\n\n""""""\n\n\nimport os\n\n#PATHS\nCAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(CAR_PATH, \'data\')\nMODELS_PATH = os.path.join(CAR_PATH, \'models\')\n\n#VEHICLE\nDRIVE_LOOP_HZ = 20\nMAX_LOOPS = 100000\n\n#CAMERA\nIMAGE_W = 160\nIMAGE_H = 120\nIMAGE_DEPTH = 3         # default RGB=3, make 1 for mono\n\n#9865, over rides only if needed, ie. TX2..\nPCA9685_I2C_ADDR = 0x40\nPCA9685_I2C_BUSNUM = None\n\n#STEERING\nSTEERING_CHANNEL = 1\nSTEERING_LEFT_PWM = 460\nSTEERING_RIGHT_PWM = 290\n\n#THROTTLE\nTHROTTLE_CHANNEL = 0\nTHROTTLE_FORWARD_PWM = 500\nTHROTTLE_STOPPED_PWM = 370\nTHROTTLE_REVERSE_PWM = 220\n\n#TRAINING\nDEFAULT_MODEL_TYPE = \'linear\' #(linear|categorical|rnn|imu|behavior|3d|localizer|latent)\nBATCH_SIZE = 128\nTRAIN_TEST_SPLIT = 0.8\nMAX_EPOCHS = 100\nSHOW_PLOT = True\nVERBOSE_TRAIN = True\nUSE_EARLY_STOP = True\nEARLY_STOP_PATIENCE = 5\nMIN_DELTA = .0005\nPRINT_MODEL_SUMMARY = True      #print layers and weights to stdout\nOPTIMIZER = None                #adam, sgd, rmsprop, etc.. None accepts default\nLEARNING_RATE = 0.001           #only used when OPTIMIZER specified\nLEARNING_RATE_DECAY = 0.0       #only used when OPTIMIZER specified\nCACHE_IMAGES = True             #keep images in memory. will speed succesive epochs, but crater if not enough mem.\nPRUNE_CNN = False\nPRUNE_PERCENT_TARGET = 75 # The desired percentage of pruning.\nPRUNE_PERCENT_PER_ITERATION = 20 # Percenge of pruning that is perform per iteration.\nPRUNE_VAL_LOSS_DEGRADATION_LIMIT = 0.2 # The max amout of validation loss that is permitted during pruning.\nPRUNE_EVAL_PERCENT_OF_DATASET = .05  # percent of dataset used to perform evaluation of model.\n\n# Region of interst cropping\n# only supported in Categorical and Linear models.\nROI_CROP_TOP = 0\nROI_CROP_BOTTOM = 0\n\n#model transfer options\nFREEZE_LAYERS = False\nNUM_LAST_LAYERS_TO_TRAIN = 7\n\n#For the categorical model, this limits the upper bound of the learned throttle\n#it\'s very IMPORTANT that this value is matched from the training PC config.py and the robot.py\n#and ideally wouldn\'t change once set.\nMODEL_CATEGORICAL_MAX_THROTTLE_RANGE = 0.5\n\n#RNN or 3D\nSEQUENCE_LENGTH = 3\n\n#SOMBRERO\nHAVE_SOMBRERO = False\n\n#RECORD OPTIONS\nRECORD_DURING_AI = False\n'"
donkeycar/templates/complete.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey 2 car\n\nUsage:\n    manage.py (drive) [--model=<model>] [--js] [--type=(linear|categorical|rnn|imu|behavior|3d|localizer|latent)] [--camera=(single|stereo)] [--meta=<key:value> ...] [--myconfig=<filename>]\n    manage.py (train) [--tub=<tub1,tub2,..tubn>] [--file=<file> ...] (--model=<model>) [--transfer=<model>] [--type=(linear|categorical|rnn|imu|behavior|3d|localizer)] [--continuous] [--aug] [--myconfig=<filename>]\n\n\nOptions:\n    -h --help               Show this screen.\n    --js                    Use physical joystick.\n    -f --file=<file>        A text file containing paths to tub files, one per line. Option may be used more than once.\n    --meta=<key:value>      Key/Value strings describing describing a piece of meta data about this drive. Option may be used more than once.\n    --myconfig=filename     Specify myconfig file to use. \n                            [default: myconfig.py]\n""""""\nimport os\nimport time\n\nfrom docopt import docopt\nimport numpy as np\n\nimport donkeycar as dk\n\n#import parts\nfrom donkeycar.parts.transform import Lambda, TriggeredCallback, DelayedTrigger\nfrom donkeycar.parts.datastore import TubHandler\nfrom donkeycar.parts.controller import LocalWebController, \\\n    JoystickController, WebFpv\nfrom donkeycar.parts.throttle_filter import ThrottleFilter\nfrom donkeycar.parts.behavior import BehaviorPart\nfrom donkeycar.parts.file_watcher import FileWatcher\nfrom donkeycar.parts.launch import AiLaunch\nfrom donkeycar.utils import *\n\ndef drive(cfg, model_path=None, use_joystick=False, model_type=None, camera_type=\'single\', meta=[]):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n\n    if cfg.DONKEY_GYM:\n        #the simulator will use cuda and then we usually run out of resources\n        #if we also try to use cuda. so disable for donkey_gym.\n        os.environ[""CUDA_VISIBLE_DEVICES""]=""-1""\n\n    if model_type is None:\n        if cfg.TRAIN_LOCALIZER:\n            model_type = ""localizer""\n        elif cfg.TRAIN_BEHAVIORS:\n            model_type = ""behavior""\n        else:\n            model_type = cfg.DEFAULT_MODEL_TYPE\n\n    #Initialize car\n    V = dk.vehicle.Vehicle()\n\n    print(""cfg.CAMERA_TYPE"", cfg.CAMERA_TYPE)\n    if camera_type == ""stereo"":\n\n        if cfg.CAMERA_TYPE == ""WEBCAM"":\n            from donkeycar.parts.camera import Webcam\n\n            camA = Webcam(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, iCam = 0)\n            camB = Webcam(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, iCam = 1)\n\n        elif cfg.CAMERA_TYPE == ""CVCAM"":\n            from donkeycar.parts.cv import CvCam\n\n            camA = CvCam(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, iCam = 0)\n            camB = CvCam(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, iCam = 1)\n        else:\n            raise(Exception(""Unsupported camera type: %s"" % cfg.CAMERA_TYPE))\n\n        V.add(camA, outputs=[\'cam/image_array_a\'], threaded=True)\n        V.add(camB, outputs=[\'cam/image_array_b\'], threaded=True)\n\n        from donkeycar.parts.image import StereoPair\n\n        V.add(StereoPair(), inputs=[\'cam/image_array_a\', \'cam/image_array_b\'],\n            outputs=[\'cam/image_array\'])\n    elif cfg.CAMERA_TYPE == ""D435"":\n        from donkeycar.parts.realsense435i import RealSense435i\n        cam = RealSense435i(\n            enable_rgb=cfg.REALSENSE_D435_RGB,\n            enable_depth=cfg.REALSENSE_D435_DEPTH,\n            enable_imu=cfg.REALSENSE_D435_IMU,\n            device_id=cfg.REALSENSE_D435_ID)\n        V.add(cam, inputs=[],\n              outputs=[\'cam/image_array\', \'cam/depth_array\',\n                       \'imu/acl_x\', \'imu/acl_y\', \'imu/acl_z\',\n                       \'imu/gyr_x\', \'imu/gyr_y\', \'imu/gyr_z\'],\n              threaded=True)\n\n    else:\n        if cfg.DONKEY_GYM:\n            from donkeycar.parts.dgym import DonkeyGymEnv\n\n        inputs = []\n        threaded = True\n        if cfg.DONKEY_GYM:\n            from donkeycar.parts.dgym import DonkeyGymEnv \n            cam = DonkeyGymEnv(cfg.DONKEY_SIM_PATH, host=cfg.SIM_HOST, env_name=cfg.DONKEY_GYM_ENV_NAME, conf=cfg.GYM_CONF, delay=cfg.SIM_ARTIFICIAL_LATENCY)\n            threaded = True\n            inputs = [\'angle\', \'throttle\']\n        elif cfg.CAMERA_TYPE == ""PICAM"":\n            from donkeycar.parts.camera import PiCamera\n            cam = PiCamera(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, framerate=cfg.CAMERA_FRAMERATE, vflip=cfg.CAMERA_VFLIP, hflip=cfg.CAMERA_HFLIP)\n        elif cfg.CAMERA_TYPE == ""WEBCAM"":\n            from donkeycar.parts.camera import Webcam\n            cam = Webcam(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH)\n        elif cfg.CAMERA_TYPE == ""CVCAM"":\n            from donkeycar.parts.cv import CvCam\n            cam = CvCam(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH)\n        elif cfg.CAMERA_TYPE == ""CSIC"":\n            from donkeycar.parts.camera import CSICamera\n            cam = CSICamera(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, framerate=cfg.CAMERA_FRAMERATE, gstreamer_flip=cfg.CSIC_CAM_GSTREAMER_FLIP_PARM)\n        elif cfg.CAMERA_TYPE == ""V4L"":\n            from donkeycar.parts.camera import V4LCamera\n            cam = V4LCamera(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH, framerate=cfg.CAMERA_FRAMERATE)\n        elif cfg.CAMERA_TYPE == ""MOCK"":\n            from donkeycar.parts.camera import MockCamera\n            cam = MockCamera(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH)\n        elif cfg.CAMERA_TYPE == ""IMAGE_LIST"":\n            from donkeycar.parts.camera import ImageListCamera\n            cam = ImageListCamera(path_mask=cfg.PATH_MASK)\n        else:\n            raise(Exception(""Unkown camera type: %s"" % cfg.CAMERA_TYPE))\n\n        V.add(cam, inputs=inputs, outputs=[\'cam/image_array\'], threaded=threaded)\n\n    if use_joystick or cfg.USE_JOYSTICK_AS_DEFAULT:\n        #modify max_throttle closer to 1.0 to have more power\n        #modify steering_scale lower than 1.0 to have less responsive steering\n        if cfg.CONTROLLER_TYPE == ""MM1"":\n            from donkeycar.parts.robohat import RoboHATController            \n            ctr = RoboHATController(cfg)\n        elif ""custom"" == cfg.CONTROLLER_TYPE:\n            #\n            # custom controller created with `donkey createjs` command\n            #\n            from my_joystick import MyJoystickController\n            ctr = MyJoystickController(\n                throttle_dir=cfg.JOYSTICK_THROTTLE_DIR,\n                throttle_scale=cfg.JOYSTICK_MAX_THROTTLE,\n                steering_scale=cfg.JOYSTICK_STEERING_SCALE,\n                auto_record_on_throttle=cfg.AUTO_RECORD_ON_THROTTLE)\n            ctr.set_deadzone(cfg.JOYSTICK_DEADZONE)\n        else:\n            from donkeycar.parts.controller import get_js_controller\n\n            ctr = get_js_controller(cfg)\n\n            if cfg.USE_NETWORKED_JS:\n                from donkeycar.parts.controller import JoyStickSub\n                netwkJs = JoyStickSub(cfg.NETWORK_JS_SERVER_IP)\n                V.add(netwkJs, threaded=True)\n                ctr.js = netwkJs\n        \n        V.add(ctr, \n          inputs=[\'cam/image_array\'],\n          outputs=[\'user/angle\', \'user/throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n\n    else:\n        #This web controller will create a web server that is capable\n        #of managing steering, throttle, and modes, and more.\n        ctr = LocalWebController(port=cfg.WEB_CONTROL_PORT, mode=cfg.WEB_INIT_MODE)\n        \n        V.add(ctr,\n          inputs=[\'cam/image_array\', \'tub/num_records\'],\n          outputs=[\'user/angle\', \'user/throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n\n    #this throttle filter will allow one tap back for esc reverse\n    th_filter = ThrottleFilter()\n    V.add(th_filter, inputs=[\'user/throttle\'], outputs=[\'user/throttle\'])\n\n    #See if we should even run the pilot module.\n    #This is only needed because the part run_condition only accepts boolean\n    class PilotCondition:\n        def run(self, mode):\n            if mode == \'user\':\n                return False\n            else:\n                return True\n\n    V.add(PilotCondition(), inputs=[\'user/mode\'], outputs=[\'run_pilot\'])\n\n    class LedConditionLogic:\n        def __init__(self, cfg):\n            self.cfg = cfg\n\n        def run(self, mode, recording, recording_alert, behavior_state, model_file_changed, track_loc):\n            #returns a blink rate. 0 for off. -1 for on. positive for rate.\n\n            if track_loc is not None:\n                led.set_rgb(*self.cfg.LOC_COLORS[track_loc])\n                return -1\n\n            if model_file_changed:\n                led.set_rgb(self.cfg.MODEL_RELOADED_LED_R, self.cfg.MODEL_RELOADED_LED_G, self.cfg.MODEL_RELOADED_LED_B)\n                return 0.1\n            else:\n                led.set_rgb(self.cfg.LED_R, self.cfg.LED_G, self.cfg.LED_B)\n\n            if recording_alert:\n                led.set_rgb(*recording_alert)\n                return self.cfg.REC_COUNT_ALERT_BLINK_RATE\n            else:\n                led.set_rgb(self.cfg.LED_R, self.cfg.LED_G, self.cfg.LED_B)\n\n            if behavior_state is not None and model_type == \'behavior\':\n                r, g, b = self.cfg.BEHAVIOR_LED_COLORS[behavior_state]\n                led.set_rgb(r, g, b)\n                return -1 #solid on\n\n            if recording:\n                return -1 #solid on\n            elif mode == \'user\':\n                return 1\n            elif mode == \'local_angle\':\n                return 0.5\n            elif mode == \'local\':\n                return 0.1\n            return 0\n\n    if cfg.HAVE_RGB_LED and not cfg.DONKEY_GYM:\n        from donkeycar.parts.led_status import RGB_LED\n        led = RGB_LED(cfg.LED_PIN_R, cfg.LED_PIN_G, cfg.LED_PIN_B, cfg.LED_INVERT)\n        led.set_rgb(cfg.LED_R, cfg.LED_G, cfg.LED_B)\n\n        V.add(LedConditionLogic(cfg), inputs=[\'user/mode\', \'recording\', ""records/alert"", \'behavior/state\', \'modelfile/modified\', ""pilot/loc""],\n              outputs=[\'led/blink_rate\'])\n\n        V.add(led, inputs=[\'led/blink_rate\'])\n\n    def get_record_alert_color(num_records):\n        col = (0, 0, 0)\n        for count, color in cfg.RECORD_ALERT_COLOR_ARR:\n            if num_records >= count:\n                col = color\n        return col\n\n    class RecordTracker:\n        def __init__(self):\n            self.last_num_rec_print = 0\n            self.dur_alert = 0\n            self.force_alert = 0\n\n        def run(self, num_records):\n            if num_records is None:\n                return 0\n\n            if self.last_num_rec_print != num_records or self.force_alert:\n                self.last_num_rec_print = num_records\n\n                if num_records % 10 == 0:\n                    print(""recorded"", num_records, ""records"")\n\n                if num_records % cfg.REC_COUNT_ALERT == 0 or self.force_alert:\n                    self.dur_alert = num_records // cfg.REC_COUNT_ALERT * cfg.REC_COUNT_ALERT_CYC\n                    self.force_alert = 0\n\n            if self.dur_alert > 0:\n                self.dur_alert -= 1\n\n            if self.dur_alert != 0:\n                return get_record_alert_color(num_records)\n\n            return 0\n\n    rec_tracker_part = RecordTracker()\n    V.add(rec_tracker_part, inputs=[""tub/num_records""], outputs=[\'records/alert\'])\n\n    if cfg.AUTO_RECORD_ON_THROTTLE and isinstance(ctr, JoystickController):\n        #then we are not using the circle button. hijack that to force a record count indication\n        def show_record_acount_status():\n            rec_tracker_part.last_num_rec_print = 0\n            rec_tracker_part.force_alert = 1\n        ctr.set_button_down_trigger(\'circle\', show_record_acount_status)\n\n    #Sombrero\n    if cfg.HAVE_SOMBRERO:\n        from donkeycar.parts.sombrero import Sombrero\n        s = Sombrero()\n\n    #IMU\n    if cfg.HAVE_IMU:\n        from donkeycar.parts.imu import IMU\n        imu = IMU(sensor=cfg.IMU_SENSOR, dlp_setting=cfg.IMU_DLP_CONFIG)\n        V.add(imu, outputs=[\'imu/acl_x\', \'imu/acl_y\', \'imu/acl_z\',\n            \'imu/gyr_x\', \'imu/gyr_y\', \'imu/gyr_z\'], threaded=True)\n\n    class ImgPreProcess():\n        \'\'\'\n        preprocess camera image for inference.\n        normalize and crop if needed.\n        \'\'\'\n        def __init__(self, cfg):\n            self.cfg = cfg\n\n        def run(self, img_arr):\n            return normalize_and_crop(img_arr, self.cfg)\n\n    if ""coral"" in model_type:\n        inf_input = \'cam/image_array\'\n    else:\n        inf_input = \'cam/normalized/cropped\'\n        V.add(ImgPreProcess(cfg),\n            inputs=[\'cam/image_array\'],\n            outputs=[inf_input],\n            run_condition=\'run_pilot\')\n\n    # Use the FPV preview, which will show the cropped image output, or the full frame.\n    if cfg.USE_FPV:\n        V.add(WebFpv(), inputs=[\'cam/image_array\'], threaded=True)\n\n    #Behavioral state\n    if cfg.TRAIN_BEHAVIORS:\n        bh = BehaviorPart(cfg.BEHAVIOR_LIST)\n        V.add(bh, outputs=[\'behavior/state\', \'behavior/label\', ""behavior/one_hot_state_array""])\n        try:\n            ctr.set_button_down_trigger(\'L1\', bh.increment_state)\n        except:\n            pass\n\n        inputs = [inf_input, ""behavior/one_hot_state_array""]\n    #IMU\n    elif model_type == ""imu"":\n        assert(cfg.HAVE_IMU)\n        #Run the pilot if the mode is not user.\n        inputs=[inf_input,\n            \'imu/acl_x\', \'imu/acl_y\', \'imu/acl_z\',\n            \'imu/gyr_x\', \'imu/gyr_y\', \'imu/gyr_z\']\n    else:\n        inputs=[inf_input]\n\n    def load_model(kl, model_path):\n        start = time.time()\n        print(\'loading model\', model_path)\n        kl.load(model_path)\n        print(\'finished loading in %s sec.\' % (str(time.time() - start)) )\n\n    def load_weights(kl, weights_path):\n        start = time.time()\n        try:\n            print(\'loading model weights\', weights_path)\n            kl.model.load_weights(weights_path)\n            print(\'finished loading in %s sec.\' % (str(time.time() - start)) )\n        except Exception as e:\n            print(e)\n            print(\'ERR>> problems loading weights\', weights_path)\n\n    def load_model_json(kl, json_fnm):\n        start = time.time()\n        print(\'loading model json\', json_fnm)\n        from tensorflow.python import keras\n        try:\n            with open(json_fnm, \'r\') as handle:\n                contents = handle.read()\n                kl.model = keras.models.model_from_json(contents)\n            print(\'finished loading json in %s sec.\' % (str(time.time() - start)) )\n        except Exception as e:\n            print(e)\n            print(""ERR>> problems loading model json"", json_fnm)\n\n    if model_path:\n        #When we have a model, first create an appropriate Keras part\n        kl = dk.utils.get_model_by_type(model_type, cfg)\n\n        model_reload_cb = None\n\n        if \'.h5\' in model_path or \'.uff\' in model_path or \'tflite\' in model_path or \'.pkl\' in model_path:\n            #when we have a .h5 extension\n            #load everything from the model file\n            load_model(kl, model_path)\n\n            def reload_model(filename):\n                load_model(kl, filename)\n\n            model_reload_cb = reload_model\n\n        elif \'.json\' in model_path:\n            #when we have a .json extension\n            #load the model from there and look for a matching\n            #.wts file with just weights\n            load_model_json(kl, model_path)\n            weights_path = model_path.replace(\'.json\', \'.weights\')\n            load_weights(kl, weights_path)\n\n            def reload_weights(filename):\n                weights_path = filename.replace(\'.json\', \'.weights\')\n                load_weights(kl, weights_path)\n\n            model_reload_cb = reload_weights\n\n        else:\n            print(""ERR>> Unknown extension type on model file!!"")\n            return\n\n        #this part will signal visual LED, if connected\n        V.add(FileWatcher(model_path, verbose=True), outputs=[\'modelfile/modified\'])\n\n        #these parts will reload the model file, but only when ai is running so we don\'t interrupt user driving\n        V.add(FileWatcher(model_path), outputs=[\'modelfile/dirty\'], run_condition=""ai_running"")\n        V.add(DelayedTrigger(100), inputs=[\'modelfile/dirty\'], outputs=[\'modelfile/reload\'], run_condition=""ai_running"")\n        V.add(TriggeredCallback(model_path, model_reload_cb), inputs=[""modelfile/reload""], run_condition=""ai_running"")\n\n        outputs=[\'pilot/angle\', \'pilot/throttle\']\n\n        if cfg.TRAIN_LOCALIZER:\n            outputs.append(""pilot/loc"")\n\n        V.add(kl, inputs=inputs,\n            outputs=outputs,\n            run_condition=\'run_pilot\')\n\n    #Choose what inputs should change the car.\n    class DriveMode:\n        def run(self, mode,\n                    user_angle, user_throttle,\n                    pilot_angle, pilot_throttle):\n            if mode == \'user\':\n                return user_angle, user_throttle\n\n            elif mode == \'local_angle\':\n                return pilot_angle if pilot_angle else 0.0, user_throttle\n\n            else:\n                return pilot_angle if pilot_angle else 0.0, pilot_throttle * cfg.AI_THROTTLE_MULT if pilot_throttle else 0.0\n\n    V.add(DriveMode(),\n          inputs=[\'user/mode\', \'user/angle\', \'user/throttle\',\n                  \'pilot/angle\', \'pilot/throttle\'],\n          outputs=[\'angle\', \'throttle\'])\n\n\n    #to give the car a boost when starting ai mode in a race.\n    aiLauncher = AiLaunch(cfg.AI_LAUNCH_DURATION, cfg.AI_LAUNCH_THROTTLE, cfg.AI_LAUNCH_KEEP_ENABLED)\n\n    V.add(aiLauncher,\n        inputs=[\'user/mode\', \'throttle\'],\n        outputs=[\'throttle\'])\n\n    if isinstance(ctr, JoystickController):\n        ctr.set_button_down_trigger(cfg.AI_LAUNCH_ENABLE_BUTTON, aiLauncher.enable_ai_launch)\n\n\n    class AiRunCondition:\n        \'\'\'\n        A bool part to let us know when ai is running.\n        \'\'\'\n        def run(self, mode):\n            if mode == ""user"":\n                return False\n            return True\n\n    V.add(AiRunCondition(), inputs=[\'user/mode\'], outputs=[\'ai_running\'])\n\n    #Ai Recording\n    class AiRecordingCondition:\n        \'\'\'\n        return True when ai mode, otherwize respect user mode recording flag\n        \'\'\'\n        def run(self, mode, recording):\n            if mode == \'user\':\n                return recording\n            return True\n\n    if cfg.RECORD_DURING_AI:\n        V.add(AiRecordingCondition(), inputs=[\'user/mode\', \'recording\'], outputs=[\'recording\'])\n\n    #Drive train setup\n    if cfg.DONKEY_GYM or cfg.DRIVE_TRAIN_TYPE == ""MOCK"":\n        pass\n    elif cfg.DRIVE_TRAIN_TYPE == ""SERVO_ESC"":\n        from donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\n\n        steering_controller = PCA9685(cfg.STEERING_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n        steering = PWMSteering(controller=steering_controller,\n                                        left_pulse=cfg.STEERING_LEFT_PWM,\n                                        right_pulse=cfg.STEERING_RIGHT_PWM)\n\n        throttle_controller = PCA9685(cfg.THROTTLE_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n        throttle = PWMThrottle(controller=throttle_controller,\n                                        max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                        zero_pulse=cfg.THROTTLE_STOPPED_PWM,\n                                        min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\n        V.add(steering, inputs=[\'angle\'], threaded=True)\n        V.add(throttle, inputs=[\'throttle\'], threaded=True)\n\n\n    elif cfg.DRIVE_TRAIN_TYPE == ""DC_STEER_THROTTLE"":\n        from donkeycar.parts.actuator import Mini_HBridge_DC_Motor_PWM\n\n        steering = Mini_HBridge_DC_Motor_PWM(cfg.HBRIDGE_PIN_LEFT, cfg.HBRIDGE_PIN_RIGHT)\n        throttle = Mini_HBridge_DC_Motor_PWM(cfg.HBRIDGE_PIN_FWD, cfg.HBRIDGE_PIN_BWD)\n\n        V.add(steering, inputs=[\'angle\'])\n        V.add(throttle, inputs=[\'throttle\'])\n\n\n    elif cfg.DRIVE_TRAIN_TYPE == ""DC_TWO_WHEEL"":\n        from donkeycar.parts.actuator import TwoWheelSteeringThrottle, Mini_HBridge_DC_Motor_PWM\n\n        left_motor = Mini_HBridge_DC_Motor_PWM(cfg.HBRIDGE_PIN_LEFT_FWD, cfg.HBRIDGE_PIN_LEFT_BWD)\n        right_motor = Mini_HBridge_DC_Motor_PWM(cfg.HBRIDGE_PIN_RIGHT_FWD, cfg.HBRIDGE_PIN_RIGHT_BWD)\n        two_wheel_control = TwoWheelSteeringThrottle()\n\n        V.add(two_wheel_control,\n                inputs=[\'throttle\', \'angle\'],\n                outputs=[\'left_motor_speed\', \'right_motor_speed\'])\n\n        V.add(left_motor, inputs=[\'left_motor_speed\'])\n        V.add(right_motor, inputs=[\'right_motor_speed\'])\n\n    elif cfg.DRIVE_TRAIN_TYPE == ""SERVO_HBRIDGE_PWM"":\n        from donkeycar.parts.actuator import ServoBlaster, PWMSteering\n        steering_controller = ServoBlaster(cfg.STEERING_CHANNEL) #really pin\n        #PWM pulse values should be in the range of 100 to 200\n        assert(cfg.STEERING_LEFT_PWM <= 200)\n        assert(cfg.STEERING_RIGHT_PWM <= 200)\n        steering = PWMSteering(controller=steering_controller,\n                                        left_pulse=cfg.STEERING_LEFT_PWM,\n                                        right_pulse=cfg.STEERING_RIGHT_PWM)\n\n\n        from donkeycar.parts.actuator import Mini_HBridge_DC_Motor_PWM\n        motor = Mini_HBridge_DC_Motor_PWM(cfg.HBRIDGE_PIN_FWD, cfg.HBRIDGE_PIN_BWD)\n\n        V.add(steering, inputs=[\'angle\'], threaded=True)\n        V.add(motor, inputs=[""throttle""])\n        \n    elif cfg.DRIVE_TRAIN_TYPE == ""MM1"":\n        from donkeycar.parts.robohat import RoboHATDriver\n        V.add(RoboHATDriver(cfg), inputs=[\'angle\', \'throttle\'])\n    \n    elif cfg.DRIVE_TRAIN_TYPE == ""PIGPIO_PWM"":\n        from donkeycar.parts.actuator import PWMSteering, PWMThrottle, PiGPIO_PWM\n        steering_controller = PiGPIO_PWM(cfg.STEERING_PWM_PIN, freq=cfg.STEERING_PWM_FREQ, inverted=cfg.STEERING_PWM_INVERTED)\n        steering = PWMSteering(controller=steering_controller,\n                                        left_pulse=cfg.STEERING_LEFT_PWM, \n                                        right_pulse=cfg.STEERING_RIGHT_PWM)\n        \n        throttle_controller = PiGPIO_PWM(cfg.THROTTLE_PWM_PIN, freq=cfg.THROTTLE_PWM_FREQ, inverted=cfg.THROTTLE_PWM_INVERTED)\n        throttle = PWMThrottle(controller=throttle_controller,\n                                            max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                            zero_pulse=cfg.THROTTLE_STOPPED_PWM, \n                                            min_pulse=cfg.THROTTLE_REVERSE_PWM)\n        V.add(steering, inputs=[\'angle\'], threaded=True)\n        V.add(throttle, inputs=[\'throttle\'], threaded=True)\n\n    # OLED setup\n    if cfg.USE_SSD1306_128_32:\n        from donkeycar.parts.oled import OLEDPart\n        auto_record_on_throttle = cfg.USE_JOYSTICK_AS_DEFAULT and cfg.AUTO_RECORD_ON_THROTTLE\n        oled_part = OLEDPart(cfg.SSD1306_128_32_I2C_BUSNUM, auto_record_on_throttle=auto_record_on_throttle)\n        V.add(oled_part, inputs=[\'recording\', \'tub/num_records\', \'user/mode\'], outputs=[], threaded=True)\n\n    #add tub to save data\n\n    inputs=[\'cam/image_array\',\n            \'user/angle\', \'user/throttle\',\n            \'user/mode\']\n\n    types=[\'image_array\',\n           \'float\', \'float\',\n           \'str\']\n\n    if cfg.TRAIN_BEHAVIORS:\n        inputs += [\'behavior/state\', \'behavior/label\', ""behavior/one_hot_state_array""]\n        types += [\'int\', \'str\', \'vector\']\n\n    if cfg.CAMERA_TYPE == ""D435"" and cfg.REALSENSE_D435_DEPTH:\n        inputs += [\'cam/depth_array\']\n        types += [\'gray16_array\']\n\n    if cfg.HAVE_IMU or (cfg.CAMERA_TYPE == ""D435"" and cfg.REALSENSE_D435_IMU):\n        inputs += [\'imu/acl_x\', \'imu/acl_y\', \'imu/acl_z\',\n            \'imu/gyr_x\', \'imu/gyr_y\', \'imu/gyr_z\']\n\n        types +=[\'float\', \'float\', \'float\',\n           \'float\', \'float\', \'float\']\n\n    if cfg.RECORD_DURING_AI:\n        inputs += [\'pilot/angle\', \'pilot/throttle\']\n        types += [\'float\', \'float\']\n\n    th = TubHandler(path=cfg.DATA_PATH)\n    tub = th.new_tub_writer(inputs=inputs, types=types, user_meta=meta)\n    V.add(tub, inputs=inputs, outputs=[""tub/num_records""], run_condition=\'recording\')\n\n    if cfg.PUB_CAMERA_IMAGES:\n        from donkeycar.parts.network import TCPServeValue\n        from donkeycar.parts.image import ImgArrToJpg\n        pub = TCPServeValue(""camera"")\n        V.add(ImgArrToJpg(), inputs=[\'cam/image_array\'], outputs=[\'jpg/bin\'])\n        V.add(pub, inputs=[\'jpg/bin\'])\n\n    if type(ctr) is LocalWebController:\n        if cfg.DONKEY_GYM:\n            print(""You can now go to http://localhost:%d to drive your car."" % cfg.WEB_CONTROL_PORT)\n        else:\n            print(""You can now go to <your hostname.local>:%d to drive your car."" % cfg.WEB_CONTROL_PORT)\n    elif isinstance(ctr, JoystickController):\n        print(""You can now move your joystick to drive your car."")\n        #tell the controller about the tub\n        ctr.set_tub(tub)\n\n        if cfg.BUTTON_PRESS_NEW_TUB:\n\n            def new_tub_dir():\n                V.parts.pop()\n                tub = th.new_tub_writer(inputs=inputs, types=types, user_meta=meta)\n                V.add(tub, inputs=inputs, outputs=[""tub/num_records""], run_condition=\'recording\')\n                ctr.set_tub(tub)\n\n            ctr.set_button_down_trigger(\'cross\', new_tub_dir)\n        ctr.print_controls()\n\n    #run the vehicle for 20 seconds\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ,\n            max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config(myconfig=args[\'--myconfig\'])\n\n    if args[\'drive\']:\n        model_type = args[\'--type\']\n        camera_type = args[\'--camera\']\n\n        drive(cfg, model_path=args[\'--model\'], use_joystick=args[\'--js\'],\n              model_type=model_type, camera_type=camera_type,\n              meta=args[\'--meta\'])\n\n    if args[\'train\']:\n        from train import multi_train, preprocessFileList\n\n        tub = args[\'--tub\']\n        model = args[\'--model\']\n        transfer = args[\'--transfer\']\n        model_type = args[\'--type\']\n        continuous = args[\'--continuous\']\n        aug = args[\'--aug\']\n        dirs = preprocessFileList( args[\'--file\'] )\n\n        if tub is not None:\n            tub_paths = [os.path.expanduser(n) for n in tub.split(\',\')]\n            dirs.extend( tub_paths )\n\n        if model_type is None:\n            model_type = cfg.DEFAULT_MODEL_TYPE\n            print(""using default model type of"", model_type)\n\n        multi_train(cfg, dirs, model, transfer, model_type, continuous, aug)\n\n'"
donkeycar/templates/cv_control.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey 2 car\n\nUsage:\n    manage.py (drive)\n\n\nOptions:\n    -h --help          Show this screen.    \n""""""\nimport os\nimport time\n\nfrom docopt import docopt\nimport numpy as np\n\nimport donkeycar as dk\nfrom donkeycar.parts.datastore import TubHandler\nfrom donkeycar.parts.camera import PiCamera\n\n\nclass MyCVController:\n    \'\'\'\n    CV based controller\n    \'\'\'\n\n    def run(self, cam_img):\n\n        #do image processing here. output variables steering and throttle to control vehicle.\n\n        steering = 0.0 # from zero to one\n        throttle = 0.2 # from -1 to 1\n        recording = False # Set to true if desired to save camera frames\n\n        return steering, throttle, recording\n\n\n\ndef drive(cfg):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n    \n    #Initialize car\n    V = dk.vehicle.Vehicle()\n\n    #Camera\n    cam = PiCamera(image_w=cfg.IMAGE_W, image_h=cfg.IMAGE_H, image_d=cfg.IMAGE_DEPTH)\n    V.add(cam, outputs=[\'cam/image_array\'], threaded=True)\n        \n    #Controller\n    V.add(MyCVController(), \n          inputs=[\'cam/image_array\'],\n          outputs=[\'steering\', \'throttle\', \'recording\'])\n\n       \n    #Sombrero\n    if cfg.HAVE_SOMBRERO:\n        from donkeycar.parts.sombrero import Sombrero\n        s = Sombrero()\n\n        \n    #Drive train setup\n\n    from donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\n\n    steering_controller = PCA9685(cfg.STEERING_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n    steering = PWMSteering(controller=steering_controller,\n                                    left_pulse=cfg.STEERING_LEFT_PWM, \n                                    right_pulse=cfg.STEERING_RIGHT_PWM)\n    \n    throttle_controller = PCA9685(cfg.THROTTLE_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n    throttle = PWMThrottle(controller=throttle_controller,\n                                    max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                    zero_pulse=cfg.THROTTLE_STOPPED_PWM, \n                                    min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\n    V.add(steering, inputs=[\'steering\'])\n    V.add(throttle, inputs=[\'throttle\'])\n    \n    #add tub to save data\n\n    inputs=[\'cam/image_array\',\n            \'steering\', \'throttle\']\n\n    types=[\'image_array\',\n           \'float\', \'float\']\n\n    th = TubHandler(path=cfg.DATA_PATH)\n    tub = th.new_tub_writer(inputs=inputs, types=types)\n    V.add(tub, inputs=inputs, outputs=[""tub/num_records""], run_condition=\'recording\')\n\n    #run the vehicle\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ, \n            max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n    \n    if args[\'drive\']:\n        drive(cfg)\n    \n'"
donkeycar/templates/just_drive.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey 2 car\n\nUsage:\n    manage.py (drive)\n\nOptions:\n    -h --help          Show this screen.\n""""""\nimport os\nimport time\n\nfrom docopt import docopt\n\nimport donkeycar as dk\nfrom donkeycar.parts.datastore import TubHandler\nfrom donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\n\n\ndef drive(cfg):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n\n    #Initialize car\n    V = dk.vehicle.Vehicle()\n    \n    class MyController:\n        \'\'\'\n        a simple controller class that outputs a constant steering and throttle.\n        \'\'\'\n        def run(self):\n            steering = 0.0\n            throttle = 0.1\n            return steering, throttle\n\n    V.add(MyController(), outputs=[\'angle\', \'throttle\'])\n\n    #Drive train setup\n    steering_controller = PCA9685(cfg.STEERING_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n    steering = PWMSteering(controller=steering_controller,\n                                    left_pulse=cfg.STEERING_LEFT_PWM, \n                                    right_pulse=cfg.STEERING_RIGHT_PWM)\n    \n    throttle_controller = PCA9685(cfg.THROTTLE_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n    throttle = PWMThrottle(controller=throttle_controller,\n                                    max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                    zero_pulse=cfg.THROTTLE_STOPPED_PWM, \n                                    min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\n    V.add(steering, inputs=[\'angle\'])\n    V.add(throttle, inputs=[\'throttle\'])\n    \n    #run the vehicle for 20 seconds\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ, \n            max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n    \n    if args[\'drive\']:\n        drive(cfg)\n'"
donkeycar/templates/myconfig.py,0,"b'# """""" \n# My CAR CONFIG \n\n# This file is read by your car application\'s manage.py script to change the car\n# performance\n\n# If desired, all config overrides can be specified here. \n# The update operation will not touch this file.\n# """"""\n\n'"
donkeycar/templates/path_follow.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to drive a donkey car using Intel T265\n\nUsage:\n    manage.py (drive) [--log=INFO]\n \n\nOptions:\n    -h --help          Show this screen.\n    --js               Use physical joystick.\n    -f --file=<file>   A text file containing paths to tub files, one per line. Option may be used more than once.\n    --meta=<key:value> Key/Value strings describing describing a piece of meta data about this drive. Option may be used more than once.\n""""""\nimport os\nimport sys\nimport time\nimport logging\nimport json\nfrom subprocess import Popen\nimport shlex\n\nfrom docopt import docopt\nimport numpy as np\nimport pigpio\n\nimport donkeycar as dk\nfrom donkeycar.parts.controller import WebFpv, get_js_controller\nfrom donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\nfrom donkeycar.parts.path import Path, PathPlot, CTE, PID_Pilot, PlotCircle, PImage, OriginOffset\nfrom donkeycar.parts.transform import PIDController\nfrom donkeycar.parts.pigpio_enc import PiPGIOEncoder, OdomDist\nfrom donkeycar.parts.realsense2 import RS_T265\n        \n\ndef drive(cfg):\n    \'\'\'\n    Construct a working robotic vehicle from many parts.\n    Each part runs as a job in the Vehicle loop, calling either\n    it\'s run or run_threaded method depending on the constructor flag `threaded`.\n    All parts are updated one after another at the framerate given in\n    cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner.\n    Parts may have named outputs and inputs. The framework handles passing named outputs\n    to parts requesting the same named input.\n    \'\'\'\n    \n    #Initialize car\n    V = dk.vehicle.Vehicle()\n\n    if cfg.HAVE_SOMBRERO:\n        from donkeycar.utils import Sombrero\n        s = Sombrero()\n   \n    ctr = get_js_controller(cfg)\n\n    V.add(ctr, \n          inputs=[\'null\'],\n          outputs=[\'user/angle\', \'user/throttle\', \'user/mode\', \'recording\'],\n          threaded=True)\n\n    if cfg.HAVE_ODOM:\n        pi = pigpio.pi()\n        enc = PiPGIOEncoder(cfg.ODOM_PIN, pi)\n        V.add(enc, outputs=[\'enc/ticks\'])\n\n        odom = OdomDist(mm_per_tick=cfg.MM_PER_TICK, debug=cfg.ODOM_DEBUG)\n        V.add(odom, inputs=[\'enc/ticks\', \'user/throttle\'], outputs=[\'enc/dist_m\', \'enc/vel_m_s\', \'enc/delta_vel_m_s\'])\n\n        if not os.path.exists(cfg.WHEEL_ODOM_CALIB):\n            print(""You must supply a json file when using odom with T265. There is a sample file in templates."")\n            print(""cp donkeycar/donkeycar/templates/calibration_odometry.json ."")\n            exit(1)\n\n    else:\n        # we give the T265 no calib to indicated we don\'t have odom\n        cfg.WHEEL_ODOM_CALIB = None\n\n        #This dummy part to satisfy input needs of RS_T265 part.\n        class NoOdom():\n            def run(self):\n                return 0.0\n\n        V.add(NoOdom(), outputs=[\'enc/vel_m_s\'])\n   \n    # This requires use of the Intel Realsense T265\n    rs = RS_T265(image_output=False, calib_filename=cfg.WHEEL_ODOM_CALIB)\n    V.add(rs, inputs=[\'enc/vel_m_s\'], outputs=[\'rs/pos\', \'rs/vel\', \'rs/acc\', \'rs/camera/left/img_array\'], threaded=True)\n\n    # Pull out the realsense T265 position stream, output 2d coordinates we can use to map.\n    class PosStream:\n        def run(self, pos):\n            #y is up, x is right, z is backwards/forwards\n            return pos.x, pos.z\n\n    V.add(PosStream(), inputs=[\'rs/pos\'], outputs=[\'pos/x\', \'pos/y\'])\n\n    # This part will reset the car back to the origin. You must put the car in the known origin\n    # and push the cfg.RESET_ORIGIN_BTN on your controller. This will allow you to induce an offset\n    # in the mapping.\n    origin_reset = OriginOffset()\n    V.add(origin_reset, inputs=[\'pos/x\', \'pos/y\'], outputs=[\'pos/x\', \'pos/y\'] )\n    ctr.set_button_down_trigger(cfg.RESET_ORIGIN_BTN, origin_reset.init_to_last)\n\n    class UserCondition:\n        def run(self, mode):\n            if mode == \'user\':\n                return True\n            else:\n                return False\n\n    V.add(UserCondition(), inputs=[\'user/mode\'], outputs=[\'run_user\'])\n\n    #See if we should even run the pilot module. \n    #This is only needed because the part run_condition only accepts boolean\n    class PilotCondition:\n        def run(self, mode):\n            if mode == \'user\':\n                return False\n            else:\n                return True\n\n    V.add(PilotCondition(), inputs=[\'user/mode\'], outputs=[\'run_pilot\'])\n\n    # This is the path object. It will record a path when distance changes and it travels\n    # at least cfg.PATH_MIN_DIST meters. Except when we are in follow mode, see below...\n    path = Path(min_dist=cfg.PATH_MIN_DIST)\n    V.add(path, inputs=[\'pos/x\', \'pos/y\'], outputs=[\'path\'], run_condition=\'run_user\')\n\n    # When a path is loaded, we will be in follow mode. We will not record.\n    path_loaded = False\n    if os.path.exists(cfg.PATH_FILENAME):\n        path.load(cfg.PATH_FILENAME)\n        path_loaded = True\n\n    def save_path():\n        path.save(cfg.PATH_FILENAME)\n        print(""saved path:"", cfg.PATH_FILENAME)\n\n    # Here\'s a trigger to save the path. Complete one circuit of your course, when you\n    # have exactly looped, or just shy of the loop, then save the path and shutdown\n    # this process. Restart and the path will be loaded.\n    ctr.set_button_down_trigger(cfg.SAVE_PATH_BTN, save_path)\n\n    # Here\'s an image we can map to.\n    img = PImage(clear_each_frame=True)\n    V.add(img, outputs=[\'map/image\'])\n\n    # This PathPlot will draw path on the image\n    plot = PathPlot(scale=cfg.PATH_SCALE, offset=cfg.PATH_OFFSET)\n    V.add(plot, inputs=[\'map/image\', \'path\'], outputs=[\'map/image\'])\n\n    # This will use path and current position to output cross track error\n    cte = CTE()\n    V.add(cte, inputs=[\'path\', \'pos/x\', \'pos/y\'], outputs=[\'cte/error\'], run_condition=\'run_pilot\')\n\n    # This will use the cross track error and PID constants to try to steer back towards the path.\n    pid = PIDController(p=cfg.PID_P, i=cfg.PID_I, d=cfg.PID_D)\n    pilot = PID_Pilot(pid, cfg.PID_THROTTLE)\n    V.add(pilot, inputs=[\'cte/error\'], outputs=[\'pilot/angle\', \'pilot/throttle\'], run_condition=""run_pilot"")\n\n    def dec_pid_d():\n        pid.Kd -= 0.5\n        logging.info(""pid: d- %f"" % pid.Kd)\n\n    def inc_pid_d():\n        pid.Kd += 0.5\n        logging.info(""pid: d+ %f"" % pid.Kd)\n\n    # Buttons to tune PID constants\n    ctr.set_button_down_trigger(""L2"", dec_pid_d)\n    ctr.set_button_down_trigger(""R2"", inc_pid_d)\n\n    # Plot a circle on the map where the car is located\n    loc_plot = PlotCircle(scale=cfg.PATH_SCALE, offset=cfg.PATH_OFFSET)\n    V.add(loc_plot, inputs=[\'map/image\', \'pos/x\', \'pos/y\'], outputs=[\'map/image\'])\n\n    #This web controller will create a web server. We aren\'t using any controls, just for visualization.\n    web_ctr = WebFpv()\n    V.add(web_ctr,\n          inputs=[\'map/image\'],\n          threaded=True)\n    \n\n    #Choose what inputs should change the car.\n    class DriveMode:\n        def run(self, mode, \n                    user_angle, user_throttle,\n                    pilot_angle, pilot_throttle):\n            if mode == \'user\':\n                #print(user_angle, user_throttle)\n                return user_angle, user_throttle\n            \n            elif mode == \'local_angle\':\n                return pilot_angle, user_throttle\n            \n            else: \n                return pilot_angle, pilot_throttle\n        \n    V.add(DriveMode(), \n          inputs=[\'user/mode\', \'user/angle\', \'user/throttle\',\n                  \'pilot/angle\', \'pilot/throttle\'], \n          outputs=[\'angle\', \'throttle\'])\n    \n\n    if not cfg.DONKEY_GYM:\n        steering_controller = PCA9685(cfg.STEERING_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n        steering = PWMSteering(controller=steering_controller,\n                                        left_pulse=cfg.STEERING_LEFT_PWM, \n                                        right_pulse=cfg.STEERING_RIGHT_PWM)\n        \n        throttle_controller = PCA9685(cfg.THROTTLE_CHANNEL, cfg.PCA9685_I2C_ADDR, busnum=cfg.PCA9685_I2C_BUSNUM)\n        throttle = PWMThrottle(controller=throttle_controller,\n                                        max_pulse=cfg.THROTTLE_FORWARD_PWM,\n                                        zero_pulse=cfg.THROTTLE_STOPPED_PWM, \n                                        min_pulse=cfg.THROTTLE_REVERSE_PWM)\n\n        V.add(steering, inputs=[\'angle\'])\n        V.add(throttle, inputs=[\'throttle\'])\n\n    # Print Joystick controls\n    ctr.print_controls()\n\n    if path_loaded:\n        print(""###############################################################################"")\n        print(""Loaded path:"", cfg.PATH_FILENAME)\n        print(""Make sure your car is sitting at the origin of the path."")\n        print(""View web page and refresh. You should see your path."")\n        print(""Hit \'select\' twice to change to ai drive mode."")\n        print(""Delete file"", cfg.PATH_FILENAME, ""and re-start"")\n        print(""to record a new path."")\n        print(""###############################################################################"")\n\n    else:\n        print(""###############################################################################"")\n        print(""You are now in record mode. Open the web page to your car"")\n        print(""and as you drive you should see a path."")\n        print(""Complete one circuit of your course."")\n        print(""When you have exactly looped, or just shy of the "")\n        print(""loop, then save the path (press %s)."" % cfg.SAVE_PATH_BTN)\n        print(""Close this process with Ctrl+C."")\n        print(""Place car exactly at the start."")\n        print(""Then restart the car with \'python manage drive\'."")\n        print(""It will reload the path and you will be ready to  "")\n        print(""follow the path using  \'select\' to change to ai drive mode."")\n        print(""###############################################################################"")\n\n    V.start(rate_hz=cfg.DRIVE_LOOP_HZ, \n        max_loop_count=cfg.MAX_LOOPS)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n\n    log_level = args[\'--log\'] or ""INFO""\n    numeric_level = getattr(logging, log_level.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError(\'Invalid log level: %s\' % log_level)\n    logging.basicConfig(level=numeric_level)\n\n    \n    if args[\'drive\']:\n        drive(cfg)\n'"
donkeycar/templates/square.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nWeb controller.\n\nThis example shows how a user use a web controller to controll\na square that move around the image frame.\n\n\nUsage:\n    manage.py (drive) [--model=<model>]\n    manage.py (train) [--tub=<tub1,tub2,..tubn>] (--model=<model>)\n\n""""""\n\n\nimport os\nfrom docopt import docopt\nimport donkeycar as dk \n\nfrom donkeycar.parts.datastore import TubGroup, TubHandler\nfrom donkeycar.parts.transform import Lambda\nfrom donkeycar.parts.simulation import SquareBoxCamera, MovingSquareTelemetry\nfrom donkeycar.parts.controller import LocalWebController\nfrom donkeycar.parts.keras import KerasCategorical\n\n\ndef drive(cfg, model_path=None):\n    V = dk.vehicle.Vehicle()\n    #initialize values\n    V.mem.put([\'square/angle\', \'square/throttle\'], (100,100))  \n    \n    #display square box given by cooridantes.\n    cam = SquareBoxCamera(resolution=(cfg.IMAGE_H, cfg.IMAGE_W))\n    V.add(cam, \n          inputs=[\'square/angle\', \'square/throttle\'],\n          outputs=[\'cam/image_array\'])\n    \n    #display the image and read user values from a local web controller\n    ctr = LocalWebController()\n    V.add(ctr, \n          inputs=[\'cam/image_array\'],\n          outputs=[\'user/angle\', \'user/throttle\', \n                   \'user/mode\', \'recording\'],\n          threaded=True)\n    \n    #See if we should even run the pilot module. \n    #This is only needed because the part run_contion only accepts boolean\n    def pilot_condition(mode):\n        if mode == \'user\':\n            return False\n        else:\n            return True\n        \n    pilot_condition_part = Lambda(pilot_condition)\n    V.add(pilot_condition_part, inputs=[\'user/mode\'], outputs=[\'run_pilot\'])\n    \n    #Run the pilot if the mode is not user.\n    kl = KerasCategorical()\n    if model_path:\n        kl.load(model_path)\n\n    V.add(kl, inputs=[\'cam/image_array\'],\n          outputs=[\'pilot/angle\', \'pilot/throttle\'],\n          run_condition=\'run_pilot\')\n    \n    \n    #See if we should even run the pilot module. \n    def drive_mode(mode, \n                   user_angle, user_throttle,\n                   pilot_angle, pilot_throttle):\n        if mode == \'user\':\n            return user_angle, user_throttle\n        \n        elif mode == \'pilot_angle\':\n            return pilot_angle, user_throttle\n        \n        else: \n            return pilot_angle, pilot_throttle\n        \n    drive_mode_part = Lambda(drive_mode)\n    V.add(drive_mode_part, \n          inputs=[\'user/mode\', \'user/angle\', \'user/throttle\',\n                  \'pilot/angle\', \'pilot/throttle\'], \n          outputs=[\'angle\', \'throttle\'])\n    \n    \n    \n    #transform angle and throttle values to coordinate values\n    f = lambda x : int(x * 100 + 100)\n    l = Lambda(f)\n    V.add(l, inputs=[\'user/angle\'], outputs=[\'square/angle\'])\n    V.add(l, inputs=[\'user/throttle\'], outputs=[\'square/throttle\'])\n    \n    #add tub to save data\n    inputs=[\'cam/image_array\',\n            \'user/angle\', \'user/throttle\', \n            \'pilot/angle\', \'pilot/throttle\', \n            \'square/angle\', \'square/throttle\',\n            \'user/mode\']\n    types=[\'image_array\',\n           \'float\', \'float\',  \n           \'float\', \'float\', \n           \'float\', \'float\',\n           \'str\']\n    \n    th = TubHandler(path=cfg.DATA_PATH)\n    tub = th.new_tub_writer(inputs=inputs, types=types)\n    V.add(tub, inputs=inputs, run_condition=\'recording\')\n    \n    #run the vehicle for 20 seconds\n    V.start(rate_hz=50, max_loop_count=10000)\n    \n    \n    \ndef train(cfg, tub_names, model_name):\n    \n    X_keys = [\'cam/image_array\']\n    y_keys = [\'user/angle\', \'user/throttle\']\n    \n    def rt(record):\n        record[\'user/angle\'] = dk.utils.linear_bin(record[\'user/angle\'])\n        return record\n\n    def combined_gen(gens):\n        import itertools\n        combined_gen = itertools.chain()\n        for gen in gens:\n            combined_gen = itertools.chain(combined_gen, gen)\n        return combined_gen\n    \n    kl = KerasCategorical()\n    print(\'tub_names\', tub_names)\n    if not tub_names:\n        tub_names = os.path.join(cfg.DATA_PATH, \'*\')\n    tubgroup = TubGroup(tub_names)\n    train_gen, val_gen = tubgroup.get_train_val_gen(X_keys, y_keys, record_transform=rt,\n                                                    batch_size=cfg.BATCH_SIZE,\n                                                    train_frac=cfg.TRAIN_TEST_SPLIT)\n\n    model_path = os.path.expanduser(model_name)\n\n    total_records = len(tubgroup.df)\n    total_train = int(total_records * cfg.TRAIN_TEST_SPLIT)\n    total_val = total_records - total_train\n    print(\'train: %d, validation: %d\' % (total_train, total_val))\n    steps_per_epoch = total_train // cfg.BATCH_SIZE\n    print(\'steps_per_epoch\', steps_per_epoch)\n\n    kl.train(train_gen,\n             val_gen,\n             saved_model_path=model_path,\n             steps=steps_per_epoch,\n             train_split=cfg.TRAIN_TEST_SPLIT)\n\n\n    \nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n    \n    if args[\'drive\']:\n        drive(cfg, args[\'--model\'])\n        \n    elif args[\'train\']:\n        tub = args[\'--tub\']\n        model = args[\'--model\']\n        train(cfg, tub, model)\n'"
donkeycar/templates/train.py,0,"b'#!/usr/bin/env python3\n""""""\nScripts to train a keras model using tensorflow.\nUses the data written by the donkey v2.2 tub writer,\nbut faster training with proper sampling of distribution over tubs. \nHas settings for continuous training that will look for new files as it trains. \nModify on_best_model if you wish continuous training to update your pi as it builds.\nYou can drop this in your ~/mycar dir.\nBasic usage should feel familiar: python train.py --model models/mypilot\n\n\nUsage:\n    train.py [--tub=<tub1,tub2,..tubn>] [--file=<file> ...] (--model=<model>) [--transfer=<model>] [--type=(linear|latent|categorical|rnn|imu|behavior|3d|look_ahead|tensorrt_linear|tflite_linear|coral_tflite_linear)] [--figure_format=<figure_format>] [--continuous] [--aug]\n\nOptions:\n    -h --help              Show this screen.\n    -f --file=<file>       A text file containing paths to tub files, one per line. Option may be used more than once.\n    --figure_format=png    The file format of the generated figure (see https://matplotlib.org/api/_as_gen/matplotlib.pyplot.savefig.html), e.g. \'png\', \'pdf\', \'svg\', ...\n""""""\nimport os\nimport glob\nimport random\nimport json\nimport time\nimport zlib\nfrom os.path import basename, join, splitext, dirname\nimport pickle\nimport datetime\n\nfrom tensorflow.python import keras\nfrom docopt import docopt\nimport numpy as np\nfrom PIL import Image\n\nimport donkeycar as dk\nfrom donkeycar.parts.datastore import Tub\nfrom donkeycar.parts.keras import KerasLinear, KerasIMU,\\\n     KerasCategorical, KerasBehavioral, Keras3D_CNN,\\\n     KerasRNN_LSTM, KerasLatent, KerasLocalizer\nfrom donkeycar.parts.augment import augment_image\nfrom donkeycar.utils import *\n\nfigure_format = \'png\'\n\n\n\'\'\'\nmatplotlib can be a pain to setup on a Mac. So handle the case where it is absent. When present,\nuse it to generate a plot of training results.\n\'\'\'\ntry:\n    import matplotlib.pyplot as plt\n    do_plot = True\nexcept:\n    do_plot = False\n    print(""matplotlib not installed"")\n    \n\n\'\'\'\nTub management\n\'\'\'\ndef make_key(sample):\n    tub_path = sample[\'tub_path\']\n    index = sample[\'index\']\n    return tub_path + str(index)\n\ndef make_next_key(sample, index_offset):\n    tub_path = sample[\'tub_path\']\n    index = sample[\'index\'] + index_offset\n    return tub_path + str(index)\n\n\ndef collate_records(records, gen_records, opts):\n    \'\'\'\n    open all the .json records from records list passed in,\n    read their contents,\n    add them to a list of gen_records, passed in.\n    use the opts dict to specify config choices\n    \'\'\'\n\n    new_records = {}\n    \n    for record_path in records:\n\n        basepath = os.path.dirname(record_path)        \n        index = get_record_index(record_path)\n        sample = { \'tub_path\' : basepath, ""index"" : index }\n             \n        key = make_key(sample)\n\n        if key in gen_records:\n            continue\n\n        try:\n            with open(record_path, \'r\') as fp:\n                json_data = json.load(fp)\n        except:\n            continue\n\n        image_filename = json_data[""cam/image_array""]\n        image_path = os.path.join(basepath, image_filename)\n\n        sample[\'record_path\'] = record_path\n        sample[""image_path""] = image_path\n        sample[""json_data""] = json_data        \n\n        angle = float(json_data[\'user/angle\'])\n        throttle = float(json_data[""user/throttle""])\n\n        if opts[\'categorical\']:\n            angle = dk.utils.linear_bin(angle)\n            throttle = dk.utils.linear_bin(throttle, N=20, offset=0, R=opts[\'cfg\'].MODEL_CATEGORICAL_MAX_THROTTLE_RANGE)\n\n        sample[\'angle\'] = angle\n        sample[\'throttle\'] = throttle\n\n        try:\n            accl_x = float(json_data[\'imu/acl_x\'])\n            accl_y = float(json_data[\'imu/acl_y\'])\n            accl_z = float(json_data[\'imu/acl_z\'])\n\n            gyro_x = float(json_data[\'imu/gyr_x\'])\n            gyro_y = float(json_data[\'imu/gyr_y\'])\n            gyro_z = float(json_data[\'imu/gyr_z\'])\n\n            sample[\'imu_array\'] = np.array([accl_x, accl_y, accl_z, gyro_x, gyro_y, gyro_z])\n        except:\n            pass\n\n        try:\n            behavior_arr = np.array(json_data[\'behavior/one_hot_state_array\'])\n            sample[""behavior_arr""] = behavior_arr\n        except:\n            pass\n\n        try:\n            location_arr = np.array(json_data[\'location/one_hot_state_array\'])\n            sample[""location""] = location_arr\n        except:\n            pass\n\n\n        sample[\'img_data\'] = None\n\n        # Initialise \'train\' to False\n        sample[\'train\'] = False\n        \n        # We need to maintain the correct train - validate ratio across the dataset, even if continous training\n        # so don\'t add this sample to the main records list (gen_records) yet.\n        new_records[key] = sample\n        \n    # new_records now contains all our NEW samples\n    # - set a random selection to be the training samples based on the ratio in CFG file\n    shufKeys = list(new_records.keys())\n    random.shuffle(shufKeys)\n    trainCount = 0\n    #  Ratio of samples to use as training data, the remaining are used for evaluation\n    targetTrainCount = int(opts[\'cfg\'].TRAIN_TEST_SPLIT * len(shufKeys))\n    for key in shufKeys:\n        new_records[key][\'train\'] = True\n        trainCount += 1\n        if trainCount >= targetTrainCount:\n            break\n    # Finally add all the new records to the existing list\n    gen_records.update(new_records)\n\ndef save_json_and_weights(model, filename):\n    \'\'\'\n    given a keras model and a .h5 filename, save the model file\n    in the json format and the weights file in the h5 format\n    \'\'\'\n    if not \'.h5\' == filename[-3:]:\n        raise Exception(""Model filename should end with .h5"")\n\n    arch = model.to_json()\n    json_fnm = filename[:-2] + ""json""\n    weights_fnm = filename[:-2] + ""weights""\n\n    with open(json_fnm, ""w"") as outfile:\n        parsed = json.loads(arch)\n        arch_pretty = json.dumps(parsed, indent=4, sort_keys=True)\n        outfile.write(arch_pretty)\n\n    model.save_weights(weights_fnm)\n    return json_fnm, weights_fnm\n\n\nclass MyCPCallback(keras.callbacks.ModelCheckpoint):\n    \'\'\'\n    custom callback to interact with best val loss during continuous training\n    \'\'\'\n\n    def __init__(self, send_model_cb=None, cfg=None, *args, **kwargs):\n        super(MyCPCallback, self).__init__(*args, **kwargs)\n        self.reset_best_end_of_epoch = False\n        self.send_model_cb = send_model_cb\n        self.last_modified_time = None\n        self.cfg = cfg\n\n    def reset_best(self):\n        self.reset_best_end_of_epoch = True\n\n    def on_epoch_end(self, epoch, logs=None):\n        super(MyCPCallback, self).on_epoch_end(epoch, logs)\n\n        if self.send_model_cb:\n            \'\'\'\n            check whether the file changed and send to the pi\n            \'\'\'\n            filepath = self.filepath.format(epoch=epoch, **logs)\n            if os.path.exists(filepath):\n                last_modified_time = os.path.getmtime(filepath)\n                if self.last_modified_time is None or self.last_modified_time < last_modified_time:\n                    self.last_modified_time = last_modified_time\n                    self.send_model_cb(self.cfg, self.model, filepath)\n\n        \'\'\'\n        when reset best is set, we want to make sure to run an entire epoch\n        before setting our new best on the new total records\n        \'\'\'        \n        if self.reset_best_end_of_epoch:\n            self.reset_best_end_of_epoch = False\n            self.best = np.Inf\n        \n\ndef on_best_model(cfg, model, model_filename):\n\n    model.save(model_filename, include_optimizer=False)\n        \n    if not cfg.SEND_BEST_MODEL_TO_PI:\n        return\n\n    on_windows = os.name == \'nt\'\n\n    #If we wish, send the best model to the pi.\n    #On mac or linux we have scp:\n    if not on_windows:\n        print(\'sending model to the pi\')\n        \n        command = \'scp %s %s@%s:~/%s/models/;\' % (model_filename, cfg.PI_USERNAME, cfg.PI_HOSTNAME, cfg.PI_DONKEY_ROOT)\n    \n        print(""sending"", command)\n        res = os.system(command)\n        print(res)\n\n    else: #yes, we are on windows machine\n\n        #On windoz no scp. In order to use this you must first setup\n        #an ftp daemon on the pi. ie. sudo apt-get install vsftpd\n        #and then make sure you enable write permissions in the conf\n        try:\n            import paramiko\n        except:\n            raise Exception(""first install paramiko: pip install paramiko"")\n\n        host = cfg.PI_HOSTNAME\n        username = cfg.PI_USERNAME\n        password = cfg.PI_PASSWD\n        server = host\n        files = []\n\n        localpath = model_filename\n        remotepath = \'/home/%s/%s/%s\' %(username, cfg.PI_DONKEY_ROOT, model_filename.replace(\'\\\\\', \'/\'))\n        files.append((localpath, remotepath))\n\n        print(""sending"", files)\n\n        try:\n            ssh = paramiko.SSHClient()\n            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n            ssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))\n            ssh.connect(server, username=username, password=password)\n            sftp = ssh.open_sftp()\n        \n            for localpath, remotepath in files:\n                sftp.put(localpath, remotepath)\n\n            sftp.close()\n            ssh.close()\n            print(""send succeded"")\n        except:\n            print(""send failed"")\n    \n\ndef train(cfg, tub_names, model_name, transfer_model, model_type, continuous, aug):\n    \'\'\'\n    use the specified data in tub_names to train an artifical neural network\n    saves the output trained model as model_name\n    \'\'\' \n    verbose = cfg.VERBOSE_TRAIN\n\n    if model_type is None:\n        model_type = cfg.DEFAULT_MODEL_TYPE\n\n    if ""tflite"" in model_type:\n        #even though we are passed the .tflite output file, we train with an intermediate .h5\n        #output and then convert to final .tflite at the end.\n        assert("".tflite"" in model_name)\n        #we only support the linear model type right now for tflite\n        assert(""linear"" in model_type)\n        model_name = model_name.replace("".tflite"", "".h5"")\n    elif ""tensorrt"" in model_type:\n        #even though we are passed the .uff output file, we train with an intermediate .h5\n        #output and then convert to final .uff at the end.\n        assert("".uff"" in model_name)\n        #we only support the linear model type right now for tensorrt\n        assert(""linear"" in model_type)\n        model_name = model_name.replace("".uff"", "".h5"")\n\n    if model_name and not \'.h5\' == model_name[-3:]:\n        raise Exception(""Model filename should end with .h5"")\n    \n    if continuous:\n        print(""continuous training"")\n    \n    gen_records = {}\n    opts = { \'cfg\' : cfg}\n\n    if ""linear"" in model_type:\n        train_type = ""linear""\n    else:\n        train_type = model_type\n\n    kl = get_model_by_type(train_type, cfg=cfg)\n\n    opts[\'categorical\'] = type(kl) in [KerasCategorical, KerasBehavioral]\n\n    print(\'training with model type\', type(kl))\n\n    if transfer_model:\n        print(\'loading weights from model\', transfer_model)\n        kl.load(transfer_model)\n\n        #when transfering models, should we freeze all but the last N layers?\n        if cfg.FREEZE_LAYERS:\n            num_to_freeze = len(kl.model.layers) - cfg.NUM_LAST_LAYERS_TO_TRAIN \n            print(\'freezing %d layers\' % num_to_freeze)           \n            for i in range(num_to_freeze):\n                kl.model.layers[i].trainable = False        \n\n    if cfg.OPTIMIZER:\n        kl.set_optimizer(cfg.OPTIMIZER, cfg.LEARNING_RATE, cfg.LEARNING_RATE_DECAY)\n\n    kl.compile()\n\n    if cfg.PRINT_MODEL_SUMMARY:\n        print(kl.model.summary())\n    \n    opts[\'keras_pilot\'] = kl\n    opts[\'continuous\'] = continuous\n    opts[\'model_type\'] = model_type\n\n    extract_data_from_pickles(cfg, tub_names)\n\n    records = gather_records(cfg, tub_names, opts, verbose=True)\n    print(\'collating %d records ...\' % (len(records)))\n    collate_records(records, gen_records, opts)\n\n    def generator(save_best, opts, data, batch_size, isTrainSet=True, min_records_to_train=1000):\n        \n        num_records = len(data)\n\n        while True:\n\n            if isTrainSet and opts[\'continuous\']:\n                \'\'\'\n                When continuous training, we look for new records after each epoch.\n                This will add new records to the train and validation set.\n                \'\'\'\n                records = gather_records(cfg, tub_names, opts)\n                if len(records) > num_records:\n                    collate_records(records, gen_records, opts)\n                    new_num_rec = len(data)\n                    if new_num_rec > num_records:\n                        print(\'picked up\', new_num_rec - num_records, \'new records!\')\n                        num_records = new_num_rec \n                        save_best.reset_best()\n                if num_records < min_records_to_train:\n                    print(""not enough records to train. need %d, have %d. waiting..."" % (min_records_to_train, num_records))\n                    time.sleep(10)\n                    continue\n\n            batch_data = []\n\n            keys = list(data.keys())\n\n            random.shuffle(keys)\n\n            kl = opts[\'keras_pilot\']\n\n            if type(kl.model.output) is list:\n                model_out_shape = (2, 1)\n            else:\n                model_out_shape = kl.model.output.shape\n\n            if type(kl.model.input) is list:\n                model_in_shape = (2, 1)\n            else:    \n                model_in_shape = kl.model.input.shape\n\n            has_imu = type(kl) is KerasIMU\n            has_bvh = type(kl) is KerasBehavioral\n            img_out = type(kl) is KerasLatent\n            loc_out = type(kl) is KerasLocalizer\n            \n            if img_out:\n                import cv2\n\n            for key in keys:\n\n                if not key in data:\n                    continue\n\n                _record = data[key]\n\n                if _record[\'train\'] != isTrainSet:\n                    continue\n\n                if continuous:\n                    #in continuous mode we need to handle files getting deleted\n                    filename = _record[\'image_path\']\n                    if not os.path.exists(filename):\n                        data.pop(key, None)\n                        continue\n\n                batch_data.append(_record)\n\n                if len(batch_data) == batch_size:\n                    inputs_img = []\n                    inputs_imu = []\n                    inputs_bvh = []\n                    angles = []\n                    throttles = []\n                    out_img = []\n                    out_loc = []\n                    out = []\n\n                    for record in batch_data:\n                        #get image data if we don\'t already have it\n                        if record[\'img_data\'] is None:\n                            filename = record[\'image_path\']\n                            \n                            img_arr = load_scaled_image_arr(filename, cfg)\n\n                            if img_arr is None:\n                                break\n                            \n                            if aug:\n                                img_arr = augment_image(img_arr)\n\n                            if cfg.CACHE_IMAGES:\n                                record[\'img_data\'] = img_arr\n                        else:\n                            img_arr = record[\'img_data\']\n                            \n                        if img_out:                            \n                            rz_img_arr = cv2.resize(img_arr, (127, 127)) / 255.0\n                            out_img.append(rz_img_arr[:,:,0].reshape((127, 127, 1)))\n\n                        if loc_out:\n                            out_loc.append(record[\'location\'])\n                            \n                        if has_imu:\n                            inputs_imu.append(record[\'imu_array\'])\n                        \n                        if has_bvh:\n                            inputs_bvh.append(record[\'behavior_arr\'])\n\n                        inputs_img.append(img_arr)\n                        angles.append(record[\'angle\'])\n                        throttles.append(record[\'throttle\'])\n                        out.append([record[\'angle\'], record[\'throttle\']])\n\n                    if img_arr is None:\n                        continue\n\n                    img_arr = np.array(inputs_img).reshape(batch_size,\\\n                        cfg.TARGET_H, cfg.TARGET_W, cfg.TARGET_D)\n\n                    if has_imu:\n                        X = [img_arr, np.array(inputs_imu)]\n                    elif has_bvh:\n                        X = [img_arr, np.array(inputs_bvh)]\n                    else:\n                        X = [img_arr]\n\n                    if img_out:\n                        y = [out_img, np.array(angles), np.array(throttles)]\n                    elif out_loc:\n                        y = [ np.array(angles), np.array(throttles), np.array(out_loc)]\n                    elif model_out_shape[1] == 2:\n                        y = [np.array([out]).reshape(batch_size, 2) ]\n                    else:\n                        y = [np.array(angles), np.array(throttles)]\n\n                    yield X, y\n\n                    batch_data = []\n    \n    model_path = os.path.expanduser(model_name)\n\n    \n    #checkpoint to save model after each epoch and send best to the pi.\n    save_best = MyCPCallback(send_model_cb=on_best_model,\n                                    filepath=model_path,\n                                    monitor=\'val_loss\', \n                                    verbose=verbose, \n                                    save_best_only=True, \n                                    mode=\'min\',\n                                    cfg=cfg)\n\n    train_gen = generator(save_best, opts, gen_records, cfg.BATCH_SIZE, True)\n    val_gen = generator(save_best, opts, gen_records, cfg.BATCH_SIZE, False)\n    \n    total_records = len(gen_records)\n\n    num_train = 0\n    num_val = 0\n\n    for key, _record in gen_records.items():\n        if _record[\'train\'] == True:\n            num_train += 1\n        else:\n            num_val += 1\n\n    print(""train: %d, val: %d"" % (num_train, num_val))\n    print(\'total records: %d\' %(total_records))\n    \n    if not continuous:\n        steps_per_epoch = num_train // cfg.BATCH_SIZE\n    else:\n        steps_per_epoch = 100\n    \n    val_steps = num_val // cfg.BATCH_SIZE\n    print(\'steps_per_epoch\', steps_per_epoch)\n\n    cfg.model_type = model_type\n\n    go_train(kl, cfg, train_gen, val_gen, gen_records, model_name, steps_per_epoch, val_steps, continuous, verbose, save_best)\n\n    \n    \ndef go_train(kl, cfg, train_gen, val_gen, gen_records, model_name, steps_per_epoch, val_steps, continuous, verbose, save_best=None):\n\n    start = time.time()\n\n    model_path = os.path.expanduser(model_name)\n\n    #checkpoint to save model after each epoch and send best to the pi.\n    if save_best is None:\n        save_best = MyCPCallback(send_model_cb=on_best_model,\n                                    filepath=model_path,\n                                    monitor=\'val_loss\', \n                                    verbose=verbose, \n                                    save_best_only=True, \n                                    mode=\'min\',\n                                    cfg=cfg)\n\n    #stop training if the validation error stops improving.\n    early_stop = keras.callbacks.EarlyStopping(monitor=\'val_loss\', \n                                                min_delta=cfg.MIN_DELTA, \n                                                patience=cfg.EARLY_STOP_PATIENCE, \n                                                verbose=verbose, \n                                                mode=\'auto\')\n\n    if steps_per_epoch < 2:\n        raise Exception(""Too little data to train. Please record more records."")\n\n    if continuous:\n        epochs = 100000\n    else:\n        epochs = cfg.MAX_EPOCHS\n\n    workers_count = 1\n    use_multiprocessing = False\n\n    callbacks_list = [save_best]\n\n    if cfg.USE_EARLY_STOP and not continuous:\n        callbacks_list.append(early_stop)\n\n    history = kl.model.fit_generator(\n                    train_gen, \n                    steps_per_epoch=steps_per_epoch, \n                    epochs=epochs, \n                    verbose=cfg.VERBOSE_TRAIN,\n                    validation_data=val_gen,\n                    callbacks=callbacks_list, \n                    validation_steps=val_steps,\n                    workers=workers_count,\n                    use_multiprocessing=use_multiprocessing)\n                    \n    full_model_val_loss = min(history.history[\'val_loss\'])\n    max_val_loss = full_model_val_loss + cfg.PRUNE_VAL_LOSS_DEGRADATION_LIMIT\n\n    duration_train = time.time() - start\n    print(""Training completed in %s."" % str(datetime.timedelta(seconds=round(duration_train))) )\n\n    print(""\\n\\n----------- Best Eval Loss :%f ---------"" % save_best.best)\n\n    if cfg.SHOW_PLOT:\n        try:\n            if do_plot:\n                plt.figure(1)\n\n                # Only do accuracy if we have that data (e.g. categorical outputs)\n                if \'angle_out_acc\' in history.history:\n                    plt.subplot(121)\n\n                # summarize history for loss\n                plt.plot(history.history[\'loss\'])\n                plt.plot(history.history[\'val_loss\'])\n                plt.title(\'model loss\')\n                plt.ylabel(\'loss\')\n                plt.xlabel(\'epoch\')\n                plt.legend([\'train\', \'validate\'], loc=\'upper right\')\n                \n                # summarize history for acc\n                if \'angle_out_acc\' in history.history:\n                    plt.subplot(122)\n                    plt.plot(history.history[\'angle_out_acc\'])\n                    plt.plot(history.history[\'val_angle_out_acc\'])\n                    plt.title(\'model angle accuracy\')\n                    plt.ylabel(\'acc\')\n                    plt.xlabel(\'epoch\')\n                    #plt.legend([\'train\', \'validate\'], loc=\'upper left\')\n\n                plt.savefig(model_path + \'_loss_acc_%f.%s\' % (save_best.best, figure_format))\n                plt.show()\n            else:\n                print(""not saving loss graph because matplotlib not set up."")\n        except Exception as ex:\n            print(""problems with loss graph: {}"".format( ex ) )\n\n    #Save tflite, optionally in the int quant format for Coral TPU\n    if ""tflite"" in cfg.model_type:\n        print(""\\n\\n--------- Saving TFLite Model ---------"")\n        tflite_fnm = model_path.replace("".h5"", "".tflite"")\n        assert("".tflite"" in tflite_fnm)\n\n        prepare_for_coral = ""coral"" in cfg.model_type\n\n        if prepare_for_coral:\n            #compile a list of records to calibrate the quantization\n            data_list = []\n            max_items = 1000\n            for key, _record in gen_records.items():\n                data_list.append(_record)\n                if len(data_list) == max_items:\n                    break   \n\n            stride = 1\n            num_calibration_steps = len(data_list) // stride\n\n            #a generator function to help train the quantizer with the expected range of data from inputs\n            def representative_dataset_gen():\n                start = 0\n                end = stride\n                for _ in range(num_calibration_steps):\n                    batch_data = data_list[start:end]\n                    inputs = []\n                \n                    for record in batch_data:\n                        filename = record[\'image_path\']                        \n                        img_arr = load_scaled_image_arr(filename, cfg)\n                        inputs.append(img_arr)\n\n                    start += stride\n                    end += stride\n\n                    # Get sample input data as a numpy array in a method of your choosing.\n                    yield [ np.array(inputs, dtype=np.float32).reshape(stride, cfg.TARGET_H, cfg.TARGET_W, cfg.TARGET_D) ]\n        else:\n            representative_dataset_gen = None\n\n        from donkeycar.parts.tflite import keras_model_to_tflite\n        keras_model_to_tflite(model_path, tflite_fnm, representative_dataset_gen)\n        print(""Saved TFLite model:"", tflite_fnm)\n        if prepare_for_coral:\n            print(""compile for Coral w: edgetpu_compiler"", tflite_fnm)\n            os.system(""edgetpu_compiler "" + tflite_fnm)\n\n    #Save tensorrt\n    if ""tensorrt"" in cfg.model_type:\n        print(""\\n\\n--------- Saving TensorRT Model ---------"")\n        # TODO RAHUL\n        # flatten model_path\n        # convert to uff\n        # print(""Saved TensorRT model:"", uff_filename)\n\n    \ndef sequence_train(cfg, tub_names, model_name, transfer_model, model_type, continuous, aug):\n    \'\'\'\n    use the specified data in tub_names to train an artifical neural network\n    saves the output trained model as model_name\n    trains models which take sequence of images\n    \'\'\'\n    assert(not continuous)\n\n    print(""sequence of images training"")    \n\n    kl = dk.utils.get_model_by_type(model_type=model_type, cfg=cfg)\n    \n    if cfg.PRINT_MODEL_SUMMARY:\n        print(kl.model.summary())\n    \n    tubs = gather_tubs(cfg, tub_names)\n    \n    verbose = cfg.VERBOSE_TRAIN\n\n    records = []\n\n    for tub in tubs:\n        record_paths = glob.glob(os.path.join(tub.path, \'record_*.json\'))\n        print(""Tub:"", tub.path, ""has"", len(record_paths), \'records\')\n\n        record_paths.sort(key=get_record_index)\n        records += record_paths\n\n\n    print(\'collating records\')\n    gen_records = {}\n\n    for record_path in records:\n\n        with open(record_path, \'r\') as fp:\n            json_data = json.load(fp)\n\n        basepath = os.path.dirname(record_path)\n        image_filename = json_data[""cam/image_array""]\n        image_path = os.path.join(basepath, image_filename)\n        sample = { \'record_path\' : record_path, ""image_path"" : image_path, ""json_data"" : json_data }\n\n        sample[""tub_path""] = basepath\n        sample[""index""] = get_image_index(image_filename)\n\n        angle = float(json_data[\'user/angle\'])\n        throttle = float(json_data[""user/throttle""])\n\n        sample[\'target_output\'] = np.array([angle, throttle])\n        sample[\'angle\'] = angle\n        sample[\'throttle\'] = throttle\n\n\n        sample[\'img_data\'] = None\n\n        key = make_key(sample)\n\n        gen_records[key] = sample\n\n\n\n    print(\'collating sequences\')\n\n    sequences = []\n    \n    target_len = cfg.SEQUENCE_LENGTH\n    look_ahead = False\n    \n    if model_type == ""look_ahead"":\n        target_len = cfg.SEQUENCE_LENGTH * 2\n        look_ahead = True\n\n    for k, sample in gen_records.items():\n\n        seq = []\n\n        for i in range(target_len):\n            key = make_next_key(sample, i)\n            if key in gen_records:\n                seq.append(gen_records[key])\n            else:\n                continue\n\n        if len(seq) != target_len:\n            continue\n\n        sequences.append(seq)\n\n    print(""collated"", len(sequences), ""sequences of length"", target_len)\n\n    #shuffle and split the data\n    train_data, val_data  = train_test_split(sequences, test_size=(1 - cfg.TRAIN_TEST_SPLIT))\n\n\n    def generator(data, opt, batch_size=cfg.BATCH_SIZE):\n        num_records = len(data)\n\n        while True:\n            #shuffle again for good measure\n            random.shuffle(data)\n\n            for offset in range(0, num_records, batch_size):\n                batch_data = data[offset:offset+batch_size]\n\n                if len(batch_data) != batch_size:\n                    break\n\n                b_inputs_img = []\n                b_vec_in = []\n                b_labels = []\n                b_vec_out = []\n\n                for seq in batch_data:\n                    inputs_img = []\n                    vec_in = []\n                    labels = []\n                    vec_out = []\n                    num_images_target = len(seq)\n                    iTargetOutput = -1\n                    if opt[\'look_ahead\']:\n                        num_images_target = cfg.SEQUENCE_LENGTH\n                        iTargetOutput = cfg.SEQUENCE_LENGTH - 1\n\n                    for iRec, record in enumerate(seq):\n                        #get image data if we don\'t already have it\n                        if len(inputs_img) < num_images_target:\n                            if record[\'img_data\'] is None:\n                                img_arr = load_scaled_image_arr(record[\'image_path\'], cfg)\n                                if img_arr is None:\n                                    break\n                                if aug:\n                                    img_arr = augment_image(img_arr)\n                                \n                                if cfg.CACHE_IMAGES:\n                                    record[\'img_data\'] = img_arr\n                            else:\n                                img_arr = record[\'img_data\']                  \n                                \n                            inputs_img.append(img_arr)\n\n                        if iRec >= iTargetOutput:\n                            vec_out.append(record[\'angle\'])\n                            vec_out.append(record[\'throttle\'])\n                        else:\n                            vec_in.append(0.0) #record[\'angle\'])\n                            vec_in.append(0.0) #record[\'throttle\'])\n                        \n                    label_vec = seq[iTargetOutput][\'target_output\']\n\n                    if look_ahead:\n                        label_vec = np.array(vec_out)\n\n                    labels.append(label_vec)\n\n                    b_inputs_img.append(inputs_img)\n                    b_vec_in.append(vec_in)\n\n                    b_labels.append(labels)\n\n                \n                if look_ahead:\n                    X = [np.array(b_inputs_img).reshape(batch_size,\\\n                        cfg.TARGET_H, cfg.TARGET_W, cfg.SEQUENCE_LENGTH)]\n                    X.append(np.array(b_vec_in))\n                    y = np.array(b_labels).reshape(batch_size, (cfg.SEQUENCE_LENGTH + 1) * 2)\n                else:\n                    X = [np.array(b_inputs_img).reshape(batch_size,\\\n                        cfg.SEQUENCE_LENGTH, cfg.TARGET_H, cfg.TARGET_W, cfg.TARGET_D)]\n                    y = np.array(b_labels).reshape(batch_size, 2)\n\n                yield X, y\n\n    opt = { \'look_ahead\' : look_ahead, \'cfg\' : cfg }\n\n    train_gen = generator(train_data, opt)\n    val_gen = generator(val_data, opt)   \n\n    model_path = os.path.expanduser(model_name)\n\n    total_records = len(sequences)\n    total_train = len(train_data)\n    total_val = len(val_data)\n\n    print(\'train: %d, validation: %d\' %(total_train, total_val))\n    steps_per_epoch = total_train // cfg.BATCH_SIZE\n    val_steps = total_val // cfg.BATCH_SIZE\n    print(\'steps_per_epoch\', steps_per_epoch)\n\n    if steps_per_epoch < 2:\n        raise Exception(""Too little data to train. Please record more records."")\n    \n    cfg.model_type = model_type\n\n    go_train(kl, cfg, train_gen, val_gen, gen_records, model_name, steps_per_epoch, val_steps, continuous, verbose)\n    \n    \'\'\' \n    kl.train(train_gen, \n        val_gen, \n        saved_model_path=model_path,\n        steps=steps_per_epoch,\n        train_split=cfg.TRAIN_TEST_SPLIT,\n        use_early_stop = cfg.USE_EARLY_STOP)\n    \'\'\'\n\n\ndef multi_train(cfg, tub, model, transfer, model_type, continuous, aug):\n    \'\'\'\n    choose the right regime for the given model type\n    \'\'\'\n    train_fn = train\n    if model_type in (""rnn"",\'3d\',\'look_ahead\'):\n        train_fn = sequence_train\n\n    train_fn(cfg, tub, model, transfer, model_type, continuous, aug)\n\n\ndef prune(model, validation_generator, val_steps, cfg):\n    percent_pruning = float(cfg.PRUNE_PERCENT_PER_ITERATION)\n    total_channels = get_total_channels(model)\n    n_channels_delete = int(math.floor(percent_pruning / 100 * total_channels))\n\n    apoz_df = get_model_apoz(model, validation_generator)\n\n    model = prune_model(model, apoz_df, n_channels_delete)\n\n    name = \'{}/model_pruned_{}_percent.h5\'.format(cfg.MODELS_PATH, percent_pruning)\n\n    model.save(name)\n\n    return model, n_channels_delete\n\n\ndef extract_data_from_pickles(cfg, tubs):\n    """"""\n    Extracts record_{id}.json and image from a pickle with the same id if exists in the tub.\n    Then writes extracted json/jpg along side the source pickle that tub.\n    This assumes the format {id}.pickle in the tub directory.\n    :param cfg: config with data location configuration. Generally the global config object.\n    :param tubs: The list of tubs involved in training.\n    :return: implicit None.\n    """"""\n    t_paths = gather_tub_paths(cfg, tubs)\n    for tub_path in t_paths:\n        file_paths = glob.glob(join(tub_path, \'*.pickle\'))\n        print(\'found {} pickles writing json records and images in tub {}\'.format(len(file_paths), tub_path))\n        for file_path in file_paths:\n            # print(\'loading data from {}\'.format(file_paths))\n            with open(file_path, \'rb\') as f:\n                p = zlib.decompress(f.read())\n            data = pickle.loads(p)\n           \n            base_path = dirname(file_path)\n            filename = splitext(basename(file_path))[0]\n            image_path = join(base_path, filename + \'.jpg\')\n            img = Image.fromarray(np.uint8(data[\'val\'][\'cam/image_array\']))\n            img.save(image_path)\n            \n            data[\'val\'][\'cam/image_array\'] = filename + \'.jpg\'\n\n            with open(join(base_path, \'record_{}.json\'.format(filename)), \'w\') as f:\n                json.dump(data[\'val\'], f)\n\n\ndef prune_model(model, apoz_df, n_channels_delete):\n    from kerassurgeon import Surgeon\n    import pandas as pd\n\n    # Identify 5% of channels with the highest APoZ in model\n    sorted_apoz_df = apoz_df.sort_values(\'apoz\', ascending=False)\n    high_apoz_index = sorted_apoz_df.iloc[0:n_channels_delete, :]\n\n    # Create the Surgeon and add a \'delete_channels\' job for each layer\n    # whose channels are to be deleted.\n    surgeon = Surgeon(model, copy=True)\n    for name in high_apoz_index.index.unique().values:\n        channels = list(pd.Series(high_apoz_index.loc[name, \'index\'],\n                                  dtype=np.int64).values)\n        surgeon.add_job(\'delete_channels\', model.get_layer(name),\n                        channels=channels)\n    # Delete channels\n    return surgeon.operate()\n\n\ndef get_total_channels(model):\n    start = None\n    end = None\n    channels = 0\n    for layer in model.layers[start:end]:\n        if layer.__class__.__name__ == \'Conv2D\':\n            channels += layer.filters\n    return channels\n\n\ndef get_model_apoz(model, generator):\n    from kerassurgeon.identify import get_apoz\n    import pandas as pd\n\n    # Get APoZ\n    start = None\n    end = None\n    apoz = []\n    for layer in model.layers[start:end]:\n        if layer.__class__.__name__ == \'Conv2D\':\n            print(layer.name)\n            apoz.extend([(layer.name, i, value) for (i, value)\n                         in enumerate(get_apoz(model, layer, generator))])\n\n    layer_name, index, apoz_value = zip(*apoz)\n    apoz_df = pd.DataFrame({\'layer\': layer_name, \'index\': index,\n                            \'apoz\': apoz_value})\n    apoz_df = apoz_df.set_index(\'layer\')\n    return apoz_df\n\n    \ndef removeComments( dir_list ):\n    for i in reversed(range(len(dir_list))):\n        if dir_list[i].startswith(""#""):\n            del dir_list[i]\n        elif len(dir_list[i]) == 0:\n            del dir_list[i]\n\ndef preprocessFileList( filelist ):\n    dirs = []\n    if filelist is not None:\n        for afile in filelist:\n            with open(afile, ""r"") as f:\n                tmp_dirs = f.read().split(\'\\n\')\n                dirs.extend(tmp_dirs)\n\n    removeComments( dirs )\n    return dirs\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    cfg = dk.load_config()\n    tub = args[\'--tub\']\n    model = args[\'--model\']\n    transfer = args[\'--transfer\']\n    model_type = args[\'--type\']\n\n    if model_type is None:\n        model_type = cfg.DEFAULT_MODEL_TYPE\n        print(""using default model type of"", model_type)\n\n    if args[\'--figure_format\']:\n        figure_format = args[\'--figure_format\']\n    continuous = args[\'--continuous\']\n    aug = args[\'--aug\']\n    \n    dirs = preprocessFileList( args[\'--file\'] )\n    if tub is not None:\n        tub_paths = [os.path.expanduser(n) for n in tub.split(\',\')]\n        dirs.extend( tub_paths )\n\n    multi_train(cfg, dirs, model, transfer, model_type, continuous, aug)\n'"
donkeycar/tests/__init__.py,0,b'# -*- coding: utf-8 -*-\n'
donkeycar/tests/setup.py,0,"b'import os\nimport platform\nimport pytest\nfrom donkeycar.parts.datastore import Tub\nfrom donkeycar.parts.simulation import SquareBoxCamera, MovingSquareTelemetry\nfrom donkeycar.management.base import CreateCar\nimport numpy as np\n\ndef on_pi():\n    if \'arm\' in platform.machine():\n        return True\n    return False\n\ntemp_tub_path = None\n\n@pytest.fixture\ndef tub_path(tmpdir):\n    tub_path = tmpdir.mkdir(\'tubs\').join(\'tub\')\n    return str(tub_path)\n\n@pytest.fixture\ndef tub(tub_path):\n    t = create_sample_tub(tub_path, records=128)\n    return t\n\n@pytest.fixture\ndef tubs(tmpdir, tubs=5):\n    tubs_dir = tmpdir.mkdir(\'tubs\')\n    tub_paths = [ str(tubs_dir.join(\'tub_{}\'.format(i))) for i in range(tubs) ]\n    tubs = [ create_sample_tub(tub_path, records=5) for tub_path in tub_paths ]\n    return (str(tubs_dir), tub_paths, tubs)\n    \ndef create_sample_tub(path, records=128):\n    inputs=[\'cam/image_array\', \'user/angle\', \'user/throttle\', \'location/one_hot_state_array\']\n    types=[\'image_array\', \'float\', \'float\', \'vector\']\n    t = Tub(path, inputs=inputs, types=types)\n    cam = SquareBoxCamera()\n    tel = MovingSquareTelemetry()\n    num_loc = 10\n    for _ in range(records):\n        x, y = tel.run()\n        img_arr = cam.run(x, y)\n        loc = [0 for i in range(num_loc)]\n        loc[1] = 1.0\n        t.put_record({\'cam/image_array\': img_arr, \'user/angle\': x, \'user/throttle\':y , \'location/one_hot_state_array\':loc})\n\n    global temp_tub_path\n    temp_tub_path = t\n    print(""setting temp tub path to:"", temp_tub_path)\n\n    return t\n\ndef d2_path(temp_path):\n    path = os.path.join(temp_path, \'d2\')\n    return str(path)\n\ndef default_template(d2_path):\n    c = CreateCar()\n    c.create_car(d2_path, overwrite=True)\n    return d2_path\n\ndef custom_template(d2_path, template):\n    c = CreateCar()\n    c.create_car(d2_path, template=template, overwrite=True)\n    return d2_path\n\n\ndef create_sample_record():\n    cam = SquareBoxCamera()\n    tel = MovingSquareTelemetry()\n    x, y = tel.run()\n    img_arr = cam.run(x, y)\n    return {\'cam/image_array\': img_arr, \'angle\': x, \'throttle\':y}'"
donkeycar/tests/test_actuator.py,0,"b""from .setup import on_pi\n\nfrom donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle\nimport pytest\n\n\n@pytest.mark.skipif(on_pi() == False, reason='Not on RPi')\ndef test_PCA9685():\n    c = PCA9685(0)\n\n@pytest.mark.skipif(on_pi() == False, reason='Not on RPi')\ndef test_PWMSteering():\n    c = PCA9685(0)\n    s = PWMSteering(c)\n"""
donkeycar/tests/test_controller.py,0,"b'import pytest\nfrom .setup import on_pi\nfrom donkeycar.parts.controller import PS3Joystick, PS3JoystickController\n\n\ndef test_ps3_joystick():\n    js = PS3Joystick()\n    assert js is not None\n    js.init()\n\ndef test_ps3_joystick_controller():\n    js = PS3JoystickController()\n    assert js is not None\n    js.init_js()\n    js.run_threaded(None)\n    js.print_controls()\n    def test_fn():\n        pass\n    js.set_button_down_trigger(""x"", test_fn)\n    js.erase_last_N_records()\n    js.on_throttle_changes()\n    js.emergency_stop()\n    #js.update()\n    js.set_steering(0.0)\n    js.set_throttle(0.0)\n    js.toggle_manual_recording()\n    js.increase_max_throttle()\n    js.decrease_max_throttle()\n    js.toggle_constant_throttle()\n    js.toggle_mode()\n    js.chaos_monkey_on_left()\n    js.chaos_monkey_on_right()\n    js.chaos_monkey_off()'"
donkeycar/tests/test_keras.py,0,"b'# -*- coding: utf-8 -*-\nimport pytest\nfrom donkeycar.parts.keras import *\nfrom donkeycar.utils import *\nimport numpy as np\n\n@pytest.fixture\ndef pilot():\n    kp = KerasPilot()\n    return kp\n\ndef test_pilot_types(pilot):\n    assert 1 == 1\n\ndef test_categorical():\n    km = KerasCategorical()\n    assert km.model is not None\n    img = get_test_img(km.model)\n    km.run(img)\n\ndef test_categorical_med():\n    input_shape=(64, 64, 1)\n    km = KerasCategorical(input_shape=input_shape)\n    assert km.model is not None\n    img = get_test_img(km.model)\n    km.run(img)\n\ndef test_categorical_tiny():\n    input_shape=(32, 32, 1)\n    km = KerasCategorical(input_shape=input_shape)\n    assert km.model is not None\n    img = get_test_img(km.model)\n    km.run(img)\n    \ndef test_linear():\n    km = KerasLinear()\n    assert km.model is not None   \n    img = get_test_img(km.model)\n    km.run(img)\n\ndef test_imu():\n    km = KerasIMU()\n    assert km.model is not None\n    img = get_test_img(km.model)\n    imu = np.random.rand(6).tolist()\n    km.run(img, *imu)\n\ndef test_rnn():\n    km = KerasRNN_LSTM()\n    assert km.model is not None\n    img = get_test_img(km.model)\n    km.run(img)\n    \ndef test_3dconv():\n    km = Keras3D_CNN()\n    assert km.model is not None\n    img = get_test_img(km.model)\n    km.run(img)\n\ndef test_localizer():\n    km = KerasLocalizer()\n    assert km.model is not None   \n    img = get_test_img(km.model)\n    km.run(img)'"
donkeycar/tests/test_launch.py,0,"b'import pytest\r\nimport time\r\n\r\nfrom donkeycar import utils\r\nfrom donkeycar.parts.launch import *\r\n\r\n\r\ndef test_ai_launch():\r\n    ai_launch = AiLaunch(launch_duration=1.0, launch_throttle=1.0)\r\n\r\n    mode = ""user""\r\n    ai_throttle = 0.0\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 0.0)\r\n\r\n    mode = ""local""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 0.0)\r\n\r\n    mode = ""user""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 0.0)\r\n\r\n    ai_launch.enable_ai_launch()\r\n    mode = ""local""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 1.0)\r\n\r\n    time.sleep(1.1)\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 0.0)\r\n\r\n\r\ndef test_ai_launch_keep_enabled():\r\n    ai_launch = AiLaunch(launch_duration=1.0, launch_throttle=1.0, keep_enabled=True)\r\n\r\n    mode = ""user""\r\n    ai_throttle = 0.0\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 0.0)\r\n\r\n    mode = ""local""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 1.0)\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    time.sleep(1.1)\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n    assert(new_throttle == 0.0)\r\n\r\n    mode = ""user""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(ai_launch.enabled==False)\r\n    assert(new_throttle == 0.0)\r\n\r\n    mode = ""local""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 1.0)\r\n\r\n    time.sleep(1.1)\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    mode = ""user""\r\n\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n    new_throttle = ai_launch.run(mode, ai_throttle)\r\n\r\n    assert(new_throttle == 0.0)\r\n\r\n'"
donkeycar/tests/test_management.py,0,b'\nfrom donkeycar.management import base\nfrom tempfile import tempdir\n\ndef get_test_tub_path():\n    tempdir()\n\ndef test_tubcheck():\n    tc = base.TubCheck()'
donkeycar/tests/test_memory.py,0,"b""#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nimport unittest\nimport pytest\nfrom donkeycar.memory import Memory\n\nclass TestMemory(unittest.TestCase):\n\n    def test_setitem_single_item(self):\n        mem = Memory()\n        mem['myitem'] = 999\n        assert mem['myitem'] == 999\n\n    def test_setitem_multi_items(self):\n        mem = Memory()\n        mem[('myitem1', 'myitem2')] = [888, '999']\n        assert mem[('myitem1', 'myitem2')] == [888, '999']\n\n    def test_put_single_item(self):\n        mem = Memory()\n        mem.put(['myitem'], 999)\n        assert mem['myitem'] == 999\n\n    def test_put_single_item_as_tuple(self):\n        mem = Memory()\n        mem.put(('myitem',), 999)\n        assert mem['myitem'] == 999\n\n    def test_put_multi_item(self):\n        mem = Memory()\n        mem.put(['my1stitem','my2nditem'], [777, '999'])\n        assert mem['my1stitem'] == 777\n        assert mem['my2nditem'] == '999'\n\n    def test_put_multi_item_as_tuple(self):\n        mem = Memory()\n        mem.put(('my1stitem','my2nditem'), (777, '999'))\n        assert mem['my1stitem'] == 777\n        assert mem['my2nditem'] == '999'\n\n    def test_get_multi_item(self):\n        mem = Memory()\n        mem.put(['my1stitem','my2nditem'], [777, '999'])\n        assert mem.get(['my1stitem','my2nditem']) == [777, '999']\n\n    def test_update_item(self):\n        mem = Memory()\n        mem.put(['myitem'], 888)\n        assert mem['myitem'] == 888\n\n        mem.update({'myitem': '111'})\n        assert mem['myitem'] == '111'\n\n    def test_get_keys(self):\n        mem = Memory()\n        mem.put(['myitem'], 888)\n        assert list(mem.keys()) == ['myitem']\n\n    def test_get_values(self):\n        mem = Memory()\n        mem.put(['myitem'], 888)\n        assert list(mem.values()) == [888]\n\n    def test_get_iter(self):\n        mem = Memory()\n        mem.put(['myitem'], 888)\n        \n        assert dict(mem.items()) == {'myitem': 888}\n"""
donkeycar/tests/test_parts.py,0,b'\n'
donkeycar/tests/test_robohat.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\nimport pytest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom donkeycar.parts.robohat import RoboHATController, RoboHATDriver\nimport donkeycar.templates.cfg_complete as cfg\nimport donkeycar as dk\n\n\nclass TestRoboHATDriver():\n    @patch(\'serial.Serial\')\n    def test_trim_out_of_bound_value(self, serial):\n        driver = RoboHATDriver(cfg)\n\n        assert driver.trim_out_of_bound_value(-1.1) == -1.0\n        assert driver.trim_out_of_bound_value(-0.9) == -0.9\n        assert driver.trim_out_of_bound_value(0) == 0\n        assert driver.trim_out_of_bound_value(0.9) == 0.9\n        assert driver.trim_out_of_bound_value(1.0) == 1.0\n        assert driver.trim_out_of_bound_value(1.1) == 1.0\n\n    @patch(\'serial.Serial\')\n    def test_set_pulse(self, serial):\n        driver = RoboHATDriver(cfg)\n\n        driver.write_pwm = MagicMock()\n\n        # angle, throttle\n        driver.set_pulse(0.0, 0.0)\n        driver.write_pwm.assert_called_with(""1500,1500"")\n\n        driver.set_pulse(1.0, 0.0)\n        driver.write_pwm.assert_called_with(""1000,1500"")\n\n        driver.set_pulse(0.0, 1.0)\n        driver.write_pwm.assert_called_with(""1500,2000"")\n\n        driver.set_pulse(1.0, 1.0)\n        driver.write_pwm.assert_called_with(""1000,2000"")\n\n        driver.set_pulse(-1.0, 0.0)\n        driver.write_pwm.assert_called_with(""2000,1500"")\n\n        driver.set_pulse(0.0, -1.0)\n        driver.write_pwm.assert_called_with(""1500,1000"")\n\n        driver.set_pulse(-1.0, -1.0)\n        driver.write_pwm.assert_called_with(""2000,1000"")\n\n        driver.set_pulse(-2.0, -2.0)\n        driver.write_pwm.assert_called_with(""2000,1000"")\n\n    @patch(\'serial.Serial\')\n    def test_set_pulse_with_adjusted_throttle(self, serial):\n        driver = RoboHATDriver(cfg)\n        driver.MAX_FORWARD = 1800\n\n        driver.write_pwm = MagicMock()\n        driver.set_pulse(1.0, 1.0)\n        driver.write_pwm.assert_called_with(""1000,1800"")\n\n        driver.set_pulse(0.0, 0.5)\n        # (1800 - 1500)/ 2 + 1500 = 1650\n        driver.write_pwm.assert_called_with(""1500,1650"")\n\n        # Change the STOPPED_PWM\n        driver.STOPPED_PWM = 1400\n        driver.set_pulse(0.0, 0.5)\n        # (1800 -1400)/2 +  1400 = 1600\n        driver.write_pwm.assert_called_with(""1500,1600"")\n\n        # Test Reverse\n        driver.STOPPED_PWM = 1500\n        driver.MAX_REVERSE = 1000\n        driver.set_pulse(0.0, -0.5)\n        driver.write_pwm.assert_called_with(""1500,1250"")\n\n        driver.MAX_REVERSE = 1250\n        driver.set_pulse(0.0, -0.5)\n        driver.write_pwm.assert_called_with(""1500,1375"")\n\n\n\n\nclass TestRoboHATController():\n    @patch(\'serial.Serial\')\n    def test_controller_update(self, serial):\n        controller = RoboHATController(cfg)\n\n        # print(""timeout = {}"".format(seriala.timeout))\n        controller.serial.readline.side_effect = [b""1500, 1500\\n"",\n                                                  b""2000, 1500\\n"",\n                                                  b""1600, 1600\\n"",\n                                                  b""2000, 2000\\n"",\n                                                  b""1000, 1000\\n"",\n                                                  b""1200, 1200\\n""]\n\n        # 1500, 1500\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == 0\n\n        # 2000, 1500\n        controller.read_serial()\n        assert controller.angle == -1.0\n        assert controller.throttle == 0\n\n        # 1600, 1600\n        controller.read_serial()\n        assert controller.angle == -0.2\n        assert controller.throttle == 0.2\n\n        # 2000, 2000\n        controller.read_serial()\n        assert controller.angle == -1.0\n        assert controller.throttle == 1.0\n\n        # 1000, 1000\n        controller.read_serial()\n        assert controller.angle == 1.0\n        assert controller.throttle == -1.0\n\n        # 1200, 1200\n        controller.read_serial()\n        assert controller.angle == 0.6\n        assert controller.throttle == -0.6\n\n    @patch(\'serial.Serial\')\n    def test_controller_update_with_adjusted_mid_steering(self, serial):\n        controller = RoboHATController(cfg)\n        controller.STEERING_MID = 1450\n\n        controller.serial.readline.side_effect = [b""1450, 1500\\n"",\n                                                  b""1500, 1500\\n""]\n\n        # 1450, 1500\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == 0\n\n        # 1500, 1500\n        controller.read_serial()\n        # 1450 is the mid value. So the correct angle should be (1450-1500)/ (2000-1450)\n        assert controller.angle == -0.09\n        assert controller.throttle == 0\n\n    @patch(\'serial.Serial\')\n    def test_controller_update_with_adjusted_max_throttle(self, serial):\n        \'\'\'\n        The adjusted MAX_FORWARD here should not affect the output throttle\n        value.\n        For example, when RC controller sending a 2000 value it means the user\n        want to go full speed. The throttle value should therefore be 1.0. It is\n        the RoboHATDriver responsibility to translate this 1.0 to an adjusted\n        pwm value.\n        \'\'\'\n        controller = RoboHATController(cfg, True)\n        controller.MAX_FORWARD = 1800\n        controller.STOPPED_PWM = 1500\n\n        controller.serial.readline.side_effect = [b""1500, 2000\\n"",\n                                                  b""1500, 1500\\n"",\n                                                  b""1500, 1750\\n""]\n\n        # 1500, 2000\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == 1.0\n\n        # 1500, 1500\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == 0\n\n        # 1500, 1750\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == 0.5\n\n    @patch(\'serial.Serial\')\n    def test_controller_update_with_adjusted_max_reverse(self, serial):\n        controller = RoboHATController(cfg, True)\n        controller.STOPPED_PWM = 1500\n        controller.MAX_REVERSE = 1200\n\n        controller.serial.readline.side_effect = [b""1500, 1000\\n"",\n                                                  b""1500, 1250\\n"",\n                                                  b""1500, 1000\\n"",\n                                                  b""1500, 1250\\n"",\n                                                  b""1500, 1500\\n""]\n\n        # 1500, 1000\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == -1.0\n\n        # 1500, 1250\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == -0.5\n\n        # 1500, 1000\n        controller.STOPPED_PWM = 1400\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == -1.0\n\n        # 1500, 1250\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == -0.5\n\n        # 1500, 1500\n        controller.read_serial()\n        assert controller.angle == 0\n        assert controller.throttle == 0\n\n\n    def test_controller_load_cfg(self):\n        \'\'\'\n        Make sure the controller load the value from config\n        \'\'\'\n\n        cfg.MM1_MAX_FORWARD = 1800\n        cfg.MM1_MAX_REVERSE = 1200\n        cfg.MM1_STOPPED_PWM = 1550\n        cfg.MM1_STEERING_MID = 1450\n\n        controller = RoboHATController(cfg)\n        assert controller.MAX_FORWARD == 1800\n        assert controller.MAX_REVERSE == 1200\n        assert controller.STOPPED_PWM == 1550\n        assert controller.STEERING_MID == 1450\n\n    def test_timeout_set(self):\n        assert 1 == 1\n'"
donkeycar/tests/test_scripts.py,0,"b'from donkeycar import utils\nimport pytest\n\n\ndef is_error(err):\n    for e in err:\n        #Catch error if \'Error\' is in the stderr output.\n        if \'Error\' in e.decode():\n            return True\n        #Catch error when the wrong command is used.\n        if \'Usage:\' in e.decode():\n            return True\n    return False\n\n\n@pytest.fixture\ndef cardir(tmpdir):\n    path = str(tmpdir.mkdir(""mycar""))\n    return path\n\n\ndef test_createcar(cardir):\n    cmd = [\'donkey\', \'createcar\', \'--path\', cardir]\n    out, err, proc_id = utils.run_shell_command(cmd)\n    assert is_error(err) is False\n\ndef test_drivesim(cardir):\n    cmd = [\'donkey\', \'createcar\', \'--path\', cardir ,\'--template\', \'square\']\n    out, err, proc_id = utils.run_shell_command(cmd, timeout=10)\n    cmd = [\'python\', \'manage.py\', \'drive\']\n    out, err, proc_id = utils.run_shell_command(cmd, cwd = cardir)\n    print(err)\n\n    if is_error(err) is True:\n        print(\'out\', out)\n        print(\'error: \', err)\n        raise ValueError (err)\n\ndef test_bad_command_fails():\n    cmd = [\'donkey\', \'not a comand\']\n    out, err, proc_id = utils.run_shell_command(cmd)\n    print(err)\n    print(out)\n    assert is_error(err) is True\n'"
donkeycar/tests/test_template.py,0,"b'# -*- coding: utf-8 -*-\nimport tempfile\nfrom tempfile import gettempdir\nimport unittest\nfrom donkeycar.parts.datastore import TubWriter, Tub\nfrom donkeycar.parts.datastore import TubHandler\nfrom donkeycar.templates import complete\nimport donkeycar as dk\nimport os\n\nimport pytest\n\n#fixtures\nfrom .setup import tub, tub_path, on_pi, default_template, d2_path, custom_template\n\ndef test_config():\n    path = default_template(d2_path(gettempdir()))\n    cfg = dk.load_config(os.path.join(path, \'config.py\'))\n    assert(cfg != None)\n\ndef test_drive():\n    path = default_template(d2_path(gettempdir()))\n    myconfig = open(os.path.join(path, \'myconfig.py\'), ""wt"")\n    myconfig.write(""CAMERA_TYPE = \'MOCK\'\\n"")\n    myconfig.write(""DRIVE_TRAIN_TYPE = \'None\'"")\n    myconfig.close()\n    cfg = dk.load_config(os.path.join(path, \'config.py\'))\n    cfg.MAX_LOOPS = 10\n    complete.drive(cfg=cfg)\n\n\ndef test_custom_templates():\n    template_names = [""complete"", ""basic_web"", ""square""]\n    for template in template_names:\n        path = custom_template(d2_path(gettempdir()), template=template)\n        cfg = dk.load_config(os.path.join(path, \'config.py\'))\n        assert(cfg != None)\n        mcfg = dk.load_config(os.path.join(path, \'myconfig.py\'))\n        assert(mcfg != None)\n'"
donkeycar/tests/test_train.py,0,"b'# -*- coding: utf-8 -*- #\nimport pytest\nimport os\n\nfrom donkeycar.templates.train import multi_train\nfrom donkeycar.parts.datastore import Tub\nfrom donkeycar.parts.simulation import SquareBoxCamera, MovingSquareTelemetry\n\nfrom donkeycar.templates.train import gather_records, collate_records\n\n#fixtures\nfrom .setup import tub, tub_path, on_pi\nfrom .setup import create_sample_tub\n\ndef cfg_defaults(cfg):\n    cfg.MAX_EPOCHS = 1\n    cfg.BATCH_SIZE = 10\n    cfg.SHOW_PLOT = False\n    cfg.VERBOSE_TRAIN = False\n    cfg.OPTIMIZER = ""adam""\n    cfg.TARGET_H = cfg.IMAGE_H - cfg.ROI_CROP_TOP - cfg.ROI_CROP_BOTTOM\n    cfg.TARGET_W = cfg.IMAGE_W\n    cfg.TARGET_D = cfg.IMAGE_DEPTH\n    cfg.NUM_LOCATIONS = 10\n\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_cat(tub, tub_path):\n    t = Tub(tub_path)\n    assert t is not None\n\n    import donkeycar.templates.cfg_complete as cfg\n    tempfolder = tub_path[:-3]\n    model_path = os.path.join(tempfolder, \'test.h5\')\n    cfg_defaults(cfg)\n\n    tub = tub_path\n    model = model_path\n    transfer = None\n    model_type = ""categorical""\n    continuous = False\n    aug = False\n    multi_train(cfg, tub, model, transfer, model_type, continuous, aug)\n\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_linear(tub, tub_path):\n    t = Tub(tub_path)\n    assert t is not None\n\n    import donkeycar.templates.cfg_complete as cfg\n    tempfolder = tub_path[:-3]\n    model_path = os.path.join(tempfolder, \'test.h5\')\n    cfg_defaults(cfg)\n\n    tub = tub_path\n    model = model_path\n    transfer = None\n    model_type = ""linear""\n    continuous = False\n    aug = False\n    multi_train(cfg, tub, model, transfer, model_type, continuous, aug)\n\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_localizer(tub, tub_path):\n    t = Tub(tub_path)\n    assert t is not None\n\n    import donkeycar.templates.cfg_complete as cfg\n    tempfolder = tub_path[:-3]\n    model_path = os.path.join(tempfolder, \'test.h5\')\n    cfg_defaults(cfg)\n\n    tub = tub_path\n    model = model_path\n    transfer = None\n    model_type = ""localizer""\n    continuous = False\n    aug = False\n    multi_train(cfg, tub, model, transfer, model_type, continuous, aug)\n\n\'\'\'\n\nlatent test requires opencv right now. and fails on travis ci. \nre-enable when we figure out a recipe for opencv on travis.\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_latent(tub, tub_path):\n    t = Tub(tub_path)\n    assert t is not None\n\n    import donkeycar.templates.cfg_complete as cfg\n    tempfolder = tub_path[:-3]\n    model_path = os.path.join(tempfolder, \'test.h5\')\n    cfg.MAX_EPOCHS = 1\n    cfg.BATCH_SIZE = 10\n    cfg.SHOW_PLOT = False\n    cfg.VERBOSE_TRAIN = False\n    cfg.OPTIMIZER = ""adam""\n\n    tub = tub_path\n    model = model_path\n    transfer = None\n    model_type = ""latent""\n    continuous = False\n    aug = True\n    multi_train(cfg, tub, model, transfer, model_type, continuous, aug)\n\'\'\'\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_seq(tub, tub_path):\n    t = Tub(tub_path)\n    assert t is not None\n\n    import donkeycar.templates.cfg_complete as cfg\n    tempfolder = tub_path[:-3]\n    model_path = os.path.join(tempfolder, \'test.h5\')\n    cfg_defaults(cfg)\n\n    tub = tub_path\n    model = model_path\n    transfer = None\n    model_type = ""rnn""\n    continuous = False\n    aug = True\n    multi_train(cfg, tub, model, transfer, model_type, continuous, aug)\n\n# Helper function to calculate the Train-Test split (ratio) of a collated dataset\ndef calculate_TrainTestSplit(gen_records):\n    train_recs = 0\n    test_recs = 0\n    for key in gen_records:\n        if gen_records[key][\'train\']:\n            train_recs += 1\n        else:\n            test_recs += 1\n\n    total_recs = len(gen_records)\n    print(""Total Records: {}"".format(total_recs))\n    print(""Train Records: {}"".format(train_recs))\n    print(""Test Records: {}"".format(test_recs))\n    assert total_recs == train_recs + test_recs\n    ratio = train_recs / total_recs\n    print(""Calculated Train-Test Split: {}"".format(ratio))\n    return ratio\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_TrainTestSplit_simple(tub_path):\n    # Check whether the Train-Test splitting is working correctly on a dataset.\n    initial_records = 100\n\n    # Setup the test data\n    t = create_sample_tub(tub_path, records=initial_records)\n    assert t is not None\n\n    # Import the configuration\n    import donkeycar.templates.cfg_complete as cfg\n\n    # Initial Setup\n    opts = {\'categorical\' : False}\n    opts[\'cfg\'] = cfg\n\n    orig_TRAIN_TEST_SPLIT = cfg.TRAIN_TEST_SPLIT\n    records = gather_records(cfg, tub_path, opts, verbose=True)\n    assert len(records) == initial_records\n\n    # Attempt a 50:50 split\n    gen_records = {}\n    cfg.TRAIN_TEST_SPLIT = 0.5\n    print()\n    print(""Testing a {} Test-Train split..."".format(opts[\'cfg\'].TRAIN_TEST_SPLIT))\n    print()\n    collate_records(records, gen_records, opts)\n    ratio = calculate_TrainTestSplit(gen_records)\n    assert ratio == cfg.TRAIN_TEST_SPLIT\n\n    # Attempt a split based on config file (reset of the record set)\n    gen_records = {}\n    cfg.TRAIN_TEST_SPLIT = orig_TRAIN_TEST_SPLIT\n    print()\n    print(""Testing a {} Test-Train split..."".format(opts[\'cfg\'].TRAIN_TEST_SPLIT))\n    print()\n    collate_records(records, gen_records, opts)\n    ratio = calculate_TrainTestSplit(gen_records)\n    assert ratio == cfg.TRAIN_TEST_SPLIT\n\n@pytest.mark.skipif(on_pi() == True, reason=\'Too slow on RPi\')\ndef test_train_TrainTestSplit_continuous(tub_path):\n    # Check whether the Train-Test splitting is working correctly when a dataset is extended.\n    initial_records = 100\n\n    # Setup the test data\n    t = create_sample_tub(tub_path, records=initial_records)\n    assert t is not None\n\n    # Import the configuration\n    import donkeycar.templates.cfg_complete as cfg\n\n    # Initial Setup\n    gen_records = {}\n    opts = {\'categorical\' : False}\n    opts[\'cfg\'] = cfg\n\n    # Perform the initial split\n    print()\n    print(""Initial split of {} records to {} Test-Train split..."".format(initial_records, opts[\'cfg\'].TRAIN_TEST_SPLIT))\n    print()\n    records = gather_records(cfg, tub_path, opts, verbose=True)\n    assert len(records) == initial_records\n    collate_records(records, gen_records, opts)\n    ratio = calculate_TrainTestSplit(gen_records)\n    assert ratio == cfg.TRAIN_TEST_SPLIT\n\n    # Add some more records and recheck the ratio (only the NEW records should be added)\n    additional_records = 200\n    print()\n    print(""Added an extra {} records, aiming for overall {} Test-Train split..."".format(additional_records, opts[\'cfg\'].TRAIN_TEST_SPLIT))\n    print()\n    create_sample_tub(tub_path, records=additional_records)\n    records = gather_records(cfg, tub_path, opts, verbose=True)\n    assert len(records) == (initial_records + additional_records)\n    collate_records(records, gen_records, opts)\n    ratio = calculate_TrainTestSplit(gen_records)\n    assert ratio == cfg.TRAIN_TEST_SPLIT\n'"
donkeycar/tests/test_tub.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport tempfile\nimport unittest\n\nfrom donkeycar.parts.datastore import TubHandler\nfrom donkeycar.parts.datastore import TubWriter, Tub\nfrom donkeycar.utils import arr_to_img, img_to_arr\nfrom PIL import ImageChops\n\n# fixtures\nfrom .setup import tub, tub_path\n\n\ndef test_tub_load(tub, tub_path):\n    """"""Tub loads from existing tub path.""""""\n    t = Tub(tub_path)\n    assert t is not None\n\n\ndef test_tub_update_df(tub):\n    """""" Tub updates its dataframe """"""\n    tub.update_df()\n    assert len(tub.df) == 128\n\n\ndef test_tub_add_record(tub):\n    """"""Tub can save a record and then retrieve it.""""""\n    import numpy as np\n    img_arr = np.zeros((120, 160))\n    x = 123\n    y = 90\n    rec_in = {\'cam/image_array\': img_arr, \'user/angle\': x, \'user/throttle\': y}\n    rec_index = tub.put_record(rec_in)\n    rec_out = tub.get_record(rec_index)\n    # Ignore the milliseconds key, which is added when the record is written\n    if \'milliseconds\' in rec_out:\n        rec_out.pop(\'milliseconds\')\n    assert rec_in.keys() == rec_out.keys()\n\n\ndef test_tub_write_numpy(tub):\n    """"""Tub can save a record that contains a numpy float, and retrieve it as a\n        python float.""""""\n    import numpy as np\n    x = 123\n    z = np.float32(11.1)\n    rec_in = {\'user/angle\': x, \'user/throttle\':z}\n    rec_index = tub.put_record(rec_in)\n    rec_out = tub.get_record(rec_index)\n    assert type(rec_out[\'user/throttle\']) == float\n\n\ndef test_tub_exclude(tub):\n    """""" Make sure the Tub will exclude records in the exclude set """"""\n    ri = lambda fnm: int(os.path.basename(fnm).split(\'_\')[1].split(\'.\')[0])\n\n    before = tub.gather_records()\n    # Make sure we gathered records correctly\n    assert len(before) == tub.get_num_records()\n    tub.exclude.add(1)\n    after = tub.gather_records()\n    # Make sure we excluded the correct number of records\n    assert len(after) == (tub.get_num_records() - 1)\n    before = set([ri(f) for f in before])\n    after = set([ri(f) for f in after])\n    diff = before - after\n    assert len(diff) == 1\n    # Make sure we exclude the correct index\n    assert 1 in diff\n\n\ndef test_tub_augment(tub):\n    """"""Tub with augmented images which only differ slightly.""""""\n    import numpy as np\n    index = tub.get_index(shuffled=False)\n    img_arr_before = [tub.get_record(ix)[\'cam/image_array\'] for ix in index]\n    tub.augment_images()\n    img_arr_after = [tub.get_record(ix)[\'cam/image_array\'] for ix in index]\n    total_change = 0\n    for img_arr_b, img_arr_a in zip(img_arr_before, img_arr_after):\n        assert img_arr_a.shape == img_arr_b.shape, \'image size broken\'\n        img_a = arr_to_img(img_arr_a)\n        img_b = arr_to_img(img_arr_b)\n        diff = ImageChops.difference(img_a, img_b)\n        diff_arr = img_to_arr(diff)\n        # normalise this number\n        num_pixel_channel = np.prod(diff_arr.shape)\n        avg_change = diff_arr.sum() / num_pixel_channel\n        # check that the augmented image is different if not totally black\n        if img_arr_b.max() > 0:\n            assert avg_change != 0, ""Augmentation didn\'t do anything""\n        # change per chanel can be maximally 255 in 8-bit\n        total_change += avg_change\n\n    # An average change of 1 (per 255) would be already too big on the moving\n    # square images. Empirically we see changes in the order of 0.1\n    assert total_change / len(img_arr_before) < 1.0, \\\n        \'The augmented pictures differ too much.\'\n\n\nclass TestTubWriter(unittest.TestCase):\n    def setUp(self):\n        self.tempfolder = tempfile.TemporaryDirectory().name\n        self.path = os.path.join(self.tempfolder, \'new\')\n        self.inputs = [\'name\', \'age\', \'pic\']\n        self.types = [\'str\', \'float\', \'str\']\n\n    def test_tub_create(self):\n        tub = TubWriter(self.path, inputs=self.inputs, types=self.types)\n\n    def test_tub_path(self):\n        tub = TubWriter(self.path, inputs=self.inputs, types=self.types)\n        tub.run(\'will\', 323, \'asdfasdf\')\n\n    def test_make_paths_absolute(self):\n        tub = Tub(self.path, inputs=[\'file_path\'], types=[\'image\'])\n        rel_file_name = \'test.jpg\'\n        record_dict = {\'file_path\': rel_file_name}\n        abs_record_dict = tub.make_record_paths_absolute(record_dict)\n\n        assert abs_record_dict[\'file_path\'] == os.path.join(self.path, rel_file_name)\n\n    def test_tub_meta(self):\n        meta = [""location:Here"", ""task:sometask""]\n        tub = Tub(self.path, inputs=[\'file_path\'], types=[\'image\'], user_meta=meta)\n        t2 = Tub(self.path)\n        assert ""location"" in tub.meta\n        assert ""location"" in t2.meta\n        assert ""sometask"" == t2.meta[""task""]\n\n    def test_tub_like_driver(self):\n        """""" The manage.py/donkey2.py drive command creates a tub using TubHandler,\n            so test that way.\n        """"""\n        os.makedirs(self.tempfolder)\n        meta = [""location:Here2"", ""task:sometask2""]\n        th = TubHandler(self.tempfolder)\n        tub = th.new_tub_writer(inputs=self.inputs, types=self.types, user_meta=meta)\n        t2 = Tub(tub.path)\n        assert tub.meta == t2.meta\n        assert tub.meta[\'location\'] == ""Here2""\n        assert t2.meta[\'inputs\'] == self.inputs\n        assert t2.meta[\'location\'] == ""Here2""\n'"
donkeycar/tests/test_util_data.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Jun 25 14:17:59 2017\n\n@author: wroscoe\n""""""\nimport unittest\nimport pytest\n\n\nfrom donkeycar.utils import *\n\n\ndef create_lbin(marker_index):\n    """""" Create a linear binary array with value set """"""\n    l = [0] * 15\n    l[marker_index] = 1\n    return l\n\n\nclass TestLinearBin(unittest.TestCase):\n\n    def test_zero(self):\n        res = linear_bin(0)\n        assert res[7] == 1\n        assert sum(res[:7]) == 0\n        assert sum(res[8:]) == 0\n\n    def test_positive(self):\n        res = linear_bin(1)\n        assert res[14] == 1\n        assert sum(res[:14]) == 0\n\n    def test_negative(self):\n        res = linear_bin(-1)\n        assert res[0] == 1\n        assert sum(res[1:]) == 0\n\n    def test_illegal_type(self):\n        with pytest.raises(TypeError):\n            linear_bin(\'0\')\n\n\nclass TestLinearUnbin(unittest.TestCase):\n\n    def test_zero(self):\n        l = create_lbin(7)\n        res = linear_unbin(l)\n        assert res == 0.0\n\n    def test_positive(self):\n        l = create_lbin(14)\n        res = linear_unbin(l)\n        assert res == 1.0\n\n    def test_negative(self):\n        l = create_lbin(0)\n        res = linear_unbin(l)\n        assert res == -1.0\n\n    def test_empty_list(self):\n        res = linear_unbin( [0] * 15 )\n        assert res == -1.0\n\n\nclass TestMapping(unittest.TestCase):\n\n    def test_positive(self):\n        min = map_range(-100, -100, 100, 0, 1000)\n        half = map_range(0, -100, 100, 0, 1000)\n        max = map_range(100, -100, 100, 0, 1000)\n        assert min == 0\n        assert half == 500\n        assert max == 1000\n\n    def test_negative(self):\n        ranges = (0, 100, 0, 1000)\n        min = map_range(0, *ranges)\n        half = map_range(50, *ranges)\n        max = map_range(100, *ranges)\n        assert min == 0\n        assert half == 500\n        assert max == 1000\n\n    def test_reverse(self):\n        ranges = (100, 0, 0, 1000)\n        min = map_range(0, *ranges)\n        half = map_range(50, *ranges)\n        max = map_range(100, *ranges)\n        assert min == 1000\n        assert half == 500\n        assert max == 0\n\n\nclass TestMapRangeFloat(unittest.TestCase):\n\n    def test_source_int_range(self):\n        assert 0.0 == map_range_float(0, 0, 100, 0, 1.0)\n        assert 0.5 == map_range_float(50, 0, 100, 0, 1.0)\n        assert 1.0 == map_range_float(100, 0, 100, 0, 1.0)\n\n        # Try a different range\n        assert 0.0 == map_range_float(0, 0, 20, 0, 1.0)\n        assert 0.25 == map_range_float(5, 0, 20, 0, 1.0)\n        assert 0.5 == map_range_float(10, 0, 20, 0, 1.0)\n        assert 0.75 == map_range_float(15, 0, 20, 0, 1.0)\n        assert 1.0 == map_range_float(20, 0, 20, 0, 1.0)\n\n    def test_source_float_range(self):\n        assert 0.0 == map_range_float(0, 0, 1.0, 0, 1.0)\n        assert 0.5 == map_range_float(0.5, 0, 1.0, 0, 1.0)\n        assert 1.0 == map_range_float(1.0, 0, 1.0, 0, 1.0)\n        assert 0.95 == map_range_float(0.95, 0, 1.0, 0, 1.0)\n\n    def test_negative(self):\n        assert 0.0 == map_range_float(0, -100, 100, -1.0, 1.0)\n        assert -1.0 == map_range_float(-100, -100, 100, -1.0, 1.0)\n        assert 1.0 == map_range_float(100, -100, 100, -1.0, 1.0)\n\n    def test_scale_down(self):\n        assert 0.0 == map_range_float(0, 0, 100, 0, 0.5)\n        assert 0.25 == map_range_float(50, 0, 100, 0, 0.5)\n        assert 0.5 == map_range_float(100, 0, 100, 0, 0.5)\n\n    def test_scale_up(self):\n        assert 0.0 == map_range_float(0, 0, 100, 0, 2.0)\n        assert 1.0 == map_range_float(50, 0, 100, 0, 2.0)\n        assert 2.0 == map_range_float(100, 0, 100, 0, 2.0)\n\n\nclass TestMergeDicts(unittest.TestCase):\n\n    def test_merge_two_dicts(self):\n        d1 = { \'a\' : 1, \'b\' : 2, \'c\' : 3 }\n        d2 = { 10 : \'hi\', \'bob\' : 20 }\n        res = merge_two_dicts(d1, d2)\n\n        assert res == { \'a\' : 1, \'b\' : 2, \'c\' : 3, 10 : \'hi\', \'bob\' : 20 }\n\nclass TestParamGen(unittest.TestCase):\n\n    def test_param_gen(self):\n        g = param_gen({ \'a\' : [ \'opt1\', \'opt2\' ], \'b\' : [ \'opt3\', \'opt4\' ] })\n        l = [ x for x in g ]\n        expected = [\n                {\'a\': \'opt1\', \'b\': \'opt3\'},\n                {\'a\': \'opt1\', \'b\': \'opt4\'},\n                {\'a\': \'opt2\', \'b\': \'opt3\'},\n                {\'a\': \'opt2\', \'b\': \'opt4\'}\n            ]\n        self.assertCountEqual(expected, l)\n\ndef test_train_test_split():\n    data_set = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n    train_set, val_set = train_test_split(data_set, test_size=0.2)\n    print(train_set)\n    print(val_set)\n    assert(len(train_set)==8)\n    assert(len(val_set)==2)\n'"
donkeycar/tests/test_vehicle.py,0,"b'import pytest\nimport donkeycar as dk\nfrom donkeycar.parts.transform import Lambda\n\n\ndef _get_sample_lambda():\n    def f():\n        return 1\n    f.update = f\n    return Lambda(f)\n\n\n@pytest.fixture()\ndef vehicle():\n    v = dk.Vehicle()\n    v.add(_get_sample_lambda(), outputs=[\'test_out\'])\n    return v\n\n\ndef test_create_vehicle():\n    v = dk.Vehicle()\n    assert v.parts == []\n\n\ndef test_add_part():\n    v = dk.Vehicle()\n    v.add(_get_sample_lambda(), outputs=[\'test_out\'])\n    assert len(v.parts) == 1\n\n\ndef test_vehicle_run(vehicle):\n    vehicle.start(rate_hz=20, max_loop_count=2)\n    assert vehicle is not None\n\n\ndef test_should_raise_assertion_on_non_list_inputs_for_add_part():\n    vehicle = dk.Vehicle()\n    inputs = \'any\'\n    with pytest.raises(AssertionError):\n        vehicle.add(_get_sample_lambda(), inputs=inputs)\n        pytest.fail(""inputs is not a list: %r"" % inputs)\n\n\ndef test_should_raise_assertion_on_non_list_outputs_for_add_part():\n    vehicle = dk.Vehicle()\n    outputs = \'any\'\n    with pytest.raises(AssertionError):\n        vehicle.add(_get_sample_lambda(), outputs=outputs)\n        pytest.fail(""outputs is not a list: %r"" % outputs)\n\n\ndef test_should_raise_assertion_on_non_boolean_threaded_for_add_part():\n    vehicle = dk.Vehicle()\n    threaded = \'non_boolean\'\n    with pytest.raises(AssertionError):\n        vehicle.add(_get_sample_lambda(), threaded=threaded)\n        pytest.fail(""threaded is not a boolean: %r"" % threaded)'"
donkeycar/tests/test_web_controller.py,0,b'# -*- coding: utf-8 -*-\nimport pytest\nimport json\nfrom donkeycar.parts.web_controller.web import LocalWebController\n\n@pytest.fixture\ndef server():\n    server = LocalWebController()\n    return server\n\n\ndef test_json_output(server):\n    result = server.run()\n    json_result = json.dumps(result)\n    d = json.loads(json_result)\n    assert d is not None\n    assert int(d[0]) == 0\n\n\n'
donkeycar/tests/test_web_socket.py,0,"b'\nfrom donkeycar.parts.web_controller.web import WebSocketCalibrateAPI\nfrom functools import partial\n\nfrom tornado import testing\nimport tornado.websocket\nimport tornado.web\nimport tornado.ioloop\nimport json\nfrom unittest.mock import Mock\nfrom donkeycar.parts.actuator import PWMSteering, PWMThrottle\n\n\nclass WebSocketCalibrateTest(testing.AsyncHTTPTestCase):\n    """"""\n    Example of WebSocket usage as a client\n    in AsyncHTTPTestCase-based unit tests.\n    """"""\n\n    def get_app(self):\n        app = tornado.web.Application([(\'/\', WebSocketCalibrateAPI)])\n        self.app = app\n\n        return app\n\n    def get_ws_url(self):\n        return ""ws://localhost:"" + str(self.get_http_port()) + ""/""\n\n    @tornado.testing.gen_test\n    def test_calibrate_servo_esc_1(self):\n        ws_client = yield tornado.websocket.websocket_connect(self.get_ws_url())\n\n        # Now we can run a test on the WebSocket.\n        self.app.drive_train = dict()\n        self.app.drive_train[\'steering\'] = Mock()\n        self.app.drive_train_type = ""SERVO_ESC""\n\n        data = {""config"": {""STEERING_LEFT_PWM"": 444}}\n        yield ws_client.write_message(json.dumps(data))\n        yield ws_client.close()\n\n        assert self.app.drive_train[\'steering\'].left_pulse == 444\n        assert isinstance(self.app.drive_train[\'steering\'].right_pulse, Mock)\n\n    @tornado.testing.gen_test\n    def test_calibrate_servo_esc_2(self):\n        ws_client = yield tornado.websocket.websocket_connect(self.get_ws_url())\n\n        # Now we can run a test on the WebSocket.\n        self.app.drive_train = dict()\n        self.app.drive_train[\'steering\'] = Mock()\n        self.app.drive_train_type = ""SERVO_ESC""\n\n        data = {""config"": {""STEERING_RIGHT_PWM"": 555}}\n        yield ws_client.write_message(json.dumps(data))\n        yield ws_client.close()\n\n        assert self.app.drive_train[\'steering\'].right_pulse == 555\n        assert isinstance(self.app.drive_train[\'steering\'].left_pulse, Mock)\n\n    @tornado.testing.gen_test\n    def test_calibrate_servo_esc_3(self):\n        ws_client = yield tornado.websocket.websocket_connect(self.get_ws_url())\n\n        # Now we can run a test on the WebSocket.\n        self.app.drive_train = dict()\n        self.app.drive_train[\'throttle\'] = Mock()\n        self.app.drive_train_type = ""SERVO_ESC""\n\n        data = {""config"": {""THROTTLE_FORWARD_PWM"": 666}}\n        yield ws_client.write_message(json.dumps(data))\n        yield ws_client.close()\n\n        assert self.app.drive_train[\'throttle\'].max_pulse == 666\n        assert isinstance(self.app.drive_train[\'throttle\'].min_pulse, Mock)\n\n    @tornado.testing.gen_test\n    def test_calibrate_mm1(self):\n        ws_client = yield tornado.websocket.websocket_connect(self.get_ws_url())\n\n        # Now we can run a test on the WebSocket.\n        self.app.drive_train = Mock()\n        self.app.drive_train_type = ""MM1""\n        data = {""config"": {""MM1_STEERING_MID"": 1234}}\n        yield ws_client.write_message(json.dumps(data))\n        yield ws_client.close()\n\n        assert self.app.drive_train.STEERING_MID == 1234\n'"
donkeycar/contrib/robohat/code.py,0,"b'# Donkey Car Driver for Robotics Masters Robo HAT MM1\n#\n# Notes:\n#   This is to be run using CircuitPython 5.3\n#   Date: 15/05/2019\n#   Updated: 20/02/2020\n#   Updated: 8/03/2020 (sctse999)\n#   Updated: 11/05/2020 (wallarug)\n#\n#\n\nimport time\nimport board\nimport busio\n\nfrom digitalio import DigitalInOut, Direction\nfrom pulseio import PWMOut, PulseIn, PulseOut\n\nimport adafruit_logging as logging\nlogger = logging.getLogger(\'code\')\nlogger.setLevel(logging.INFO)\n\n# Customisation these variables\nDEBUG = False\nUSB_SERIAL = False\nSMOOTHING_INTERVAL_IN_S = 0.025\nACCEL_RATE = 10\n\n## cannot have DEBUG and USB_SERIAL\nif USB_SERIAL:\n    DEBUG = False\n\n## functions\ndef servo_duty_cycle(pulse_ms, frequency = 60):\n    """"""\n    Formula for working out the servo duty_cycle at 16 bit input\n    """"""\n    period_ms = 1.0 / frequency * 1000.0\n    duty_cycle = int(pulse_ms / 1000 / (period_ms / 65535.0))\n    return duty_cycle\n\n\ndef state_changed(control):\n    """"""\n    Reads the RC channel and smooths value\n    """"""\n    control.channel.pause()\n    for i in range(0, len(control.channel)):\n        val = control.channel[i]\n        # prevent ranges outside of control space\n        if(val < 1000 or val > 2000):\n            continue\n        # set new value\n        control.value = (control.value + val) / 2\n\n    if DEBUG:\n        logger.debug(""%f\\t%s (%i): %i (%i)"" % (time.monotonic(), control.name, len(\n            control.channel), control.value, servo_duty_cycle(control.value)))\n    control.channel.clear()\n    control.channel.resume()\n\n\nclass Control:\n    """"""\n    Class for a RC Control Channel\n    """"""\n\n    def __init__(self, name, servo, channel, value):\n        self.name = name\n        self.servo = servo\n        self.channel = channel\n        self.value = value\n        self.servo.duty_cycle = servo_duty_cycle(value)\n\n\n# set up on-board LED\nled = DigitalInOut(board.LED)\nled.direction = Direction.OUTPUT\n\n# set up serial UART to Raspberry Pi\n# note UART(TX, RX, baudrate)\nuart = busio.UART(board.TX1, board.RX1, baudrate=115200, timeout=0.001)\n\n# set up servos\nsteering_pwm = PWMOut(board.SERVO2, duty_cycle=2 ** 15, frequency=60)\nthrottle_pwm = PWMOut(board.SERVO1, duty_cycle=2 ** 15, frequency=60)\n\n# set up RC channels.  NOTE: input channels are RCC3 & RCC4 (not RCC1 & RCC2)\nsteering_channel = PulseIn(board.RCC4, maxlen=64, idle_state=0)\nthrottle_channel = PulseIn(board.RCC3, maxlen=64, idle_state=0)\n\n# setup Control objects.  1500 pulse is off and center steering\nsteering = Control(""Steering"", steering_pwm, steering_channel, 1500)\nthrottle = Control(""Throttle"", throttle_pwm, throttle_channel, 1500)\n\n# Hardware Notification: starting\nlogger.info(""preparing to start..."")\nfor i in range(0, 2):\n    led.value = True\n    time.sleep(0.5)\n    led.value = False\n    time.sleep(0.5)\n\nlast_update = time.monotonic()\n\n# GOTO: main()\ndef main():\n    global last_update\n\n    data = bytearray(\'\')\n    datastr = \'\'\n    last_input = 0\n    steering_val = steering.value\n    throttle_val = throttle.value\n\n    while True:\n        # only update every smoothing interval (to avoid jumping)\n        if(last_update + SMOOTHING_INTERVAL_IN_S > time.monotonic()):\n            continue\n        last_update = time.monotonic()\n\n        # check for new RC values (channel will contain data)\n        if(len(throttle.channel) != 0):\n            state_changed(throttle)\n\n        if(len(steering.channel) != 0):\n            state_changed(steering)\n\n        if DEBUG:\n            logger.info(""Get: steering=%i, throttle=%i"" % (int(steering.value), int(throttle.value)))\n        \n        if(USB_SERIAL):\n            # simulator USB\n            print(""%i, %i"" % (int(steering.value), int(throttle.value)))\n        else:\n            # write the RC values to the RPi Serial\n            uart.write(b""%i, %i\\r\\n"" % (int(steering.value), int(throttle.value)))\n\n        while True:\n            # wait for data on the serial port and read 1 byte\n            byte = uart.read(1)\n\n            # if no data, break and continue with RC control\n            if(byte == None):\n                break\n            last_input = time.monotonic()\n\n            if (DEBUG):\n                logger.debug(""Read from UART: %s"" % (byte))\n\n            # if data is recieved, check if it is the end of a stream\n            if(byte == b\'\\r\'):\n                data = bytearray(\'\')\n                break\n\n            data[len(data):len(data)] = byte\n\n            # convert bytearray to string\n            datastr = \'\'.join([chr(c) for c in data]).strip()\n\n        # if we make it here, there is serial data from the previous step\n        if(len(datastr) >= 10):\n            steering_val = steering.value\n            throttle_val = throttle.value\n            try:\n                steering_val = int(datastr[:4])\n                throttle_val = int(datastr[-4:])\n            except ValueError:\n                None\n\n            data = bytearray(\'\')\n            datastr = \'\'\n            last_input = time.monotonic()\n            logger.info(""Set: steering=%i, throttle=%i"" % (steering_val, throttle_val))\n\n        if(last_input + 10 < time.monotonic()):\n            # set the servo for RC control\n            steering.servo.duty_cycle = servo_duty_cycle(steering.value)\n            throttle.servo.duty_cycle = servo_duty_cycle(throttle.value)\n        else:\n            # set the servo for serial data (recieved)\n            steering.servo.duty_cycle = servo_duty_cycle(steering_val)\n            throttle.servo.duty_cycle = servo_duty_cycle(throttle_val)\n\n\n# Run\nlogger.info(""Run!"")\nmain()\n'"
donkeycar/parts/voice_control/alexa.py,0,"b'import time\nimport requests\n\n\nclass AlexaController(object):\n    \'\'\'\n    Accept simple command from alexa. For the command supported, please refer\n    to the README.md\n    \'\'\'\n    API_ENDPOINT = ""http://alexa.robocarstore.com""\n\n    def __init__(self, ctr, cfg, debug=False):\n        self.running = True\n        self.debug = debug\n        self.ctr = ctr\n        self.cfg = cfg  # Pass the config object for altering AI_THROTTLE_MULT\n        self.DEFAULT_AI_THROTTLE_MULT = self.cfg.AI_THROTTLE_MULT\n\n        if self.cfg.ALEXA_DEVICE_CODE is None:\n            raise Exception(""Please set cfg.ALEXA_DEVICE_CODE in myconfig.py"")\n\n    def log(self, msg):\n        print(""Voice control: {}"".format(msg))\n\n    def get_command(self):\n        url = ""{}/{}"".format(self.API_ENDPOINT, \'command\')\n\n        params = {\n            \'deviceCode\': self.cfg.ALEXA_DEVICE_CODE\n        }\n\n        command = None\n\n        try:\n            response = requests.get(url=url, params=params, timeout=5)\n            response.raise_for_status()\n            result = response.json()\n            if ""command"" in result:\n                command = result[\'command\']\n            else:\n                self.log(""Warning - No command found in response"")\n        except requests.exceptions.RequestException as e:\n            self.log(""Warning - Failed to reach Alexa API endpoint"")\n        except ValueError:  # Catch JSONDecodeError\n            self.log(\'Warning - Decoding JSON failed\')\n\n        return command\n\n    def update(self):\n        while (self.running):\n            command = self.get_command()\n            if self.debug:\n                self.log(""Command = {}"".format(command))\n            elif command is not None:\n                self.log(""Command = {}"".format(command))\n\n            if command == ""autopilot"":\n                self.ctr.mode = ""local""\n            elif command == ""speedup"":\n                self.cfg.AI_THROTTLE_MULT += 0.05\n            elif command == ""slowdown"":\n                self.cfg.AI_THROTTLE_MULT -= 0.05\n            elif command == ""stop"":\n                self.ctr.mode = ""user""\n                self.cfg.AI_THROTTLE_MULT = self.DEFAULT_AI_THROTTLE_MULT\n\n            if self.debug:\n                self.log(""mode = {}, cfg.AI_THROTTLE_MULT={}"".format(\n                    self.ctr.mode, self.cfg.AI_THROTTLE_MULT))\n\n            time.sleep(0.25)  # Give a break between requests\n\n    def run_threaded(self):\n        pass\n\n    def shutdown(self):\n        self.running = False\n'"
donkeycar/parts/web_controller/web.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Jun 24 20:10:44 2017\n\n@author: wroscoe\n\nremotes.py\n\nThe client and web server needed to control a car remotely.\n""""""\n\n\nimport os\nimport json\nimport time\nimport asyncio\n\nimport requests\nfrom tornado.ioloop import IOLoop\nfrom tornado.web import Application, RedirectHandler, StaticFileHandler, \\\n    RequestHandler\nfrom tornado.httpserver import HTTPServer\nimport tornado.gen\nimport tornado.websocket\nfrom socket import gethostname\n\nfrom ... import utils\n\n\nclass RemoteWebServer():\n    \'\'\'\n    A controller that repeatedly polls a remote webserver and expects\n    the response to be angle, throttle and drive mode.\n    \'\'\'\n\n    def __init__(self, remote_url, connection_timeout=.25):\n\n        self.control_url = remote_url\n        self.time = 0.\n        self.angle = 0.\n        self.throttle = 0.\n        self.mode = \'user\'\n        self.recording = False\n        # use one session for all requests\n        self.session = requests.Session()\n\n    def update(self):\n        \'\'\'\n        Loop to run in separate thread the updates angle, throttle and\n        drive mode.\n        \'\'\'\n\n        while True:\n            # get latest value from server\n            self.angle, self.throttle, self.mode, self.recording = self.run()\n\n    def run_threaded(self):\n        \'\'\'\n        Return the last state given from the remote server.\n        \'\'\'\n        return self.angle, self.throttle, self.mode, self.recording\n\n    def run(self):\n        \'\'\'\n        Posts current car sensor data to webserver and returns\n        angle and throttle recommendations.\n        \'\'\'\n\n        data = {}\n        response = None\n        while response is None:\n            try:\n                response = self.session.post(self.control_url,\n                                             files={\'json\': json.dumps(data)},\n                                             timeout=0.25)\n\n            except requests.exceptions.ReadTimeout as err:\n                print(""\\n Request took too long. Retrying"")\n                # Lower throttle to prevent runaways.\n                return self.angle, self.throttle * .8, None\n\n            except requests.ConnectionError as err:\n                # try to reconnect every 3 seconds\n                print(""\\n Vehicle could not connect to server. Make sure you\'ve "" +\n                    ""started your server and you\'re referencing the right port."")\n                time.sleep(3)\n\n        data = json.loads(response.text)\n        angle = float(data[\'angle\'])\n        throttle = float(data[\'throttle\'])\n        drive_mode = str(data[\'drive_mode\'])\n        recording = bool(data[\'recording\'])\n\n        return angle, throttle, drive_mode, recording\n\n    def shutdown(self):\n        pass\n\n\nclass LocalWebController(tornado.web.Application):\n\n    def __init__(self, port=8887, mode=\'user\'):\n        \'\'\'\n        Create and publish variables needed on many of\n        the web handlers.\n        \'\'\'\n\n        print(\'Starting Donkey Server...\', end=\'\')\n\n        this_dir = os.path.dirname(os.path.realpath(__file__))\n        self.static_file_path = os.path.join(this_dir, \'templates\', \'static\')\n        self.angle = 0.0\n        self.throttle = 0.0\n        self.mode = mode\n        self.recording = False\n        self.port = port\n\n        self.num_records = 0\n        self.wsclients = []\n\n\n        handlers = [\n            (r""/"", RedirectHandler, dict(url=""/drive"")),\n            (r""/drive"", DriveAPI),\n            (r""/wsDrive"", WebSocketDriveAPI),\n            (r""/wsCalibrate"", WebSocketCalibrateAPI),\n            (r""/calibrate"", CalibrateHandler),\n            (r""/video"", VideoAPI),\n            (r""/static/(.*)"", StaticFileHandler,\n             {""path"": self.static_file_path}),\n        ]\n\n        settings = {\'debug\': True}\n        super().__init__(handlers, **settings)\n        print(""... you can now go to {}.local:8887 to drive ""\n              ""your car."".format(gethostname()))\n\n    def update(self):\n        \'\'\' Start the tornado webserver. \'\'\'\n        asyncio.set_event_loop(asyncio.new_event_loop())\n        self.listen(self.port)\n        IOLoop.instance().start()\n\n    def run_threaded(self, img_arr=None, num_records=0):\n        self.img_arr = img_arr\n        self.num_records = num_records\n\n        # Send record count to websocket clients\n        if (self.num_records is not None and self.recording is True):\n            if self.num_records % 10 == 0:\n                for wsclient in self.wsclients:\n                    try:\n                        data = {\n                            \'num_records\': self.num_records\n                        }\n                        wsclient.write_message(json.dumps(data))\n                    except:\n                        pass\n        \n        return self.angle, self.throttle, self.mode, self.recording\n\n    def run(self, img_arr=None):\n        self.img_arr = img_arr\n        return self.angle, self.throttle, self.mode, self.recording\n\n    def shutdown(self):\n        pass\n\n\nclass DriveAPI(RequestHandler):\n\n    def get(self):\n        data = {}\n        self.render(""templates/vehicle.html"", **data)\n\n    def post(self):\n        \'\'\'\n        Receive post requests as user changes the angle\n        and throttle of the vehicle on a the index webpage\n        \'\'\'\n        data = tornado.escape.json_decode(self.request.body)\n        self.application.angle = data[\'angle\']\n        self.application.throttle = data[\'throttle\']\n        self.application.mode = data[\'drive_mode\']\n        self.application.recording = data[\'recording\']\n\n\nclass CalibrateHandler(RequestHandler):\n    """""" Serves the calibration web page""""""\n    async def get(self):\n        await self.render(""templates/calibrate.html"")\n\n\nclass WebSocketDriveAPI(tornado.websocket.WebSocketHandler):\n    def check_origin(self, origin):\n        return True\n\n    def open(self):\n        # print(""New client connected"")\n        self.application.wsclients.append(self)\n\n    def on_message(self, message):\n        data = json.loads(message)\n        \n        self.application.angle = data[\'angle\']\n        self.application.throttle = data[\'throttle\']\n        self.application.mode = data[\'drive_mode\']\n        self.application.recording = data[\'recording\']\n\n    def on_close(self):\n        # print(""Client disconnected"")\n        self.application.wsclients.remove(self)\n\n\nclass WebSocketCalibrateAPI(tornado.websocket.WebSocketHandler):\n    def check_origin(self, origin):\n        return True\n\n    def open(self):\n        print(""New client connected"")\n\n    def on_message(self, message):\n        print(f""wsCalibrate {message}"")\n        data = json.loads(message)\n        if \'throttle\' in data:\n            print(data[\'throttle\'])\n            self.application.throttle = data[\'throttle\']\n\n        if \'config\' in data:\n            config = data[\'config\']\n            if self.application.drive_train_type == ""SERVO_ESC"":\n                if \'STEERING_LEFT_PWM\' in config:\n                    self.application.drive_train[\'steering\'].left_pulse = config[\'STEERING_LEFT_PWM\']\n\n                if \'STEERING_RIGHT_PWM\' in config:\n                    self.application.drive_train[\'steering\'].right_pulse = config[\'STEERING_RIGHT_PWM\']\n\n                if \'THROTTLE_FORWARD_PWM\' in config:\n                    self.application.drive_train[\'throttle\'].max_pulse = config[\'THROTTLE_FORWARD_PWM\']\n\n                if \'THROTTLE_STOPPED_PWM\' in config:\n                    self.application.drive_train[\'throttle\'].zero_pulse = config[\'THROTTLE_STOPPED_PWM\']\n\n                if \'THROTTLE_REVERSE_PWM\' in config:\n                    self.application.drive_train[\'throttle\'].min_pulse = config[\'THROTTLE_REVERSE_PWM\']\n\n            elif self.application.drive_train_type == ""MM1"":\n                if \'MM1_STEERING_MID\' in config:\n                    self.application.drive_train.STEERING_MID = config[\'MM1_STEERING_MID\']\n                if \'MM1_MAX_FORWARD\' in config:\n                    self.application.drive_train.MAX_FORWARD = config[\'MM1_MAX_FORWARD\']\n                if \'MM1_MAX_REVERSE\' in config:\n                    self.application.drive_train.MAX_REVERSE = config[\'MM1_MAX_REVERSE\']\n\n\n    def on_close(self):\n        print(""Client disconnected"")\n\n\nclass VideoAPI(RequestHandler):\n    \'\'\'\n    Serves a MJPEG of the images posted from the vehicle.\n    \'\'\'\n\n    async def get(self):\n\n        self.set_header(""Content-type"",\n                        ""multipart/x-mixed-replace;boundary=--boundarydonotcross"")\n\n        served_image_timestamp = time.time()\n        my_boundary = ""--boundarydonotcross\\n""\n        while True:\n\n            interval = .01\n            if served_image_timestamp + interval < time.time() and \\\n                    hasattr(self.application, \'img_arr\'):\n\n                img = utils.arr_to_binary(self.application.img_arr)\n                self.write(my_boundary)\n                self.write(""Content-type: image/jpeg\\r\\n"")\n                self.write(""Content-length: %s\\r\\n\\r\\n"" % len(img))\n                self.write(img)\n                served_image_timestamp = time.time()\n                try:\n                    await self.flush()\n                except tornado.iostream.StreamClosedError:\n                    pass\n            else:\n                await tornado.gen.sleep(interval)\n\n\nclass BaseHandler(RequestHandler):\n    """""" Serves the FPV web page""""""\n    async def get(self):\n        data = {}\n        await self.render(""templates/base_fpv.html"", **data)\n\n\nclass WebFpv(Application):\n    """"""\n    Class for running an FPV web server that only shows the camera in real-time.\n    The web page contains the camera view and auto-adjusts to the web browser\n    window size. Conjecture: this picture up-scaling is performed by the\n    client OS using graphics acceleration. Hence a web browser on the PC is\n    faster than a pure python application based on open cv or similar.\n    """"""\n\n    def __init__(self, port=8890):\n        self.port = port\n        this_dir = os.path.dirname(os.path.realpath(__file__))\n        self.static_file_path = os.path.join(this_dir, \'templates\', \'static\')\n\n        """"""Construct and serve the tornado application.""""""\n        handlers = [\n            (r""/"", BaseHandler),\n            (r""/video"", VideoAPI),\n            (r""/static/(.*)"", StaticFileHandler,\n             {""path"": self.static_file_path})\n        ]\n\n        settings = {\'debug\': True}\n        super().__init__(handlers, **settings)\n        print(""Started Web FPV server. You can now go to {}.local:{} to ""\n              ""view the car camera"".format(gethostname(), self.port))\n\n    def update(self):\n        """""" Start the tornado webserver. """"""\n        asyncio.set_event_loop(asyncio.new_event_loop())\n        self.listen(self.port)\n        IOLoop.instance().start()\n\n    def run_threaded(self, img_arr=None):\n        self.img_arr = img_arr\n\n    def run(self, img_arr=None):\n        self.img_arr = img_arr\n\n    def shutdown(self):\n        pass\n\n\n\n'"
