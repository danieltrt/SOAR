file_path,api_count,code
setup.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nfrom glob import glob\nimport os\nfrom os.path import basename\nfrom os.path import splitext\nimport sys\n\nfrom setuptools import find_packages, setup\n\n\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\n\ndef read_version():\n    return read(\'VERSION\').strip()\n\n\ntest_dependencies = [\'tox\', \'flake8\', \'pytest\', \'pytest-cov\', \'pytest-xdist\', \'mock\',\n                     \'sagemaker==1.50.1\', \'tensorflow<2.0\', \'docker-compose\', \'boto3==1.10.50\',\n                     \'six==1.13.0\', \'python-dateutil>=2.1,<2.8.1\', \'botocore==1.13.50\',\n                     \'requests-mock\', \'awscli==1.16.314\']\n\nif sys.version_info.major > 2:\n    test_dependencies.append(\'sagemaker-experiments==0.1.7\')\n\nsetup(\n    name=\'sagemaker_tensorflow_training\',\n    version=read_version(),\n    description=\'Open source library for creating \'\n                \'TensorFlow containers to run on Amazon SageMaker.\',\n\n    packages=find_packages(where=\'src\', exclude=(\'test\',)),\n    package_dir={\'\': \'src\'},\n    py_modules=[splitext(basename(path))[0] for path in glob(\'src/*.py\')],\n\n    long_description=read(\'README.rst\'),\n    author=\'Amazon Web Services\',\n    url=\'https://github.com/aws/sagemaker-tensorflow-containers\',\n    license=\'Apache License 2.0\',\n\n    classifiers=[\n        ""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Developers"",\n        ""Natural Language :: English"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python"",\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n\n    install_requires=[\'sagemaker-training>=3.5.2\', \'numpy\', \'scipy\', \'sklearn\',\n                      \'pandas\', \'Pillow\', \'h5py\'],\n    extras_require={\n        \'test\': test_dependencies,\n        \'benchmark\': [\'click\']\n    },\n)\n'"
docker/__init__.py,0,b''
test-toolkit/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
benchmarks/horovod-resnet/execute_horovod_training.py,0,"b'#!/usr/bin/env python\n# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom __future__ import absolute_import\n\nimport itertools\nimport json\nimport os\nimport shutil\nimport subprocess\n\nimport click\nimport pandas as pd\nfrom sagemaker import Session\nfrom sagemaker.tensorflow import TensorFlow\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\nbenchmark_results_dir = os.path.join(\'s3://\', Session().default_bucket(), \'hvd-benchmarking\')\n\n\n@click.group()\ndef cli():\n    pass\n\n\ndef generate_report():\n    results_dir = os.path.join(dir_path, \'results\')\n\n    if os.path.exists(results_dir):\n        shutil.rmtree(results_dir)\n\n    subprocess.call([\'aws\', \'s3\', \'cp\', \'--recursive\', benchmark_results_dir, results_dir])\n\n    jobs = {}\n\n    for job_name in os.listdir(results_dir):\n        jobs[job_name] = {}\n\n        _, instance_type, instance_count, device, py_version, _, _, _, _, _, _, _ = job_name.split(\'-\')\n\n        current_dir = os.path.join(results_dir, job_name)\n\n        model_dir = os.path.join(current_dir, \'output\', \'model.tar.gz\')\n        subprocess.call([\'tar\', \'-xvzf\', model_dir], cwd=current_dir)\n\n        jobs[job_name][\'instance_type\'] = instance_type\n        jobs[job_name][\'instance_count\'] = instance_count\n        jobs[job_name][\'device\'] = device\n        jobs[job_name][\'py_version\'] = py_version\n\n        benchmark_log = os.path.join(current_dir, \'benchmark_run.log\')\n\n        if os.path.exists(benchmark_log):\n            with open(benchmark_log) as f:\n                data = json.load(f)\n\n\n                jobs[job_name][\'dataset\'] = data[\'dataset\'][\'name\']\n                jobs[job_name][\'num_cores\'] = data[\'machine_config\'][\'cpu_info\'][\'num_cores\']\n                jobs[job_name][\'cpu_info\'] = data[\'machine_config\'][\'cpu_info\'][\'cpu_info\']\n                jobs[job_name][\'mhz_per_cpu\'] = data[\'machine_config\'][\'cpu_info\'][\'mhz_per_cpu\']\n                jobs[job_name][\'gpu_count\'] = data[\'machine_config\'][\'gpu_info\'][\'count\']\n                jobs[job_name][\'gpu_model\'] = data[\'machine_config\'][\'gpu_info\'][\'model\']\n\n                def find_value(parameter):\n                    other_key = [k for k in parameter if k != \'name\'][0]\n                    return parameter[other_key]\n\n                for parameter in data[\'run_parameters\']:\n                    jobs[job_name][parameter[\'name\']] = find_value(parameter)\n\n                jobs[job_name][\'model_name\'] = data[\'model_name\']\n                jobs[job_name][\'run_date\'] = data[\'run_date\']\n                jobs[job_name][\'tensorflow_version\'] = data[\'tensorflow_version\'][\'version\']\n                jobs[job_name][\'tensorflow_version_git_hash\'] = data[\'tensorflow_version\'][\'git_hash\']\n\n    return pd.DataFrame(jobs)\n\n\n@cli.command(\'train\')\n@click.option(\'--framework-version\', required=True, type=click.Choice([\'1.11\', \'1.12\']))\n@click.option(\'--device\', required=True, type=click.Choice([\'cpu\', \'gpu\']))\n@click.option(\'--py-versions\', multiple=True, type=str)\n@click.option(\'--training-input-mode\', default=\'File\', type=click.Choice([\'File\', \'Pipe\']))\n@click.option(\'--networking-isolation/--no-networking-isolation\', default=False)\n@click.option(\'--wait/--no-wait\', default=False)\n@click.option(\'--security-groups\', multiple=True, type=str)\n@click.option(\'--subnets\', multiple=True, type=str)\n@click.option(\'--role\', default=\'SageMakerRole\', type=str)\n@click.option(\'--instance-counts\', multiple=True, type=int)\n@click.option(\'--instance-types\', multiple=True, type=str)\n@click.argument(\'script_args\', nargs=-1, type=str)\ndef train(framework_version,\n          device,\n          py_versions,\n          training_input_mode,\n          networking_isolation,\n          wait,\n          security_groups,\n          subnets,\n          role,\n          instance_counts,\n          instance_types,\n          script_args):\n    iterator = itertools.product(instance_types, py_versions, instance_counts)\n    for instance_type, py_version, instance_count in iterator:\n        base_name = job_name(instance_type, instance_count, device, py_version)\n\n        mpi_options = \'-x HOROVOD_HIERARCHICAL_ALLREDUCE=1 -x HOROVOD_FUSION_THRESHOLD=16777216 -x TF_CPP_MIN_LOG_LEVEL=0 -x HOROVOD_TIMELINE --output-filename /opt/ml/model/hlog\'\n        estimator = TensorFlow(\n            entry_point=os.path.join(dir_path, \'train.sh\'),\n            role=role,\n            dependencies=[os.path.join(dir_path, \'train_imagenet_resnet_hvd.py\')],\n            base_job_name=base_name,\n            train_instance_count=instance_count,\n            train_instance_type=instance_type,\n            framework_version=framework_version,\n            py_version=py_version,\n            script_mode=True,\n            hyperparameters={\n                \'sagemaker_mpi_enabled\': True,\n                \'sagemaker_mpi_num_of_processes_per_host\': 8,\n                \'sagemaker_mpi_custom_mpi_options\': mpi_options\n            },\n            output_path=benchmark_results_dir,\n            security_group_ids=security_groups,\n            subnets=subnets\n        )\n\n        estimator.fit(wait=wait)\n\n        if wait:\n            artifacts_path = os.path.join(dir_path, \'results\',\n                                          estimator.latest_training_job.job_name)\n            model_path = os.path.join(artifacts_path, \'model.tar.gz\')\n            os.makedirs(artifacts_path)\n            subprocess.call([\'aws\', \'s3\', \'cp\', estimator.model_data, model_path])\n            subprocess.call([\'tar\', \'-xvzf\', model_path], cwd=artifacts_path)\n\n            print(\'Model downloaded at %s\' % model_path)\n\n\ndef job_name(instance_type,\n             instance_count,\n             device,\n             python_version):\n    instance_typename = instance_type.replace(\'.\', \'\').replace(\'ml\', \'\')\n\n    return \'hvd-%s-%s-%s-%s\' % (\n        instance_typename, instance_count, device, python_version)\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/horovod-resnet/train_imagenet_resnet_hvd.py,158,"b'#!/usr/bin/env python\n# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom __future__ import print_function\n\ntry:\n    from builtins import range\nexcept ImportError:\n    pass\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.image.python.ops import distort_image_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.contrib.data.python.ops import interleave_ops\nfrom tensorflow.contrib.data.python.ops import batching\nimport horovod.tensorflow as hvd\nimport os\nimport sys\nimport time\nimport argparse\nimport random\nimport shutil\nimport logging\nimport math\nimport re\nfrom glob import glob\nfrom operator import itemgetter\nfrom tensorflow.python.util import nest\n\ndef rank0log(logger, *args, **kwargs):\n    if hvd.rank() == 0:\n        if logger:\n            logger.info(\'\'.join([str(x) for x in list(args)]))\n        else:\n            print(*args, **kwargs)\n\n\nclass LayerBuilder(object):\n    def __init__(self, activation=None, data_format=\'channels_last\',\n                 training=False, use_batch_norm=False, batch_norm_config=None,\n                 conv_initializer=None, adv_bn_init=False):\n        self.activation = activation\n        self.data_format = data_format\n        self.training = training\n        self.use_batch_norm = use_batch_norm\n        self.batch_norm_config = batch_norm_config\n        self.conv_initializer = conv_initializer\n        self.adv_bn_init = adv_bn_init\n        if self.batch_norm_config is None:\n            self.batch_norm_config = {\n                \'decay\': 0.9,\n                \'epsilon\': 1e-4,\n                \'scale\': True,\n                \'zero_debias_moving_mean\': False,\n            }\n\n    def _conv2d(self, inputs, activation, *args, **kwargs):\n        x = tf.layers.conv2d(\n            inputs, data_format=self.data_format,\n            use_bias=not self.use_batch_norm,\n            kernel_initializer=self.conv_initializer,\n            activation=None if self.use_batch_norm else activation,\n            *args, **kwargs)\n        if self.use_batch_norm:\n            x = self.batch_norm(x)\n            x = activation(x) if activation is not None else x\n        return x\n\n    def conv2d_linear_last_bn(self, inputs, *args, **kwargs):\n        x = tf.layers.conv2d(\n            inputs, data_format=self.data_format,\n            use_bias=False,\n            kernel_initializer=self.conv_initializer,\n            activation=None, *args, **kwargs)\n        param_initializers = {\n            \'moving_mean\': tf.zeros_initializer(),\n            \'moving_variance\': tf.ones_initializer(),\n            \'beta\': tf.zeros_initializer(),\n        }\n        if self.adv_bn_init:\n            param_initializers[\'gamma\'] = tf.zeros_initializer()\n        else:\n            param_initializers[\'gamma\'] = tf.ones_initializer()\n        x = self.batch_norm(x, param_initializers=param_initializers)\n        return x\n\n    def conv2d_linear(self, inputs, *args, **kwargs):\n        return self._conv2d(inputs, None, *args, **kwargs)\n\n    def conv2d(self, inputs, *args, **kwargs):\n        return self._conv2d(inputs, self.activation, *args, **kwargs)\n\n    def pad2d(self, inputs, begin, end=None):\n        if end is None:\n            end = begin\n        try:\n            _ = begin[1]\n        except TypeError:\n            begin = [begin, begin]\n        try:\n            _ = end[1]\n        except TypeError:\n            end = [end, end]\n        if self.data_format == \'channels_last\':\n            padding = [[0, 0], [begin[0], end[0]], [begin[1], end[1]], [0, 0]]\n        else:\n            padding = [[0, 0], [0, 0], [begin[0], end[0]], [begin[1], end[1]]]\n        return tf.pad(inputs, padding)\n\n    def max_pooling2d(self, inputs, *args, **kwargs):\n        return tf.layers.max_pooling2d(\n            inputs, data_format=self.data_format, *args, **kwargs)\n\n    def average_pooling2d(self, inputs, *args, **kwargs):\n        return tf.layers.average_pooling2d(\n            inputs, data_format=self.data_format, *args, **kwargs)\n\n    def dense_linear(self, inputs, units, **kwargs):\n        return tf.layers.dense(inputs, units, activation=None)\n\n    def dense(self, inputs, units, **kwargs):\n        return tf.layers.dense(inputs, units, activation=self.activation)\n\n    def activate(self, inputs, activation=None):\n        activation = activation or self.activation\n        return activation(inputs) if activation is not None else inputs\n\n    def batch_norm(self, inputs, **kwargs):\n        all_kwargs = dict(self.batch_norm_config)\n        all_kwargs.update(kwargs)\n        data_format = \'NHWC\' if self.data_format == \'channels_last\' else \'NCHW\'\n        return tf.contrib.layers.batch_norm(\n            inputs, is_training=self.training, data_format=data_format,\n            fused=True, **all_kwargs)\n\n    def spatial_average2d(self, inputs):\n        shape = inputs.get_shape().as_list()\n        if self.data_format == \'channels_last\':\n            n, h, w, c = shape\n        else:\n            n, c, h, w = shape\n        n = -1 if n is None else n\n        x = tf.layers.average_pooling2d(inputs, (h, w), (1, 1),\n                                        data_format=self.data_format)\n        return tf.reshape(x, [n, c])\n\n    def flatten2d(self, inputs):\n        x = inputs\n        if self.data_format != \'channel_last\':\n            # Note: This ensures the output order matches that of NHWC networks\n            x = tf.transpose(x, [0, 2, 3, 1])\n        input_shape = x.get_shape().as_list()\n        num_inputs = 1\n        for dim in input_shape[1:]:\n            num_inputs *= dim\n        return tf.reshape(x, [-1, num_inputs], name=\'flatten\')\n\n    def residual2d(self, inputs, network, units=None, scale=1.0, activate=False):\n        outputs = network(inputs)\n        c_axis = -1 if self.data_format == \'channels_last\' else 1\n        h_axis = 1 if self.data_format == \'channels_last\' else 2\n        w_axis = h_axis + 1\n        ishape, oshape = [y.get_shape().as_list() for y in [inputs, outputs]]\n        ichans, ochans = ishape[c_axis], oshape[c_axis]\n        strides = ((ishape[h_axis] - 1) // oshape[h_axis] + 1,\n                   (ishape[w_axis] - 1) // oshape[w_axis] + 1)\n        with tf.name_scope(\'residual\'):\n            if (ochans != ichans or strides[0] != 1 or strides[1] != 1):\n                inputs = self.conv2d_linear(inputs, units, 1, strides, \'SAME\')\n            x = inputs + scale * outputs\n            if activate:\n                x = self.activate(x)\n        return x\n\n\ndef resnet_bottleneck_v1(builder, inputs, depth, depth_bottleneck, stride,\n                         basic=False):\n    num_inputs = inputs.get_shape().as_list()[1]\n    x = inputs\n    with tf.name_scope(\'resnet_v1\'):\n        if depth == num_inputs:\n            if stride == 1:\n                shortcut = x\n            else:\n                shortcut = builder.max_pooling2d(x, 1, stride)\n        else:\n            shortcut = builder.conv2d_linear(x, depth, 1, stride, \'SAME\')\n        if basic:\n            x = builder.pad2d(x, 1)\n            x = builder.conv2d(x, depth_bottleneck, 3, stride, \'VALID\')\n            x = builder.conv2d_linear(x, depth, 3, 1, \'SAME\')\n        else:\n            x = builder.conv2d(x, depth_bottleneck, 1, 1, \'SAME\')\n            x = builder.conv2d(x, depth_bottleneck, 3, stride, \'SAME\')\n            # x = builder.conv2d_linear(x, depth,            1, 1,      \'SAME\')\n            x = builder.conv2d_linear_last_bn(x, depth, 1, 1, \'SAME\')\n        x = tf.nn.relu(x + shortcut)\n        return x\n\n\ndef inference_resnet_v1_impl(builder, inputs, layer_counts, basic=False):\n    x = inputs\n    x = builder.pad2d(x, 3)\n    x = builder.conv2d(x, 64, 7, 2, \'VALID\')\n    x = builder.max_pooling2d(x, 3, 2, \'SAME\')\n    for i in range(layer_counts[0]):\n        x = resnet_bottleneck_v1(builder, x, 256, 64, 1, basic)\n    for i in range(layer_counts[1]):\n        x = resnet_bottleneck_v1(builder, x, 512, 128, 2 if i == 0 else 1, basic)\n    for i in range(layer_counts[2]):\n        x = resnet_bottleneck_v1(builder, x, 1024, 256, 2 if i == 0 else 1, basic)\n    for i in range(layer_counts[3]):\n        x = resnet_bottleneck_v1(builder, x, 2048, 512, 2 if i == 0 else 1, basic)\n    return builder.spatial_average2d(x)\n\n\ndef inference_resnet_v1(inputs, nlayer, data_format=\'channels_last\',\n                        training=False, conv_initializer=None, adv_bn_init=False):\n    """"""Deep Residual Networks family of models\n    https://arxiv.org/abs/1512.03385\n    """"""\n    builder = LayerBuilder(tf.nn.relu, data_format, training, use_batch_norm=True,\n                           conv_initializer=conv_initializer, adv_bn_init=adv_bn_init)\n    if nlayer == 18:\n        return inference_resnet_v1_impl(builder, inputs, [2, 2, 2, 2], basic=True)\n    elif nlayer == 34:\n        return inference_resnet_v1_impl(builder, inputs, [3, 4, 6, 3], basic=True)\n    elif nlayer == 50:\n        return inference_resnet_v1_impl(builder, inputs, [3, 4, 6, 3])\n    elif nlayer == 101:\n        return inference_resnet_v1_impl(builder, inputs, [3, 4, 23, 3])\n    elif nlayer == 152:\n        return inference_resnet_v1_impl(builder, inputs, [3, 8, 36, 3])\n    else:\n        raise ValueError(""Invalid nlayer (%i); must be one of: 18,34,50,101,152"" %\n                         nlayer)\n\n\ndef get_model_func(model_name):\n    if model_name.startswith(\'resnet\'):\n        nlayer = int(model_name[len(\'resnet\'):])\n        return lambda images, *args, **kwargs: \\\n            inference_resnet_v1(images, nlayer, *args, **kwargs)\n    else:\n        raise ValueError(""Invalid model type: %s"" % model_name)\n\n\ndef deserialize_image_record(record):\n    feature_map = {\n        \'image/encoded\': tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/class/label\': tf.FixedLenFeature([1], tf.int64, -1),\n        \'image/class/text\': tf.FixedLenFeature([], tf.string, \'\'),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32)\n    }\n    with tf.name_scope(\'deserialize_image_record\'):\n        obj = tf.parse_single_example(record, feature_map)\n        imgdata = obj[\'image/encoded\']\n        label = tf.cast(obj[\'image/class/label\'], tf.int32)\n        bbox = tf.stack([obj[\'image/object/bbox/%s\' % x].values\n                         for x in [\'ymin\', \'xmin\', \'ymax\', \'xmax\']])\n        bbox = tf.transpose(tf.expand_dims(bbox, 0), [0, 2, 1])\n        text = obj[\'image/class/text\']\n        return imgdata, label, bbox, text\n\n\ndef decode_jpeg(imgdata, channels=3):\n    return tf.image.decode_jpeg(imgdata, channels=channels,\n                                fancy_upscaling=False,\n                                dct_method=\'INTEGER_FAST\')\n\n\ndef crop_and_resize_image(image, original_bbox, height, width, \n                          distort=False, nsummary=10):\n    with tf.name_scope(\'crop_and_resize\'):\n        # Evaluation is done on a center-crop of this ratio\n        eval_crop_ratio = 0.8\n        if distort:\n            initial_shape = [int(round(height / eval_crop_ratio)),\n                             int(round(width / eval_crop_ratio)),\n                             3]\n            bbox_begin, bbox_size, bbox = \\\n                tf.image.sample_distorted_bounding_box(\n                    initial_shape,\n                    bounding_boxes=tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4]),\n                    # tf.zeros(shape=[1,0,4]), # No bounding boxes\n                    min_object_covered=0.1,\n                    aspect_ratio_range=[3. / 4., 4. / 3.],\n                    area_range=[0.08, 1.0],\n                    max_attempts=100,\n                    seed=11 * hvd.rank(),  # Need to set for deterministic results\n                    use_image_if_no_bounding_boxes=True)\n            bbox = bbox[0, 0]  # Remove batch, box_idx dims\n        else:\n            # Central crop\n            ratio_y = ratio_x = eval_crop_ratio\n            bbox = tf.constant([0.5 * (1 - ratio_y), 0.5 * (1 - ratio_x),\n                                0.5 * (1 + ratio_y), 0.5 * (1 + ratio_x)])\n        image = tf.image.crop_and_resize(\n            image[None, :, :, :], bbox[None, :], [0], [height, width])[0]\n        return image\n\n\ndef parse_and_preprocess_image_record(record, counter, height, width,\n                                      brightness, contrast, saturation, hue,\n                                      distort=False, nsummary=10, increased_aug=False):\n    imgdata, label, bbox, text = deserialize_image_record(record)\n    label -= 1  # Change to 0-based (don\'t use background class)\n    with tf.name_scope(\'preprocess_train\'):\n        try:\n            image = decode_jpeg(imgdata, channels=3)\n        except:\n            image = tf.image.decode_png(imgdata, channels=3)\n        image = crop_and_resize_image(image, bbox, height, width, distort)\n        if distort:\n            image = tf.image.random_flip_left_right(image)\n            if increased_aug:\n                image = tf.image.random_brightness(image, max_delta=brightness)\n                image = distort_image_ops.random_hsv_in_yiq(image, \n                                                            lower_saturation=saturation, \n                                                            upper_saturation=2.0 - saturation, \n                                                            max_delta_hue=hue * math.pi)\n                image = tf.image.random_contrast(image, lower=contrast, upper=2.0 - contrast)\n                tf.summary.image(\'distorted_color_image\', tf.expand_dims(image, 0))\n        image = tf.clip_by_value(image, 0., 255.)\n        image = tf.cast(image, tf.uint8)\n        return image, label\n\ndef make_dataset(filenames, take_count, batch_size, height, width,\n                 brightness, contrast, saturation, hue,\n                 training=False, num_threads=10, nsummary=10, shard=False, synthetic=False,\n                 increased_aug=False):\n    if synthetic and training:\n        input_shape = [height, width, 3]\n        input_element = nest.map_structure(lambda s: tf.constant(0.5, tf.float32, s), tf.TensorShape(input_shape))\n        label_element = nest.map_structure(lambda s: tf.constant(1, tf.int32, s), tf.TensorShape([1]))\n        element = (input_element, label_element)\n        ds = tf.data.Dataset.from_tensors(element).repeat()\n    else:\n        shuffle_buffer_size = 10000\n        num_readers = 1\n        if hvd.size() > len(filenames):\n            assert (hvd.size() % len(filenames)) == 0\n            filenames = filenames * (hvd.size() / len(filenames))\n\n        ds = tf.data.Dataset.from_tensor_slices(filenames)\n        if shard:\n            # split the dataset into parts for each GPU\n            ds = ds.shard(hvd.size(), hvd.rank())\n\n        if not training:\n            ds = ds.take(take_count)  # make sure all ranks have the same amount\n\n        if training:\n            ds = ds.shuffle(1000, seed=7 * (1 + hvd.rank()))\n\n        ds = ds.interleave(\n            tf.data.TFRecordDataset, cycle_length=num_readers, block_length=1)\n        counter = tf.data.Dataset.range(sys.maxsize)\n        ds = tf.data.Dataset.zip((ds, counter))\n        preproc_func = lambda record, counter_: parse_and_preprocess_image_record(\n            record, counter_, height, width, brightness, contrast, saturation, hue,\n            distort=training, nsummary=nsummary if training else 0, increased_aug=increased_aug)\n        ds = ds.map(preproc_func, num_parallel_calls=num_threads)\n        if training:\n            ds = ds.apply(tf.data.experimental.shuffle_and_repeat(shuffle_buffer_size, seed=5*(1+hvd.rank())))\n    ds = ds.batch(batch_size)\n    return ds\n\n\ndef stage(tensors):\n    """"""Stages the given tensors in a StagingArea for asynchronous put/get.\n    """"""\n    stage_area = data_flow_ops.StagingArea(\n        dtypes=[tensor.dtype for tensor in tensors],\n        shapes=[tensor.get_shape() for tensor in tensors])\n    put_op = stage_area.put(tensors)\n    get_tensors = stage_area.get()\n    tf.add_to_collection(\'STAGING_AREA_PUTS\', put_op)\n    return put_op, get_tensors\n\n\nclass PrefillStagingAreasHook(tf.train.SessionRunHook):\n    def after_create_session(self, session, coord):\n        enqueue_ops = tf.get_collection(\'STAGING_AREA_PUTS\')\n        for i in range(len(enqueue_ops)):\n            session.run(enqueue_ops[:i + 1])\n\n\nclass LogSessionRunHook(tf.train.SessionRunHook):\n    def __init__(self, global_batch_size, num_records, display_every=10, logger=None):\n        self.global_batch_size = global_batch_size\n        self.num_records = num_records\n        self.display_every = display_every\n        self.logger = logger\n\n    def after_create_session(self, session, coord):\n        rank0log(self.logger, \'  Step Epoch Speed   Loss  FinLoss   LR\')\n        self.elapsed_secs = 0.\n        self.count = 0\n\n    def before_run(self, run_context):\n        self.t0 = time.time()\n        return tf.train.SessionRunArgs(\n            fetches=[tf.train.get_global_step(),\n                     \'loss:0\', \'total_loss:0\', \'learning_rate:0\'])\n\n    def after_run(self, run_context, run_values):\n        self.elapsed_secs += time.time() - self.t0\n        self.count += 1\n        global_step, loss, total_loss, lr = run_values.results\n        if global_step == 1 or global_step % self.display_every == 0:\n            dt = self.elapsed_secs / self.count\n            img_per_sec = self.global_batch_size / dt\n            epoch = global_step * self.global_batch_size / self.num_records\n            self.logger.info(\'%6i %5.1f %7.1f %6.3f %6.3f %7.5f\' %\n                             (global_step, epoch, img_per_sec, loss, total_loss, lr))\n            self.elapsed_secs = 0.\n            self.count = 0\n\n\ndef _fp32_trainvar_getter(getter, name, shape=None, dtype=None,\n                          trainable=True, regularizer=None,\n                          *args, **kwargs):\n    storage_dtype = tf.float32 if trainable else dtype\n    variable = getter(name, shape, dtype=storage_dtype,\n                      trainable=trainable,\n                      regularizer=regularizer if trainable and \'BatchNorm\' not in name and \'batchnorm\' not in name and \'batch_norm\' not in name and \'Batch_Norm\' not in name else None,\n                      *args, **kwargs)\n    if trainable and dtype != tf.float32:\n        cast_name = name + \'/fp16_cast\'\n        try:\n            cast_variable = tf.get_default_graph().get_tensor_by_name(\n                cast_name + \':0\')\n        except KeyError:\n            cast_variable = tf.cast(variable, dtype, name=cast_name)\n        cast_variable._ref = variable._ref\n        variable = cast_variable\n    return variable\n\n\ndef fp32_trainable_vars(name=\'fp32_vars\', *args, **kwargs):\n    """"""A varible scope with custom variable getter to convert fp16 trainable\n    variables with fp32 storage followed by fp16 cast.\n    """"""\n    return tf.variable_scope(\n        name, custom_getter=_fp32_trainvar_getter, *args, **kwargs)\n\n\nclass MixedPrecisionOptimizer(tf.train.Optimizer):\n    """"""An optimizer that updates trainable variables in fp32.""""""\n\n    def __init__(self, optimizer,\n                 scale=None,\n                 name=""MixedPrecisionOptimizer"",\n                 use_locking=False):\n        super(MixedPrecisionOptimizer, self).__init__(\n            name=name, use_locking=use_locking)\n        self._optimizer = optimizer\n        self._scale = float(scale) if scale is not None else 1.0\n\n    def compute_gradients(self, loss, var_list=None, *args, **kwargs):\n        if var_list is None:\n            var_list = (\n                    tf.trainable_variables() +\n                    tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n\n        replaced_list = var_list\n\n        if self._scale != 1.0:\n            loss = tf.scalar_mul(self._scale, loss)\n\n        gradvar = self._optimizer.compute_gradients(loss, replaced_list, *args, **kwargs)\n\n        final_gradvar = []\n        for orig_var, (grad, var) in zip(var_list, gradvar):\n            if var is not orig_var:\n                grad = tf.cast(grad, orig_var.dtype)\n            if self._scale != 1.0:\n                grad = tf.scalar_mul(1. / self._scale, grad)\n            final_gradvar.append((grad, orig_var))\n\n        return final_gradvar\n\n    def apply_gradients(self, *args, **kwargs):\n        return self._optimizer.apply_gradients(*args, **kwargs)\n\nclass LarcOptimizer(tf.train.Optimizer):\n    """""" LARC implementation\n        -------------------\n        Parameters:\n          - optimizer:     initial optimizer that you wanna apply\n                           example: tf.train.MomentumOptimizer\n          - learning_rate: initial learning_rate from initial optimizer\n          - clip:          if True apply LARC otherwise LARS\n          - epsilon:       default value is weights or grads are 0.\n          - name\n          - use_locking\n    """"""\n\n    def __init__(self, optimizer, learning_rate, eta, clip=True, epsilon=1.,\n                 name=""LarcOptimizer"", use_locking=False):\n        super(LarcOptimizer, self).__init__(\n            name=name, use_locking=use_locking)\n        self._optimizer = optimizer\n        self._learning_rate = learning_rate\n        self._eta = float(eta)\n        self._clip = clip\n        self._epsilon = float(epsilon)\n\n    def compute_gradients(self, *args, **kwargs):\n        return self._optimizer.compute_gradients(*args, **kwargs)\n\n    def apply_gradients(self, gradvars, *args, **kwargs):\n        v_list = [tf.norm(tensor=v, ord=2) for _, v in gradvars]\n        g_list = [tf.norm(tensor=g, ord=2) if g is not None else 0.0\n                  for g, _ in gradvars]\n        v_norms = tf.stack(v_list)\n        g_norms = tf.stack(g_list)\n        zeds = tf.zeros_like(v_norms)\n        # assign epsilon if weights or grads = 0, to avoid division by zero\n        # also prevent biases to get stuck at initialization (0.)\n        cond = tf.logical_and(\n            tf.not_equal(v_norms, zeds),\n            tf.not_equal(g_norms, zeds))\n        true_vals = tf.scalar_mul(self._eta, tf.div(v_norms, g_norms))\n        # true_vals = tf.scalar_mul(tf.cast(self._eta, tf.float32), tf.div(tf.cast(v_norms, tf.float32), tf.cast(g_norms, tf.float32)))\n        false_vals = tf.fill(tf.shape(v_norms), self._epsilon)\n        larc_local_lr = tf.where(cond, true_vals, false_vals)\n        if self._clip:\n            ones = tf.ones_like(v_norms)\n            lr = tf.fill(tf.shape(v_norms), self._learning_rate)\n            # We need gradients to compute local learning rate,\n            # so compute_gradients from initial optimizer have to called\n            # for which learning rate is already fixed\n            # We then have to scale the gradients instead of the learning rate.\n            larc_local_lr = tf.minimum(tf.div(larc_local_lr, lr), ones)\n        gradvars = [(tf.multiply(larc_local_lr[i], g), v)\n                    if g is not None else (None, v)\n                    for i, (g, v) in enumerate(gradvars)]\n        return self._optimizer.apply_gradients(gradvars, *args, **kwargs)\n\n\ndef get_with_default(obj, key, default_value):\n    return obj[key] if key in obj and obj[key] is not None else default_value\n\n\ndef get_lr(lr, steps, lr_steps, warmup_it, decay_steps, global_step, lr_decay_mode,\n           cdr_first_decay_ratio, cdr_t_mul, cdr_m_mul, cdr_alpha, lc_periods, lc_alpha, lc_beta):\n    if lr_decay_mode == \'steps\':\n        learning_rate = tf.train.piecewise_constant(global_step,\n                                                    steps, lr_steps)\n    elif lr_decay_mode == \'poly\' or lr_decay_mode == \'poly_cycle\':\n        cycle = lr_decay_mode == \'poly_cycle\'\n        learning_rate = tf.train.polynomial_decay(lr,\n                                                  global_step - warmup_it,\n                                                  decay_steps=decay_steps - warmup_it,\n                                                  end_learning_rate=0.00001,\n                                                  power=2,\n                                                  cycle=cycle)\n    elif lr_decay_mode == \'cosine_decay_restarts\':\n        learning_rate = tf.train.cosine_decay_restarts(lr, \n                                                       global_step - warmup_it,\n                                                       (decay_steps - warmup_it) * cdr_first_decay_ratio,\n                                                       t_mul=cdr_t_mul, \n                                                       m_mul=cdr_m_mul,\n                                                       alpha=cdr_alpha)\n    elif lr_decay_mode == \'cosine\':\n        learning_rate = tf.train.cosine_decay(lr,\n                                              global_step - warmup_it,\n                                              decay_steps=decay_steps - warmup_it,\n                                              alpha=0.0)\n    elif lr_decay_mode == \'linear_cosine\':\n        learning_rate = tf.train.linear_cosine_decay(lr,\n                                                     global_step - warmup_it,\n                                                     decay_steps=decay_steps - warmup_it,\n                                                     num_periods=lc_periods,#0.47,\n                                                     alpha=lc_alpha,#0.0,\n                                                     beta=lc_beta)#0.00001)\n    else:\n        raise ValueError(\'Invalid type of lr_decay_mode\')\n    return learning_rate\n\n\ndef warmup_decay(warmup_lr, global_step, warmup_steps, warmup_end_lr):\n    from tensorflow.python.ops import math_ops\n    p = tf.cast(global_step, tf.float32) / tf.cast(warmup_steps, tf.float32)\n    diff = math_ops.subtract(warmup_end_lr, warmup_lr)\n    res = math_ops.add(warmup_lr, math_ops.multiply(diff, p))\n    return res\n\n\ndef cnn_model_function(features, labels, mode, params):\n    labels = tf.reshape(labels, (-1,))  # Squash unnecessary unary dim\n    lr = params[\'lr\']\n    lr_steps = params[\'lr_steps\']\n    steps = params[\'steps\']\n    use_larc = params[\'use_larc\']\n    leta = params[\'leta\']\n    lr_decay_mode = params[\'lr_decay_mode\']\n    decay_steps = params[\'decay_steps\']\n    cdr_first_decay_ratio = params[\'cdr_first_decay_ratio\']\n    cdr_t_mul = params[\'cdr_t_mul\']\n    cdr_m_mul = params[\'cdr_m_mul\']\n    cdr_alpha = params[\'cdr_alpha\']\n    lc_periods = params[\'lc_periods\']\n    lc_alpha = params[\'lc_alpha\']\n    lc_beta = params[\'lc_beta\']\n\n    model_name = params[\'model\']\n    num_classes = params[\'n_classes\']\n    model_dtype = get_with_default(params, \'dtype\', tf.float32)\n    model_format = get_with_default(params, \'format\', \'channels_first\')\n    device = get_with_default(params, \'device\', \'/gpu:0\')\n    model_func = get_model_func(model_name)\n    inputs = features  # TODO: Should be using feature columns?\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    momentum = params[\'mom\']\n    weight_decay = params[\'wdecay\']\n    warmup_lr = params[\'warmup_lr\']\n    warmup_it = params[\'warmup_it\']\n    loss_scale = params[\'loss_scale\']\n\n    adv_bn_init = params[\'adv_bn_init\']\n    conv_init = params[\'conv_init\']\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        with tf.device(\'/cpu:0\'):\n            preload_op, (inputs, labels) = stage([inputs, labels])\n\n    with tf.device(device):\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            gpucopy_op, (inputs, labels) = stage([inputs, labels])\n        inputs = tf.cast(inputs, model_dtype)\n        imagenet_mean = np.array([121, 115, 100], dtype=np.float32)\n        imagenet_std = np.array([70, 68, 71], dtype=np.float32)\n        inputs = tf.subtract(inputs, imagenet_mean)\n        inputs = tf.multiply(inputs, 1. / imagenet_std)\n        if model_format == \'channels_first\':\n            inputs = tf.transpose(inputs, [0, 3, 1, 2])\n        with fp32_trainable_vars(\n                regularizer=tf.contrib.layers.l2_regularizer(weight_decay)):\n            top_layer = model_func(\n                inputs, data_format=model_format, training=is_training,\n                conv_initializer=conv_init, adv_bn_init=adv_bn_init)\n            logits = tf.layers.dense(top_layer, num_classes,\n                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n        predicted_classes = tf.argmax(logits, axis=1, output_type=tf.int32)\n        logits = tf.cast(logits, tf.float32)\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            probabilities = tf.softmax(logits)\n            predictions = {\n                \'class_ids\': predicted_classes[:, None],\n                \'probabilities\': probabilities,\n                \'logits\': logits\n            }\n            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n        loss = tf.losses.sparse_softmax_cross_entropy(\n            logits=logits, labels=labels)\n        loss = tf.identity(loss, name=\'loss\')  # For access by logger (TODO: Better way to access it?)\n\n        if mode == tf.estimator.ModeKeys.EVAL:\n            with tf.device(None):  # Allow fallback to CPU if no GPU support for these ops\n                accuracy = tf.metrics.accuracy(\n                    labels=labels, predictions=predicted_classes)\n                top5acc = tf.metrics.mean(\n                    tf.cast(tf.nn.in_top_k(logits, labels, 5), tf.float32))\n                newaccuracy = (hvd.allreduce(accuracy[0]), accuracy[1])\n                newtop5acc = (hvd.allreduce(top5acc[0]), top5acc[1])\n                metrics = {\'val-top1acc\': newaccuracy, \'val-top5acc\': newtop5acc}\n            return tf.estimator.EstimatorSpec(\n                mode, loss=loss, eval_metric_ops=metrics)\n\n        assert (mode == tf.estimator.ModeKeys.TRAIN)\n        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        total_loss = tf.add_n([loss] + reg_losses, name=\'total_loss\')\n\n        batch_size = tf.shape(inputs)[0]\n\n        global_step = tf.train.get_global_step()\n\n        with tf.device(\'/cpu:0\'):  # Allow fallback to CPU if no GPU support for these ops\n            learning_rate = tf.cond(global_step < warmup_it,\n                                    lambda: warmup_decay(warmup_lr, global_step, warmup_it,\n                                                         lr),\n                                    lambda: get_lr(lr, steps, lr_steps, warmup_it, decay_steps, global_step,\n                                                   lr_decay_mode, \n                                                   cdr_first_decay_ratio, cdr_t_mul, cdr_m_mul, cdr_alpha, \n                                                   lc_periods, lc_alpha, lc_beta))\n            learning_rate = tf.identity(learning_rate, \'learning_rate\')\n            tf.summary.scalar(\'learning_rate\', learning_rate)\n\n        opt = tf.train.MomentumOptimizer(\n            learning_rate, momentum, use_nesterov=True)\n        opt = hvd.DistributedOptimizer(opt)\n        if use_larc:\n            opt = LarcOptimizer(opt, learning_rate, leta, clip=True)\n        opt = MixedPrecisionOptimizer(opt, scale=loss_scale)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) or []\n        with tf.control_dependencies(update_ops):\n            gate_gradients = (tf.train.Optimizer.GATE_NONE)\n            train_op = opt.minimize(\n                total_loss, global_step=tf.train.get_global_step(),\n                gate_gradients=gate_gradients)\n        train_op = tf.group(preload_op, gpucopy_op, train_op)  # , update_ops)\n\n        return tf.estimator.EstimatorSpec(mode, loss=total_loss, train_op=train_op)\n\n\ndef get_num_records(filenames):\n    def count_records(tf_record_filename):\n        count = 0\n        for _ in tf.python_io.tf_record_iterator(tf_record_filename):\n            count += 1\n        return count\n\n    nfile = len(filenames)\n    return (count_records(filenames[0]) * (nfile - 1) +\n            count_records(filenames[-1]))\n\n\ndef add_bool_argument(cmdline, shortname, longname=None, default=False, help=None):\n    if longname is None:\n        shortname, longname = None, shortname\n    elif default == True:\n        raise ValueError(""""""Boolean arguments that are True by default should not have short names."""""")\n    name = longname[2:]\n    feature_parser = cmdline.add_mutually_exclusive_group(required=False)\n    if shortname is not None:\n        feature_parser.add_argument(shortname, \'--\' + name, dest=name, action=\'store_true\', help=help, default=default)\n    else:\n        feature_parser.add_argument(\'--\' + name, dest=name, action=\'store_true\', help=help, default=default)\n    feature_parser.add_argument(\'--no\' + name, dest=name, action=\'store_false\')\n    return cmdline\n\n\ndef add_cli_args():\n    cmdline = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # Basic options\n    cmdline.add_argument(\'-m\', \'--model\', default=\'resnet50\',\n                         help=""""""Name of model to run: resnet[18,34,50,101,152]"""""")\n    cmdline.add_argument(\'--data_dir\',\n                         help=""""""Path to dataset in TFRecord format\n                         (aka Example protobufs). Files should be\n                         named \'train-*\' and \'validation-*\'."""""")\n    add_bool_argument(cmdline, \'--synthetic\', help=""""""Whether to use synthetic data for training"""""")\n    cmdline.add_argument(\'-b\', \'--batch_size\', default=256, type=int,\n                         help=""""""Size of each minibatch per GPU"""""")\n    cmdline.add_argument(\'--num_batches\', type=int,\n                         help=""""""Number of batches to run.\n                         Ignored during eval or if num epochs given"""""")\n    cmdline.add_argument(\'--num_epochs\', type=int,\n                         help=""""""Number of epochs to run.\n                         Overrides --num_batches. Ignored during eval."""""")\n    cmdline.add_argument(\'--log_dir\', default=\'imagenet_resnet\',\n                         help=""""""Directory in which to write training\n                         summaries and checkpoints. If the log directory already \n                         contains some checkpoints, it tries to resume training\n                         from the last saved checkpoint. Pass --clear_log if you\n                         want to clear all checkpoints and start a fresh run"""""")\n    add_bool_argument(cmdline, \'--clear_log\', default=False,\n                      help=""""""Clear the log folder passed so a fresh run can be started"""""")\n    cmdline.add_argument(\'--log_name\', type=str, default=\'hvd_train.log\')\n    add_bool_argument(cmdline, \'--local_ckpt\',\n                      help=""""""Performs local checkpoints (i.e. one per node)"""""")\n    cmdline.add_argument(\'--display_every\', default=50, type=int,\n                         help=""""""How often (in iterations) to print out\n                         running information."""""")\n    add_bool_argument(cmdline, \'--eval\',\n                      help=""""""Evaluate the top-1 and top-5 accuracy of\n                      the latest checkpointed model. If you want to evaluate using multiple GPUs ensure that \n                      all processes have access to all checkpoints. Either if checkpoints \n                      were saved using --local_ckpt or they were saved to a shared directory which all processes\n                      can access."""""")\n    cmdline.add_argument(\'--eval_interval\', type=int,\n                         help=""""""Evaluate accuracy per eval_interval number of epochs"""""")\n    add_bool_argument(cmdline, \'--fp16\', default=True,\n                      help=""""""Train using float16 (half) precision instead\n                      of float32."""""")\n    cmdline.add_argument(\'--num_gpus\', default=1, type=int,\n                         help=""""""Specify total number of GPUS used to train a checkpointed model during eval.\n                                Used only to calculate epoch number to print during evaluation"""""")\n\n    cmdline.add_argument(\'--save_checkpoints_steps\', type=int, default=1000)\n    cmdline.add_argument(\'--save_summary_steps\', type=int, default=0)\n    add_bool_argument(cmdline, \'--adv_bn_init\', default=True,\n                      help=""""""init gamme of the last BN of each ResMod at 0."""""")\n    add_bool_argument(cmdline, \'--adv_conv_init\', default=True,\n                      help=""""""init conv with MSRA initializer"""""")\n\n    cmdline.add_argument(\'--lr\', type=float,\n                         help=""""""Start learning rate"""""")\n    cmdline.add_argument(\'--mom\', default=0.90, type=float,\n                         help=""""""Momentum"""""")\n    cmdline.add_argument(\'--wdecay\', default=0.0001, type=float,\n                         help=""""""Weight decay"""""")\n    cmdline.add_argument(\'--loss_scale\', default=1024., type=float,\n                         help=""""""loss scale"""""")\n    cmdline.add_argument(\'--warmup_lr\', default=0.001, type=float,\n                         help=""""""Warmup starting from this learning rate"""""")\n    cmdline.add_argument(\'--warmup_epochs\', default=0, type=int,\n                         help=""""""Number of epochs in which to warmup to given lr"""""")\n    cmdline.add_argument(\'--lr_decay_steps\', default=\'30,60,80\', type=str,\n                         help=""""""epoch numbers at which lr is decayed by lr_decay_lrs. \n                         Used when lr_decay_mode is steps"""""")\n    cmdline.add_argument(\'--lr_decay_lrs\', default=\'\', type=str,\n                         help=""""""learning rates at specific epochs"""""")\n    cmdline.add_argument(\'--lr_decay_mode\', default=\'poly\',\n                         help=""""""Takes either `steps` (decay by a factor at specified steps) \n                         or `poly`(polynomial_decay with degree 2)"""""")\n    \n    add_bool_argument(cmdline, \'--use_larc\', default=False, \n                        help=""""""Use Layer wise Adaptive Rate Control which helps convergence at really large batch sizes"""""")\n    cmdline.add_argument(\'--leta\', default=0.013, type=float,\n                         help=""""""The trust coefficient for LARC optimization, LARC Eta"""""")\n    \n    cmdline.add_argument(\'--cdr_first_decay_ratio\', default=0.33, type=float,\n                         help=""""""Cosine Decay Restart First Deacy Steps ratio"""""")\n    cmdline.add_argument(\'--cdr_t_mul\', default=2.0, type=float,\n                         help=""""""Cosine Decay Restart t_mul"""""")\n    cmdline.add_argument(\'--cdr_m_mul\', default=0.1, type=float,\n                         help=""""""Cosine Decay Restart m_mul"""""")\n    cmdline.add_argument(\'--cdr_alpha\', default=0.0, type=float,\n                         help=""""""Cosine Decay Restart alpha"""""")\n    cmdline.add_argument(\'--lc_periods\', default=0.47, type=float,\n                         help=""""""Linear Cosine num of periods"""""")\n    cmdline.add_argument(\'--lc_alpha\', default=0.0, type=float,\n                         help=""""""linear Cosine alpha"""""")\n    cmdline.add_argument(\'--lc_beta\', default=0.00001, type=float,\n                         help=""""""Liner Cosine Beta"""""")\n\n    add_bool_argument(cmdline, \'--increased_aug\', default=False, \n                         help=""""""Increase augmentations helpful when training with large number of GPUs such as 128 or 256"""""")\n    cmdline.add_argument(\'--contrast\', default=0.6, type=float,\n                         help=""""""contrast factor"""""")\n    cmdline.add_argument(\'--saturation\', default=0.6, type=float,\n                         help=""""""saturation factor"""""")\n    cmdline.add_argument(\'--hue\', default=0.13, type=float,\n                         help=""""""hue max delta factor, hue delta = hue * math.pi"""""")\n    cmdline.add_argument(\'--brightness\', default=0.3, type=float,\n                         help=""""""Brightness factor"""""")\n    return cmdline\n\n\ndef sort_and_load_ckpts(log_dir):\n    ckpts = []\n    for f in os.listdir(log_dir):\n        m = re.match(r\'model.ckpt-([0-9]+).index\', f)\n        if m is None:\n            continue\n        fullpath = os.path.join(log_dir, f)\n        ckpts.append({\'step\': int(m.group(1)),\n                      \'path\': os.path.splitext(fullpath)[0],\n                      \'mtime\': os.stat(fullpath).st_mtime,\n                      })\n    ckpts.sort(key=itemgetter(\'step\'))\n    return ckpts\n\n\ndef main():\n    gpu_thread_count = 2\n    os.environ[\'TF_GPU_THREAD_MODE\'] = \'gpu_private\'\n    os.environ[\'TF_GPU_THREAD_COUNT\'] = str(gpu_thread_count)\n    os.environ[\'TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT\'] = \'1\'\n    os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n    hvd.init()\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.visible_device_list = str(hvd.local_rank())\n    config.gpu_options.force_gpu_compatible = True  # Force pinned memory\n    config.intra_op_parallelism_threads = 1  # Avoid pool of Eigen threads\n    config.inter_op_parallelism_threads = 5\n\n    # random.seed(5 * (1 + hvd.rank()))\n    # np.random.seed(7 * (1 + hvd.rank()))\n    # tf.set_random_seed(31 * (1 + hvd.rank()))\n\n    cmdline = add_cli_args()\n    FLAGS, unknown_args = cmdline.parse_known_args()\n    if len(unknown_args) > 0:\n        for bad_arg in unknown_args:\n            print(""ERROR: Unknown command line arg: %s"" % bad_arg)\n        raise ValueError(""Invalid command line arg(s)"")\n\n    FLAGS.data_dir = None if FLAGS.data_dir == """" else FLAGS.data_dir\n    FLAGS.log_dir = None if FLAGS.log_dir == """" else FLAGS.log_dir\n\n    if FLAGS.eval:\n        FLAGS.log_name = \'eval_\' + FLAGS.log_name\n        if hvd.rank() != 0:\n            return\n    if FLAGS.local_ckpt:\n        do_checkpoint = hvd.local_rank() == 0\n    else:\n        do_checkpoint = hvd.rank() == 0\n    if hvd.local_rank() == 0 and FLAGS.clear_log and os.path.isdir(FLAGS.log_dir):\n        shutil.rmtree(FLAGS.log_dir)\n    barrier = hvd.allreduce(tf.constant(0, dtype=tf.float32))\n    tf.Session(config=config).run(barrier)\n\n    if hvd.local_rank() == 0 and not os.path.isdir(FLAGS.log_dir):\n        os.makedirs(FLAGS.log_dir)\n    barrier = hvd.allreduce(tf.constant(0, dtype=tf.float32))\n    tf.Session(config=config).run(barrier)\n    \n    logger = logging.getLogger(FLAGS.log_name)\n    logger.setLevel(logging.INFO)  # INFO, ERROR\n    # file handler which logs debug messages\n    # console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n    # add formatter to the handlers\n    # formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    formatter = logging.Formatter(\'%(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    if not hvd.rank():\n        fh = logging.FileHandler(os.path.join(FLAGS.log_dir, FLAGS.log_name))\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        # add handlers to logger\n        logger.addHandler(fh)\n    \n    height, width = 224, 224\n    global_batch_size = FLAGS.batch_size * hvd.size()\n    rank0log(logger, \'PY\' + str(sys.version) + \'TF\' + str(tf.__version__))\n    rank0log(logger, ""Horovod size: "", hvd.size())\n\n    if FLAGS.data_dir:\n        filename_pattern = os.path.join(FLAGS.data_dir, \'%s-*\')\n        train_filenames = sorted(tf.gfile.Glob(filename_pattern % \'train\'))\n        eval_filenames = sorted(tf.gfile.Glob(filename_pattern % \'validation\'))\n        num_training_samples = get_num_records(train_filenames)\n        rank0log(logger, ""Using data from: "", FLAGS.data_dir)\n        if not FLAGS.eval:\n            rank0log(logger, \'Found \', num_training_samples, \' training samples\')\n    else:\n        if not FLAGS.synthetic:\n            raise ValueError(\'data_dir missing. Please pass --synthetic if you want to run on synthetic data. Else please pass --data_dir\')\n        train_filenames = eval_filenames = []\n        num_training_samples = 1281167\n    training_samples_per_rank = num_training_samples // hvd.size()\n\n    if FLAGS.num_epochs:\n        nstep = num_training_samples * FLAGS.num_epochs // global_batch_size\n    elif FLAGS.num_batches:\n        nstep = FLAGS.num_batches\n        FLAGS.num_epochs = max(nstep * global_batch_size // num_training_samples, 1)\n    else:\n        raise ValueError(""Either num_epochs or num_batches has to be passed"")\n    nstep_per_epoch = num_training_samples // global_batch_size\n    decay_steps = nstep\n\n    if FLAGS.lr_decay_mode == \'steps\':\n        steps = [int(x) * nstep_per_epoch for x in FLAGS.lr_decay_steps.split(\',\')]\n        lr_steps = [float(x) for x in FLAGS.lr_decay_lrs.split(\',\')]\n    else:\n        steps = []\n        lr_steps = []\n\n    if not FLAGS.lr:\n        if FLAGS.use_larc:\n            FLAGS.lr = 3.7\n        else:\n            FLAGS.lr = (hvd.size() * FLAGS.batch_size * 0.1) / 256\n    if not FLAGS.save_checkpoints_steps:\n        # default to save one checkpoint per epoch\n        FLAGS.save_checkpoints_steps = nstep_per_epoch\n    if not FLAGS.save_summary_steps:\n        # default to save one checkpoint per epoch\n        FLAGS.save_summary_steps = nstep_per_epoch\n    \n    if not FLAGS.eval:\n        rank0log(logger, \'Using a learning rate of \', FLAGS.lr)\n        rank0log(logger, \'Checkpointing every \' + str(FLAGS.save_checkpoints_steps) + \' steps\')\n        rank0log(logger, \'Saving summary every \' + str(FLAGS.save_summary_steps) + \' steps\')\n\n    warmup_it = nstep_per_epoch * FLAGS.warmup_epochs\n\n    classifier = tf.estimator.Estimator(\n        model_fn=cnn_model_function,\n        model_dir=FLAGS.log_dir,\n        params={\n            \'model\': FLAGS.model,\n            \'decay_steps\': decay_steps,\n            \'n_classes\': 1000,\n            \'dtype\': tf.float16 if FLAGS.fp16 else tf.float32,\n            \'format\': \'channels_first\',\n            \'device\': \'/gpu:0\',\n            \'lr\': FLAGS.lr,\n            \'mom\': FLAGS.mom,\n            \'wdecay\': FLAGS.wdecay,\n            \'use_larc\': FLAGS.use_larc,\n            \'leta\': FLAGS.leta,\n            \'steps\': steps,\n            \'lr_steps\': lr_steps,\n            \'lr_decay_mode\': FLAGS.lr_decay_mode,\n            \'warmup_it\': warmup_it,\n            \'warmup_lr\': FLAGS.warmup_lr,\n            \'cdr_first_decay_ratio\': FLAGS.cdr_first_decay_ratio,\n            \'cdr_t_mul\': FLAGS.cdr_t_mul,\n            \'cdr_m_mul\': FLAGS.cdr_m_mul,\n            \'cdr_alpha\': FLAGS.cdr_alpha,\n            \'lc_periods\': FLAGS.lc_periods,\n            \'lc_alpha\': FLAGS.lc_alpha,\n            \'lc_beta\': FLAGS.lc_beta,\n            \'loss_scale\': FLAGS.loss_scale,\n            \'adv_bn_init\': FLAGS.adv_bn_init,\n            \'conv_init\': tf.variance_scaling_initializer() if FLAGS.adv_conv_init else None\n        },\n        config=tf.estimator.RunConfig(\n            # tf_random_seed=31 * (1 + hvd.rank()),\n            session_config=config,\n            save_summary_steps=FLAGS.save_summary_steps if do_checkpoint else None,\n            save_checkpoints_steps=FLAGS.save_checkpoints_steps if do_checkpoint else None,\n            keep_checkpoint_max=None))\n\n    if not FLAGS.eval:\n        num_preproc_threads = 5\n        rank0log(logger, ""Using preprocessing threads per GPU: "", num_preproc_threads)\n        training_hooks = [hvd.BroadcastGlobalVariablesHook(0),\n                          PrefillStagingAreasHook()]\n        if hvd.rank() == 0:\n            training_hooks.append(\n                LogSessionRunHook(global_batch_size,\n                                  num_training_samples,\n                                  FLAGS.display_every, logger))\n        try:\n            start_time = time.time()\n            classifier.train(\n                input_fn=lambda: make_dataset(\n                    train_filenames,\n                    training_samples_per_rank,\n                    FLAGS.batch_size, height, width, \n                    FLAGS.brightness, FLAGS.contrast, FLAGS.saturation, FLAGS.hue, \n                    training=True, num_threads=num_preproc_threads, \n                    shard=True, synthetic=FLAGS.synthetic, increased_aug=FLAGS.increased_aug),\n                max_steps=nstep,\n                hooks=training_hooks)\n            rank0log(logger, ""Finished in "", time.time() - start_time)\n        except KeyboardInterrupt:\n            print(""Keyboard interrupt"")\n    elif FLAGS.eval and not FLAGS.synthetic:\n        rank0log(logger, ""Evaluating"")\n        rank0log(logger, ""Validation dataset size: {}"".format(get_num_records(eval_filenames)))\n        barrier = hvd.allreduce(tf.constant(0, dtype=tf.float32))\n        tf.Session(config=config).run(barrier)\n        time.sleep(5)  # a little extra margin...\n        if FLAGS.num_gpus == 1:\n            rank0log(logger, """"""If you are evaluating checkpoints of a multi-GPU run on a single GPU,\n             ensure you set --num_gpus to the number of GPUs it was trained on.\n             This will ensure that the epoch number is accurately displayed in the below logs."""""")\n        try:\n            ckpts = sort_and_load_ckpts(FLAGS.log_dir)\n            for i, c in enumerate(ckpts):\n                if i < len(ckpts) - 1:\n                    if (not FLAGS.eval_interval) or \\\n                            (i % FLAGS.eval_interval != 0):\n                        continue\n                eval_result = classifier.evaluate(\n                    input_fn=lambda: make_dataset(\n                        eval_filenames,\n                        get_num_records(eval_filenames), FLAGS.batch_size,\n                        height, width, \n                        FLAGS.brightness, FLAGS.contrast, FLAGS.saturation, FLAGS.hue,\n                        training=False, shard=True, increased_aug=False),\n                    checkpoint_path=c[\'path\'])\n                c[\'epoch\'] = math.ceil(c[\'step\'] / (num_training_samples / (FLAGS.batch_size * FLAGS.num_gpus)))\n                c[\'top1\'] = eval_result[\'val-top1acc\']\n                c[\'top5\'] = eval_result[\'val-top5acc\']\n                c[\'loss\'] = eval_result[\'loss\']\n            rank0log(logger, \' step  epoch  top1    top5     loss   checkpoint_time(UTC)\')\n            barrier = hvd.allreduce(tf.constant(0, dtype=tf.float32))\n            for i, c in enumerate(ckpts):\n                tf.Session(config=config).run(barrier)\n                if \'top1\' not in c:\n                    continue\n                rank0log(logger,\'{:5d}  {:5.1f}  {:5.3f}  {:6.2f}  {:6.2f}  {time}\'\n                         .format(c[\'step\'],\n                                 c[\'epoch\'],\n                                 c[\'top1\'] * 100,\n                                 c[\'top5\'] * 100,\n                                 c[\'loss\'],\n                                 time=time.strftime(\'%Y-%m-%d %H:%M:%S\', \n                                    time.localtime(c[\'mtime\']))))\n            rank0log(logger, ""Finished evaluation"")\n        except KeyboardInterrupt:\n            logger.error(""Keyboard interrupt"")\n\nif __name__ == \'__main__\':\n    main()\n'"
benchmarks/tf_benchmarks/execute_tensorflow_training.py,0,"b'#!/usr/bin/env python\n# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport argparse\nimport itertools\nimport os\n\nfrom sagemaker import Session\nfrom sagemaker.estimator import Framework\nfrom sagemaker.tensorflow import TensorFlow\n\ndefault_bucket = Session().default_bucket\ndir_path = os.path.dirname(os.path.realpath(__file__))\n\n_DEFAULT_HYPERPARAMETERS = {\n    \'batch_size\':           32,\n    \'model\':                \'resnet32\',\n    \'num_epochs\':           10,\n    \'data_format\':          \'NHWC\',\n    \'summary_verbosity\':    1,\n    \'save_summaries_steps\': 10,\n    \'data_name\':            \'cifar10\'\n}\n\n\nclass ScriptModeTensorFlow(Framework):\n    """"""This class is temporary until the final version of Script Mode is released.\n    """"""\n\n    __framework_name__ = ""tensorflow-scriptmode-beta""\n\n    create_model = TensorFlow.create_model\n\n    def __init__(self, py_version=\'py3\', **kwargs):\n        super(ScriptModeTensorFlow, self).__init__(**kwargs)\n        self.py_version = py_version\n        self.image_name = None\n        self.framework_version = \'1.10.0\'\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-t\', \'--instance-types\', nargs=\'+\', help=\'<Required> Set flag\', required=True)\n    parser.add_argument(\'-r\', \'--role\', required=True)\n    parser.add_argument(\'-w\', \'--wait\', action=\'store_true\')\n    parser.add_argument(\'--region\', default=\'us-west-2\')\n    parser.add_argument(\'--py-versions\', nargs=\'+\', help=\'<Required> Set flag\', default=[\'py3\'])\n    parser.add_argument(\'--checkpoint-path\',\n                        default=os.path.join(default_bucket(), \'benchmarks\', \'checkpoints\'),\n                        help=\'The S3 location where the model checkpoints and tensorboard events are saved after training\')\n\n    return parser.parse_known_args()\n\n\ndef main(args, script_args):\n    for instance_type, py_version in itertools.product(args.instance_types, args.py_versions):\n        base_name = \'%s-%s-%s\' % (py_version, instance_type[3:5], instance_type[6:])\n        model_dir = os.path.join(args.checkpoint_path, base_name)\n\n        job_hps = create_hyperparameters(model_dir, script_args)\n\n        print(\'hyperparameters:\')\n        print(job_hps)\n\n        estimator = ScriptModeTensorFlow(\n            entry_point=\'tf_cnn_benchmarks.py\',\n            role=\'SageMakerRole\',\n            source_dir=os.path.join(dir_path, \'tf_cnn_benchmarks\'),\n            base_job_name=base_name,\n            train_instance_count=1,\n            hyperparameters=job_hps,\n            train_instance_type=instance_type,\n        )\n\n        input_dir = \'s3://sagemaker-sample-data-%s/spark/mnist/train/\' % args.region\n        estimator.fit({\'train\': input_dir}, wait=args.wait)\n\n    print(""To use TensorBoard, execute the following command:"")\n    cmd = \'S3_USE_HTTPS=0 S3_VERIFY_SSL=0  AWS_REGION=%s tensorboard --host localhost --port 6006 --logdir %s\'\n    print(cmd % (args.region, args.checkpoint_path))\n\n\ndef create_hyperparameters(model_dir, script_args):\n    job_hps = _DEFAULT_HYPERPARAMETERS.copy()\n\n    job_hps.update({\'train_dir\': model_dir, \'eval_dir\': model_dir})\n\n    script_arg_keys_without_dashes = [key[2:] if key.startswith(\'--\') else key[1:] for key in script_args[::2]]\n    script_arg_values = script_args[1::2]\n    job_hps.update(dict(zip(script_arg_keys_without_dashes, script_arg_values)))\n\n    return job_hps\n\n\nif __name__ == \'__main__\':\n    args, script_args = get_args()\n    main(args, script_args)\n'"
docker/build_artifacts/__init__.py,0,b''
docker/build_artifacts/deep_learning_container.py,0,"b'# Copyright 2018-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport json\nimport logging\nimport re\n\nimport requests\n\n\ndef _validate_instance_id(instance_id):\n    """"""\n    Validate instance ID\n    """"""\n    instance_id_regex = r\'^(i-\\S{17})\'\n    compiled_regex = re.compile(instance_id_regex)\n    match = compiled_regex.match(instance_id)\n\n    if not match:\n        return None\n\n    return match.group(1)\n\n\ndef _retrieve_instance_id():\n    """"""\n    Retrieve instance ID from instance metadata service\n    """"""\n    instance_id = None\n    url = ""http://169.254.169.254/latest/meta-data/instance-id""\n    response = requests_helper(url, timeout=0.1)\n\n    if response is not None:\n        instance_id = _validate_instance_id(response.text)\n\n    return instance_id\n\n\ndef _retrieve_instance_region():\n    """"""\n    Retrieve instance region from instance metadata service\n    """"""\n    region = None\n    valid_regions = [\'ap-northeast-1\', \'ap-northeast-2\', \'ap-southeast-1\', \'ap-southeast-2\',\n                     \'ap-south-1\', \'ca-central-1\', \'eu-central-1\', \'eu-north-1\',\n                     \'eu-west-1\', \'eu-west-2\', \'eu-west-3\', \'sa-east-1\',\n                     \'us-east-1\', \'us-east-2\', \'us-west-1\', \'us-west-2\']\n\n    url = ""http://169.254.169.254/latest/dynamic/instance-identity/document""\n    response = requests_helper(url, timeout=0.1)\n\n    if response is not None:\n        response_json = json.loads(response.text)\n\n        if response_json[\'region\'] in valid_regions:\n            region = response_json[\'region\']\n\n    return region\n\n\ndef query_bucket():\n    """"""\n    GET request on an empty object from an Amazon S3 bucket\n    """"""\n    response = None\n    instance_id = _retrieve_instance_id()\n    region = _retrieve_instance_region()\n\n    if instance_id is not None and region is not None:\n        url = (""https://aws-deep-learning-containers-{0}.s3.{0}.amazonaws.com""\n               ""/dlc-containers.txt?x-instance-id={1}"".format(region, instance_id))\n        response = requests_helper(url, timeout=0.2)\n\n    logging.debug(""Query bucket finished: {}"".format(response))\n\n    return response\n\n\ndef requests_helper(url, timeout):\n    response = None\n    try:\n        response = requests.get(url, timeout=timeout)\n    except requests.exceptions.RequestException as e:\n        logging.error(""Request exception: {}"".format(e))\n\n    return response\n\n\ndef main():\n    """"""\n    Invoke bucket query\n    """"""\n    # Logs are not necessary for normal run. Remove this line while debugging.\n    logging.getLogger().disabled = True\n\n    logging.basicConfig(level=logging.ERROR)\n    query_bucket()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
docker/build_artifacts/dockerd-entrypoint.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os.path\nimport shlex\nimport subprocess\nimport sys\n\nif not os.path.exists(""/opt/ml/input/config""):\n    subprocess.call([\'python\', \'/usr/local/bin/deep_learning_container.py\', \'&>/dev/null\', \'&\'])\n\nsubprocess.check_call(shlex.split(\' \'.join(sys.argv[1:])))\n'"
src/sagemaker_tensorflow_container/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\n'"
src/sagemaker_tensorflow_container/s3_utils.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \'license\' file accompanying this file. This file is\n# distributed on an \'AS IS\' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nimport boto3\nfrom six.moves.urllib.parse import urlparse\n\n\ndef configure(model_dir, job_region):\n\n    os.environ[\'S3_REGION\'] = _s3_region(job_region, model_dir)\n\n    # setting log level to WARNING\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n    os.environ[\'S3_USE_HTTPS\'] = \'1\'\n\n\ndef _s3_region(job_region, model_dir):\n    if model_dir and model_dir.startswith(\'s3://\'):\n        s3 = boto3.client(\'s3\', region_name=job_region)\n\n        # We get the AWS region of the checkpoint bucket, which may be different from\n        # the region this container is currently running in.\n        parsed_url = urlparse(model_dir)\n        bucket_name = parsed_url.netloc\n\n        bucket_location = s3.get_bucket_location(Bucket=bucket_name)[\'LocationConstraint\']\n\n        return bucket_location or job_region\n    else:\n        return job_region\n'"
src/sagemaker_tensorflow_container/training.py,4,"b'# Copyright 2018-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \'license\' file accompanying this file. This file is\n# distributed on an \'AS IS\' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport json\nimport logging\nimport multiprocessing\nimport os\nimport subprocess\nimport time\n\nfrom sagemaker_training import entry_point, environment, mapping, runner\nimport tensorflow as tf\n\nfrom sagemaker_tensorflow_container import s3_utils\n\nlogger = logging.getLogger(__name__)\n\nSAGEMAKER_PARAMETER_SERVER_ENABLED = \'sagemaker_parameter_server_enabled\'\nMODEL_DIR = \'/opt/ml/model\'\n\n\ndef _is_host_master(hosts, current_host):\n    return current_host == hosts[0]\n\n\ndef _build_tf_config(hosts, current_host, ps_task=False):\n    """"""Builds a dictionary containing cluster information based on number of hosts and number of\n    parameter servers.\n\n    Args:\n        hosts (list[str]): List of host names in the cluster\n        current_host (str): Current host name\n        ps_task (bool): Set to True if this config is built for a parameter server process\n            (default: False)\n\n    Returns:\n        dict[str: dict]: A dictionary describing the cluster setup for distributed training.\n            For more information regarding TF_CONFIG:\n            https://cloud.google.com/ml-engine/docs/tensorflow/distributed-training-details\n    """"""\n    # Assign the first host as the master. Rest of the hosts if any will be worker hosts.\n    # The first ps_num hosts will also have a parameter task assign to them.\n    masters = hosts[:1]\n    workers = hosts[1:]\n    ps = hosts if len(hosts) > 1 else None\n\n    def host_addresses(hosts, port=2222):\n        return [\'{}:{}\'.format(host, port) for host in hosts]\n\n    tf_config = {\n        \'cluster\': {\n            \'master\': host_addresses(masters)\n        },\n        \'environment\': \'cloud\'\n    }\n\n    if ps:\n        tf_config[\'cluster\'][\'ps\'] = host_addresses(ps, port=\'2223\')\n\n    if workers:\n        tf_config[\'cluster\'][\'worker\'] = host_addresses(workers)\n\n    if ps_task:\n        if ps is None:\n            raise ValueError(\n                \'Cannot have a ps task if there are no parameter servers in the cluster\')\n        task_type = \'ps\'\n        task_index = ps.index(current_host)\n    elif _is_host_master(hosts, current_host):\n        task_type = \'master\'\n        task_index = 0\n    else:\n        task_type = \'worker\'\n        task_index = workers.index(current_host)\n\n    tf_config[\'task\'] = {\'index\': task_index, \'type\': task_type}\n    return tf_config\n\n\ndef _run_ps(env, cluster):\n    logger.info(\'Running distributed training job with parameter servers\')\n\n    cluster_spec = tf.train.ClusterSpec(cluster)\n    task_index = env.hosts.index(env.current_host)\n    # Force parameter server to run on cpu. Running multiple TensorFlow processes on the same\n    # GPU is not safe:\n    # https://stackoverflow.com/questions/46145100/is-it-unsafe-to-run-multiple-tensorflow-processes-on-the-same-gpu\n    no_gpu_config = tf.ConfigProto(device_count={\'GPU\': 0})\n\n    server = tf.train.Server(\n        cluster_spec, job_name=\'ps\', task_index=task_index, config=no_gpu_config\n    )\n\n    multiprocessing.Process(target=lambda: server.join()).start()\n\n\ndef _run_worker(env, cmd_args, tf_config):\n    env_vars = env.to_env_vars()\n    env_vars[\'TF_CONFIG\'] = json.dumps(tf_config)\n\n    entry_point.run(env.module_dir, env.user_entry_point, cmd_args, env_vars)\n\n\ndef _wait_until_master_is_down(master):\n    while True:\n        try:\n            subprocess.check_call(\n                [\'curl\', \'{}:2222\'.format(master)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            logger.info(\'master {} is still up, waiting for it to exit\'.format(master))\n            time.sleep(10)\n        except subprocess.CalledProcessError:\n            logger.info(\'master {} is down, stopping parameter server\'.format(master))\n            return\n\n\ndef train(env, cmd_args):\n    """"""Get training job environment from env and run the training job.\n\n    Args:\n        env (sagemaker_training.env.TrainingEnv): Instance of TrainingEnv class\n    """"""\n    parameter_server_enabled = env.additional_framework_parameters.get(\n        SAGEMAKER_PARAMETER_SERVER_ENABLED, False)\n    if len(env.hosts) > 1 and parameter_server_enabled:\n\n        tf_config = _build_tf_config(hosts=env.hosts, current_host=env.current_host)\n\n        logger.info(\'Running distributed training job with parameter servers\')\n        logger.info(\'Launching parameter server process\')\n        _run_ps(env, tf_config[\'cluster\'])\n        logger.info(\'Launching worker process\')\n        _run_worker(env, cmd_args, tf_config)\n\n        if not _is_host_master(env.hosts, env.current_host):\n            _wait_until_master_is_down(env.hosts[0])\n\n    else:\n\n        mpi_enabled = env.additional_framework_parameters.get(\'sagemaker_mpi_enabled\')\n\n        if mpi_enabled:\n            runner_type = runner.MPIRunnerType\n        else:\n            runner_type = runner.ProcessRunnerType\n\n        entry_point.run(env.module_dir, env.user_entry_point, cmd_args, env.to_env_vars(),\n                        runner_type=runner_type)\n\n\ndef _log_model_missing_warning(model_dir):\n    pb_file_exists = False\n    file_exists = False\n    for dirpath, dirnames, filenames in os.walk(model_dir):\n        if filenames:\n            file_exists = True\n        for f in filenames:\n            if \'saved_model.pb\' in f or \'saved_model.pbtxt\' in f:\n                pb_file_exists = True\n                path, direct_parent_dir = os.path.split(dirpath)\n                if not str.isdigit(direct_parent_dir):\n                    logger.warn(\'Your model will NOT be servable with SageMaker TensorFlow Serving containers. \'\n                                \'The SavedModel bundle is under directory \\""{}\\"", not a numeric name.\'\n                                .format(direct_parent_dir))\n\n    if not file_exists:\n        logger.warn(\'No model artifact is saved under path {}.\'\n                    \' Your training job will not save any model files to S3.\\n\'\n                    \'For details of how to construct your training script see:\\n\'\n                    \'https://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\'\n                    .format(model_dir))\n    elif not pb_file_exists:\n        logger.warn(\'Your model will NOT be servable with SageMaker TensorFlow Serving container. \'\n                    \'The model artifact was not saved in the TensorFlow SavedModel directory structure:\\n\'\n                    \'https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\')\n\n\ndef _model_dir_with_training_job(model_dir, job_name):\n    if model_dir.startswith(\'/opt/ml\'):\n        return model_dir\n    else:\n        return \'{}/{}/model\'.format(model_dir, job_name)\n\n\ndef main():\n    """"""Training entry point\n    """"""\n    hyperparameters = environment.read_hyperparameters()\n    env = environment.Environment(hyperparameters=hyperparameters)\n\n    user_hyperparameters = env.hyperparameters\n\n    # If the training job is part of the multiple training jobs for tuning, we need to append the training job name to\n    # model_dir in case they read from/write to the same object\n    if \'_tuning_objective_metric\' in hyperparameters:\n        model_dir = _model_dir_with_training_job(hyperparameters.get(\'model_dir\'), env.job_name)\n        logger.info(\'Appending the training job name to model_dir: {}\'.format(model_dir))\n        user_hyperparameters[\'model_dir\'] = model_dir\n\n    s3_utils.configure(user_hyperparameters.get(\'model_dir\'), os.environ.get(\'SAGEMAKER_REGION\'))\n    train(env, mapping.to_cmd_args(user_hyperparameters))\n    _log_model_missing_warning(MODEL_DIR)\n'"
test-toolkit/integration/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport logging\nimport os\n\nlogging.getLogger(\'boto3\').setLevel(logging.INFO)\nlogging.getLogger(\'botocore\').setLevel(logging.INFO)\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'resources\')\n\n# these regions have some p2 and p3 instances, but not enough for automated testing\nNO_P2_REGIONS = [\n    \'ca-central-1\',\n    \'eu-central-1\',\n    \'eu-west-2\',\n    \'us-west-1\',\n    \'eu-west-3\',\n    \'eu-north-1\',\n    \'sa-east-1\',\n    \'ap-east-1\',\n    \'me-south-1\'\n]\nNO_P3_REGIONS = [\n    \'ap-southeast-1\',\n    \'ap-southeast-2\',\n    \'ap-south-1\',\n    \'ca-central-1\',\n    \'eu-central-1\',\n    \'eu-west-2\',\n    \'us-west-1\'\n    \'eu-west-3\',\n    \'eu-north-1\',\n    \'sa-east-1\',\n    \'ap-east-1\',\n    \'me-south-1\'\n]\n'"
test-toolkit/integration/conftest.py,0,"b'#  Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport logging\nimport os\n\nimport boto3\nimport pytest\nfrom sagemaker import LocalSession, Session\n\nfrom integration import image_utils\nfrom integration import NO_P2_REGIONS, NO_P3_REGIONS\n\n\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\'boto\').setLevel(logging.INFO)\nlogging.getLogger(\'botocore\').setLevel(logging.INFO)\nlogging.getLogger(\'factory.py\').setLevel(logging.INFO)\nlogging.getLogger(\'auth.py\').setLevel(logging.INFO)\nlogging.getLogger(\'connectionpool.py\').setLevel(logging.INFO)\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\'--build-image\', \'-B\', action=\'store_true\')\n    parser.addoption(\'--push-image\', \'-P\', action=\'store_true\')\n    parser.addoption(\'--dockerfile-type\', \'-T\', choices=[\'dlc.cpu\', \'dlc.gpu\', \'tf\'],\n                     default=\'tf\')\n    parser.addoption(\'--dockerfile\', \'-D\', default=None)\n    parser.addoption(\'--docker-base-name\', default=\'sagemaker-tensorflow-training\')\n    parser.addoption(\'--tag\', default=None)\n    parser.addoption(\'--region\', default=\'us-west-2\')\n    parser.addoption(\'--framework-version\', default=\'1.15.2\')\n    parser.addoption(\'--processor\', default=\'cpu\', choices=[\'cpu\', \'gpu\', \'cpu,gpu\'])\n    parser.addoption(\'--py-version\', default=\'3\', choices=[\'2\', \'3\', \'2,3\'])\n    parser.addoption(\'--account-id\', default=\'142577830533\')\n    parser.addoption(\'--instance-type\', default=None)\n\n\ndef pytest_configure(config):\n    os.environ[\'TEST_PY_VERSIONS\'] = config.getoption(\'--py-version\')\n    os.environ[\'TEST_PROCESSORS\'] = config.getoption(\'--processor\')\n\n\n@pytest.fixture(scope=\'session\', name=\'dockerfile_type\')\ndef fixture_dockerfile_type(request):\n    return request.config.getoption(\'--dockerfile-type\')\n\n\n@pytest.fixture(scope=\'session\', name=\'dockerfile\')\ndef fixture_dockerfile(request, dockerfile_type):\n    dockerfile = request.config.getoption(\'--dockerfile\')\n    return dockerfile if dockerfile else \'Dockerfile.{}\'.format(dockerfile_type)\n\n\n@pytest.fixture(scope=\'session\', name=\'build_image\', autouse=True)\ndef fixture_build_image(request, framework_version, dockerfile, image_uri, region):\n    build_image = request.config.getoption(\'--build-image\')\n    if build_image:\n        return image_utils.build_image(framework_version=framework_version,\n                                       dockerfile=dockerfile,\n                                       image_uri=image_uri,\n                                       region=region,\n                                       cwd=os.path.join(DIR_PATH, \'..\', \'..\'))\n\n    return image_uri\n\n\n@pytest.fixture(scope=\'session\', name=\'push_image\', autouse=True)\ndef fixture_push_image(request, image_uri, region, account_id):\n    push_image = request.config.getoption(\'--push-image\')\n    if push_image:\n        return image_utils.push_image(image_uri, region, account_id)\n    return None\n\n\n@pytest.fixture(scope=\'session\')\ndef docker_base_name(request):\n    return request.config.getoption(\'--docker-base-name\')\n\n\n@pytest.fixture(scope=\'session\')\ndef region(request):\n    return request.config.getoption(\'--region\')\n\n\n@pytest.fixture(scope=\'session\')\ndef framework_version(request):\n    return request.config.getoption(\'--framework-version\')\n\n\n@pytest.fixture(scope=\'session\')\ndef tag(request, framework_version, processor, py_version):\n    provided_tag = request.config.getoption(\'--tag\')\n    default_tag = \'{}-{}-py{}\'.format(framework_version, processor, py_version)\n    return provided_tag if provided_tag is not None else default_tag\n\n\n@pytest.fixture(scope=\'session\')\ndef sagemaker_session(region):\n    return Session(boto_session=boto3.Session(region_name=region))\n\n\n@pytest.fixture(scope=\'session\')\ndef sagemaker_local_session(region):\n    return LocalSession(boto_session=boto3.Session(region_name=region))\n\n\n@pytest.fixture(scope=\'session\')\ndef account_id(request):\n    return request.config.getoption(\'--account-id\')\n\n\n@pytest.fixture\ndef instance_type(request, processor):\n    provided_instance_type = request.config.getoption(\'--instance-type\')\n    default_instance_type = \'ml.c4.xlarge\' if processor == \'cpu\' else \'ml.p2.xlarge\'\n    return provided_instance_type if provided_instance_type is not None else default_instance_type\n\n\n@pytest.fixture(autouse=True)\ndef skip_by_device_type(request, processor):\n    is_gpu = (processor == \'gpu\')\n    if (request.node.get_closest_marker(\'skip_gpu\') and is_gpu) or \\\n            (request.node.get_closest_marker(\'skip_cpu\') and not is_gpu):\n        pytest.skip(\'Skipping because running on \\\'{}\\\' instance\'.format(processor))\n\n\n@pytest.fixture(autouse=True)\ndef skip_gpu_instance_restricted_regions(region, instance_type):\n    if (region in NO_P2_REGIONS and instance_type.startswith(\'ml.p2\')) or \\\n            (region in NO_P3_REGIONS and instance_type.startswith(\'ml.p3\')):\n        pytest.skip(\'Skipping GPU test in region {}\'.format(region))\n\n\n@pytest.fixture(autouse=True)\ndef skip_by_dockerfile_type(request, dockerfile_type):\n    is_generic = (dockerfile_type == \'tf\')\n    if request.node.get_closest_marker(\'skip_generic\') and is_generic:\n        pytest.skip(\'Skipping because running generic image without mpi and horovod\')\n\n\n@pytest.fixture(name=\'docker_registry\', scope=\'session\')\ndef fixture_docker_registry(account_id, region):\n    return \'{}.dkr.ecr.{}.amazonaws.com\'.format(account_id, region) if account_id else None\n\n\n@pytest.fixture(name=\'image_uri\', scope=\'session\')\ndef fixture_image_uri(docker_registry, docker_base_name, tag):\n    if docker_registry:\n        return \'{}/{}:{}\'.format(docker_registry, docker_base_name, tag)\n    return \'{}:{}\'.format(docker_base_name, tag)\n'"
test-toolkit/integration/image_utils.py,0,"b'# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport subprocess\nimport sys\n\nCYAN_COLOR = \'\\033[36m\'\nEND_COLOR = \'\\033[0m\'\nDLC_AWS_ID = \'763104351884\'\n\n\ndef build_image(framework_version, dockerfile, image_uri, region, cwd=\'.\'):\n    _check_call(\'python setup.py sdist\')\n\n    if \'dlc\' in dockerfile:\n        ecr_login(region, DLC_AWS_ID)\n\n    dockerfile_location = os.path.join(\'test-toolkit\', \'docker\', framework_version, dockerfile)\n\n    subprocess.check_call(\n        [\'docker\', \'build\', \'-t\', image_uri, \'-f\', dockerfile_location, \'--build-arg\',\n         \'region={}\'.format(region), cwd], cwd=cwd)\n    print(\'created image {}\'.format(image_uri))\n    return image_uri\n\n\ndef push_image(ecr_image, region, aws_id):\n    ecr_login(region, aws_id)\n    _check_call(\'docker push {}\'.format(ecr_image))\n\n\ndef ecr_login(region, aws_id):\n    login = _check_call(\'aws ecr get-login --registry-ids {} \'.format(aws_id)\n                        + \'--no-include-email --region {}\'.format(region))\n    _check_call(login.decode(\'utf-8\').rstrip(\'\\n\'))\n\n\ndef _check_call(cmd, *popenargs, **kwargs):\n    if isinstance(cmd, str):\n        cmd = cmd.split("" "")\n    _print_cmd(cmd)\n    return subprocess.check_output(cmd, *popenargs, **kwargs)\n\n\ndef _print_cmd(cmd):\n    print(\'executing docker command: {}{}{}\'.format(CYAN_COLOR, \' \'.join(cmd), END_COLOR))\n    sys.stdout.flush()\n'"
test-toolkit/integration/utils.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport random\nimport time\n\nimport pytest\n\n\ndef unique_name_from_base(base, max_length=63):\n    unique = \'%04x\' % random.randrange(16**4)  # 4-digit hex\n    ts = str(int(time.time()))\n    available_length = max_length - 2 - len(ts) - len(unique)\n    trimmed = base[:available_length]\n    return \'{}-{}-{}\'.format(trimmed, ts, unique)\n\n\n@pytest.fixture(params=os.environ[\'TEST_PY_VERSIONS\'].split(\',\'), scope=\'session\')\ndef py_version(request):\n    return request.param\n\n\n@pytest.fixture(params=os.environ[\'TEST_PROCESSORS\'].split(\',\'), scope=\'session\')\ndef processor(request):\n    return request.param\n'"
test-toolkit/resources/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test-toolkit/unit/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test-toolkit/unit/test_s3_utils.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \'license\' file accompanying this file. This file is\n# distributed on an \'AS IS\' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nfrom mock import MagicMock, patch\n\nfrom sagemaker_tensorflow_container import s3_utils\n\n\nBUCKET_REGION = \'us-west-2\'\nJOB_REGION = \'us-west-1\'\nJOB_BUKCET = \'sagemaker-us-west-2-000-00-1\'\nPREFIX = \'sagemaker/something\'\nMODEL_DIR = \'s3://{}/{}\'.format(JOB_BUKCET, PREFIX)\n\n\n@patch(\'boto3.client\')\ndef test_configure(client):\n    s3 = MagicMock()\n    client.return_value = s3\n    loc = {\'LocationConstraint\': BUCKET_REGION}\n    s3.get_bucket_location.return_value = loc\n\n    s3_utils.configure(MODEL_DIR, JOB_REGION)\n\n    assert os.environ[\'S3_REGION\'] == BUCKET_REGION\n    assert os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] == \'1\'\n    assert os.environ[\'S3_USE_HTTPS\'] == \'1\'\n\n\ndef test_configure_local_dir():\n    s3_utils.configure(\'/opt/ml/model\', JOB_REGION)\n\n    assert os.environ[\'S3_REGION\'] == JOB_REGION\n    assert os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] == \'1\'\n    assert os.environ[\'S3_USE_HTTPS\'] == \'1\'\n'"
test-toolkit/unit/test_training.py,3,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \'license\' file accompanying this file. This file is\n# distributed on an \'AS IS\' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport sys\n\nfrom mock import MagicMock, patch\nimport pytest\nfrom sagemaker_training import runner\nimport tensorflow as tf\n\nfrom sagemaker_tensorflow_container import training\n\nMODULE_DIR = \'s3://my/bucket\'\nMODULE_NAME = \'script_name\'\nLOG_LEVEL = \'Debug\'\nHOST1 = \'host1\'\nHOST2 = \'host2\'\nHOST_LIST = [HOST1, HOST2]\nCURRENT_HOST = HOST1\nCMD_ARGS = {\'some_key\': \'some_value\'}\nCLUSTER_WITH_PS = {\n    \'master\': [\'{}:2222\'.format(HOST1)],\n    \'worker\': [\'{}:2222\'.format(HOST2)],\n    \'ps\': [\'{}:2223\'.format(HOST1), \'{}:2223\'.format(HOST2)]\n}\nMASTER_TASK = {\'index\': 0, \'type\': \'master\'}\nWORKER_TASK = {\'index\': 0, \'type\': \'worker\'}\nPS_TASK_1 = {\'index\': 0, \'type\': \'ps\'}\nPS_TASK_2 = {\'index\': 1, \'type\': \'ps\'}\nMODEL_DIR = \'s3://bucket/prefix\'\nMODEL_DIR_CMD_LIST = [\'--model_dir\', MODEL_DIR]\nREGION = \'us-west-2\'\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'resources\')\n\n\n@pytest.fixture\ndef distributed_training_env():\n    env = simple_training_env()\n\n    env.hosts = HOST_LIST\n    env.additional_framework_parameters = {\n        training.SAGEMAKER_PARAMETER_SERVER_ENABLED: True\n    }\n    return env\n\n\n@pytest.fixture\ndef single_machine_training_env():\n    return simple_training_env()\n\n\ndef simple_training_env():\n    env = MagicMock()\n    env.module_dir = MODULE_DIR\n    env.user_entry_point = MODULE_NAME\n    env.hyperparameters = {\'model_dir\': MODEL_DIR}\n    env.log_level = LOG_LEVEL\n    env.additional_framework_parameters = {}\n    env.hosts = CURRENT_HOST\n    env.current_host = CURRENT_HOST\n    env.to_env_vars = lambda: {}\n    env.job_name = \'test-training-job\'\n    return env\n\n\ndef test_is_host_master():\n    assert training._is_host_master(HOST_LIST, CURRENT_HOST) is True\n    assert training._is_host_master(HOST_LIST, \'host2\') is False\n    assert training._is_host_master(HOST_LIST, \'somehost\') is False\n\n\n@patch(\'sagemaker_training.entry_point.run\')\ndef test_single_machine(run_module, single_machine_training_env):\n    training.train(single_machine_training_env, MODEL_DIR_CMD_LIST)\n    run_module.assert_called_with(MODULE_DIR, MODULE_NAME, MODEL_DIR_CMD_LIST,\n                                  single_machine_training_env.to_env_vars(),\n                                  runner_type=runner.ProcessRunnerType)\n\n\n@patch(\'sagemaker_training.entry_point.run\')\ndef test_train_horovod(run_module, single_machine_training_env):\n    single_machine_training_env.additional_framework_parameters[\'sagemaker_mpi_enabled\'] = True\n\n    training.train(single_machine_training_env, MODEL_DIR_CMD_LIST)\n    run_module.assert_called_with(MODULE_DIR, MODULE_NAME, MODEL_DIR_CMD_LIST,\n                                  single_machine_training_env.to_env_vars(),\n                                  runner_type=runner.MPIRunnerType)\n\n\n@pytest.mark.skip_on_pipeline\n@pytest.mark.skipif(sys.version_info.major != 3,\n                    reason=""Skip this for python 2 because of dict key order mismatch"")\n@patch(\'tensorflow.train.ClusterSpec\')\n@patch(\'tensorflow.train.Server\')\n@patch(\'sagemaker_training.entry_point.run\')\n@patch(\'multiprocessing.Process\', lambda target: target())\n@patch(\'time.sleep\', MagicMock())\ndef test_train_distributed_master(run, tf_server, cluster_spec, distributed_training_env):\n    training.train(distributed_training_env, MODEL_DIR_CMD_LIST)\n\n    cluster_spec.assert_called_with({\'worker\': [\'host2:2222\'],\n                                     \'master\': [\'host1:2222\'],\n                                     \'ps\': [\'host1:2223\', \'host2:2223\']})\n\n    tf_server.assert_called_with(\n        cluster_spec(), job_name=\'ps\', task_index=0, config=tf.ConfigProto(device_count={\'GPU\': 0})\n    )\n    tf_server().join.assert_called_with()\n\n    tf_config = \'{""cluster"": {\' \\\n                \'""master"": [""host1:2222""], \' \\\n                \'""ps"": [""host1:2223"", ""host2:2223""], \' \\\n                \'""worker"": [""host2:2222""]}, \' \\\n                \'""environment"": ""cloud"", \' \\\n                \'""task"": {""index"": 0, ""type"": ""master""}}\'\n\n    run.assert_called_with(\'s3://my/bucket\', \'script_name\', MODEL_DIR_CMD_LIST,\n                           {\'TF_CONFIG\': tf_config})\n\n\n@pytest.mark.skip_on_pipeline\n@pytest.mark.skipif(sys.version_info.major != 3,\n                    reason=""Skip this for python 2 because of dict key order mismatch"")\n@patch(\'tensorflow.train.ClusterSpec\')\n@patch(\'tensorflow.train.Server\')\n@patch(\'sagemaker_training.entry_point.run\')\n@patch(\'multiprocessing.Process\', lambda target: target())\n@patch(\'time.sleep\', MagicMock())\ndef test_train_distributed_worker(run, tf_server, cluster_spec, distributed_training_env):\n    distributed_training_env.current_host = HOST2\n\n    training.train(distributed_training_env, MODEL_DIR_CMD_LIST)\n\n    cluster_spec.assert_called_with({\'worker\': [\'host2:2222\'],\n                                     \'master\': [\'host1:2222\'],\n                                     \'ps\': [\'host1:2223\', \'host2:2223\']})\n\n    tf_server.assert_called_with(\n        cluster_spec(), job_name=\'ps\', task_index=1, config=tf.ConfigProto(device_count={\'GPU\': 0})\n    )\n    tf_server().join.assert_called_with()\n\n    tf_config = \'{""cluster"": {\' \\\n                \'""master"": [""host1:2222""], \' \\\n                \'""ps"": [""host1:2223"", ""host2:2223""], \' \\\n                \'""worker"": [""host2:2222""]}, \' \\\n                \'""environment"": ""cloud"", \' \\\n                \'""task"": {""index"": 0, ""type"": ""worker""}}\'\n\n    run.assert_called_with(\'s3://my/bucket\', \'script_name\', MODEL_DIR_CMD_LIST,\n                           {\'TF_CONFIG\': tf_config})\n\n\n@patch(\'sagemaker_training.entry_point.run\')\ndef test_train_distributed_no_ps(run, distributed_training_env):\n    distributed_training_env.additional_framework_parameters[\n        training.SAGEMAKER_PARAMETER_SERVER_ENABLED] = False\n    distributed_training_env.current_host = HOST2\n    training.train(distributed_training_env, MODEL_DIR_CMD_LIST)\n\n    run.assert_called_with(MODULE_DIR, MODULE_NAME, MODEL_DIR_CMD_LIST,\n                           distributed_training_env.to_env_vars(), runner_type=runner.ProcessRunnerType)\n\n\ndef test_build_tf_config():\n    assert training._build_tf_config(HOST_LIST, HOST1) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': MASTER_TASK\n    }\n    assert training._build_tf_config(HOST_LIST, HOST1, ps_task=True) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': PS_TASK_1\n    }\n    assert training._build_tf_config(HOST_LIST, HOST2) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': WORKER_TASK\n    }\n    assert training._build_tf_config(HOST_LIST, HOST2, ps_task=True) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': PS_TASK_2}\n\n\ndef test_build_tf_config_error():\n    with pytest.raises(ValueError) as error:\n        training._build_tf_config([HOST1], HOST1, ps_task=True)\n    assert \'Cannot have a ps task if there are no parameter servers in the cluster\' in str(error.value)\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_no_model(logger):\n    path = os.path.join(RESOURCE_PATH, \'test_dir_empty\')\n    if not os.path.exists(path):\n        os.mkdir(path)\n    training._log_model_missing_warning(path)\n    logger.warn.assert_called_with(\'No model artifact is saved under path {}.\'\n                                   \' Your training job will not save any model files to S3.\\n\'\n                                   \'For details of how to construct your training script see:\\n\'\n                                   \'https://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\'  # noqa\n                                   .format(path))\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_wrong_format(logger):\n    training._log_model_missing_warning(os.path.join(RESOURCE_PATH, \'test_dir_wrong_model\'))\n    logger.warn.assert_called_with(\'Your model will NOT be servable with SageMaker TensorFlow Serving container. \'\n                                   \'The model artifact was not saved in the TensorFlow \'\n                                   \'SavedModel directory structure:\\n\'\n                                   \'https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\')\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_wrong_parent_dir(logger):\n    training._log_model_missing_warning(os.path.join(RESOURCE_PATH, \'test_dir_wrong_parent_dir\'))\n    logger.warn.assert_called_with(\'Your model will NOT be servable with SageMaker TensorFlow Serving containers. \'\n                                   \'The SavedModel bundle is under directory \\""{}\\"", not a numeric name.\'\n                                   .format(\'not-digit\'))\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_correct(logger):\n    training._log_model_missing_warning(os.path.join(RESOURCE_PATH, \'test_dir_correct_model\'))\n    logger.warn.assert_not_called()\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main(configure_s3_env, read_hyperparameters, training_env,\n              set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    read_hyperparameters.assert_called_once_with()\n    training_env.assert_called_once_with(hyperparameters={})\n    train.assert_called_once_with(single_machine_training_env, MODEL_DIR_CMD_LIST)\n    configure_s3_env.assert_called_once()\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={\'model_dir\': MODEL_DIR})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main_simple_training_model_dir(configure_s3_env, read_hyperparameters, training_env,\n                                        set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    configure_s3_env.assert_called_once_with(MODEL_DIR, REGION)\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={\'model_dir\': MODEL_DIR,\n                                                                            \'_tuning_objective_metric\': \'auc\'})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main_tuning_model_dir(configure_s3_env, read_hyperparameters, training_env,\n                               set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    expected_model_dir = \'{}/{}/model\'.format(MODEL_DIR, single_machine_training_env.job_name)\n    configure_s3_env.assert_called_once_with(expected_model_dir, REGION)\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={\'model_dir\': \'/opt/ml/model\',\n                                                                            \'_tuning_objective_metric\': \'auc\'})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main_tuning_mpi_model_dir(configure_s3_env, read_hyperparameters, training_env,\n                                   set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    configure_s3_env.assert_called_once_with(\'/opt/ml/model\', REGION)\n'"
test/integration/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport logging\nimport os\n\nlogging.getLogger(\'boto3\').setLevel(logging.INFO)\nlogging.getLogger(\'botocore\').setLevel(logging.INFO)\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'resources\')\n\n# these regions have some p2 and p3 instances, but not enough for automated testing\nNO_P2_REGIONS = [\n    \'ca-central-1\',\n    \'eu-central-1\',\n    \'eu-west-2\',\n    \'us-west-1\',\n    \'eu-west-3\',\n    \'eu-north-1\',\n    \'sa-east-1\',\n    \'ap-east-1\',\n    \'me-south-1\'\n]\nNO_P3_REGIONS = [\n    \'ap-southeast-1\',\n    \'ap-southeast-2\',\n    \'ap-south-1\',\n    \'ca-central-1\',\n    \'eu-central-1\',\n    \'eu-west-2\',\n    \'us-west-1\'\n    \'eu-west-3\',\n    \'eu-north-1\',\n    \'sa-east-1\',\n    \'ap-east-1\',\n    \'me-south-1\'\n]\n'"
test/integration/conftest.py,0,"b'#  Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport logging\nimport os\n\nimport boto3\nimport pytest\nfrom sagemaker import LocalSession, Session\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom test.integration import NO_P2_REGIONS, NO_P3_REGIONS\n\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\'boto\').setLevel(logging.INFO)\nlogging.getLogger(\'botocore\').setLevel(logging.INFO)\nlogging.getLogger(\'factory.py\').setLevel(logging.INFO)\nlogging.getLogger(\'auth.py\').setLevel(logging.INFO)\nlogging.getLogger(\'connectionpool.py\').setLevel(logging.INFO)\n\nSCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\'--docker-base-name\', default=\'sagemaker-tensorflow-scriptmode\')\n    parser.addoption(\'--tag\', default=None)\n    parser.addoption(\'--region\', default=\'us-west-2\')\n    parser.addoption(\'--framework-version\', default=TensorFlow.LATEST_VERSION)\n    parser.addoption(\'--processor\', default=\'cpu\', choices=[\'cpu\', \'gpu\', \'cpu,gpu\'])\n    parser.addoption(\'--py-version\', default=\'3\', choices=[\'2\', \'3\', \'2,3\', \'37\'])\n    parser.addoption(\'--account-id\', default=\'142577830533\')\n    parser.addoption(\'--instance-type\', default=None)\n\n\ndef pytest_configure(config):\n    os.environ[\'TEST_PY_VERSIONS\'] = config.getoption(\'--py-version\')\n    os.environ[\'TEST_PROCESSORS\'] = config.getoption(\'--processor\')\n\n\n@pytest.fixture(scope=\'session\')\ndef docker_base_name(request):\n    return request.config.getoption(\'--docker-base-name\')\n\n\n@pytest.fixture(scope=\'session\')\ndef region(request):\n    return request.config.getoption(\'--region\')\n\n\n@pytest.fixture(scope=\'session\')\ndef framework_version(request):\n    return request.config.getoption(\'--framework-version\')\n\n\n@pytest.fixture\ndef tag(request, framework_version, processor, py_version):\n    provided_tag = request.config.getoption(\'--tag\')\n    default_tag = \'{}-{}-py{}\'.format(framework_version, processor, py_version)\n    return provided_tag if provided_tag is not None else default_tag\n\n\n@pytest.fixture(scope=\'session\')\ndef sagemaker_session(region):\n    return Session(boto_session=boto3.Session(region_name=region))\n\n\n@pytest.fixture(scope=\'session\')\ndef sagemaker_local_session(region):\n    return LocalSession(boto_session=boto3.Session(region_name=region))\n\n\n@pytest.fixture(scope=\'session\')\ndef account_id(request):\n    return request.config.getoption(\'--account-id\')\n\n\n@pytest.fixture\ndef instance_type(request, processor):\n    provided_instance_type = request.config.getoption(\'--instance-type\')\n    default_instance_type = \'ml.c4.xlarge\' if processor == \'cpu\' else \'ml.p2.xlarge\'\n    return provided_instance_type if provided_instance_type is not None else default_instance_type\n\n\n@pytest.fixture()\ndef py_version():\n    if \'TEST_PY_VERSIONS\' in os.environ:\n        return os.environ[\'TEST_PY_VERSIONS\'].split(\',\')\n    return None\n\n\n@pytest.fixture()\ndef processor():\n    if \'TEST_PROCESSORS\' in os.environ:\n        return os.environ[\'TEST_PROCESSORS\'].split(\',\')\n    return None\n\n\n@pytest.fixture(autouse=True)\ndef skip_by_device_type(request, processor):\n    is_gpu = (processor == \'gpu\')\n    if (request.node.get_closest_marker(\'skip_gpu\') and is_gpu) or \\\n            (request.node.get_closest_marker(\'skip_cpu\') and not is_gpu):\n        pytest.skip(\'Skipping because running on \\\'{}\\\' instance\'.format(processor))\n\n\n@pytest.fixture(autouse=True)\ndef skip_gpu_instance_restricted_regions(region, instance_type):\n    if (region in NO_P2_REGIONS and instance_type.startswith(\'ml.p2\')) or \\\n            (region in NO_P3_REGIONS and instance_type.startswith(\'ml.p3\')):\n        pytest.skip(\'Skipping GPU test in region {}\'.format(region))\n\n\n@pytest.fixture\ndef docker_image(docker_base_name, tag):\n    return \'{}:{}\'.format(docker_base_name, tag)\n\n\n@pytest.fixture(autouse=True)\ndef skip_py2_containers(request, tag):\n    if request.node.get_closest_marker(\'skip_py2_containers\'):\n        if \'py2\' in tag:\n            pytest.skip(\'Skipping python2 container with tag {}\'.format(tag))\n\n\n@pytest.fixture\ndef ecr_image(account_id, docker_base_name, tag, region):\n    return \'{}.dkr.ecr.{}.amazonaws.com/{}:{}\'.format(\n        account_id, region, docker_base_name, tag)\n'"
test/integration/utils.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport random\nimport time\n\nimport pytest\n\n\ndef unique_name_from_base(base, max_length=63):\n    unique = \'%04x\' % random.randrange(16**4)  # 4-digit hex\n    ts = str(int(time.time()))\n    available_length = max_length - 2 - len(ts) - len(unique)\n    trimmed = base[:available_length]\n    return \'{}-{}-{}\'.format(trimmed, ts, unique)\n\n\n@pytest.fixture(params=os.environ[\'TEST_PY_VERSIONS\'].split(\',\'))\ndef py_version(request):\n    return request.param\n\n\n@pytest.fixture(params=os.environ[\'TEST_PROCESSORS\'].split(\',\'))\ndef processor(request):\n    return request.param\n'"
test/resources/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test/resources/gpu_device_placement.py,5,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nimport tensorflow as tf\n\n# https://www.tensorflow.org/programmers_guide/using_gpu\nprint(\'-\' * 87)\nprint(\'Run GPU test.\')\nwith tf.device(\'/gpu:0\'):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name=\'a\')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name=\'b\')\nc = tf.matmul(a, b)\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n# Runs the op.\nprint(sess.run(c))\nprint(\'-\' * 87)\nprint(\'\')\n'"
test/resources/keras_inception.py,3,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nimport argparse\nimport os\n\nfrom tensorflow import keras\nimport tensorflow as tf\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', type=str)\n\nargs = parser.parse_args()\n\n\n# Loading pre-trained Keras model\nmodel = keras.applications.inception_v3.InceptionV3(weights=\'imagenet\')\n\n# Exports the keras model as TensorFlow Serving Saved Model\nwith tf.Session() as session:\n\n    init = tf.global_variables_initializer()\n    session.run(init)\n\n    tf.saved_model.simple_save(\n        session,\n        os.path.join(args.model_dir, \'inception-model/1\'),\n        inputs={\'input_image\': model.input},\n        outputs={t.name: t for t in model.outputs})\n'"
test/unit/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test/unit/test_deep_learning_container.py,0,"b""# Copyright 2018-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport unittest\n\nfrom docker.build_artifacts import deep_learning_container as deep_learning_container_to_test\nimport pytest\nimport requests\n\n\n@pytest.fixture(name='fixture_valid_instance_id')\ndef fixture_valid_instance_id(requests_mock):\n    return requests_mock.get('http://169.254.169.254/latest/meta-data/instance-id',\n                             text='i-123t32e11s32t1231')\n\n\n@pytest.fixture(name='fixture_invalid_instance_id')\ndef fixture_invalid_instance_id(requests_mock):\n    return requests_mock.get('http://169.254.169.254/latest/meta-data/instance-id', text='i-123')\n\n\n@pytest.fixture(name='fixture_none_instance_id')\ndef fixture_none_instance_id(requests_mock):\n    return requests_mock.get('http://169.254.169.254/latest/meta-data/instance-id', text=None)\n\n\n@pytest.fixture(name='fixture_invalid_region')\ndef fixture_invalid_region(requests_mock):\n    return requests_mock.get('http://169.254.169.254/latest/dynamic/instance-identity/document',\n                             json={'region': 'test'})\n\n\n@pytest.fixture(name='fixture_valid_region')\ndef fixture_valid_region(requests_mock):\n    return requests_mock.get('http://169.254.169.254/latest/dynamic/instance-identity/document',\n                             json={'region': 'us-east-1'})\n\n\ndef test_retrieve_instance_id(fixture_valid_instance_id):\n    result = deep_learning_container_to_test._retrieve_instance_id()\n    assert 'i-123t32e11s32t1231' == result\n\n\ndef test_retrieve_none_instance_id(fixture_none_instance_id):\n    result = deep_learning_container_to_test._retrieve_instance_id()\n    assert result is None\n\n\ndef test_retrieve_invalid_instance_id(fixture_invalid_instance_id):\n    result = deep_learning_container_to_test._retrieve_instance_id()\n    assert result is None\n\n\ndef test_retrieve_invalid_region(fixture_invalid_region):\n    result = deep_learning_container_to_test._retrieve_instance_region()\n    assert result is None\n\n\ndef test_retrieve_valid_region(fixture_valid_region):\n    result = deep_learning_container_to_test._retrieve_instance_region()\n    assert 'us-east-1' == result\n\n\ndef test_query_bucket(requests_mock, fixture_valid_region, fixture_valid_instance_id):\n    fixture_valid_instance_id.return_value = 'i-123t32e11s32t1231'\n    fixture_valid_region.return_value = 'us-east-1'\n    requests_mock.get(('https://aws-deep-learning-containers-us-east-1.s3.us-east-1.amazonaws.com'\n                       '/dlc-containers.txt?x-instance-id=i-123t32e11s32t1231'),\n                      text='Access Denied')\n    actual_response = deep_learning_container_to_test.query_bucket()\n    assert 'Access Denied' == actual_response.text\n\n\ndef test_query_bucket_region_none(fixture_invalid_region, fixture_valid_instance_id):\n    fixture_valid_instance_id.return_value = 'i-123t32e11s32t1231'\n    fixture_invalid_region.return_value = None\n    actual_response = deep_learning_container_to_test.query_bucket()\n    assert actual_response is None\n\n\ndef test_query_bucket_instance_id_none(requests_mock, fixture_valid_region, fixture_none_instance_id):\n    fixture_none_instance_id.return_value = None\n    fixture_valid_region.return_value = 'us-east-1'\n    actual_response = deep_learning_container_to_test.query_bucket()\n    assert actual_response is None\n\n\ndef test_query_bucket_instance_id_invalid(requests_mock, fixture_valid_region, fixture_invalid_instance_id):\n    fixture_invalid_instance_id.return_value = None\n    fixture_valid_region.return_value = 'us-east-1'\n    actual_response = deep_learning_container_to_test.query_bucket()\n    assert actual_response is None\n\n\ndef test_HTTP_error_on_S3(requests_mock, fixture_valid_region, fixture_valid_instance_id):\n    fixture_valid_instance_id.return_value = 'i-123t32e11s32t1231'\n    fixture_valid_region.return_value = 'us-east-1'\n    query_s3_url = ('https://aws-deep-learning-containers-us-east-1.s3.us-east-1.amazonaws.com'\n                    '/dlc-containers.txt?x-instance-id=i-123t32e11s32t1231')\n\n    requests_mock.get(\n        query_s3_url,\n        exc=requests.exceptions.HTTPError)\n    requests_mock.side_effect = requests.exceptions.HTTPError\n\n    with pytest.raises(requests.exceptions.HTTPError):\n        actual_response = requests.get(query_s3_url)\n        assert actual_response is None\n\n\ndef test_connection_error_on_S3(requests_mock, fixture_valid_region, fixture_valid_instance_id):\n    fixture_valid_instance_id.return_value = 'i-123t32e11s32t1231'\n    fixture_valid_region.return_value = 'us-east-1'\n    query_s3_url = ('https://aws-deep-learning-containers-us-east-1.s3.us-east-1.amazonaws.com'\n                    '/dlc-containers.txt?x-instance-id=i-123t32e11s32t1231')\n\n    requests_mock.get(\n        query_s3_url,\n        exc=requests.exceptions.ConnectionError)\n\n    with pytest.raises(requests.exceptions.ConnectionError):\n        actual_response = requests.get(\n            query_s3_url)\n\n        assert actual_response is None\n\n\ndef test_timeout_error_on_S3(requests_mock, fixture_valid_region, fixture_valid_instance_id):\n    fixture_valid_instance_id.return_value = 'i-123t32e11s32t1231'\n    fixture_valid_region.return_value = 'us-east-1'\n    query_s3_url = ('https://aws-deep-learning-containers-us-east-1.s3.us-east-1.amazonaws.com'\n                    '/dlc-containers.txt?x-instance-id=i-123t32e11s32t1231')\n\n    requests_mock.get(\n        query_s3_url,\n        exc=requests.Timeout)\n\n    with pytest.raises(requests.exceptions.Timeout):\n        actual_response = requests.get(\n            query_s3_url)\n\n        assert actual_response is None\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/unit/test_s3_utils.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \'license\' file accompanying this file. This file is\n# distributed on an \'AS IS\' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nfrom mock import MagicMock, patch\n\nfrom sagemaker_tensorflow_container import s3_utils\n\n\nBUCKET_REGION = \'us-west-2\'\nJOB_REGION = \'us-west-1\'\nJOB_BUKCET = \'sagemaker-us-west-2-000-00-1\'\nPREFIX = \'sagemaker/something\'\nMODEL_DIR = \'s3://{}/{}\'.format(JOB_BUKCET, PREFIX)\n\n\n@patch(\'boto3.client\')\ndef test_configure(client):\n    s3 = MagicMock()\n    client.return_value = s3\n    loc = {\'LocationConstraint\': BUCKET_REGION}\n    s3.get_bucket_location.return_value = loc\n\n    s3_utils.configure(MODEL_DIR, JOB_REGION)\n\n    assert os.environ[\'S3_REGION\'] == BUCKET_REGION\n    assert os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] == \'1\'\n    assert os.environ[\'S3_USE_HTTPS\'] == \'1\'\n\n\ndef test_configure_local_dir():\n    s3_utils.configure(\'/opt/ml/model\', JOB_REGION)\n\n    assert os.environ[\'S3_REGION\'] == JOB_REGION\n    assert os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] == \'1\'\n    assert os.environ[\'S3_USE_HTTPS\'] == \'1\'\n'"
test/unit/test_training.py,3,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \'license\' file accompanying this file. This file is\n# distributed on an \'AS IS\' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport sys\n\nfrom mock import MagicMock, patch\nimport pytest\nfrom sagemaker_training import runner\nimport tensorflow as tf\n\nfrom sagemaker_tensorflow_container import training\n\nMODULE_DIR = \'s3://my/bucket\'\nMODULE_NAME = \'script_name\'\nLOG_LEVEL = \'Debug\'\nHOST1 = \'host1\'\nHOST2 = \'host2\'\nHOST_LIST = [HOST1, HOST2]\nCURRENT_HOST = HOST1\nCMD_ARGS = {\'some_key\': \'some_value\'}\nCLUSTER_WITH_PS = {\n    \'master\': [\'{}:2222\'.format(HOST1)],\n    \'worker\': [\'{}:2222\'.format(HOST2)],\n    \'ps\': [\'{}:2223\'.format(HOST1), \'{}:2223\'.format(HOST2)]\n}\nMASTER_TASK = {\'index\': 0, \'type\': \'master\'}\nWORKER_TASK = {\'index\': 0, \'type\': \'worker\'}\nPS_TASK_1 = {\'index\': 0, \'type\': \'ps\'}\nPS_TASK_2 = {\'index\': 1, \'type\': \'ps\'}\nMODEL_DIR = \'s3://bucket/prefix\'\nMODEL_DIR_CMD_LIST = [\'--model_dir\', MODEL_DIR]\nREGION = \'us-west-2\'\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'resources\')\n\n\n@pytest.fixture\ndef distributed_training_env():\n    env = simple_training_env()\n\n    env.hosts = HOST_LIST\n    env.additional_framework_parameters = {\n        training.SAGEMAKER_PARAMETER_SERVER_ENABLED: True\n    }\n    return env\n\n\n@pytest.fixture\ndef single_machine_training_env():\n    return simple_training_env()\n\n\ndef simple_training_env():\n    env = MagicMock()\n    env.module_dir = MODULE_DIR\n    env.user_entry_point = MODULE_NAME\n    env.hyperparameters = {\'model_dir\': MODEL_DIR}\n    env.log_level = LOG_LEVEL\n    env.additional_framework_parameters = {}\n    env.hosts = CURRENT_HOST\n    env.current_host = CURRENT_HOST\n    env.to_env_vars = lambda: {}\n    env.job_name = \'test-training-job\'\n    return env\n\n\ndef test_is_host_master():\n    assert training._is_host_master(HOST_LIST, CURRENT_HOST) is True\n    assert training._is_host_master(HOST_LIST, \'host2\') is False\n    assert training._is_host_master(HOST_LIST, \'somehost\') is False\n\n\n@patch(\'sagemaker_training.entry_point.run\')\ndef test_single_machine(run_module, single_machine_training_env):\n    training.train(single_machine_training_env, MODEL_DIR_CMD_LIST)\n    run_module.assert_called_with(MODULE_DIR, MODULE_NAME, MODEL_DIR_CMD_LIST,\n                                  single_machine_training_env.to_env_vars(),\n                                  runner_type=runner.ProcessRunnerType)\n\n\n@patch(\'sagemaker_training.entry_point.run\')\ndef test_train_horovod(run_module, single_machine_training_env):\n    single_machine_training_env.additional_framework_parameters[\'sagemaker_mpi_enabled\'] = True\n\n    training.train(single_machine_training_env, MODEL_DIR_CMD_LIST)\n    run_module.assert_called_with(MODULE_DIR, MODULE_NAME, MODEL_DIR_CMD_LIST,\n                                  single_machine_training_env.to_env_vars(),\n                                  runner_type=runner.MPIRunnerType)\n\n\n@pytest.mark.skip_on_pipeline\n@pytest.mark.skipif(sys.version_info.major != 3,\n                    reason=""Skip this for python 2 because of dict key order mismatch"")\n@patch(\'tensorflow.train.ClusterSpec\')\n@patch(\'tensorflow.train.Server\')\n@patch(\'sagemaker_training.entry_point.run\')\n@patch(\'multiprocessing.Process\', lambda target: target())\n@patch(\'time.sleep\', MagicMock())\ndef test_train_distributed_master(run, tf_server, cluster_spec, distributed_training_env):\n    training.train(distributed_training_env, MODEL_DIR_CMD_LIST)\n\n    cluster_spec.assert_called_with({\'worker\': [\'host2:2222\'],\n                                     \'master\': [\'host1:2222\'],\n                                     \'ps\': [\'host1:2223\', \'host2:2223\']})\n\n    tf_server.assert_called_with(\n        cluster_spec(), job_name=\'ps\', task_index=0, config=tf.ConfigProto(device_count={\'GPU\': 0})\n    )\n    tf_server().join.assert_called_with()\n\n    tf_config = \'{""cluster"": {\' \\\n                \'""master"": [""host1:2222""], \' \\\n                \'""ps"": [""host1:2223"", ""host2:2223""], \' \\\n                \'""worker"": [""host2:2222""]}, \' \\\n                \'""environment"": ""cloud"", \' \\\n                \'""task"": {""index"": 0, ""type"": ""master""}}\'\n\n    run.assert_called_with(\'s3://my/bucket\', \'script_name\', MODEL_DIR_CMD_LIST,\n                           {\'TF_CONFIG\': tf_config})\n\n\n@pytest.mark.skip_on_pipeline\n@pytest.mark.skipif(sys.version_info.major != 3,\n                    reason=""Skip this for python 2 because of dict key order mismatch"")\n@patch(\'tensorflow.train.ClusterSpec\')\n@patch(\'tensorflow.train.Server\')\n@patch(\'sagemaker_training.entry_point.run\')\n@patch(\'multiprocessing.Process\', lambda target: target())\n@patch(\'time.sleep\', MagicMock())\ndef test_train_distributed_worker(run, tf_server, cluster_spec, distributed_training_env):\n    distributed_training_env.current_host = HOST2\n\n    training.train(distributed_training_env, MODEL_DIR_CMD_LIST)\n\n    cluster_spec.assert_called_with({\'worker\': [\'host2:2222\'],\n                                     \'master\': [\'host1:2222\'],\n                                     \'ps\': [\'host1:2223\', \'host2:2223\']})\n\n    tf_server.assert_called_with(\n        cluster_spec(), job_name=\'ps\', task_index=1, config=tf.ConfigProto(device_count={\'GPU\': 0})\n    )\n    tf_server().join.assert_called_with()\n\n    tf_config = \'{""cluster"": {\' \\\n                \'""master"": [""host1:2222""], \' \\\n                \'""ps"": [""host1:2223"", ""host2:2223""], \' \\\n                \'""worker"": [""host2:2222""]}, \' \\\n                \'""environment"": ""cloud"", \' \\\n                \'""task"": {""index"": 0, ""type"": ""worker""}}\'\n\n    run.assert_called_with(\'s3://my/bucket\', \'script_name\', MODEL_DIR_CMD_LIST,\n                           {\'TF_CONFIG\': tf_config})\n\n\n@patch(\'sagemaker_training.entry_point.run\')\ndef test_train_distributed_no_ps(run, distributed_training_env):\n    distributed_training_env.additional_framework_parameters[\n        training.SAGEMAKER_PARAMETER_SERVER_ENABLED] = False\n    distributed_training_env.current_host = HOST2\n    training.train(distributed_training_env, MODEL_DIR_CMD_LIST)\n\n    run.assert_called_with(MODULE_DIR, MODULE_NAME, MODEL_DIR_CMD_LIST,\n                           distributed_training_env.to_env_vars(), runner_type=runner.ProcessRunnerType)\n\n\ndef test_build_tf_config():\n    assert training._build_tf_config(HOST_LIST, HOST1) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': MASTER_TASK\n    }\n    assert training._build_tf_config(HOST_LIST, HOST1, ps_task=True) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': PS_TASK_1\n    }\n    assert training._build_tf_config(HOST_LIST, HOST2) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': WORKER_TASK\n    }\n    assert training._build_tf_config(HOST_LIST, HOST2, ps_task=True) == {\n        \'cluster\': CLUSTER_WITH_PS,\n        \'environment\': \'cloud\',\n        \'task\': PS_TASK_2}\n\n\ndef test_build_tf_config_error():\n    with pytest.raises(ValueError) as error:\n        training._build_tf_config([HOST1], HOST1, ps_task=True)\n    assert \'Cannot have a ps task if there are no parameter servers in the cluster\' in str(error.value)\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_no_model(logger):\n    path = os.path.join(RESOURCE_PATH, \'test_dir_empty\')\n    if not os.path.exists(path):\n        os.mkdir(path)\n    training._log_model_missing_warning(path)\n    logger.warn.assert_called_with(\'No model artifact is saved under path {}.\'\n                                   \' Your training job will not save any model files to S3.\\n\'\n                                   \'For details of how to construct your training script see:\\n\'\n                                   \'https://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\'  # noqa\n                                   .format(path))\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_wrong_format(logger):\n    training._log_model_missing_warning(os.path.join(RESOURCE_PATH, \'test_dir_wrong_model\'))\n    logger.warn.assert_called_with(\'Your model will NOT be servable with SageMaker TensorFlow Serving container. \'\n                                   \'The model artifact was not saved in the TensorFlow \'\n                                   \'SavedModel directory structure:\\n\'\n                                   \'https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\')\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_wrong_parent_dir(logger):\n    training._log_model_missing_warning(os.path.join(RESOURCE_PATH, \'test_dir_wrong_parent_dir\'))\n    logger.warn.assert_called_with(\'Your model will NOT be servable with SageMaker TensorFlow Serving containers. \'\n                                   \'The SavedModel bundle is under directory \\""{}\\"", not a numeric name.\'\n                                   .format(\'not-digit\'))\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\ndef test_log_model_missing_warning_correct(logger):\n    training._log_model_missing_warning(os.path.join(RESOURCE_PATH, \'test_dir_correct_model\'))\n    logger.warn.assert_not_called()\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main(configure_s3_env, read_hyperparameters, training_env,\n              set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    read_hyperparameters.assert_called_once_with()\n    training_env.assert_called_once_with(hyperparameters={})\n    train.assert_called_once_with(single_machine_training_env, MODEL_DIR_CMD_LIST)\n    configure_s3_env.assert_called_once()\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={\'model_dir\': MODEL_DIR})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main_simple_training_model_dir(configure_s3_env, read_hyperparameters, training_env,\n                                        set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    configure_s3_env.assert_called_once_with(MODEL_DIR, REGION)\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={\'model_dir\': MODEL_DIR,\n                                                                            \'_tuning_objective_metric\': \'auc\'})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main_tuning_model_dir(configure_s3_env, read_hyperparameters, training_env,\n                               set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    expected_model_dir = \'{}/{}/model\'.format(MODEL_DIR, single_machine_training_env.job_name)\n    configure_s3_env.assert_called_once_with(expected_model_dir, REGION)\n\n\n@patch(\'sagemaker_tensorflow_container.training.logger\')\n@patch(\'sagemaker_tensorflow_container.training.train\')\n@patch(\'logging.Logger.setLevel\')\n@patch(\'sagemaker_training.environment.Environment\')\n@patch(\'sagemaker_training.environment.read_hyperparameters\', return_value={\'model_dir\': \'/opt/ml/model\',\n                                                                            \'_tuning_objective_metric\': \'auc\'})\n@patch(\'sagemaker_tensorflow_container.s3_utils.configure\')\ndef test_main_tuning_mpi_model_dir(configure_s3_env, read_hyperparameters, training_env,\n                                   set_level, train, logger, single_machine_training_env):\n    training_env.return_value = single_machine_training_env\n    os.environ[\'SAGEMAKER_REGION\'] = REGION\n    training.main()\n    configure_s3_env.assert_called_once_with(\'/opt/ml/model\', REGION)\n'"
benchmarks/tf_benchmarks/tf_cnn_benchmarks/tf_cnn_benchmarks.py,2,"b'# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n""""""Benchmark script for TensorFlow.\n\nOriginally copied from:\nhttps://github.com/tensorflow/benchmarks/blob/e3bd1370ba21b02c4d34340934ffb4941977d96f/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nfrom absl import app\nfrom absl import flags as absl_flags\nimport tensorflow.compat.v1 as tf\n\nimport benchmark_cnn\nimport cnn_util\nimport flags\nimport mlperf\nfrom cnn_util import log_fn\n\n\nflags.define_flags()\nfor name in flags.param_specs.keys():\n    absl_flags.declare_key_flag(name)\n\nabsl_flags.DEFINE_boolean(\n    \'ml_perf_compliance_logging\', False,\n    \'Print logs required to be compliant with MLPerf. If set, must clone the \'\n    \'MLPerf training repo https://github.com/mlperf/training and add \'\n    \'https://github.com/mlperf/training/tree/master/compliance to the \'\n    \'PYTHONPATH\')\n\n\ndef main(positional_arguments):\n    # Command-line arguments like \'--distortions False\' are equivalent to\n    # \'--distortions=True False\', where False is a positional argument. To prevent\n    # this from silently running with distortions, we do not allow positional\n    # arguments.\n    assert len(positional_arguments) >= 1\n    if len(positional_arguments) > 1:\n        raise ValueError(\'Received unknown positional arguments: %s\'\n                         % positional_arguments[1:])\n\n    params = benchmark_cnn.make_params_from_flags()\n    with mlperf.mlperf_logger(absl_flags.FLAGS.ml_perf_compliance_logging,\n                              params.model):\n        params = benchmark_cnn.setup(params)\n        bench = benchmark_cnn.BenchmarkCNN(params)\n\n    tfversion = cnn_util.tensorflow_version_tuple()\n    log_fn(\'TensorFlow:  %i.%i\' % (tfversion[0], tfversion[1]))\n\n    bench.print_info()\n    bench.run()\n\n\nif __name__ == \'__main__\':\n    tf.disable_v2_behavior()\n    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\n'"
docker/1.15.0/py3/dockerd-entrypoint.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os.path\nimport shlex\nimport subprocess\nimport sys\n\nif not os.path.exists(""/opt/ml/input/config""):\n    subprocess.call([\'python\', \'/usr/local/bin/deep_learning_container.py\', \'&>/dev/null\', \'&\'])\n\nsubprocess.check_call(shlex.split(\' \'.join(sys.argv[1:])))\n'"
test-toolkit/integration/local/test_horovod.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport json\nimport os\nimport tarfile\n\nimport pytest\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom integration.utils import processor, py_version  # noqa: F401\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n\n\n@pytest.mark.skip_gpu\n@pytest.mark.skip_generic\n@pytest.mark.parametrize(\'instances, processes\', [\n    [1, 2],\n    (2, 1),\n    (2, 2),\n    (5, 2)])\ndef test_distributed_training_horovod_basic(instances,\n                                            processes,\n                                            sagemaker_local_session,\n                                            image_uri,\n                                            tmpdir,\n                                            framework_version):\n    output_path = \'file://%s\' % tmpdir\n    estimator = TensorFlow(\n        entry_point=os.path.join(RESOURCE_PATH, \'hvdbasic\', \'train_hvd_basic.py\'),\n        role=\'SageMakerRole\',\n        train_instance_type=\'local\',\n        sagemaker_session=sagemaker_local_session,\n        train_instance_count=instances,\n        image_name=image_uri,\n        output_path=output_path,\n        framework_version=framework_version,\n        hyperparameters={\'sagemaker_mpi_enabled\': True,\n                         \'sagemaker_network_interface_name\': \'eth0\',\n                         \'sagemaker_mpi_num_of_processes_per_host\': processes})\n\n    estimator.fit(\'file://{}\'.format(os.path.join(RESOURCE_PATH, \'mnist\', \'data-distributed\')))\n\n    tmp = str(tmpdir)\n    extract_files(output_path.replace(\'file://\', \'\'), tmp)\n\n    size = instances * processes\n\n    for rank in range(size):\n        local_rank = rank % processes\n        assert read_json(\'local-rank-%s-rank-%s\' % (local_rank, rank), tmp) == {\n            \'local-rank\': local_rank, \'rank\': rank, \'size\': size}\n\n\ndef read_json(file, tmp):\n    with open(os.path.join(tmp, file)) as f:\n        return json.load(f)\n\n\ndef assert_files_exist_in_tar(output_path, files):\n    if output_path.startswith(\'file://\'):\n        output_path = output_path[7:]\n    model_file = os.path.join(output_path, \'model.tar.gz\')\n    with tarfile.open(model_file) as tar:\n        for f in files:\n            tar.getmember(f)\n\n\ndef extract_files(output_path, tmpdir):\n    with tarfile.open(os.path.join(output_path, \'model.tar.gz\')) as tar:\n        tar.extractall(tmpdir)\n'"
test-toolkit/integration/local/test_training.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport tarfile\n\nimport pytest\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom integration.utils import processor, py_version  # noqa: F401\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\nTF_CHECKPOINT_FILES = [\'graph.pbtxt\', \'model.ckpt-0.index\', \'model.ckpt-0.meta\']\n\n\n@pytest.fixture  # noqa: F811\ndef py_full_version(py_version):  # noqa: F811\n    if py_version == \'2\':\n        return \'2.7\'\n    else:\n        return \'3.6\'\n\n\n@pytest.mark.skip_gpu\ndef test_mnist_cpu(sagemaker_local_session, image_uri, tmpdir, framework_version):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'mnist\', \'mnist.py\'),\n                    instance_type=\'local\',\n                    instance_count=1,\n                    sagemaker_local_session=sagemaker_local_session,\n                    image_uri=image_uri,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data\')))\n    _assert_files_exist_in_tar(output_path, [\'my_model.h5\'])\n\n\n@pytest.mark.skip_gpu\ndef test_distributed_training_cpu_no_ps(sagemaker_local_session,\n                                        image_uri,\n                                        tmpdir,\n                                        framework_version):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'mnist\', \'mnist_estimator.py\'),\n                    instance_type=\'local\',\n                    instance_count=2,\n                    sagemaker_local_session=sagemaker_local_session,\n                    image_uri=image_uri,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data-distributed\')))\n    _assert_files_exist_in_tar(output_path, TF_CHECKPOINT_FILES)\n\n\n@pytest.mark.skip_gpu\ndef test_distributed_training_cpu_ps(sagemaker_local_session,\n                                     image_uri,\n                                     tmpdir,\n                                     framework_version):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'mnist\', \'mnist_estimator.py\'),\n                    instance_type=\'local\',\n                    instance_count=2,\n                    sagemaker_local_session=sagemaker_local_session,\n                    image_uri=image_uri,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    hyperparameters={\'sagemaker_parameter_server_enabled\': True},\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data-distributed\')))\n    _assert_files_exist_in_tar(output_path, TF_CHECKPOINT_FILES)\n\n\ndef run_tf_training(script,\n                    instance_type,\n                    instance_count,\n                    sagemaker_local_session,\n                    image_uri,\n                    framework_version,\n                    training_data_path,\n                    output_path=None,\n                    hyperparameters=None):\n\n    hyperparameters = hyperparameters or {}\n\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_count=instance_count,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_local_session,\n                           image_name=image_uri,\n                           model_dir=\'/opt/ml/model\',\n                           output_path=output_path,\n                           hyperparameters=hyperparameters,\n                           base_job_name=\'test-tf\',\n                           framework_version=framework_version,\n                           py_version=\'py3\')\n\n    estimator.fit(training_data_path)\n\n\ndef _assert_files_exist_in_tar(output_path, files):\n    if output_path.startswith(\'file://\'):\n        output_path = output_path[7:]\n    model_file = os.path.join(output_path, \'model.tar.gz\')\n    with tarfile.open(model_file) as tar:\n        for f in files:\n            tar.getmember(f)\n'"
test-toolkit/integration/sagemaker/test_horovod.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nimport pytest\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom integration.utils import processor, py_version, unique_name_from_base  # noqa: F401\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n\n\n@pytest.mark.skip_generic\ndef test_distributed_training_horovod(sagemaker_session,\n                                      instance_type,\n                                      image_uri,\n                                      tmpdir,\n                                      framework_version):\n\n    mpi_options = \'-verbose -x orte_base_help_aggregate=0\'\n    estimator = TensorFlow(\n        entry_point=os.path.join(RESOURCE_PATH, \'mnist\', \'horovod_mnist.py\'),\n        role=\'SageMakerRole\',\n        train_instance_type=instance_type,\n        train_instance_count=2,\n        image_name=image_uri,\n        framework_version=framework_version,\n        py_version=\'py3\',\n        script_mode=True,\n        hyperparameters={\'sagemaker_mpi_enabled\': True,\n                         \'sagemaker_mpi_custom_mpi_options\': mpi_options,\n                         \'sagemaker_mpi_num_of_processes_per_host\': 1},\n        sagemaker_session=sagemaker_session)\n\n    estimator.fit(job_name=unique_name_from_base(\'test-tf-horovod\'))\n\n    model_data_source = sagemaker.local.data.get_data_source_instance(\n        estimator.model_data, sagemaker_session)\n\n    for filename in model_data_source.get_file_list():\n        assert os.path.basename(filename) == \'model.tar.gz\'\n'"
test-toolkit/integration/sagemaker/test_mnist.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nimport boto3\nimport pytest\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter\nfrom six.moves.urllib.parse import urlparse\n\nfrom integration.utils import processor, py_version, unique_name_from_base  # noqa: F401\nfrom timeout import timeout\n\n\n@pytest.mark.deploy_test\ndef test_mnist(sagemaker_session, image_uri, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           sagemaker_session=sagemaker_session,\n                           image_name=image_uri,\n                           framework_version=framework_version,\n                           script_mode=True)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data\'),\n        key_prefix=\'scriptmode/mnist\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-sagemaker-mnist\'))\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef test_distributed_mnist_no_ps(sagemaker_session, image_uri, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_count=2,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_session,\n                           image_name=image_uri,\n                           framework_version=framework_version,\n                           script_mode=True)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data\'),\n        key_prefix=\'scriptmode/mnist\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-tf-sm-distributed-mnist\'))\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef test_distributed_mnist_ps(sagemaker_session, image_uri, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist_estimator.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           hyperparameters={\'sagemaker_parameter_server_enabled\': True},\n                           train_instance_count=2,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_session,\n                           image_name=image_uri,\n                           framework_version=framework_version,\n                           script_mode=True)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data-distributed\'),\n        key_prefix=\'scriptmode/mnist-distributed\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-tf-sm-distributed-mnist\'))\n    _assert_checkpoint_exists(sagemaker_session.boto_region_name, estimator.model_dir, 0)\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef test_tuning(sagemaker_session, image_uri, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist.py\')\n\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           sagemaker_session=sagemaker_session,\n                           image_name=image_uri,\n                           framework_version=framework_version,\n                           script_mode=True)\n\n    hyperparameter_ranges = {\'epochs\': IntegerParameter(1, 2)}\n    objective_metric_name = \'accuracy\'\n    metric_definitions = [{\'Name\': objective_metric_name, \'Regex\': \'accuracy = ([0-9\\\\.]+)\'}]\n\n    tuner = HyperparameterTuner(estimator,\n                                objective_metric_name,\n                                hyperparameter_ranges,\n                                metric_definitions,\n                                max_jobs=2,\n                                max_parallel_jobs=2)\n\n    with timeout(minutes=20):\n        inputs = estimator.sagemaker_session.upload_data(\n            path=os.path.join(resource_path, \'mnist\', \'data\'),\n            key_prefix=\'scriptmode/mnist\')\n\n        tuning_job_name = unique_name_from_base(\'test-tf-sm-tuning\', max_length=32)\n        tuner.fit(inputs, job_name=tuning_job_name)\n        tuner.wait()\n\n\ndef _assert_checkpoint_exists(region, model_dir, checkpoint_number):\n    _assert_s3_file_exists(region, os.path.join(model_dir, \'graph.pbtxt\'))\n    _assert_s3_file_exists(region,\n                           os.path.join(model_dir, \'model.ckpt-{}.index\'.format(checkpoint_number)))\n    _assert_s3_file_exists(region,\n                           os.path.join(model_dir, \'model.ckpt-{}.meta\'.format(checkpoint_number)))\n\n\ndef _assert_s3_file_exists(region, s3_url):\n    parsed_url = urlparse(s3_url)\n    s3 = boto3.resource(\'s3\', region_name=region)\n    s3.Object(parsed_url.netloc, parsed_url.path.lstrip(\'/\')).load()\n'"
test-toolkit/integration/sagemaker/test_tuning_model_dir.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter\n\nfrom integration.utils import processor, py_version, unique_name_from_base  # noqa: F401\n\n\ndef test_model_dir_with_training_job_name(sagemaker_session, image_uri, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'../..\', \'resources\')\n    script = os.path.join(resource_path, \'tuning_model_dir\', \'entry.py\')\n\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           image_name=image_uri,\n                           framework_version=framework_version,\n                           py_version=\'py3\',\n                           sagemaker_session=sagemaker_session)\n\n    tuner = HyperparameterTuner(estimator=estimator,\n                                objective_metric_name=\'accuracy\',\n                                hyperparameter_ranges={\'arbitrary_value\': IntegerParameter(0, 1)},\n                                metric_definitions=[{\'Name\': \'accuracy\', \'Regex\': \'accuracy=([01])\'}],\n                                max_jobs=1,\n                                max_parallel_jobs=1)\n\n    # User script has logic to check for the correct model_dir\n    tuner.fit(job_name=unique_name_from_base(\'test-tf-model-dir\', max_length=32))\n    tuner.wait()\n'"
test-toolkit/integration/sagemaker/timeout.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# or in the ""license"" file accompanying this file. This file is distributed\n# on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nfrom contextlib import contextmanager\nimport logging\nimport signal\n\nLOGGER = logging.getLogger(\'timeout\')\n\n\nclass TimeoutError(Exception):\n    pass\n\n\n@contextmanager\ndef timeout(seconds=0, minutes=0, hours=0):\n    """"""Add a signal-based timeout to any block of code.\n    If multiple time units are specified, they will be added together to determine time limit.\n    Usage:\n    with timeout(seconds=5):\n        my_slow_function(...)\n    Args:\n        - seconds: The time limit, in seconds.\n        - minutes: The time limit, in minutes.\n        - hours: The time limit, in hours.\n    """"""\n\n    limit = seconds + 60 * minutes + 3600 * hours\n\n    def handler(signum, frame):\n        raise TimeoutError(\'timed out after {} seconds\'.format(limit))\n\n    try:\n        signal.signal(signal.SIGALRM, handler)\n        signal.alarm(limit)\n\n        yield\n    finally:\n        signal.alarm(0)\n'"
test-toolkit/resources/hvdbasic/train_hvd_basic.py,0,"b""import json\nimport os\nimport horovod.tensorflow as hvd\n\nhvd.init()\n\nwith open(os.path.join('/opt/ml/model/local-rank-%s-rank-%s' % (hvd.local_rank(), hvd.rank())), 'w+') as f:\n    basic_info = {'local-rank': hvd.local_rank(), 'rank': hvd.rank(), 'size': hvd.size()}\n\n    print(basic_info)\n    json.dump(basic_info, f)\n"""
test-toolkit/resources/mnist/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test-toolkit/resources/mnist/horovod_mnist.py,21,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport tensorflow as tf\nimport horovod.tensorflow as hvd\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n  tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \'GPU\')\n\n(mnist_images, mnist_labels), _ = \\\n    tf.keras.datasets.mnist.load_data(path=\'mnist-%d.npz\' % hvd.rank())\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n     tf.cast(mnist_labels, tf.int64))\n)\ndataset = dataset.repeat().shuffle(10000).batch(128)\n\nmnist_model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, [3, 3], activation=\'relu\'),\n    tf.keras.layers.Conv2D(64, [3, 3], activation=\'relu\'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\'relu\'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(10, activation=\'softmax\')\n])\nloss = tf.losses.SparseCategoricalCrossentropy()\n\n# Horovod: adjust learning rate based on number of GPUs.\nopt = tf.optimizers.Adam(0.001 * hvd.size())\n\ncheckpoint_dir = \'./checkpoints\'\ncheckpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)\n\n\n@tf.function\ndef training_step(images, labels, first_batch):\n  with tf.GradientTape() as tape:\n    probs = mnist_model(images, training=True)\n    loss_value = loss(labels, probs)\n\n  # Horovod: add Horovod Distributed GradientTape.\n  tape = hvd.DistributedGradientTape(tape)\n\n  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n  opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n\n  # Horovod: broadcast initial variable states from rank 0 to all other processes.\n  # This is necessary to ensure consistent initialization of all workers when\n  # training is started with random weights or restored from a checkpoint.\n  #\n  # Note: broadcast should be done after the first gradient step to ensure optimizer\n  # initialization.\n  if first_batch:\n    hvd.broadcast_variables(mnist_model.variables, root_rank=0)\n    hvd.broadcast_variables(opt.variables(), root_rank=0)\n\n  return loss_value\n\n\n# Horovod: adjust number of steps based on number of GPUs.\nfor batch, (images, labels) in enumerate(dataset.take(600 // hvd.size())):\n  loss_value = training_step(images, labels, batch == 0)\n\n  if batch % 10 == 0 and hvd.local_rank() == 0:\n    print(\'Step #%d\\tLoss: %.6f\' % (batch, loss_value))\n\n# Horovod: save checkpoints only on worker 0 to prevent other workers from\n# corrupting it.\nif hvd.rank() == 0:\n  # Export the keras model as Tensorflow SavedModelBundle\n  mnist_model.save(\n      os.path.join(\'/opt/ml/model/mnist/1\'),\n      save_format=\'tf\')\n'"
test-toolkit/resources/mnist/mnist.py,5,"b""import argparse\nimport json\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\n\n\ndef _parse_args():\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=1)\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n\n    return parser.parse_known_args()\n\n\ndef _load_training_data(base_dir):\n    x_train = np.load(os.path.join(base_dir, 'train', 'x_train.npy'))\n    y_train = np.load(os.path.join(base_dir, 'train', 'y_train.npy'))\n    return x_train, y_train\n\n\ndef _load_testing_data(base_dir):\n    x_test = np.load(os.path.join(base_dir, 'test', 'x_test.npy'))\n    y_test = np.load(os.path.join(base_dir, 'test', 'y_test.npy'))\n    return x_test, y_test\n\nargs, unknown = _parse_args()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nx_train, y_train = _load_training_data(args.train)\nx_test, y_test = _load_testing_data(args.train)\nmodel.fit(x_train, y_train, epochs=args.epochs)\nmodel.evaluate(x_test, y_test)\n\nif args.current_host == args.hosts[0]:\n    model.save(os.path.join('/opt/ml/model', 'my_model.h5'))\n"""
test-toolkit/resources/mnist/mnist_estimator.py,37,"b'""""""Convolutional Neural Network Estimator for MNIST, built with tf.layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport argparse\nimport json\n\ndef cnn_model_fn(features, labels, mode):\n  """"""Model function for CNN.""""""\n  # Input Layer\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n  # MNIST images are 28x28 pixels, and have one color channel\n  input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])\n\n  # Convolutional Layer #1\n  # Computes 32 features using a 5x5 filter with ReLU activation.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\n  conv1 = tf.compat.v1.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=""same"",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #1\n  # First max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\n  pool1 = tf.compat.v1.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n  # Convolutional Layer #2\n  # Computes 64 features using a 5x5 filter.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\n  conv2 = tf.compat.v1.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=""same"",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #2\n  # Second max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\n  pool2 = tf.compat.v1.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n  # Flatten tensor into a batch of vectors\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n  # Dense Layer\n  # Densely connected layer with 1024 neurons\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n  # Output Tensor Shape: [batch_size, 1024]\n  dense = tf.compat.v1.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n  # Add dropout operation; 0.6 probability that element will be kept\n  dropout = tf.compat.v1.layers.dropout(\n      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n  # Logits layer\n  # Input Tensor Shape: [batch_size, 1024]\n  # Output Tensor Shape: [batch_size, 10]\n  logits = tf.compat.v1.layers.dense(inputs=dropout, units=10)\n\n  predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      ""classes"": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")\n  }\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Calculate Loss (for both TRAIN and EVAL modes)\n  loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n  # Configure the Training Op (for TRAIN mode)\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.compat.v1.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n  # Add evaluation metrics (for EVAL mode)\n  eval_metric_ops = {\n      ""accuracy"": tf.compat.v1.metrics.accuracy(\n          labels=labels, predictions=predictions[""classes""])}\n  return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\ndef _load_training_data(base_dir):\n    x_train = np.load(os.path.join(base_dir, \'train_data.npy\'))\n    y_train = np.load(os.path.join(base_dir, \'train_labels.npy\'))\n    return x_train, y_train\n\ndef _load_testing_data(base_dir):\n    x_test = np.load(os.path.join(base_dir, \'eval_data.npy\'))\n    y_test = np.load(os.path.join(base_dir, \'eval_labels.npy\'))\n    return x_test, y_test\n\ndef _parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train\', type=str, default=os.environ[\'SM_CHANNEL_TRAINING\'])\n    parser.add_argument(\'--model_dir\', type=str)\n    parser.add_argument(\'--max-steps\', type=int, default=200)\n    parser.add_argument(\'--save-checkpoint-steps\', type=int, default=200)\n    parser.add_argument(\'--throttle-secs\', type=int, default=60)\n    parser.add_argument(\'--hosts\', type=list, default=json.loads(os.environ[\'SM_HOSTS\']))\n    parser.add_argument(\'--current-host\', type=str, default=os.environ[\'SM_CURRENT_HOST\'])\n    parser.add_argument(\'--batch-size\', type=int, default=100)\n    parser.add_argument(\'--export-model-during-training\', type=bool, default=False)\n    return parser.parse_known_args()\n\ndef serving_input_fn():\n    inputs = {\'x\': tf.compat.v1.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\nif __name__ == ""__main__"":\n    args, unknown = _parse_args()\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n\n    logger = tf.get_logger()\n    logger.setLevel(logging.DEBUG)\n    # tf.logging.set_verbosity(tf.logging.DEBUG)\n    train_data, train_labels = _load_training_data(args.train)\n    eval_data, eval_labels = _load_testing_data(args.train)\n\n    # Saving a checkpoint after every step\n    run_config = tf.estimator.RunConfig(save_checkpoints_steps=args.save_checkpoint_steps)\n    mnist_classifier = tf.estimator.Estimator(\n        model_fn=cnn_model_fn, model_dir=args.model_dir, config=run_config)\n\n    # Set up logging for predictions\n    # Log the values in the ""Softmax"" tensor with label ""probabilities""\n    tensors_to_log = {""probabilities"": ""softmax_tensor""}\n    logging_hook = tf.estimator.LoggingTensorHook(\n        tensors=tensors_to_log, every_n_iter=50\n    )\n\n    # Train the model\n    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        x={""x"": train_data},\n        y=train_labels,\n        batch_size=args.batch_size,\n        num_epochs=None,\n        shuffle=True)\n\n    exporter = tf.compat.v1.estimator.LatestExporter(\'Servo\', serving_input_receiver_fn=serving_input_fn) \\\n        if args.export_model_during_training else None\n    # Evaluate the model and print results\n    eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        x={""x"": eval_data},\n        y=eval_labels,\n        num_epochs=1,\n        shuffle=False)\n\n    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=args.max_steps)\n    eval_spec = tf.estimator.EvalSpec(eval_input_fn, throttle_secs=args.throttle_secs, exporters=exporter)\n    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)\n\n    if args.current_host == args.hosts[0]:\n        mnist_classifier.export_saved_model(\'/opt/ml/model\', serving_input_fn)\n'"
test-toolkit/resources/tuning_model_dir/entry.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', type=str)\nparser.add_argument(\'--arbitrary_value\', type=int, default=0)\nargs = parser.parse_args()\n\nassert os.environ[\'TRAINING_JOB_NAME\'] in args.model_dir, \'model_dir not unique to training job: %s\' % args.model_dir\n\n# For the ""hyperparameter tuning"" to work\nprint(\'accuracy=1\')\n'"
test/integration/local/test_horovod.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport json\nimport os\nimport tarfile\n\nimport pytest\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom test.integration.utils import processor, py_version  # noqa: F401\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n\n\n@pytest.mark.skip_gpu\n@pytest.mark.parametrize(\'instances, processes\', [\n    [1, 2],\n    (2, 1),\n    (2, 2),\n    (5, 2)])\ndef test_distributed_training_horovod_basic(instances,\n                                            processes,\n                                            sagemaker_local_session,\n                                            docker_image,\n                                            tmpdir,\n                                            framework_version):\n    output_path = \'file://%s\' % tmpdir\n    estimator = TensorFlow(\n        entry_point=os.path.join(RESOURCE_PATH, \'hvdbasic\', \'train_hvd_basic.py\'),\n        role=\'SageMakerRole\',\n        train_instance_type=\'local\',\n        sagemaker_session=sagemaker_local_session,\n        train_instance_count=instances,\n        image_name=docker_image,\n        output_path=output_path,\n        framework_version=framework_version,\n        hyperparameters={\'sagemaker_mpi_enabled\': True,\n                         \'sagemaker_network_interface_name\': \'eth0\',\n                         \'sagemaker_mpi_num_of_processes_per_host\': processes})\n\n    estimator.fit(\'file://{}\'.format(os.path.join(RESOURCE_PATH, \'mnist\', \'data-distributed\')))\n\n    tmp = str(tmpdir)\n    extract_files(output_path.replace(\'file://\', \'\'), tmp)\n\n    size = instances * processes\n\n    for rank in range(size):\n        local_rank = rank % processes\n        assert read_json(\'local-rank-%s-rank-%s\' % (local_rank, rank), tmp) == {\n            \'local-rank\': local_rank, \'rank\': rank, \'size\': size}\n\n\ndef read_json(file, tmp):\n    with open(os.path.join(tmp, file)) as f:\n        return json.load(f)\n\n\ndef assert_files_exist_in_tar(output_path, files):\n    if output_path.startswith(\'file://\'):\n        output_path = output_path[7:]\n    model_file = os.path.join(output_path, \'model.tar.gz\')\n    with tarfile.open(model_file) as tar:\n        for f in files:\n            tar.getmember(f)\n\n\ndef extract_files(output_path, tmpdir):\n    with tarfile.open(os.path.join(output_path, \'model.tar.gz\')) as tar:\n        tar.extractall(tmpdir)\n'"
test/integration/local/test_keras.py,0,"b'# Copyright 2018-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport logging\nimport os\n\nimport numpy as np\nimport pytest\nfrom sagemaker.tensorflow import serving, TensorFlow\n\nfrom test.integration import RESOURCE_PATH\nfrom test.integration.utils import processor, py_version  # noqa: F401\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\n@pytest.mark.skip(reason=""Serving part fails because of version mismatch."")\ndef test_keras_training(sagemaker_local_session, docker_image, tmpdir, framework_version):\n    entry_point = os.path.join(RESOURCE_PATH, \'keras_inception.py\')\n    output_path = \'file://{}\'.format(tmpdir)\n\n    estimator = TensorFlow(\n        entry_point=entry_point,\n        role=\'SageMakerRole\',\n        train_instance_count=1,\n        train_instance_type=\'local\',\n        image_name=docker_image,\n        sagemaker_session=sagemaker_local_session,\n        model_dir=\'/opt/ml/model\',\n        output_path=output_path,\n        framework_version=framework_version,\n        py_version=\'py3\')\n\n    estimator.fit()\n\n    model = serving.Model(model_data=output_path,\n                          role=\'SageMakerRole\',\n                          framework_version=framework_version,\n                          sagemaker_session=sagemaker_local_session)\n\n    predictor = model.deploy(initial_instance_count=1, instance_type=\'local\')\n\n    assert predictor.predict(np.random.randn(4, 4, 4, 2) * 255)\n\n    predictor.delete_endpoint()\n'"
test/integration/local/test_training.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport tarfile\n\nimport pytest\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom test.integration.utils import processor, py_version  # noqa: F401\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\nTF_CHECKPOINT_FILES = [\'graph.pbtxt\', \'model.ckpt-0.index\', \'model.ckpt-0.meta\']\n\n\n@pytest.fixture  # noqa: F811\ndef py_full_version(py_version):  # noqa: F811\n    if py_version == \'2\':\n        return \'2.7\'\n    elif py_version == \'37\':\n        return \'3.7\'\n    else:\n        return \'3.6\'\n\n\n@pytest.mark.skip_gpu\ndef test_py_versions(sagemaker_local_session, docker_image, py_full_version, framework_version, tmpdir):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'test_py_version\', \'entry.py\'),\n                    instance_type=\'local\',\n                    instance_count=1,\n                    sagemaker_local_session=sagemaker_local_session,\n                    docker_image=docker_image,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    training_data_path=None)\n\n    with tarfile.open(os.path.join(str(tmpdir), \'output.tar.gz\')) as tar:\n        output_file = tar.getmember(\'py_version\')\n        tar.extractall(path=str(tmpdir), members=[output_file])\n\n    with open(os.path.join(str(tmpdir), \'py_version\')) as f:\n        assert f.read().strip()[0] == py_full_version[0]\n\n\n@pytest.mark.skip_gpu\ndef test_mnist_cpu(sagemaker_local_session, docker_image, tmpdir, framework_version):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'mnist\', \'mnist.py\'),\n                    instance_type=\'local\',\n                    instance_count=1,\n                    sagemaker_local_session=sagemaker_local_session,\n                    docker_image=docker_image,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data\')))\n    _assert_files_exist_in_tar(output_path, [\'my_model.h5\'])\n\n\n@pytest.mark.skip_cpu\ndef test_gpu(sagemaker_local_session, docker_image, framework_version):\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'gpu_device_placement.py\'),\n                    instance_type=\'local_gpu\',\n                    instance_count=1,\n                    sagemaker_local_session=sagemaker_local_session,\n                    docker_image=docker_image,\n                    framework_version=framework_version,\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data\')))\n\n\n@pytest.mark.skip_gpu\ndef test_distributed_training_cpu_no_ps(sagemaker_local_session,\n                                        docker_image,\n                                        tmpdir,\n                                        framework_version):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'mnist\', \'mnist_estimator.py\'),\n                    instance_type=\'local\',\n                    instance_count=2,\n                    sagemaker_local_session=sagemaker_local_session,\n                    docker_image=docker_image,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data-distributed\')))\n    _assert_files_exist_in_tar(output_path, TF_CHECKPOINT_FILES)\n\n\n@pytest.mark.skip_gpu\ndef test_distributed_training_cpu_ps(sagemaker_local_session,\n                                     docker_image,\n                                     tmpdir,\n                                     framework_version):\n    output_path = \'file://{}\'.format(tmpdir)\n    run_tf_training(script=os.path.join(RESOURCE_PATH, \'mnist\', \'mnist_estimator.py\'),\n                    instance_type=\'local\',\n                    instance_count=2,\n                    sagemaker_local_session=sagemaker_local_session,\n                    docker_image=docker_image,\n                    framework_version=framework_version,\n                    output_path=output_path,\n                    hyperparameters={\'sagemaker_parameter_server_enabled\': True},\n                    training_data_path=\'file://{}\'.format(\n                        os.path.join(RESOURCE_PATH, \'mnist\', \'data-distributed\')))\n    _assert_files_exist_in_tar(output_path, TF_CHECKPOINT_FILES)\n\n\ndef run_tf_training(script,\n                    instance_type,\n                    instance_count,\n                    sagemaker_local_session,\n                    docker_image,\n                    framework_version,\n                    training_data_path,\n                    output_path=None,\n                    hyperparameters=None):\n\n    hyperparameters = hyperparameters or {}\n\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_count=instance_count,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_local_session,\n                           image_name=docker_image,\n                           model_dir=\'/opt/ml/model\',\n                           output_path=output_path,\n                           hyperparameters=hyperparameters,\n                           base_job_name=\'test-tf\',\n                           framework_version=framework_version,\n                           py_version=\'py3\')\n\n    estimator.fit(training_data_path)\n\n\ndef _assert_files_exist_in_tar(output_path, files):\n    if output_path.startswith(\'file://\'):\n        output_path = output_path[7:]\n    model_file = os.path.join(output_path, \'model.tar.gz\')\n    with tarfile.open(model_file) as tar:\n        for f in files:\n            tar.getmember(f)\n'"
test/integration/sagemaker/test_experiments.py,0,"b'# Copyright 2018-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# or in the ""license"" file accompanying this file. This file is distributed\n# on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport time\n\nimport pytest\nfrom sagemaker import utils\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom test.integration import RESOURCE_PATH\nfrom timeout import timeout\n\nDATA_PATH = os.path.join(RESOURCE_PATH, ""mnist"")\nSCRIPT_PATH = os.path.join(DATA_PATH, ""mnist_gluon_basic_hook_demo.py"")\n\n\n@pytest.mark.skip_py2_containers\ndef test_training(sagemaker_session, ecr_image, instance_type, framework_version, py_version):\n\n    if py_version is None or \'2\' in py_version:\n        pytest.skip(\'Skipping python2 {}\'.format(py_version))\n        return\n\n    from smexperiments.experiment import Experiment\n    from smexperiments.trial import Trial\n    from smexperiments.trial_component import TrialComponent\n\n    sm_client = sagemaker_session.sagemaker_client\n\n    experiment_name = ""tf-container-integ-test-{}"".format(int(time.time()))\n\n    experiment = Experiment.create(\n        experiment_name=experiment_name,\n        description=""Integration test experiment from sagemaker-tf-container"",\n        sagemaker_boto_client=sm_client,\n    )\n\n    trial_name = ""tf-container-integ-test-{}"".format(int(time.time()))\n    trial = Trial.create(\n        experiment_name=experiment_name, trial_name=trial_name, sagemaker_boto_client=sm_client\n    )\n\n    training_job_name = utils.unique_name_from_base(""test-tf-experiments-mnist"")\n\n    # create a training job and wait for it to complete\n    with timeout(minutes=15):\n        resource_path = os.path.join(os.path.dirname(__file__), "".."", "".."", ""resources"")\n        script = os.path.join(resource_path, ""mnist"", ""mnist.py"")\n        estimator = TensorFlow(\n            entry_point=script,\n            role=""SageMakerRole"",\n            train_instance_type=instance_type,\n            train_instance_count=1,\n            sagemaker_session=sagemaker_session,\n            image_name=ecr_image,\n            framework_version=framework_version,\n            script_mode=True,\n        )\n        inputs = estimator.sagemaker_session.upload_data(\n            path=os.path.join(resource_path, ""mnist"", ""data""), key_prefix=""scriptmode/mnist""\n        )\n        estimator.fit(inputs, job_name=training_job_name)\n\n    training_job = sm_client.describe_training_job(TrainingJobName=training_job_name)\n    training_job_arn = training_job[""TrainingJobArn""]\n\n    # verify trial component auto created from the training job\n    trial_components = list(\n        TrialComponent.list(source_arn=training_job_arn, sagemaker_boto_client=sm_client)\n    )\n\n    trial_component_summary = trial_components[0]\n    trial_component = TrialComponent.load(\n        trial_component_name=trial_component_summary.trial_component_name,\n        sagemaker_boto_client=sm_client,\n    )\n\n    # associate the trial component with the trial\n    trial.add_trial_component(trial_component)\n\n    # cleanup\n    trial.remove_trial_component(trial_component_summary.trial_component_name)\n    trial_component.delete()\n    trial.delete()\n    experiment.delete()\n'"
test/integration/sagemaker/test_horovod.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nfrom test.integration.utils import processor, py_version, unique_name_from_base  # noqa: F401\n\nRESOURCE_PATH = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n\n\ndef test_distributed_training_horovod(sagemaker_session,\n                                      instance_type,\n                                      ecr_image,\n                                      tmpdir,\n                                      framework_version):\n\n    mpi_options = \'-verbose -x orte_base_help_aggregate=0\'\n    estimator = TensorFlow(\n        entry_point=os.path.join(RESOURCE_PATH, \'mnist\', \'horovod_mnist.py\'),\n        role=\'SageMakerRole\',\n        train_instance_type=instance_type,\n        train_instance_count=2,\n        image_name=ecr_image,\n        framework_version=framework_version,\n        py_version=\'py3\',\n        script_mode=True,\n        hyperparameters={\'sagemaker_mpi_enabled\': True,\n                         \'sagemaker_mpi_custom_mpi_options\': mpi_options,\n                         \'sagemaker_mpi_num_of_processes_per_host\': 1},\n        sagemaker_session=sagemaker_session)\n\n    estimator.fit(job_name=unique_name_from_base(\'test-tf-horovod\'))\n\n    model_data_source = sagemaker.local.data.get_data_source_instance(\n        estimator.model_data, sagemaker_session)\n\n    for filename in model_data_source.get_file_list():\n        assert os.path.basename(filename) == \'model.tar.gz\'\n'"
test/integration/sagemaker/test_mnist.py,0,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nimport boto3\nimport pytest\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter\nfrom six.moves.urllib.parse import urlparse\n\nfrom test.integration.utils import processor, py_version, unique_name_from_base  # noqa: F401\nfrom timeout import timeout\n\n\n@pytest.mark.deploy_test\ndef test_mnist(sagemaker_session, ecr_image, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           sagemaker_session=sagemaker_session,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           script_mode=True)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data\'),\n        key_prefix=\'scriptmode/mnist\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-sagemaker-mnist\'))\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef test_distributed_mnist_no_ps(sagemaker_session, ecr_image, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_count=2,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_session,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           script_mode=True)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data\'),\n        key_prefix=\'scriptmode/mnist\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-tf-sm-distributed-mnist\'))\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef test_distributed_mnist_ps(sagemaker_session, ecr_image, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist_estimator.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           hyperparameters={\'sagemaker_parameter_server_enabled\': True},\n                           train_instance_count=2,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_session,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           script_mode=True)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data-distributed\'),\n        key_prefix=\'scriptmode/mnist-distributed\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-tf-sm-distributed-mnist\'))\n    _assert_checkpoint_exists(sagemaker_session.boto_region_name, estimator.model_dir, 0)\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef test_s3_plugin(sagemaker_session, ecr_image, instance_type, region, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist_estimator.py\')\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           hyperparameters={\n                               # Saving a checkpoint after every 5 steps to hammer the S3 plugin\n                               \'save-checkpoint-steps\': 10,\n                               # Disable throttling for checkpoint and model saving\n                               \'throttle-secs\': 0,\n                               # Without the patch training jobs would fail around 100th to\n                               # 150th step\n                               \'max-steps\': 200,\n                               # Large batch size would result in a larger checkpoint file\n                               \'batch-size\': 1024,\n                               # This makes the training job exporting model during training.\n                               # Stale model garbage collection will also be performed.\n                               \'export-model-during-training\': True\n                           },\n                           train_instance_count=1,\n                           train_instance_type=instance_type,\n                           sagemaker_session=sagemaker_session,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           script_mode=True)\n    estimator.fit(\'s3://sagemaker-sample-data-{}/tensorflow/mnist\'.format(region),\n                  job_name=unique_name_from_base(\'test-tf-sm-s3-mnist\'))\n    _assert_s3_file_exists(region, estimator.model_data)\n    _assert_checkpoint_exists(region, estimator.model_dir, 200)\n\n\ndef test_tuning(sagemaker_session, ecr_image, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'mnist.py\')\n\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           sagemaker_session=sagemaker_session,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           script_mode=True)\n\n    hyperparameter_ranges = {\'epochs\': IntegerParameter(1, 2)}\n    objective_metric_name = \'accuracy\'\n    metric_definitions = [{\'Name\': objective_metric_name, \'Regex\': \'accuracy = ([0-9\\\\.]+)\'}]\n\n    tuner = HyperparameterTuner(estimator,\n                                objective_metric_name,\n                                hyperparameter_ranges,\n                                metric_definitions,\n                                max_jobs=2,\n                                max_parallel_jobs=2)\n\n    with timeout(minutes=20):\n        inputs = estimator.sagemaker_session.upload_data(\n            path=os.path.join(resource_path, \'mnist\', \'data\'),\n            key_prefix=\'scriptmode/mnist\')\n\n        tuning_job_name = unique_name_from_base(\'test-tf-sm-tuning\', max_length=32)\n        tuner.fit(inputs, job_name=tuning_job_name)\n        tuner.wait()\n\n\n@pytest.mark.skip_py2_containers\ndef test_tf1x_smdebug(sagemaker_session, ecr_image, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'..\', \'..\', \'resources\')\n    script = os.path.join(resource_path, \'mnist\', \'tf1x_mnist_smdebug.py\')\n    hyperparameters = {\'smdebug_path\': \'/opt/ml/output/tensors\'}\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           sagemaker_session=sagemaker_session,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           script_mode=True,\n                           hyperparameters=hyperparameters)\n    inputs = estimator.sagemaker_session.upload_data(\n        path=os.path.join(resource_path, \'mnist\', \'data\'),\n        key_prefix=\'scriptmode/mnist_smdebug\')\n    estimator.fit(inputs, job_name=unique_name_from_base(\'test-sagemaker-mnist-smdebug\'))\n    _assert_s3_file_exists(sagemaker_session.boto_region_name, estimator.model_data)\n\n\ndef _assert_checkpoint_exists(region, model_dir, checkpoint_number):\n    _assert_s3_file_exists(region, os.path.join(model_dir, \'graph.pbtxt\'))\n    _assert_s3_file_exists(region,\n                           os.path.join(model_dir, \'model.ckpt-{}.index\'.format(checkpoint_number)))\n    _assert_s3_file_exists(region,\n                           os.path.join(model_dir, \'model.ckpt-{}.meta\'.format(checkpoint_number)))\n\n\ndef _assert_s3_file_exists(region, s3_url):\n    parsed_url = urlparse(s3_url)\n    s3 = boto3.resource(\'s3\', region_name=region)\n    s3.Object(parsed_url.netloc, parsed_url.path.lstrip(\'/\')).load()\n'"
test/integration/sagemaker/test_tuning_model_dir.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\n\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker.tuner import HyperparameterTuner, IntegerParameter\n\nfrom test.integration.utils import processor, py_version, unique_name_from_base  # noqa: F401\n\n\ndef test_model_dir_with_training_job_name(sagemaker_session, ecr_image, instance_type, framework_version):\n    resource_path = os.path.join(os.path.dirname(__file__), \'../..\', \'resources\')\n    script = os.path.join(resource_path, \'tuning_model_dir\', \'entry.py\')\n\n    estimator = TensorFlow(entry_point=script,\n                           role=\'SageMakerRole\',\n                           train_instance_type=instance_type,\n                           train_instance_count=1,\n                           image_name=ecr_image,\n                           framework_version=framework_version,\n                           py_version=\'py3\',\n                           sagemaker_session=sagemaker_session)\n\n    tuner = HyperparameterTuner(estimator=estimator,\n                                objective_metric_name=\'accuracy\',\n                                hyperparameter_ranges={\'arbitrary_value\': IntegerParameter(0, 1)},\n                                metric_definitions=[{\'Name\': \'accuracy\', \'Regex\': \'accuracy=([01])\'}],\n                                max_jobs=1,\n                                max_parallel_jobs=1)\n\n    # User script has logic to check for the correct model_dir\n    tuner.fit(job_name=unique_name_from_base(\'test-tf-model-dir\', max_length=32))\n    tuner.wait()\n'"
test/integration/sagemaker/timeout.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"").\n# You may not use this file except in compliance with the License.\n# A copy of the License is located at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# or in the ""license"" file accompanying this file. This file is distributed\n# on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n# express or implied. See the License for the specific language governing\n# permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nfrom contextlib import contextmanager\nimport logging\nimport signal\n\nLOGGER = logging.getLogger(\'timeout\')\n\n\nclass TimeoutError(Exception):\n    pass\n\n\n@contextmanager\ndef timeout(seconds=0, minutes=0, hours=0):\n    """"""Add a signal-based timeout to any block of code.\n    If multiple time units are specified, they will be added together to determine time limit.\n    Usage:\n    with timeout(seconds=5):\n        my_slow_function(...)\n    Args:\n        - seconds: The time limit, in seconds.\n        - minutes: The time limit, in minutes.\n        - hours: The time limit, in hours.\n    """"""\n\n    limit = seconds + 60 * minutes + 3600 * hours\n\n    def handler(signum, frame):\n        raise TimeoutError(\'timed out after {} seconds\'.format(limit))\n\n    try:\n        signal.signal(signal.SIGALRM, handler)\n        signal.alarm(limit)\n\n        yield\n    finally:\n        signal.alarm(0)\n'"
test/resources/hvdbasic/train_hvd_basic.py,0,"b""import json\nimport os\nimport horovod.tensorflow as hvd\n\nhvd.init()\n\nwith open(os.path.join('/opt/ml/model/local-rank-%s-rank-%s' % (hvd.local_rank(), hvd.rank())), 'w+') as f:\n    basic_info = {'local-rank': hvd.local_rank(), 'rank': hvd.rank(), 'size': hvd.size()}\n\n    print(basic_info)\n    json.dump(basic_info, f)\n"""
test/resources/mnist/__init__.py,0,"b'#  Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"").\n#  You may not use this file except in compliance with the License.\n#  A copy of the License is located at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  or in the ""license"" file accompanying this file. This file is distributed\n#  on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n#  express or implied. See the License for the specific language governing\n#  permissions and limitations under the License.\nfrom __future__ import absolute_import\n'"
test/resources/mnist/horovod_mnist.py,4,"b'# Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import, print_function\n\nimport os\nimport subprocess\n\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport horovod.tensorflow.keras as hvd\n\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.visible_device_list = str(hvd.local_rank())\nK.set_session(tf.Session(config=config))\n\nbatch_size = 128\nnum_classes = 10\n\nepochs = 1\n\n# Input image dimensions\nimg_rows, img_cols = 28, 28\n\n# The data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train[:600]\ny_train = y_train[:600]\nx_test = x_test[:100]\ny_test = y_test[:100]\n\nif K.image_data_format() == \'channels_first\':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\nx_train /= 255\nx_test /= 255\nprint(\'x_train shape:\', x_train.shape)\nprint(x_train.shape[0], \'train samples\')\nprint(x_test.shape[0], \'test samples\')\n\n# Convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation=\'relu\',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation=\'softmax\'))\n\n# Horovod: adjust learning rate based on number of GPUs.\nopt = keras.optimizers.Adadelta(1.0 * hvd.size())\n\n# Horovod: add Horovod Distributed Optimizer.\nopt = hvd.DistributedOptimizer(opt)\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=opt,\n              metrics=[\'accuracy\'])\n\ncallbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n]\n\n# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\nif hvd.rank() == 0:\n    callbacks.append(keras.callbacks.ModelCheckpoint(\'./checkpoint-{epoch}.h5\'))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          callbacks=callbacks,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\'Test loss:\', score[0])\nprint(\'Test accuracy:\', score[1])\n\n\nif hvd.rank() == 0:\n    # Exports the keras model as TensorFlow Serving Saved Model\n    with K.get_session() as session:\n\n        init = tf.global_variables_initializer()\n        session.run(init)\n\n        tf.saved_model.simple_save(\n            session,\n            os.path.join(\'/opt/ml/model/mnist/1\'),\n            inputs={\'input_image\': model.input},\n            outputs={t.name: t for t in model.outputs})\n'"
test/resources/mnist/mnist.py,5,"b""import argparse\nimport json\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\n\n\ndef _parse_args():\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=1)\n    # Data, model, and output directories\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n\n    return parser.parse_known_args()\n\n\ndef _load_training_data(base_dir):\n    x_train = np.load(os.path.join(base_dir, 'train', 'x_train.npy'))\n    y_train = np.load(os.path.join(base_dir, 'train', 'y_train.npy'))\n    return x_train, y_train\n\n\ndef _load_testing_data(base_dir):\n    x_test = np.load(os.path.join(base_dir, 'test', 'x_test.npy'))\n    y_test = np.load(os.path.join(base_dir, 'test', 'y_test.npy'))\n    return x_test, y_test\n\n\ndef assert_can_track_sagemaker_experiments():\n    in_sagemaker_training = 'TRAINING_JOB_ARN' in os.environ\n    in_python_three = sys.version_info[0] == 3\n\n    if in_sagemaker_training and in_python_three:\n        import smexperiments.tracker\n\n        with smexperiments.tracker.Tracker.load() as tracker:\n            tracker.log_parameter('param', 1)\n            tracker.log_metric('metric', 1.0)\n\n\nargs, unknown = _parse_args()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nx_train, y_train = _load_training_data(args.train)\nx_test, y_test = _load_testing_data(args.train)\nmodel.fit(x_train, y_train, epochs=args.epochs)\nmodel.evaluate(x_test, y_test)\n\nif args.current_host == args.hosts[0]:\n    model.save(os.path.join('/opt/ml/model', 'my_model.h5'))\n    assert_can_track_sagemaker_experiments()\n"""
test/resources/mnist/mnist_estimator.py,36,"b'""""""Convolutional Neural Network Estimator for MNIST, built with tf.layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport argparse\nimport json\n\ndef cnn_model_fn(features, labels, mode):\n  """"""Model function for CNN.""""""\n  # Input Layer\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n  # MNIST images are 28x28 pixels, and have one color channel\n  input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])\n\n  # Convolutional Layer #1\n  # Computes 32 features using a 5x5 filter with ReLU activation.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\n  conv1 = tf.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=""same"",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #1\n  # First max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n  # Convolutional Layer #2\n  # Computes 64 features using a 5x5 filter.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\n  conv2 = tf.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=""same"",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #2\n  # Second max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n  # Flatten tensor into a batch of vectors\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n  # Dense Layer\n  # Densely connected layer with 1024 neurons\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n  # Output Tensor Shape: [batch_size, 1024]\n  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n  # Add dropout operation; 0.6 probability that element will be kept\n  dropout = tf.layers.dropout(\n      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n  # Logits layer\n  # Input Tensor Shape: [batch_size, 1024]\n  # Output Tensor Shape: [batch_size, 10]\n  logits = tf.layers.dense(inputs=dropout, units=10)\n\n  predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      ""classes"": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")\n  }\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Calculate Loss (for both TRAIN and EVAL modes)\n  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n  # Configure the Training Op (for TRAIN mode)\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n  # Add evaluation metrics (for EVAL mode)\n  eval_metric_ops = {\n      ""accuracy"": tf.metrics.accuracy(\n          labels=labels, predictions=predictions[""classes""])}\n  return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\ndef _load_training_data(base_dir):\n    x_train = np.load(os.path.join(base_dir, \'train_data.npy\'))\n    y_train = np.load(os.path.join(base_dir, \'train_labels.npy\'))\n    return x_train, y_train\n\ndef _load_testing_data(base_dir):\n    x_test = np.load(os.path.join(base_dir, \'eval_data.npy\'))\n    y_test = np.load(os.path.join(base_dir, \'eval_labels.npy\'))\n    return x_test, y_test\n\ndef _parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train\', type=str, default=os.environ[\'SM_CHANNEL_TRAINING\'])\n    parser.add_argument(\'--model_dir\', type=str)\n    parser.add_argument(\'--max-steps\', type=int, default=200)\n    parser.add_argument(\'--save-checkpoint-steps\', type=int, default=200)\n    parser.add_argument(\'--throttle-secs\', type=int, default=60)\n    parser.add_argument(\'--hosts\', type=list, default=json.loads(os.environ[\'SM_HOSTS\']))\n    parser.add_argument(\'--current-host\', type=str, default=os.environ[\'SM_CURRENT_HOST\'])\n    parser.add_argument(\'--batch-size\', type=int, default=100)\n    parser.add_argument(\'--export-model-during-training\', type=bool, default=False)\n    return parser.parse_known_args()\n\ndef serving_input_fn():\n    inputs = {\'x\': tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\nif __name__ == ""__main__"":\n    args, unknown = _parse_args()\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    train_data, train_labels = _load_training_data(args.train)\n    eval_data, eval_labels = _load_testing_data(args.train)\n\n    # Saving a checkpoint after every step\n    run_config = tf.estimator.RunConfig(save_checkpoints_steps=args.save_checkpoint_steps)\n    mnist_classifier = tf.estimator.Estimator(\n        model_fn=cnn_model_fn, model_dir=args.model_dir, config=run_config)\n\n    # Set up logging for predictions\n    # Log the values in the ""Softmax"" tensor with label ""probabilities""\n    tensors_to_log = {""probabilities"": ""softmax_tensor""}\n    logging_hook = tf.train.LoggingTensorHook(\n        tensors=tensors_to_log, every_n_iter=50\n    )\n\n    # Train the model\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={""x"": train_data},\n        y=train_labels,\n        batch_size=args.batch_size,\n        num_epochs=None,\n        shuffle=True)\n\n    exporter = tf.estimator.LatestExporter(\'Servo\', serving_input_receiver_fn=serving_input_fn) \\\n        if args.export_model_during_training else None\n    # Evaluate the model and print results\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={""x"": eval_data},\n        y=eval_labels,\n        num_epochs=1,\n        shuffle=False)\n\n    train_spec = tf.estimator.TrainSpec(train_input_fn, max_steps=args.max_steps)\n    eval_spec = tf.estimator.EvalSpec(eval_input_fn, throttle_secs=args.throttle_secs, exporters=exporter)\n    tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec)\n\n    if args.current_host == args.hosts[0]:\n        mnist_classifier.export_savedmodel(\'/opt/ml/model\', serving_input_fn)\n'"
test/resources/mnist/tf1x_mnist_smdebug.py,5,"b'import argparse\nimport json\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport smdebug.tensorflow as smd\nfrom smdebug.core.collection import CollectionKeys\nfrom smdebug.core.reduction_config import ALLOWED_NORMS, ALLOWED_REDUCTIONS\nfrom smdebug.tensorflow import ReductionConfig, SaveConfig\nfrom smdebug.trials import create_trial\n\n\ndef _parse_args():\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument(\'--epochs\', type=int, default=1)\n    # Data, model, and output directories\n    parser.add_argument(\'--model-dir\', type=str, default=os.environ[\'SM_MODEL_DIR\'])\n    parser.add_argument(\n        ""--smdebug_path"",\n        type=str,\n        default=None,\n        help=""S3 URI of the bucket where tensor data will be stored."",\n    )\n    parser.add_argument(\'--train\', type=str, default=os.environ[\'SM_CHANNEL_TRAINING\'])\n    parser.add_argument(\'--hosts\', type=list, default=json.loads(os.environ[\'SM_HOSTS\']))\n    parser.add_argument(\'--current-host\', type=str, default=os.environ[\'SM_CURRENT_HOST\'])\n\n    return parser.parse_known_args()\n\n\ndef _load_training_data(base_dir):\n    x_train = np.load(os.path.join(base_dir, \'train\', \'x_train.npy\'))\n    y_train = np.load(os.path.join(base_dir, \'train\', \'y_train.npy\'))\n    return x_train, y_train\n\n\ndef _load_testing_data(base_dir):\n    x_test = np.load(os.path.join(base_dir, \'test\', \'x_test.npy\'))\n    y_test = np.load(os.path.join(base_dir, \'test\', \'y_test.npy\'))\n    return x_test, y_test\n\n\ndef create_smdebug_hook(out_dir):\n    include_collections = [\n        CollectionKeys.WEIGHTS,\n        CollectionKeys.BIASES,\n        CollectionKeys.GRADIENTS,\n        CollectionKeys.LOSSES,\n        CollectionKeys.OUTPUTS,\n        CollectionKeys.METRICS,\n        CollectionKeys.LOSSES,\n        CollectionKeys.OPTIMIZER_VARIABLES,\n    ]\n    save_config = SaveConfig(save_interval=3)\n    hook = smd.KerasHook(\n        out_dir,\n        save_config=save_config,\n        include_collections=include_collections,\n        reduction_config=ReductionConfig(norms=ALLOWED_NORMS, reductions=ALLOWED_REDUCTIONS),\n    )\n    return hook\n\n\nargs, unknown = _parse_args()\n\nhook = create_smdebug_hook(args.smdebug_path)\nhooks = [hook]\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\nmodel.compile(optimizer=\'adam\',\n              loss=\'sparse_categorical_crossentropy\',\n              metrics=[\'accuracy\'])\nx_train, y_train = _load_training_data(args.train)\nx_test, y_test = _load_testing_data(args.train)\nmodel.fit(x_train, y_train, epochs=args.epochs, callbacks=hooks)\nmodel.evaluate(x_test, y_test, callbacks=hooks)\n\nif args.current_host == args.hosts[0]:\n    model.save(os.path.join(\'/opt/ml/model\', \'my_model.h5\'))\n\nprint(""Created the trial with out_dir {0}"".format(args.smdebug_path))\ntrial = create_trial(args.smdebug_path)\nassert trial\n\nprint(f""trial.tensor_names() = {trial.tensor_names()}"")\n\nweights_tensors = hook.collection_manager.get(""weights"").tensor_names\nassert len(weights_tensors) > 0\n\nlosses_tensors = hook.collection_manager.get(""losses"").tensor_names\nassert len(losses_tensors) > 0\n'"
test/resources/test_py_version/entry.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport os\nimport sys\n\n\npy_version = \'%s.%s\' % (sys.version_info.major, sys.version_info.minor)\n\nwith open(os.path.join(os.environ[\'SM_OUTPUT_DIR\'], \'py_version\'), \'a\') as f:\n    f.write(py_version)\n'"
test/resources/tuning_model_dir/entry.py,0,"b'# Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is\n# distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom __future__ import absolute_import\n\nimport argparse\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', type=str)\nparser.add_argument(\'--arbitrary_value\', type=int, default=0)\nargs = parser.parse_args()\n\nassert os.environ[\'TRAINING_JOB_NAME\'] in args.model_dir, \'model_dir not unique to training job: %s\' % args.model_dir\n\n# For the ""hyperparameter tuning"" to work\nprint(\'accuracy=1\')\n'"
