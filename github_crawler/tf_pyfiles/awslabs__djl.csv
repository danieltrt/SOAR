file_path,api_count,code
basicdataset/src/main/resources/imagenet/extract_imagenet.py,0,"b'""""""Prepare the ImageNet dataset""""""\nimport os\nimport argparse\nimport hashlib\nimport requests\nimport tarfile\nimport pickle\nimport gzip\nimport subprocess\nfrom tqdm import tqdm\n\n_TRAIN_TAR = \'ILSVRC2012_img_train.tar\'\n_TRAIN_TAR_SHA1 = \'43eda4fe35c1705d6606a6a7a633bc965d194284\'\n_VAL_TAR = \'ILSVRC2012_img_val.tar\'\n_VAL_TAR_SHA1 = \'5f3f73da3395154b60528b2b2a2caf2374f5f178\'\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download an given URL\n    Parameters\n    ----------\n    url : str\n        URL to download\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if already exists.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn\'t match.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        print(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Setup the ImageNet dataset.\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', required=True,\n                        help=""The directory that contains downloaded tar files"")\n    parser.add_argument(\'--target-dir\',\n                        help=""The directory to store extracted images"")\n    parser.add_argument(\'--checksum\', action=\'store_true\',\n                        help=""If check integrity before extracting."")\n    parser.add_argument(\'--with-rec\', action=\'store_true\',\n                        help=""If build image record files."")\n    parser.add_argument(\'--num-thread\', type=int, default=1,\n                        help=""Number of threads to use when building image record file."")\n    args = parser.parse_args()\n    if args.target_dir is None:\n        args.target_dir = args.download_dir\n    return args\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    return sha1.hexdigest() == sha1_hash\n\ndef check_file(filename, checksum, sha1):\n    if not os.path.exists(filename):\n        raise ValueError(\'File not found: \'+filename)\n    if checksum and not check_sha1(filename, sha1):\n        raise ValueError(\'Corrupted file: \'+filename)\n\ndef build_rec_process(img_dir, train=False, num_thread=1):\n    rec_dir = os.path.abspath(os.path.join(img_dir, \'../rec\'))\n    makedirs(rec_dir)\n    prefix = \'train\' if train else \'val\'\n    print(\'Building ImageRecord file for \' + prefix + \' ...\')\n    to_path = rec_dir\n\n    # download lst file and im2rec script\n    script_path = os.path.join(rec_dir, \'im2rec.py\')\n    script_url = \'https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py\'\n    download(script_url, script_path)\n\n    lst_path = os.path.join(rec_dir, prefix + \'.lst\')\n    lst_url = \'http://data.mxnet.io/models/imagenet/resnet/\' + prefix + \'.lst\'\n    download(lst_url, lst_path)\n\n    # execution\n    import sys\n    cmd = [\n        sys.executable,\n        script_path,\n        rec_dir,\n        img_dir,\n        \'--recursive\',\n        \'--pass-through\',\n        \'--pack-label\',\n        \'--num-thread\',\n        str(num_thread)\n    ]\n    subprocess.call(cmd)\n    os.remove(script_path)\n    os.remove(lst_path)\n    print(\'ImageRecord file for \' + prefix + \' has been built!\')\n\ndef extract_train(tar_fname, target_dir, with_rec=False, num_thread=1):\n    os.makedirs(target_dir)\n    with tarfile.open(tar_fname) as tar:\n        print(""Extracting ""+tar_fname+""..."")\n        # extract each class one-by-one\n        pbar = tqdm(total=len(tar.getnames()))\n        for class_tar in tar:\n            pbar.set_description(\'Extract \'+class_tar.name)\n            tar.extract(class_tar, target_dir)\n            class_fname = os.path.join(target_dir, class_tar.name)\n            class_dir = os.path.splitext(class_fname)[0]\n            os.mkdir(class_dir)\n            with tarfile.open(class_fname) as f:\n                f.extractall(class_dir)\n            os.remove(class_fname)\n            pbar.update(1)\n        pbar.close()\n    if with_rec:\n        build_rec_process(target_dir, True, num_thread)\n\ndef extract_val(tar_fname, target_dir, with_rec=False, num_thread=1):\n    os.makedirs(target_dir)\n    print(\'Extracting \' + tar_fname)\n    with tarfile.open(tar_fname) as tar:\n        tar.extractall(target_dir)\n    # build rec file before images are moved into subfolders\n    if with_rec:\n        build_rec_process(target_dir, False, num_thread)\n    # move images to proper subfolders\n    val_maps_file = os.path.join(os.path.dirname(__file__), \'imagenet_val_maps.pklz\')\n    with gzip.open(val_maps_file, \'rb\') as f:\n        dirs, mappings = pickle.load(f)\n    for d in dirs:\n        os.makedirs(os.path.join(target_dir, d))\n    for m in mappings:\n        os.rename(os.path.join(target_dir, m[0]), os.path.join(target_dir, m[1], m[0]))\n\ndef main():\n    args = parse_args()\n\n    target_dir = os.path.expanduser(args.target_dir)\n    if os.path.exists(target_dir):\n        raise ValueError(\'Target dir [\'+target_dir+\'] exists. Remove it first\')\n\n    download_dir = os.path.expanduser(args.download_dir)\n    train_tar_fname = os.path.join(download_dir, _TRAIN_TAR)\n    check_file(train_tar_fname, args.checksum, _TRAIN_TAR_SHA1)\n    val_tar_fname = os.path.join(download_dir, _VAL_TAR)\n    check_file(val_tar_fname, args.checksum, _VAL_TAR_SHA1)\n\n    build_rec = args.with_rec\n    if build_rec:\n        os.makedirs(os.path.join(target_dir, \'rec\'))\n    extract_train(train_tar_fname, os.path.join(target_dir, \'train\'), build_rec, args.num_thread)\n    extract_val(val_tar_fname, os.path.join(target_dir, \'val\'), build_rec, args.num_thread)\n\nif __name__ == \'__main__\':\n    main()\n'"
mxnet/mxnet-model-zoo/src/main/scripts/convertEmbedding.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""). You may not use this file except in compliance\n# with the License. A copy of the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the ""license"" file accompanying this file. This file is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES\n# OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions\n# and limitations under the License.\n\nimport mxnet as mx\nimport numpy as np\n\nfilename = \'glove.6B.50d.npz\'\nnp_loaded = np.load(filename, allow_pickle=True)\n\nidx_to_vec = mx.nd.array(np_loaded[\'idx_to_vec\'])\nmx.ndarray.save(\'idx_to_vec.mx\', idx_to_vec)\n\nunknown_token = str(np_loaded[\'unknown_token\'])\nwith open(\'unknown_token.txt\', \'w\') as f:\n    f.write(unknown_token)\n\nidx_to_token = [str(s) for s in np_loaded[\'idx_to_token\']]\nwith open(\'idx_to_token.txt\', \'w\') as f:\n    for s in idx_to_token:\n        print(idx_to_token, file=f)\n'"
mxnet/mxnet-model-zoo/src/main/scripts/exportYolo.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport json\nimport mxnet as mx\nfrom gluoncv import model_zoo\n\ntoExport = [\n    (""darknet53"", ""voc"", (320, 320)),\n    (""darknet53"", ""voc"", (416, 416)),\n    (""mobilenet1.0"", ""voc"", (320, 320)),\n    (""mobilenet1.0"", ""voc"", (416, 416)),\n    (""darknet53"", ""coco"", (320, 320)),\n    (""darknet53"", ""coco"", (416, 416)),\n    (""darknet53"", ""coco"", (608, 608)),\n    (""mobilenet1.0"", ""coco"", (320, 320)),\n    (""mobilenet1.0"", ""coco"", (416, 416)),\n    (""mobilenet1.0"", ""coco"", (608, 608)),\n    ]\n\nversion = ""0.0.1""\n\nmetadata = {\n        \'metadataVersion\': 0.1,\n        \'groupId\': \'ai.djl.mxnet\',\n        \'artifactId\': \'yolo\',\n        \'name\': \'yolo\',\n        \'description\': \'Yolo GluonCV Model\',\n        \'website\': \'https://gluon-cv.mxnet.io/\',\n        \'licenses\': {\n            \'license\': {\n                \'name\': \'The Apache License, Version 2.0\',\n                \'url\': \'https://www.apache.org/licenses/LICENSE-2.0\'\n                }\n            },\n        \'artifacts\': []\n        }\n\nfor baseModel, dataset, imageSize in toExport:\n    mx.autograd.set_training(False)\n    mx.autograd.set_recording(False)\n    modelName = ""yolo3_"" + baseModel + ""_"" + dataset\n    print(""Exporting"", modelName, imageSize)\n    height, width = imageSize\n    artifact = {\n            \'version\': version,\n            \'snapshot\': False,\n            \'name\': \'yolo\',\n            \'properties\': {\n                \'dataset\': dataset,\n                \'version\': \'3\',\n                \'backbone\': baseModel,\n                \'imageSize\': height\n                },\n            \'arguments\': {\n                \'threshold\': 0.2\n                },\n            \'files\': dict()\n            }\n    net = model_zoo.get_model(modelName, pretrained=True)\n    imageShape = (32, 3, height, width)\n    x = mx.nd.random.normal(shape=imageShape)\n    net(x)\n    net.hybridize()\n    net(x)\n\n    dirName = ""out/"" + version + ""/"" + modelName + ""-"" + str(height) + ""x"" + str(width) + ""/""\n    os.makedirs(dirName)\n    net.export(dirName + ""yolo"")\n\n    if dataset == ""coco"":\n        artifact[\'files\'][\'classes\'] = {\n                \'uri\': \'https://mlrepo.djl.ai/model/cv/object_detection/ai/djl/mxnet/classes_coco.txt\',\n                \'sha1Hash\': \'1febf3c237fb06e472a001fd8e03f16cc6174090\',\n                \'name\': \'classes.txt\',\n                \'size\': 620\n                }\n    elif dataset == ""voc"":\n        artifact[\'files\'][\'classes\'] = {\n                \'uri\': \'https://mlrepo.djl.ai/model/cv/object_detection/ai/djl/mxnet/classes_voc.txt\',\n                \'sha1Hash\': \'c6796ef8b46238f33366d94f3e16801cbda9e7f8\',\n                \'name\': \'classes.txt\',\n                \'size\': 134\n                }\n\n    symbolFilename = dirName + ""yolo-symbol.json""\n    artifact[\'files\'][\'symbol\'] = {\n            \'uri\': symbolFilename[4:],\n            \'sha1Hash\': os.popen(""gsha1sum "" + symbolFilename).read().split()[0],\n            \'size\': os.path.getsize(symbolFilename)\n            }\n\n    paramFilename = dirName + ""yolo-0000.params""\n    os.popen(""gzip "" + paramFilename).read()\n    paramFilename = paramFilename + "".gz""\n    artifact[\'files\'][\'parameters\'] = {\n            \'uri\': paramFilename[4:],\n            \'sha1Hash\': os.popen(""gsha1sum "" + paramFilename).read().split()[0],\n            \'size\': os.path.getsize(paramFilename)\n            }\n\n    metadata[\'artifacts\'].append(artifact)\n\nwith open(\'out/metadata.json\', \'w\') as f:\n    json.dump(metadata, f, indent=2)\n'"
