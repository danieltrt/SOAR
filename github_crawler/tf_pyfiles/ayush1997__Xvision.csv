file_path,api_count,code
DeepLearning/plot.py,0,"b'import matplotlib.pyplot as plt\nimport numpy\nimport seaborn\ncost=[1.0144926, 0.96602422, 0.024328001, 0.084135853, 0.016160158, 0.025197307, 0.009908339, 0.01657806, 0.0081786923, 0.013121139, 0.0070881136, 0.011028621, 0.0063030426, 0.0096297041, 0.0056963619, 0.0085926503, 0.005211385, 0.0077778683, 0.0048120469, 0.0071275895, 0.0044746906, 0.006589605, 0.004183189, 0.006135819, 0.0039332267, 0.0057431147, 0.0037121987, 0.0054046381, 0.0035161029, 0.0051041786, 0.0033395544, 0.0048419037, 0.0031820005, 0.0046072733, 0.0030369759, 0.0043934993, 0.0029067376, 0.0042033275, 0.0027854857, 0.0040287729]\n\n\n\nx = range(0,100*len(cost),100)\n\n\n\n\n# plotting using matplot lib\nplt.plot(x,cost)\nplt.show()\n\n\n# pred=[ 0,0,1,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,\n#   0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,\n#   0,1,0,0,0,0,1,0,0,0,0,0,0,0]\n#\n# act=[ 1,0,1,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,\n#   0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0,\n#   0,0,1,0,0,0,1,0,0,1,1,0,1,0]\n#\n# normal=0\n# nodule=0\n# nod_nor=0\n# nor_nod=0\n# for i,j in zip(pred,act):\n#     if i==j and j==0:\n#         normal+=1\n#     elif i==j and j==1:\n#         nodule+=1\n#     elif i!=j and j==0:\n#         nor_nod+=1\n#     elif i!=j and j==1:\n#         nod_nor+=1\n#\n# print normal\n# print nodule\n# print nod_nor\n# print nor_nod\n\n\npred_perc = [[ 0.98667473,0.01332524],\n [ 0.37804329,0.62195677],\n [ 0.25628123,0.7437188 ],\n [ 0.30383977,0.6961602 ],\n [ 0.75620246,0.2437976 ],\n [ 0.8936432, 0.10635684],\n [ 0.28952771,0.71047229],\n [ 0.98636836,0.01363169],\n [ 0.88667697,0.11332297],\n [ 0.20244174,0.79755831],\n [ 0.98216808,0.01783191],\n [ 0.90132445,0.09867556],\n [ 0.55609512,0.44390485],\n [ 0.7679649, 0.23203513],\n [ 0.98855376,0.01144631],\n [ 0.11122131,0.88877869],\n [ 0.46338663,0.5366134 ],\n [ 0.90723628,0.09276367],\n [ 0.77705479,0.22294515],\n [ 0.94254065,0.05745932],\n [ 0.69600475,0.30399522],\n [ 0.60293436,0.39706561],\n [ 0.06954173,0.93045825],\n [ 0.00311055,0.99688941],\n [ 0.98544693,0.01455308]]\npredicted =[0,1,1,1,0,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0]\nactual =[0 ,1 ,1 ,1 ,0 ,1 ,1 ,0 ,1 ,1 ,0 ,1 ,1 ,1 ,0 ,0 ,0 ,1 ,0 ,0 ,0 ,0 ,1 ,1, 0]\n#\n# [[ 0.37725854  0.62274146]\n#  [ 0.99791104  0.00208891]\n#  [ 0.70968902  0.29031098]\n#  [ 0.23615305  0.76384693]\n#  [ 0.48953882  0.51046121]\n#  [ 0.67623889  0.32376111]\n#  [ 0.88649976  0.11350021]\n#  [ 0.99060351  0.00939645]\n#  [ 0.98265082  0.01734922]\n#  [ 0.76106918  0.23893082]\n#  [ 0.13883038  0.86116958]\n#  [ 0.99685919  0.00314084]\n#  [ 0.9452222   0.05477774]\n#  [ 0.04830367  0.95169628]\n#  [ 0.02982853  0.97017145]\n#  [ 0.75569326  0.24430674]\n#  [ 0.00201784  0.99798214]\n#  [ 0.89771366  0.10228631]\n#  [ 0.45069715  0.54930282]\n#  [ 0.56799167  0.43200833]\n#  [ 0.31027865  0.68972141]\n#  [ 0.41974404  0.58025599]\n#  [ 0.97264206  0.02735789]\n#  [ 0.93642086  0.0635791 ]\n#  [ 0.74562198  0.25437808]]\n# predicted [1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0]\n# actual [0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0]\n\n# ind = {""0"":""NORMAL"",""1"":""NODULE""}\n# import numpy as np\n# from math import ceil\n# import matplotlib.pyplot as plt\n# # plt.style.use(\'ggplot\')\n#\n# # fig = plt.figure()\n# PLOT_ROW_SIZE=3\n# PLOT_COLUMNS_SIZE=5\n# for i in range(1,16):\n#\n#     im = plt.imread(""final_test_images_calc_nodule_only/""+str(i)+"".png"")\n#\n#     plt.subplot(PLOT_ROW_SIZE,PLOT_COLUMNS_SIZE,i)\n#     plt.xticks([])\n#     plt.yticks([])\n#     plt.xlabel(""NORMAL : ""+str(round(pred_perc[i][0],3))+""| NODULE : ""+str(round(pred_perc[i][1],3))+""\\n TRUE : ""+ ind[str(actual[i])], fontsize=12)\n#     plt.imshow(im)\n#\n#\n#\n#\n# # plt.tick_params(axis=\'both\', which=\'both\', bottom=\'off\', top=\'off\', labelbottom=\'off\', right=\'off\', left=\'off\', labelleft=\'off\')\n# # ax.set_xticklabels([])\n# # fig.subplots_adjust(bottom=0.08,left = 0.05,right=0.97,top=0.93,wspace = 0.5,hspace = 0.5)\n# plt.show()\n'"
DeepLearning/test_model.py,54,"b'import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport pickle\nimport time\nfrom sklearn.metrics import precision_recall_fscore_support\nimport sys\n\nprint sys.argv\ntesting_folder = sys.argv[1]\ntest_labels = sys.argv[3]\n\n\nbatch = 25\nepoch = 20\ntesting_folder_len = len([name for name in os.listdir(os.getcwd()+""/""+testing_folder)])\n\nfilename = test_labels\nfileObject = open(filename,\'r\')\ntest_labels = pickle.load(fileObject)\nprint ""test_labels"",len(test_labels)\n\n# n_input = 200704\nn_input = 25088\n# The number of classes which the ConvNet has to classify into .\nn_classes = 2\n# The number of neurons in the each Hidden Layer .\nn_hidden1 = 512\nn_hidden2 = 512\n\ndef get_vgg_model():\n    # download(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\')\n    with open(""vgg16.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n\n    return {\n        \'graph_def\': graph_def\n    }\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    if img.dtype == np.uint8:\n        img = img / 255.0\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n\n    return (norm_img).astype(np.float32)\ndef deprocess(img):\n    return np.clip(img * 255, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5+127.5).astype(np.uint8)\n\n\nepsilon = 1e-3\ng2 = tf.Graph()\nwith g2.as_default():\n\n    # Tensorflow Graph input .\n    x = tf.placeholder(""float"", [None, n_input])\n    y = tf.placeholder(""float"", [None, n_classes])\n\n    train_label = tf.argmax(y,1)\n    with tf.name_scope(\'layer1\'):\n        W_1 = tf.get_variable(\n                    name=""W1"",\n                    shape=[n_input, n_hidden1],\n                    dtype=tf.float32,\n                    initializer=tf.contrib.layers.xavier_initializer())\n\n        # b_1 = tf.get_variable(\n        #     name=\'b1\',\n        #     shape=[n_hidden1],\n        #     dtype=tf.float32,\n        #     initializer=tf.constant_initializer(0.0))\n\n        z1_BN = tf.matmul(x, W_1)\n        batch_mean1, batch_var1 = tf.nn.moments(z1_BN,[0])\n        scale1 = tf.Variable(tf.ones([n_hidden1]))\n        beta1 = tf.Variable(tf.zeros([n_hidden1]))\n        BN1 = tf.nn.batch_normalization(z1_BN,batch_mean1,batch_var1,beta1,scale1,epsilon)\n\n        # h_1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, W_1),b_1))\n        h_1 = tf.nn.relu(BN1)\n\n    with tf.name_scope(\'layer2\'):\n        W_2 = tf.get_variable(\n                    name=""W2"",\n                    shape=[n_hidden1,n_hidden2],\n                    dtype=tf.float32,\n                    initializer=tf.contrib.layers.xavier_initializer())\n\n        # b_2 = tf.get_variable(\n        #     name=\'b2\',\n        #     shape=[n_hidden2],\n        #     dtype=tf.float32,\n        #     initializer=tf.constant_initializer(0.0))\n\n        z2_BN = tf.matmul(h_1, W_2)\n        batch_mean2, batch_var2 = tf.nn.moments(z2_BN,[0])\n        scale2 = tf.Variable(tf.ones([n_hidden2]))\n        beta2 = tf.Variable(tf.zeros([n_hidden2]))\n        BN2 = tf.nn.batch_normalization(z2_BN,batch_mean2,batch_var2,beta2,scale2,epsilon)\n\n        # h_2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h_1, W_2),b_2))\n        h_2 = tf.nn.relu(BN2)\n\n    with tf.name_scope(\'output\'):\n        W_3 = tf.get_variable(\n                   name=""W3"",\n                   shape=[n_hidden2,n_classes],\n                   dtype=tf.float32,\n                   initializer=tf.contrib.layers.xavier_initializer())\n\n        b_3 = tf.get_variable(\n           name=\'b3\',\n           shape=[n_classes],\n           dtype=tf.float32,\n           initializer=tf.constant_initializer(0.0))\n\n        h_3 = tf.nn.bias_add(tf.matmul(h_2, W_3),b_3)\n\n    # h_3 = tf.nn.softmax(h_3)\n    # h_3 = h_3\n\n    Cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = h_3, labels=  y))\n    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(Cost)\n    # optimizer = tf.train.AdamOptimizer(0.01).minimize(Cost)\n\n    # saver = tf.train.Saver()\n    #Monitor accuracy\n    soft = tf.nn.softmax(h_3)\n    predicted_y = tf.argmax(tf.nn.softmax(h_3), 1)\n    actual_y = tf.argmax(y, 1)\n\n\n    correct_prediction = tf.equal(predicted_y, actual_y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n\n\n    # names = [op.name for op in g2.get_operations()]\n    # print names\n    saver = tf.train.Saver()\n\n\nclass_pred = np.array([])\nclass_actual=np.array([])\n\nr = (testing_folder_len - (testing_folder_len%25))+1\nprint r\n\n\nwith tf.Session(graph=g2) as sess1:\n    # To initialize values with saved data\n    sess1.run(tf.global_variables_initializer())\n    saver.restore(sess1, os.getcwd()+""/""+sys.argv[4]+""/""+""my-model-""+str(epoch-1)+"".ckpt"")\n\n    for j in range(0,r,25):\n        test_img = []\n\n        file_Name = os.getcwd()+""/""+sys.argv[2]+""/""+ str(j)\n        fileObject = open(file_Name,\'r\')\n        # load the object from the file into var b\n        content_features = pickle.load(fileObject)\n        print content_features.shape\n\n        if j==r-1:\n            test_label = test_labels[j:]\n            print ""test_label"",test_label.shape\n        else:\n            test_label = test_labels[j:25+j]\n            print ""test_label"",test_label.shape\n\n        acc,pred,s,actual = sess1.run([accuracy,predicted_y,soft,actual_y], feed_dict={x: content_features,y: test_label})\n        print acc\n        print s\n        print ""predicted"",pred\n        print ""actual"",actual\n\n        # print type(pred)\n        # class_pred+=list(pred)\n        class_pred = np.concatenate((class_pred, pred), axis=0)\n        class_actual = np.concatenate((class_actual, actual), axis=0)\n        # class_actual+=list(actual)\n        print np.unique(class_actual,return_counts=True)\n\nprint class_pred\nprint class_actual\n\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\nconf_matrix = confusion_matrix(class_actual,class_pred)\n\nprint conf_matrix\nprfs = precision_recall_fscore_support(class_actual, class_pred)\nprint ""precision : "",prfs[0] \nprint ""recall : "",prfs[1] \nprint ""fscore : "",prfs[2] \nprint ""support : "",prfs[3] \nplt.matshow(conf_matrix)\nplt.colorbar()\nplt.ylabel(\'True label\')\nplt.xlabel(\'Predicted label\')\n\nplt.show()\n\n\n# python test_model.py <testing images folder> <save test matrix> <testing label pickle> <save model checkpoints>\n'"
DeepLearning/train.py,5,"b'import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom skimage.transform import resize as imresize\nimport pickle\nimport time\nimport sys\n\nprint sys.argv\ntraining_folder = sys.argv[1]\ntesting_folder = sys.argv[2]\nbatch = 20\n\ntraining_folder_len = len([name for name in os.listdir(os.getcwd()+""/""+training_folder)])\ntesting_folder_len = len([name for name in os.listdir(os.getcwd()+""/""+testing_folder)])\n\ndef get_vgg_model():\n    # download(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\')\n    with open(""vgg16.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    # download(\'https://s3.amazonaws.com/cadl/models/synset.txt\')\n    # with open(\'synset.txt\') as f:\n    #     labels = [(idx, l.strip()) for idx, l in enumerate(f.readlines())]\n\n    return {\n        \'graph_def\': graph_def\n        # \'labels\': labels\n        # \'preprocess\': preprocess,\n        # \'deprocess\': deprocess\n    }\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    if img.dtype == np.uint8:\n        img = img / 255.0\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n\n    return (norm_img).astype(np.float32)\ndef deprocess(img):\n    return np.clip(img * 255, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5+127.5).astype(np.uint8)\n\nnet = get_vgg_model()\n\n# labels = net[\'labels\']\n\ng1 = tf.Graph()\n\n\nwith tf.Session(graph=g1) as sess, g1.device(\'/cpu:0\'):\n    tf.import_graph_def(net[\'graph_def\'], name=\'vgg\')\n    # names = [op.name for op in g1.get_operations()]\n# print names\n\n\n\ndef get_content_feature(img_4d):\n    with tf.Session(graph=g1) as sess, g1.device(\'/gpu:0\'):\n\n\n            content_layer = \'vgg/pool5:0\'\n            content_features= g1.get_tensor_by_name(content_layer).eval(\n                    session=sess,\n                    feed_dict={x1: img_4d\n                    })\n\n            # train_new.append(content_features)\n            print content_features.shape\n            return content_features\n\n\nm=0\n# prepare trainig feature set\n# os.mkdir(os.getcwd()+""/""+sys.argv[3])\nos.mkdir(os.getcwd()+""/""+sys.argv[3])\nos.mkdir(os.getcwd()+""/""+sys.argv[4])\n\nr = (training_folder_len - (training_folder_len%batch))+1\nprint r\n\nfor j in range(0,r,20):\n# for j in range(1):\n    img=[]\n    # labels = []\n    start_time = time.time()\n    if j==r-1:\n        m = j+batch-training_folder_len\n        print m\n\n    for i in range(j+0,j+20-m):\n    # for i in range(980,994):\n\n        og = plt.imread(sys.argv[1]+""/""+str(i)+"".png"")\n        og = preprocess(og)\n        img.append(og)\n    print ""j="",j\n\n    x1 = g1.get_tensor_by_name(\'vgg/images\' + \':0\')\n\n    img_4d = np.array(img)\n    # img_4d = img_4d.reshape((1,224,244,3))\n    # img_4d = img[np.newaxis]\n\n    print img_4d.shape , ""Image Shape""\n#\n\n    # content_features = content_features.reshape((content_features.shape[0],7*7*512))\n    content_features = get_content_feature(img_4d).reshape((get_content_feature(img_4d).shape[0],7*7*512))\n    print content_features.shape , ""Feature Map Shape""\n\n\n    # file_Name = ""/home/ayush/Documents/xray/DeepLearning/features-nodule-only/""+str(j)\n    file_Name = os.getcwd()+""/""+sys.argv[3]+""/""+str(j)\n    # open the file for writing\n    fileObject = open(file_Name,\'wb\')\n\n    # this writes the object a to the\n    # file named \'testfile\'\n    pickle.dump(content_features,fileObject)\n                                                                                                                    \n    # here we close the fileObject                          \n    fileObject.close()                                \n                                                            \n    print(""--- %s seconds ---"" % (time.time() - start_time))\n\n\n# prepare test set\nr = (testing_folder_len - (testing_folder_len%25))+1\nprint r\nm=0\n\nfor j in range(0,r,25):\n    test_img = []\n    start_time = time.time()\n    x1 = g1.get_tensor_by_name(\'vgg/images\' + \':0\')\n\n    if j==r-1:\n        m = j+25-testing_folder_len\n        print m\n\n    for i in range(j+0,j+25-m):\n    # for i in range(980,994):\n\n        og = plt.imread(sys.argv[2]+""/""+str(i)+"".png"")\n        og = preprocess(og)\n        test_img.append(og)\n    print ""j="",j\n\n\n\n    img_4d = np.array(test_img)\n    # img_4d = img_4d.reshape((1,224,244,3))\n    # img_4d = img[np.newaxis]\n\n    test_img = get_content_feature(img_4d).reshape((get_content_feature(img_4d).shape[0],7*7*512))\n    print ""test"",test_img.shape\n\n    # print ""new test"",test_img.shape\n\n    # file_Name = ""/home/ayush/Documents/xray/DeepLearning/test-features-nodule-only/""+str(j)\n    file_Name = os.getcwd()+""/""+sys.argv[4]+""/""+str(j)\n    # open the file for writing\n    fileObject = open(file_Name,\'wb\')\n\n    # this writes the object a to the\n    # file named \'testfile\'\n    pickle.dump(test_img,fileObject)\n\n    # here we close the fileObject\n    fileObject.close()\n\n    print(""--- %s seconds ---"" % (time.time() - start_time))\n\n\n# python train.py <training images folder> <testing image folder> <save train matrix> <save test matrix>\n'"
DeepLearning/train_model.py,55,"b'import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport pickle\nimport time\nimport sys\n\nprint sys.argv\ntraining_folder = sys.argv[1]\ntrain_labels = sys.argv[3]\nmode_folder = sys.argv[4]\nbatch = 20\n\ntraining_folder_len = len([name for name in os.listdir(os.getcwd()+""/""+training_folder)])\n\n\nfilename = train_labels\nfileObject = open(filename,\'r\')\ntrain_labels = pickle.load(fileObject)\nprint ""train_labels"",len(train_labels)\n\n\nn_input = 25088\n# The number of classes which the ConvNet has to classify into .\nn_classes = 2\n# The number of neurons in the each Hidden Layer .\nn_hidden1 = 512\nn_hidden2 = 512\n\ndef get_vgg_model():\n    # download(\'https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\')\n    with open(""vgg16.tfmodel"", mode=\'rb\') as f:\n        graph_def = tf.GraphDef()\n        try:\n            graph_def.ParseFromString(f.read())\n        except:\n            print(\'try adding PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python \' +\n                  \'to environment.  e.g.:\\n\' +\n                  \'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python ipython\\n\' +\n                  \'See here for info: \' +\n                  \'https://github.com/tensorflow/tensorflow/issues/582\')\n\n    # download(\'https://s3.amazonaws.com/cadl/models/synset.txt\')\n    # with open(\'synset.txt\') as f:\n    #     labels = [(idx, l.strip()) for idx, l in enumerate(f.readlines())]\n\n    return {\n        \'graph_def\': graph_def\n        # \'labels\': labels\n        # \'preprocess\': preprocess,\n        # \'deprocess\': deprocess\n    }\n\ndef preprocess(img, crop=True, resize=True, dsize=(224, 224)):\n    if img.dtype == np.uint8:\n        img = img / 255.0\n\n    if crop:\n        short_edge = min(img.shape[:2])\n        yy = int((img.shape[0] - short_edge) / 2)\n        xx = int((img.shape[1] - short_edge) / 2)\n        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n    else:\n        crop_img = img\n\n    if resize:\n        norm_img = imresize(crop_img, dsize, preserve_range=True)\n    else:\n        norm_img = crop_img\n\n    return (norm_img).astype(np.float32)\ndef deprocess(img):\n    return np.clip(img * 255, 0, 255).astype(np.uint8)\n    # return ((img / np.max(np.abs(img))) * 127.5+127.5).astype(np.uint8)\n\n\nepsilon = 1e-3\ng2 = tf.Graph()\nwith g2.as_default():\n\n    # Tensorflow Graph input .\n    x = tf.placeholder(""float"", [None, n_input])\n    y = tf.placeholder(""float"", [None, n_classes])\n\n    train_label = tf.argmax(y,1)\n    with tf.name_scope(\'layer1\'):\n        W_1 = tf.get_variable(\n                    name=""W1"",\n                    shape=[n_input, n_hidden1],\n                    dtype=tf.float32,\n                    initializer=tf.contrib.layers.xavier_initializer())\n\n        # b_1 = tf.get_variable(\n        #     name=\'b1\',\n        #     shape=[n_hidden1],\n        #     dtype=tf.float32,\n        #     initializer=tf.constant_initializer(0.0))\n\n        z1_BN = tf.matmul(x, W_1)\n        batch_mean1, batch_var1 = tf.nn.moments(z1_BN,[0])\n        scale1 = tf.Variable(tf.ones([n_hidden1]))\n        beta1 = tf.Variable(tf.zeros([n_hidden1]))\n        BN1 = tf.nn.batch_normalization(z1_BN,batch_mean1,batch_var1,beta1,scale1,epsilon)\n\n        # h_1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, W_1),b_1))\n        h_1 = tf.nn.relu(BN1)\n\n    with tf.name_scope(\'layer2\'):\n        W_2 = tf.get_variable(\n                    name=""W2"",\n                    shape=[n_hidden1,n_hidden2],\n                    dtype=tf.float32,\n                    initializer=tf.contrib.layers.xavier_initializer())\n\n        # b_2 = tf.get_variable(\n        #     name=\'b2\',\n        #     shape=[n_hidden2],\n        #     dtype=tf.float32,\n        #     initializer=tf.constant_initializer(0.0))\n\n        z2_BN = tf.matmul(h_1, W_2)\n        batch_mean2, batch_var2 = tf.nn.moments(z2_BN,[0])\n        scale2 = tf.Variable(tf.ones([n_hidden2]))\n        beta2 = tf.Variable(tf.zeros([n_hidden2]))\n        BN2 = tf.nn.batch_normalization(z2_BN,batch_mean2,batch_var2,beta2,scale2,epsilon)\n\n        # h_2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h_1, W_2),b_2))\n        h_2 = tf.nn.relu(BN2)\n\n    with tf.name_scope(\'output\'):\n        W_3 = tf.get_variable(\n                   name=""W3"",\n                   shape=[n_hidden2,n_classes],\n                   dtype=tf.float32,\n                   initializer=tf.contrib.layers.xavier_initializer())\n\n        b_3 = tf.get_variable(\n           name=\'b3\',\n           shape=[n_classes],\n           dtype=tf.float32,\n           initializer=tf.constant_initializer(0.0))\n\n        h_3 = tf.nn.bias_add(tf.matmul(h_2, W_3),b_3)\n\n    # h_3 = tf.nn.softmax(h_3)\n    # h_3 = h_3\n\n    Cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = h_3, labels = y))\n    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(Cost)\n    # optimizer = tf.train.AdamOptimizer(0.01).minimize(Cost)\n\n    # saver = tf.train.Saver()\n    #Monitor accuracy\n    soft = tf.nn.softmax(h_3)\n    predicted_y = tf.argmax(tf.nn.softmax(h_3), 1)\n    actual_y = tf.argmax(y, 1)\n\n\n    correct_prediction = tf.equal(predicted_y, actual_y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))\n\n\n    # names = [op.name for op in g2.get_operations()]\n    # print names\n    saver = tf.train.Saver(max_to_keep=15)\n\n\nr = (training_folder_len - (training_folder_len%batch))+1\nprint r\n\nwith tf.Session(graph=g2) as sess2:\n# sess =  tf.Session(graph=g2)\n\n    sess2.run(tf.global_variables_initializer())\n    print ""shit""\n    # saver.save(sess2, \'my-model\')\n\n    accuracy_list=[]\n    cost=[]\n    n_epochs = 20\n    for epoch in range(n_epochs):\n\n        start_time = time.time()\n\n        for j in range(0,r,20):\n\n            file_Name =  os.getcwd()+""/""+sys.argv[2]+""/""+ str(j)\n            fileObject = open(file_Name,\'r\')\n            # load the object from the file into var b\n            content_features = pickle.load(fileObject)\n\n            # print type(content_features)\n            content_features = content_features.reshape((content_features.shape[0],7*7*512))\n            # content_features = content_features.reshape((content_features.shape[0],28*28*256))\n            print content_features.shape , ""Feature Map Shape""\n\n            print ""j="",j\n\n            if j==r-1:\n                label = train_labels[j:]\n                print label.shape\n            else:\n                label = train_labels[j+0:j+20]\n                print label.shape\n\n            _,l,w1,cst = sess2.run([optimizer,train_label,W_1,Cost], feed_dict={x: content_features, y:label})\n\n            print l\n            # print str(epoch) + ""-------------------------------------""\n\n\n            if j % 100==0:\n                print "" Epoch=""+str(epoch),""j=""+str(j)\n            #     accuracy_list.append(acc)\n                cost.append(cst)\n\n                print ""COST"",cost\n\n        print(""--- %s seconds ---"" % (time.time() - start_time))\n        j=0\n        path_name = os.getcwd()+""/""+sys.argv[4]+""/""+""my-model-""+str(epoch)+"".ckpt""\n        save_path = saver.save(sess2, path_name)\n        print path_name,""saved""\n        #\n\n# python train_model.py <Training images folder> <Train images codes folder> <Training image labels file> <Folder to save models>\n'"
scraper/scraper.py,0,"b'from lxml import html\nimport requests\nimport re\nimport json\nimport urllib\nimport sys\n\npath = sys.argv[1]\nif path[-1]!=""/"":\n    path+=""/""\n\n\ndomain = \'https://openi.nlm.nih.gov/\'\nurl_list = []\nfor i in range(0,75):\n    url = \'https://openi.nlm.nih.gov/gridquery.php?q=&it=x,xg&sub=x&m=\'+str(1+100*i)+\'&n=\'+str(100+100*i)\n    url_list.append(url)\nregex = re.compile(r""var oi = (.*);"")\nfinal_data = {}\nimg_no = 0\n\n\ndef extract(url):\n    global img_no\n\n    img_no += 1\n    r = requests.get(url)\n    tree = html.fromstring(r.text)\n\n    div = tree.xpath(\'//table[@class=""masterresultstable""]\\\n        //div[@class=""meshtext-wrapper-left""]\')\n\n    if div != []:\n        div = div[0]\n    else:\n        return\n\n    typ = div.xpath(\'.//strong/text()\')[0]\n    items = div.xpath(\'.//li/text()\')\n    img = tree.xpath(\'//img[@id=""theImage""]/@src\')[0]\n\n\n    final_data[img_no] = {}\n    final_data[img_no][\'type\'] = typ\n    final_data[img_no][\'items\'] = items\n    final_data[img_no][\'img\'] = domain + img\n    urllib.urlretrieve(domain+img, path+str(img_no)+"".png"")\n    with open(\'data_new.json\', \'w\') as f:\n        json.dump(final_data, f)\n    print final_data[img_no]\n\n\ndef main():\n    for url in url_list :\n        r = requests.get(url)\n        tree = html.fromstring(r.text)\n\n        script = tree.xpath(\'//script[@language=""javascript""]/text()\')[0]\n\n        json_string = regex.findall(script)[0]\n        json_data = json.loads(json_string)\n\n        next_page_url = tree.xpath(\'//footer/a/@href\')\n\n        print \'extract\'\n        links = [domain + x[\'nodeRef\'] for x in json_data]\n        for link in links:\n            extract(link)\n\nif __name__ == \'__main__\':\n\n    main()\n\n\n#python scraper.py <path to folders>\n'"
