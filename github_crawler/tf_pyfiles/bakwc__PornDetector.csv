file_path,api_count,code
nnpcr.py,41,"b'#!/usr/bin/env python\n\ntry:\n    import cPickle as pickle\n    from urllib2 import urlopen\nexcept ImportError:\n    import pickle\n    from urllib.request import urlopen\n\nimport sys\nimport os\nimport random\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.contrib.losses.python.losses import loss_ops\nimport logging\n\nFILE_SEED = 42\nIMG_SIZE = 128\n\n\ndef loadCache(fname):\n    return pickle.load(open(fname, \'rb\'))\n\n\ndef saveCache(obj, fileName):\n    pickle.dump(obj, open(fileName + \'.tmp\', \'wb\'), -1)\n    os.rename(fileName + \'.tmp\', fileName)\n\n\ndef loadDir(dirName):\n    files = os.listdir(dirName)\n    fnames = []\n    for f in files:\n        if not f.endswith(\'.jpg\'):\n            continue\n        fileName = dirName + \'/\' + f\n        fnames.append(fileName)\n    return fnames\n\n\ndef loadFileLists():\n    random.seed(FILE_SEED)\n\n    positiveFiles = sorted(loadDir(\'2\'))\n    negativeFiles = sorted(loadDir(\'1\'))\n\n    random.shuffle(positiveFiles)\n    random.shuffle(negativeFiles)\n\n    minLen = min(len(positiveFiles), len(negativeFiles))\n\n    p20 = int(0.2 * minLen)\n\n    testPositive = positiveFiles[:p20]\n    testNegative = negativeFiles[:p20]\n    positiveFiles = positiveFiles[p20:]\n    negativeFiles = negativeFiles[p20:]\n\n    trainSamples = [(f, 1) for f in positiveFiles] + [(f, 0) for f in negativeFiles]\n    testSamples = [(f, 1) for f in testPositive] + [(f, 0) for f in testNegative]\n\n    random.shuffle(trainSamples)\n    random.shuffle(testSamples)\n\n    trainX = [e[0] for e in trainSamples]\n    trainY = [e[1] for e in trainSamples]\n    testX = [e[0] for e in testSamples]\n    testY = [e[1] for e in testSamples]\n\n    return trainX, trainY, testX, testY\n\n\ndef loadFeatures(files):\n    data = np.ndarray((len(files), IMG_SIZE * IMG_SIZE * 3))\n    for n, f in enumerate(files):\n        logging.debug(\'loading file #%d\' % n)\n        img = cv2.imread(f)\n        # print(img.shape)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        #cv2.imshow(""orig"", img)\n        h, w, _ = img.shape\n        if w > h:\n            diff = w - h\n            img = img[:, diff / 2: diff / 2 + h]\n        elif w < h:\n            diff = h - w\n            img = img[diff / 2: diff / 2 + w, :]\n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n        data[n] = img.ravel()\n        # cv2.imshow(""res"", img)\n        # cv2.waitKey(0)\n    return data\n\n\ndef denseToOneHot(labels_dense, num_classes):\n    """"""Convert class labels from scalars to one-hot vectors.""""""\n    num_labels = labels_dense.shape[0]\n    index_offset = np.arange(num_labels) * num_classes\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    return labels_one_hot\n\n\ndef loadDataset():\n    try:\n        trainX, trainY, testX, testY = loadCache(\'nncache.bin\')\n    except:\n        trainX, trainY, testX, testY = loadFileLists()\n        trainX = loadFeatures(trainX)\n        testX = loadFeatures(testX)\n        saveCache((trainX, trainY, testX, testY), \'nncache.bin\')\n    trainY = denseToOneHot(np.array(trainY), 2)\n    testY = denseToOneHot(np.array(testY), 2)\n    return trainX, trainY, testX, testY\n\n\nclass Batcher(object):\n\n    def __init__(self, x, y, batchSize):\n        assert len(y) >= batchSize\n        self.__batchSize = batchSize\n        self.__x = x\n        self.__y = y\n        self.shuffle()\n        self.__currentIdx = 0\n        self.__epochNumber = 0\n\n    def shuffle(self):\n        perm = np.arange(len(self.__y))\n        np.random.shuffle(perm)\n        self.__x = self.__x[perm]\n        self.__y = self.__y[perm]\n\n    def nextBatch(self):\n        nextIdx = self.__currentIdx + self.__batchSize\n        if nextIdx > len(self.__y):\n            nextIdx = self.__batchSize\n            self.__currentIdx = 0\n            self.shuffle()\n            self.__epochNumber += 1\n        x = self.__x[self.__currentIdx:nextIdx]\n        y = self.__y[self.__currentIdx:nextIdx]\n        self.__currentIdx = nextIdx\n        return x, y\n\n    def getEpochNumber(self):\n        return self.__epochNumber\n\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\nxavier = tf.contrib.layers.xavier_initializer\n\n\nclass Estimator(object):\n\n    def __init__(self):\n        x = tf.placeholder(tf.float32, shape=[None, IMG_SIZE * IMG_SIZE * 3])\n        y_ = tf.placeholder(tf.float32, shape=[None, 2])\n\n        x_image = tf.reshape(x, [-1, IMG_SIZE, IMG_SIZE, 3])\t\t# 128\n\n        W_conv1 = tf.get_variable(""W_conv1"", shape=[3, 3, 3, 6], initializer=xavier())\n        b_conv1 = tf.get_variable(\'b_conv1\', [1, 1, 1, 6])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n        h_pool1 = max_pool_2x2(h_conv1)\t\t\t\t\t\t\t\t# 64\n\n        W_conv2 = tf.get_variable(""W_conv2"", shape=[3, 3, 6, 6], initializer=xavier())\n        b_conv2 = tf.get_variable(\'b_conv2\', [1, 1, 1, 6])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n        h_pool2 = max_pool_2x2(h_conv2)\t\t\t\t\t\t\t\t# 32\n\n        W_conv3 = tf.get_variable(""W_conv3"", shape=[3, 3, 6, 12], initializer=xavier())\n        b_conv3 = tf.get_variable(\'b_conv3\', [1, 1, 1, 12])\n        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n        h_pool3 = max_pool_2x2(h_conv3)\t\t\t\t\t\t\t\t# 16\n\n        W_conv4 = tf.get_variable(""W_conv4"", shape=[3, 3, 12, 24], initializer=xavier())\n        b_conv4 = tf.get_variable(\'b_conv4\', [1, 1, 1, 24])\n        h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n        h_pool4 = max_pool_2x2(h_conv4)\t\t\t\t\t\t\t\t# 8\n\n        h_pool4_flat = tf.reshape(h_pool4, [-1, 8 * 8 * 24])\n\n        W_fc1 = tf.get_variable(""W_fc1"", shape=[8 * 8 * 24, 1024], initializer=xavier())\n        b_fc1 = tf.get_variable(\'b_fc1\', [1024], initializer=init_ops.zeros_initializer)\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n\n        keep_prob = tf.placeholder(tf.float32)\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n        W_fcO = tf.get_variable(""W_fcO"", shape=[1024, 2], initializer=xavier())\n        b_fcO = tf.get_variable(\'b_fcO\', [2], initializer=init_ops.zeros_initializer)\n\n        logits = tf.matmul(h_fc1_drop, W_fcO) + b_fcO\n        y_conv = tf.nn.softmax(logits)\n\n        cross_entropy = loss_ops.softmax_cross_entropy(logits, y_)\n\n        train_step = tf.train.AdagradOptimizer(0.01).minimize(cross_entropy)\n\n        self.predictions = predictions = tf.argmax(y_conv, 1)\n\n        correct_prediction = tf.equal(predictions, tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        self.x = x\n        self.y_ = y_\n        self.keep_prob = keep_prob\n        self.trainStep = train_step\n        self.accuracy = accuracy\n\n    def train(self, x, y, keepProb=1.0):\n        self.trainStep.run(feed_dict={\n            self.x: x,\n            self.y_: y,\n            self.keep_prob: keepProb,\n        })\n\n    def getAccuracy(self, x, y):\n        return self.accuracy.eval(feed_dict={\n            self.x: x,\n            self.y_: y,\n            self.keep_prob: 1.0,\n        })\n\n    def predict(self, x):\n        return self.predictions.eval(feed_dict={\n            self.x: x,\n            self.keep_prob: 1.0,\n        })\n\n\nclass NNPCR(object):\n\n    def __init__(self):\n        tf.set_random_seed(FILE_SEED)\n        self.__sess = tf.InteractiveSession()\n        self.__est = Estimator()\n\n    def train(self, numIterations=1500):\n        logging.info(\'loading dataset\')\n        trainX, trainY, testX, testY = loadDataset()\n        batcher = Batcher(trainX, trainY, 100)\n\n        self.__sess.run(tf.initialize_all_variables())\n\n        logging.info(\'training\')\n\n        for i in range(numIterations):\n            if i % 50 == 0:\n                en = batcher.getEpochNumber()\n                acc = self.__est.getAccuracy(testX, testY)\n                logging.info(\'epoch %d, iteration %d, accuracy %f\' % (en, i, acc))\n            batch = batcher.nextBatch()\n            self.__est.train(batch[0], batch[1], keepProb=0.5)\n\n    def testAccuracy(self):\n        logging.info(\'loading dataset\')\n        trainX, trainY, testX, testY = loadDataset()\n        return self.__est.getAccuracy(trainX, trainY), self.__est.getAccuracy(testX, testY)\n\n    def saveModel(self, fileName):\n        saver = tf.train.Saver()\n        saver.save(self.__sess, fileName)\n\n    def loadModel(self, fileName):\n        saver = tf.train.Saver()\n        saver.restore(self.__sess, fileName)\n\n    def predict(self, files):\n        features = loadFeatures(files)\n        return self.__est.predict(features)\n\n\ndef printUsage():\n    print(\'Usage: \')\n    print(\'  %s train                              - train model\' % sys.argv[0])\n    print(\'  %s file testImg.jpg                   - check given file\' % sys.argv[0])\n    print(\'  %s url http://sample.com/img.jpg      - check given url\' % sys.argv[0])\n    sys.exit(42)\n\n\nif __name__ == \'__main__\':\n\n    logging.basicConfig(format=u\'[%(asctime)s %(filename)s:%(lineno)d %(levelname)s]  %(message)s\', level=logging.INFO)\n\n    pcr = NNPCR()\n\n    if len(sys.argv) < 2:\n        printUsage()\n    mode = sys.argv[1]\n    if mode == \'train\':\n        pcr.train()\n        pcr.saveModel(\'nnmodel.bin\')\n    elif mode == \'file\':\n        if len(sys.argv) < 3:\n            printUsage()\n        fileName = sys.argv[2]\n        pcr.loadModel(\'nnmodel.bin\')\n        print(pcr.predict([fileName])[0])\n    elif mode == \'url\':\n        if len(sys.argv) < 3:\n            printUsage()\n        url = sys.argv[2]\n        f = open(\'tmp.jpg\', \'wb\')\n        f.write(urlopen(url).read())\n        f.close()\n        pcr.loadModel(\'nnmodel.bin\')\n        print(pcr.predict([\'tmp.jpg\'])[0])\n        os.remove(\'tmp.jpg\')\n    else:\n        printUsage()\n'"
pcr.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nimport zlib\nimport cv2\nimport time\nimport random\nfrom threading import Thread\nfrom sklearn.cluster import MiniBatchKMeans\nfrom scipy.sparse import lil_matrix, csr_matrix\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier\ntry:\n    import cPickle as pickle\n    from urllib2 import urlopen\n    from Queue import Queue\nexcept ImportError:\n    import pickle\n    from urllib.request import urlopen\n    from queue import Queue\n\nFILE_LOAD_THREADS = 1\nFILE_SEED = 24\nCLUSTER_SEED = 24\nCLUSTERS_NUMBER = 1000\nBAYES_ALPHA = 0.1\nADA_BOOST_ESTIMATORS = 110\nVERBOSE = True\nUSE_CACHE = True\n\n_g_removed = False\n\n\nclass PCR:\n\n    def __init__(self):\n        self.__clustersNumber = CLUSTERS_NUMBER\n        self.__queue = Queue()\n        self.__verbose = VERBOSE\n        self.__useCache = USE_CACHE\n\n        for i in range(FILE_LOAD_THREADS):\n            t = Thread(target=self.__worker)\n            t.daemon = True\n            t.start()\n\n        self.__kmeans = MiniBatchKMeans(\n            n_clusters=self.__clustersNumber,\n            random_state=CLUSTER_SEED,\n            verbose=self.__verbose)\n        self.__tfidf = TfidfTransformer()\n        self.__tfidf1 = TfidfTransformer()\n\n        self.__clf = AdaBoostClassifier(MultinomialNB(alpha=BAYES_ALPHA), n_estimators=ADA_BOOST_ESTIMATORS)\n        self.__clf1 = AdaBoostClassifier(MultinomialNB(alpha=BAYES_ALPHA), n_estimators=ADA_BOOST_ESTIMATORS)\n\n    def __worker(self):\n        while True:\n            task = self.__queue.get()\n            func, args = task\n            try:\n                func(args)\n            except Exception as e:\n                print(\'EXCEPTION:\', e)\n            self.__queue.task_done()\n\n    def train(self, positiveFiles, negativeFiles):\n        cachedData = self.__loadCache()\n        if cachedData is None:\n            self.__log(\'loading positives\')\n            positiveSamples = self.__loadSamples(positiveFiles)\n            self.__log(\'loading negatives\')\n            negativeSamples = self.__loadSamples(negativeFiles)\n\n            totalDescriptors = []\n            self.__addDescriptors(totalDescriptors, positiveSamples)\n            self.__addDescriptors(totalDescriptors, negativeSamples)\n\n            self.__kmeans.fit(totalDescriptors)\n            clusters = self.__kmeans.predict(totalDescriptors)\n\n            self.__printDistribution(clusters)\n            self.__saveCache((positiveSamples, negativeSamples, self.__kmeans, clusters))\n        else:\n            self.__log(\'using cache\')\n            positiveSamples, negativeSamples, self.__kmeans, clusters = cachedData\n\n        totalSamplesNumber = len(negativeSamples) + len(positiveSamples)\n        counts = lil_matrix((totalSamplesNumber, self.__clustersNumber))\n        counts1 = lil_matrix((totalSamplesNumber, 256))\n        self.__currentSample = 0\n        self.__currentDescr = 0\n        self.__calculteCounts(positiveSamples, counts, counts1, clusters)\n        self.__calculteCounts(negativeSamples, counts, counts1, clusters)\n        counts = csr_matrix(counts)\n        counts1 = csr_matrix(counts1)\n\n        self.__log(\'training bayes classifier\')\n        tfidf = self.__tfidf.fit_transform(counts)\n        tfidf1 = self.__tfidf1.fit_transform(counts1)\n        classes = [True] * len(positiveSamples) + [False] * len(negativeSamples)\n        self.__clf.fit(tfidf, classes)\n        self.__clf1.fit(tfidf1, classes)\n\n        self.__log(\'training complete\')\n\n    def predict(self, files):\n        self.__log(\'loading files\')\n        samples = self.__loadSamples(files)\n        totalDescriptors = []\n        self.__addDescriptors(totalDescriptors, samples)\n        self.__log(\'predicting classes\')\n        clusters = self.__kmeans.predict(totalDescriptors)\n        counts = lil_matrix((len(samples), self.__clustersNumber))\n        counts1 = lil_matrix((len(samples), 256))\n        self.__currentSample = 0\n        self.__currentDescr = 0\n        self.__calculteCounts(samples, counts, counts1, clusters)\n        counts = csr_matrix(counts)\n        counts1 = csr_matrix(counts1)\n\n        tfidf = self.__tfidf.transform(counts)\n        tfidf1 = self.__tfidf1.transform(counts1)\n\n        self.__log(\'classifying\')\n\n        weights = self.__clf.predict_log_proba(tfidf.toarray())\n        weights1 = self.__clf1.predict_log_proba(tfidf1.toarray())\n        predictions = []\n        for i in range(0, len(weights)):\n            w = weights[i][0] - weights[i][1]\n            w1 = weights1[i][0] - weights1[i][1]\n\n            pred = w < 0\n            pred1 = w1 < 0\n\n            if pred != pred1:\n                pred = w + w1 < 0\n\n            predictions.append(pred)\n\n        self.__log(\'prediction complete\')\n        return predictions\n\n    def saveModel(self, fileName):\n        data = pickle.dumps((self.__clustersNumber, self.__kmeans, self.__tfidf,\n                             self.__tfidf1, self.__clf, self.__clf1), -1)\n        data = zlib.compress(data)\n        open(fileName, \'wb\').write(data)\n\n    def loadModel(self, fileName):\n        data = open(fileName, \'rb\').read()\n        data = zlib.decompress(data)\n        data = pickle.loads(data)\n        self.__clustersNumber, self.__kmeans, self.__tfidf, self.__tfidf1, self.__clf, self.__clf1 = data\n\n    def __log(self, message):\n        if self.__verbose:\n            print(message)\n\n    def __saveCache(self, data):\n        if not self.__useCache:\n            return\n        data = pickle.dumps(data, -1)\n        data = zlib.compress(data)\n        open(\'cache.bin\', \'w\').write(data)\n\n    def __loadCache(self):\n        if not self.__useCache:\n            return None\n        if not os.path.isfile(\'cache.bin\'):\n            return None\n        data = open(\'cache.bin\', \'r\').read()\n        data = zlib.decompress(data)\n        data = pickle.loads(data)\n        return data\n\n    def __calculteCounts(self, samples, counts, counts1, clusters):\n        cn = self.__clustersNumber\n        for s in samples:\n            currentCounts = {}\n            for d in s[0]:\n                currentCounts[clusters[self.__currentDescr]] = currentCounts.get(clusters[self.__currentDescr], 0) + 1\n                self.__currentDescr += 1\n            for clu, cnt in currentCounts.iteritems():\n                counts[self.__currentSample, clu] = cnt\n            for i, histCnt in enumerate(s[1]):\n                counts1[self.__currentSample, i] = histCnt[0]\n            self.__currentSample += 1\n\n    def __printDistribution(self, clusters):\n        if not self.__verbose:\n            return\n        distr = {}\n        for c in clusters:\n            distr[c] = distr.get(c, 0) + 1\n        v = sorted(distr.values(), reverse=True)\n        print(\'distribution:\', v[0:15], \'...\', v[-15:])\n\n    def __addDescriptors(self, totalDescriptors, samples):\n        for sample in samples:\n            for descriptor in sample[0]:\n                totalDescriptors.append(descriptor)\n\n    def __loadSamples(self, files):\n        samples = [[]] * len(files)\n        n = 0\n        for f in files:\n            self.__queue.put((self.__loadSingleSample, (f, samples, n)))\n            n += 1\n        self.__queue.join()\n        if _g_removed:\n            print(\' === REMOVED = TERMINATE\')\n            sys.exit(44)\n        return samples\n\n    def __loadSingleSample(self, args):\n        global _g_removed\n        fileName, samples, sampleNum = args\n        des, hist = self.__getFeatures(fileName)\n        if des is None:\n            print(\'ERROR: failed to load\', fileName)\n            os.remove(fileName)\n            _g_removed = True\n            # sys.exit(44)\n            des = []\n            hist = [[0]] * 256\n        samples[sampleNum] = (des, hist)\n\n    def __getFeatures(self, fileName):\n        fid = \'cache/\' + str(zlib.crc32(fileName))\n        self.__log(\'loading %s\' % fileName)\n        if os.path.isfile(fid):\n            des, hist = pickle.loads(open(fid, \'rb\').read())\n        else:\n            img = cv2.imread(fileName)\n\n            if img.shape[1] > 1000:\n                cf = 1000.0 / img.shape[1]\n                newSize = (int(cf * img.shape[0]), int(cf * img.shape[1]), img.shape[2])\n                img.resize(newSize)\n\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n            s = cv2.SIFT(nfeatures=400)\n\n            d = cv2.DescriptorExtractor_create(""OpponentSIFT"")\n            kp = s.detect(gray, None)\n            kp, des = d.compute(img, kp)\n\n            hist = self.__getColorHist(img)\n\n            #open(fid, \'wb\').write(pickle.dumps((des, hist), -1))\n\n        return des, hist\n\n    def __getColorHist(self, img):\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        dist = cv2.calcHist([hsv], [0], None, [256], [0, 256])\n        return dist\n\n\ndef loadDir(dirName):\n    files = os.listdir(dirName)\n    fnames = []\n    for f in files:\n        if not f.endswith(\'.jpg\'):\n            continue\n        fileName = dirName + \'/\' + f\n        fnames.append(fileName)\n    return fnames\n\n\ndef loadFileLists():\n    random.seed(FILE_SEED)\n\n    positiveFiles = sorted(loadDir(\'2\'))\n    negativeFiles = sorted(loadDir(\'1\'))\n\n    random.shuffle(positiveFiles)\n    random.shuffle(negativeFiles)\n\n    minLen = min(len(positiveFiles), len(negativeFiles))\n\n    p20 = int(0.2 * minLen)\n\n    testFiles = positiveFiles[:p20] + negativeFiles[:p20]\n    positiveFiles = positiveFiles[p20:]\n    negativeFiles = negativeFiles[p20:]\n\n    print(testFiles[0], negativeFiles[0], positiveFiles[0])\n\n    testFiles = loadDir(\'1test\')\n\n    return positiveFiles, negativeFiles, testFiles\n\n\ndef train():\n    positiveFiles, negativeFiles, testFiles = loadFileLists()\n\n    pcr = PCR()\n    pcr.train(positiveFiles, negativeFiles)\n    pcr.saveModel(\'model.bin\')\n\n\ndef predict():\n\n    positiveFiles, negativeFiles, testFiles = loadFileLists()\n    testFiles = testFiles\n\n    pcr = PCR()\n    pcr.loadModel(\'model.bin\')\n    pred = pcr.predict(testFiles)\n    total = 0\n    correct = 0\n\n    for i in xrange(0, len(testFiles)):\n        isCorrect = ((testFiles[i][0] == \'1\' and not pred[i]) or (testFiles[i][0] == \'2\' and pred[i]))\n\n        print(isCorrect, pred[i], testFiles[i])\n        # if not isCorrect:\n        #  print testFiles[i]\n        correct += int(isCorrect)\n\n        total += 1\n    print(\'sum: \\t\', float(correct) / total)\n\n\ndef predictTest():\n    files = [\'test.jpg\']\n    pcr = PCR()\n    pcr.loadModel(\'model.bin\')\n    pred = pcr.predict(files)\n    print(\'\\n\\n ===\', pred[0], \'===\\n\\n\')\n\n\ndef predictUrl(url):\n    f = open(\'test.jpg\', \'wb\')\n    f.write(urlopen(url).read())\n    f.close()\n    time.sleep(0.5)\n    predictTest()\n\n\ndef printUsage():\n    print(\'Usage: \')\n    print(\'  %s train                              - train model\' % sys.argv[0])\n    print(\'  %s url http://sample.com/img.jpg      - check given url\' % sys.argv[0])\n    sys.exit(42)\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        printUsage()\n    mode = sys.argv[1]\n    if mode == \'train\':\n        train()\n        time.sleep(0.5)\n        predict()\n    elif mode == \'url\':\n        if len(sys.argv) < 3:\n            printUsage()\n        url = sys.argv[2]\n        predictUrl(url)\n    else:\n        printUsage()\n'"
