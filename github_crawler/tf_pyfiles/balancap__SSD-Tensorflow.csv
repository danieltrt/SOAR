file_path,api_count,code
caffe_to_tensorflow.py,12,"b'""""""Convert a Caffe model file to TensorFlow checkpoint format.\n\nAssume that the network built is a equivalent (or a sub-) to the Caffe\ndefinition.\n""""""\nimport tensorflow as tf\n\nfrom nets import caffe_scope\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n# =========================================================================== #\n# Main flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'ssd_300_vgg\', \'Name of the model to convert.\')\ntf.app.flags.DEFINE_string(\n    \'num_classes\', 21, \'Number of classes in the dataset.\')\ntf.app.flags.DEFINE_string(\n    \'caffemodel_path\', None,\n    \'The path to the Caffe model file to convert.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\n# =========================================================================== #\n# Main converting routine.\n# =========================================================================== #\ndef main(_):\n    # Caffe scope...\n    caffemodel = caffe_scope.CaffeScope()\n    caffemodel.load(FLAGS.caffemodel_path)\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n    with tf.Graph().as_default():\n        global_step = slim.create_global_step()\n        num_classes = int(FLAGS.num_classes)\n\n        # Select the network.\n        ssd_class = nets_factory.get_network(FLAGS.model_name)\n        ssd_params = ssd_class.default_params._replace(num_classes=num_classes)\n        ssd_net = ssd_class(ssd_params)\n        ssd_shape = ssd_net.params.img_shape\n\n        # Image placeholder and model.\n        shape = (1, ssd_shape[0], ssd_shape[1], 3)\n        img_input = tf.placeholder(shape=shape, dtype=tf.float32)\n        # Create model.\n        with slim.arg_scope(ssd_net.arg_scope_caffe(caffemodel)):\n            ssd_net.net(img_input, is_training=False)\n\n        init_op = tf.global_variables_initializer()\n        with tf.Session() as session:\n            # Run the init operation.\n            session.run(init_op)\n\n            # Save model in checkpoint.\n            saver = tf.train.Saver()\n            ckpt_path = FLAGS.caffemodel_path.replace(\'.caffemodel\', \'.ckpt\')\n            saver.save(session, ckpt_path, write_meta_graph=False)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n'"
eval_ssd_network.py,64,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a SSD model\non a given dataset.""""""\nimport math\nimport sys\nimport six\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport tf_extended as tfe\nimport tf_utils\nfrom tensorflow.python.framework import ops\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\n# =========================================================================== #\n# Some default EVAL parameters\n# =========================================================================== #\n# List of recalls values at which precision is evaluated.\nLIST_RECALLS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85,\n                0.90, 0.95, 0.96, 0.97, 0.98, 0.99]\nDATA_FORMAT = \'NHWC\'\n\n# =========================================================================== #\n# SSD evaluation Flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_float(\n    \'select_threshold\', 0.01, \'Selection threshold.\')\ntf.app.flags.DEFINE_integer(\n    \'select_top_k\', 400, \'Select top-k detected bounding boxes.\')\ntf.app.flags.DEFINE_integer(\n    \'keep_top_k\', 200, \'Keep top-k detected objects.\')\ntf.app.flags.DEFINE_float(\n    \'nms_threshold\', 0.45, \'Non-Maximum Selection threshold.\')\ntf.app.flags.DEFINE_float(\n    \'matching_threshold\', 0.5, \'Matching threshold with groundtruth objects.\')\ntf.app.flags.DEFINE_integer(\n    \'eval_resize\', 4, \'Image resizing: None / CENTRAL_CROP / PAD_AND_RESIZE / WARP_RESIZE.\')\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', None, \'Eval image size.\')\ntf.app.flags.DEFINE_boolean(\n    \'remove_difficult\', True, \'Remove difficult objects from evaluation.\')\n\n# =========================================================================== #\n# Main evaluation flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_integer(\n    \'num_classes\', 21, \'Number of classes to use in the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 1, \'The number of samples in each batch.\')\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', None,\n    \'Max number of batches to evaluate by default use all.\')\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\ntf.app.flags.DEFINE_string(\n    \'eval_dir\', \'/tmp/tfmodel/\', \'Directory where the results are saved to.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 0.1, \'GPU memory fraction to use.\')\ntf.app.flags.DEFINE_boolean(\n    \'wait_for_checkpoints\', False, \'Wait for new checkpoints in the eval loop.\')\n\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n    with tf.Graph().as_default():\n        tf_global_step = slim.get_or_create_global_step()\n\n        # =================================================================== #\n        # Dataset + SSD model + Pre-processing\n        # =================================================================== #\n        dataset = dataset_factory.get_dataset(\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n        # Get the SSD network and its anchors.\n        ssd_class = nets_factory.get_network(FLAGS.model_name)\n        ssd_params = ssd_class.default_params._replace(num_classes=FLAGS.num_classes)\n        ssd_net = ssd_class(ssd_params)\n\n        # Evaluation shape and associated anchors: eval_image_size\n        ssd_shape = ssd_net.params.img_shape\n        ssd_anchors = ssd_net.anchors(ssd_shape)\n\n        # Select the preprocessing function.\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name, is_training=False)\n\n        tf_utils.print_configuration(FLAGS.__flags, ssd_params,\n                                     dataset.data_sources, FLAGS.eval_dir)\n        # =================================================================== #\n        # Create a dataset provider and batches.\n        # =================================================================== #\n        with tf.device(\'/cpu:0\'):\n            with tf.name_scope(FLAGS.dataset_name + \'_data_provider\'):\n                provider = slim.dataset_data_provider.DatasetDataProvider(\n                    dataset,\n                    common_queue_capacity=2 * FLAGS.batch_size,\n                    common_queue_min=FLAGS.batch_size,\n                    shuffle=False)\n            # Get for SSD network: image, labels, bboxes.\n            [image, shape, glabels, gbboxes] = provider.get([\'image\', \'shape\',\n                                                             \'object/label\',\n                                                             \'object/bbox\'])\n            if FLAGS.remove_difficult:\n                [gdifficults] = provider.get([\'object/difficult\'])\n            else:\n                gdifficults = tf.zeros(tf.shape(glabels), dtype=tf.int64)\n\n            # Pre-processing image, labels and bboxes.\n            image, glabels, gbboxes, gbbox_img = \\\n                image_preprocessing_fn(image, glabels, gbboxes,\n                                       out_shape=ssd_shape,\n                                       data_format=DATA_FORMAT,\n                                       resize=FLAGS.eval_resize,\n                                       difficults=None)\n\n            # Encode groundtruth labels and bboxes.\n            gclasses, glocalisations, gscores = \\\n                ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)\n            batch_shape = [1] * 5 + [len(ssd_anchors)] * 3\n\n            # Evaluation batch.\n            r = tf.train.batch(\n                tf_utils.reshape_list([image, glabels, gbboxes, gdifficults, gbbox_img,\n                                       gclasses, glocalisations, gscores]),\n                batch_size=FLAGS.batch_size,\n                num_threads=FLAGS.num_preprocessing_threads,\n                capacity=5 * FLAGS.batch_size,\n                dynamic_pad=True)\n            (b_image, b_glabels, b_gbboxes, b_gdifficults, b_gbbox_img, b_gclasses,\n             b_glocalisations, b_gscores) = tf_utils.reshape_list(r, batch_shape)\n\n        # =================================================================== #\n        # SSD Network + Ouputs decoding.\n        # =================================================================== #\n        dict_metrics = {}\n        arg_scope = ssd_net.arg_scope(data_format=DATA_FORMAT)\n        with slim.arg_scope(arg_scope):\n            predictions, localisations, logits, end_points = \\\n                ssd_net.net(b_image, is_training=False)\n        # Add losses functions.\n        ssd_net.losses(logits, localisations,\n                       b_gclasses, b_glocalisations, b_gscores)\n\n        # Performing post-processing on CPU: loop-intensive, usually more efficient.\n        with tf.device(\'/device:CPU:0\'):\n            # Detected objects from SSD output.\n            localisations = ssd_net.bboxes_decode(localisations, ssd_anchors)\n            rscores, rbboxes = \\\n                ssd_net.detected_bboxes(predictions, localisations,\n                                        select_threshold=FLAGS.select_threshold,\n                                        nms_threshold=FLAGS.nms_threshold,\n                                        clipping_bbox=None,\n                                        top_k=FLAGS.select_top_k,\n                                        keep_top_k=FLAGS.keep_top_k)\n            # Compute TP and FP statistics.\n            num_gbboxes, tp, fp, rscores = \\\n                tfe.bboxes_matching_batch(rscores.keys(), rscores, rbboxes,\n                                          b_glabels, b_gbboxes, b_gdifficults,\n                                          matching_threshold=FLAGS.matching_threshold)\n\n        # Variables to restore: moving avg. or normal weights.\n        if FLAGS.moving_average_decay:\n            variable_averages = tf.train.ExponentialMovingAverage(\n                FLAGS.moving_average_decay, tf_global_step)\n            variables_to_restore = variable_averages.variables_to_restore(\n                slim.get_model_variables())\n            variables_to_restore[tf_global_step.op.name] = tf_global_step\n        else:\n            variables_to_restore = slim.get_variables_to_restore()\n\n        # =================================================================== #\n        # Evaluation metrics.\n        # =================================================================== #\n        with tf.device(\'/device:CPU:0\'):\n            dict_metrics = {}\n            # First add all losses.\n            for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n                dict_metrics[loss.op.name] = slim.metrics.streaming_mean(loss)\n            # Extra losses as well.\n            for loss in tf.get_collection(\'EXTRA_LOSSES\'):\n                dict_metrics[loss.op.name] = slim.metrics.streaming_mean(loss)\n\n            # Add metrics to summaries and Print on screen.\n            for name, metric in dict_metrics.items():\n                # summary_name = \'eval/%s\' % name\n                summary_name = name\n                op = tf.summary.scalar(summary_name, metric[0], collections=[])\n                # op = tf.Print(op, [metric[0]], summary_name)\n                tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n            # FP and TP metrics.\n            tp_fp_metric = tfe.streaming_tp_fp_arrays(num_gbboxes, tp, fp, rscores)\n            for c in tp_fp_metric[0].keys():\n                dict_metrics[\'tp_fp_%s\' % c] = (tp_fp_metric[0][c],\n                                                tp_fp_metric[1][c])\n\n            # Add to summaries precision/recall values.\n            aps_voc07 = {}\n            aps_voc12 = {}\n            for c in tp_fp_metric[0].keys():\n                # Precison and recall values.\n                prec, rec = tfe.precision_recall(*tp_fp_metric[0][c])\n\n                # Average precision VOC07.\n                v = tfe.average_precision_voc07(prec, rec)\n                summary_name = \'AP_VOC07/%s\' % c\n                op = tf.summary.scalar(summary_name, v, collections=[])\n                # op = tf.Print(op, [v], summary_name)\n                tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n                aps_voc07[c] = v\n\n                # Average precision VOC12.\n                v = tfe.average_precision_voc12(prec, rec)\n                summary_name = \'AP_VOC12/%s\' % c\n                op = tf.summary.scalar(summary_name, v, collections=[])\n                # op = tf.Print(op, [v], summary_name)\n                tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n                aps_voc12[c] = v\n\n            # Mean average precision VOC07.\n            summary_name = \'AP_VOC07/mAP\'\n            mAP = tf.add_n(list(aps_voc07.values())) / len(aps_voc07)\n            op = tf.summary.scalar(summary_name, mAP, collections=[])\n            op = tf.Print(op, [mAP], summary_name)\n            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n            # Mean average precision VOC12.\n            summary_name = \'AP_VOC12/mAP\'\n            mAP = tf.add_n(list(aps_voc12.values())) / len(aps_voc12)\n            op = tf.summary.scalar(summary_name, mAP, collections=[])\n            op = tf.Print(op, [mAP], summary_name)\n            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n        # for i, v in enumerate(l_precisions):\n        #     summary_name = \'eval/precision_at_recall_%.2f\' % LIST_RECALLS[i]\n        #     op = tf.summary.scalar(summary_name, v, collections=[])\n        #     op = tf.Print(op, [v], summary_name)\n        #     tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n        # Split into values and updates ops.\n        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map(dict_metrics)\n\n        # =================================================================== #\n        # Evaluation loop.\n        # =================================================================== #\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\n        # config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\n        # Number of batches...\n        if FLAGS.max_num_batches:\n            num_batches = FLAGS.max_num_batches\n        else:\n            num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n        if not FLAGS.wait_for_checkpoints:\n            if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n                checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n            else:\n                checkpoint_path = FLAGS.checkpoint_path\n            tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n            # Standard evaluation loop.\n            start = time.time()\n            slim.evaluation.evaluate_once(\n                master=FLAGS.master,\n                checkpoint_path=checkpoint_path,\n                logdir=FLAGS.eval_dir,\n                num_evals=num_batches,\n                eval_op=list(names_to_updates.values()),\n                variables_to_restore=variables_to_restore,\n                session_config=config)\n            # Log time spent.\n            elapsed = time.time()\n            elapsed = elapsed - start\n            print(\'Time spent : %.3f seconds.\' % elapsed)\n            print(\'Time spent per BATCH: %.3f seconds.\' % (elapsed / num_batches))\n\n        else:\n            checkpoint_path = FLAGS.checkpoint_path\n            tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n            # Waiting loop.\n            slim.evaluation.evaluation_loop(\n                master=FLAGS.master,\n                checkpoint_dir=checkpoint_path,\n                logdir=FLAGS.eval_dir,\n                num_evals=num_batches,\n                eval_op=list(names_to_updates.values()),\n                variables_to_restore=variables_to_restore,\n                eval_interval_secs=60,\n                max_number_of_evaluations=np.inf,\n                session_config=config,\n                timeout=None)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
inspect_checkpoint.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple script for inspect checkpoint files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nFLAGS = None\n\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors):\n    """"""Prints tensors in a checkpoint file.\n\n    If no `tensor_name` is provided, prints the tensor names and shapes\n    in the checkpoint file.\n\n    If `tensor_name` is provided, prints the content of the tensor.\n\n    Args:\n        file_name: Name of the checkpoint file.\n        tensor_name: Name of the tensor in the checkpoint file to print.\n        all_tensors: Boolean indicating whether to print all tensors.\n    """"""\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        if all_tensors:\n            var_to_shape_map = reader.get_variable_to_shape_map()\n            for key in var_to_shape_map:\n                print(""tensor_name: "", key)\n                print(reader.get_tensor(key))\n        elif not tensor_name:\n            print(reader.debug_string().decode(""utf-8""))\n        else:\n            print(""tensor_name: "", tensor_name)\n            print(reader.get_tensor(tensor_name))\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\n\ndef parse_numpy_printoption(kv_str):\n    """"""Sets a single numpy printoption from a string of the form \'x=y\'.\n\n    See documentation on numpy.set_printoptions() for details about what values\n    x and y can take. x can be any option listed there other than \'formatter\'.\n\n    Args:\n        kv_str: A string of the form \'x=y\', such as \'threshold=100000\'\n\n    Raises:\n        argparse.ArgumentTypeError: If the string couldn\'t be used to set any\n                nump printoption.\n    """"""\n    k_v_str = kv_str.split(""="", 1)\n    if len(k_v_str) != 2 or not k_v_str[0]:\n        raise argparse.ArgumentTypeError(""\'%s\' is not in the form k=v."" % kv_str)\n    k, v_str = k_v_str\n    printoptions = np.get_printoptions()\n    if k not in printoptions:\n        raise argparse.ArgumentTypeError(""\'%s\' is not a valid printoption."" % k)\n    v_type = type(printoptions[k])\n    if v_type is type(None):\n        raise argparse.ArgumentTypeError(\n                ""Setting \'%s\' from the command line is not supported."" % k)\n    try:\n        v = (v_type(v_str) if v_type is not bool\n             else flags.BooleanParser().Parse(v_str))\n    except ValueError as e:\n        raise argparse.ArgumentTypeError(e.message)\n    np.set_printoptions(**{k: v})\n\n\ndef main(unused_argv):\n    if not FLAGS.file_name:\n        print(""Usage: inspect_checkpoint --file_name=checkpoint_file_name ""\n              ""[--tensor_name=tensor_to_print]"")\n        sys.exit(1)\n    else:\n        print_tensors_in_checkpoint_file(FLAGS.file_name, FLAGS.tensor_name,\n                                         FLAGS.all_tensors)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n    parser.add_argument(\n            ""--file_name"", type=str, default="""", help=""Checkpoint filename. ""\n                                        ""Note, if using Checkpoint V2 format, file_name is the ""\n                                        ""shared prefix between all files in the checkpoint."")\n    parser.add_argument(\n            ""--tensor_name"",\n            type=str,\n            default="""",\n            help=""Name of the tensor to inspect"")\n    parser.add_argument(\n            ""--all_tensors"",\n            nargs=""?"",\n            const=True,\n            type=""bool"",\n            default=False,\n            help=""If True, print the values of all the tensors."")\n    parser.add_argument(\n            ""--printoptions"",\n            nargs=""*"",\n            type=parse_numpy_printoption,\n            help=""Argument for numpy.set_printoptions(), in the form \'k=v\'."")\n    FLAGS, unparsed = parser.parse_known_args()\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
tf_convert_data.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convert a dataset to TFRecords format, which can be easily integrated into\na TensorFlow pipeline.\n\nUsage:\n```shell\npython tf_convert_data.py \\\n    --dataset_name=pascalvoc \\\n    --dataset_dir=/tmp/pascalvoc \\\n    --output_name=pascalvoc \\\n    --output_dir=/tmp/\n```\n""""""\nimport tensorflow as tf\n\nfrom datasets import pascalvoc_to_tfrecords\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'pascalvoc\',\n    \'The name of the dataset to convert.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None,\n    \'Directory where the original dataset is stored.\')\ntf.app.flags.DEFINE_string(\n    \'output_name\', \'pascalvoc\',\n    \'Basename used for TFRecords output files.\')\ntf.app.flags.DEFINE_string(\n    \'output_dir\', \'./\',\n    \'Output directory where to store TFRecords files.\')\n\n\ndef main(_):\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n    print(\'Dataset directory:\', FLAGS.dataset_dir)\n    print(\'Output directory:\', FLAGS.output_dir)\n\n    if FLAGS.dataset_name == \'pascalvoc\':\n        pascalvoc_to_tfrecords.run(FLAGS.dataset_dir, FLAGS.output_dir, FLAGS.output_name)\n    else:\n        raise ValueError(\'Dataset [%s] was not recognized.\' % FLAGS.dataset_name)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n'"
tf_utils.py,20,"b'# Copyright 2016 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Diverse TensorFlow utils, for training, evaluation and so on!\n""""""\nimport os\nfrom pprint import pprint\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.data import parallel_reader\n\nslim = tf.contrib.slim\n\n\n# =========================================================================== #\n# General tools.\n# =========================================================================== #\ndef reshape_list(l, shape=None):\n    """"""Reshape list of (list): 1D to 2D or the other way around.\n\n    Args:\n      l: List or List of list.\n      shape: 1D or 2D shape.\n    Return\n      Reshaped list.\n    """"""\n    r = []\n    if shape is None:\n        # Flatten everything.\n        for a in l:\n            if isinstance(a, (list, tuple)):\n                r = r + list(a)\n            else:\n                r.append(a)\n    else:\n        # Reshape to list of list.\n        i = 0\n        for s in shape:\n            if s == 1:\n                r.append(l[i])\n            else:\n                r.append(l[i:i+s])\n            i += s\n    return r\n\n\n# =========================================================================== #\n# Training utils.\n# =========================================================================== #\ndef print_configuration(flags, ssd_params, data_sources, save_dir=None):\n    """"""Print the training configuration.\n    """"""\n    def print_config(stream=None):\n        print(\'\\n# =========================================================================== #\', file=stream)\n        print(\'# Training | Evaluation flags:\', file=stream)\n        print(\'# =========================================================================== #\', file=stream)\n        pprint(flags, stream=stream)\n\n        print(\'\\n# =========================================================================== #\', file=stream)\n        print(\'# SSD net parameters:\', file=stream)\n        print(\'# =========================================================================== #\', file=stream)\n        pprint(dict(ssd_params._asdict()), stream=stream)\n\n        print(\'\\n# =========================================================================== #\', file=stream)\n        print(\'# Training | Evaluation dataset files:\', file=stream)\n        print(\'# =========================================================================== #\', file=stream)\n        data_files = parallel_reader.get_data_files(data_sources)\n        pprint(sorted(data_files), stream=stream)\n        print(\'\', file=stream)\n\n    print_config(None)\n    # Save to a text file as well.\n    if save_dir is not None:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        path = os.path.join(save_dir, \'training_config.txt\')\n        with open(path, ""w"") as out:\n            print_config(out)\n\n\ndef configure_learning_rate(flags, num_samples_per_epoch, global_step):\n    """"""Configures the learning rate.\n\n    Args:\n      num_samples_per_epoch: The number of samples in each epoch of training.\n      global_step: The global_step tensor.\n    Returns:\n      A `Tensor` representing the learning rate.\n    """"""\n    decay_steps = int(num_samples_per_epoch / flags.batch_size *\n                      flags.num_epochs_per_decay)\n\n    if flags.learning_rate_decay_type == \'exponential\':\n        return tf.train.exponential_decay(flags.learning_rate,\n                                          global_step,\n                                          decay_steps,\n                                          flags.learning_rate_decay_factor,\n                                          staircase=True,\n                                          name=\'exponential_decay_learning_rate\')\n    elif flags.learning_rate_decay_type == \'fixed\':\n        return tf.constant(flags.learning_rate, name=\'fixed_learning_rate\')\n    elif flags.learning_rate_decay_type == \'polynomial\':\n        return tf.train.polynomial_decay(flags.learning_rate,\n                                         global_step,\n                                         decay_steps,\n                                         flags.end_learning_rate,\n                                         power=1.0,\n                                         cycle=False,\n                                         name=\'polynomial_decay_learning_rate\')\n    else:\n        raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                         flags.learning_rate_decay_type)\n\n\ndef configure_optimizer(flags, learning_rate):\n    """"""Configures the optimizer used for training.\n\n    Args:\n      learning_rate: A scalar or `Tensor` learning rate.\n    Returns:\n      An instance of an optimizer.\n    """"""\n    if flags.optimizer == \'adadelta\':\n        optimizer = tf.train.AdadeltaOptimizer(\n            learning_rate,\n            rho=flags.adadelta_rho,\n            epsilon=flags.opt_epsilon)\n    elif flags.optimizer == \'adagrad\':\n        optimizer = tf.train.AdagradOptimizer(\n            learning_rate,\n            initial_accumulator_value=flags.adagrad_initial_accumulator_value)\n    elif flags.optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate,\n            beta1=flags.adam_beta1,\n            beta2=flags.adam_beta2,\n            epsilon=flags.opt_epsilon)\n    elif flags.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(\n            learning_rate,\n            learning_rate_power=flags.ftrl_learning_rate_power,\n            initial_accumulator_value=flags.ftrl_initial_accumulator_value,\n            l1_regularization_strength=flags.ftrl_l1,\n            l2_regularization_strength=flags.ftrl_l2)\n    elif flags.optimizer == \'momentum\':\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate,\n            momentum=flags.momentum,\n            name=\'Momentum\')\n    elif flags.optimizer == \'rmsprop\':\n        optimizer = tf.train.RMSPropOptimizer(\n            learning_rate,\n            decay=flags.rmsprop_decay,\n            momentum=flags.rmsprop_momentum,\n            epsilon=flags.opt_epsilon)\n    elif flags.optimizer == \'sgd\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Optimizer [%s] was not recognized\', flags.optimizer)\n    return optimizer\n\n\ndef add_variables_summaries(learning_rate):\n    summaries = []\n    for variable in slim.get_model_variables():\n        summaries.append(tf.summary.histogram(variable.op.name, variable))\n    summaries.append(tf.summary.scalar(\'training/Learning Rate\', learning_rate))\n    return summaries\n\n\ndef update_model_scope(var, ckpt_scope, new_scope):\n    return var.op.name.replace(new_scope,\'vgg_16\')\n\n\ndef get_init_fn(flags):\n    """"""Returns a function run by the chief worker to warm-start the training.\n    Note that the init_fn is only run when initializing the model during the very\n    first global step.\n\n    Returns:\n      An init function run by the supervisor.\n    """"""\n    if flags.checkpoint_path is None:\n        return None\n    # Warn the user if a checkpoint exists in the train_dir. Then ignore.\n    if tf.train.latest_checkpoint(flags.train_dir):\n        tf.logging.info(\n            \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n            % flags.train_dir)\n        return None\n\n    exclusions = []\n    if flags.checkpoint_exclude_scopes:\n        exclusions = [scope.strip()\n                      for scope in flags.checkpoint_exclude_scopes.split(\',\')]\n\n    # TODO(sguada) variables.filter_variables()\n    variables_to_restore = []\n    for var in slim.get_model_variables():\n        excluded = False\n        for exclusion in exclusions:\n            if var.op.name.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n    # Change model scope if necessary.\n    if flags.checkpoint_model_scope is not None:\n        variables_to_restore = \\\n            {var.op.name.replace(flags.model_name,\n                                 flags.checkpoint_model_scope): var\n             for var in variables_to_restore}\n\n\n    if tf.gfile.IsDirectory(flags.checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(flags.checkpoint_path)\n    else:\n        checkpoint_path = flags.checkpoint_path\n    tf.logging.info(\'Fine-tuning from %s. Ignoring missing vars: %s\' % (checkpoint_path, flags.ignore_missing_vars))\n\n    return slim.assign_from_checkpoint_fn(\n        checkpoint_path,\n        variables_to_restore,\n        ignore_missing_vars=flags.ignore_missing_vars)\n\n\ndef get_variables_to_train(flags):\n    """"""Returns a list of variables to train.\n\n    Returns:\n      A list of variables to train by the optimizer.\n    """"""\n    if flags.trainable_scopes is None:\n        return tf.trainable_variables()\n    else:\n        scopes = [scope.strip() for scope in flags.trainable_scopes.split(\',\')]\n\n    variables_to_train = []\n    for scope in scopes:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        variables_to_train.extend(variables)\n    return variables_to_train\n\n\n# =========================================================================== #\n# Evaluation utils.\n# =========================================================================== #\n'"
train_ssd_network.py,77,"b'# Copyright 2016 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a SSD model using a given dataset.""""""\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\nimport tf_utils\n\nslim = tf.contrib.slim\n\nDATA_FORMAT = \'NCHW\'\n\n# =========================================================================== #\n# SSD Network flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_float(\n    \'loss_alpha\', 1., \'Alpha parameter in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'negative_ratio\', 3., \'Negative ratio in the loss function.\')\ntf.app.flags.DEFINE_float(\n    \'match_threshold\', 0.5, \'Matching threshold in the loss function.\')\n\n# =========================================================================== #\n# General Flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\ntf.app.flags.DEFINE_float(\n    \'gpu_memory_fraction\', 0.8, \'GPU memory fraction to use.\')\n\n# =========================================================================== #\n# Optimization Flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n# =========================================================================== #\n# Learning Rate Flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 2.0,\n    \'Number of epochs after which learning rate decays.\')\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n# =========================================================================== #\n# Dataset Flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\ntf.app.flags.DEFINE_integer(\n    \'num_classes\', 21, \'Number of classes to use in the dataset.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'ssd_300_vgg\', \'The name of the architecture to train.\')\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\n# =========================================================================== #\n# Fine-Tuning Flags.\n# =========================================================================== #\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_model_scope\', None,\n    \'Model scope in the checkpoint. None if the same as the trained model.\')\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\n# =========================================================================== #\n# Main training routine.\n# =========================================================================== #\ndef main(_):\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n    tf.logging.set_verbosity(tf.logging.DEBUG)\n    with tf.Graph().as_default():\n        # Config model_deploy. Keep TF Slim Models structure.\n        # Useful if want to need multiple GPUs and/or servers in the future.\n        deploy_config = model_deploy.DeploymentConfig(\n            num_clones=FLAGS.num_clones,\n            clone_on_cpu=FLAGS.clone_on_cpu,\n            replica_id=0,\n            num_replicas=1,\n            num_ps_tasks=0)\n        # Create global_step.\n        with tf.device(deploy_config.variables_device()):\n            global_step = slim.create_global_step()\n\n        # Select the dataset.\n        dataset = dataset_factory.get_dataset(\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n        # Get the SSD network and its anchors.\n        ssd_class = nets_factory.get_network(FLAGS.model_name)\n        ssd_params = ssd_class.default_params._replace(num_classes=FLAGS.num_classes)\n        ssd_net = ssd_class(ssd_params)\n        ssd_shape = ssd_net.params.img_shape\n        ssd_anchors = ssd_net.anchors(ssd_shape)\n\n        # Select the preprocessing function.\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name, is_training=True)\n\n        tf_utils.print_configuration(FLAGS.__flags, ssd_params,\n                                     dataset.data_sources, FLAGS.train_dir)\n        # =================================================================== #\n        # Create a dataset provider and batches.\n        # =================================================================== #\n        with tf.device(deploy_config.inputs_device()):\n            with tf.name_scope(FLAGS.dataset_name + \'_data_provider\'):\n                provider = slim.dataset_data_provider.DatasetDataProvider(\n                    dataset,\n                    num_readers=FLAGS.num_readers,\n                    common_queue_capacity=20 * FLAGS.batch_size,\n                    common_queue_min=10 * FLAGS.batch_size,\n                    shuffle=True)\n            # Get for SSD network: image, labels, bboxes.\n            [image, shape, glabels, gbboxes] = provider.get([\'image\', \'shape\',\n                                                             \'object/label\',\n                                                             \'object/bbox\'])\n            # Pre-processing image, labels and bboxes.\n            image, glabels, gbboxes = \\\n                image_preprocessing_fn(image, glabels, gbboxes,\n                                       out_shape=ssd_shape,\n                                       data_format=DATA_FORMAT)\n            # Encode groundtruth labels and bboxes.\n            gclasses, glocalisations, gscores = \\\n                ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)\n            batch_shape = [1] + [len(ssd_anchors)] * 3\n\n            # Training batches and queue.\n            r = tf.train.batch(\n                tf_utils.reshape_list([image, gclasses, glocalisations, gscores]),\n                batch_size=FLAGS.batch_size,\n                num_threads=FLAGS.num_preprocessing_threads,\n                capacity=5 * FLAGS.batch_size)\n            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n                tf_utils.reshape_list(r, batch_shape)\n\n            # Intermediate queueing: unique batch computation pipeline for all\n            # GPUs running the training.\n            batch_queue = slim.prefetch_queue.prefetch_queue(\n                tf_utils.reshape_list([b_image, b_gclasses, b_glocalisations, b_gscores]),\n                capacity=2 * deploy_config.num_clones)\n\n        # =================================================================== #\n        # Define the model running on every GPU.\n        # =================================================================== #\n        def clone_fn(batch_queue):\n            """"""Allows data parallelism by creating multiple\n            clones of network_fn.""""""\n            # Dequeue batch.\n            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n                tf_utils.reshape_list(batch_queue.dequeue(), batch_shape)\n\n            # Construct SSD network.\n            arg_scope = ssd_net.arg_scope(weight_decay=FLAGS.weight_decay,\n                                          data_format=DATA_FORMAT)\n            with slim.arg_scope(arg_scope):\n                predictions, localisations, logits, end_points = \\\n                    ssd_net.net(b_image, is_training=True)\n            # Add loss function.\n            ssd_net.losses(logits, localisations,\n                           b_gclasses, b_glocalisations, b_gscores,\n                           match_threshold=FLAGS.match_threshold,\n                           negative_ratio=FLAGS.negative_ratio,\n                           alpha=FLAGS.loss_alpha,\n                           label_smoothing=FLAGS.label_smoothing)\n            return end_points\n\n        # Gather initial summaries.\n        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        # =================================================================== #\n        # Add summaries from first clone.\n        # =================================================================== #\n        clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n        first_clone_scope = deploy_config.clone_scope(0)\n        # Gather update_ops from the first clone. These contain, for example,\n        # the updates for the batch_norm variables created by network_fn.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n        # Add summaries for end_points.\n        end_points = clones[0].outputs\n        for end_point in end_points:\n            x = end_points[end_point]\n            summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n            summaries.add(tf.summary.scalar(\'sparsity/\' + end_point,\n                                            tf.nn.zero_fraction(x)))\n        # Add summaries for losses and extra losses.\n        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n            summaries.add(tf.summary.scalar(loss.op.name, loss))\n        for loss in tf.get_collection(\'EXTRA_LOSSES\', first_clone_scope):\n            summaries.add(tf.summary.scalar(loss.op.name, loss))\n\n        # Add summaries for variables.\n        for variable in slim.get_model_variables():\n            summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n        # =================================================================== #\n        # Configure the moving averages.\n        # =================================================================== #\n        if FLAGS.moving_average_decay:\n            moving_average_variables = slim.get_model_variables()\n            variable_averages = tf.train.ExponentialMovingAverage(\n                FLAGS.moving_average_decay, global_step)\n        else:\n            moving_average_variables, variable_averages = None, None\n\n        # =================================================================== #\n        # Configure the optimization procedure.\n        # =================================================================== #\n        with tf.device(deploy_config.optimizer_device()):\n            learning_rate = tf_utils.configure_learning_rate(FLAGS,\n                                                             dataset.num_samples,\n                                                             global_step)\n            optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n            summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n\n        if FLAGS.moving_average_decay:\n            # Update ops executed locally by trainer.\n            update_ops.append(variable_averages.apply(moving_average_variables))\n\n        # Variables to train.\n        variables_to_train = tf_utils.get_variables_to_train(FLAGS)\n\n        # and returns a train_tensor and summary_op\n        total_loss, clones_gradients = model_deploy.optimize_clones(\n            clones,\n            optimizer,\n            var_list=variables_to_train)\n        # Add total_loss to summary.\n        summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n        update_op = tf.group(*update_ops)\n        train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                          name=\'train_op\')\n\n        # Add the summaries from the first clone. These contain the summaries\n        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                           first_clone_scope))\n        # Merge all summaries together.\n        summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n        # =================================================================== #\n        # Kicks off the training.\n        # =================================================================== #\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n        config = tf.ConfigProto(log_device_placement=False,\n                                gpu_options=gpu_options)\n        saver = tf.train.Saver(max_to_keep=5,\n                               keep_checkpoint_every_n_hours=1.0,\n                               write_version=2,\n                               pad_step_number=False)\n        slim.learning.train(\n            train_tensor,\n            logdir=FLAGS.train_dir,\n            master=\'\',\n            is_chief=True,\n            init_fn=tf_utils.get_init_fn(FLAGS),\n            summary_op=summary_op,\n            number_of_steps=FLAGS.max_number_of_steps,\n            log_every_n_steps=FLAGS.log_every_n_steps,\n            save_summaries_secs=FLAGS.save_summaries_secs,\n            saver=saver,\n            save_interval_secs=FLAGS.save_interval_secs,\n            session_config=config,\n            sync_optimizer=None)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
datasets/__init__.py,0,b'\n'
datasets/cifar10.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_cifar10_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/dataset_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import imagenet\n\nfrom datasets import pascalvoc_2007\nfrom datasets import pascalvoc_2012\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'imagenet\': imagenet,\n    \'pascalvoc_2007\': pascalvoc_2007,\n    \'pascalvoc_2012\': pascalvoc_2012,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Given a dataset name and a split_name returns a Dataset.\n\n    Args:\n        name: String, the name of the dataset.\n        split_name: A train/test split name.\n        dataset_dir: The directory where the dataset files are stored.\n        file_pattern: The file pattern to use for matching the dataset source files.\n        reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n            reader defined by each dataset is used.\n    Returns:\n        A `Dataset` class.\n    Raises:\n        ValueError: If the dataset `name` is unknown.\n    """"""\n    if name not in datasets_map:\n        raise ValueError(\'Name of dataset unknown %s\' % name)\n    return datasets_map[name].get_split(split_name,\n                                        dataset_dir,\n                                        file_pattern,\n                                        reader)\n'"
datasets/dataset_utils.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(value):\n    """"""Wrapper for inserting int64 features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef float_feature(value):\n    """"""Wrapper for inserting float features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef bytes_feature(value):\n    """"""Wrapper for inserting bytes features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n    return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n    }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    """"""Downloads the `tarball_url` and uncompresses it locally.\n\n    Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n    """"""\n    filename = tarball_url.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n            filename, float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n    """"""Writes a file with the list of class names.\n\n    Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n    """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'w\') as f:\n        for label in labels_to_class_names:\n            class_name = labels_to_class_names[label]\n            f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n    """"""Specifies whether or not the dataset directory contains a label map file.\n\n    Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n    Returns:\n    `True` if the labels file exists and `False` otherwise.\n    """"""\n    return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n    """"""Reads the labels file and returns a mapping from ID to class name.\n\n    Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n    Returns:\n    A map from a label (integer) to class name.\n    """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'rb\') as f:\n        lines = f.read()\n    lines = lines.split(b\'\\n\')\n    lines = filter(None, lines)\n\n    labels_to_class_names = {}\n    for line in lines:\n        index = line.index(b\':\')\n        labels_to_class_names[int(line[:index])] = line[index+1:]\n    return labels_to_class_names\n'"
datasets/imagenet.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n\ndef create_readable_names_for_imagenet_labels():\n    """"""Create a dict mapping label id to human readable string.\n\n    Returns:\n            labels_to_names: dictionary where keys are integers from to 1000\n            and values are human-readable names.\n\n    We retrieve a synset file, which contains a list of valid synset labels used\n    by ILSVRC competition. There is one synset one per line, eg.\n                    #   n01440764\n                    #   n01443537\n    We also retrieve a synset_to_human_file, which contains a mapping from synsets\n    to human-readable names for every synset in Imagenet. These are stored in a\n    tsv format, as follows:\n                    #   n02119247    black fox\n                    #   n02119359    silver fox\n    We assign each synset (in alphabetical order) an integer, starting from 1\n    (since 0 is reserved for the background class).\n\n    Code is based on\n    https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463\n    """"""\n\n    # pylint: disable=g-line-too-long\n    base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/inception/inception/data/\'\n    synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n    synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n    filename, _ = urllib.request.urlretrieve(synset_url)\n    synset_list = [s.strip() for s in open(filename).readlines()]\n    num_synsets_in_ilsvrc = len(synset_list)\n    assert num_synsets_in_ilsvrc == 1000\n\n    filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n    synset_to_human_list = open(filename).readlines()\n    num_synsets_in_all_imagenet = len(synset_to_human_list)\n    assert num_synsets_in_all_imagenet == 21842\n\n    synset_to_human = {}\n    for s in synset_to_human_list:\n        parts = s.strip().split(\'\\t\')\n        assert len(parts) == 2\n        synset = parts[0]\n        human = parts[1]\n        synset_to_human[synset] = human\n\n    label_index = 1\n    labels_to_names = {0: \'background\'}\n    for synset in synset_list:\n        name = synset_to_human[synset]\n        labels_to_names[label_index] = name\n        label_index += 1\n\n    return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n    Args:\n        split_name: A train/test split name.\n        dataset_dir: The base directory of the dataset sources.\n        file_pattern: The file pattern to use when matching the dataset sources.\n            It is assumed that the pattern contains a \'%s\' string so that the split\n            name can be inserted.\n        reader: The TensorFlow reader type.\n\n    Returns:\n        A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n    if split_name not in _SPLITS_TO_SIZES:\n        raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n    if not file_pattern:\n        file_pattern = _FILE_PATTERN\n    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.TFRecordReader\n\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature(\n                (), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature(\n                (), tf.string, default_value=\'jpeg\'),\n        \'image/class/label\': tf.FixedLenFeature(\n                [], dtype=tf.int64, default_value=-1),\n        \'image/class/text\': tf.FixedLenFeature(\n                [], dtype=tf.string, default_value=\'\'),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(\n                dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(\n                dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(\n                dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(\n                dtype=tf.float32),\n        \'image/object/class/label\': tf.VarLenFeature(\n                dtype=tf.int64),\n    }\n\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n        \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n    }\n\n    decoder = slim.tfexample_decoder.TFExampleDecoder(\n            keys_to_features, items_to_handlers)\n\n    labels_to_names = None\n    if dataset_utils.has_labels(dataset_dir):\n        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n    else:\n        labels_to_names = create_readable_names_for_imagenet_labels()\n        dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n    return slim.dataset.Dataset(\n            data_sources=file_pattern,\n            reader=reader,\n            decoder=decoder,\n            num_samples=_SPLITS_TO_SIZES[split_name],\n            items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n            num_classes=_NUM_CLASSES,\n            labels_to_names=labels_to_names)\n'"
datasets/pascalvoc_2007.py,1,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Pascal VOC Dataset (images + annotations).\n""""""\nimport tensorflow as tf\nfrom datasets import pascalvoc_common\n\nslim = tf.contrib.slim\n\nFILE_PATTERN = \'voc_2007_%s_*.tfrecord\'\nITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'shape\': \'Shape of the image\',\n    \'object/bbox\': \'A list of bounding boxes, one per each object.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n# (Images, Objects) statistics on every class.\nTRAIN_STATISTICS = {\n    \'none\': (0, 0),\n    \'aeroplane\': (238, 306),\n    \'bicycle\': (243, 353),\n    \'bird\': (330, 486),\n    \'boat\': (181, 290),\n    \'bottle\': (244, 505),\n    \'bus\': (186, 229),\n    \'car\': (713, 1250),\n    \'cat\': (337, 376),\n    \'chair\': (445, 798),\n    \'cow\': (141, 259),\n    \'diningtable\': (200, 215),\n    \'dog\': (421, 510),\n    \'horse\': (287, 362),\n    \'motorbike\': (245, 339),\n    \'person\': (2008, 4690),\n    \'pottedplant\': (245, 514),\n    \'sheep\': (96, 257),\n    \'sofa\': (229, 248),\n    \'train\': (261, 297),\n    \'tvmonitor\': (256, 324),\n    \'total\': (5011, 12608),\n}\nTEST_STATISTICS = {\n    \'none\': (0, 0),\n    \'aeroplane\': (1, 1),\n    \'bicycle\': (1, 1),\n    \'bird\': (1, 1),\n    \'boat\': (1, 1),\n    \'bottle\': (1, 1),\n    \'bus\': (1, 1),\n    \'car\': (1, 1),\n    \'cat\': (1, 1),\n    \'chair\': (1, 1),\n    \'cow\': (1, 1),\n    \'diningtable\': (1, 1),\n    \'dog\': (1, 1),\n    \'horse\': (1, 1),\n    \'motorbike\': (1, 1),\n    \'person\': (1, 1),\n    \'pottedplant\': (1, 1),\n    \'sheep\': (1, 1),\n    \'sofa\': (1, 1),\n    \'train\': (1, 1),\n    \'tvmonitor\': (1, 1),\n    \'total\': (20, 20),\n}\nSPLITS_TO_SIZES = {\n    \'train\': 5011,\n    \'test\': 4952,\n}\nSPLITS_TO_STATISTICS = {\n    \'train\': TRAIN_STATISTICS,\n    \'test\': TEST_STATISTICS,\n}\nNUM_CLASSES = 20\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n    Args:\n      split_name: A train/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a \'%s\' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n    if not file_pattern:\n        file_pattern = FILE_PATTERN\n    return pascalvoc_common.get_split(split_name, dataset_dir,\n                                      file_pattern, reader,\n                                      SPLITS_TO_SIZES,\n                                      ITEMS_TO_DESCRIPTIONS,\n                                      NUM_CLASSES)\n'"
datasets/pascalvoc_2012.py,1,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Pascal VOC Dataset (images + annotations).\n""""""\nimport tensorflow as tf\nfrom datasets import pascalvoc_common\n\nslim = tf.contrib.slim\n\nFILE_PATTERN = \'voc_2012_%s_*.tfrecord\'\nITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'shape\': \'Shape of the image\',\n    \'object/bbox\': \'A list of bounding boxes, one per each object.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n# (Images, Objects) statistics on every class.\nTRAIN_STATISTICS = {\n    \'none\': (0, 0),\n    \'aeroplane\': (670, 865),\n    \'bicycle\': (552, 711),\n    \'bird\': (765, 1119),\n    \'boat\': (508, 850),\n    \'bottle\': (706, 1259),\n    \'bus\': (421, 593),\n    \'car\': (1161, 2017),\n    \'cat\': (1080, 1217),\n    \'chair\': (1119, 2354),\n    \'cow\': (303, 588),\n    \'diningtable\': (538, 609),\n    \'dog\': (1286, 1515),\n    \'horse\': (482, 710),\n    \'motorbike\': (526, 713),\n    \'person\': (4087, 8566),\n    \'pottedplant\': (527, 973),\n    \'sheep\': (325, 813),\n    \'sofa\': (507, 566),\n    \'train\': (544, 628),\n    \'tvmonitor\': (575, 784),\n    \'total\': (11540, 27450),\n}\nSPLITS_TO_SIZES = {\n    \'train\': 17125,\n}\nSPLITS_TO_STATISTICS = {\n    \'train\': TRAIN_STATISTICS,\n}\nNUM_CLASSES = 20\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n    Args:\n      split_name: A train/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a \'%s\' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n    if not file_pattern:\n        file_pattern = FILE_PATTERN\n    return pascalvoc_common.get_split(split_name, dataset_dir,\n                                      file_pattern, reader,\n                                      SPLITS_TO_SIZES,\n                                      ITEMS_TO_DESCRIPTIONS,\n                                      NUM_CLASSES)\n\n'"
datasets/pascalvoc_common.py,15,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Pascal VOC Dataset (images + annotations).\n""""""\nimport os\n\nimport tensorflow as tf\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\nVOC_LABELS = {\n    \'none\': (0, \'Background\'),\n    \'aeroplane\': (1, \'Vehicle\'),\n    \'bicycle\': (2, \'Vehicle\'),\n    \'bird\': (3, \'Animal\'),\n    \'boat\': (4, \'Vehicle\'),\n    \'bottle\': (5, \'Indoor\'),\n    \'bus\': (6, \'Vehicle\'),\n    \'car\': (7, \'Vehicle\'),\n    \'cat\': (8, \'Animal\'),\n    \'chair\': (9, \'Indoor\'),\n    \'cow\': (10, \'Animal\'),\n    \'diningtable\': (11, \'Indoor\'),\n    \'dog\': (12, \'Animal\'),\n    \'horse\': (13, \'Animal\'),\n    \'motorbike\': (14, \'Vehicle\'),\n    \'person\': (15, \'Person\'),\n    \'pottedplant\': (16, \'Indoor\'),\n    \'sheep\': (17, \'Animal\'),\n    \'sofa\': (18, \'Indoor\'),\n    \'train\': (19, \'Vehicle\'),\n    \'tvmonitor\': (20, \'Indoor\'),\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern, reader,\n              split_to_sizes, items_to_descriptions, num_classes):\n    """"""Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n\n    Args:\n      split_name: A train/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a \'%s\' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n    if split_name not in split_to_sizes:\n        raise ValueError(\'split name %s was not recognized.\' % split_name)\n    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.TFRecordReader\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/difficult\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/truncated\': tf.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n        \'object/difficult\': slim.tfexample_decoder.Tensor(\'image/object/bbox/difficult\'),\n        \'object/truncated\': slim.tfexample_decoder.Tensor(\'image/object/bbox/truncated\'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(\n        keys_to_features, items_to_handlers)\n\n    labels_to_names = None\n    if dataset_utils.has_labels(dataset_dir):\n        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n    # else:\n    #     labels_to_names = create_readable_names_for_imagenet_labels()\n    #     dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n    return slim.dataset.Dataset(\n            data_sources=file_pattern,\n            reader=reader,\n            decoder=decoder,\n            num_samples=split_to_sizes[split_name],\n            items_to_descriptions=items_to_descriptions,\n            num_classes=num_classes,\n            labels_to_names=labels_to_names)\n'"
datasets/pascalvoc_to_tfrecords.py,5,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts Pascal VOC data to TFRecords file format with Example protos.\n\nThe raw Pascal VOC data set is expected to reside in JPEG files located in the\ndirectory \'JPEGImages\'. Similarly, bounding box annotations are supposed to be\nstored in the \'Annotation directory\'\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\nEach validation TFRecord file contains ~500 records. Each training TFREcord\nfile contains ~1000 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n    image/encoded: string containing JPEG encoded image in RGB colorspace\n    image/height: integer, image height in pixels\n    image/width: integer, image width in pixels\n    image/channels: integer, specifying the number of channels, always 3\n    image/format: string, specifying the format, always\'JPEG\'\n\n\n    image/object/bbox/xmin: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/xmax: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/ymin: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/ymax: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/label: list of integer specifying the classification index.\n    image/object/bbox/label_text: list of string descriptions.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n""""""\nimport os\nimport sys\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\nimport xml.etree.ElementTree as ET\n\nfrom datasets.dataset_utils import int64_feature, float_feature, bytes_feature\nfrom datasets.pascalvoc_common import VOC_LABELS\n\n# Original dataset organisation.\nDIRECTORY_ANNOTATIONS = \'Annotations/\'\nDIRECTORY_IMAGES = \'JPEGImages/\'\n\n# TFRecords convertion parameters.\nRANDOM_SEED = 4242\nSAMPLES_PER_FILES = 200\n\n\ndef _process_image(directory, name):\n    """"""Process a image and annotation file.\n\n    Args:\n      filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n      coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    Returns:\n      image_buffer: string, JPEG encoding of RGB image.\n      height: integer, image height in pixels.\n      width: integer, image width in pixels.\n    """"""\n    # Read the image file.\n    filename = directory + DIRECTORY_IMAGES + name + \'.jpg\'\n    image_data = tf.gfile.FastGFile(filename, \'r\').read()\n\n    # Read the XML annotation file.\n    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + \'.xml\')\n    tree = ET.parse(filename)\n    root = tree.getroot()\n\n    # Image shape.\n    size = root.find(\'size\')\n    shape = [int(size.find(\'height\').text),\n             int(size.find(\'width\').text),\n             int(size.find(\'depth\').text)]\n    # Find annotations.\n    bboxes = []\n    labels = []\n    labels_text = []\n    difficult = []\n    truncated = []\n    for obj in root.findall(\'object\'):\n        label = obj.find(\'name\').text\n        labels.append(int(VOC_LABELS[label][0]))\n        labels_text.append(label.encode(\'ascii\'))\n\n        if obj.find(\'difficult\'):\n            difficult.append(int(obj.find(\'difficult\').text))\n        else:\n            difficult.append(0)\n        if obj.find(\'truncated\'):\n            truncated.append(int(obj.find(\'truncated\').text))\n        else:\n            truncated.append(0)\n\n        bbox = obj.find(\'bndbox\')\n        bboxes.append((float(bbox.find(\'ymin\').text) / shape[0],\n                       float(bbox.find(\'xmin\').text) / shape[1],\n                       float(bbox.find(\'ymax\').text) / shape[0],\n                       float(bbox.find(\'xmax\').text) / shape[1]\n                       ))\n    return image_data, shape, bboxes, labels, labels_text, difficult, truncated\n\n\ndef _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n                        difficult, truncated):\n    """"""Build an Example proto for an image example.\n\n    Args:\n      image_data: string, JPEG encoding of RGB image;\n      labels: list of integers, identifier for the ground truth;\n      labels_text: list of strings, human-readable labels;\n      bboxes: list of bounding boxes; each box is a list of integers;\n          specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong\n          to the same label as the image label.\n      shape: 3 integers, image shapes in pixels.\n    Returns:\n      Example proto\n    """"""\n    xmin = []\n    ymin = []\n    xmax = []\n    ymax = []\n    for b in bboxes:\n        assert len(b) == 4\n        # pylint: disable=expression-not-assigned\n        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n        # pylint: enable=expression-not-assigned\n\n    image_format = b\'JPEG\'\n    example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/height\': int64_feature(shape[0]),\n            \'image/width\': int64_feature(shape[1]),\n            \'image/channels\': int64_feature(shape[2]),\n            \'image/shape\': int64_feature(shape),\n            \'image/object/bbox/xmin\': float_feature(xmin),\n            \'image/object/bbox/xmax\': float_feature(xmax),\n            \'image/object/bbox/ymin\': float_feature(ymin),\n            \'image/object/bbox/ymax\': float_feature(ymax),\n            \'image/object/bbox/label\': int64_feature(labels),\n            \'image/object/bbox/label_text\': bytes_feature(labels_text),\n            \'image/object/bbox/difficult\': int64_feature(difficult),\n            \'image/object/bbox/truncated\': int64_feature(truncated),\n            \'image/format\': bytes_feature(image_format),\n            \'image/encoded\': bytes_feature(image_data)}))\n    return example\n\n\ndef _add_to_tfrecord(dataset_dir, name, tfrecord_writer):\n    """"""Loads data from image and annotations files and add them to a TFRecord.\n\n    Args:\n      dataset_dir: Dataset directory;\n      name: Image name to add to the TFRecord;\n      tfrecord_writer: The TFRecord writer to use for writing.\n    """"""\n    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \\\n        _process_image(dataset_dir, name)\n    example = _convert_to_example(image_data, labels, labels_text,\n                                  bboxes, shape, difficult, truncated)\n    tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(output_dir, name, idx):\n    return \'%s/%s_%03d.tfrecord\' % (output_dir, name, idx)\n\n\ndef run(dataset_dir, output_dir, name=\'voc_train\', shuffling=False):\n    """"""Runs the conversion operation.\n\n    Args:\n      dataset_dir: The dataset directory where the dataset is stored.\n      output_dir: Output directory.\n    """"""\n    if not tf.gfile.Exists(dataset_dir):\n        tf.gfile.MakeDirs(dataset_dir)\n\n    # Dataset filenames, and shuffling.\n    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)\n    filenames = sorted(os.listdir(path))\n    if shuffling:\n        random.seed(RANDOM_SEED)\n        random.shuffle(filenames)\n\n    # Process dataset files.\n    i = 0\n    fidx = 0\n    while i < len(filenames):\n        # Open new TFRecord file.\n        tf_filename = _get_output_filename(output_dir, name, fidx)\n        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:\n            j = 0\n            while i < len(filenames) and j < SAMPLES_PER_FILES:\n                sys.stdout.write(\'\\r>> Converting image %d/%d\' % (i+1, len(filenames)))\n                sys.stdout.flush()\n\n                filename = filenames[i]\n                img_name = filename[:-4]\n                _add_to_tfrecord(dataset_dir, img_name, tfrecord_writer)\n                i += 1\n                j += 1\n            fidx += 1\n\n    # Finally, write the labels file:\n    # labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n    # dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n    print(\'\\nFinished converting the Pascal VOC dataset!\')\n'"
deployment/__init__.py,0,b'\n'
deployment/model_deploy.py,53,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n    g = tf.Graph()\n\n    # Set up DeploymentConfig\n    config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n    # Create the global step on the device storing the variables.\n    with tf.device(config.variables_device()):\n        global_step = slim.create_global_step()\n\n    # Define the inputs\n    with tf.device(config.inputs_device()):\n        images, labels = LoadData(...)\n        inputs_queue = slim.data.prefetch_queue((images, labels))\n\n    # Define the optimizer.\n    with tf.device(config.optimizer_device()):\n        optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n    # Define the model including the loss.\n    def model_fn(inputs_queue):\n        images, labels = inputs_queue.dequeue()\n        predictions = CreateNetwork(images)\n        slim.losses.log_loss(predictions, labels)\n\n    model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                   optimizer=optimizer)\n\n    # Run training.\n    slim.learning.train(model_dp.train_op, my_log_dir,\n                                            summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n    * outputs: The return values of the calls to `model_fn()`.\n    * scope: The scope used to create the clone.\n    * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n    * train_op: An operation that run the optimizer training op and include\n        all the update ops created by `model_fn`. Present only if an optimizer\n        was specified.\n    * summary_op: An operation that run the summaries created by `model_fn`\n        and process_gradients.\n    * total_loss: A `Tensor` that contains the sum of all losses created by\n        `model_fn` plus the regularization losses.\n    * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n    * num_clones: Number of model clones to deploy in each replica.\n    * clone_on_cpu: True if clones should be placed on CPU.\n    * replica_id: Integer.  Index of the replica for which the model is\n            deployed.  Usually 0 for the chief replica.\n    * num_replicas: Number of replicas to use.\n    * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n    * worker_job_name: A name for the worker job.\n    * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n    - describe side effect to the graph.\n    - what happens to summaries and update_ops.\n    - which graph collections are altered.\n    - write a tutorial on how to use this.\n    - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n           ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',    # The scope used to create it.\n                                \'device\',   # The device used to create.\n                                ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                        ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'fake_multiple_gpus\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n    """"""Creates multiple clones according to config using a `model_fn`.\n\n    The returned values of `model_fn(*args, **kwargs)` are collected along with\n    the scope and device used to created it in a namedtuple\n    `Clone(outputs, scope, device)`\n\n    Note: it is assumed that any loss created by `model_fn` is collected at\n    the tf.GraphKeys.LOSSES collection.\n\n    To recover the losses, summaries or update_ops created by the clone use:\n    ```python\n        losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n    ```\n\n    The deployment options are specified by the config object and support\n    deploying one or several clones on different GPUs and one or several replicas\n    of such clones.\n\n    The argument `model_fn` is called `config.num_clones` times to create the\n    model clones as `model_fn(*args, **kwargs)`.\n\n    If `config` specifies deployment on multiple replicas then the default\n    tensorflow device is set appropriatly for each call to `model_fn` and for the\n    slim variable creation functions: model and global variables will be created\n    on the `ps` device, the clone operations will be on the `worker` device.\n\n    Args:\n        config: A DeploymentConfig object.\n        model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n        args: Optional list of arguments to pass to `model_fn`.\n        kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n    Returns:\n        A list of namedtuples `Clone`.\n    """"""\n    clones = []\n    args = args or []\n    kwargs = kwargs or {}\n    with slim.arg_scope([slim.model_variable, slim.variable],\n                        device=config.variables_device()):\n        # Create clones.\n        for i in range(0, config.num_clones):\n            with tf.name_scope(config.clone_scope(i)) as clone_scope:\n                clone_device = config.clone_device(i)\n                with tf.device(clone_device):\n                    with tf.variable_scope(tf.get_variable_scope(),\n                                           reuse=True if i > 0 else None):\n                        outputs = model_fn(*args, **kwargs)\n                    clones.append(Clone(outputs, clone_scope, clone_device))\n    return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n    """"""Gather the loss for a single clone.\n\n    Args:\n        clone: A Clone namedtuple.\n        num_clones: The number of clones being deployed.\n        regularization_losses: Possibly empty list of regularization_losses\n            to add to the clone losses.\n\n    Returns:\n        A tensor for the total loss for the clone.  Can be None.\n    """"""\n    # The return value.\n    sum_loss = None\n    # Individual components of the loss that will need summaries.\n    clone_loss = None\n    regularization_loss = None\n    # Compute and aggregate losses on the clone device.\n    with tf.device(clone.device):\n        all_losses = []\n        clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n        if clone_losses:\n            clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n            if num_clones > 1:\n                clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                                    name=\'scaled_clone_loss\')\n            all_losses.append(clone_loss)\n        if regularization_losses:\n            regularization_loss = tf.add_n(regularization_losses,\n                                           name=\'regularization_loss\')\n            all_losses.append(regularization_loss)\n        if all_losses:\n            sum_loss = tf.add_n(all_losses)\n    # Add the summaries out of the clone device block.\n    if clone_loss is not None:\n        tf.summary.scalar(\'clone_loss\', clone_loss)\n        # tf.summary.scalar(clone.scope + \'/clone_loss\', clone_loss)\n    if regularization_loss is not None:\n        tf.summary.scalar(\'regularization_loss\', regularization_loss)\n    return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                                        **kwargs):\n    """"""Compute losses and gradients for a single clone.\n\n    Args:\n        optimizer: A tf.Optimizer  object.\n        clone: A Clone namedtuple.\n        num_clones: The number of clones being deployed.\n        regularization_losses: Possibly empty list of regularization_losses\n            to add to the clone losses.\n        **kwargs: Dict of kwarg to pass to compute_gradients().\n\n    Returns:\n        A tuple (clone_loss, clone_grads_and_vars).\n            - clone_loss: A tensor for the total loss for the clone.  Can be None.\n            - clone_grads_and_vars: List of (gradient, variable) for the clone.\n                Can be empty.\n    """"""\n    sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n    clone_grad = None\n    if sum_loss is not None:\n        with tf.device(clone.device):\n            clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n    return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n    """"""Compute clone losses and gradients for the given list of `Clones`.\n\n    Note: The regularization_losses are added to the first clone losses.\n\n    Args:\n      clones: List of `Clones` created by `create_clones()`.\n      optimizer: An `Optimizer` object.\n      regularization_losses: Optional list of regularization losses. If None it\n         will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n         exclude them.\n      **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n    Returns:\n      A tuple (total_loss, grads_and_vars).\n        - total_loss: A Tensor containing the average of the clone losses\n            including the regularization loss.\n        - grads_and_vars: A List of tuples (gradient, variable) containing the\n            sum of the gradients for each variable.\n\n    """"""\n    grads_and_vars = []\n    clones_losses = []\n    num_clones = len(clones)\n    if regularization_losses is None:\n        regularization_losses = tf.get_collection(\n                tf.GraphKeys.REGULARIZATION_LOSSES)\n    for clone in clones:\n        with tf.name_scope(clone.scope):\n            clone_loss, clone_grad = _optimize_clone(\n                    optimizer, clone, num_clones, regularization_losses, **kwargs)\n            if clone_loss is not None:\n                clones_losses.append(clone_loss)\n                grads_and_vars.append(clone_grad)\n            # Only use regularization_losses for the first clone\n            regularization_losses = None\n    # Compute the total_loss summing all the clones_losses.\n    total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n    # Sum the gradients accross clones.\n    grads_and_vars = _sum_clones_gradients(grads_and_vars)\n    return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n    """"""Deploys a Slim-constructed model across multiple clones.\n\n    The deployment options are specified by the config object and support\n    deploying one or several clones on different GPUs and one or several replicas\n    of such clones.\n\n    The argument `model_fn` is called `config.num_clones` times to create the\n    model clones as `model_fn(*args, **kwargs)`.\n\n    The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n    the deployed model is configured for training with that optimizer.\n\n    If `config` specifies deployment on multiple replicas then the default\n    tensorflow device is set appropriatly for each call to `model_fn` and for the\n    slim variable creation functions: model and global variables will be created\n    on the `ps` device, the clone operations will be on the `worker` device.\n\n    Args:\n      config: A `DeploymentConfig` object.\n      model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n      args: Optional list of arguments to pass to `model_fn`.\n      kwargs: Optional list of keyword arguments to pass to `model_fn`.\n      optimizer: Optional `Optimizer` object.  If passed the model is deployed\n          for training with that optimizer.\n      summarize_gradients: Whether or not add summaries to the gradients.\n\n    Returns:\n      A `DeployedModel` namedtuple.\n\n    """"""\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    # Create Clones.\n    clones = create_clones(config, model_fn, args, kwargs)\n    first_clone = clones[0]\n\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by model_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n    train_op = None\n    total_loss = None\n    with tf.device(config.optimizer_device()):\n        if optimizer:\n            # Place the global step on the device storing the variables.\n            with tf.device(config.variables_device()):\n                global_step = slim.get_or_create_global_step()\n\n            # Compute the gradients for the clones.\n            total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n            if clones_gradients:\n                if summarize_gradients:\n                    # Add summaries to the gradients.\n                    summaries |= set(_add_gradients_summaries(clones_gradients))\n\n                # Create gradient updates.\n                grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                         global_step=global_step)\n                update_ops.append(grad_updates)\n\n                update_op = tf.group(*update_ops)\n                train_op = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                              name=\'train_op\')\n        else:\n            clones_losses = []\n            regularization_losses = tf.get_collection(\n                    tf.GraphKeys.REGULARIZATION_LOSSES)\n            for clone in clones:\n                with tf.name_scope(clone.scope):\n                    clone_loss = _gather_clone_loss(clone, len(clones),\n                                                    regularization_losses)\n                    if clone_loss is not None:\n                        clones_losses.append(clone_loss)\n                    # Only use regularization_losses for the first clone\n                    regularization_losses = None\n            if clones_losses:\n                total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n        # Add the summaries from the first clone. These contain the summaries\n        # created by model_fn and either optimize_clones() or _gather_clone_loss().\n        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                           first_clone.scope))\n\n        if total_loss is not None:\n            # Add total_loss to summary.\n            summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n        if summaries:\n            # Merge all summaries together.\n            summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n        else:\n            summary_op = None\n\n    return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n    """"""Calculate the sum gradient for each shared variable across all clones.\n\n    This function assumes that the clone_grads has been scaled appropriately by\n    1 / num_clones.\n\n    Args:\n      clone_grads: A List of List of tuples (gradient, variable), one list per\n        `Clone`.\n\n    Returns:\n      List of tuples of (gradient, variable) where the gradient has been summed\n        across all clones.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*clone_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n        grads = []\n        var = grad_and_vars[0][1]\n        for g, v in grad_and_vars:\n            assert v == var\n            if g is not None:\n                grads.append(g)\n        if grads:\n            if len(grads) > 1:\n                sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n            else:\n                sum_grad = grads[0]\n            sum_grads.append((sum_grad, var))\n    return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n    """"""Add histogram summaries to gradients.\n\n    Note: The summaries are also added to the SUMMARIES collection.\n\n    Args:\n        grads_and_vars: A list of gradient to variable pairs (tuples).\n\n    Returns:\n        The _list_ of the added summaries for grads_and_vars.\n    """"""\n    summaries = []\n    for grad, var in grads_and_vars:\n        if grad is not None:\n            if isinstance(grad, tf.IndexedSlices):\n                grad_values = grad.values\n            else:\n                grad_values = grad\n            summaries.append(tf.histogram_summary(var.op.name + \':gradient\',\n                                                  grad_values))\n            summaries.append(tf.histogram_summary(var.op.name + \':gradient_norm\',\n                                                  tf.global_norm([grad_values])))\n        else:\n            tf.logging.info(\'Var %s has no gradient\', var.op.name)\n    return summaries\n\n\nclass DeploymentConfig(object):\n    """"""Configuration for deploying a model with `deploy()`.\n\n    You can pass an instance of this class to `deploy()` to specify exactly\n    how to deploy the model to build.  If you do not pass one, an instance built\n    from the default deployment_hparams will be used.\n    """"""\n\n    def __init__(self,\n                 num_clones=1,\n                 clone_on_cpu=False,\n                 fake_multiple_gpus=False,\n                 replica_id=0,\n                 num_replicas=1,\n                 num_ps_tasks=0,\n                 worker_job_name=\'worker\',\n                 ps_job_name=\'ps\'):\n        """"""Create a DeploymentConfig.\n\n        The config describes how to deploy a model across multiple clones and\n        replicas.  The model will be replicated `num_clones` times in each replica.\n        If `clone_on_cpu` is True, each clone will placed on CPU.\n\n        If `fake_multiple_gpus` is True, the model will only be replicated once on\n        a single GPU. This trick enables larger batch sizes, necessary for training\n        deep networks such as InceptionV3/V4, on a single GPU.\n\n        If `num_replicas` is 1, the model is deployed via a single process.  In that\n        case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n        If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n        must specify TensorFlow devices for the `worker` and `ps` jobs and\n        `num_ps_tasks` must be positive.\n\n        Args:\n          num_clones: Number of model clones to deploy in each replica.\n          clone_on_cpu: If True clones would be placed on CPU.\n          replica_id: Integer.  Index of the replica for which the model is\n              deployed.  Usually 0 for the chief replica.\n          num_replicas: Number of replicas to use.\n          num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n          worker_job_name: A name for the worker job.\n          ps_job_name: A name for the parameter server job.\n\n        Raises:\n            ValueError: If the arguments are invalid.\n        """"""\n        if num_replicas > 1:\n            if num_ps_tasks < 1:\n                raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n        if num_replicas > 1 or num_ps_tasks > 0:\n            if not worker_job_name:\n                raise ValueError(\'Must specify worker_job_name when using replicas\')\n            if not ps_job_name:\n                raise ValueError(\'Must specify ps_job_name when using parameter server\')\n        if replica_id >= num_replicas:\n            raise ValueError(\'replica_id must be less than num_replicas\')\n        self._num_clones = num_clones\n        self._clone_on_cpu = clone_on_cpu\n        self._fake_multiple_gpus = fake_multiple_gpus\n        self._replica_id = replica_id\n        self._num_replicas = num_replicas\n        self._num_ps_tasks = num_ps_tasks\n        self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n        self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n    @property\n    def num_clones(self):\n        return self._num_clones\n\n    @property\n    def clone_on_cpu(self):\n        return self._clone_on_cpu\n\n    @property\n    def fake_multiple_gpus(self):\n        return self._fake_multiple_gpus\n\n    @property\n    def replica_id(self):\n        return self._replica_id\n\n    @property\n    def num_replicas(self):\n        return self._num_replicas\n\n    @property\n    def num_ps_tasks(self):\n        return self._num_ps_tasks\n\n    @property\n    def ps_device(self):\n        return self._ps_device\n\n    @property\n    def worker_device(self):\n        return self._worker_device\n\n    def caching_device(self):\n        """"""Returns the device to use for caching variables.\n\n        Variables are cached on the worker CPU when using replicas.\n\n        Returns:\n            A device string or None if the variables do not need to be cached.\n        """"""\n        if self._num_ps_tasks > 0:\n            return lambda op: op.device\n        else:\n            return None\n\n    def clone_device(self, clone_index):\n        """"""Device used to create the clone and all the ops inside the clone.\n\n        Args:\n            clone_index: Int, representing the clone_index.\n\n        Returns:\n            A value suitable for `tf.device()`.\n\n        Raises:\n            ValueError: if `clone_index` is greater or equal to the number of clones"".\n        """"""\n        if clone_index >= self._num_clones:\n            raise ValueError(\'clone_index must be less than num_clones\')\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._worker_device\n        if self._clone_on_cpu:\n            device += \'/device:CPU:0\'\n        else:\n            if self._num_clones > 1 and not self._fake_multiple_gpus:\n                device += \'/device:GPU:%d\' % clone_index\n        return device\n\n    def clone_scope(self, clone_index):\n        """"""Name scope to create the clone.\n\n        Args:\n            clone_index: Int, representing the clone_index.\n\n        Returns:\n            A name_scope suitable for `tf.name_scope()`.\n\n        Raises:\n            ValueError: if `clone_index` is greater or equal to the number of clones"".\n        """"""\n        if clone_index >= self._num_clones:\n            raise ValueError(\'clone_index must be less than num_clones\')\n        scope = \'\'\n        if self._num_clones > 1:\n            scope = \'clone_%d\' % clone_index\n        return scope\n\n    def optimizer_device(self):\n        """"""Device to use with the optimizer.\n\n        Returns:\n            A value suitable for `tf.device()`.\n        """"""\n        if self._num_ps_tasks > 0 or self._num_clones > 0:\n            return self._worker_device + \'/device:CPU:0\'\n        else:\n            return \'\'\n\n    def inputs_device(self):\n        """"""Device to use to build the inputs.\n\n        Returns:\n            A value suitable for `tf.device()`.\n        """"""\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._worker_device\n        device += \'/device:CPU:0\'\n        return device\n\n    def variables_device(self):\n        """"""Returns the device to use for variables created inside the clone.\n\n        Returns:\n            A value suitable for `tf.device()`.\n        """"""\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._ps_device\n        device += \'/device:CPU:0\'\n\n        class _PSDeviceChooser(object):\n            """"""Slim device chooser for variables when using PS.""""""\n\n            def __init__(self, device, tasks):\n                self._device = device\n                self._tasks = tasks\n                self._task = 0\n\n            def choose(self, op):\n                if op.device:\n                    return op.device\n                node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n                if node_def.op == \'Variable\':\n                    t = self._task\n                    self._task = (self._task + 1) % self._tasks\n                    d = \'%s/task:%d\' % (self._device, t)\n                    return d\n                else:\n                    return op.device\n\n        if not self._num_ps_tasks:\n            return device\n        else:\n            chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n            return chooser.choose\n'"
nets/__init__.py,0,b'\n'
nets/caffe_scope.py,4,"b'""""""Specific Caffe scope used to import weights from a .caffemodel file.\n\nThe idea is to create special initializers loading weights from protobuf\n.caffemodel files.\n""""""\nimport caffe\nfrom caffe.proto import caffe_pb2\n\nimport numpy as np\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass CaffeScope(object):\n    """"""Caffe scope.\n    """"""\n    def __init__(self):\n        """"""Initialize the caffee scope.\n        """"""\n        self.counters = {}\n        self.layers = {}\n        self.caffe_layers = None\n        self.bgr_to_rgb = 0\n\n    def load(self, filename, bgr_to_rgb=True):\n        """"""Load weights from a .caffemodel file and initialize counters.\n\n        Params:\n          filename: caffemodel file.\n        """"""\n        print(\'Loading Caffe file:\', filename)\n        caffemodel_params = caffe_pb2.NetParameter()\n        caffemodel_str = open(filename, \'rb\').read()\n        caffemodel_params.ParseFromString(caffemodel_str)\n        self.caffe_layers = caffemodel_params.layer\n\n        # Layers collection.\n        self.layers[\'convolution\'] = [i for i, l in enumerate(self.caffe_layers)\n                                      if l.type == \'Convolution\']\n        self.layers[\'l2_normalization\'] = [i for i, l in enumerate(self.caffe_layers)\n                                           if l.type == \'Normalize\']\n        # BGR to RGB convertion. Tries to find the first convolution with 3\n        # and exchange parameters.\n        if bgr_to_rgb:\n            self.bgr_to_rgb = 1\n\n    def conv_weights_init(self):\n        def _initializer(shape, dtype, partition_info=None):\n            counter = self.counters.get(self.conv_weights_init, 0)\n            idx = self.layers[\'convolution\'][counter]\n            layer = self.caffe_layers[idx]\n            # Weights: reshape and transpose dimensions.\n            w = np.array(layer.blobs[0].data)\n            w = np.reshape(w, layer.blobs[0].shape.dim)\n            # w = np.transpose(w, (1, 0, 2, 3))\n            w = np.transpose(w, (2, 3, 1, 0))\n            if self.bgr_to_rgb == 1 and w.shape[2] == 3:\n                print(\'Convert BGR to RGB in convolution layer:\', layer.name)\n                w[:, :, (0, 1, 2)] = w[:, :, (2, 1, 0)]\n                self.bgr_to_rgb += 1\n            self.counters[self.conv_weights_init] = counter + 1\n            print(\'Load weights from convolution layer:\', layer.name, w.shape)\n            return tf.cast(w, dtype)\n        return _initializer\n\n    def conv_biases_init(self):\n        def _initializer(shape, dtype, partition_info=None):\n            counter = self.counters.get(self.conv_biases_init, 0)\n            idx = self.layers[\'convolution\'][counter]\n            layer = self.caffe_layers[idx]\n            # Biases data...\n            b = np.array(layer.blobs[1].data)\n            self.counters[self.conv_biases_init] = counter + 1\n            print(\'Load biases from convolution layer:\', layer.name, b.shape)\n            return tf.cast(b, dtype)\n        return _initializer\n\n    def l2_norm_scale_init(self):\n        def _initializer(shape, dtype, partition_info=None):\n            counter = self.counters.get(self.l2_norm_scale_init, 0)\n            idx = self.layers[\'l2_normalization\'][counter]\n            layer = self.caffe_layers[idx]\n            # Scaling parameter.\n            s = np.array(layer.blobs[0].data)\n            s = np.reshape(s, layer.blobs[0].shape.dim)\n            self.counters[self.l2_norm_scale_init] = counter + 1\n            print(\'Load scaling from L2 normalization layer:\', layer.name, s.shape)\n            return tf.cast(s, dtype)\n        return _initializer\n'"
nets/custom_layers.py,19,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implement some custom layers, not provided by TensorFlow.\n\nTrying to follow as much as possible the style/standards used in\ntf.contrib.layers\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.contrib.framework.python.ops import variables\nfrom tensorflow.contrib.layers.python.layers import utils\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import variable_scope\n\n\ndef abs_smooth(x):\n    """"""Smoothed absolute function. Useful to compute an L1 smooth error.\n\n    Define as:\n        x^2 / 2         if abs(x) < 1\n        abs(x) - 0.5    if abs(x) > 1\n    We use here a differentiable definition using min(x) and abs(x). Clearly\n    not optimal, but good enough for our purpose!\n    """"""\n    absx = tf.abs(x)\n    minx = tf.minimum(absx, 1)\n    r = 0.5 * ((absx - 1) * minx + absx)\n    return r\n\n\n@add_arg_scope\ndef l2_normalization(\n        inputs,\n        scaling=False,\n        scale_initializer=init_ops.ones_initializer(),\n        reuse=None,\n        variables_collections=None,\n        outputs_collections=None,\n        data_format=\'NHWC\',\n        trainable=True,\n        scope=None):\n    """"""Implement L2 normalization on every feature (i.e. spatial normalization).\n\n    Should be extended in some near future to other dimensions, providing a more\n    flexible normalization framework.\n\n    Args:\n      inputs: a 4-D tensor with dimensions [batch_size, height, width, channels].\n      scaling: whether or not to add a post scaling operation along the dimensions\n        which have been normalized.\n      scale_initializer: An initializer for the weights.\n      reuse: whether or not the layer and its variables should be reused. To be\n        able to reuse the layer scope must be given.\n      variables_collections: optional list of collections for all the variables or\n        a dictionary containing a different list of collection per variable.\n      outputs_collections: collection to add the outputs.\n      data_format:  NHWC or NCHW data format.\n      trainable: If `True` also add variables to the graph collection\n        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n      scope: Optional scope for `variable_scope`.\n    Returns:\n      A `Tensor` representing the output of the operation.\n    """"""\n\n    with variable_scope.variable_scope(\n            scope, \'L2Normalization\', [inputs], reuse=reuse) as sc:\n        inputs_shape = inputs.get_shape()\n        inputs_rank = inputs_shape.ndims\n        dtype = inputs.dtype.base_dtype\n        if data_format == \'NHWC\':\n            # norm_dim = tf.range(1, inputs_rank-1)\n            norm_dim = tf.range(inputs_rank-1, inputs_rank)\n            params_shape = inputs_shape[-1:]\n        elif data_format == \'NCHW\':\n            # norm_dim = tf.range(2, inputs_rank)\n            norm_dim = tf.range(1, 2)\n            params_shape = (inputs_shape[1])\n\n        # Normalize along spatial dimensions.\n        outputs = nn.l2_normalize(inputs, norm_dim, epsilon=1e-12)\n        # Additional scaling.\n        if scaling:\n            scale_collections = utils.get_variable_collections(\n                variables_collections, \'scale\')\n            scale = variables.model_variable(\'gamma\',\n                                             shape=params_shape,\n                                             dtype=dtype,\n                                             initializer=scale_initializer,\n                                             collections=scale_collections,\n                                             trainable=trainable)\n            if data_format == \'NHWC\':\n                outputs = tf.multiply(outputs, scale)\n            elif data_format == \'NCHW\':\n                scale = tf.expand_dims(scale, axis=-1)\n                scale = tf.expand_dims(scale, axis=-1)\n                outputs = tf.multiply(outputs, scale)\n                # outputs = tf.transpose(outputs, perm=(0, 2, 3, 1))\n\n        return utils.collect_named_outputs(outputs_collections,\n                                           sc.original_name_scope, outputs)\n\n\n@add_arg_scope\ndef pad2d(inputs,\n          pad=(0, 0),\n          mode=\'CONSTANT\',\n          data_format=\'NHWC\',\n          trainable=True,\n          scope=None):\n    """"""2D Padding layer, adding a symmetric padding to H and W dimensions.\n\n    Aims to mimic padding in Caffe and MXNet, helping the port of models to\n    TensorFlow. Tries to follow the naming convention of `tf.contrib.layers`.\n\n    Args:\n      inputs: 4D input Tensor;\n      pad: 2-Tuple with padding values for H and W dimensions;\n      mode: Padding mode. C.f. `tf.pad`\n      data_format:  NHWC or NCHW data format.\n    """"""\n    with tf.name_scope(scope, \'pad2d\', [inputs]):\n        # Padding shape.\n        if data_format == \'NHWC\':\n            paddings = [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]]\n        elif data_format == \'NCHW\':\n            paddings = [[0, 0], [0, 0], [pad[0], pad[0]], [pad[1], pad[1]]]\n        net = tf.pad(inputs, paddings, mode=mode)\n        return net\n\n\n@add_arg_scope\ndef channel_to_last(inputs,\n                    data_format=\'NHWC\',\n                    scope=None):\n    """"""Move the channel axis to the last dimension. Allows to\n    provide a single output format whatever the input data format.\n\n    Args:\n      inputs: Input Tensor;\n      data_format: NHWC or NCHW.\n    Return:\n      Input in NHWC format.\n    """"""\n    with tf.name_scope(scope, \'channel_to_last\', [inputs]):\n        if data_format == \'NHWC\':\n            net = inputs\n        elif data_format == \'NCHW\':\n            net = tf.transpose(inputs, perm=(0, 2, 3, 1))\n        return net\n'"
nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings inception_v1, inception_v2 and inception_v3 under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\n# from nets.inception_v1 import inception_v1\n# from nets.inception_v1 import inception_v1_arg_scope\n# from nets.inception_v1 import inception_v1_base\n# from nets.inception_v2 import inception_v2\n# from nets.inception_v2 import inception_v2_arg_scope\n# from nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\n# pylint: enable=unused-import\n'"
nets/inception_resnet_v2.py,39,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(3, [tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(3, [tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n\n        # 149 x 149 x 32\n        net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                          scope=\'Conv2d_1a_3x3\')\n        end_points[\'Conv2d_1a_3x3\'] = net\n        # 147 x 147 x 32\n        net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                          scope=\'Conv2d_2a_3x3\')\n        end_points[\'Conv2d_2a_3x3\'] = net\n        # 147 x 147 x 64\n        net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n        end_points[\'Conv2d_2b_3x3\'] = net\n        # 73 x 73 x 64\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_3a_3x3\')\n        end_points[\'MaxPool_3a_3x3\'] = net\n        # 73 x 73 x 80\n        net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                          scope=\'Conv2d_3b_1x1\')\n        end_points[\'Conv2d_3b_1x1\'] = net\n        # 71 x 71 x 192\n        net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                          scope=\'Conv2d_4a_3x3\')\n        end_points[\'Conv2d_4a_3x3\'] = net\n        # 35 x 35 x 192\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_5a_3x3\')\n        end_points[\'MaxPool_5a_3x3\'] = net\n\n        # 35 x 35 x 320\n        with tf.variable_scope(\'Mixed_5b\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                        scope=\'Conv2d_0b_5x5\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                        scope=\'Conv2d_0c_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                         scope=\'AvgPool_0a_3x3\')\n            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                       scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [tower_conv, tower_conv1_1,\n                              tower_conv2_2, tower_pool_1])\n\n        end_points[\'Mixed_5b\'] = net\n        net = slim.repeat(net, 10, block35, scale=0.17)\n\n        # 17 x 17 x 1024\n        with tf.variable_scope(\'Mixed_6a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                        stride=2, padding=\'VALID\',\n                                        scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(3, [tower_conv, tower_conv1_2, tower_pool])\n\n        end_points[\'Mixed_6a\'] = net\n        net = slim.repeat(net, 20, block17, scale=0.10)\n\n        # Auxillary tower\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = slim.avg_pool2d(net, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n        with tf.variable_scope(\'Mixed_7a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(3, [tower_conv_1, tower_conv1_1,\n                              tower_conv2_2, tower_pool])\n\n        end_points[\'Mixed_7a\'] = net\n\n        net = slim.repeat(net, 9, block8, scale=0.20)\n        net = block8(net, activation_fn=None)\n\n        net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        end_points[\'Conv2d_7b_1x1\'] = net\n\n        with tf.variable_scope(\'Logits\'):\n          end_points[\'PrePool\'] = net\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a_8x8\')\n          net = slim.flatten(net)\n\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'Dropout\')\n\n          end_points[\'PreLogitsFlatten\'] = net\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
nets/inception_v3.py,82,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(3, [\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(3, [\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(3, [\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(3, [\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef inception_v3_arg_scope(weight_decay=0.00004,\n                           stddev=0.1):\n  """"""Defines the default InceptionV3 arg scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': 0.9997,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=slim.batch_norm,\n        normalizer_params=batch_norm_params) as sc:\n      return sc\n'"
nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.\n""""""\n\nimport functools\nimport tensorflow as tf\n\n# from nets import inception\n# from nets import overfeat\n# from nets import resnet_v1\n# from nets import resnet_v2\nfrom nets import vgg\n# from nets import xception\n\nfrom nets import ssd_vgg_300\nfrom nets import ssd_vgg_512\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'ssd_300_vgg\': ssd_vgg_300.ssd_net,\n                \'ssd_300_vgg_caffe\': ssd_vgg_300.ssd_net,\n                \'ssd_512_vgg\': ssd_vgg_512.ssd_net,\n                \'ssd_512_vgg_caffe\': ssd_vgg_512.ssd_net,\n                }\n\narg_scopes_map = {\'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'ssd_300_vgg\': ssd_vgg_300.ssd_arg_scope,\n                  \'ssd_300_vgg_caffe\': ssd_vgg_300.ssd_arg_scope_caffe,\n                  \'ssd_512_vgg\': ssd_vgg_512.ssd_arg_scope,\n                  \'ssd_512_vgg_caffe\': ssd_vgg_512.ssd_arg_scope_caffe,\n                  }\n\nnetworks_obj = {\'ssd_300_vgg\': ssd_vgg_300.SSDNet,\n                \'ssd_512_vgg\': ssd_vgg_512.SSDNet,\n                }\n\n\ndef get_network(name):\n    """"""Get a network object from a name.\n    """"""\n    # params = networks_obj[name].default_params if params is None else params\n    return networks_obj[name]\n\n\ndef get_network_fn(name, num_classes, is_training=False, **kwargs):\n    """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n    Args:\n      name: The name of the network.\n      num_classes: The number of classes to use for classification.\n      is_training: `True` if the model is being used for training and `False`\n        otherwise.\n      weight_decay: The l2 coefficient for the model weights.\n    Returns:\n      network_fn: A function that applies the model to a batch of images. It has\n        the following signature: logits, end_points = network_fn(images)\n    Raises:\n      ValueError: If network `name` is not recognized.\n    """"""\n    if name not in networks_map:\n        raise ValueError(\'Name of network unknown %s\' % name)\n    arg_scope = arg_scopes_map[name](**kwargs)\n    func = networks_map[name]\n    @functools.wraps(func)\n    def network_fn(images, **kwargs):\n        with slim.arg_scope(arg_scope):\n            return func(images, num_classes, is_training=is_training, **kwargs)\n    if hasattr(func, \'default_image_size\'):\n        network_fn.default_image_size = func.default_image_size\n\n    return network_fn\n'"
nets/np_methods.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional Numpy methods. Big mess of many things!\n""""""\nimport numpy as np\n\n\n# =========================================================================== #\n# Numpy implementations of SSD boxes functions.\n# =========================================================================== #\ndef ssd_bboxes_decode(feat_localizations,\n                      anchor_bboxes,\n                      prior_scaling=[0.1, 0.1, 0.2, 0.2]):\n    """"""Compute the relative bounding boxes from the layer features and\n    reference anchor bounding boxes.\n\n    Return:\n      numpy array Nx4: ymin, xmin, ymax, xmax\n    """"""\n    # Reshape for easier broadcasting.\n    l_shape = feat_localizations.shape\n    feat_localizations = np.reshape(feat_localizations,\n                                    (-1, l_shape[-2], l_shape[-1]))\n    yref, xref, href, wref = anchor_bboxes\n    xref = np.reshape(xref, [-1, 1])\n    yref = np.reshape(yref, [-1, 1])\n\n    # Compute center, height and width\n    cx = feat_localizations[:, :, 0] * wref * prior_scaling[0] + xref\n    cy = feat_localizations[:, :, 1] * href * prior_scaling[1] + yref\n    w = wref * np.exp(feat_localizations[:, :, 2] * prior_scaling[2])\n    h = href * np.exp(feat_localizations[:, :, 3] * prior_scaling[3])\n    # bboxes: ymin, xmin, xmax, ymax.\n    bboxes = np.zeros_like(feat_localizations)\n    bboxes[:, :, 0] = cy - h / 2.\n    bboxes[:, :, 1] = cx - w / 2.\n    bboxes[:, :, 2] = cy + h / 2.\n    bboxes[:, :, 3] = cx + w / 2.\n    # Back to original shape.\n    bboxes = np.reshape(bboxes, l_shape)\n    return bboxes\n\n\ndef ssd_bboxes_select_layer(predictions_layer,\n                            localizations_layer,\n                            anchors_layer,\n                            select_threshold=0.5,\n                            img_shape=(300, 300),\n                            num_classes=21,\n                            decode=True):\n    """"""Extract classes, scores and bounding boxes from features in one layer.\n\n    Return:\n      classes, scores, bboxes: Numpy arrays...\n    """"""\n    # First decode localizations features if necessary.\n    if decode:\n        localizations_layer = ssd_bboxes_decode(localizations_layer, anchors_layer)\n\n    # Reshape features to: Batches x N x N_labels | 4.\n    p_shape = predictions_layer.shape\n    batch_size = p_shape[0] if len(p_shape) == 5 else 1\n    predictions_layer = np.reshape(predictions_layer,\n                                   (batch_size, -1, p_shape[-1]))\n    l_shape = localizations_layer.shape\n    localizations_layer = np.reshape(localizations_layer,\n                                     (batch_size, -1, l_shape[-1]))\n\n    # Boxes selection: use threshold or score > no-label criteria.\n    if select_threshold is None or select_threshold == 0:\n        # Class prediction and scores: assign 0. to 0-class\n        classes = np.argmax(predictions_layer, axis=2)\n        scores = np.amax(predictions_layer, axis=2)\n        mask = (classes > 0)\n        classes = classes[mask]\n        scores = scores[mask]\n        bboxes = localizations_layer[mask]\n    else:\n        sub_predictions = predictions_layer[:, :, 1:]\n        idxes = np.where(sub_predictions > select_threshold)\n        classes = idxes[-1]+1\n        scores = sub_predictions[idxes]\n        bboxes = localizations_layer[idxes[:-1]]\n\n    return classes, scores, bboxes\n\n\ndef ssd_bboxes_select(predictions_net,\n                      localizations_net,\n                      anchors_net,\n                      select_threshold=0.5,\n                      img_shape=(300, 300),\n                      num_classes=21,\n                      decode=True):\n    """"""Extract classes, scores and bounding boxes from network output layers.\n\n    Return:\n      classes, scores, bboxes: Numpy arrays...\n    """"""\n    l_classes = []\n    l_scores = []\n    l_bboxes = []\n    # l_layers = []\n    # l_idxes = []\n    for i in range(len(predictions_net)):\n        classes, scores, bboxes = ssd_bboxes_select_layer(\n            predictions_net[i], localizations_net[i], anchors_net[i],\n            select_threshold, img_shape, num_classes, decode)\n        l_classes.append(classes)\n        l_scores.append(scores)\n        l_bboxes.append(bboxes)\n        # Debug information.\n        # l_layers.append(i)\n        # l_idxes.append((i, idxes))\n\n    classes = np.concatenate(l_classes, 0)\n    scores = np.concatenate(l_scores, 0)\n    bboxes = np.concatenate(l_bboxes, 0)\n    return classes, scores, bboxes\n\n\n# =========================================================================== #\n# Common functions for bboxes handling and selection.\n# =========================================================================== #\ndef bboxes_sort(classes, scores, bboxes, top_k=400):\n    """"""Sort bounding boxes by decreasing order and keep only the top_k\n    """"""\n    # if priority_inside:\n    #     inside = (bboxes[:, 0] > margin) & (bboxes[:, 1] > margin) & \\\n    #         (bboxes[:, 2] < 1-margin) & (bboxes[:, 3] < 1-margin)\n    #     idxes = np.argsort(-scores)\n    #     inside = inside[idxes]\n    #     idxes = np.concatenate([idxes[inside], idxes[~inside]])\n    idxes = np.argsort(-scores)\n    classes = classes[idxes][:top_k]\n    scores = scores[idxes][:top_k]\n    bboxes = bboxes[idxes][:top_k]\n    return classes, scores, bboxes\n\n\ndef bboxes_clip(bbox_ref, bboxes):\n    """"""Clip bounding boxes with respect to reference bbox.\n    """"""\n    bboxes = np.copy(bboxes)\n    bboxes = np.transpose(bboxes)\n    bbox_ref = np.transpose(bbox_ref)\n    bboxes[0] = np.maximum(bboxes[0], bbox_ref[0])\n    bboxes[1] = np.maximum(bboxes[1], bbox_ref[1])\n    bboxes[2] = np.minimum(bboxes[2], bbox_ref[2])\n    bboxes[3] = np.minimum(bboxes[3], bbox_ref[3])\n    bboxes = np.transpose(bboxes)\n    return bboxes\n\n\ndef bboxes_resize(bbox_ref, bboxes):\n    """"""Resize bounding boxes based on a reference bounding box,\n    assuming that the latter is [0, 0, 1, 1] after transform.\n    """"""\n    bboxes = np.copy(bboxes)\n    # Translate.\n    bboxes[:, 0] -= bbox_ref[0]\n    bboxes[:, 1] -= bbox_ref[1]\n    bboxes[:, 2] -= bbox_ref[0]\n    bboxes[:, 3] -= bbox_ref[1]\n    # Resize.\n    resize = [bbox_ref[2] - bbox_ref[0], bbox_ref[3] - bbox_ref[1]]\n    bboxes[:, 0] /= resize[0]\n    bboxes[:, 1] /= resize[1]\n    bboxes[:, 2] /= resize[0]\n    bboxes[:, 3] /= resize[1]\n    return bboxes\n\n\ndef bboxes_jaccard(bboxes1, bboxes2):\n    """"""Computing jaccard index between bboxes1 and bboxes2.\n    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\n    """"""\n    bboxes1 = np.transpose(bboxes1)\n    bboxes2 = np.transpose(bboxes2)\n    # Intersection bbox and volume.\n    int_ymin = np.maximum(bboxes1[0], bboxes2[0])\n    int_xmin = np.maximum(bboxes1[1], bboxes2[1])\n    int_ymax = np.minimum(bboxes1[2], bboxes2[2])\n    int_xmax = np.minimum(bboxes1[3], bboxes2[3])\n\n    int_h = np.maximum(int_ymax - int_ymin, 0.)\n    int_w = np.maximum(int_xmax - int_xmin, 0.)\n    int_vol = int_h * int_w\n    # Union volume.\n    vol1 = (bboxes1[2] - bboxes1[0]) * (bboxes1[3] - bboxes1[1])\n    vol2 = (bboxes2[2] - bboxes2[0]) * (bboxes2[3] - bboxes2[1])\n    jaccard = int_vol / (vol1 + vol2 - int_vol)\n    return jaccard\n\n\ndef bboxes_intersection(bboxes_ref, bboxes2):\n    """"""Computing jaccard index between bboxes1 and bboxes2.\n    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\n    """"""\n    bboxes_ref = np.transpose(bboxes_ref)\n    bboxes2 = np.transpose(bboxes2)\n    # Intersection bbox and volume.\n    int_ymin = np.maximum(bboxes_ref[0], bboxes2[0])\n    int_xmin = np.maximum(bboxes_ref[1], bboxes2[1])\n    int_ymax = np.minimum(bboxes_ref[2], bboxes2[2])\n    int_xmax = np.minimum(bboxes_ref[3], bboxes2[3])\n\n    int_h = np.maximum(int_ymax - int_ymin, 0.)\n    int_w = np.maximum(int_xmax - int_xmin, 0.)\n    int_vol = int_h * int_w\n    # Union volume.\n    vol = (bboxes_ref[2] - bboxes_ref[0]) * (bboxes_ref[3] - bboxes_ref[1])\n    score = int_vol / vol\n    return score\n\n\ndef bboxes_nms(classes, scores, bboxes, nms_threshold=0.45):\n    """"""Apply non-maximum selection to bounding boxes.\n    """"""\n    keep_bboxes = np.ones(scores.shape, dtype=np.bool)\n    for i in range(scores.size-1):\n        if keep_bboxes[i]:\n            # Computer overlap with bboxes which are following.\n            overlap = bboxes_jaccard(bboxes[i], bboxes[(i+1):])\n            # Overlap threshold for keeping + checking part of the same class\n            keep_overlap = np.logical_or(overlap < nms_threshold, classes[(i+1):] != classes[i])\n            keep_bboxes[(i+1):] = np.logical_and(keep_bboxes[(i+1):], keep_overlap)\n\n    idxes = np.where(keep_bboxes)\n    return classes[idxes], scores[idxes], bboxes[idxes]\n\n\ndef bboxes_nms_fast(classes, scores, bboxes, threshold=0.45):\n    """"""Apply non-maximum selection to bounding boxes.\n    """"""\n    pass\n\n\n\n\n'"
nets/ssd_common.py,68,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Shared function between different SSD implementations.\n""""""\nimport numpy as np\nimport tensorflow as tf\nimport tf_extended as tfe\n\n\n# =========================================================================== #\n# TensorFlow implementation of boxes SSD encoding / decoding.\n# =========================================================================== #\ndef tf_ssd_bboxes_encode_layer(labels,\n                               bboxes,\n                               anchors_layer,\n                               num_classes,\n                               no_annotation_label,\n                               ignore_threshold=0.5,\n                               prior_scaling=[0.1, 0.1, 0.2, 0.2],\n                               dtype=tf.float32):\n    """"""Encode groundtruth labels and bounding boxes using SSD anchors from\n    one layer.\n\n    Arguments:\n      labels: 1D Tensor(int64) containing groundtruth labels;\n      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;\n      anchors_layer: Numpy array with layer anchors;\n      matching_threshold: Threshold for positive match with groundtruth bboxes;\n      prior_scaling: Scaling of encoded coordinates.\n\n    Return:\n      (target_labels, target_localizations, target_scores): Target Tensors.\n    """"""\n    # Anchors coordinates and volume.\n    yref, xref, href, wref = anchors_layer\n    ymin = yref - href / 2.\n    xmin = xref - wref / 2.\n    ymax = yref + href / 2.\n    xmax = xref + wref / 2.\n    vol_anchors = (xmax - xmin) * (ymax - ymin)\n\n    # Initialize tensors...\n    shape = (yref.shape[0], yref.shape[1], href.size)\n    feat_labels = tf.zeros(shape, dtype=tf.int64)\n    feat_scores = tf.zeros(shape, dtype=dtype)\n\n    feat_ymin = tf.zeros(shape, dtype=dtype)\n    feat_xmin = tf.zeros(shape, dtype=dtype)\n    feat_ymax = tf.ones(shape, dtype=dtype)\n    feat_xmax = tf.ones(shape, dtype=dtype)\n\n    def jaccard_with_anchors(bbox):\n        """"""Compute jaccard score between a box and the anchors.\n        """"""\n        int_ymin = tf.maximum(ymin, bbox[0])\n        int_xmin = tf.maximum(xmin, bbox[1])\n        int_ymax = tf.minimum(ymax, bbox[2])\n        int_xmax = tf.minimum(xmax, bbox[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        # Volumes.\n        inter_vol = h * w\n        union_vol = vol_anchors - inter_vol \\\n            + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n        jaccard = tf.div(inter_vol, union_vol)\n        return jaccard\n\n    def intersection_with_anchors(bbox):\n        """"""Compute intersection between score a box and the anchors.\n        """"""\n        int_ymin = tf.maximum(ymin, bbox[0])\n        int_xmin = tf.maximum(xmin, bbox[1])\n        int_ymax = tf.minimum(ymax, bbox[2])\n        int_xmax = tf.minimum(xmax, bbox[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        inter_vol = h * w\n        scores = tf.div(inter_vol, vol_anchors)\n        return scores\n\n    def condition(i, feat_labels, feat_scores,\n                  feat_ymin, feat_xmin, feat_ymax, feat_xmax):\n        """"""Condition: check label index.\n        """"""\n        r = tf.less(i, tf.shape(labels))\n        return r[0]\n\n    def body(i, feat_labels, feat_scores,\n             feat_ymin, feat_xmin, feat_ymax, feat_xmax):\n        """"""Body: update feature labels, scores and bboxes.\n        Follow the original SSD paper for that purpose:\n          - assign values when jaccard > 0.5;\n          - only update if beat the score of other bboxes.\n        """"""\n        # Jaccard score.\n        label = labels[i]\n        bbox = bboxes[i]\n        jaccard = jaccard_with_anchors(bbox)\n        # Mask: check threshold + scores + no annotations + num_classes.\n        mask = tf.greater(jaccard, feat_scores)\n        # mask = tf.logical_and(mask, tf.greater(jaccard, matching_threshold))\n        mask = tf.logical_and(mask, feat_scores > -0.5)\n        mask = tf.logical_and(mask, label < num_classes)\n        imask = tf.cast(mask, tf.int64)\n        fmask = tf.cast(mask, dtype)\n        # Update values using mask.\n        feat_labels = imask * label + (1 - imask) * feat_labels\n        feat_scores = tf.where(mask, jaccard, feat_scores)\n\n        feat_ymin = fmask * bbox[0] + (1 - fmask) * feat_ymin\n        feat_xmin = fmask * bbox[1] + (1 - fmask) * feat_xmin\n        feat_ymax = fmask * bbox[2] + (1 - fmask) * feat_ymax\n        feat_xmax = fmask * bbox[3] + (1 - fmask) * feat_xmax\n\n        # Check no annotation label: ignore these anchors...\n        # interscts = intersection_with_anchors(bbox)\n        # mask = tf.logical_and(interscts > ignore_threshold,\n        #                       label == no_annotation_label)\n        # # Replace scores by -1.\n        # feat_scores = tf.where(mask, -tf.cast(mask, dtype), feat_scores)\n\n        return [i+1, feat_labels, feat_scores,\n                feat_ymin, feat_xmin, feat_ymax, feat_xmax]\n    # Main loop definition.\n    i = 0\n    [i, feat_labels, feat_scores,\n     feat_ymin, feat_xmin,\n     feat_ymax, feat_xmax] = tf.while_loop(condition, body,\n                                           [i, feat_labels, feat_scores,\n                                            feat_ymin, feat_xmin,\n                                            feat_ymax, feat_xmax])\n    # Transform to center / size.\n    feat_cy = (feat_ymax + feat_ymin) / 2.\n    feat_cx = (feat_xmax + feat_xmin) / 2.\n    feat_h = feat_ymax - feat_ymin\n    feat_w = feat_xmax - feat_xmin\n    # Encode features.\n    feat_cy = (feat_cy - yref) / href / prior_scaling[0]\n    feat_cx = (feat_cx - xref) / wref / prior_scaling[1]\n    feat_h = tf.log(feat_h / href) / prior_scaling[2]\n    feat_w = tf.log(feat_w / wref) / prior_scaling[3]\n    # Use SSD ordering: x / y / w / h instead of ours.\n    feat_localizations = tf.stack([feat_cx, feat_cy, feat_w, feat_h], axis=-1)\n    return feat_labels, feat_localizations, feat_scores\n\n\ndef tf_ssd_bboxes_encode(labels,\n                         bboxes,\n                         anchors,\n                         num_classes,\n                         no_annotation_label,\n                         ignore_threshold=0.5,\n                         prior_scaling=[0.1, 0.1, 0.2, 0.2],\n                         dtype=tf.float32,\n                         scope=\'ssd_bboxes_encode\'):\n    """"""Encode groundtruth labels and bounding boxes using SSD net anchors.\n    Encoding boxes for all feature layers.\n\n    Arguments:\n      labels: 1D Tensor(int64) containing groundtruth labels;\n      bboxes: Nx4 Tensor(float) with bboxes relative coordinates;\n      anchors: List of Numpy array with layer anchors;\n      matching_threshold: Threshold for positive match with groundtruth bboxes;\n      prior_scaling: Scaling of encoded coordinates.\n\n    Return:\n      (target_labels, target_localizations, target_scores):\n        Each element is a list of target Tensors.\n    """"""\n    with tf.name_scope(scope):\n        target_labels = []\n        target_localizations = []\n        target_scores = []\n        for i, anchors_layer in enumerate(anchors):\n            with tf.name_scope(\'bboxes_encode_block_%i\' % i):\n                t_labels, t_loc, t_scores = \\\n                    tf_ssd_bboxes_encode_layer(labels, bboxes, anchors_layer,\n                                               num_classes, no_annotation_label,\n                                               ignore_threshold,\n                                               prior_scaling, dtype)\n                target_labels.append(t_labels)\n                target_localizations.append(t_loc)\n                target_scores.append(t_scores)\n        return target_labels, target_localizations, target_scores\n\n\ndef tf_ssd_bboxes_decode_layer(feat_localizations,\n                               anchors_layer,\n                               prior_scaling=[0.1, 0.1, 0.2, 0.2]):\n    """"""Compute the relative bounding boxes from the layer features and\n    reference anchor bounding boxes.\n\n    Arguments:\n      feat_localizations: Tensor containing localization features.\n      anchors: List of numpy array containing anchor boxes.\n\n    Return:\n      Tensor Nx4: ymin, xmin, ymax, xmax\n    """"""\n    yref, xref, href, wref = anchors_layer\n\n    # Compute center, height and width\n    cx = feat_localizations[:, :, :, :, 0] * wref * prior_scaling[0] + xref\n    cy = feat_localizations[:, :, :, :, 1] * href * prior_scaling[1] + yref\n    w = wref * tf.exp(feat_localizations[:, :, :, :, 2] * prior_scaling[2])\n    h = href * tf.exp(feat_localizations[:, :, :, :, 3] * prior_scaling[3])\n    # Boxes coordinates.\n    ymin = cy - h / 2.\n    xmin = cx - w / 2.\n    ymax = cy + h / 2.\n    xmax = cx + w / 2.\n    bboxes = tf.stack([ymin, xmin, ymax, xmax], axis=-1)\n    return bboxes\n\n\ndef tf_ssd_bboxes_decode(feat_localizations,\n                         anchors,\n                         prior_scaling=[0.1, 0.1, 0.2, 0.2],\n                         scope=\'ssd_bboxes_decode\'):\n    """"""Compute the relative bounding boxes from the SSD net features and\n    reference anchors bounding boxes.\n\n    Arguments:\n      feat_localizations: List of Tensors containing localization features.\n      anchors: List of numpy array containing anchor boxes.\n\n    Return:\n      List of Tensors Nx4: ymin, xmin, ymax, xmax\n    """"""\n    with tf.name_scope(scope):\n        bboxes = []\n        for i, anchors_layer in enumerate(anchors):\n            bboxes.append(\n                tf_ssd_bboxes_decode_layer(feat_localizations[i],\n                                           anchors_layer,\n                                           prior_scaling))\n        return bboxes\n\n\n# =========================================================================== #\n# SSD boxes selection.\n# =========================================================================== #\ndef tf_ssd_bboxes_select_layer(predictions_layer, localizations_layer,\n                               select_threshold=None,\n                               num_classes=21,\n                               ignore_class=0,\n                               scope=None):\n    """"""Extract classes, scores and bounding boxes from features in one layer.\n    Batch-compatible: inputs are supposed to have batch-type shapes.\n\n    Args:\n      predictions_layer: A SSD prediction layer;\n      localizations_layer: A SSD localization layer;\n      select_threshold: Classification threshold for selecting a box. All boxes\n        under the threshold are set to \'zero\'. If None, no threshold applied.\n    Return:\n      d_scores, d_bboxes: Dictionary of scores and bboxes Tensors of\n        size Batches X N x 1 | 4. Each key corresponding to a class.\n    """"""\n    select_threshold = 0.0 if select_threshold is None else select_threshold\n    with tf.name_scope(scope, \'ssd_bboxes_select_layer\',\n                       [predictions_layer, localizations_layer]):\n        # Reshape features: Batches x N x N_labels | 4\n        p_shape = tfe.get_shape(predictions_layer)\n        predictions_layer = tf.reshape(predictions_layer,\n                                       tf.stack([p_shape[0], -1, p_shape[-1]]))\n        l_shape = tfe.get_shape(localizations_layer)\n        localizations_layer = tf.reshape(localizations_layer,\n                                         tf.stack([l_shape[0], -1, l_shape[-1]]))\n\n        d_scores = {}\n        d_bboxes = {}\n        for c in range(0, num_classes):\n            if c != ignore_class:\n                # Remove boxes under the threshold.\n                scores = predictions_layer[:, :, c]\n                fmask = tf.cast(tf.greater_equal(scores, select_threshold), scores.dtype)\n                scores = scores * fmask\n                bboxes = localizations_layer * tf.expand_dims(fmask, axis=-1)\n                # Append to dictionary.\n                d_scores[c] = scores\n                d_bboxes[c] = bboxes\n\n        return d_scores, d_bboxes\n\n\ndef tf_ssd_bboxes_select(predictions_net, localizations_net,\n                         select_threshold=None,\n                         num_classes=21,\n                         ignore_class=0,\n                         scope=None):\n    """"""Extract classes, scores and bounding boxes from network output layers.\n    Batch-compatible: inputs are supposed to have batch-type shapes.\n\n    Args:\n      predictions_net: List of SSD prediction layers;\n      localizations_net: List of localization layers;\n      select_threshold: Classification threshold for selecting a box. All boxes\n        under the threshold are set to \'zero\'. If None, no threshold applied.\n    Return:\n      d_scores, d_bboxes: Dictionary of scores and bboxes Tensors of\n        size Batches X N x 1 | 4. Each key corresponding to a class.\n    """"""\n    with tf.name_scope(scope, \'ssd_bboxes_select\',\n                       [predictions_net, localizations_net]):\n        l_scores = []\n        l_bboxes = []\n        for i in range(len(predictions_net)):\n            scores, bboxes = tf_ssd_bboxes_select_layer(predictions_net[i],\n                                                        localizations_net[i],\n                                                        select_threshold,\n                                                        num_classes,\n                                                        ignore_class)\n            l_scores.append(scores)\n            l_bboxes.append(bboxes)\n        # Concat results.\n        d_scores = {}\n        d_bboxes = {}\n        for c in l_scores[0].keys():\n            ls = [s[c] for s in l_scores]\n            lb = [b[c] for b in l_bboxes]\n            d_scores[c] = tf.concat(ls, axis=1)\n            d_bboxes[c] = tf.concat(lb, axis=1)\n        return d_scores, d_bboxes\n\n\ndef tf_ssd_bboxes_select_layer_all_classes(predictions_layer, localizations_layer,\n                                           select_threshold=None):\n    """"""Extract classes, scores and bounding boxes from features in one layer.\n     Batch-compatible: inputs are supposed to have batch-type shapes.\n\n     Args:\n       predictions_layer: A SSD prediction layer;\n       localizations_layer: A SSD localization layer;\n      select_threshold: Classification threshold for selecting a box. If None,\n        select boxes whose classification score is higher than \'no class\'.\n     Return:\n      classes, scores, bboxes: Input Tensors.\n     """"""\n    # Reshape features: Batches x N x N_labels | 4\n    p_shape = tfe.get_shape(predictions_layer)\n    predictions_layer = tf.reshape(predictions_layer,\n                                   tf.stack([p_shape[0], -1, p_shape[-1]]))\n    l_shape = tfe.get_shape(localizations_layer)\n    localizations_layer = tf.reshape(localizations_layer,\n                                     tf.stack([l_shape[0], -1, l_shape[-1]]))\n    # Boxes selection: use threshold or score > no-label criteria.\n    if select_threshold is None or select_threshold == 0:\n        # Class prediction and scores: assign 0. to 0-class\n        classes = tf.argmax(predictions_layer, axis=2)\n        scores = tf.reduce_max(predictions_layer, axis=2)\n        scores = scores * tf.cast(classes > 0, scores.dtype)\n    else:\n        sub_predictions = predictions_layer[:, :, 1:]\n        classes = tf.argmax(sub_predictions, axis=2) + 1\n        scores = tf.reduce_max(sub_predictions, axis=2)\n        # Only keep predictions higher than threshold.\n        mask = tf.greater(scores, select_threshold)\n        classes = classes * tf.cast(mask, classes.dtype)\n        scores = scores * tf.cast(mask, scores.dtype)\n    # Assume localization layer already decoded.\n    bboxes = localizations_layer\n    return classes, scores, bboxes\n\n\ndef tf_ssd_bboxes_select_all_classes(predictions_net, localizations_net,\n                                     select_threshold=None,\n                                     scope=None):\n    """"""Extract classes, scores and bounding boxes from network output layers.\n    Batch-compatible: inputs are supposed to have batch-type shapes.\n\n    Args:\n      predictions_net: List of SSD prediction layers;\n      localizations_net: List of localization layers;\n      select_threshold: Classification threshold for selecting a box. If None,\n        select boxes whose classification score is higher than \'no class\'.\n    Return:\n      classes, scores, bboxes: Tensors.\n    """"""\n    with tf.name_scope(scope, \'ssd_bboxes_select\',\n                       [predictions_net, localizations_net]):\n        l_classes = []\n        l_scores = []\n        l_bboxes = []\n        for i in range(len(predictions_net)):\n            classes, scores, bboxes = \\\n                tf_ssd_bboxes_select_layer_all_classes(predictions_net[i],\n                                                       localizations_net[i],\n                                                       select_threshold)\n            l_classes.append(classes)\n            l_scores.append(scores)\n            l_bboxes.append(bboxes)\n\n        classes = tf.concat(l_classes, axis=1)\n        scores = tf.concat(l_scores, axis=1)\n        bboxes = tf.concat(l_bboxes, axis=1)\n        return classes, scores, bboxes\n\n'"
nets/ssd_vgg_300.py,90,"b'# Copyright 2016 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Definition of 300 VGG-based SSD network.\n\nThis model was initially introduced in:\nSSD: Single Shot MultiBox Detector\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,\nCheng-Yang Fu, Alexander C. Berg\nhttps://arxiv.org/abs/1512.02325\n\nTwo variants of the model are defined: the 300x300 and 512x512 models, the\nlatter obtaining a slightly better accuracy on Pascal VOC.\n\nUsage:\n    with slim.arg_scope(ssd_vgg.ssd_vgg()):\n        outputs, end_points = ssd_vgg.ssd_vgg(inputs)\n\nThis network port of the original Caffe model. The padding in TF and Caffe\nis slightly different, and can lead to severe accuracy drop if not taken care\nin a correct way!\n\nIn Caffe, the output size of convolution and pooling layers are computing as\nfollowing: h_o = (h_i + 2 * pad_h - kernel_h) / stride_h + 1\n\nNevertheless, there is a subtle difference between both for stride > 1. In\nthe case of convolution:\n    top_size = floor((bottom_size + 2*pad - kernel_size) / stride) + 1\nwhereas for pooling:\n    top_size = ceil((bottom_size + 2*pad - kernel_size) / stride) + 1\nHence implicitely allowing some additional padding even if pad = 0. This\nbehaviour explains why pooling with stride and kernel of size 2 are behaving\nthe same way in TensorFlow and Caffe.\n\nNevertheless, this is not the case anymore for other kernel sizes, hence\nmotivating the use of special padding layer for controlling these side-effects.\n\n@@ssd_vgg_300\n""""""\nimport math\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_extended as tfe\nfrom nets import custom_layers\nfrom nets import ssd_common\n\nslim = tf.contrib.slim\n\n\n# =========================================================================== #\n# SSD class definition.\n# =========================================================================== #\nSSDParams = namedtuple(\'SSDParameters\', [\'img_shape\',\n                                         \'num_classes\',\n                                         \'no_annotation_label\',\n                                         \'feat_layers\',\n                                         \'feat_shapes\',\n                                         \'anchor_size_bounds\',\n                                         \'anchor_sizes\',\n                                         \'anchor_ratios\',\n                                         \'anchor_steps\',\n                                         \'anchor_offset\',\n                                         \'normalizations\',\n                                         \'prior_scaling\'\n                                         ])\n\n\nclass SSDNet(object):\n    """"""Implementation of the SSD VGG-based 300 network.\n\n    The default features layers with 300x300 image input are:\n      conv4 ==> 38 x 38\n      conv7 ==> 19 x 19\n      conv8 ==> 10 x 10\n      conv9 ==> 5 x 5\n      conv10 ==> 3 x 3\n      conv11 ==> 1 x 1\n    The default image size used to train this network is 300x300.\n    """"""\n    default_params = SSDParams(\n        img_shape=(300, 300),\n        num_classes=21,\n        no_annotation_label=21,\n        feat_layers=[\'block4\', \'block7\', \'block8\', \'block9\', \'block10\', \'block11\'],\n        feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n        anchor_size_bounds=[0.15, 0.90],\n        # anchor_size_bounds=[0.20, 0.90],\n        anchor_sizes=[(21., 45.),\n                      (45., 99.),\n                      (99., 153.),\n                      (153., 207.),\n                      (207., 261.),\n                      (261., 315.)],\n        # anchor_sizes=[(30., 60.),\n        #               (60., 111.),\n        #               (111., 162.),\n        #               (162., 213.),\n        #               (213., 264.),\n        #               (264., 315.)],\n        anchor_ratios=[[2, .5],\n                       [2, .5, 3, 1./3],\n                       [2, .5, 3, 1./3],\n                       [2, .5, 3, 1./3],\n                       [2, .5],\n                       [2, .5]],\n        anchor_steps=[8, 16, 32, 64, 100, 300],\n        anchor_offset=0.5,\n        normalizations=[20, -1, -1, -1, -1, -1],\n        prior_scaling=[0.1, 0.1, 0.2, 0.2]\n        )\n\n    def __init__(self, params=None):\n        """"""Init the SSD net with some parameters. Use the default ones\n        if none provided.\n        """"""\n        if isinstance(params, SSDParams):\n            self.params = params\n        else:\n            self.params = SSDNet.default_params\n\n    # ======================================================================= #\n    def net(self, inputs,\n            is_training=True,\n            update_feat_shapes=True,\n            dropout_keep_prob=0.5,\n            prediction_fn=slim.softmax,\n            reuse=None,\n            scope=\'ssd_300_vgg\'):\n        """"""SSD network definition.\n        """"""\n        r = ssd_net(inputs,\n                    num_classes=self.params.num_classes,\n                    feat_layers=self.params.feat_layers,\n                    anchor_sizes=self.params.anchor_sizes,\n                    anchor_ratios=self.params.anchor_ratios,\n                    normalizations=self.params.normalizations,\n                    is_training=is_training,\n                    dropout_keep_prob=dropout_keep_prob,\n                    prediction_fn=prediction_fn,\n                    reuse=reuse,\n                    scope=scope)\n        # Update feature shapes (try at least!)\n        if update_feat_shapes:\n            shapes = ssd_feat_shapes_from_net(r[0], self.params.feat_shapes)\n            self.params = self.params._replace(feat_shapes=shapes)\n        return r\n\n    def arg_scope(self, weight_decay=0.0005, data_format=\'NHWC\'):\n        """"""Network arg_scope.\n        """"""\n        return ssd_arg_scope(weight_decay, data_format=data_format)\n\n    def arg_scope_caffe(self, caffe_scope):\n        """"""Caffe arg_scope used for weights importing.\n        """"""\n        return ssd_arg_scope_caffe(caffe_scope)\n\n    # ======================================================================= #\n    def update_feature_shapes(self, predictions):\n        """"""Update feature shapes from predictions collection (Tensor or Numpy\n        array).\n        """"""\n        shapes = ssd_feat_shapes_from_net(predictions, self.params.feat_shapes)\n        self.params = self.params._replace(feat_shapes=shapes)\n\n    def anchors(self, img_shape, dtype=np.float32):\n        """"""Compute the default anchor boxes, given an image shape.\n        """"""\n        return ssd_anchors_all_layers(img_shape,\n                                      self.params.feat_shapes,\n                                      self.params.anchor_sizes,\n                                      self.params.anchor_ratios,\n                                      self.params.anchor_steps,\n                                      self.params.anchor_offset,\n                                      dtype)\n\n    def bboxes_encode(self, labels, bboxes, anchors,\n                      scope=None):\n        """"""Encode labels and bounding boxes.\n        """"""\n        return ssd_common.tf_ssd_bboxes_encode(\n            labels, bboxes, anchors,\n            self.params.num_classes,\n            self.params.no_annotation_label,\n            ignore_threshold=0.5,\n            prior_scaling=self.params.prior_scaling,\n            scope=scope)\n\n    def bboxes_decode(self, feat_localizations, anchors,\n                      scope=\'ssd_bboxes_decode\'):\n        """"""Encode labels and bounding boxes.\n        """"""\n        return ssd_common.tf_ssd_bboxes_decode(\n            feat_localizations, anchors,\n            prior_scaling=self.params.prior_scaling,\n            scope=scope)\n\n    def detected_bboxes(self, predictions, localisations,\n                        select_threshold=None, nms_threshold=0.5,\n                        clipping_bbox=None, top_k=400, keep_top_k=200):\n        """"""Get the detected bounding boxes from the SSD network output.\n        """"""\n        # Select top_k bboxes from predictions, and clip\n        rscores, rbboxes = \\\n            ssd_common.tf_ssd_bboxes_select(predictions, localisations,\n                                            select_threshold=select_threshold,\n                                            num_classes=self.params.num_classes)\n        rscores, rbboxes = \\\n            tfe.bboxes_sort(rscores, rbboxes, top_k=top_k)\n        # Apply NMS algorithm.\n        rscores, rbboxes = \\\n            tfe.bboxes_nms_batch(rscores, rbboxes,\n                                 nms_threshold=nms_threshold,\n                                 keep_top_k=keep_top_k)\n        if clipping_bbox is not None:\n            rbboxes = tfe.bboxes_clip(clipping_bbox, rbboxes)\n        return rscores, rbboxes\n\n    def losses(self, logits, localisations,\n               gclasses, glocalisations, gscores,\n               match_threshold=0.5,\n               negative_ratio=3.,\n               alpha=1.,\n               label_smoothing=0.,\n               scope=\'ssd_losses\'):\n        """"""Define the SSD network losses.\n        """"""\n        return ssd_losses(logits, localisations,\n                          gclasses, glocalisations, gscores,\n                          match_threshold=match_threshold,\n                          negative_ratio=negative_ratio,\n                          alpha=alpha,\n                          label_smoothing=label_smoothing,\n                          scope=scope)\n\n\n# =========================================================================== #\n# SSD tools...\n# =========================================================================== #\ndef ssd_size_bounds_to_values(size_bounds,\n                              n_feat_layers,\n                              img_shape=(300, 300)):\n    """"""Compute the reference sizes of the anchor boxes from relative bounds.\n    The absolute values are measured in pixels, based on the network\n    default size (300 pixels).\n\n    This function follows the computation performed in the original\n    implementation of SSD in Caffe.\n\n    Return:\n      list of list containing the absolute sizes at each scale. For each scale,\n      the ratios only apply to the first value.\n    """"""\n    assert img_shape[0] == img_shape[1]\n\n    img_size = img_shape[0]\n    min_ratio = int(size_bounds[0] * 100)\n    max_ratio = int(size_bounds[1] * 100)\n    step = int(math.floor((max_ratio - min_ratio) / (n_feat_layers - 2)))\n    # Start with the following smallest sizes.\n    sizes = [[img_size * size_bounds[0] / 2, img_size * size_bounds[0]]]\n    for ratio in range(min_ratio, max_ratio + 1, step):\n        sizes.append((img_size * ratio / 100.,\n                      img_size * (ratio + step) / 100.))\n    return sizes\n\n\ndef ssd_feat_shapes_from_net(predictions, default_shapes=None):\n    """"""Try to obtain the feature shapes from the prediction layers. The latter\n    can be either a Tensor or Numpy ndarray.\n\n    Return:\n      list of feature shapes. Default values if predictions shape not fully\n      determined.\n    """"""\n    feat_shapes = []\n    for l in predictions:\n        # Get the shape, from either a np array or a tensor.\n        if isinstance(l, np.ndarray):\n            shape = l.shape\n        else:\n            shape = l.get_shape().as_list()\n        shape = shape[1:4]\n        # Problem: undetermined shape...\n        if None in shape:\n            return default_shapes\n        else:\n            feat_shapes.append(shape)\n    return feat_shapes\n\n\ndef ssd_anchor_one_layer(img_shape,\n                         feat_shape,\n                         sizes,\n                         ratios,\n                         step,\n                         offset=0.5,\n                         dtype=np.float32):\n    """"""Computer SSD default anchor boxes for one feature layer.\n\n    Determine the relative position grid of the centers, and the relative\n    width and height.\n\n    Arguments:\n      feat_shape: Feature shape, used for computing relative position grids;\n      size: Absolute reference sizes;\n      ratios: Ratios to use on these features;\n      img_shape: Image shape, used for computing height, width relatively to the\n        former;\n      offset: Grid offset.\n\n    Return:\n      y, x, h, w: Relative x and y grids, and height and width.\n    """"""\n    # Compute the position grid: simple way.\n    # y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n    # y = (y.astype(dtype) + offset) / feat_shape[0]\n    # x = (x.astype(dtype) + offset) / feat_shape[1]\n    # Weird SSD-Caffe computation using steps values...\n    y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n    y = (y.astype(dtype) + offset) * step / img_shape[0]\n    x = (x.astype(dtype) + offset) * step / img_shape[1]\n\n    # Expand dims to support easy broadcasting.\n    y = np.expand_dims(y, axis=-1)\n    x = np.expand_dims(x, axis=-1)\n\n    # Compute relative height and width.\n    # Tries to follow the original implementation of SSD for the order.\n    num_anchors = len(sizes) + len(ratios)\n    h = np.zeros((num_anchors, ), dtype=dtype)\n    w = np.zeros((num_anchors, ), dtype=dtype)\n    # Add first anchor boxes with ratio=1.\n    h[0] = sizes[0] / img_shape[0]\n    w[0] = sizes[0] / img_shape[1]\n    di = 1\n    if len(sizes) > 1:\n        h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]\n        w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]\n        di += 1\n    for i, r in enumerate(ratios):\n        h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)\n        w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)\n    return y, x, h, w\n\n\ndef ssd_anchors_all_layers(img_shape,\n                           layers_shape,\n                           anchor_sizes,\n                           anchor_ratios,\n                           anchor_steps,\n                           offset=0.5,\n                           dtype=np.float32):\n    """"""Compute anchor boxes for all feature layers.\n    """"""\n    layers_anchors = []\n    for i, s in enumerate(layers_shape):\n        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,\n                                             anchor_sizes[i],\n                                             anchor_ratios[i],\n                                             anchor_steps[i],\n                                             offset=offset, dtype=dtype)\n        layers_anchors.append(anchor_bboxes)\n    return layers_anchors\n\n\n# =========================================================================== #\n# Functional definition of VGG-based SSD 300.\n# =========================================================================== #\ndef tensor_shape(x, rank=3):\n    """"""Returns the dimensions of a tensor.\n    Args:\n      image: A N-D Tensor of shape.\n    Returns:\n      A list of dimensions. Dimensions that are statically known are python\n        integers,otherwise they are integer scalar tensors.\n    """"""\n    if x.get_shape().is_fully_defined():\n        return x.get_shape().as_list()\n    else:\n        static_shape = x.get_shape().with_rank(rank).as_list()\n        dynamic_shape = tf.unstack(tf.shape(x), rank)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef ssd_multibox_layer(inputs,\n                       num_classes,\n                       sizes,\n                       ratios=[1],\n                       normalization=-1,\n                       bn_normalization=False):\n    """"""Construct a multibox layer, return a class and localization predictions.\n    """"""\n    net = inputs\n    if normalization > 0:\n        net = custom_layers.l2_normalization(net, scaling=True)\n    # Number of anchors.\n    num_anchors = len(sizes) + len(ratios)\n\n    # Location.\n    num_loc_pred = num_anchors * 4\n    loc_pred = slim.conv2d(net, num_loc_pred, [3, 3], activation_fn=None,\n                           scope=\'conv_loc\')\n    loc_pred = custom_layers.channel_to_last(loc_pred)\n    loc_pred = tf.reshape(loc_pred,\n                          tensor_shape(loc_pred, 4)[:-1]+[num_anchors, 4])\n    # Class prediction.\n    num_cls_pred = num_anchors * num_classes\n    cls_pred = slim.conv2d(net, num_cls_pred, [3, 3], activation_fn=None,\n                           scope=\'conv_cls\')\n    cls_pred = custom_layers.channel_to_last(cls_pred)\n    cls_pred = tf.reshape(cls_pred,\n                          tensor_shape(cls_pred, 4)[:-1]+[num_anchors, num_classes])\n    return cls_pred, loc_pred\n\n\ndef ssd_net(inputs,\n            num_classes=SSDNet.default_params.num_classes,\n            feat_layers=SSDNet.default_params.feat_layers,\n            anchor_sizes=SSDNet.default_params.anchor_sizes,\n            anchor_ratios=SSDNet.default_params.anchor_ratios,\n            normalizations=SSDNet.default_params.normalizations,\n            is_training=True,\n            dropout_keep_prob=0.5,\n            prediction_fn=slim.softmax,\n            reuse=None,\n            scope=\'ssd_300_vgg\'):\n    """"""SSD net definition.\n    """"""\n    # if data_format == \'NCHW\':\n    #     inputs = tf.transpose(inputs, perm=(0, 3, 1, 2))\n\n    # End_points collect relevant activations for external use.\n    end_points = {}\n    with tf.variable_scope(scope, \'ssd_300_vgg\', [inputs], reuse=reuse):\n        # Original VGG-16 blocks.\n        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n        end_points[\'block1\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n        # Block 2.\n        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n        end_points[\'block2\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n        # Block 3.\n        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n        end_points[\'block3\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n        # Block 4.\n        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n        end_points[\'block4\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n        # Block 5.\n        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n        end_points[\'block5\'] = net\n        net = slim.max_pool2d(net, [3, 3], stride=1, scope=\'pool5\')\n\n        # Additional SSD blocks.\n        # Block 6: let\'s dilate the hell out of it!\n        net = slim.conv2d(net, 1024, [3, 3], rate=6, scope=\'conv6\')\n        end_points[\'block6\'] = net\n        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)\n        # Block 7: 1x1 conv. Because the fuck.\n        net = slim.conv2d(net, 1024, [1, 1], scope=\'conv7\')\n        end_points[\'block7\'] = net\n        net = tf.layers.dropout(net, rate=dropout_keep_prob, training=is_training)\n\n        # Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).\n        end_point = \'block8\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 256, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 512, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block9\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 256, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block10\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = slim.conv2d(net, 256, [3, 3], scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block11\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = slim.conv2d(net, 256, [3, 3], scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n\n        # Prediction and localisations layers.\n        predictions = []\n        logits = []\n        localisations = []\n        for i, layer in enumerate(feat_layers):\n            with tf.variable_scope(layer + \'_box\'):\n                p, l = ssd_multibox_layer(end_points[layer],\n                                          num_classes,\n                                          anchor_sizes[i],\n                                          anchor_ratios[i],\n                                          normalizations[i])\n            predictions.append(prediction_fn(p))\n            logits.append(p)\n            localisations.append(l)\n\n        return predictions, localisations, logits, end_points\nssd_net.default_image_size = 300\n\n\ndef ssd_arg_scope(weight_decay=0.0005, data_format=\'NHWC\'):\n    """"""Defines the VGG arg scope.\n\n    Args:\n      weight_decay: The l2 regularization coefficient.\n\n    Returns:\n      An arg_scope.\n    """"""\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n                        biases_initializer=tf.zeros_initializer()):\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                            padding=\'SAME\',\n                            data_format=data_format):\n            with slim.arg_scope([custom_layers.pad2d,\n                                 custom_layers.l2_normalization,\n                                 custom_layers.channel_to_last],\n                                data_format=data_format) as sc:\n                return sc\n\n\n# =========================================================================== #\n# Caffe scope: importing weights at initialization.\n# =========================================================================== #\ndef ssd_arg_scope_caffe(caffe_scope):\n    """"""Caffe scope definition.\n\n    Args:\n      caffe_scope: Caffe scope object with loaded weights.\n\n    Returns:\n      An arg_scope.\n    """"""\n    # Default network arg scope.\n    with slim.arg_scope([slim.conv2d],\n                        activation_fn=tf.nn.relu,\n                        weights_initializer=caffe_scope.conv_weights_init(),\n                        biases_initializer=caffe_scope.conv_biases_init()):\n        with slim.arg_scope([slim.fully_connected],\n                            activation_fn=tf.nn.relu):\n            with slim.arg_scope([custom_layers.l2_normalization],\n                                scale_initializer=caffe_scope.l2_norm_scale_init()):\n                with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                                    padding=\'SAME\') as sc:\n                    return sc\n\n\n# =========================================================================== #\n# SSD loss function.\n# =========================================================================== #\ndef ssd_losses(logits, localisations,\n               gclasses, glocalisations, gscores,\n               match_threshold=0.5,\n               negative_ratio=3.,\n               alpha=1.,\n               label_smoothing=0.,\n               device=\'/cpu:0\',\n               scope=None):\n    with tf.name_scope(scope, \'ssd_losses\'):\n        lshape = tfe.get_shape(logits[0], 5)\n        num_classes = lshape[-1]\n        batch_size = lshape[0]\n\n        # Flatten out all vectors!\n        flogits = []\n        fgclasses = []\n        fgscores = []\n        flocalisations = []\n        fglocalisations = []\n        for i in range(len(logits)):\n            flogits.append(tf.reshape(logits[i], [-1, num_classes]))\n            fgclasses.append(tf.reshape(gclasses[i], [-1]))\n            fgscores.append(tf.reshape(gscores[i], [-1]))\n            flocalisations.append(tf.reshape(localisations[i], [-1, 4]))\n            fglocalisations.append(tf.reshape(glocalisations[i], [-1, 4]))\n        # And concat the crap!\n        logits = tf.concat(flogits, axis=0)\n        gclasses = tf.concat(fgclasses, axis=0)\n        gscores = tf.concat(fgscores, axis=0)\n        localisations = tf.concat(flocalisations, axis=0)\n        glocalisations = tf.concat(fglocalisations, axis=0)\n        dtype = logits.dtype\n\n        # Compute positive matching mask...\n        pmask = gscores > match_threshold\n        fpmask = tf.cast(pmask, dtype)\n        n_positives = tf.reduce_sum(fpmask)\n\n        # Hard negative mining...\n        no_classes = tf.cast(pmask, tf.int32)\n        predictions = slim.softmax(logits)\n        nmask = tf.logical_and(tf.logical_not(pmask),\n                               gscores > -0.5)\n        fnmask = tf.cast(nmask, dtype)\n        nvalues = tf.where(nmask,\n                           predictions[:, 0],\n                           1. - fnmask)\n        nvalues_flat = tf.reshape(nvalues, [-1])\n        # Number of negative entries to select.\n        max_neg_entries = tf.cast(tf.reduce_sum(fnmask), tf.int32)\n        n_neg = tf.cast(negative_ratio * n_positives, tf.int32) + batch_size\n        n_neg = tf.minimum(n_neg, max_neg_entries)\n\n        val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)\n        max_hard_pred = -val[-1]\n        # Final negative mask.\n        nmask = tf.logical_and(nmask, nvalues < max_hard_pred)\n        fnmask = tf.cast(nmask, dtype)\n\n        # Add cross-entropy loss.\n        with tf.name_scope(\'cross_entropy_pos\'):\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                  labels=gclasses)\n            loss = tf.div(tf.reduce_sum(loss * fpmask), batch_size, name=\'value\')\n            tf.losses.add_loss(loss)\n\n        with tf.name_scope(\'cross_entropy_neg\'):\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                  labels=no_classes)\n            loss = tf.div(tf.reduce_sum(loss * fnmask), batch_size, name=\'value\')\n            tf.losses.add_loss(loss)\n\n        # Add localization loss: smooth L1, L2, ...\n        with tf.name_scope(\'localization\'):\n            # Weights Tensor: positive mask + random negative.\n            weights = tf.expand_dims(alpha * fpmask, axis=-1)\n            loss = custom_layers.abs_smooth(localisations - glocalisations)\n            loss = tf.div(tf.reduce_sum(loss * weights), batch_size, name=\'value\')\n            tf.losses.add_loss(loss)\n\n\ndef ssd_losses_old(logits, localisations,\n                   gclasses, glocalisations, gscores,\n                   match_threshold=0.5,\n                   negative_ratio=3.,\n                   alpha=1.,\n                   label_smoothing=0.,\n                   device=\'/cpu:0\',\n                   scope=None):\n    """"""Loss functions for training the SSD 300 VGG network.\n\n    This function defines the different loss components of the SSD, and\n    adds them to the TF loss collection.\n\n    Arguments:\n      logits: (list of) predictions logits Tensors;\n      localisations: (list of) localisations Tensors;\n      gclasses: (list of) groundtruth labels Tensors;\n      glocalisations: (list of) groundtruth localisations Tensors;\n      gscores: (list of) groundtruth score Tensors;\n    """"""\n    with tf.device(device):\n        with tf.name_scope(scope, \'ssd_losses\'):\n            l_cross_pos = []\n            l_cross_neg = []\n            l_loc = []\n            for i in range(len(logits)):\n                dtype = logits[i].dtype\n                with tf.name_scope(\'block_%i\' % i):\n                    # Sizing weight...\n                    wsize = tfe.get_shape(logits[i], rank=5)\n                    wsize = wsize[1] * wsize[2] * wsize[3]\n\n                    # Positive mask.\n                    pmask = gscores[i] > match_threshold\n                    fpmask = tf.cast(pmask, dtype)\n                    n_positives = tf.reduce_sum(fpmask)\n\n                    # Select some random negative entries.\n                    # n_entries = np.prod(gclasses[i].get_shape().as_list())\n                    # r_positive = n_positives / n_entries\n                    # r_negative = negative_ratio * n_positives / (n_entries - n_positives)\n\n                    # Negative mask.\n                    no_classes = tf.cast(pmask, tf.int32)\n                    predictions = slim.softmax(logits[i])\n                    nmask = tf.logical_and(tf.logical_not(pmask),\n                                           gscores[i] > -0.5)\n                    fnmask = tf.cast(nmask, dtype)\n                    nvalues = tf.where(nmask,\n                                       predictions[:, :, :, :, 0],\n                                       1. - fnmask)\n                    nvalues_flat = tf.reshape(nvalues, [-1])\n                    # Number of negative entries to select.\n                    n_neg = tf.cast(negative_ratio * n_positives, tf.int32)\n                    n_neg = tf.maximum(n_neg, tf.size(nvalues_flat) // 8)\n                    n_neg = tf.maximum(n_neg, tf.shape(nvalues)[0] * 4)\n                    max_neg_entries = 1 + tf.cast(tf.reduce_sum(fnmask), tf.int32)\n                    n_neg = tf.minimum(n_neg, max_neg_entries)\n\n                    val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)\n                    max_hard_pred = -val[-1]\n                    # Final negative mask.\n                    nmask = tf.logical_and(nmask, nvalues < max_hard_pred)\n                    fnmask = tf.cast(nmask, dtype)\n\n                    # Add cross-entropy loss.\n                    with tf.name_scope(\'cross_entropy_pos\'):\n                        fpmask = wsize * fpmask\n                        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],\n                                                                              labels=gclasses[i])\n                        loss = tf.losses.compute_weighted_loss(loss, fpmask)\n                        l_cross_pos.append(loss)\n\n                    with tf.name_scope(\'cross_entropy_neg\'):\n                        fnmask = wsize * fnmask\n                        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],\n                                                                              labels=no_classes)\n                        loss = tf.losses.compute_weighted_loss(loss, fnmask)\n                        l_cross_neg.append(loss)\n\n                    # Add localization loss: smooth L1, L2, ...\n                    with tf.name_scope(\'localization\'):\n                        # Weights Tensor: positive mask + random negative.\n                        weights = tf.expand_dims(alpha * fpmask, axis=-1)\n                        loss = custom_layers.abs_smooth(localisations[i] - glocalisations[i])\n                        loss = tf.losses.compute_weighted_loss(loss, weights)\n                        l_loc.append(loss)\n\n            # Additional total losses...\n            with tf.name_scope(\'total\'):\n                total_cross_pos = tf.add_n(l_cross_pos, \'cross_entropy_pos\')\n                total_cross_neg = tf.add_n(l_cross_neg, \'cross_entropy_neg\')\n                total_cross = tf.add(total_cross_pos, total_cross_neg, \'cross_entropy\')\n                total_loc = tf.add_n(l_loc, \'localization\')\n\n                # Add to EXTRA LOSSES TF.collection\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_cross_pos)\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_cross_neg)\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_cross)\n                tf.add_to_collection(\'EXTRA_LOSSES\', total_loc)\n'"
nets/ssd_vgg_512.py,50,"b'# Copyright 2016 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Definition of 512 VGG-based SSD network.\n\nThis model was initially introduced in:\nSSD: Single Shot MultiBox Detector\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,\nCheng-Yang Fu, Alexander C. Berg\nhttps://arxiv.org/abs/1512.02325\n\nTwo variants of the model are defined: the 300x300 and 512x512 models, the\nlatter obtaining a slightly better accuracy on Pascal VOC.\n\nUsage:\n    with slim.arg_scope(ssd_vgg.ssd_vgg()):\n        outputs, end_points = ssd_vgg.ssd_vgg(inputs)\n@@ssd_vgg\n""""""\nimport math\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_extended as tfe\nfrom nets import custom_layers\nfrom nets import ssd_common\nfrom nets import ssd_vgg_300\n\nslim = tf.contrib.slim\n\n\n# =========================================================================== #\n# SSD class definition.\n# =========================================================================== #\nSSDParams = namedtuple(\'SSDParameters\', [\'img_shape\',\n                                         \'num_classes\',\n                                         \'no_annotation_label\',\n                                         \'feat_layers\',\n                                         \'feat_shapes\',\n                                         \'anchor_size_bounds\',\n                                         \'anchor_sizes\',\n                                         \'anchor_ratios\',\n                                         \'anchor_steps\',\n                                         \'anchor_offset\',\n                                         \'normalizations\',\n                                         \'prior_scaling\'\n                                         ])\n\n\nclass SSDNet(object):\n    """"""Implementation of the SSD VGG-based 512 network.\n\n    The default features layers with 512x512 image input are:\n      conv4 ==> 64 x 64\n      conv7 ==> 32 x 32\n      conv8 ==> 16 x 16\n      conv9 ==> 8 x 8\n      conv10 ==> 4 x 4\n      conv11 ==> 2 x 2\n      conv12 ==> 1 x 1\n    The default image size used to train this network is 512x512.\n    """"""\n    default_params = SSDParams(\n        img_shape=(512, 512),\n        num_classes=21,\n        no_annotation_label=21,\n        feat_layers=[\'block4\', \'block7\', \'block8\', \'block9\', \'block10\', \'block11\', \'block12\'],\n        feat_shapes=[(64, 64), (32, 32), (16, 16), (8, 8), (4, 4), (2, 2), (1, 1)],\n        anchor_size_bounds=[0.10, 0.90],\n        anchor_sizes=[(20.48, 51.2),\n                      (51.2, 133.12),\n                      (133.12, 215.04),\n                      (215.04, 296.96),\n                      (296.96, 378.88),\n                      (378.88, 460.8),\n                      (460.8, 542.72)],\n        anchor_ratios=[[2, .5],\n                       [2, .5, 3, 1./3],\n                       [2, .5, 3, 1./3],\n                       [2, .5, 3, 1./3],\n                       [2, .5, 3, 1./3],\n                       [2, .5],\n                       [2, .5]],\n        anchor_steps=[8, 16, 32, 64, 128, 256, 512],\n        anchor_offset=0.5,\n        normalizations=[20, -1, -1, -1, -1, -1, -1],\n        prior_scaling=[0.1, 0.1, 0.2, 0.2]\n        )\n\n    def __init__(self, params=None):\n        """"""Init the SSD net with some parameters. Use the default ones\n        if none provided.\n        """"""\n        if isinstance(params, SSDParams):\n            self.params = params\n        else:\n            self.params = SSDNet.default_params\n\n    # ======================================================================= #\n    def net(self, inputs,\n            is_training=True,\n            update_feat_shapes=True,\n            dropout_keep_prob=0.5,\n            prediction_fn=slim.softmax,\n            reuse=None,\n            scope=\'ssd_512_vgg\'):\n        """"""Network definition.\n        """"""\n        r = ssd_net(inputs,\n                    num_classes=self.params.num_classes,\n                    feat_layers=self.params.feat_layers,\n                    anchor_sizes=self.params.anchor_sizes,\n                    anchor_ratios=self.params.anchor_ratios,\n                    normalizations=self.params.normalizations,\n                    is_training=is_training,\n                    dropout_keep_prob=dropout_keep_prob,\n                    prediction_fn=prediction_fn,\n                    reuse=reuse,\n                    scope=scope)\n        # Update feature shapes (try at least!)\n        if update_feat_shapes:\n            shapes = ssd_feat_shapes_from_net(r[0], self.params.feat_shapes)\n            self.params = self.params._replace(feat_shapes=shapes)\n        return r\n\n    def arg_scope(self, weight_decay=0.0005, data_format=\'NHWC\'):\n        """"""Network arg_scope.\n        """"""\n        return ssd_arg_scope(weight_decay, data_format=data_format)\n\n    def arg_scope_caffe(self, caffe_scope):\n        """"""Caffe arg_scope used for weights importing.\n        """"""\n        return ssd_arg_scope_caffe(caffe_scope)\n\n    # ======================================================================= #\n    def anchors(self, img_shape, dtype=np.float32):\n        """"""Compute the default anchor boxes, given an image shape.\n        """"""\n        return ssd_anchors_all_layers(img_shape,\n                                      self.params.feat_shapes,\n                                      self.params.anchor_sizes,\n                                      self.params.anchor_ratios,\n                                      self.params.anchor_steps,\n                                      self.params.anchor_offset,\n                                      dtype)\n\n    def bboxes_encode(self, labels, bboxes, anchors,\n                      scope=None):\n        """"""Encode labels and bounding boxes.\n        """"""\n        return ssd_common.tf_ssd_bboxes_encode(\n            labels, bboxes, anchors,\n            self.params.num_classes,\n            self.params.no_annotation_label,\n            ignore_threshold=0.5,\n            prior_scaling=self.params.prior_scaling,\n            scope=scope)\n\n    def bboxes_decode(self, feat_localizations, anchors,\n                      scope=\'ssd_bboxes_decode\'):\n        """"""Encode labels and bounding boxes.\n        """"""\n        return ssd_common.tf_ssd_bboxes_decode(\n            feat_localizations, anchors,\n            prior_scaling=self.params.prior_scaling,\n            scope=scope)\n\n    def detected_bboxes(self, predictions, localisations,\n                        select_threshold=None, nms_threshold=0.5,\n                        clipping_bbox=None, top_k=400, keep_top_k=200):\n        """"""Get the detected bounding boxes from the SSD network output.\n        """"""\n        # Select top_k bboxes from predictions, and clip\n        rscores, rbboxes = \\\n            ssd_common.tf_ssd_bboxes_select(predictions, localisations,\n                                            select_threshold=select_threshold,\n                                            num_classes=self.params.num_classes)\n        rscores, rbboxes = \\\n            tfe.bboxes_sort(rscores, rbboxes, top_k=top_k)\n        # Apply NMS algorithm.\n        rscores, rbboxes = \\\n            tfe.bboxes_nms_batch(rscores, rbboxes,\n                                 nms_threshold=nms_threshold,\n                                 keep_top_k=keep_top_k)\n        # if clipping_bbox is not None:\n        #     rbboxes = tfe.bboxes_clip(clipping_bbox, rbboxes)\n        return rscores, rbboxes\n\n    def losses(self, logits, localisations,\n               gclasses, glocalisations, gscores,\n               match_threshold=0.5,\n               negative_ratio=3.,\n               alpha=1.,\n               label_smoothing=0.,\n               scope=\'ssd_losses\'):\n        """"""Define the SSD network losses.\n        """"""\n        return ssd_losses(logits, localisations,\n                          gclasses, glocalisations, gscores,\n                          match_threshold=match_threshold,\n                          negative_ratio=negative_ratio,\n                          alpha=alpha,\n                          label_smoothing=label_smoothing,\n                          scope=scope)\n\n\n# =========================================================================== #\n# SSD tools...\n# =========================================================================== #\ndef layer_shape(layer):\n    """"""Returns the dimensions of a 4D layer tensor.\n    Args:\n      layer: A 4-D Tensor of shape `[height, width, channels]`.\n    Returns:\n      Dimensions that are statically known are python integers,\n        otherwise they are integer scalar tensors.\n    """"""\n    if layer.get_shape().is_fully_defined():\n        return layer.get_shape().as_list()\n    else:\n        static_shape = layer.get_shape().with_rank(4).as_list()\n        dynamic_shape = tf.unstack(tf.shape(layer), 3)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef ssd_size_bounds_to_values(size_bounds,\n                              n_feat_layers,\n                              img_shape=(512, 512)):\n    """"""Compute the reference sizes of the anchor boxes from relative bounds.\n    The absolute values are measured in pixels, based on the network\n    default size (512 pixels).\n\n    This function follows the computation performed in the original\n    implementation of SSD in Caffe.\n\n    Return:\n      list of list containing the absolute sizes at each scale. For each scale,\n      the ratios only apply to the first value.\n    """"""\n    assert img_shape[0] == img_shape[1]\n\n    img_size = img_shape[0]\n    min_ratio = int(size_bounds[0] * 100)\n    max_ratio = int(size_bounds[1] * 100)\n    step = int(math.floor((max_ratio - min_ratio) / (n_feat_layers - 2)))\n    # Start with the following smallest sizes.\n    sizes = [[img_size * 0.04, img_size * 0.1]]\n    for ratio in range(min_ratio, max_ratio + 1, step):\n        sizes.append((img_size * ratio / 100.,\n                      img_size * (ratio + step) / 100.))\n    return sizes\n\n\ndef ssd_feat_shapes_from_net(predictions, default_shapes=None):\n    """"""Try to obtain the feature shapes from the prediction layers.\n\n    Return:\n      list of feature shapes. Default values if predictions shape not fully\n      determined.\n    """"""\n    feat_shapes = []\n    for l in predictions:\n        shape = l.get_shape().as_list()[1:4]\n        if None in shape:\n            return default_shapes\n        else:\n            feat_shapes.append(shape)\n    return feat_shapes\n\n\ndef ssd_anchor_one_layer(img_shape,\n                         feat_shape,\n                         sizes,\n                         ratios,\n                         step,\n                         offset=0.5,\n                         dtype=np.float32):\n    """"""Computer SSD default anchor boxes for one feature layer.\n\n    Determine the relative position grid of the centers, and the relative\n    width and height.\n\n    Arguments:\n      feat_shape: Feature shape, used for computing relative position grids;\n      size: Absolute reference sizes;\n      ratios: Ratios to use on these features;\n      img_shape: Image shape, used for computing height, width relatively to the\n        former;\n      offset: Grid offset.\n\n    Return:\n      y, x, h, w: Relative x and y grids, and height and width.\n    """"""\n    # Compute the position grid: simple way.\n    # y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n    # y = (y.astype(dtype) + offset) / feat_shape[0]\n    # x = (x.astype(dtype) + offset) / feat_shape[1]\n    # Weird SSD-Caffe computation using steps values...\n    y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n    y = (y.astype(dtype) + offset) * step / img_shape[0]\n    x = (x.astype(dtype) + offset) * step / img_shape[1]\n\n    # Expand dims to support easy broadcasting.\n    y = np.expand_dims(y, axis=-1)\n    x = np.expand_dims(x, axis=-1)\n\n    # Compute relative height and width.\n    # Tries to follow the original implementation of SSD for the order.\n    num_anchors = len(sizes) + len(ratios)\n    h = np.zeros((num_anchors, ), dtype=dtype)\n    w = np.zeros((num_anchors, ), dtype=dtype)\n    # Add first anchor boxes with ratio=1.\n    h[0] = sizes[0] / img_shape[0]\n    w[0] = sizes[0] / img_shape[1]\n    di = 1\n    if len(sizes) > 1:\n        h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]\n        w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]\n        di += 1\n    for i, r in enumerate(ratios):\n        h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)\n        w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)\n    return y, x, h, w\n\n\ndef ssd_anchors_all_layers(img_shape,\n                           layers_shape,\n                           anchor_sizes,\n                           anchor_ratios,\n                           anchor_steps,\n                           offset=0.5,\n                           dtype=np.float32):\n    """"""Compute anchor boxes for all feature layers.\n    """"""\n    layers_anchors = []\n    for i, s in enumerate(layers_shape):\n        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,\n                                             anchor_sizes[i],\n                                             anchor_ratios[i],\n                                             anchor_steps[i],\n                                             offset=offset, dtype=dtype)\n        layers_anchors.append(anchor_bboxes)\n    return layers_anchors\n\n\n# =========================================================================== #\n# Functional definition of VGG-based SSD 512.\n# =========================================================================== #\ndef ssd_net(inputs,\n            num_classes=SSDNet.default_params.num_classes,\n            feat_layers=SSDNet.default_params.feat_layers,\n            anchor_sizes=SSDNet.default_params.anchor_sizes,\n            anchor_ratios=SSDNet.default_params.anchor_ratios,\n            normalizations=SSDNet.default_params.normalizations,\n            is_training=True,\n            dropout_keep_prob=0.5,\n            prediction_fn=slim.softmax,\n            reuse=None,\n            scope=\'ssd_512_vgg\'):\n    """"""SSD net definition.\n    """"""\n    # End_points collect relevant activations for external use.\n    end_points = {}\n    with tf.variable_scope(scope, \'ssd_512_vgg\', [inputs], reuse=reuse):\n        # Original VGG-16 blocks.\n        net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n        end_points[\'block1\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n        # Block 2.\n        net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n        end_points[\'block2\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n        # Block 3.\n        net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n        end_points[\'block3\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n        # Block 4.\n        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n        end_points[\'block4\'] = net\n        net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n        # Block 5.\n        net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n        end_points[\'block5\'] = net\n        net = slim.max_pool2d(net, [3, 3], 1, scope=\'pool5\')\n\n        # Additional SSD blocks.\n        # Block 6: let\'s dilate the hell out of it!\n        net = slim.conv2d(net, 1024, [3, 3], rate=6, scope=\'conv6\')\n        end_points[\'block6\'] = net\n        # Block 7: 1x1 conv. Because the fuck.\n        net = slim.conv2d(net, 1024, [1, 1], scope=\'conv7\')\n        end_points[\'block7\'] = net\n\n        # Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).\n        end_point = \'block8\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 256, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 512, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block9\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 256, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block10\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 256, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block11\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 256, [3, 3], stride=2, scope=\'conv3x3\', padding=\'VALID\')\n        end_points[end_point] = net\n        end_point = \'block12\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(net, 128, [1, 1], scope=\'conv1x1\')\n            net = custom_layers.pad2d(net, pad=(1, 1))\n            net = slim.conv2d(net, 256, [4, 4], scope=\'conv4x4\', padding=\'VALID\')\n            # Fix padding to match Caffe version (pad=1).\n            # pad_shape = [(i-j) for i, j in zip(layer_shape(net), [0, 1, 1, 0])]\n            # net = tf.slice(net, [0, 0, 0, 0], pad_shape, name=\'caffe_pad\')\n        end_points[end_point] = net\n\n        # Prediction and localisations layers.\n        predictions = []\n        logits = []\n        localisations = []\n        for i, layer in enumerate(feat_layers):\n            with tf.variable_scope(layer + \'_box\'):\n                p, l = ssd_vgg_300.ssd_multibox_layer(end_points[layer],\n                                                      num_classes,\n                                                      anchor_sizes[i],\n                                                      anchor_ratios[i],\n                                                      normalizations[i])\n            predictions.append(prediction_fn(p))\n            logits.append(p)\n            localisations.append(l)\n\n        return predictions, localisations, logits, end_points\nssd_net.default_image_size = 512\n\n\ndef ssd_arg_scope(weight_decay=0.0005, data_format=\'NHWC\'):\n    """"""Defines the VGG arg scope.\n\n    Args:\n      weight_decay: The l2 regularization coefficient.\n\n    Returns:\n      An arg_scope.\n    """"""\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n                        biases_initializer=tf.zeros_initializer()):\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                            padding=\'SAME\',\n                            data_format=data_format):\n            with slim.arg_scope([custom_layers.pad2d,\n                                 custom_layers.l2_normalization,\n                                 custom_layers.channel_to_last],\n                                data_format=data_format) as sc:\n                return sc\n\n\n# =========================================================================== #\n# Caffe scope: importing weights at initialization.\n# =========================================================================== #\ndef ssd_arg_scope_caffe(caffe_scope):\n    """"""Caffe scope definition.\n\n    Args:\n      caffe_scope: Caffe scope object with loaded weights.\n\n    Returns:\n      An arg_scope.\n    """"""\n    # Default network arg scope.\n    with slim.arg_scope([slim.conv2d],\n                        activation_fn=tf.nn.relu,\n                        weights_initializer=caffe_scope.conv_weights_init(),\n                        biases_initializer=caffe_scope.conv_biases_init()):\n        with slim.arg_scope([slim.fully_connected],\n                            activation_fn=tf.nn.relu):\n            with slim.arg_scope([custom_layers.l2_normalization],\n                                scale_initializer=caffe_scope.l2_norm_scale_init()):\n                with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                                    padding=\'SAME\') as sc:\n                    return sc\n\n\n# =========================================================================== #\n# SSD loss function.\n# =========================================================================== #\ndef ssd_losses(logits, localisations,\n               gclasses, glocalisations, gscores,\n               match_threshold=0.5,\n               negative_ratio=3.,\n               alpha=1.,\n               label_smoothing=0.,\n               scope=None):\n    """"""Loss functions for training the SSD 300 VGG network.\n\n    This function defines the different loss components of the SSD, and\n    adds them to the TF loss collection.\n\n    Arguments:\n      logits: (list of) predictions logits Tensors;\n      localisations: (list of) localisations Tensors;\n      gclasses: (list of) groundtruth labels Tensors;\n      glocalisations: (list of) groundtruth localisations Tensors;\n      gscores: (list of) groundtruth score Tensors;\n    """"""\n    with tf.name_scope(scope, \'ssd_losses\'):\n        l_cross_pos = []\n        l_cross_neg = []\n        l_loc = []\n        for i in range(len(logits)):\n            dtype = logits[i].dtype\n            with tf.name_scope(\'block_%i\' % i):\n                # Determine weights Tensor.\n                pmask = gscores[i] > match_threshold\n                fpmask = tf.cast(pmask, dtype)\n                n_positives = tf.reduce_sum(fpmask)\n\n                # Select some random negative entries.\n                # n_entries = np.prod(gclasses[i].get_shape().as_list())\n                # r_positive = n_positives / n_entries\n                # r_negative = negative_ratio * n_positives / (n_entries - n_positives)\n\n                # Negative mask.\n                no_classes = tf.cast(pmask, tf.int32)\n                predictions = slim.softmax(logits[i])\n                nmask = tf.logical_and(tf.logical_not(pmask),\n                                       gscores[i] > -0.5)\n                fnmask = tf.cast(nmask, dtype)\n                nvalues = tf.where(nmask,\n                                   predictions[:, :, :, :, 0],\n                                   1. - fnmask)\n                nvalues_flat = tf.reshape(nvalues, [-1])\n                # Number of negative entries to select.\n                n_neg = tf.cast(negative_ratio * n_positives, tf.int32)\n                n_neg = tf.maximum(n_neg, tf.size(nvalues_flat) // 8)\n                n_neg = tf.maximum(n_neg, tf.shape(nvalues)[0] * 4)\n                max_neg_entries = 1 + tf.cast(tf.reduce_sum(fnmask), tf.int32)\n                n_neg = tf.minimum(n_neg, max_neg_entries)\n\n                val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)\n                minval = val[-1]\n                # Final negative mask.\n                nmask = tf.logical_and(nmask, -nvalues > minval)\n                fnmask = tf.cast(nmask, dtype)\n\n                # Add cross-entropy loss.\n                with tf.name_scope(\'cross_entropy_pos\'):\n                    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],\n                                                                          labels=gclasses[i])\n                    loss = tf.losses.compute_weighted_loss(loss, fpmask)\n                    l_cross_pos.append(loss)\n\n                with tf.name_scope(\'cross_entropy_neg\'):\n                    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],\n                                                                          labels=no_classes)\n                    loss = tf.losses.compute_weighted_loss(loss, fnmask)\n                    l_cross_neg.append(loss)\n\n                # Add localization loss: smooth L1, L2, ...\n                with tf.name_scope(\'localization\'):\n                    # Weights Tensor: positive mask + random negative.\n                    weights = tf.expand_dims(alpha * fpmask, axis=-1)\n                    loss = custom_layers.abs_smooth(localisations[i] - glocalisations[i])\n                    loss = tf.losses.compute_weighted_loss(loss, weights)\n                    l_loc.append(loss)\n\n        # Additional total losses...\n        with tf.name_scope(\'total\'):\n            total_cross_pos = tf.add_n(l_cross_pos, \'cross_entropy_pos\')\n            total_cross_neg = tf.add_n(l_cross_neg, \'cross_entropy_neg\')\n            total_cross = tf.add(total_cross_pos, total_cross_neg, \'cross_entropy\')\n            total_loc = tf.add_n(l_loc, \'localization\')\n\n            # Add to EXTRA LOSSES TF.collection\n            tf.add_to_collection(\'EXTRA_LOSSES\', total_cross_pos)\n            tf.add_to_collection(\'EXTRA_LOSSES\', total_cross_neg)\n            tf.add_to_collection(\'EXTRA_LOSSES\', total_cross)\n            tf.add_to_collection(\'EXTRA_LOSSES\', total_loc)\n'"
nets/vgg.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
nets/xception.py,31,"b'""""""Definition of Xception model introduced by F. Chollet.\n\nUsage:\n  with slim.arg_scope(xception.xception_arg_scope()):\n    outputs, end_points = xception.xception(inputs)\n@@xception\n""""""\n\nimport tensorflow as tf\nslim = tf.contrib.slim\n\n\n# =========================================================================== #\n# Xception implementation (clean)\n# =========================================================================== #\ndef xception(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             reuse=None,\n             scope=\'xception\'):\n    """"""Xception model from https://arxiv.org/pdf/1610.02357v2.pdf\n\n    The default image size used to train this network is 299x299.\n    """"""\n\n    # end_points collect relevant activations for external use, for example\n    # summaries or losses.\n    end_points = {}\n\n    with tf.variable_scope(scope, \'xception\', [inputs]):\n        # Block 1.\n        end_point = \'block1\'\n        with tf.variable_scope(end_point):\n            net = slim.conv2d(inputs, 32, [3, 3], stride=2, padding=\'VALID\', scope=\'conv1\')\n            net = slim.conv2d(net, 64, [3, 3], padding=\'VALID\', scope=\'conv2\')\n        end_points[end_point] = net\n\n        # Residual block 2.\n        end_point = \'block2\'\n        with tf.variable_scope(end_point):\n            res = slim.conv2d(net, 128, [1, 1], stride=2, activation_fn=None, scope=\'res\')\n            net = slim.separable_convolution2d(net, 128, [3, 3], 1, scope=\'sepconv1\')\n            net = slim.separable_convolution2d(net, 128, [3, 3], 1, activation_fn=None, scope=\'sepconv2\')\n            net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool\')\n            net = res + net\n        end_points[end_point] = net\n\n        # Residual block 3.\n        end_point = \'block3\'\n        with tf.variable_scope(end_point):\n            res = slim.conv2d(net, 256, [1, 1], stride=2, activation_fn=None, scope=\'res\')\n            net = tf.nn.relu(net)\n            net = slim.separable_convolution2d(net, 256, [3, 3], 1, scope=\'sepconv1\')\n            net = slim.separable_convolution2d(net, 256, [3, 3], 1, activation_fn=None, scope=\'sepconv2\')\n            net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool\')\n            net = res + net\n        end_points[end_point] = net\n\n        # Residual block 4.\n        end_point = \'block4\'\n        with tf.variable_scope(end_point):\n            res = slim.conv2d(net, 728, [1, 1], stride=2, activation_fn=None, scope=\'res\')\n            net = tf.nn.relu(net)\n            net = slim.separable_convolution2d(net, 728, [3, 3], 1, scope=\'sepconv1\')\n            net = slim.separable_convolution2d(net, 728, [3, 3], 1, activation_fn=None, scope=\'sepconv2\')\n            net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool\')\n            net = res + net\n        end_points[end_point] = net\n\n        # Middle flow blocks.\n        for i in range(8):\n            end_point = \'block\' + str(i + 5)\n            with tf.variable_scope(end_point):\n                res = net\n                net = tf.nn.relu(net)\n                net = slim.separable_convolution2d(net, 728, [3, 3], 1, activation_fn=None,\n                                                   scope=\'sepconv1\')\n                net = tf.nn.relu(net)\n                net = slim.separable_convolution2d(net, 728, [3, 3], 1, activation_fn=None,\n                                                   scope=\'sepconv2\')\n                net = tf.nn.relu(net)\n                net = slim.separable_convolution2d(net, 728, [3, 3], 1, activation_fn=None,\n                                                   scope=\'sepconv3\')\n                net = res + net\n            end_points[end_point] = net\n\n        # Exit flow: blocks 13 and 14.\n        end_point = \'block13\'\n        with tf.variable_scope(end_point):\n            res = slim.conv2d(net, 1024, [1, 1], stride=2, activation_fn=None, scope=\'res\')\n            net = tf.nn.relu(net)\n            net = slim.separable_convolution2d(net, 728, [3, 3], 1, activation_fn=None, scope=\'sepconv1\')\n            net = tf.nn.relu(net)\n            net = slim.separable_convolution2d(net, 1024, [3, 3], 1, activation_fn=None, scope=\'sepconv2\')\n            net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool\')\n            net = res + net\n        end_points[end_point] = net\n\n        end_point = \'block14\'\n        with tf.variable_scope(end_point):\n            net = slim.separable_convolution2d(net, 1536, [3, 3], 1, scope=\'sepconv1\')\n            net = slim.separable_convolution2d(net, 2048, [3, 3], 1, scope=\'sepconv2\')\n        end_points[end_point] = net\n\n        # Global averaging.\n        end_point = \'dense\'\n        with tf.variable_scope(end_point):\n            net = tf.reduce_mean(net, [1, 2], name=\'reduce_avg\')\n            logits = slim.fully_connected(net, 1000, activation_fn=None)\n\n            end_points[\'logits\'] = logits\n            end_points[\'predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n        return logits, end_points\nxception.default_image_size = 299\n\n\ndef xception_arg_scope(weight_decay=0.00001, stddev=0.1):\n    """"""Defines the default Xception arg scope.\n\n    Args:\n      weight_decay: The weight decay to use for regularizing the model.\n      stddev: The standard deviation of the trunctated normal weight initializer.\n\n    Returns:\n      An `arg_scope` to use for the xception model.\n    """"""\n    batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': 0.9997,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n    }\n\n    # Set weight_decay for weights in Conv and FC layers.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_convolution2d],\n                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n        with slim.arg_scope(\n                [slim.conv2d, slim.separable_convolution2d],\n                padding=\'SAME\',\n                weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode=\'FAN_IN\', uniform=False),\n                activation_fn=tf.nn.relu,\n                normalizer_fn=slim.batch_norm,\n                normalizer_params=batch_norm_params):\n            with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as sc:\n                return sc\n\n\n# =========================================================================== #\n# Xception arg scope (Keras hack!)\n# =========================================================================== #\ndef xception_keras_arg_scope(hdf5_file, weight_decay=0.00001):\n    """"""Defines an Xception arg scope which initialize layers weights\n    using a Keras HDF5 file.\n\n    Quite hacky implementaion, but seems to be working!\n\n    Args:\n      hdf5_file: HDF5 file handle.\n      weight_decay: The weight decay to use for regularizing the model.\n\n    Returns:\n      An `arg_scope` to use for the xception model.\n    """"""\n    # Default batch normalization parameters.\n    batch_norm_params = {\n        \'center\': True,\n        \'scale\': False,\n        \'decay\': 0.9997,\n        \'epsilon\': 0.001,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n    }\n\n    # Read weights from HDF5 file.\n    def keras_bn_params():\n        def _beta_initializer(shape, dtype, partition_info=None):\n            keras_bn_params.bidx += 1\n            k = \'batchnormalization_%i\' % keras_bn_params.bidx\n            kb = \'batchnormalization_%i_beta:0\' % keras_bn_params.bidx\n            return tf.cast(hdf5_file[k][kb][:], dtype)\n\n        def _gamma_initializer(shape, dtype, partition_info=None):\n            keras_bn_params.gidx += 1\n            k = \'batchnormalization_%i\' % keras_bn_params.gidx\n            kg = \'batchnormalization_%i_gamma:0\' % keras_bn_params.gidx\n            return tf.cast(hdf5_file[k][kg][:], dtype)\n\n        def _mean_initializer(shape, dtype, partition_info=None):\n            keras_bn_params.midx += 1\n            k = \'batchnormalization_%i\' % keras_bn_params.midx\n            km = \'batchnormalization_%i_running_mean:0\' % keras_bn_params.midx\n            return tf.cast(hdf5_file[k][km][:], dtype)\n\n        def _variance_initializer(shape, dtype, partition_info=None):\n            keras_bn_params.vidx += 1\n            k = \'batchnormalization_%i\' % keras_bn_params.vidx\n            kv = \'batchnormalization_%i_running_std:0\' % keras_bn_params.vidx\n            return tf.cast(hdf5_file[k][kv][:], dtype)\n\n        # Batch normalisation initializers.\n        params = batch_norm_params.copy()\n        params[\'initializers\'] = {\n            \'beta\': _beta_initializer,\n            \'gamma\': _gamma_initializer,\n            \'moving_mean\': _mean_initializer,\n            \'moving_variance\': _variance_initializer,\n        }\n        return params\n    keras_bn_params.bidx = 0\n    keras_bn_params.gidx = 0\n    keras_bn_params.midx = 0\n    keras_bn_params.vidx = 0\n\n    def keras_conv2d_weights():\n        def _initializer(shape, dtype, partition_info=None):\n            keras_conv2d_weights.idx += 1\n            k = \'convolution2d_%i\' % keras_conv2d_weights.idx\n            kw = \'convolution2d_%i_W:0\' % keras_conv2d_weights.idx\n            return tf.cast(hdf5_file[k][kw][:], dtype)\n        return _initializer\n    keras_conv2d_weights.idx = 0\n\n    def keras_sep_conv2d_weights():\n        def _initializer(shape, dtype, partition_info=None):\n            # Depthwise or Pointwise convolution?\n            if shape[0] > 1 or shape[1] > 1:\n                keras_sep_conv2d_weights.didx += 1\n                k = \'separableconvolution2d_%i\' % keras_sep_conv2d_weights.didx\n                kd = \'separableconvolution2d_%i_depthwise_kernel:0\' % keras_sep_conv2d_weights.didx\n                weights = hdf5_file[k][kd][:]\n            else:\n                keras_sep_conv2d_weights.pidx += 1\n                k = \'separableconvolution2d_%i\' % keras_sep_conv2d_weights.pidx\n                kp = \'separableconvolution2d_%i_pointwise_kernel:0\' % keras_sep_conv2d_weights.pidx\n                weights = hdf5_file[k][kp][:]\n            return tf.cast(weights, dtype)\n        return _initializer\n    keras_sep_conv2d_weights.didx = 0\n    keras_sep_conv2d_weights.pidx = 0\n\n    def keras_dense_weights():\n        def _initializer(shape, dtype, partition_info=None):\n            keras_dense_weights.idx += 1\n            k = \'dense_%i\' % keras_dense_weights.idx\n            kw = \'dense_%i_W:0\' % keras_dense_weights.idx\n            return tf.cast(hdf5_file[k][kw][:], dtype)\n        return _initializer\n    keras_dense_weights.idx = 1\n\n    def keras_dense_biases():\n        def _initializer(shape, dtype, partition_info=None):\n            keras_dense_biases.idx += 1\n            k = \'dense_%i\' % keras_dense_biases.idx\n            kb = \'dense_%i_b:0\' % keras_dense_biases.idx\n            return tf.cast(hdf5_file[k][kb][:], dtype)\n        return _initializer\n    keras_dense_biases.idx = 1\n\n    # Default network arg scope.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_convolution2d],\n                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n        with slim.arg_scope(\n                [slim.conv2d, slim.separable_convolution2d],\n                padding=\'SAME\',\n                activation_fn=tf.nn.relu,\n                normalizer_fn=slim.batch_norm,\n                normalizer_params=keras_bn_params()):\n            with slim.arg_scope([slim.max_pool2d], padding=\'SAME\'):\n\n                # Weights initializers from Keras weights.\n                with slim.arg_scope([slim.conv2d],\n                                    weights_initializer=keras_conv2d_weights()):\n                    with slim.arg_scope([slim.separable_convolution2d],\n                                        weights_initializer=keras_sep_conv2d_weights()):\n                        with slim.arg_scope([slim.fully_connected],\n                                            weights_initializer=keras_dense_weights(),\n                                            biases_initializer=keras_dense_biases()) as sc:\n                            return sc\n\n'"
notebooks/visualization.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport cv2\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.cm as mpcm\n\n\n# =========================================================================== #\n# Some colormaps.\n# =========================================================================== #\ndef colors_subselect(colors, num_classes=21):\n    dt = len(colors) // num_classes\n    sub_colors = []\n    for i in range(num_classes):\n        color = colors[i*dt]\n        if isinstance(color[0], float):\n            sub_colors.append([int(c * 255) for c in color])\n        else:\n            sub_colors.append([c for c in color])\n    return sub_colors\n\ncolors_plasma = colors_subselect(mpcm.plasma.colors, num_classes=21)\ncolors_tableau = [(255, 255, 255), (31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n                  (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n                  (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n                  (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n                  (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]\n\n\n# =========================================================================== #\n# OpenCV drawing.\n# =========================================================================== #\ndef draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n    """"""Draw a collection of lines on an image.\n    """"""\n    for line in lines:\n        for x1, y1, x2, y2 in line:\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n\n\ndef draw_rectangle(img, p1, p2, color=[255, 0, 0], thickness=2):\n    cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n\n\ndef draw_bbox(img, bbox, shape, label, color=[255, 0, 0], thickness=2):\n    p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n    p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n    cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n    p1 = (p1[0]+15, p1[1])\n    cv2.putText(img, str(label), p1[::-1], cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)\n\n\ndef bboxes_draw_on_img(img, classes, scores, bboxes, colors, thickness=2):\n    shape = img.shape\n    for i in range(bboxes.shape[0]):\n        bbox = bboxes[i]\n        color = colors[classes[i]]\n        # Draw bounding box...\n        p1 = (int(bbox[0] * shape[0]), int(bbox[1] * shape[1]))\n        p2 = (int(bbox[2] * shape[0]), int(bbox[3] * shape[1]))\n        cv2.rectangle(img, p1[::-1], p2[::-1], color, thickness)\n        # Draw text...\n        s = \'%s/%.3f\' % (classes[i], scores[i])\n        p1 = (p1[0]-5, p1[1])\n        cv2.putText(img, s, p1[::-1], cv2.FONT_HERSHEY_DUPLEX, 0.4, color, 1)\n\n\n# =========================================================================== #\n# Matplotlib show...\n# =========================================================================== #\ndef plt_bboxes(img, classes, scores, bboxes, figsize=(10,10), linewidth=1.5):\n    """"""Visualize bounding boxes. Largely inspired by SSD-MXNET!\n    """"""\n    fig = plt.figure(figsize=figsize)\n    plt.imshow(img)\n    height = img.shape[0]\n    width = img.shape[1]\n    colors = dict()\n    for i in range(classes.shape[0]):\n        cls_id = int(classes[i])\n        if cls_id >= 0:\n            score = scores[i]\n            if cls_id not in colors:\n                colors[cls_id] = (random.random(), random.random(), random.random())\n            ymin = int(bboxes[i, 0] * height)\n            xmin = int(bboxes[i, 1] * width)\n            ymax = int(bboxes[i, 2] * height)\n            xmax = int(bboxes[i, 3] * width)\n            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n                                 ymax - ymin, fill=False,\n                                 edgecolor=colors[cls_id],\n                                 linewidth=linewidth)\n            plt.gca().add_patch(rect)\n            class_name = str(cls_id)\n            plt.gca().text(xmin, ymin - 2,\n                           \'{:s} | {:.3f}\'.format(class_name, score),\n                           bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n                           fontsize=12, color=\'white\')\n    plt.show()\n'"
preprocessing/__init__.py,0,b'\n'
preprocessing/inception_preprocessing.py,62,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n    Args:\n        x: input Tensor.\n        func: Python function to apply.\n        num_cases: Python int32, number of cases to sample sel from.\n\n    Returns:\n        The result of func(x, sel), where func receives the value of the\n        selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n            func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n            for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n\n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n\n    Args:\n        image: 3-D Tensor containing single image in [0, 1].\n        color_ordering: Python int, a type of distortion (valid values: 0-3).\n        fast_mode: Avoids slower ops (random_hue and random_contrast)\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n        ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n        image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged\n            as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n            image.\n        min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n            area of the image must contain at least this fraction of any bounding box\n            supplied.\n        aspect_ratio_range: An optional list of `floats`. The cropped area of the\n            image must have an aspect ratio = width / height within this range.\n        area_range: An optional list of `floats`. The cropped area of the image\n            must contain a fraction of the supplied image within in this range.\n        max_attempts: An optional `int`. Number of attempts at generating a cropped\n            region of the image of the specified constraints. After `max_attempts`\n            failures, return the entire image.\n        scope: Optional scope for name_scope.\n    Returns:\n        A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n                tf.shape(image),\n                bounding_boxes=bbox,\n                min_object_covered=min_object_covered,\n                aspect_ratio_range=aspect_ratio_range,\n                area_range=area_range,\n                max_attempts=max_attempts,\n                use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True, scope=None):\n    """"""Distort one image for training a network.\n\n    Distorting images provides a useful technique for augmenting the data\n    set during training in order to make the network invariant to aspects\n    of the image that do not effect the label.\n\n    Additionally it would create image_summaries to display the different\n    transformations applied to the image.\n\n    Args:\n        image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n            [0, 1], otherwise it would converted to tf.float32 assuming that the range\n            is [0, MAX], where MAX is largest positive representable number for\n            int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n        height: integer\n        width: integer\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged\n            as [ymin, xmin, ymax, xmax].\n        fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n            bi-cubic resizing, random_hue or random_contrast).\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D float Tensor of distorted image used for training with range [-1, 1].\n    """"""\n    with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                               dtype=tf.float32,\n                               shape=[1, 1, 4])\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                      bbox)\n        tf.image_summary(\'image_with_bounding_boxes\', image_with_box)\n\n        distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n        # Restore the shape since the dynamic slice based upon the bbox_size loses\n        # the third dimension.\n        distorted_image.set_shape([None, None, 3])\n        image_with_distorted_box = tf.image.draw_bounding_boxes(\n                tf.expand_dims(image, 0), distorted_bbox)\n        tf.image_summary(\'images_with_distorted_bounding_box\',\n                         image_with_distorted_box)\n\n        # This resizing operation may distort the images because the aspect\n        # ratio is not respected. We select a resize method in a round robin\n        # fashion based on the thread number.\n        # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n        # We select only 1 case for fast_mode bilinear.\n        num_resize_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n                distorted_image,\n                lambda x, method: tf.image.resize_images(x, [height, width], method),\n                num_cases=num_resize_cases)\n\n        tf.image_summary(\'cropped_resized_image\',\n                         tf.expand_dims(distorted_image, 0))\n\n        # Randomly flip the image horizontally.\n        distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n        # Randomly distort the colors. There are 4 ways to do it.\n        distorted_image = apply_with_random_selector(\n                distorted_image,\n                lambda x, ordering: distort_color(x, ordering, fast_mode),\n                num_cases=4)\n\n        tf.image_summary(\'final_distorted_image\',\n                         tf.expand_dims(distorted_image, 0))\n        distorted_image = tf.sub(distorted_image, 0.5)\n        distorted_image = tf.mul(distorted_image, 2.0)\n        return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n    """"""Prepare one image for evaluation.\n\n    If height and width are specified it would output an image with that size by\n    applying resize_bilinear.\n\n    If central_fraction is specified it would cropt the central fraction of the\n    input image.\n\n    Args:\n        image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n            [0, 1], otherwise it would converted to tf.float32 assuming that the range\n            is [0, MAX], where MAX is largest positive representable number for\n            int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n        height: integer\n        width: integer\n        central_fraction: Optional Float, fraction of the image to crop.\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D float Tensor of prepared image.\n    """"""\n    with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        if central_fraction:\n            image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n        if height and width:\n            # Resize the image to the specified height and width.\n            image = tf.expand_dims(image, 0)\n            image = tf.image.resize_bilinear(image, [height, width],\n                                             align_corners=False)\n            image = tf.squeeze(image, [0])\n        image = tf.sub(image, 0.5)\n        image = tf.mul(image, 2.0)\n        return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False, bbox=None, fast_mode=True):\n    """"""Pre-process one image for training or evaluation.\n\n    Args:\n        image: 3-D Tensor [height, width, channels] with the image.\n        height: integer, image expected height.\n        width: integer, image expected width.\n        is_training: Boolean. If true it would transform an image for train,\n            otherwise it would transform it for evaluation.\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged as\n            [ymin, xmin, ymax, xmax].\n        fast_mode: Optional boolean, if True avoids slower transformations.\n\n    Returns:\n        3-D float Tensor containing an appropriately scaled image\n\n    Raises:\n        ValueError: if user does not provide bounding box\n    """"""\n    if is_training:\n        return preprocess_for_train(image, height, width, bbox, fast_mode)\n    else:\n        return preprocess_for_eval(image, height, width)\n'"
preprocessing/preprocessing_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# from preprocessing import cifarnet_preprocessing\n# from preprocessing import inception_preprocessing\n# from preprocessing import vgg_preprocessing\n\nfrom preprocessing import ssd_vgg_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n    """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n    Args:\n      name: The name of the preprocessing function.\n      is_training: `True` if the model is being used for training.\n\n    Returns:\n      preprocessing_fn: A function that preprocessing a single image (pre-batch).\n        It has the following signature:\n          image = preprocessing_fn(image, output_height, output_width, ...).\n\n    Raises:\n      ValueError: If Preprocessing `name` is not recognized.\n    """"""\n    preprocessing_fn_map = {\n        \'ssd_300_vgg\': ssd_vgg_preprocessing,\n        \'ssd_512_vgg\': ssd_vgg_preprocessing,\n    }\n\n    if name not in preprocessing_fn_map:\n        raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n    def preprocessing_fn(image, labels, bboxes,\n                         out_shape, data_format=\'NHWC\', **kwargs):\n        return preprocessing_fn_map[name].preprocess_image(\n            image, labels, bboxes, out_shape, data_format=data_format,\n            is_training=is_training, **kwargs)\n    return preprocessing_fn\n'"
preprocessing/ssd_vgg_preprocessing.py,59,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Pre-processing images for SSD-type networks.\n""""""\nfrom enum import Enum, IntEnum\nimport numpy as np\n\nimport tensorflow as tf\nimport tf_extended as tfe\n\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom preprocessing import tf_image\nfrom nets import ssd_common\n\nslim = tf.contrib.slim\n\n# Resizing strategies.\nResize = IntEnum(\'Resize\', (\'NONE\',                # Nothing!\n                            \'CENTRAL_CROP\',        # Crop (and pad if necessary).\n                            \'PAD_AND_RESIZE\',      # Pad, and resize to output shape.\n                            \'WARP_RESIZE\'))        # Warp resize.\n\n# VGG mean parameters.\n_R_MEAN = 123.\n_G_MEAN = 117.\n_B_MEAN = 104.\n\n# Some training pre-processing parameters.\nBBOX_CROP_OVERLAP = 0.5         # Minimum overlap to keep a bbox after cropping.\nMIN_OBJECT_COVERED = 0.25\nCROP_RATIO_RANGE = (0.6, 1.67)  # Distortion ratio during cropping.\nEVAL_SIZE = (300, 300)\n\n\ndef tf_image_whitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n    """"""Subtracts the given means from each image channel.\n\n    Returns:\n        the centered image.\n    """"""\n    if image.get_shape().ndims != 3:\n        raise ValueError(\'Input must be of size [height, width, C>0]\')\n    num_channels = image.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n        raise ValueError(\'len(means) must match the number of channels\')\n\n    mean = tf.constant(means, dtype=image.dtype)\n    image = image - mean\n    return image\n\n\ndef tf_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n    """"""Re-convert to original image distribution, and convert to int if\n    necessary.\n\n    Returns:\n      Centered image.\n    """"""\n    mean = tf.constant(means, dtype=image.dtype)\n    image = image + mean\n    if to_int:\n        image = tf.cast(image, tf.int32)\n    return image\n\n\ndef np_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n    """"""Re-convert to original image distribution, and convert to int if\n    necessary. Numpy version.\n\n    Returns:\n      Centered image.\n    """"""\n    img = np.copy(image)\n    img += np.array(means, dtype=img.dtype)\n    if to_int:\n        img = img.astype(np.uint8)\n    return img\n\n\ndef tf_summary_image(image, bboxes, name=\'image\', unwhitened=False):\n    """"""Add image with bounding boxes to summary.\n    """"""\n    if unwhitened:\n        image = tf_image_unwhitened(image)\n    image = tf.expand_dims(image, 0)\n    bboxes = tf.expand_dims(bboxes, 0)\n    image_with_box = tf.image.draw_bounding_boxes(image, bboxes)\n    tf.summary.image(name, image_with_box)\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n    Args:\n        x: input Tensor.\n        func: Python function to apply.\n        num_cases: Python int32, number of cases to sample sel from.\n\n    Returns:\n        The result of func(x, sel), where func receives the value of the\n        selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n            func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n            for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n\n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n\n    Args:\n        image: 3-D Tensor containing single image in [0, 1].\n        color_ordering: Python int, a type of distortion (valid values: 0-3).\n        fast_mode: Avoids slower ops (random_hue and random_contrast)\n        scope: Optional scope for name_scope.\n    Returns:\n        3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n        ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                labels,\n                                bboxes,\n                                min_object_covered=0.3,\n                                aspect_ratio_range=(0.9, 1.1),\n                                area_range=(0.1, 1.0),\n                                max_attempts=200,\n                                clip_bboxes=True,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n        image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n        bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n            where each coordinate is [0, 1) and the coordinates are arranged\n            as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n            image.\n        min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n            area of the image must contain at least this fraction of any bounding box\n            supplied.\n        aspect_ratio_range: An optional list of `floats`. The cropped area of the\n            image must have an aspect ratio = width / height within this range.\n        area_range: An optional list of `floats`. The cropped area of the image\n            must contain a fraction of the supplied image within in this range.\n        max_attempts: An optional `int`. Number of attempts at generating a cropped\n            region of the image of the specified constraints. After `max_attempts`\n            failures, return the entire image.\n        scope: Optional scope for name_scope.\n    Returns:\n        A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bboxes]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n                tf.shape(image),\n                bounding_boxes=tf.expand_dims(bboxes, 0),\n                min_object_covered=min_object_covered,\n                aspect_ratio_range=aspect_ratio_range,\n                area_range=area_range,\n                max_attempts=max_attempts,\n                use_image_if_no_bounding_boxes=True)\n        distort_bbox = distort_bbox[0, 0]\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        # Restore the shape since the dynamic slice loses 3rd dimension.\n        cropped_image.set_shape([None, None, 3])\n\n        # Update bounding boxes: resize and filter out.\n        bboxes = tfe.bboxes_resize(distort_bbox, bboxes)\n        labels, bboxes = tfe.bboxes_filter_overlap(labels, bboxes,\n                                                   threshold=BBOX_CROP_OVERLAP,\n                                                   assign_negative=False)\n        return cropped_image, labels, bboxes, distort_bbox\n\n\ndef preprocess_for_train(image, labels, bboxes,\n                         out_shape, data_format=\'NHWC\',\n                         scope=\'ssd_preprocessing_train\'):\n    """"""Preprocesses the given image for training.\n\n    Note that the actual resizing scale is sampled from\n        [`resize_size_min`, `resize_size_max`].\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        output_height: The height of the image after preprocessing.\n        output_width: The width of the image after preprocessing.\n        resize_side_min: The lower bound for the smallest side of the image for\n            aspect-preserving resizing.\n        resize_side_max: The upper bound for the smallest side of the image for\n            aspect-preserving resizing.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    fast_mode = False\n    with tf.name_scope(scope, \'ssd_preprocessing_train\', [image, labels, bboxes]):\n        if image.get_shape().ndims != 3:\n            raise ValueError(\'Input must be of size [height, width, C>0]\')\n        # Convert to float scaled [0, 1].\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        tf_summary_image(image, bboxes, \'image_with_bboxes\')\n\n        # # Remove DontCare labels.\n        # labels, bboxes = ssd_common.tf_bboxes_filter_labels(out_label,\n        #                                                     labels,\n        #                                                     bboxes)\n\n        # Distort image and bounding boxes.\n        dst_image = image\n        dst_image, labels, bboxes, distort_bbox = \\\n            distorted_bounding_box_crop(image, labels, bboxes,\n                                        min_object_covered=MIN_OBJECT_COVERED,\n                                        aspect_ratio_range=CROP_RATIO_RANGE)\n        # Resize image to output size.\n        dst_image = tf_image.resize_image(dst_image, out_shape,\n                                          method=tf.image.ResizeMethod.BILINEAR,\n                                          align_corners=False)\n        tf_summary_image(dst_image, bboxes, \'image_shape_distorted\')\n\n        # Randomly flip the image horizontally.\n        dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)\n\n        # Randomly distort the colors. There are 4 ways to do it.\n        dst_image = apply_with_random_selector(\n                dst_image,\n                lambda x, ordering: distort_color(x, ordering, fast_mode),\n                num_cases=4)\n        tf_summary_image(dst_image, bboxes, \'image_color_distorted\')\n\n        # Rescale to VGG input scale.\n        image = dst_image * 255.\n        image = tf_image_whitened(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n        # Image data format.\n        if data_format == \'NCHW\':\n            image = tf.transpose(image, perm=(2, 0, 1))\n        return image, labels, bboxes\n\n\ndef preprocess_for_eval(image, labels, bboxes,\n                        out_shape=EVAL_SIZE, data_format=\'NHWC\',\n                        difficults=None, resize=Resize.WARP_RESIZE,\n                        scope=\'ssd_preprocessing_train\'):\n    """"""Preprocess an image for evaluation.\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        out_shape: Output shape after pre-processing (if resize != None)\n        resize: Resize strategy.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    with tf.name_scope(scope):\n        if image.get_shape().ndims != 3:\n            raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n        image = tf.to_float(image)\n        image = tf_image_whitened(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n        # Add image rectangle to bboxes.\n        bbox_img = tf.constant([[0., 0., 1., 1.]])\n        if bboxes is None:\n            bboxes = bbox_img\n        else:\n            bboxes = tf.concat([bbox_img, bboxes], axis=0)\n\n        if resize == Resize.NONE:\n            # No resizing...\n            pass\n        elif resize == Resize.CENTRAL_CROP:\n            # Central cropping of the image.\n            image, bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(\n                image, bboxes, out_shape[0], out_shape[1])\n        elif resize == Resize.PAD_AND_RESIZE:\n            # Resize image first: find the correct factor...\n            shape = tf.shape(image)\n            factor = tf.minimum(tf.to_double(1.0),\n                                tf.minimum(tf.to_double(out_shape[0] / shape[0]),\n                                           tf.to_double(out_shape[1] / shape[1])))\n            resize_shape = factor * tf.to_double(shape[0:2])\n            resize_shape = tf.cast(tf.floor(resize_shape), tf.int32)\n\n            image = tf_image.resize_image(image, resize_shape,\n                                          method=tf.image.ResizeMethod.BILINEAR,\n                                          align_corners=False)\n            # Pad to expected size.\n            image, bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(\n                image, bboxes, out_shape[0], out_shape[1])\n        elif resize == Resize.WARP_RESIZE:\n            # Warp resize of the image.\n            image = tf_image.resize_image(image, out_shape,\n                                          method=tf.image.ResizeMethod.BILINEAR,\n                                          align_corners=False)\n\n        # Split back bounding boxes.\n        bbox_img = bboxes[0]\n        bboxes = bboxes[1:]\n        # Remove difficult boxes.\n        if difficults is not None:\n            mask = tf.logical_not(tf.cast(difficults, tf.bool))\n            labels = tf.boolean_mask(labels, mask)\n            bboxes = tf.boolean_mask(bboxes, mask)\n        # Image data format.\n        if data_format == \'NCHW\':\n            image = tf.transpose(image, perm=(2, 0, 1))\n        return image, labels, bboxes, bbox_img\n\n\ndef preprocess_image(image,\n                     labels,\n                     bboxes,\n                     out_shape,\n                     data_format,\n                     is_training=False,\n                     **kwargs):\n    """"""Pre-process an given image.\n\n    Args:\n      image: A `Tensor` representing an image of arbitrary size.\n      output_height: The height of the image after preprocessing.\n      output_width: The width of the image after preprocessing.\n      is_training: `True` if we\'re preprocessing the image for training and\n        `False` otherwise.\n      resize_side_min: The lower bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, then this value\n        is used for rescaling.\n      resize_side_max: The upper bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, this value is\n         ignored. Otherwise, the resize side is sampled from\n         [resize_size_min, resize_size_max].\n\n    Returns:\n      A preprocessed image.\n    """"""\n    if is_training:\n        return preprocess_for_train(image, labels, bboxes,\n                                    out_shape=out_shape,\n                                    data_format=data_format)\n    else:\n        return preprocess_for_eval(image, labels, bboxes,\n                                   out_shape=out_shape,\n                                   data_format=data_format,\n                                   **kwargs)\n'"
preprocessing/tf_image.py,15,"b'# Copyright 2015 The TensorFlow Authors and Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Custom image operations.\nMost of the following methods extend TensorFlow image library, and part of\nthe code is shameless copy-paste of the former!\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_image_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\n\n\n# =========================================================================== #\n# Modification of TensorFlow image routines.\n# =========================================================================== #\ndef _assert(cond, ex_type, msg):\n    """"""A polymorphic assert, works with tensors and boolean expressions.\n    If `cond` is not a tensor, behave like an ordinary assert statement, except\n    that a empty list is returned. If `cond` is a tensor, return a list\n    containing a single TensorFlow assert op.\n    Args:\n      cond: Something evaluates to a boolean value. May be a tensor.\n      ex_type: The exception class to use.\n      msg: The error message.\n    Returns:\n      A list, containing at most one assert op.\n    """"""\n    if _is_tensor(cond):\n        return [control_flow_ops.Assert(cond, [msg])]\n    else:\n        if not cond:\n            raise ex_type(msg)\n        else:\n            return []\n\n\ndef _is_tensor(x):\n    """"""Returns `True` if `x` is a symbolic tensor-like object.\n    Args:\n      x: A python object to check.\n    Returns:\n      `True` if `x` is a `tf.Tensor` or `tf.Variable`, otherwise `False`.\n    """"""\n    return isinstance(x, (ops.Tensor, variables.Variable))\n\n\ndef _ImageDimensions(image):\n    """"""Returns the dimensions of an image tensor.\n    Args:\n      image: A 3-D Tensor of shape `[height, width, channels]`.\n    Returns:\n      A list of `[height, width, channels]` corresponding to the dimensions of the\n        input image.  Dimensions that are statically known are python integers,\n        otherwise they are integer scalar tensors.\n    """"""\n    if image.get_shape().is_fully_defined():\n        return image.get_shape().as_list()\n    else:\n        static_shape = image.get_shape().with_rank(3).as_list()\n        dynamic_shape = array_ops.unstack(array_ops.shape(image), 3)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef _Check3DImage(image, require_static=True):\n    """"""Assert that we are working with properly shaped image.\n    Args:\n      image: 3-D Tensor of shape [height, width, channels]\n        require_static: If `True`, requires that all dimensions of `image` are\n        known and non-zero.\n    Raises:\n      ValueError: if `image.shape` is not a 3-vector.\n    Returns:\n      An empty list, if `image` has fully defined dimensions. Otherwise, a list\n        containing an assert op is returned.\n    """"""\n    try:\n        image_shape = image.get_shape().with_rank(3)\n    except ValueError:\n        raise ValueError(""\'image\' must be three-dimensional."")\n    if require_static and not image_shape.is_fully_defined():\n        raise ValueError(""\'image\' must be fully defined."")\n    if any(x == 0 for x in image_shape):\n        raise ValueError(""all dims of \'image.shape\' must be > 0: %s"" %\n                         image_shape)\n    if not image_shape.is_fully_defined():\n        return [check_ops.assert_positive(array_ops.shape(image),\n                                          [""all dims of \'image.shape\' ""\n                                           ""must be > 0.""])]\n    else:\n        return []\n\n\ndef fix_image_flip_shape(image, result):\n    """"""Set the shape to 3 dimensional if we don\'t know anything else.\n    Args:\n      image: original image size\n      result: flipped or transformed image\n    Returns:\n      An image whose shape is at least None,None,None.\n    """"""\n    image_shape = image.get_shape()\n    if image_shape == tensor_shape.unknown_shape():\n        result.set_shape([None, None, None])\n    else:\n        result.set_shape(image_shape)\n    return result\n\n\n# =========================================================================== #\n# Image + BBoxes methods: cropping, resizing, flipping, ...\n# =========================================================================== #\ndef bboxes_crop_or_pad(bboxes,\n                       height, width,\n                       offset_y, offset_x,\n                       target_height, target_width):\n    """"""Adapt bounding boxes to crop or pad operations.\n    Coordinates are always supposed to be relative to the image.\n\n    Arguments:\n      bboxes: Tensor Nx4 with bboxes coordinates [y_min, x_min, y_max, x_max];\n      height, width: Original image dimension;\n      offset_y, offset_x: Offset to apply,\n        negative if cropping, positive if padding;\n      target_height, target_width: Target dimension after cropping / padding.\n    """"""\n    with tf.name_scope(\'bboxes_crop_or_pad\'):\n        # Rescale bounding boxes in pixels.\n        scale = tf.cast(tf.stack([height, width, height, width]), bboxes.dtype)\n        bboxes = bboxes * scale\n        # Add offset.\n        offset = tf.cast(tf.stack([offset_y, offset_x, offset_y, offset_x]), bboxes.dtype)\n        bboxes = bboxes + offset\n        # Rescale to target dimension.\n        scale = tf.cast(tf.stack([target_height, target_width,\n                                  target_height, target_width]), bboxes.dtype)\n        bboxes = bboxes / scale\n        return bboxes\n\n\ndef resize_image_bboxes_with_crop_or_pad(image, bboxes,\n                                         target_height, target_width):\n    """"""Crops and/or pads an image to a target width and height.\n    Resizes an image to a target width and height by either centrally\n    cropping the image or padding it evenly with zeros.\n\n    If `width` or `height` is greater than the specified `target_width` or\n    `target_height` respectively, this op centrally crops along that dimension.\n    If `width` or `height` is smaller than the specified `target_width` or\n    `target_height` respectively, this op centrally pads with 0 along that\n    dimension.\n    Args:\n      image: 3-D tensor of shape `[height, width, channels]`\n      target_height: Target height.\n      target_width: Target width.\n    Raises:\n      ValueError: if `target_height` or `target_width` are zero or negative.\n    Returns:\n      Cropped and/or padded image of shape\n        `[target_height, target_width, channels]`\n    """"""\n    with tf.name_scope(\'resize_with_crop_or_pad\'):\n        image = ops.convert_to_tensor(image, name=\'image\')\n\n        assert_ops = []\n        assert_ops += _Check3DImage(image, require_static=False)\n        assert_ops += _assert(target_width > 0, ValueError,\n                              \'target_width must be > 0.\')\n        assert_ops += _assert(target_height > 0, ValueError,\n                              \'target_height must be > 0.\')\n\n        image = control_flow_ops.with_dependencies(assert_ops, image)\n        # `crop_to_bounding_box` and `pad_to_bounding_box` have their own checks.\n        # Make sure our checks come first, so that error messages are clearer.\n        if _is_tensor(target_height):\n            target_height = control_flow_ops.with_dependencies(\n                assert_ops, target_height)\n        if _is_tensor(target_width):\n            target_width = control_flow_ops.with_dependencies(assert_ops, target_width)\n\n        def max_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.maximum(x, y)\n            else:\n                return max(x, y)\n\n        def min_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.minimum(x, y)\n            else:\n                return min(x, y)\n\n        def equal_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.equal(x, y)\n            else:\n                return x == y\n\n        height, width, _ = _ImageDimensions(image)\n        width_diff = target_width - width\n        offset_crop_width = max_(-width_diff // 2, 0)\n        offset_pad_width = max_(width_diff // 2, 0)\n\n        height_diff = target_height - height\n        offset_crop_height = max_(-height_diff // 2, 0)\n        offset_pad_height = max_(height_diff // 2, 0)\n\n        # Maybe crop if needed.\n        height_crop = min_(target_height, height)\n        width_crop = min_(target_width, width)\n        cropped = tf.image.crop_to_bounding_box(image, offset_crop_height, offset_crop_width,\n                                                height_crop, width_crop)\n        bboxes = bboxes_crop_or_pad(bboxes,\n                                    height, width,\n                                    -offset_crop_height, -offset_crop_width,\n                                    height_crop, width_crop)\n        # Maybe pad if needed.\n        resized = tf.image.pad_to_bounding_box(cropped, offset_pad_height, offset_pad_width,\n                                               target_height, target_width)\n        bboxes = bboxes_crop_or_pad(bboxes,\n                                    height_crop, width_crop,\n                                    offset_pad_height, offset_pad_width,\n                                    target_height, target_width)\n\n        # In theory all the checks below are redundant.\n        if resized.get_shape().ndims is None:\n            raise ValueError(\'resized contains no shape.\')\n\n        resized_height, resized_width, _ = _ImageDimensions(resized)\n\n        assert_ops = []\n        assert_ops += _assert(equal_(resized_height, target_height), ValueError,\n                              \'resized height is not correct.\')\n        assert_ops += _assert(equal_(resized_width, target_width), ValueError,\n                              \'resized width is not correct.\')\n\n        resized = control_flow_ops.with_dependencies(assert_ops, resized)\n        return resized, bboxes\n\n\ndef resize_image(image, size,\n                 method=tf.image.ResizeMethod.BILINEAR,\n                 align_corners=False):\n    """"""Resize an image and bounding boxes.\n    """"""\n    # Resize image.\n    with tf.name_scope(\'resize_image\'):\n        height, width, channels = _ImageDimensions(image)\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize_images(image, size,\n                                       method, align_corners)\n        image = tf.reshape(image, tf.stack([size[0], size[1], channels]))\n        return image\n\n\ndef random_flip_left_right(image, bboxes, seed=None):\n    """"""Random flip left-right of an image and its bounding boxes.\n    """"""\n    def flip_bboxes(bboxes):\n        """"""Flip bounding boxes coordinates.\n        """"""\n        bboxes = tf.stack([bboxes[:, 0], 1 - bboxes[:, 3],\n                           bboxes[:, 2], 1 - bboxes[:, 1]], axis=-1)\n        return bboxes\n\n    # Random flip. Tensorflow implementation.\n    with tf.name_scope(\'random_flip_left_right\'):\n        image = ops.convert_to_tensor(image, name=\'image\')\n        _Check3DImage(image, require_static=False)\n        uniform_random = random_ops.random_uniform([], 0, 1.0, seed=seed)\n        mirror_cond = math_ops.less(uniform_random, .5)\n        # Flip image.\n        result = control_flow_ops.cond(mirror_cond,\n                                       lambda: array_ops.reverse_v2(image, [1]),\n                                       lambda: image)\n        # Flip bboxes.\n        bboxes = control_flow_ops.cond(mirror_cond,\n                                       lambda: flip_bboxes(bboxes),\n                                       lambda: bboxes)\n        return fix_image_flip_shape(image, result), bboxes\n\n'"
preprocessing/vgg_preprocessing.py,54,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n    Very Deep Convolutional Networks For Large-Scale Image Recognition\n    Karen Simonyan and Andrew Zisserman\n    arXiv technical report, 2015\n    PDF: http://arxiv.org/pdf/1409.1556.pdf\n    ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n    CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n    """"""Crops the given image using the provided offsets and sizes.\n\n    Note that the method doesn\'t assume we know the input image size but it does\n    assume we know the input image rank.\n\n    Args:\n        image: an image of shape [height, width, channels].\n        offset_height: a scalar tensor indicating the height offset.\n        offset_width: a scalar tensor indicating the width offset.\n        crop_height: the height of the cropped image.\n        crop_width: the width of the cropped image.\n\n    Returns:\n        the cropped (and resized) image.\n\n    Raises:\n        InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n            less than the crop size.\n    """"""\n    original_shape = tf.shape(image)\n\n    rank_assertion = tf.Assert(\n            tf.equal(tf.rank(image), 3),\n            [\'Rank of image must be equal to 3.\'])\n    cropped_shape = control_flow_ops.with_dependencies(\n            [rank_assertion],\n            tf.pack([crop_height, crop_width, original_shape[2]]))\n\n    size_assertion = tf.Assert(\n            tf.logical_and(\n                    tf.greater_equal(original_shape[0], crop_height),\n                    tf.greater_equal(original_shape[1], crop_width)),\n            [\'Crop size greater than the image size.\'])\n\n    offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))\n\n    # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n    # define the crop size.\n    image = control_flow_ops.with_dependencies(\n            [size_assertion],\n            tf.slice(image, offsets, cropped_shape))\n    return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n    """"""Crops the given list of images.\n\n    The function applies the same crop to each image in the list. This can be\n    effectively applied when there are multiple image inputs of the same\n    dimension such as:\n\n        image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n    Args:\n        image_list: a list of image tensors of the same dimension but possibly\n            varying channel.\n        crop_height: the new height.\n        crop_width: the new width.\n\n    Returns:\n        the image_list with cropped images.\n\n    Raises:\n        ValueError: if there are multiple image inputs provided with different size\n            or the images are smaller than the crop dimensions.\n    """"""\n    if not image_list:\n        raise ValueError(\'Empty image_list.\')\n\n    # Compute the rank assertions.\n    rank_assertions = []\n    for i in range(len(image_list)):\n        image_rank = tf.rank(image_list[i])\n        rank_assert = tf.Assert(\n                tf.equal(image_rank, 3),\n                [\'Wrong rank for tensor  %s [expected] [actual]\',\n                 image_list[i].name, 3, image_rank])\n        rank_assertions.append(rank_assert)\n\n    image_shape = control_flow_ops.with_dependencies(\n            [rank_assertions[0]],\n            tf.shape(image_list[0]))\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    crop_size_assert = tf.Assert(\n            tf.logical_and(\n                    tf.greater_equal(image_height, crop_height),\n                    tf.greater_equal(image_width, crop_width)),\n            [\'Crop size greater than the image size.\'])\n\n    asserts = [rank_assertions[0], crop_size_assert]\n\n    for i in range(1, len(image_list)):\n        image = image_list[i]\n        asserts.append(rank_assertions[i])\n        shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                                   tf.shape(image))\n        height = shape[0]\n        width = shape[1]\n\n        height_assert = tf.Assert(\n                tf.equal(height, image_height),\n                [\'Wrong height for tensor %s [expected][actual]\',\n                 image.name, height, image_height])\n        width_assert = tf.Assert(\n                tf.equal(width, image_width),\n                [\'Wrong width for tensor %s [expected][actual]\',\n                 image.name, width, image_width])\n        asserts.extend([height_assert, width_assert])\n\n    # Create a random bounding box.\n    #\n    # Use tf.random_uniform and not numpy.random.rand as doing the former would\n    # generate random numbers at graph eval time, unlike the latter which\n    # generates random numbers at graph definition time.\n    max_offset_height = control_flow_ops.with_dependencies(\n            asserts, tf.reshape(image_height - crop_height + 1, []))\n    max_offset_width = control_flow_ops.with_dependencies(\n            asserts, tf.reshape(image_width - crop_width + 1, []))\n    offset_height = tf.random_uniform(\n            [], maxval=max_offset_height, dtype=tf.int32)\n    offset_width = tf.random_uniform(\n            [], maxval=max_offset_width, dtype=tf.int32)\n\n    return [_crop(image, offset_height, offset_width,\n                  crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n    """"""Performs central crops of the given image list.\n\n    Args:\n        image_list: a list of image tensors of the same dimension but possibly\n            varying channel.\n        crop_height: the height of the image following the crop.\n        crop_width: the width of the image following the crop.\n\n    Returns:\n        the list of cropped images.\n    """"""\n    outputs = []\n    for image in image_list:\n        image_height = tf.shape(image)[0]\n        image_width = tf.shape(image)[1]\n\n        offset_height = (image_height - crop_height) / 2\n        offset_width = (image_width - crop_width) / 2\n\n        outputs.append(_crop(image, offset_height, offset_width,\n                             crop_height, crop_width))\n    return outputs\n\n\ndef _mean_image_subtraction(image, means):\n    """"""Subtracts the given means from each image channel.\n\n    For example:\n        means = [123.68, 116.779, 103.939]\n        image = _mean_image_subtraction(image, means)\n\n    Note that the rank of `image` must be known.\n\n    Args:\n        image: a tensor of size [height, width, C].\n        means: a C-vector of values to subtract from each channel.\n\n    Returns:\n        the centered image.\n\n    Raises:\n        ValueError: If the rank of `image` is unknown, if `image` has a rank other\n            than three or if the number of channels in `image` doesn\'t match the\n            number of values in `means`.\n    """"""\n    if image.get_shape().ndims != 3:\n        raise ValueError(\'Input must be of size [height, width, C>0]\')\n    num_channels = image.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n        raise ValueError(\'len(means) must match the number of channels\')\n\n    channels = tf.split(2, num_channels, image)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(channels, axis=2)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n    """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n    Computes new shape with the smallest side equal to `smallest_side` while\n    preserving the original aspect ratio.\n\n    Args:\n        height: an int32 scalar tensor indicating the current height.\n        width: an int32 scalar tensor indicating the current width.\n        smallest_side: A python integer or scalar `Tensor` indicating the size of\n            the smallest side after resize.\n\n    Returns:\n        new_height: an int32 scalar tensor indicating the new height.\n        new_width: and int32 scalar tensor indicating the new width.\n    """"""\n    smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n    height = tf.to_float(height)\n    width = tf.to_float(width)\n    smallest_side = tf.to_float(smallest_side)\n\n    scale = tf.cond(tf.greater(height, width),\n                    lambda: smallest_side / width,\n                    lambda: smallest_side / height)\n    new_height = tf.to_int32(height * scale)\n    new_width = tf.to_int32(width * scale)\n    return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n    """"""Resize images preserving the original aspect ratio.\n\n    Args:\n        image: A 3-D image `Tensor`.\n        smallest_side: A python integer or scalar `Tensor` indicating the size of\n            the smallest side after resize.\n\n    Returns:\n        resized_image: A 3-D tensor containing the resized image.\n    """"""\n    smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n    shape = tf.shape(image)\n    height = shape[0]\n    width = shape[1]\n    new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n    image = tf.expand_dims(image, 0)\n    resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                             align_corners=False)\n    resized_image = tf.squeeze(resized_image)\n    resized_image.set_shape([None, None, 3])\n    return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n    """"""Preprocesses the given image for training.\n\n    Note that the actual resizing scale is sampled from\n        [`resize_size_min`, `resize_size_max`].\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        output_height: The height of the image after preprocessing.\n        output_width: The width of the image after preprocessing.\n        resize_side_min: The lower bound for the smallest side of the image for\n            aspect-preserving resizing.\n        resize_side_max: The upper bound for the smallest side of the image for\n            aspect-preserving resizing.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    resize_side = tf.random_uniform(\n            [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n    image = _aspect_preserving_resize(image, resize_side)\n    image = _random_crop([image], output_height, output_width)[0]\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n    image = tf.image.random_flip_left_right(image)\n    return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n    """"""Preprocesses the given image for evaluation.\n\n    Args:\n        image: A `Tensor` representing an image of arbitrary size.\n        output_height: The height of the image after preprocessing.\n        output_width: The width of the image after preprocessing.\n        resize_side: The smallest side of the image for aspect-preserving resizing.\n\n    Returns:\n        A preprocessed image.\n    """"""\n    image = _aspect_preserving_resize(image, resize_side)\n    image = _central_crop([image], output_height, output_width)[0]\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n    return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n    """"""Preprocesses the given image.\n\n    Args:\n      image: A `Tensor` representing an image of arbitrary size.\n      output_height: The height of the image after preprocessing.\n      output_width: The width of the image after preprocessing.\n      is_training: `True` if we\'re preprocessing the image for training and\n        `False` otherwise.\n      resize_side_min: The lower bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, then this value\n        is used for rescaling.\n      resize_side_max: The upper bound for the smallest side of the image for\n        aspect-preserving resizing. If `is_training` is `False`, this value is\n         ignored. Otherwise, the resize side is sampled from\n         [resize_size_min, resize_size_max].\n\n    Returns:\n        A preprocessed image.\n    """"""\n    if is_training:\n        return preprocess_for_train(image, output_height, output_width,\n                                    resize_side_min, resize_side_max)\n    else:\n        return preprocess_for_eval(image, output_height, output_width,\n                                   resize_side_min)\n'"
tf_extended/__init__.py,0,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional metrics.\n""""""\n\n# pylint: disable=unused-import,line-too-long,g-importing-member,wildcard-import\nfrom tf_extended.metrics import *\nfrom tf_extended.tensors import *\nfrom tf_extended.bboxes import *\nfrom tf_extended.image import *\nfrom tf_extended.math import *\n\n'"
tf_extended/bboxes.py,108,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional bounding boxes methods.\n""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_extended import tensors as tfe_tensors\nfrom tf_extended import math as tfe_math\n\n\n# =========================================================================== #\n# Standard boxes algorithms.\n# =========================================================================== #\ndef bboxes_sort_all_classes(classes, scores, bboxes, top_k=400, scope=None):\n    """"""Sort bounding boxes by decreasing order and keep only the top_k.\n    Assume the input Tensors mix-up objects with different classes.\n    Assume a batch-type input.\n\n    Args:\n      classes: Batch x N Tensor containing integer classes.\n      scores: Batch x N Tensor containing float scores.\n      bboxes: Batch x N x 4 Tensor containing boxes coordinates.\n      top_k: Top_k boxes to keep.\n    Return:\n      classes, scores, bboxes: Sorted tensors of shape Batch x Top_k.\n    """"""\n    with tf.name_scope(scope, \'bboxes_sort\', [classes, scores, bboxes]):\n        scores, idxes = tf.nn.top_k(scores, k=top_k, sorted=True)\n\n        # Trick to be able to use tf.gather: map for each element in the batch.\n        def fn_gather(classes, bboxes, idxes):\n            cl = tf.gather(classes, idxes)\n            bb = tf.gather(bboxes, idxes)\n            return [cl, bb]\n        r = tf.map_fn(lambda x: fn_gather(x[0], x[1], x[2]),\n                      [classes, bboxes, idxes],\n                      dtype=[classes.dtype, bboxes.dtype],\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=False,\n                      infer_shape=True)\n        classes = r[0]\n        bboxes = r[1]\n        return classes, scores, bboxes\n\n\ndef bboxes_sort(scores, bboxes, top_k=400, scope=None):\n    """"""Sort bounding boxes by decreasing order and keep only the top_k.\n    If inputs are dictionnaries, assume every key is a different class.\n    Assume a batch-type input.\n\n    Args:\n      scores: Batch x N Tensor/Dictionary containing float scores.\n      bboxes: Batch x N x 4 Tensor/Dictionary containing boxes coordinates.\n      top_k: Top_k boxes to keep.\n    Return:\n      scores, bboxes: Sorted Tensors/Dictionaries of shape Batch x Top_k x 1|4.\n    """"""\n    # Dictionaries as inputs.\n    if isinstance(scores, dict) or isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_sort_dict\'):\n            d_scores = {}\n            d_bboxes = {}\n            for c in scores.keys():\n                s, b = bboxes_sort(scores[c], bboxes[c], top_k=top_k)\n                d_scores[c] = s\n                d_bboxes[c] = b\n            return d_scores, d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(scope, \'bboxes_sort\', [scores, bboxes]):\n        # Sort scores...\n        scores, idxes = tf.nn.top_k(scores, k=top_k, sorted=True)\n\n        # Trick to be able to use tf.gather: map for each element in the first dim.\n        def fn_gather(bboxes, idxes):\n            bb = tf.gather(bboxes, idxes)\n            return [bb]\n        r = tf.map_fn(lambda x: fn_gather(x[0], x[1]),\n                      [bboxes, idxes],\n                      dtype=[bboxes.dtype],\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=False,\n                      infer_shape=True)\n        bboxes = r[0]\n        return scores, bboxes\n\n\ndef bboxes_clip(bbox_ref, bboxes, scope=None):\n    """"""Clip bounding boxes to a reference box.\n    Batch-compatible if the first dimension of `bbox_ref` and `bboxes`\n    can be broadcasted.\n\n    Args:\n      bbox_ref: Reference bounding box. Nx4 or 4 shaped-Tensor;\n      bboxes: Bounding boxes to clip. Nx4 or 4 shaped-Tensor or dictionary.\n    Return:\n      Clipped bboxes.\n    """"""\n    # Bboxes is dictionary.\n    if isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_clip_dict\'):\n            d_bboxes = {}\n            for c in bboxes.keys():\n                d_bboxes[c] = bboxes_clip(bbox_ref, bboxes[c])\n            return d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(scope, \'bboxes_clip\'):\n        # Easier with transposed bboxes. Especially for broadcasting.\n        bbox_ref = tf.transpose(bbox_ref)\n        bboxes = tf.transpose(bboxes)\n        # Intersection bboxes and reference bbox.\n        ymin = tf.maximum(bboxes[0], bbox_ref[0])\n        xmin = tf.maximum(bboxes[1], bbox_ref[1])\n        ymax = tf.minimum(bboxes[2], bbox_ref[2])\n        xmax = tf.minimum(bboxes[3], bbox_ref[3])\n        # Double check! Empty boxes when no-intersection.\n        ymin = tf.minimum(ymin, ymax)\n        xmin = tf.minimum(xmin, xmax)\n        bboxes = tf.transpose(tf.stack([ymin, xmin, ymax, xmax], axis=0))\n        return bboxes\n\n\ndef bboxes_resize(bbox_ref, bboxes, name=None):\n    """"""Resize bounding boxes based on a reference bounding box,\n    assuming that the latter is [0, 0, 1, 1] after transform. Useful for\n    updating a collection of boxes after cropping an image.\n    """"""\n    # Bboxes is dictionary.\n    if isinstance(bboxes, dict):\n        with tf.name_scope(name, \'bboxes_resize_dict\'):\n            d_bboxes = {}\n            for c in bboxes.keys():\n                d_bboxes[c] = bboxes_resize(bbox_ref, bboxes[c])\n            return d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(name, \'bboxes_resize\'):\n        # Translate.\n        v = tf.stack([bbox_ref[0], bbox_ref[1], bbox_ref[0], bbox_ref[1]])\n        bboxes = bboxes - v\n        # Scale.\n        s = tf.stack([bbox_ref[2] - bbox_ref[0],\n                      bbox_ref[3] - bbox_ref[1],\n                      bbox_ref[2] - bbox_ref[0],\n                      bbox_ref[3] - bbox_ref[1]])\n        bboxes = bboxes / s\n        return bboxes\n\n\ndef bboxes_nms(scores, bboxes, nms_threshold=0.5, keep_top_k=200, scope=None):\n    """"""Apply non-maximum selection to bounding boxes. In comparison to TF\n    implementation, use classes information for matching.\n    Should only be used on single-entries. Use batch version otherwise.\n\n    Args:\n      scores: N Tensor containing float scores.\n      bboxes: N x 4 Tensor containing boxes coordinates.\n      nms_threshold: Matching threshold in NMS algorithm;\n      keep_top_k: Number of total object to keep after NMS.\n    Return:\n      classes, scores, bboxes Tensors, sorted by score.\n        Padded with zero if necessary.\n    """"""\n    with tf.name_scope(scope, \'bboxes_nms_single\', [scores, bboxes]):\n        # Apply NMS algorithm.\n        idxes = tf.image.non_max_suppression(bboxes, scores,\n                                             keep_top_k, nms_threshold)\n        scores = tf.gather(scores, idxes)\n        bboxes = tf.gather(bboxes, idxes)\n        # Pad results.\n        scores = tfe_tensors.pad_axis(scores, 0, keep_top_k, axis=0)\n        bboxes = tfe_tensors.pad_axis(bboxes, 0, keep_top_k, axis=0)\n        return scores, bboxes\n\n\ndef bboxes_nms_batch(scores, bboxes, nms_threshold=0.5, keep_top_k=200,\n                     scope=None):\n    """"""Apply non-maximum selection to bounding boxes. In comparison to TF\n    implementation, use classes information for matching.\n    Use only on batched-inputs. Use zero-padding in order to batch output\n    results.\n\n    Args:\n      scores: Batch x N Tensor/Dictionary containing float scores.\n      bboxes: Batch x N x 4 Tensor/Dictionary containing boxes coordinates.\n      nms_threshold: Matching threshold in NMS algorithm;\n      keep_top_k: Number of total object to keep after NMS.\n    Return:\n      scores, bboxes Tensors/Dictionaries, sorted by score.\n        Padded with zero if necessary.\n    """"""\n    # Dictionaries as inputs.\n    if isinstance(scores, dict) or isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_nms_batch_dict\'):\n            d_scores = {}\n            d_bboxes = {}\n            for c in scores.keys():\n                s, b = bboxes_nms_batch(scores[c], bboxes[c],\n                                        nms_threshold=nms_threshold,\n                                        keep_top_k=keep_top_k)\n                d_scores[c] = s\n                d_bboxes[c] = b\n            return d_scores, d_bboxes\n\n    # Tensors inputs.\n    with tf.name_scope(scope, \'bboxes_nms_batch\'):\n        r = tf.map_fn(lambda x: bboxes_nms(x[0], x[1],\n                                           nms_threshold, keep_top_k),\n                      (scores, bboxes),\n                      dtype=(scores.dtype, bboxes.dtype),\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=False,\n                      infer_shape=True)\n        scores, bboxes = r\n        return scores, bboxes\n\n\n# def bboxes_fast_nms(classes, scores, bboxes,\n#                     nms_threshold=0.5, eta=3., num_classes=21,\n#                     pad_output=True, scope=None):\n#     with tf.name_scope(scope, \'bboxes_fast_nms\',\n#                        [classes, scores, bboxes]):\n\n#         nms_classes = tf.zeros((0,), dtype=classes.dtype)\n#         nms_scores = tf.zeros((0,), dtype=scores.dtype)\n#         nms_bboxes = tf.zeros((0, 4), dtype=bboxes.dtype)\n\n\ndef bboxes_matching(label, scores, bboxes,\n                    glabels, gbboxes, gdifficults,\n                    matching_threshold=0.5, scope=None):\n    """"""Matching a collection of detected boxes with groundtruth values.\n    Does not accept batched-inputs.\n    The algorithm goes as follows: for every detected box, check\n    if one grountruth box is matching. If none, then considered as False Positive.\n    If the grountruth box is already matched with another one, it also counts\n    as a False Positive. We refer the Pascal VOC documentation for the details.\n\n    Args:\n      rclasses, rscores, rbboxes: N(x4) Tensors. Detected objects, sorted by score;\n      glabels, gbboxes: Groundtruth bounding boxes. May be zero padded, hence\n        zero-class objects are ignored.\n      matching_threshold: Threshold for a positive match.\n    Return: Tuple of:\n       n_gbboxes: Scalar Tensor with number of groundtruth boxes (may difer from\n         size because of zero padding).\n       tp_match: (N,)-shaped boolean Tensor containing with True Positives.\n       fp_match: (N,)-shaped boolean Tensor containing with False Positives.\n    """"""\n    with tf.name_scope(scope, \'bboxes_matching_single\',\n                       [scores, bboxes, glabels, gbboxes]):\n        rsize = tf.size(scores)\n        rshape = tf.shape(scores)\n        rlabel = tf.cast(label, glabels.dtype)\n        # Number of groundtruth boxes.\n        gdifficults = tf.cast(gdifficults, tf.bool)\n        n_gbboxes = tf.count_nonzero(tf.logical_and(tf.equal(glabels, label),\n                                                    tf.logical_not(gdifficults)))\n        # Grountruth matching arrays.\n        gmatch = tf.zeros(tf.shape(glabels), dtype=tf.bool)\n        grange = tf.range(tf.size(glabels), dtype=tf.int32)\n        # True/False positive matching TensorArrays.\n        sdtype = tf.bool\n        ta_tp_bool = tf.TensorArray(sdtype, size=rsize, dynamic_size=False, infer_shape=True)\n        ta_fp_bool = tf.TensorArray(sdtype, size=rsize, dynamic_size=False, infer_shape=True)\n\n        # Loop over returned objects.\n        def m_condition(i, ta_tp, ta_fp, gmatch):\n            r = tf.less(i, rsize)\n            return r\n\n        def m_body(i, ta_tp, ta_fp, gmatch):\n            # Jaccard score with groundtruth bboxes.\n            rbbox = bboxes[i]\n            jaccard = bboxes_jaccard(rbbox, gbboxes)\n            jaccard = jaccard * tf.cast(tf.equal(glabels, rlabel), dtype=jaccard.dtype)\n\n            # Best fit, checking it\'s above threshold.\n            idxmax = tf.cast(tf.argmax(jaccard, axis=0), tf.int32)\n            jcdmax = jaccard[idxmax]\n            match = jcdmax > matching_threshold\n            existing_match = gmatch[idxmax]\n            not_difficult = tf.logical_not(gdifficults[idxmax])\n\n            # TP: match & no previous match and FP: previous match | no match.\n            # If difficult: no record, i.e FP=False and TP=False.\n            tp = tf.logical_and(not_difficult,\n                                tf.logical_and(match, tf.logical_not(existing_match)))\n            ta_tp = ta_tp.write(i, tp)\n            fp = tf.logical_and(not_difficult,\n                                tf.logical_or(existing_match, tf.logical_not(match)))\n            ta_fp = ta_fp.write(i, fp)\n            # Update grountruth match.\n            mask = tf.logical_and(tf.equal(grange, idxmax),\n                                  tf.logical_and(not_difficult, match))\n            gmatch = tf.logical_or(gmatch, mask)\n\n            return [i+1, ta_tp, ta_fp, gmatch]\n        # Main loop definition.\n        i = 0\n        [i, ta_tp_bool, ta_fp_bool, gmatch] = \\\n            tf.while_loop(m_condition, m_body,\n                          [i, ta_tp_bool, ta_fp_bool, gmatch],\n                          parallel_iterations=1,\n                          back_prop=False)\n        # TensorArrays to Tensors and reshape.\n        tp_match = tf.reshape(ta_tp_bool.stack(), rshape)\n        fp_match = tf.reshape(ta_fp_bool.stack(), rshape)\n\n        # Some debugging information...\n        # tp_match = tf.Print(tp_match,\n        #                     [n_gbboxes,\n        #                      tf.reduce_sum(tf.cast(tp_match, tf.int64)),\n        #                      tf.reduce_sum(tf.cast(fp_match, tf.int64)),\n        #                      tf.reduce_sum(tf.cast(gmatch, tf.int64))],\n        #                     \'Matching (NG, TP, FP, GM): \')\n        return n_gbboxes, tp_match, fp_match\n\n\ndef bboxes_matching_batch(labels, scores, bboxes,\n                          glabels, gbboxes, gdifficults,\n                          matching_threshold=0.5, scope=None):\n    """"""Matching a collection of detected boxes with groundtruth values.\n    Batched-inputs version.\n\n    Args:\n      rclasses, rscores, rbboxes: BxN(x4) Tensors. Detected objects, sorted by score;\n      glabels, gbboxes: Groundtruth bounding boxes. May be zero padded, hence\n        zero-class objects are ignored.\n      matching_threshold: Threshold for a positive match.\n    Return: Tuple or Dictionaries with:\n       n_gbboxes: Scalar Tensor with number of groundtruth boxes (may difer from\n         size because of zero padding).\n       tp: (B, N)-shaped boolean Tensor containing with True Positives.\n       fp: (B, N)-shaped boolean Tensor containing with False Positives.\n    """"""\n    # Dictionaries as inputs.\n    if isinstance(scores, dict) or isinstance(bboxes, dict):\n        with tf.name_scope(scope, \'bboxes_matching_batch_dict\'):\n            d_n_gbboxes = {}\n            d_tp = {}\n            d_fp = {}\n            for c in labels:\n                n, tp, fp, _ = bboxes_matching_batch(c, scores[c], bboxes[c],\n                                                     glabels, gbboxes, gdifficults,\n                                                     matching_threshold)\n                d_n_gbboxes[c] = n\n                d_tp[c] = tp\n                d_fp[c] = fp\n            return d_n_gbboxes, d_tp, d_fp, scores\n\n    with tf.name_scope(scope, \'bboxes_matching_batch\',\n                       [scores, bboxes, glabels, gbboxes]):\n        r = tf.map_fn(lambda x: bboxes_matching(labels, x[0], x[1],\n                                                x[2], x[3], x[4],\n                                                matching_threshold),\n                      (scores, bboxes, glabels, gbboxes, gdifficults),\n                      dtype=(tf.int64, tf.bool, tf.bool),\n                      parallel_iterations=10,\n                      back_prop=False,\n                      swap_memory=True,\n                      infer_shape=True)\n        return r[0], r[1], r[2], scores\n\n\n# =========================================================================== #\n# Some filteting methods.\n# =========================================================================== #\ndef bboxes_filter_center(labels, bboxes, margins=[0., 0., 0., 0.],\n                         scope=None):\n    """"""Filter out bounding boxes whose center are not in\n    the rectangle [0, 0, 1, 1] + margins. The margin Tensor\n    can be used to enforce or loosen this condition.\n\n    Return:\n      labels, bboxes: Filtered elements.\n    """"""\n    with tf.name_scope(scope, \'bboxes_filter\', [labels, bboxes]):\n        cy = (bboxes[:, 0] + bboxes[:, 2]) / 2.\n        cx = (bboxes[:, 1] + bboxes[:, 3]) / 2.\n        mask = tf.greater(cy, margins[0])\n        mask = tf.logical_and(mask, tf.greater(cx, margins[1]))\n        mask = tf.logical_and(mask, tf.less(cx, 1. + margins[2]))\n        mask = tf.logical_and(mask, tf.less(cx, 1. + margins[3]))\n        # Boolean masking...\n        labels = tf.boolean_mask(labels, mask)\n        bboxes = tf.boolean_mask(bboxes, mask)\n        return labels, bboxes\n\n\ndef bboxes_filter_overlap(labels, bboxes,\n                          threshold=0.5, assign_negative=False,\n                          scope=None):\n    """"""Filter out bounding boxes based on (relative )overlap with reference\n    box [0, 0, 1, 1].  Remove completely bounding boxes, or assign negative\n    labels to the one outside (useful for latter processing...).\n\n    Return:\n      labels, bboxes: Filtered (or newly assigned) elements.\n    """"""\n    with tf.name_scope(scope, \'bboxes_filter\', [labels, bboxes]):\n        scores = bboxes_intersection(tf.constant([0, 0, 1, 1], bboxes.dtype),\n                                     bboxes)\n        mask = scores > threshold\n        if assign_negative:\n            labels = tf.where(mask, labels, -labels)\n            # bboxes = tf.where(mask, bboxes, bboxes)\n        else:\n            labels = tf.boolean_mask(labels, mask)\n            bboxes = tf.boolean_mask(bboxes, mask)\n        return labels, bboxes\n\n\ndef bboxes_filter_labels(labels, bboxes,\n                         out_labels=[], num_classes=np.inf,\n                         scope=None):\n    """"""Filter out labels from a collection. Typically used to get\n    of DontCare elements. Also remove elements based on the number of classes.\n\n    Return:\n      labels, bboxes: Filtered elements.\n    """"""\n    with tf.name_scope(scope, \'bboxes_filter_labels\', [labels, bboxes]):\n        mask = tf.greater_equal(labels, num_classes)\n        for l in labels:\n            mask = tf.logical_and(mask, tf.not_equal(labels, l))\n        labels = tf.boolean_mask(labels, mask)\n        bboxes = tf.boolean_mask(bboxes, mask)\n        return labels, bboxes\n\n\n# =========================================================================== #\n# Standard boxes computation.\n# =========================================================================== #\ndef bboxes_jaccard(bbox_ref, bboxes, name=None):\n    """"""Compute jaccard score between a reference box and a collection\n    of bounding boxes.\n\n    Args:\n      bbox_ref: (N, 4) or (4,) Tensor with reference bounding box(es).\n      bboxes: (N, 4) Tensor, collection of bounding boxes.\n    Return:\n      (N,) Tensor with Jaccard scores.\n    """"""\n    with tf.name_scope(name, \'bboxes_jaccard\'):\n        # Should be more efficient to first transpose.\n        bboxes = tf.transpose(bboxes)\n        bbox_ref = tf.transpose(bbox_ref)\n        # Intersection bbox and volume.\n        int_ymin = tf.maximum(bboxes[0], bbox_ref[0])\n        int_xmin = tf.maximum(bboxes[1], bbox_ref[1])\n        int_ymax = tf.minimum(bboxes[2], bbox_ref[2])\n        int_xmax = tf.minimum(bboxes[3], bbox_ref[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        # Volumes.\n        inter_vol = h * w\n        union_vol = -inter_vol \\\n            + (bboxes[2] - bboxes[0]) * (bboxes[3] - bboxes[1]) \\\n            + (bbox_ref[2] - bbox_ref[0]) * (bbox_ref[3] - bbox_ref[1])\n        jaccard = tfe_math.safe_divide(inter_vol, union_vol, \'jaccard\')\n        return jaccard\n\n\ndef bboxes_intersection(bbox_ref, bboxes, name=None):\n    """"""Compute relative intersection between a reference box and a\n    collection of bounding boxes. Namely, compute the quotient between\n    intersection area and box area.\n\n    Args:\n      bbox_ref: (N, 4) or (4,) Tensor with reference bounding box(es).\n      bboxes: (N, 4) Tensor, collection of bounding boxes.\n    Return:\n      (N,) Tensor with relative intersection.\n    """"""\n    with tf.name_scope(name, \'bboxes_intersection\'):\n        # Should be more efficient to first transpose.\n        bboxes = tf.transpose(bboxes)\n        bbox_ref = tf.transpose(bbox_ref)\n        # Intersection bbox and volume.\n        int_ymin = tf.maximum(bboxes[0], bbox_ref[0])\n        int_xmin = tf.maximum(bboxes[1], bbox_ref[1])\n        int_ymax = tf.minimum(bboxes[2], bbox_ref[2])\n        int_xmax = tf.minimum(bboxes[3], bbox_ref[3])\n        h = tf.maximum(int_ymax - int_ymin, 0.)\n        w = tf.maximum(int_xmax - int_xmin, 0.)\n        # Volumes.\n        inter_vol = h * w\n        bboxes_vol = (bboxes[2] - bboxes[0]) * (bboxes[3] - bboxes[1])\n        scores = tfe_math.safe_divide(inter_vol, bboxes_vol, \'intersection\')\n        return scores\n'"
tf_extended/image.py,0,b''
tf_extended/math.py,6,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional math functions.\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\n\n\ndef safe_divide(numerator, denominator, name):\n    """"""Divides two values, returning 0 if the denominator is <= 0.\n    Args:\n      numerator: A real `Tensor`.\n      denominator: A real `Tensor`, with dtype matching `numerator`.\n      name: Name for the returned op.\n    Returns:\n      0 if `denominator` <= 0, else `numerator` / `denominator`\n    """"""\n    return tf.where(\n        math_ops.greater(denominator, 0),\n        math_ops.divide(numerator, denominator),\n        tf.zeros_like(numerator),\n        name=name)\n\n\ndef cummax(x, reverse=False, name=None):\n    """"""Compute the cumulative maximum of the tensor `x` along `axis`. This\n    operation is similar to the more classic `cumsum`. Only support 1D Tensor\n    for now.\n\n    Args:\n    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,\n       `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,\n       `complex128`, `qint8`, `quint8`, `qint32`, `half`.\n       axis: A `Tensor` of type `int32` (default: 0).\n       reverse: A `bool` (default: False).\n       name: A name for the operation (optional).\n    Returns:\n    A `Tensor`. Has the same type as `x`.\n    """"""\n    with ops.name_scope(name, ""Cummax"", [x]) as name:\n        x = ops.convert_to_tensor(x, name=""x"")\n        # Not very optimal: should directly integrate reverse into tf.scan.\n        if reverse:\n            x = tf.reverse(x, axis=[0])\n        # \'Accumlating\' maximum: ensure it is always increasing.\n        cmax = tf.scan(lambda a, y: tf.maximum(a, y), x,\n                       initializer=None, parallel_iterations=1,\n                       back_prop=False, swap_memory=False)\n        if reverse:\n            cmax = tf.reverse(cmax, axis=[0])\n        return cmax\n'"
tf_extended/metrics.py,91,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional metrics.\n""""""\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib.framework.python.ops import variables as contrib_variables\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\n\nfrom tf_extended import math as tfe_math\n\n\n# =========================================================================== #\n# TensorFlow utils\n# =========================================================================== #\ndef _create_local(name, shape, collections=None, validate_shape=True,\n                  dtype=dtypes.float32):\n    """"""Creates a new local variable.\n    Args:\n        name: The name of the new or existing variable.\n        shape: Shape of the new or existing variable.\n        collections: A list of collection names to which the Variable will be added.\n        validate_shape: Whether to validate the shape of the variable.\n        dtype: Data type of the variables.\n    Returns:\n        The created variable.\n    """"""\n    # Make sure local variables are added to tf.GraphKeys.LOCAL_VARIABLES\n    collections = list(collections or [])\n    collections += [ops.GraphKeys.LOCAL_VARIABLES]\n    return variables.Variable(\n            initial_value=array_ops.zeros(shape, dtype=dtype),\n            name=name,\n            trainable=False,\n            collections=collections,\n            validate_shape=validate_shape)\n\n\ndef _safe_div(numerator, denominator, name):\n    """"""Divides two values, returning 0 if the denominator is <= 0.\n    Args:\n      numerator: A real `Tensor`.\n      denominator: A real `Tensor`, with dtype matching `numerator`.\n      name: Name for the returned op.\n    Returns:\n      0 if `denominator` <= 0, else `numerator` / `denominator`\n    """"""\n    return tf.where(\n        math_ops.greater(denominator, 0),\n        math_ops.divide(numerator, denominator),\n        tf.zeros_like(numerator),\n        name=name)\n\n\ndef _broadcast_weights(weights, values):\n    """"""Broadcast `weights` to the same shape as `values`.\n    This returns a version of `weights` following the same broadcast rules as\n    `mul(weights, values)`. When computing a weighted average, use this function\n    to broadcast `weights` before summing them; e.g.,\n    `reduce_sum(w * v) / reduce_sum(_broadcast_weights(w, v))`.\n    Args:\n      weights: `Tensor` whose shape is broadcastable to `values`.\n      values: `Tensor` of any shape.\n    Returns:\n      `weights` broadcast to `values` shape.\n    """"""\n    weights_shape = weights.get_shape()\n    values_shape = values.get_shape()\n    if(weights_shape.is_fully_defined() and\n       values_shape.is_fully_defined() and\n       weights_shape.is_compatible_with(values_shape)):\n        return weights\n    return math_ops.mul(\n        weights, array_ops.ones_like(values), name=\'broadcast_weights\')\n\n\n# =========================================================================== #\n# TF Extended metrics: TP and FP arrays.\n# =========================================================================== #\ndef precision_recall(num_gbboxes, num_detections, tp, fp, scores,\n                     dtype=tf.float64, scope=None):\n    """"""Compute precision and recall from scores, true positives and false\n    positives booleans arrays\n    """"""\n    # Input dictionaries: dict outputs as streaming metrics.\n    if isinstance(scores, dict):\n        d_precision = {}\n        d_recall = {}\n        for c in num_gbboxes.keys():\n            scope = \'precision_recall_%s\' % c\n            p, r = precision_recall(num_gbboxes[c], num_detections[c],\n                                    tp[c], fp[c], scores[c],\n                                    dtype, scope)\n            d_precision[c] = p\n            d_recall[c] = r\n        return d_precision, d_recall\n\n    # Sort by score.\n    with tf.name_scope(scope, \'precision_recall\',\n                       [num_gbboxes, num_detections, tp, fp, scores]):\n        # Sort detections by score.\n        scores, idxes = tf.nn.top_k(scores, k=num_detections, sorted=True)\n        tp = tf.gather(tp, idxes)\n        fp = tf.gather(fp, idxes)\n        # Computer recall and precision.\n        tp = tf.cumsum(tf.cast(tp, dtype), axis=0)\n        fp = tf.cumsum(tf.cast(fp, dtype), axis=0)\n        recall = _safe_div(tp, tf.cast(num_gbboxes, dtype), \'recall\')\n        precision = _safe_div(tp, tp + fp, \'precision\')\n        return tf.tuple([precision, recall])\n\n\ndef streaming_tp_fp_arrays(num_gbboxes, tp, fp, scores,\n                           remove_zero_scores=True,\n                           metrics_collections=None,\n                           updates_collections=None,\n                           name=None):\n    """"""Streaming computation of True and False Positive arrays. This metrics\n    also keeps track of scores and number of grountruth objects.\n    """"""\n    # Input dictionaries: dict outputs as streaming metrics.\n    if isinstance(scores, dict) or isinstance(fp, dict):\n        d_values = {}\n        d_update_ops = {}\n        for c in num_gbboxes.keys():\n            scope = \'streaming_tp_fp_%s\' % c\n            v, up = streaming_tp_fp_arrays(num_gbboxes[c], tp[c], fp[c], scores[c],\n                                           remove_zero_scores,\n                                           metrics_collections,\n                                           updates_collections,\n                                           name=scope)\n            d_values[c] = v\n            d_update_ops[c] = up\n        return d_values, d_update_ops\n\n    # Input Tensors...\n    with variable_scope.variable_scope(name, \'streaming_tp_fp\',\n                                       [num_gbboxes, tp, fp, scores]):\n        num_gbboxes = math_ops.to_int64(num_gbboxes)\n        scores = math_ops.to_float(scores)\n        stype = tf.bool\n        tp = tf.cast(tp, stype)\n        fp = tf.cast(fp, stype)\n        # Reshape TP and FP tensors and clean away 0 class values.\n        scores = tf.reshape(scores, [-1])\n        tp = tf.reshape(tp, [-1])\n        fp = tf.reshape(fp, [-1])\n        # Remove TP and FP both false.\n        mask = tf.logical_or(tp, fp)\n        if remove_zero_scores:\n            rm_threshold = 1e-4\n            mask = tf.logical_and(mask, tf.greater(scores, rm_threshold))\n            scores = tf.boolean_mask(scores, mask)\n            tp = tf.boolean_mask(tp, mask)\n            fp = tf.boolean_mask(fp, mask)\n\n        # Local variables accumlating information over batches.\n        v_nobjects = _create_local(\'v_num_gbboxes\', shape=[], dtype=tf.int64)\n        v_ndetections = _create_local(\'v_num_detections\', shape=[], dtype=tf.int32)\n        v_scores = _create_local(\'v_scores\', shape=[0, ])\n        v_tp = _create_local(\'v_tp\', shape=[0, ], dtype=stype)\n        v_fp = _create_local(\'v_fp\', shape=[0, ], dtype=stype)\n\n        # Update operations.\n        nobjects_op = state_ops.assign_add(v_nobjects,\n                                           tf.reduce_sum(num_gbboxes))\n        ndetections_op = state_ops.assign_add(v_ndetections,\n                                              tf.size(scores, out_type=tf.int32))\n        scores_op = state_ops.assign(v_scores, tf.concat([v_scores, scores], axis=0),\n                                     validate_shape=False)\n        tp_op = state_ops.assign(v_tp, tf.concat([v_tp, tp], axis=0),\n                                 validate_shape=False)\n        fp_op = state_ops.assign(v_fp, tf.concat([v_fp, fp], axis=0),\n                                 validate_shape=False)\n\n        # Value and update ops.\n        val = (v_nobjects, v_ndetections, v_tp, v_fp, v_scores)\n        with ops.control_dependencies([nobjects_op, ndetections_op,\n                                       scores_op, tp_op, fp_op]):\n            update_op = (nobjects_op, ndetections_op, tp_op, fp_op, scores_op)\n\n        if metrics_collections:\n            ops.add_to_collections(metrics_collections, val)\n        if updates_collections:\n            ops.add_to_collections(updates_collections, update_op)\n        return val, update_op\n\n\n# =========================================================================== #\n# Average precision computations.\n# =========================================================================== #\ndef average_precision_voc12(precision, recall, name=None):\n    """"""Compute (interpolated) average precision from precision and recall Tensors.\n\n    The implementation follows Pascal 2012 and ILSVRC guidelines.\n    See also: https://sanchom.wordpress.com/tag/average-precision/\n    """"""\n    with tf.name_scope(name, \'average_precision_voc12\', [precision, recall]):\n        # Convert to float64 to decrease error on Riemann sums.\n        precision = tf.cast(precision, dtype=tf.float64)\n        recall = tf.cast(recall, dtype=tf.float64)\n\n        # Add bounds values to precision and recall.\n        precision = tf.concat([[0.], precision, [0.]], axis=0)\n        recall = tf.concat([[0.], recall, [1.]], axis=0)\n        # Ensures precision is increasing in reverse order.\n        precision = tfe_math.cummax(precision, reverse=True)\n\n        # Riemann sums for estimating the integral.\n        # mean_pre = (precision[1:] + precision[:-1]) / 2.\n        mean_pre = precision[1:]\n        diff_rec = recall[1:] - recall[:-1]\n        ap = tf.reduce_sum(mean_pre * diff_rec)\n        return ap\n\n\ndef average_precision_voc07(precision, recall, name=None):\n    """"""Compute (interpolated) average precision from precision and recall Tensors.\n\n    The implementation follows Pascal 2007 guidelines.\n    See also: https://sanchom.wordpress.com/tag/average-precision/\n    """"""\n    with tf.name_scope(name, \'average_precision_voc07\', [precision, recall]):\n        # Convert to float64 to decrease error on cumulated sums.\n        precision = tf.cast(precision, dtype=tf.float64)\n        recall = tf.cast(recall, dtype=tf.float64)\n        # Add zero-limit value to avoid any boundary problem...\n        precision = tf.concat([precision, [0.]], axis=0)\n        recall = tf.concat([recall, [np.inf]], axis=0)\n\n        # Split the integral into 10 bins.\n        l_aps = []\n        for t in np.arange(0., 1.1, 0.1):\n            mask = tf.greater_equal(recall, t)\n            v = tf.reduce_max(tf.boolean_mask(precision, mask))\n            l_aps.append(v / 11.)\n        ap = tf.add_n(l_aps)\n        return ap\n\n\ndef precision_recall_values(xvals, precision, recall, name=None):\n    """"""Compute values on the precision/recall curve.\n\n    Args:\n      x: Python list of floats;\n      precision: 1D Tensor decreasing.\n      recall: 1D Tensor increasing.\n    Return:\n      list of precision values.\n    """"""\n    with ops.name_scope(name, ""precision_recall_values"",\n                        [precision, recall]) as name:\n        # Add bounds values to precision and recall.\n        precision = tf.concat([[0.], precision, [0.]], axis=0)\n        recall = tf.concat([[0.], recall, [1.]], axis=0)\n        precision = tfe_math.cummax(precision, reverse=True)\n\n        prec_values = []\n        for x in xvals:\n            mask = tf.less_equal(recall, x)\n            val = tf.reduce_min(tf.boolean_mask(precision, mask))\n            prec_values.append(val)\n        return tf.tuple(prec_values)\n\n\n# =========================================================================== #\n# TF Extended metrics: old stuff!\n# =========================================================================== #\ndef _precision_recall(n_gbboxes, n_detections, scores, tp, fp, scope=None):\n    """"""Compute precision and recall from scores, true positives and false\n    positives booleans arrays\n    """"""\n    # Sort by score.\n    with tf.name_scope(scope, \'prec_rec\', [n_gbboxes, scores, tp, fp]):\n        # Sort detections by score.\n        scores, idxes = tf.nn.top_k(scores, k=n_detections, sorted=True)\n        tp = tf.gather(tp, idxes)\n        fp = tf.gather(fp, idxes)\n        # Computer recall and precision.\n        dtype = tf.float64\n        tp = tf.cumsum(tf.cast(tp, dtype), axis=0)\n        fp = tf.cumsum(tf.cast(fp, dtype), axis=0)\n        recall = _safe_div(tp, tf.cast(n_gbboxes, dtype), \'recall\')\n        precision = _safe_div(tp, tp + fp, \'precision\')\n\n        return tf.tuple([precision, recall])\n\n\ndef streaming_precision_recall_arrays(n_gbboxes, rclasses, rscores,\n                                      tp_tensor, fp_tensor,\n                                      remove_zero_labels=True,\n                                      metrics_collections=None,\n                                      updates_collections=None,\n                                      name=None):\n    """"""Streaming computation of precision / recall arrays. This metrics\n    keeps tracks of boolean True positives and False positives arrays.\n    """"""\n    with variable_scope.variable_scope(name, \'stream_precision_recall\',\n                                       [n_gbboxes, rclasses, tp_tensor, fp_tensor]):\n        n_gbboxes = math_ops.to_int64(n_gbboxes)\n        rclasses = math_ops.to_int64(rclasses)\n        rscores = math_ops.to_float(rscores)\n\n        stype = tf.int32\n        tp_tensor = tf.cast(tp_tensor, stype)\n        fp_tensor = tf.cast(fp_tensor, stype)\n\n        # Reshape TP and FP tensors and clean away 0 class values.\n        rclasses = tf.reshape(rclasses, [-1])\n        rscores = tf.reshape(rscores, [-1])\n        tp_tensor = tf.reshape(tp_tensor, [-1])\n        fp_tensor = tf.reshape(fp_tensor, [-1])\n        if remove_zero_labels:\n            mask = tf.greater(rclasses, 0)\n            rclasses = tf.boolean_mask(rclasses, mask)\n            rscores = tf.boolean_mask(rscores, mask)\n            tp_tensor = tf.boolean_mask(tp_tensor, mask)\n            fp_tensor = tf.boolean_mask(fp_tensor, mask)\n\n        # Local variables accumlating information over batches.\n        v_nobjects = _create_local(\'v_nobjects\', shape=[], dtype=tf.int64)\n        v_ndetections = _create_local(\'v_ndetections\', shape=[], dtype=tf.int32)\n        v_scores = _create_local(\'v_scores\', shape=[0, ])\n        v_tp = _create_local(\'v_tp\', shape=[0, ], dtype=stype)\n        v_fp = _create_local(\'v_fp\', shape=[0, ], dtype=stype)\n\n        # Update operations.\n        nobjects_op = state_ops.assign_add(v_nobjects,\n                                           tf.reduce_sum(n_gbboxes))\n        ndetections_op = state_ops.assign_add(v_ndetections,\n                                              tf.size(rscores, out_type=tf.int32))\n        scores_op = state_ops.assign(v_scores, tf.concat([v_scores, rscores], axis=0),\n                                     validate_shape=False)\n        tp_op = state_ops.assign(v_tp, tf.concat([v_tp, tp_tensor], axis=0),\n                                 validate_shape=False)\n        fp_op = state_ops.assign(v_fp, tf.concat([v_fp, fp_tensor], axis=0),\n                                 validate_shape=False)\n\n        # Precision and recall computations.\n        # r = _precision_recall(nobjects_op, scores_op, tp_op, fp_op, \'value\')\n        r = _precision_recall(v_nobjects, v_ndetections, v_scores,\n                              v_tp, v_fp, \'value\')\n\n        with ops.control_dependencies([nobjects_op, ndetections_op,\n                                       scores_op, tp_op, fp_op]):\n            update_op = _precision_recall(nobjects_op, ndetections_op,\n                                          scores_op, tp_op, fp_op, \'update_op\')\n\n            # update_op = tf.Print(update_op,\n            #                      [tf.reduce_sum(tf.cast(mask, tf.int64)),\n            #                       tf.reduce_sum(tf.cast(mask2, tf.int64)),\n            #                       tf.reduce_min(rscores),\n            #                       tf.reduce_sum(n_gbboxes)],\n            #                      \'Metric: \')\n            # Some debugging stuff!\n            # update_op = tf.Print(update_op,\n            #                      [tf.shape(tp_op),\n            #                       tf.reduce_sum(tf.cast(tp_op, tf.int64), axis=0)],\n            #                      \'TP and FP shape: \')\n            # update_op[0] = tf.Print(update_op,\n            #                      [nobjects_op],\n            #                      \'# Groundtruth bboxes: \')\n            # update_op = tf.Print(update_op,\n            #                      [update_op[0][0],\n            #                       update_op[0][-1],\n            #                       tf.reduce_min(update_op[0]),\n            #                       tf.reduce_max(update_op[0]),\n            #                       tf.reduce_min(update_op[1]),\n            #                       tf.reduce_max(update_op[1])],\n            #                      \'Precision and recall :\')\n\n        if metrics_collections:\n            ops.add_to_collections(metrics_collections, r)\n        if updates_collections:\n            ops.add_to_collections(updates_collections, update_op)\n        return r, update_op\n\n'"
tf_extended/tensors.py,12,"b'# Copyright 2017 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF Extended: additional tensors operations.\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.contrib.framework.python.ops import variables as contrib_variables\nfrom tensorflow.contrib.metrics.python.ops import set_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\n\n\ndef get_shape(x, rank=None):\n    """"""Returns the dimensions of a Tensor as list of integers or scale tensors.\n\n    Args:\n      x: N-d Tensor;\n      rank: Rank of the Tensor. If None, will try to guess it.\n    Returns:\n      A list of `[d1, d2, ..., dN]` corresponding to the dimensions of the\n        input tensor.  Dimensions that are statically known are python integers,\n        otherwise they are integer scalar tensors.\n    """"""\n    if x.get_shape().is_fully_defined():\n        return x.get_shape().as_list()\n    else:\n        static_shape = x.get_shape()\n        if rank is None:\n            static_shape = static_shape.as_list()\n            rank = len(static_shape)\n        else:\n            static_shape = x.get_shape().with_rank(rank).as_list()\n        dynamic_shape = tf.unstack(tf.shape(x), rank)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef pad_axis(x, offset, size, axis=0, name=None):\n    """"""Pad a tensor on an axis, with a given offset and output size.\n    The tensor is padded with zero (i.e. CONSTANT mode). Note that the if the\n    `size` is smaller than existing size + `offset`, the output tensor\n    was the latter dimension.\n\n    Args:\n      x: Tensor to pad;\n      offset: Offset to add on the dimension chosen;\n      size: Final size of the dimension.\n    Return:\n      Padded tensor whose dimension on `axis` is `size`, or greater if\n      the input vector was larger.\n    """"""\n    with tf.name_scope(name, \'pad_axis\'):\n        shape = get_shape(x)\n        rank = len(shape)\n        # Padding description.\n        new_size = tf.maximum(size-offset-shape[axis], 0)\n        pad1 = tf.stack([0]*axis + [offset] + [0]*(rank-axis-1))\n        pad2 = tf.stack([0]*axis + [new_size] + [0]*(rank-axis-1))\n        paddings = tf.stack([pad1, pad2], axis=1)\n        x = tf.pad(x, paddings, mode=\'CONSTANT\')\n        # Reshape, to get fully defined shape if possible.\n        # TODO: fix with tf.slice\n        shape[axis] = size\n        x = tf.reshape(x, tf.stack(shape))\n        return x\n\n\n# def select_at_index(idx, val, t):\n#     """"""Return a tensor.\n#     """"""\n#     idx = tf.expand_dims(tf.expand_dims(idx, 0), 0)\n#     val = tf.expand_dims(val, 0)\n#     t = t + tf.scatter_nd(idx, val, tf.shape(t))\n#     return t\n'"
