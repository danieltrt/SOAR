file_path,api_count,code
complete.py,2,"b""#!/usr/bin/env python3\n#\n# Brandon Amos (http://bamos.github.io)\n# License: MIT\n# 2016-08-05\n\nimport argparse\nimport os\nimport tensorflow as tf\n\nfrom model import DCGAN\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--approach', type=str,\n                    choices=['adam', 'hmc'],\n                    default='adam')\nparser.add_argument('--lr', type=float, default=0.01)\nparser.add_argument('--beta1', type=float, default=0.9)\nparser.add_argument('--beta2', type=float, default=0.999)\nparser.add_argument('--eps', type=float, default=1e-8)\nparser.add_argument('--hmcBeta', type=float, default=0.2)\nparser.add_argument('--hmcEps', type=float, default=0.001)\nparser.add_argument('--hmcL', type=int, default=100)\nparser.add_argument('--hmcAnneal', type=float, default=1)\nparser.add_argument('--nIter', type=int, default=1000)\nparser.add_argument('--imgSize', type=int, default=64)\nparser.add_argument('--lam', type=float, default=0.1)\nparser.add_argument('--checkpointDir', type=str, default='checkpoint')\nparser.add_argument('--outDir', type=str, default='completions')\nparser.add_argument('--outInterval', type=int, default=50)\nparser.add_argument('--maskType', type=str,\n                    choices=['random', 'center', 'left', 'full', 'grid', 'lowres'],\n                    default='center')\nparser.add_argument('--centerScale', type=float, default=0.25)\nparser.add_argument('imgs', type=str, nargs='+')\n\nargs = parser.parse_args()\n\nassert(os.path.exists(args.checkpointDir))\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n    dcgan = DCGAN(sess, image_size=args.imgSize,\n                  batch_size=min(64, len(args.imgs)),\n                  checkpoint_dir=args.checkpointDir, lam=args.lam)\n    dcgan.complete(args)\n"""
model.py,52,"b'# Original Version: Taehoon Kim (http://carpedm20.github.io)\n#   + Source: https://github.com/carpedm20/DCGAN-tensorflow/blob/e30539fb5e20d5a0fed40935853da97e9e55eee8/model.py\n#   + License: MIT\n# [2016-08-05] Modifications for Completion: Brandon Amos (http://bamos.github.io)\n#   + License: MIT\n\nfrom __future__ import division\nimport os\nimport time\nimport math\nimport itertools\nfrom glob import glob\nimport tensorflow as tf\nfrom six.moves import xrange\n\nfrom ops import *\nfrom utils import *\n\nSUPPORTED_EXTENSIONS = [""png"", ""jpg"", ""jpeg""]\n\ndef dataset_files(root):\n    """"""Returns a list of all image files in the given directory""""""\n    return list(itertools.chain.from_iterable(\n        glob(os.path.join(root, ""*.{}"".format(ext))) for ext in SUPPORTED_EXTENSIONS))\n\n\nclass DCGAN(object):\n    def __init__(self, sess, image_size=64, is_crop=False,\n                 batch_size=64, sample_size=64, lowres=8,\n                 z_dim=100, gf_dim=64, df_dim=64,\n                 gfc_dim=1024, dfc_dim=1024, c_dim=3,\n                 checkpoint_dir=None, lam=0.1):\n        """"""\n\n        Args:\n            sess: TensorFlow session\n            batch_size: The size of batch. Should be specified before training.\n            lowres: (optional) Low resolution image/mask shrink factor. [8]\n            z_dim: (optional) Dimension of dim for Z. [100]\n            gf_dim: (optional) Dimension of gen filters in first conv layer. [64]\n            df_dim: (optional) Dimension of discrim filters in first conv layer. [64]\n            gfc_dim: (optional) Dimension of gen untis for for fully connected layer. [1024]\n            dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]\n            c_dim: (optional) Dimension of image color. [3]\n        """"""\n        # Currently, image size must be a (power of 2) and (8 or higher).\n        assert(image_size & (image_size - 1) == 0 and image_size >= 8)\n\n        self.sess = sess\n        self.is_crop = is_crop\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.sample_size = sample_size\n        self.image_shape = [image_size, image_size, c_dim]\n\n        self.lowres = lowres\n        self.lowres_size = image_size // lowres\n        self.lowres_shape = [self.lowres_size, self.lowres_size, c_dim]\n\n        self.z_dim = z_dim\n\n        self.gf_dim = gf_dim\n        self.df_dim = df_dim\n\n        self.gfc_dim = gfc_dim\n        self.dfc_dim = dfc_dim\n\n        self.lam = lam\n\n        self.c_dim = c_dim\n\n        # batch normalization : deals with poor initialization helps gradient flow\n        self.d_bns = [\n            batch_norm(name=\'d_bn{}\'.format(i,)) for i in range(4)]\n\n        log_size = int(math.log(image_size) / math.log(2))\n        self.g_bns = [\n            batch_norm(name=\'g_bn{}\'.format(i,)) for i in range(log_size)]\n\n        self.checkpoint_dir = checkpoint_dir\n        self.build_model()\n\n        self.model_name = ""DCGAN.model""\n\n    def build_model(self):\n        self.is_training = tf.placeholder(tf.bool, name=\'is_training\')\n        self.images = tf.placeholder(\n            tf.float32, [None] + self.image_shape, name=\'real_images\')\n        self.lowres_images = tf.reduce_mean(tf.reshape(self.images,\n            [self.batch_size, self.lowres_size, self.lowres,\n             self.lowres_size, self.lowres, self.c_dim]), [2, 4])\n        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name=\'z\')\n        self.z_sum = tf.summary.histogram(""z"", self.z)\n\n        self.G = self.generator(self.z)\n        self.lowres_G = tf.reduce_mean(tf.reshape(self.G,\n            [self.batch_size, self.lowres_size, self.lowres,\n             self.lowres_size, self.lowres, self.c_dim]), [2, 4])\n        self.D, self.D_logits = self.discriminator(self.images)\n\n        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n\n        self.d_sum = tf.summary.histogram(""d"", self.D)\n        self.d__sum = tf.summary.histogram(""d_"", self.D_)\n        self.G_sum = tf.summary.image(""G"", self.G)\n\n        self.d_loss_real = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,\n                                                    labels=tf.ones_like(self.D)))\n        self.d_loss_fake = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,\n                                                    labels=tf.zeros_like(self.D_)))\n        self.g_loss = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,\n                                                    labels=tf.ones_like(self.D_)))\n\n        self.d_loss_real_sum = tf.summary.scalar(""d_loss_real"", self.d_loss_real)\n        self.d_loss_fake_sum = tf.summary.scalar(""d_loss_fake"", self.d_loss_fake)\n\n        self.d_loss = self.d_loss_real + self.d_loss_fake\n\n        self.g_loss_sum = tf.summary.scalar(""g_loss"", self.g_loss)\n        self.d_loss_sum = tf.summary.scalar(""d_loss"", self.d_loss)\n\n        t_vars = tf.trainable_variables()\n\n        self.d_vars = [var for var in t_vars if \'d_\' in var.name]\n        self.g_vars = [var for var in t_vars if \'g_\' in var.name]\n\n        self.saver = tf.train.Saver(max_to_keep=1)\n\n        # Completion.\n        self.mask = tf.placeholder(tf.float32, self.image_shape, name=\'mask\')\n        self.lowres_mask = tf.placeholder(tf.float32, self.lowres_shape, name=\'lowres_mask\')\n        self.contextual_loss = tf.reduce_sum(\n            tf.contrib.layers.flatten(\n                tf.abs(tf.multiply(self.mask, self.G) - tf.multiply(self.mask, self.images))), 1)\n        self.contextual_loss += tf.reduce_sum(\n            tf.contrib.layers.flatten(\n                tf.abs(tf.multiply(self.lowres_mask, self.lowres_G) - tf.multiply(self.lowres_mask, self.lowres_images))), 1)\n        self.perceptual_loss = self.g_loss\n        self.complete_loss = self.contextual_loss + self.lam*self.perceptual_loss\n        self.grad_complete_loss = tf.gradients(self.complete_loss, self.z)\n\n    def train(self, config):\n        data = dataset_files(config.dataset)\n        np.random.shuffle(data)\n        assert(len(data) > 0)\n\n        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n                          .minimize(self.d_loss, var_list=self.d_vars)\n        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n                          .minimize(self.g_loss, var_list=self.g_vars)                \n        try:\n            tf.global_variables_initializer().run()\n        except:\n            tf.initialize_all_variables().run()\n\n        self.g_sum = tf.summary.merge(\n            [self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n        self.d_sum = tf.summary.merge(\n            [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n        self.writer = tf.summary.FileWriter(""./logs"", self.sess.graph)\n\n        sample_z = np.random.uniform(-1, 1, size=(self.sample_size , self.z_dim))\n        sample_files = data[0:self.sample_size]\n\n        sample = [get_image(sample_file, self.image_size, is_crop=self.is_crop) for sample_file in sample_files]\n        sample_images = np.array(sample).astype(np.float32)\n\n        counter = 1\n        start_time = time.time()\n\n        if self.load(self.checkpoint_dir):\n            print(""""""\n\n======\nAn existing model was found in the checkpoint directory.\nIf you just cloned this repository, it\'s a model for faces\ntrained on the CelebA dataset for 20 epochs.\nIf you want to train a new model from scratch,\ndelete the checkpoint directory or specify a different\n--checkpoint_dir argument.\n======\n\n"""""")\n        else:\n            print(""""""\n\n======\nAn existing model was not found in the checkpoint directory.\nInitializing a new one.\n======\n\n"""""")\n\n        for epoch in xrange(config.epoch):\n            data = dataset_files(config.dataset)\n            batch_idxs = min(len(data), config.train_size) // self.batch_size\n\n            for idx in xrange(0, batch_idxs):\n                batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size]\n                batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop)\n                         for batch_file in batch_files]\n                batch_images = np.array(batch).astype(np.float32)\n\n                batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\\n                            .astype(np.float32)\n\n                # Update D network\n                _, summary_str = self.sess.run([d_optim, self.d_sum],\n                    feed_dict={ self.images: batch_images, self.z: batch_z, self.is_training: True })\n                self.writer.add_summary(summary_str, counter)\n\n                # Update G network\n                _, summary_str = self.sess.run([g_optim, self.g_sum],\n                    feed_dict={ self.z: batch_z, self.is_training: True })\n                self.writer.add_summary(summary_str, counter)\n\n                # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n                _, summary_str = self.sess.run([g_optim, self.g_sum],\n                    feed_dict={ self.z: batch_z, self.is_training: True })\n                self.writer.add_summary(summary_str, counter)\n\n                errD_fake = self.d_loss_fake.eval({self.z: batch_z, self.is_training: False})\n                errD_real = self.d_loss_real.eval({self.images: batch_images, self.is_training: False})\n                errG = self.g_loss.eval({self.z: batch_z, self.is_training: False})\n\n                counter += 1\n                print(""Epoch: [{:2d}] [{:4d}/{:4d}] time: {:4.4f}, d_loss: {:.8f}, g_loss: {:.8f}"".format(\n                    epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG))\n\n                if np.mod(counter, 100) == 1:\n                    samples, d_loss, g_loss = self.sess.run(\n                        [self.G, self.d_loss, self.g_loss],\n                        feed_dict={self.z: sample_z, self.images: sample_images, self.is_training: False}\n                    )\n                    save_images(samples, [8, 8],\n                                \'./samples/train_{:02d}_{:04d}.png\'.format(epoch, idx))\n                    print(""[Sample] d_loss: {:.8f}, g_loss: {:.8f}"".format(d_loss, g_loss))\n\n                if np.mod(counter, 500) == 2:\n                    self.save(config.checkpoint_dir, counter)\n\n\n    def complete(self, config):\n        def make_dir(name):\n            # Works on python 2.7, where exist_ok arg to makedirs isn\'t available.\n            p = os.path.join(config.outDir, name)\n            if not os.path.exists(p):\n                os.makedirs(p)\n        make_dir(\'hats_imgs\')\n        make_dir(\'completed\')\n        make_dir(\'logs\')\n\n        try:\n            tf.global_variables_initializer().run()\n        except:\n            tf.initialize_all_variables().run()\n\n        isLoaded = self.load(self.checkpoint_dir)\n        assert(isLoaded)\n\n        nImgs = len(config.imgs)\n\n        batch_idxs = int(np.ceil(nImgs/self.batch_size))\n        lowres_mask = np.zeros(self.lowres_shape)\n        if config.maskType == \'random\':\n            fraction_masked = 0.2\n            mask = np.ones(self.image_shape)\n            mask[np.random.random(self.image_shape[:2]) < fraction_masked] = 0.0\n        elif config.maskType == \'center\':\n            assert(config.centerScale <= 0.5)\n            mask = np.ones(self.image_shape)\n            sz = self.image_size\n            l = int(self.image_size*config.centerScale)\n            u = int(self.image_size*(1.0-config.centerScale))\n            mask[l:u, l:u, :] = 0.0\n        elif config.maskType == \'left\':\n            mask = np.ones(self.image_shape)\n            c = self.image_size // 2\n            mask[:,:c,:] = 0.0\n        elif config.maskType == \'full\':\n            mask = np.ones(self.image_shape)\n        elif config.maskType == \'grid\':\n            mask = np.zeros(self.image_shape)\n            mask[::4,::4,:] = 1.0\n        elif config.maskType == \'lowres\':\n            lowres_mask = np.ones(self.lowres_shape)\n            mask = np.zeros(self.image_shape)\n        else:\n            assert(False)\n\n        for idx in xrange(0, batch_idxs):\n            l = idx*self.batch_size\n            u = min((idx+1)*self.batch_size, nImgs)\n            batchSz = u-l\n            batch_files = config.imgs[l:u]\n            batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop)\n                     for batch_file in batch_files]\n            batch_images = np.array(batch).astype(np.float32)\n            if batchSz < self.batch_size:\n                print(batchSz)\n                padSz = ((0, int(self.batch_size-batchSz)), (0,0), (0,0), (0,0))\n                batch_images = np.pad(batch_images, padSz, \'constant\')\n                batch_images = batch_images.astype(np.float32)\n\n            zhats = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n            m = 0\n            v = 0\n\n            nRows = np.ceil(batchSz/8)\n            nCols = min(8, batchSz)\n            save_images(batch_images[:batchSz,:,:,:], [nRows,nCols],\n                        os.path.join(config.outDir, \'before.png\'))\n            masked_images = np.multiply(batch_images, mask)\n            save_images(masked_images[:batchSz,:,:,:], [nRows,nCols],\n                        os.path.join(config.outDir, \'masked.png\'))\n            if lowres_mask.any():\n                lowres_images = np.reshape(batch_images, [self.batch_size, self.lowres_size, self.lowres,\n                    self.lowres_size, self.lowres, self.c_dim]).mean(4).mean(2)\n                lowres_images = np.multiply(lowres_images, lowres_mask)\n                lowres_images = np.repeat(np.repeat(lowres_images, self.lowres, 1), self.lowres, 2)\n                save_images(lowres_images[:batchSz,:,:,:], [nRows,nCols],\n                            os.path.join(config.outDir, \'lowres.png\'))\n            for img in range(batchSz):\n                with open(os.path.join(config.outDir, \'logs/hats_{:02d}.log\'.format(img)), \'a\') as f:\n                    f.write(\'iter loss \' +\n                            \' \'.join([\'z{}\'.format(zi) for zi in range(self.z_dim)]) +\n                            \'\\n\')\n\n            for i in xrange(config.nIter):\n                fd = {\n                    self.z: zhats,\n                    self.mask: mask,\n                    self.lowres_mask: lowres_mask,\n                    self.images: batch_images,\n                    self.is_training: False\n                }\n                run = [self.complete_loss, self.grad_complete_loss, self.G, self.lowres_G]\n                loss, g, G_imgs, lowres_G_imgs = self.sess.run(run, feed_dict=fd)\n\n                for img in range(batchSz):\n                    with open(os.path.join(config.outDir, \'logs/hats_{:02d}.log\'.format(img)), \'ab\') as f:\n                        f.write(\'{} {} \'.format(i, loss[img]).encode())\n                        np.savetxt(f, zhats[img:img+1])\n\n                if i % config.outInterval == 0:\n                    print(i, np.mean(loss[0:batchSz]))\n                    imgName = os.path.join(config.outDir,\n                                           \'hats_imgs/{:04d}.png\'.format(i))\n                    nRows = np.ceil(batchSz/8)\n                    nCols = min(8, batchSz)\n                    save_images(G_imgs[:batchSz,:,:,:], [nRows,nCols], imgName)\n                    if lowres_mask.any():\n                        imgName = imgName[:-4] + \'.lowres.png\'\n                        save_images(np.repeat(np.repeat(lowres_G_imgs[:batchSz,:,:,:],\n                                              self.lowres, 1), self.lowres, 2),\n                                    [nRows,nCols], imgName)\n\n                    inv_masked_hat_images = np.multiply(G_imgs, 1.0-mask)\n                    completed = masked_images + inv_masked_hat_images\n                    imgName = os.path.join(config.outDir,\n                                           \'completed/{:04d}.png\'.format(i))\n                    save_images(completed[:batchSz,:,:,:], [nRows,nCols], imgName)\n\n                if config.approach == \'adam\':\n                    # Optimize single completion with Adam\n                    m_prev = np.copy(m)\n                    v_prev = np.copy(v)\n                    m = config.beta1 * m_prev + (1 - config.beta1) * g[0]\n                    v = config.beta2 * v_prev + (1 - config.beta2) * np.multiply(g[0], g[0])\n                    m_hat = m / (1 - config.beta1 ** (i + 1))\n                    v_hat = v / (1 - config.beta2 ** (i + 1))\n                    zhats += - np.true_divide(config.lr * m_hat, (np.sqrt(v_hat) + config.eps))\n                    zhats = np.clip(zhats, -1, 1)\n\n                elif config.approach == \'hmc\':\n                    # Sample example completions with HMC (not in paper)\n                    zhats_old = np.copy(zhats)\n                    loss_old = np.copy(loss)\n                    v = np.random.randn(self.batch_size, self.z_dim)\n                    v_old = np.copy(v)\n\n                    for steps in range(config.hmcL):\n                        v -= config.hmcEps/2 * config.hmcBeta * g[0]\n                        zhats += config.hmcEps * v\n                        np.copyto(zhats, np.clip(zhats, -1, 1))\n                        loss, g, _, _ = self.sess.run(run, feed_dict=fd)\n                        v -= config.hmcEps/2 * config.hmcBeta * g[0]\n\n                    for img in range(batchSz):\n                        logprob_old = config.hmcBeta * loss_old[img] + np.sum(v_old[img]**2)/2\n                        logprob = config.hmcBeta * loss[img] + np.sum(v[img]**2)/2\n                        accept = np.exp(logprob_old - logprob)\n                        if accept < 1 and np.random.uniform() > accept:\n                            np.copyto(zhats[img], zhats_old[img])\n\n                    config.hmcBeta *= config.hmcAnneal\n\n                else:\n                    assert(False)\n\n    def discriminator(self, image, reuse=False):\n        with tf.variable_scope(""discriminator"") as scope:\n            if reuse:\n                scope.reuse_variables()\n\n            # TODO: Investigate how to parameterise discriminator based off image size.\n            h0 = lrelu(conv2d(image, self.df_dim, name=\'d_h0_conv\'))\n            h1 = lrelu(self.d_bns[0](conv2d(h0, self.df_dim*2, name=\'d_h1_conv\'), self.is_training))\n            h2 = lrelu(self.d_bns[1](conv2d(h1, self.df_dim*4, name=\'d_h2_conv\'), self.is_training))\n            h3 = lrelu(self.d_bns[2](conv2d(h2, self.df_dim*8, name=\'d_h3_conv\'), self.is_training))\n            h4 = linear(tf.reshape(h3, [-1, 8192]), 1, \'d_h4_lin\')\n    \n            return tf.nn.sigmoid(h4), h4\n\n    def generator(self, z):\n        with tf.variable_scope(""generator"") as scope:\n            self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, \'g_h0_lin\', with_w=True)\n    \n            # TODO: Nicer iteration pattern here. #readability\n            hs = [None]\n            hs[0] = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])\n            hs[0] = tf.nn.relu(self.g_bns[0](hs[0], self.is_training))\n\n            i = 1 # Iteration number.\n            depth_mul = 8  # Depth decreases as spatial component increases.\n            size = 8  # Size increases as depth decreases.\n\n            while size < self.image_size:\n                hs.append(None)\n                name = \'g_h{}\'.format(i)\n                hs[i], _, _ = conv2d_transpose(hs[i-1],\n                    [self.batch_size, size, size, self.gf_dim*depth_mul], name=name, with_w=True)\n                hs[i] = tf.nn.relu(self.g_bns[i](hs[i], self.is_training))\n\n                i += 1\n                depth_mul //= 2\n                size *= 2\n\n            hs.append(None)\n            name = \'g_h{}\'.format(i)\n            hs[i], _, _ = conv2d_transpose(hs[i - 1],\n                [self.batch_size, size, size, 3], name=name, with_w=True)\n    \n            return tf.nn.tanh(hs[i])\n\n    def save(self, checkpoint_dir, step):\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        self.saver.save(self.sess,\n                        os.path.join(checkpoint_dir, self.model_name),\n                        global_step=step)\n\n    def load(self, checkpoint_dir):\n        print("" [*] Reading checkpoints..."")\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n            return True\n        else:\n            return False\n'"
ops.py,28,"b'# Original Version: Taehoon Kim (http://carpedm20.github.io)\n#   + Source: https://github.com/carpedm20/DCGAN-tensorflow/blob/e30539fb5e20d5a0fed40935853da97e9e55eee8/ops.py\n#   + License: MIT\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\n\nfrom utils import *\n\nclass batch_norm(object):\n    """"""Code modification of http://stackoverflow.com/a/33950177""""""\n    def __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n\n            self.name = name\n\n    def __call__(self, x, train):\n        return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon,\n                                            center=True, scale=True, is_training=train, scope=self.name)\n\ndef binary_cross_entropy(preds, targets, name=None):\n    """"""Computes binary cross entropy given `preds`.\n\n    For brevity, let `x = `, `z = targets`.  The logistic loss is\n\n        loss(x, z) = - sum_i (x[i] * log(z[i]) + (1 - x[i]) * log(1 - z[i]))\n\n    Args:\n        preds: A `Tensor` of type `float32` or `float64`.\n        targets: A `Tensor` of the same type and shape as `preds`.\n    """"""\n    eps = 1e-12\n    with ops.op_scope([preds, targets], name, ""bce_loss"") as name:\n        preds = ops.convert_to_tensor(preds, name=""preds"")\n        targets = ops.convert_to_tensor(targets, name=""targets"")\n        return tf.reduce_mean(-(targets * tf.log(preds + eps) +\n                              (1. - targets) * tf.log(1. - preds + eps)))\n\ndef conv_cond_concat(x, y):\n    """"""Concatenate conditioning vector on feature map axis.""""""\n    x_shapes = x.get_shape()\n    y_shapes = y.get_shape()\n    return tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])\n\ndef conv2d(input_, output_dim,\n           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n           name=""conv2d""):\n    with tf.variable_scope(name):\n        w = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n\n        biases = tf.get_variable(\'biases\', [output_dim], initializer=tf.constant_initializer(0.0))\n        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n        conv = tf.nn.bias_add(conv, biases)\n\n        return conv\n\ndef conv2d_transpose(input_, output_shape,\n                     k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n                     name=""conv2d_transpose"", with_w=False):\n    with tf.variable_scope(name):\n        # filter : [height, width, output_channels, in_channels]\n        w = tf.get_variable(\'w\', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n                            initializer=tf.random_normal_initializer(stddev=stddev))\n\n        try:\n            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n                                strides=[1, d_h, d_w, 1])\n\n        # Support for verisons of TensorFlow before 0.7.0\n        except AttributeError:\n            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n                                strides=[1, d_h, d_w, 1])\n\n        biases = tf.get_variable(\'biases\', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        # deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n        deconv = tf.nn.bias_add(deconv, biases)\n\n        if with_w:\n            return deconv, w, biases\n        else:\n            return deconv\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n    with tf.variable_scope(name):\n        f1 = 0.5 * (1 + leak)\n        f2 = 0.5 * (1 - leak)\n        return f1 * x + f2 * abs(x)\n\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n    shape = input_.get_shape().as_list()\n\n    with tf.variable_scope(scope or ""Linear""):\n        matrix = tf.get_variable(""Matrix"", [shape[1], output_size], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        bias = tf.get_variable(""bias"", [output_size],\n            initializer=tf.constant_initializer(bias_start))\n        if with_w:\n            return tf.matmul(input_, matrix) + bias, matrix, bias\n        else:\n            return tf.matmul(input_, matrix) + bias\n'"
simple-distributions.py,0,"b'#!/usr/bin/env python3\n\nimport numpy as np\nfrom scipy.stats import norm\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\nimport matplotlib.mlab as mlab\n\nnp.random.seed(0)\n\nX = np.arange(-3, 3, 0.001)\nY = norm.pdf(X, 0, 1)\n\nfig = plt.figure()\nplt.plot(X, Y)\nplt.tight_layout()\nplt.savefig(""normal-pdf.png"")\n\nnSamples = 35\nX = np.random.normal(0, 1, nSamples)\nY = np.zeros(nSamples)\nfig = plt.figure(figsize=(7,3))\nplt.scatter(X, Y, color=\'k\')\nplt.xlim((-3,3))\nframe = plt.gca()\nframe.axes.get_yaxis().set_visible(False)\nplt.savefig(""normal-samples.png"")\n\ndelta = 0.025\nx = np.arange(-3.0, 3.0, delta)\ny = np.arange(-3.0, 3.0, delta)\nX, Y = np.meshgrid(x, y)\nZ = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)\n\nplt.figure()\nCS = plt.contour(X, Y, Z)\nplt.clabel(CS, inline=1, fontsize=10)\n\nnSamples = 200\nmean = [0, 0]\ncov = [[1,0], [0,1]]\nX, Y = np.random.multivariate_normal(mean, cov, nSamples).T\nplt.scatter(X, Y, color=\'k\')\n\nplt.savefig(""normal-2d.png"")\n'"
train-dcgan.py,3,"b'#!/usr/bin/env python3\n\n# Original Version: Taehoon Kim (http://carpedm20.github.io)\n#   + Source: https://github.com/carpedm20/DCGAN-tensorflow/blob/e30539fb5e20d5a0fed40935853da97e9e55eee8/main.py\n#   + License: MIT\n# [2016-08-05] Modifications for Inpainting: Brandon Amos (http://bamos.github.io)\n#   + License: MIT\n\nimport os\nimport scipy.misc\nimport numpy as np\n\nfrom model import DCGAN\nfrom utils import pp, visualize, to_json\n\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_integer(""epoch"", 25, ""Epoch to train [25]"")\nflags.DEFINE_float(""learning_rate"", 0.0002, ""Learning rate of for adam [0.0002]"")\nflags.DEFINE_float(""beta1"", 0.5, ""Momentum term of adam [0.5]"")\nflags.DEFINE_integer(""train_size"", np.inf, ""The size of train images [np.inf]"")\nflags.DEFINE_integer(""batch_size"", 64, ""The size of batch images [64]"")\nflags.DEFINE_integer(""image_size"", 64, ""The size of image to use"")\nflags.DEFINE_string(""dataset"", ""lfw-aligned-64"", ""Dataset directory."")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_string(""sample_dir"", ""samples"", ""Directory name to save the image samples [samples]"")\nFLAGS = flags.FLAGS\n\nif not os.path.exists(FLAGS.checkpoint_dir):\n    os.makedirs(FLAGS.checkpoint_dir)\nif not os.path.exists(FLAGS.sample_dir):\n    os.makedirs(FLAGS.sample_dir)\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n    dcgan = DCGAN(sess, image_size=FLAGS.image_size, batch_size=FLAGS.batch_size,\n                  is_crop=False, checkpoint_dir=FLAGS.checkpoint_dir)\n\n    dcgan.train(FLAGS)\n'"
utils.py,0,"b'# Original Version: Taehoon Kim (http://carpedm20.github.io)\n#   + Source: https://github.com/carpedm20/DCGAN-tensorflow/blob/e30539fb5e20d5a0fed40935853da97e9e55eee8/utils.py\n#   + License: MIT\n\n""""""\nSome codes from https://github.com/Newmu/dcgan_code\n""""""\nfrom __future__ import division\nimport math\nimport json\nimport random\nimport pprint\nimport scipy.misc\nimport numpy as np\nfrom time import gmtime, strftime\n\npp = pprint.PrettyPrinter()\n\nget_stddev = lambda x, k_h, k_w: 1/math.sqrt(k_w*k_h*x.get_shape()[-1])\n\ndef get_image(image_path, image_size, is_crop=True):\n    return transform(imread(image_path), image_size, is_crop)\n\ndef save_images(images, size, image_path):\n    return imsave(inverse_transform(images), size, image_path)\n\ndef imread(path):\n    return scipy.misc.imread(path, mode=\'RGB\').astype(np.float)\n\ndef merge_images(images, size):\n    return inverse_transform(images)\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((int(h * size[0]), int(w * size[1]), 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[j*h:j*h+h, i*w:i*w+w, :] = image\n\n    return img\n\ndef imsave(images, size, path):\n    img = merge(images, size)\n    return scipy.misc.imsave(path, (255*img).astype(np.uint8))\n\ndef center_crop(x, crop_h, crop_w=None, resize_w=64):\n    if crop_w is None:\n        crop_w = crop_h\n    h, w = x.shape[:2]\n    j = int(round((h - crop_h)/2.))\n    i = int(round((w - crop_w)/2.))\n    return scipy.misc.imresize(x[j:j+crop_h, i:i+crop_w],\n                               [resize_w, resize_w])\n\ndef transform(image, npx=64, is_crop=True):\n    # npx : # of pixels width/height of image\n    if is_crop:\n        cropped_image = center_crop(image, npx)\n    else:\n        cropped_image = image\n    return np.array(cropped_image)/127.5 - 1.\n\ndef inverse_transform(images):\n    return (images+1.)/2.\n\n\ndef to_json(output_path, *layers):\n    with open(output_path, ""w"") as layer_f:\n        lines = """"\n        for w, b, bn in layers:\n            layer_idx = w.name.split(\'/\')[0].split(\'h\')[1]\n\n            B = b.eval()\n\n            if ""lin/"" in w.name:\n                W = w.eval()\n                depth = W.shape[1]\n            else:\n                W = np.rollaxis(w.eval(), 2, 0)\n                depth = W.shape[0]\n\n            biases = {""sy"": 1, ""sx"": 1, ""depth"": depth, ""w"": [\'%.2f\' % elem for elem in list(B)]}\n            if bn != None:\n                gamma = bn.gamma.eval()\n                beta = bn.beta.eval()\n\n                gamma = {""sy"": 1, ""sx"": 1, ""depth"": depth, ""w"": [\'%.2f\' % elem for elem in list(gamma)]}\n                beta = {""sy"": 1, ""sx"": 1, ""depth"": depth, ""w"": [\'%.2f\' % elem for elem in list(beta)]}\n            else:\n                gamma = {""sy"": 1, ""sx"": 1, ""depth"": 0, ""w"": []}\n                beta = {""sy"": 1, ""sx"": 1, ""depth"": 0, ""w"": []}\n\n            if ""lin/"" in w.name:\n                fs = []\n                for w in W.T:\n                    fs.append({""sy"": 1, ""sx"": 1, ""depth"": W.shape[0], ""w"": [\'%.2f\' % elem for elem in list(w)]})\n\n                lines += """"""\n                    var layer_%s = {\n                        ""layer_type"": ""fc"",\n                        ""sy"": 1, ""sx"": 1,\n                        ""out_sx"": 1, ""out_sy"": 1,\n                        ""stride"": 1, ""pad"": 0,\n                        ""out_depth"": %s, ""in_depth"": %s,\n                        ""biases"": %s,\n                        ""gamma"": %s,\n                        ""beta"": %s,\n                        ""filters"": %s\n                    };"""""" % (layer_idx.split(\'_\')[0], W.shape[1], W.shape[0], biases, gamma, beta, fs)\n            else:\n                fs = []\n                for w_ in W:\n                    fs.append({""sy"": 5, ""sx"": 5, ""depth"": W.shape[3], ""w"": [\'%.2f\' % elem for elem in list(w_.flatten())]})\n\n                lines += """"""\n                    var layer_%s = {\n                        ""layer_type"": ""deconv"",\n                        ""sy"": 5, ""sx"": 5,\n                        ""out_sx"": %s, ""out_sy"": %s,\n                        ""stride"": 2, ""pad"": 1,\n                        ""out_depth"": %s, ""in_depth"": %s,\n                        ""biases"": %s,\n                        ""gamma"": %s,\n                        ""beta"": %s,\n                        ""filters"": %s\n                    };"""""" % (layer_idx, 2**(int(layer_idx)+2), 2**(int(layer_idx)+2),\n                             W.shape[0], W.shape[3], biases, gamma, beta, fs)\n        layer_f.write("" "".join(lines.replace(""\'"","""").split()))\n\ndef make_gif(images, fname, duration=2, true_image=False):\n  import moviepy.editor as mpy\n\n  def make_frame(t):\n    try:\n      x = images[int(len(images)/duration*t)]\n    except:\n      x = images[-1]\n\n    if true_image:\n      return x.astype(np.uint8)\n    else:\n      return ((x+1)/2*255).astype(np.uint8)\n\n  clip = mpy.VideoClip(make_frame, duration=duration)\n  clip.write_gif(fname, fps = len(images) / duration)\n\ndef visualize(sess, dcgan, config, option):\n  if option == 0:\n    z_sample = np.random.uniform(-0.5, 0.5, size=(config.batch_size, dcgan.z_dim))\n    samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n    save_images(samples, [8, 8], \'./samples/test_%s.png\' % strftime(""%Y-%m-%d %H:%M:%S"", gmtime()))\n  elif option == 1:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in xrange(100):\n      print("" [*] %d"" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n      save_images(samples, [8, 8], \'./samples/test_arange_%s.png\' % (idx))\n  elif option == 2:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in [random.randint(0, 99) for _ in xrange(100)]:\n      print("" [*] %d"" % idx)\n      z = np.random.uniform(-0.2, 0.2, size=(dcgan.z_dim))\n      z_sample = np.tile(z, (config.batch_size, 1))\n      #z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n      make_gif(samples, \'./samples/test_gif_%s.gif\' % (idx))\n  elif option == 3:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in xrange(100):\n      print("" [*] %d"" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n      make_gif(samples, \'./samples/test_gif_%s.gif\' % (idx))\n  elif option == 4:\n    image_set = []\n    values = np.arange(0, 1, 1./config.batch_size)\n\n    for idx in xrange(100):\n      print("" [*] %d"" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample): z[idx] = values[kdx]\n\n      image_set.append(sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}))\n      make_gif(image_set[-1], \'./samples/test_gif_%s.gif\' % (idx))\n\n    new_image_set = [merge(np.array([images[idx] for images in image_set]), [10, 10]) \\\n        for idx in range(64) + range(63, -1, -1)]\n    make_gif(new_image_set, \'./samples/test_gif_merged.gif\', duration=8)\n'"
