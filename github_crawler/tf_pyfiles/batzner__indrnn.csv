file_path,api_count,code
ind_rnn_cell.py,0,"b'""""""Module implementing the IndRNN cell""""""\n\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.layers import base as base_layer\n\ntry:\n  # TF 1.7+\n  from tensorflow.python.ops.rnn_cell_impl import LayerRNNCell\nexcept ImportError:\n  from tensorflow.python.ops.rnn_cell_impl import _LayerRNNCell as LayerRNNCell\n\n\nclass IndRNNCell(LayerRNNCell):\n  """"""Independently RNN Cell. Adapted from `rnn_cell_impl.BasicRNNCell`.\n\n  Each unit has a single recurrent weight connected to its last hidden state.\n\n  The implementation is based on:\n\n    https://arxiv.org/abs/1803.04831\n\n  Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao\n  ""Independently Recurrent Neural Network (IndRNN): Building A Longer and\n  Deeper RNN""\n\n  The default initialization values for recurrent weights, input weights and\n  biases are taken from:\n\n    https://arxiv.org/abs/1504.00941\n\n  Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n  ""A Simple Way to Initialize Recurrent Networks of Rectified Linear Units""\n\n  Args:\n    num_units: int, The number of units in the RNN cell.\n    recurrent_min_abs: float, minimum absolute value of each recurrent weight.\n    recurrent_max_abs: (optional) float, maximum absolute value of each\n      recurrent weight. For `relu` activation, `pow(2, 1/timesteps)` is\n      recommended. If None, recurrent weights will not be clipped.\n      Default: None.\n    recurrent_kernel_initializer: (optional) The initializer to use for the\n      recurrent weights. If None, every recurrent weight is initially set to 1.\n      Default: None.\n    input_kernel_initializer: (optional) The initializer to use for the input\n      weights. If None, the input weights are initialized from a random normal\n      distribution with `mean=0` and `stddev=0.001`. Default: None.\n    activation: Nonlinearity to use.  Default: `relu`.\n    reuse: (optional) Python boolean describing whether to reuse variables\n      in an existing scope.  If not `True`, and the existing scope already has\n      the given variables, an error is raised.\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such\n      cases.\n  """"""\n\n  def __init__(self,\n               num_units,\n               recurrent_min_abs=0,\n               recurrent_max_abs=None,\n               recurrent_kernel_initializer=None,\n               input_kernel_initializer=None,\n               activation=None,\n               reuse=None,\n               name=None):\n    super(IndRNNCell, self).__init__(_reuse=reuse, name=name)\n\n    # Inputs must be 2-dimensional.\n    self.input_spec = base_layer.InputSpec(ndim=2)\n\n    self._num_units = num_units\n    self._recurrent_min_abs = recurrent_min_abs\n    self._recurrent_max_abs = recurrent_max_abs\n    self._recurrent_initializer = recurrent_kernel_initializer\n    self._input_initializer = input_kernel_initializer\n    self._activation = activation or nn_ops.relu\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def build(self, inputs_shape):\n    if inputs_shape[1].value is None:\n      raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""\n                       % inputs_shape)\n\n    input_depth = inputs_shape[1].value\n    if self._input_initializer is None:\n      self._input_initializer = init_ops.random_normal_initializer(mean=0.0,\n                                                                   stddev=0.001)\n    self._input_kernel = self.add_variable(\n        ""input_kernel"",\n        shape=[input_depth, self._num_units],\n        initializer=self._input_initializer)\n\n    if self._recurrent_initializer is None:\n      self._recurrent_initializer = init_ops.constant_initializer(1.)\n    self._recurrent_kernel = self.add_variable(\n        ""recurrent_kernel"",\n        shape=[self._num_units],\n        initializer=self._recurrent_initializer)\n\n    # Clip the absolute values of the recurrent weights to the specified minimum\n    if self._recurrent_min_abs:\n      abs_kernel = math_ops.abs(self._recurrent_kernel)\n      min_abs_kernel = math_ops.maximum(abs_kernel, self._recurrent_min_abs)\n      self._recurrent_kernel = math_ops.multiply(\n          math_ops.sign(self._recurrent_kernel),\n          min_abs_kernel\n      )\n\n    # Clip the absolute values of the recurrent weights to the specified maximum\n    if self._recurrent_max_abs:\n      self._recurrent_kernel = clip_ops.clip_by_value(self._recurrent_kernel,\n                                                      -self._recurrent_max_abs,\n                                                      self._recurrent_max_abs)\n\n    self._bias = self.add_variable(\n        ""bias"",\n        shape=[self._num_units],\n        initializer=init_ops.zeros_initializer(dtype=self.dtype))\n\n    self.built = True\n\n  def call(self, inputs, state):\n    """"""Run one time step of the IndRNN.\n\n    Calculates the output and new hidden state using the IndRNN equation\n\n      `output = new_state = act(W * input + u (*) state + b)`\n\n    where `*` is the matrix multiplication and `(*)` is the Hadamard product.\n\n    Args:\n      inputs: Tensor, 2-D tensor of shape `[batch, num_units]`.\n      state: Tensor, 2-D tensor of shape `[batch, num_units]` containing the\n        previous hidden state.\n\n    Returns:\n      A tuple containing the output and new hidden state. Both are the same\n        2-D tensor of shape `[batch, num_units]`.\n    """"""\n    gate_inputs = math_ops.matmul(inputs, self._input_kernel)\n    recurrent_update = math_ops.multiply(state, self._recurrent_kernel)\n    gate_inputs = math_ops.add(gate_inputs, recurrent_update)\n    gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\n    output = self._activation(gate_inputs)\n    return output, output\n'"
ind_rnn_cell_test.py,0,"b'""""""Tests for the IndRNN cell.""""""\n\nimport numpy as np\n\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\n\nfrom ind_rnn_cell import IndRNNCell\n\n\nclass IndRNNCellTest(test.TestCase):\n  def testIndRNNCell(self):\n    """"""Tests basic cell functionality""""""\n    with self.test_session() as sess:\n      x = array_ops.zeros([1, 4])\n      m = array_ops.zeros([1, 4])\n\n      # Create the cell with input weights = 1 and constant recurrent weights\n      recurrent_init = init_ops.constant_initializer([-3., -2., 1., 3.])\n      input_init = init_ops.constant_initializer(1.)\n      cell = IndRNNCell(num_units=4,\n                        recurrent_kernel_initializer=recurrent_init,\n                        input_kernel_initializer=input_init,\n                        activation=array_ops.identity)\n      output, _ = cell(x, m)\n\n      sess.run([variables.global_variables_initializer()])\n      res = sess.run([output],\n                     {x.name: np.array([[1., 0., 0., 0.]]),\n                       m.name: np.array([[2., 2., 2., 2.]])})\n      # (Pre)activations (1*1 + 2*rec_weight) should be -5, -3, 3, 7\n      self.assertAllEqual(res[0], [[-5., -3., 3., 7.]])\n\n  def testIndRNNCellBounds(self):\n    """"""Tests cell with recurrent weights exceeding the bounds.""""""\n    with self.test_session() as sess:\n      x = array_ops.zeros([1, 4])\n      m = array_ops.zeros([1, 4])\n\n      # Create the cell with input weights = 1 and constant recurrent weights\n      recurrent_init = init_ops.constant_initializer([-5., -2., 0.1, 5.])\n      input_init = init_ops.constant_initializer(1.)\n      cell = IndRNNCell(num_units=4,\n                        recurrent_min_abs=1.,\n                        recurrent_max_abs=3.,\n                        recurrent_kernel_initializer=recurrent_init,\n                        input_kernel_initializer=input_init,\n                        activation=array_ops.identity)\n      output, _ = cell(x, m)\n\n      sess.run([variables.global_variables_initializer()])\n      res = sess.run([output],\n                     {x.name: np.array([[1., 0., 0., 0.]]),\n                       m.name: np.array([[2., 2., 2., 2.]])})\n      # Recurrent weights should be clipped to -3, -2, 1, 3\n      # (Pre)activations (1*1 + 2*rec_weight) should be -5, -3, 3, 7\n      self.assertAllEqual(res[0], [[-5., -3., 3., 7.]])\n'"
examples/addition_rnn.py,17,"b'""""""Module using IndRNNCell to solve the addition problem\n\nThe addition problem is stated in https://arxiv.org/abs/1803.04831. The\nhyper-parameters are taken from that paper as well. The network should converge\nto a MSE around zero after 1000-20000 steps, depending on the number of time\nsteps.\n""""""\nimport tensorflow as tf\nimport numpy as np\n\nfrom ind_rnn_cell import IndRNNCell\n\n# Parameters taken from https://arxiv.org/abs/1803.04831\nTIME_STEPS = 100\nNUM_UNITS = 128\nLEARNING_RATE_INIT = 0.0002\nLEARNING_RATE_DECAY_STEPS = 20000\nRECURRENT_MAX = pow(2, 1 / TIME_STEPS)\n\n# Parameters taken from https://arxiv.org/abs/1511.06464\nBATCH_SIZE = 50\n\n\ndef main():\n  # Placeholders for training data\n  inputs_ph = tf.placeholder(tf.float32, shape=(BATCH_SIZE, TIME_STEPS, 2))\n  targets_ph = tf.placeholder(tf.float32, shape=BATCH_SIZE)\n\n  # Build the graph\n  first_input_init = tf.random_uniform_initializer(-RECURRENT_MAX,\n                                                   RECURRENT_MAX)\n  first_layer = IndRNNCell(NUM_UNITS, recurrent_max_abs=RECURRENT_MAX,\n                           recurrent_kernel_initializer=first_input_init)\n  second_layer = IndRNNCell(NUM_UNITS, recurrent_max_abs=RECURRENT_MAX)\n\n  cell = tf.nn.rnn_cell.MultiRNNCell([first_layer, second_layer])\n  # cell = tf.nn.rnn_cell.BasicLSTMCell(NUM_UNITS) uncomment this for LSTM runs\n\n  output, state = tf.nn.dynamic_rnn(cell, inputs_ph, dtype=tf.float32)\n  last = output[:, -1, :]\n\n  weight = tf.get_variable(""softmax_weight"", shape=[NUM_UNITS, 1])\n  bias = tf.get_variable(""softmax_bias"", shape=[1],\n                         initializer=tf.constant_initializer(0.1))\n  prediction = tf.squeeze(tf.matmul(last, weight) + bias)\n\n  loss_op = tf.losses.mean_squared_error(tf.squeeze(targets_ph), prediction)\n\n  global_step = tf.get_variable(""global_step"", shape=[], trainable=False,\n                                initializer=tf.zeros_initializer)\n  learning_rate = tf.train.exponential_decay(LEARNING_RATE_INIT, global_step,\n                                             LEARNING_RATE_DECAY_STEPS, 0.1,\n                                             staircase=True)\n  optimizer = tf.train.AdamOptimizer(learning_rate)\n  optimize = optimizer.minimize(loss_op, global_step=global_step)\n\n  # Train the model\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    step = 0\n    while True:\n      losses = []\n      for _ in range(100):\n        # Generate new input data\n        inputs, targets = get_batch()\n        loss, _ = sess.run([loss_op, optimize],\n                           {inputs_ph: inputs, targets_ph: targets})\n        losses.append(loss)\n        step += 1\n      print(""Step [x100] {} MSE {}"".format(int(step / 100), np.mean(losses)))\n\n\ndef get_batch():\n  """"""Generate the adding problem dataset""""""\n  # Build the first sequence\n  add_values = np.random.rand(BATCH_SIZE, TIME_STEPS)\n\n  # Build the second sequence with one 1 in each half and 0s otherwise\n  add_indices = np.zeros_like(add_values)\n  half = int(TIME_STEPS / 2)\n  for i in range(BATCH_SIZE):\n    first_half = np.random.randint(half)\n    second_half = np.random.randint(half, TIME_STEPS)\n    add_indices[i, [first_half, second_half]] = 1\n\n  # Zip the values and indices in a third dimension:\n  # inputs has the shape (batch_size, time_steps, 2)\n  inputs = np.dstack((add_values, add_indices))\n  targets = np.sum(np.multiply(add_values, add_indices), axis=1)\n  return inputs, targets\n\n\nif __name__ == ""__main__"":\n  main()\n'"
examples/sequential_mnist.py,38,"b'""""""Module using IndRNNCell to solve the Sequential MNIST problem\n\nThe approach is described in https://arxiv.org/abs/1803.04831. The\nhyper-parameters are taken from that paper as well as from its author\'s\nimplementation: https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne\n\nThe main difference to the original implementation is that this one does not\nuse running averages to estimate the population statistics for batch\nnormalization. Instead, it calculates them before every validation run with a\nlarge batch from the training set (see BATCH_SIZE_BN_STATS). This makes the\nvalidation metrics stable and expressive from the first training step on. For\ndatasets larger than MNIST, the running averages should be preferred.\n""""""\nimport itertools\nimport os\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom ind_rnn_cell import IndRNNCell\n\n# Parameters taken from https://arxiv.org/abs/1803.04831\nTIME_STEPS = 784\nNUM_UNITS = 128\nLEARNING_RATE_INIT = 0.0002\nLEARNING_RATE_DECAY_STEPS = 600000\nNUM_LAYERS = 6\nRECURRENT_MAX = pow(2, 1 / TIME_STEPS)\nNUM_CLASSES = 10\n\n# Parameters taken from https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne\nCLIP_GRADIENTS = True\nLAST_LAYER_LOWER_BOUND = pow(0.5, 1 / TIME_STEPS)\nBATCH_SIZE_TRAIN = 32\n\n# Custom parameters\nBATCH_SIZE_BN_STATS = 500\nBATCH_SIZE_VALID = 2000\n\nPHASE_TRAIN = ""train""\nPHASE_BN_STATS = ""bn_stats""\nPHASE_VALID = ""validation""\nPHASE_TEST = ""test""\n\nOUT_DIR = ""out/%s/"" % datetime.utcnow()\nSAVE_PATH = OUT_DIR + ""model.ckpt""\n\n# Import MNIST data (Numpy format)\nMNIST = input_data.read_data_sets(""/tmp/data/"")\n\n\ndef main():\n  sess = tf.Session()\n\n  # Create a placeholder for switching between data sources (train, validation\n  # etc.) dynamically. Switching is done by feeding one of the entries in\n  # handles to the data_handle placeholder.\n  data_handle = tf.placeholder(tf.string, shape=[], name=""data_handle"")\n  iterator, handles, init_validation_set = get_iterators(sess, data_handle)\n  # inputs and labels can contain data from any of the datasets\n  inputs, labels = iterator.get_next()\n\n  # Create a placeholder for executing different ops in the graph depending on\n  # the current phase (train, validation etc.) dynamically. Switching is done\n  # by feeding one of the PHASE_X constants to the phase placeholder.\n  phase = tf.placeholder(tf.string, shape=[], name=""phase"")\n  loss_op, accuracy_op, train_op = build(inputs, labels, phase)\n\n  # Train the model\n  sess.run(tf.global_variables_initializer())\n  saver = tf.train.Saver()\n\n  train_losses = []\n  train_accuracies = []\n  for step in itertools.count():\n    # Execute one training step\n    loss, accuracy, _ = sess.run(\n        [loss_op, accuracy_op, train_op],\n        feed_dict={data_handle: handles[PHASE_TRAIN], phase: PHASE_TRAIN})\n    train_losses.append(loss)\n    train_accuracies.append(accuracy)\n\n    if step % 100 == 0:\n      print(""{} Step {} Loss {} Acc {}"".format(\n          datetime.utcnow(), step + 1, np.mean(train_losses),\n          np.mean(train_accuracies)))\n      train_losses.clear()\n      train_accuracies.clear()\n\n    if step % 2000 == 0:\n      # Save the model to disk\n      if not os.path.exists(OUT_DIR):\n        os.makedirs(OUT_DIR)\n      save_path = saver.save(sess, SAVE_PATH)\n      print(""Model saved in path: %s"" % save_path)\n\n    if step % 1000 == 0:\n      # Update the population statistics without learning / changing the weights\n      sess.run([loss_op], feed_dict={\n        data_handle: handles[PHASE_BN_STATS],\n        phase: PHASE_BN_STATS})\n\n      # Run one pass over the validation dataset\n      init_validation_set()\n      feed_dict = {data_handle: handles[PHASE_VALID], phase: PHASE_VALID}\n      loss, accuracy = evaluate(sess, loss_op, accuracy_op, feed_dict)\n      print(""{} Step {} valid_loss {} valid_acc {}"".format(datetime.utcnow(),\n                                                           step + 1,\n                                                           loss,\n                                                           accuracy))\n\n      if accuracy > 0.99:\n        # Run the final test\n        feed_dict = {data_handle: handles[PHASE_TEST], phase: PHASE_TEST}\n        loss, accuracy = evaluate(sess, loss_op, accuracy_op, feed_dict)\n        print(""{} Step {} test_loss {} test_acc {}"".format(datetime.utcnow(),\n                                                           step + 1,\n                                                           loss,\n                                                           accuracy))\n        # Exit\n        return\n\n\ndef evaluate(session, loss_op, accuracy_op, feed_dict):\n  """"""Evaluate the model.\n\n  Computes the loss and accuracy with repeated calls with the specified\n  feed_dict. Halts, when session.run raises an tf.errors.OutOfRangeError (i.e.\n  the whole dataset was iterated trough once and returns the average loss and\n  accuracy.\n  """"""\n  losses, accuracies = [], []\n  while True:\n    try:\n      loss, accuracy = session.run(\n          [loss_op, accuracy_op],\n          feed_dict=feed_dict)\n\n      losses.append(loss)\n      accuracies.append(accuracy)\n    except tf.errors.OutOfRangeError:\n      break\n  return np.mean(losses), np.mean(accuracies)\n\n\ndef build(inputs, labels, phase):\n  # Build the graph from inputs and labels down to the loss, accuracy and\n  # training step ops.\n  rnn_output = build_rnn(inputs, phase=phase)\n  weight = tf.get_variable(""softmax_weight"", shape=[NUM_UNITS, NUM_CLASSES],\n                           initializer=tf.glorot_uniform_initializer())\n  bias = tf.get_variable(""softmax_bias"", shape=[NUM_CLASSES],\n                         initializer=tf.constant_initializer(0.))\n  logits = tf.matmul(rnn_output, weight) + bias\n\n  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=logits, labels=labels))\n\n  global_step = tf.Variable(0, trainable=False)\n  learning_rate = tf.train.exponential_decay(LEARNING_RATE_INIT, global_step,\n                                             LEARNING_RATE_DECAY_STEPS, 0.1,\n                                             staircase=True)\n  optimizer = tf.train.AdamOptimizer(learning_rate)\n\n  if CLIP_GRADIENTS:\n    gradients, variables = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n    optimize = optimizer.apply_gradients(zip(gradients, variables))\n  else:\n    optimize = optimizer.minimize(loss, global_step=global_step)\n\n  correct_pred = tf.equal(tf.argmax(logits, 1, output_type=tf.int32), labels)\n  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n  return loss, accuracy, optimize\n\n\ndef build_rnn(inputs, phase):\n  # Build the RNN with sequence-wise batch normalization. We cannot use\n  # MultiRNNCell here, because we have to add batch normalization layers after\n  # each RNN layer. Thus, we need to unroll each RNN layer separately.\n  layer_input = inputs\n  layer_output = None\n  input_init = tf.random_uniform_initializer(-0.001, 0.001)\n  for layer in range(1, NUM_LAYERS + 1):\n    # Init only the last layer\'s recurrent weights around 1\n    recurrent_init_lower = 0 if layer < NUM_LAYERS else LAST_LAYER_LOWER_BOUND\n    recurrent_init = tf.random_uniform_initializer(recurrent_init_lower,\n                                                   RECURRENT_MAX)\n    # Build the layer\n    cell = IndRNNCell(NUM_UNITS,\n                      recurrent_max_abs=RECURRENT_MAX,\n                      input_kernel_initializer=input_init,\n                      recurrent_kernel_initializer=recurrent_init)\n    # Unroll the layer\n    layer_output, _ = tf.nn.dynamic_rnn(cell, layer_input,\n                                        dtype=tf.float32,\n                                        scope=""rnn%d"" % layer)\n\n    is_training = tf.logical_or(tf.equal(phase, PHASE_TRAIN),\n                                tf.equal(phase, PHASE_BN_STATS))\n    layer_output = tf.layers.batch_normalization(layer_output,\n                                                 training=is_training,\n                                                 momentum=0)\n\n    # Tie the BN population statistics updates to the layer_output op only, when\n    # we are in the PHASE_BN_STATS phase\n    def update_population_stats():\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      with tf.control_dependencies(update_ops):\n        return tf.identity(layer_output)\n\n    layer_output = tf.cond(tf.equal(phase, PHASE_BN_STATS),\n                           true_fn=update_population_stats,\n                           false_fn=lambda: layer_output)\n\n    layer_input = layer_output\n\n  # Return the output of the last layer in the last time step\n  # layer_output has shape [?, TIME_STEPS, NUM_UNITS]\n  return layer_output[:, -1, :]\n\n\ndef preprocess_data(inputs, labels):\n  # General preprocessing for every dataset\n  inputs = 2 * inputs - 1\n  inputs = tf.expand_dims(inputs, -1)  # expand to [?, TIME_STEPS, 1]\n  labels = tf.cast(labels, tf.int32)\n  return inputs, labels\n\n\ndef get_training_set(inputs, labels):\n  # Create the training set\n  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n  dataset = dataset.shuffle(buffer_size=1000)\n  dataset = dataset.map(preprocess_data)\n  return dataset.repeat().batch(BATCH_SIZE_TRAIN)\n\n\ndef get_bn_stats_set(inputs, labels):\n  # Create the ""set BN stats"" set\n  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n  dataset = dataset.shuffle(buffer_size=1000)\n  dataset = dataset.map(preprocess_data)\n  return dataset.repeat().batch(BATCH_SIZE_BN_STATS)\n\n\ndef get_prediction_set(inputs, labels):\n  # Create the validation or test dataset\n  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n  dataset = dataset.map(preprocess_data)\n  return dataset.batch(BATCH_SIZE_VALID)\n\n\ndef get_iterators(session, handle):\n  # Create iterators for the training, ""set BN stats"", validation and test set\n  inputs_ph = tf.placeholder(MNIST.train.images.dtype, [None, 784],\n                             name=""all_inputs"")\n  labels_ph = tf.placeholder(MNIST.train.labels.dtype, [None],\n                             name=""all_labels"")\n\n  training_dataset = get_training_set(inputs_ph, labels_ph)\n  bn_stats_dataset = get_bn_stats_set(inputs_ph, labels_ph)\n  validation_dataset = get_prediction_set(inputs_ph, labels_ph)\n  test_dataset = get_prediction_set(inputs_ph, labels_ph)\n\n  # Create an iterator for switching between datasets\n  iterator = tf.data.Iterator.from_string_handle(\n      handle, training_dataset.output_types, training_dataset.output_shapes)\n\n  # Create iterators for each dataset that the main iterator can use for the\n  # next element\n  training_iterator = training_dataset.make_initializable_iterator()\n  bn_stats_iterator = bn_stats_dataset.make_initializable_iterator()\n  validation_iterator = validation_dataset.make_initializable_iterator()\n  test_iterator = test_dataset.make_initializable_iterator()\n\n  # Initialize iterators with their corresponding datasets\n  session.run(training_iterator.initializer, feed_dict={\n    inputs_ph: MNIST.train.images,\n    labels_ph: MNIST.train.labels})\n  session.run(bn_stats_iterator.initializer, feed_dict={\n    inputs_ph: MNIST.train.images,\n    labels_ph: MNIST.train.labels})\n  session.run(test_iterator.initializer, feed_dict={\n    inputs_ph: MNIST.test.images,\n    labels_ph: MNIST.test.labels})\n\n  # The validation set is not endless like the training or set-BN-stats set. It\n  # needs to be reinitialized for every validation run. Create a function for\n  # initializing it and pass that to the calling function.\n  def init_validation_set():\n    session.run(validation_iterator.initializer, feed_dict={\n      inputs_ph: MNIST.validation.images,\n      labels_ph: MNIST.validation.labels})\n\n  # Generate handles for each iterator. These can be fed to the handle\n  # placeholder for switching dynamically between datasets\n  handles = {\n    PHASE_TRAIN: session.run(training_iterator.string_handle()),\n    PHASE_BN_STATS: session.run(bn_stats_iterator.string_handle()),\n    PHASE_VALID: session.run(validation_iterator.string_handle()),\n    PHASE_TEST: session.run(test_iterator.string_handle())\n  }\n  return iterator, handles, init_validation_set\n\n\nif __name__ == ""__main__"":\n  main()\n'"
