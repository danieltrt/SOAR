file_path,api_count,code
src/calculation_helper.py,0,"b'import math\nimport random\nimport community\nimport numpy as np\nimport networkx as nx\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\n\ndef normalized_overlap(g, node_1, node_2):\n    """"""\n    Function to calculate the normalized neighborhood overlap.\n    :param g: NX graph.\n    :param node_1: Node 1. of a pair.\n    :param node_2: Node 2. of a pair.\n    """"""\n    nebs_1 = set(nx.neighbors(g, node_1))\n    nebs_2 = set(nx.neighbors(g, node_2))\n    inter = len(nebs_1.intersection(nebs_2))\n    unio = len(nebs_1.union(set(nx.neighbors(g, node_2))))\n    return float(inter)/float(unio)\n\ndef overlap(g, node_1, node_2):\n    """"""\n    Function to calculate the neighborhood overlap.\n    :param g: NX graph.\n    :param node_1: Node 1. of a pair.\n    :param node_2: Node 2. of a pair.\n    """"""\n    nebs_1 = set(nx.neighbors(g, node_1))\n    nebs_2 = set(nx.neighbors(g, node_2))\n    inter = len(nebs_1.intersection(nebs_2))\n    return float(inter)\n\ndef unit(g, node_1, node_2):\n    """"""\n    Function to calculate the ""unit"" weight.\n    :param g: NX graph.\n    :param node_1: Node 1. of a pair.\n    :param node_2: Node 2. of a pair.\n    """"""\n    return 1\n\ndef min_norm(g, node_1, node_2):\n    """"""\n    Function to calculate the minimum normalized neighborhood overlap.\n    :param g: NX graph.\n    :param node_1: Node 1. of a pair.\n    :param node_2: Node 2. of a pair.\n    """"""\n    nebs_1 = set(nx.neighbors(g, node_1))\n    nebs_2 = set(nx.neighbors(g, node_2))\n    inter = len(nebs_1.intersection(nebs_2))\n    min_norm = min(len(nebs_1), len(nebs_2))\n    return float(inter)/float(min_norm)\n\ndef overlap_generator(args, graph):\n    """"""\n    Function to generate weight for all of the edges.\n    """"""\n    if args.overlap_weighting == ""normalized_overlap"":\n        overlap_weighter = normalized_overlap\n    elif args.overlap_weighting == ""overlap"":\n        overlap_weighter = overlap\n    elif args.overlap_weighting == ""min_norm"":\n        overlap_weighter = min_norm\n    else:\n        overlap_weighter = unit\n    print("" "")\n    print(""Weight calculation started."")\n    print("" "")\n    edges = nx.edges(graph)\n    weights = {e: overlap_weighter(graph, e[0], e[1]) for e in tqdm(edges)}\n    weights_prime = {(e[1], e[0]): value for e, value in weights.items()}\n    weights.update(weights_prime)\n    print("" "")\n    return weights\n\ndef index_generation(weights, a_walk):\n    """"""\n    Function to generate overlaps and indices.\n    """"""\n    edges = [(a_walk[i], a_walk[i+1]) for i in range(0, len(a_walk)-1)]\n    edge_set_1 = np.array(range(0, len(a_walk)-1))\n    edge_set_2 = np.array(range(1, len(a_walk)))\n    overlaps = np.array(list(map(lambda x: weights[x], edges))).reshape((-1, 1))\n    return edge_set_1, edge_set_2, overlaps\n\ndef batch_input_generator(a_walk, random_walk_length, window_size):\n    """"""\n    Function to generate features from a node sequence.\n    """"""\n    seq_1 = [a_walk[j] for j in range(random_walk_length-window_size)]\n    seq_2 = [a_walk[j] for j in range(window_size, random_walk_length)]\n    return np.array(seq_1 + seq_2)\n\ndef batch_label_generator(a_walk, random_walk_length, window_size):\n    """"""\n    Function to generate labels from a node sequence.\n    """"""\n    grams_1 = [a_walk[j+1:j+1+window_size] for j in range(random_walk_length-window_size)]\n    grams_2 = [a_walk[j-window_size:j] for j in range(window_size, random_walk_length)]\n    return np.array(grams_1 + grams_2)\n\ndef gamma_incrementer(step, gamma_0, gamma_final, current_gamma, num_steps):\n    if step > 1:\n        exponent = (0-np.log10(gamma_0))/float(num_steps)\n        current_gamma = current_gamma * (10 **exponent)*(gamma_final-gamma_0)\n        current_gamma = current_gamma + gamma_0\n    return current_gamma\n\ndef neural_modularity_calculator(graph, embedding, means):\n    """"""\n    Function to calculate the GEMSEC cluster assignments.\n    """"""\n    assignments = {}\n    for node in graph.nodes():\n        positions = means-embedding[node, :]\n        values = np.sum(np.square(positions), axis=1)\n        index = np.argmin(values)\n        assignments[int(node)] = int(index)\n    modularity = community.modularity(assignments, graph)\n    return modularity, assignments\n\ndef classical_modularity_calculator(graph, embedding, args):\n    """"""\n    Function to calculate the DeepWalk cluster centers and assignments.\n    """"""\n    kmeans = KMeans(n_clusters=args.cluster_number, random_state=0, n_init=1).fit(embedding)\n    assignments = {i: int(kmeans.labels_[i]) for i in range(embedding.shape[0])}\n    modularity = community.modularity(assignments, graph)\n    return modularity, assignments\n\nclass RandomWalker:\n    """"""\n    Class to generate vertex sequences.\n    """"""\n    def __init__(self, graph, repetitions, length):\n        print(""Model initialization started."")\n        self.graph = graph\n        self.nodes = [node for node in self.graph.nodes()]\n        self.repetitions = repetitions\n        self.length = length\n\n    def small_walk(self, start_node):\n        """"""\n        Generate a node sequence from a start node.\n        """"""\n        walk = [start_node]\n        while len(walk) != self.length:\n            end_point = walk[-1]\n            neighbors = [neb for neb in nx.neighbors(self.graph, end_point)]\n            if len(neighbors) > 0:\n                walk.append(random.choice(neighbors))\n            else:\n                break\n        return walk\n\n    def count_frequency_values(self):\n        """"""\n        Calculate the co-occurence frequencies.\n        """"""\n        raw_counts = [node for walk in self.walks for node in walk]\n        counts = Counter(raw_counts)\n        self.degrees = [counts[i] for i in range(len(self.nodes))]\n\n    def do_walks(self):\n        """"""\n        Do a series of random walks.\n        """"""\n        self.walks = []\n        for rep in range(0, self.repetitions):\n            random.shuffle(self.nodes)\n            print("" "")\n            print(""Random walk series "" + str(rep+1) + "". initiated."")\n            print("" "")\n            for node in tqdm(self.nodes):\n                walk = self.small_walk(node)\n                self.walks.append(walk)\n        self.count_frequency_values()\n        return self.degrees, self.walks\n\nclass SecondOrderRandomWalker:\n\n    def __init__(self, nx_G, is_directed, p, q):\n        self.G = nx_G\n        self.nodes = nx.nodes(self.G)\n        print(""Edge weighting.\\n"")\n        for edge in tqdm(self.G.edges()):\n            self.G[edge[0]][edge[1]][""weight""] = 1.0\n            self.G[edge[1]][edge[0]][""weight""] = 1.0\n        self.is_directed = is_directed\n        self.p = p\n        self.q = q\n\n    def node2vec_walk(self, walk_length, start_node):\n        """"""\n        Simulate a random walk starting from start node.\n        """"""\n        G = self.G\n        alias_nodes = self.alias_nodes\n        alias_edges = self.alias_edges\n\n        walk = [start_node]\n\n        while len(walk) < walk_length:\n            cur = walk[-1]\n            cur_nbrs = sorted(G.neighbors(cur))\n            if len(cur_nbrs) > 0:\n                if len(walk) == 1:\n                    walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n                else:\n                    prev = walk[-2]\n                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], alias_edges[(prev, cur)][1])]\n                    walk.append(next)\n            else:\n                break\n\n        return walk\n\n    def count_frequency_values(self, walks):\n        """""" \n        Calculate the co-occurence frequencies.\n        """"""\n        raw_counts = [node for walk in walks for node in walk]\n        counts = Counter(raw_counts)\n        self.degrees = [counts[i] for i in range(0,len(self.nodes))]\n        return self.degrees \n\n    def simulate_walks(self, num_walks, walk_length):\n        """"""\n        Repeatedly simulate random walks from each node.\n        """"""\n        G = self.G\n        walks = []\n        nodes = list(G.nodes())\n        for walk_iter in range(num_walks):\n            print("" "")\n            print(""Random walk series "" + str(walk_iter+1) + "". initiated."")\n            print("" "")\n            random.shuffle(nodes)\n            for node in tqdm(nodes):\n                walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n\n        return walks, self.count_frequency_values(walks)\n\n    def get_alias_edge(self, src, dst):\n        """"""\n        Get the alias edge setup lists for a given edge.\n        """"""\n        G = self.G\n        p = self.p\n        q = self.q\n\n        unnormalized_probs = []\n        for dst_nbr in sorted(G.neighbors(dst)):\n            if dst_nbr == src:\n                unnormalized_probs.append(G[dst][dst_nbr][""weight""]/p)\n            elif G.has_edge(dst_nbr, src):\n                unnormalized_probs.append(G[dst][dst_nbr][""weight""])\n            else:\n                unnormalized_probs.append(G[dst][dst_nbr][""weight""]/q)\n        norm_const = sum(unnormalized_probs)\n        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n\n        return alias_setup(normalized_probs)\n\n    def preprocess_transition_probs(self):\n        """"""\n        Preprocessing of transition probabilities for guiding the random walks.\n        """"""\n        G = self.G\n        is_directed = self.is_directed\n\n        alias_nodes = {}\n        print("""")\n        print(""Preprocesing.\\n"")\n        for node in tqdm(G.nodes()):\n             unnormalized_probs = [G[node][nbr][""weight""] for nbr in sorted(G.neighbors(node))]\n             norm_const = sum(unnormalized_probs)\n             normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n             alias_nodes[node] = alias_setup(normalized_probs)\n\n        alias_edges = {}\n        triads = {}\n\n        if is_directed:\n            for edge in G.edges():\n                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n        else:\n            for edge in tqdm(G.edges()):\n                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n\n        self.alias_nodes = alias_nodes\n        self.alias_edges = alias_edges\n\n        return\n\ndef alias_setup(probs):\n    """"""\n    Compute utility lists for non-uniform sampling from discrete distributions.\n    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n    for details\n    """"""\n    K = len(probs)\n    q = np.zeros(K)\n    J = np.zeros(K, dtype=np.int)\n    smaller = []\n    larger = []\n\n    for kk, prob in enumerate(probs):\n        q[kk] = K*prob\n        if q[kk] < 1.0:\n            smaller.append(kk)\n        else:\n            larger.append(kk)\n\n    while len(smaller) > 0 and len(larger) > 0:\n        small = smaller.pop()\n        large = larger.pop()\n\n        J[small] = large\n        q[large] = q[large] + q[small] - 1.0\n        if q[large] < 1.0:\n            smaller.append(large)\n        else:\n            larger.append(large)\n\n    return J, q\n\ndef alias_draw(J, q):\n    """"""\n    Draw sample from a non-uniform discrete distribution using alias sampling.\n    """"""\n    K = len(J)\n\n    kk = int(np.floor(np.random.rand()*K))\n    if np.random.rand() < q[kk]:\n        return kk\n    else:\n        return J[kk]\n'"
src/embedding_clustering.py,0,"b'""""""Running the model.""""""\n\nfrom param_parser import parameter_parser\nfrom print_and_read import graph_reader\nfrom model import GEMSECWithRegularization, GEMSEC\nfrom model import DeepWalkWithRegularization, DeepWalk\n\ndef create_and_run_model(args):\n    """"""\n    Function to read the graph, create an embedding and train it.\n    """"""\n    graph = graph_reader(args.input)\n    if args.model == ""GEMSECWithRegularization"":\n        model = GEMSECWithRegularization(args, graph)\n    elif args.model == ""GEMSEC"":\n        model = GEMSEC(args, graph)\n    elif args.model == ""DeepWalkWithRegularization"":\n        model = DeepWalkWithRegularization(args, graph)\n    else:\n        model = DeepWalk(args, graph)\n    model.train()\n\nif __name__ == ""__main__"":\n    args = parameter_parser()\n    create_and_run_model(args)\n'"
src/layers.py,25,"b'""""""Definition of computational layers.""""""\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nclass DeepWalker:\n    """"""\n    DeepWalk embedding layer class.\n    """"""\n    def __init__(self, args, vocab_size, degrees):\n        """"""\n        Initialization of the layer with proper matrices and biases.\n        The input variables are also initialized here.\n        """"""\n        self.args = args\n        self.vocab_size = vocab_size\n        self.degrees = degrees\n        self.train_labels = tf.placeholder(tf.int64, shape=[None, self.args.window_size])\n\n        self.train_inputs = tf.placeholder(tf.int64, shape=[None])\n\n        self.embedding_matrix = tf.Variable(tf.random_uniform([self.vocab_size, self.args.dimensions],\n                                                              -0.1/self.args.dimensions,\n                                                              0.1/self.args.dimensions))\n\n\n        self.nce_weights = tf.Variable(tf.truncated_normal([self.vocab_size, self.args.dimensions],\n                                       stddev=1.0/math.sqrt(self.args.dimensions)))\n\n        self.nce_biases = tf.Variable(tf.random_uniform([self.vocab_size],\n                                                        -0.1/self.args.dimensions,\n                                                        0.1/self.args.dimensions))\n\n    def __call__(self):\n        """"""\n        Calculating the embedding cost with NCE and returning it.\n        """"""\n        self.train_labels_flat = tf.reshape(self.train_labels, [-1, 1])\n        self.input_ones = tf.ones_like(self.train_labels)\n        self.train_inputs_flat = tf.reshape(tf.multiply(self.input_ones, tf.reshape(self.train_inputs,[-1, 1])), [-1])\n        self.embedding_partial = tf.nn.embedding_lookup(self.embedding_matrix,\n                                                        self.train_inputs_flat,\n                                                        max_norm=1)    \n\n        self.sampler = tf.nn.fixed_unigram_candidate_sampler(true_classes=self.train_labels_flat,\n                                                             num_true=1,\n                                                             num_sampled=self.args.negative_sample_number,\n                                                             unique=True,\n                                                             range_max=self.vocab_size,\n                                                             distortion=self.args.distortion,\n                                                             unigrams=self.degrees)\n    \n        self.embedding_losses = tf.nn.sampled_softmax_loss(weights=self.nce_weights,\n                                                           biases=self.nce_biases,\n                                                           labels=self.train_labels_flat,\n                                                           inputs=self.embedding_partial,\n                                                           num_true=1,\n                                                           num_sampled=self.args.negative_sample_number,\n                                                           num_classes=self.vocab_size,\n                                                           sampled_values=self.sampler)\n    \n        return tf.reduce_mean(self.embedding_losses)\n\nclass Clustering:\n    """"""\n    Latent space clustering class.\n    """"""\n    def __init__(self, args):\n        """"""\n        Initializing the cluster center matrix.\n        """"""\n        self.args = args\n        self.cluster_means = tf.Variable(tf.random_uniform([self.args.cluster_number, self.args.dimensions],\n                                         -0.1/self.args.dimensions,\n                                         0.1/self.args.dimensions))\n    def __call__(self, Walker):\n        """"""\n        Calculating the clustering cost.\n        """"""\n           \n        self.clustering_differences = tf.expand_dims(Walker.embedding_partial, 1) - self.cluster_means\n        self.cluster_distances = tf.norm(self.clustering_differences, ord=2, axis=2)\n        self.to_be_averaged = tf.reduce_min(self.cluster_distances, axis=1)\n        return tf.reduce_mean(self.to_be_averaged)\n\nclass Regularization:\n    """"""\n    Smoothness regularization class.\n    """"""\n    def __init__(self, args):\n        """"""\n        Initializing the indexing variables and the weight vector.\n        """"""\n        self.args = args\n        self.edge_indices_right = tf.placeholder(tf.int64, shape=[None])\n        self.edge_indices_left = tf.placeholder(tf.int64, shape=[None])\n        self.overlap = tf.placeholder(tf.float32, shape=[None, 1])\n\n    def __call__(self, Walker):\n        """"""\n        Calculating the regularization cost.\n        """"""\n        self.left_features = tf.nn.embedding_lookup(Walker.embedding_partial,\n                                                    self.edge_indices_left,\n                                                    max_norm=1)\n\n        self.right_features = tf.nn.embedding_lookup(Walker.embedding_partial,\n                                                     self.edge_indices_right,\n                                                     max_norm=1)\n        self.regularization_differences = self.left_features - self.right_features\n        noise =  np.random.uniform(-self.args.regularization_noise,\n                                   self.args.regularization_noise,\n                                   (self.args.random_walk_length-1, self.args.dimensions))\n        self.regularization_differences = self.regularization_differences + noise\n        self.regularization_distances = tf.norm(self.regularization_differences, ord=2,axis=1)\n        self.regularization_distances = tf.reshape(self.regularization_distances, [-1, 1])\n        self.regularization_loss = tf.reduce_mean(tf.matmul(tf.transpose(self.overlap), self.regularization_distances))\n        return self.args.lambd*self.regularization_loss\n'"
src/model.py,29,"b'""""""GEMSEC model classes.""""""\n\nimport math\nimport time\nimport random\nimport numpy as np\nimport networkx as nx\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom calculation_helper import overlap_generator\nfrom layers import DeepWalker, Clustering, Regularization\nfrom calculation_helper import neural_modularity_calculator, classical_modularity_calculator\nfrom calculation_helper import gamma_incrementer, RandomWalker, SecondOrderRandomWalker\nfrom calculation_helper import index_generation, batch_input_generator, batch_label_generator\nfrom print_and_read import json_dumper, log_setup\nfrom print_and_read import initiate_dump_gemsec, initiate_dump_dw\nfrom print_and_read import tab_printer, epoch_printer, log_updater\n\nclass Model(object):\n    """"""\n    Abstract model class.\n    """"""\n    def __init__(self, args, graph):\n        """"""\n        Every model needs the same initialization -- args, graph.\n        We delete the sampler object to save memory.\n        We also build the computation graph up.\n        """"""\n        self.args = args\n        self.graph = graph\n        if self.args.walker == ""first"":\n            self.walker = RandomWalker(self.graph,\n                                       self.args.num_of_walks,\n                                       self.args.random_walk_length)\n\n            self.degrees, self.walks = self.walker.do_walks()\n        else:\n            self.walker = SecondOrderRandomWalker(self.graph, False, self.args.P, self.args.Q)\n            self.walker.preprocess_transition_probs()\n            self.walks, self.degrees = self.walker.simulate_walks(self.args.num_of_walks,\n                                                                  self.args.random_walk_length)\n        self.nodes = [node for node in self.graph.nodes()]\n        del self.walker\n        self.vocab_size = len(self.degrees)\n        self.true_step_size = self.args.num_of_walks*self.vocab_size\n        self.build()\n\n    def build(self):\n        """"""\n        Building the model.\n        """"""\n        pass\n\n    def feed_dict_generator(self):\n        """"""\n        Creating the feed generator\n        """"""\n        pass\n\n    def train(self):\n        """"""\n        Training the model.\n        """"""\n        pass\n\nclass GEMSECWithRegularization(Model):\n    """"""\n    Regularized GEMSEC class.\n    """"""\n    def build(self):\n        """"""\n        Method to create the computational graph.\n        """"""\n        self.computation_graph = tf.Graph()\n        with self.computation_graph.as_default():\n\n            self.walker_layer = DeepWalker(self.args, self.vocab_size, self.degrees)\n            self.cluster_layer = Clustering(self.args)\n            self.regularizer_layer = Regularization(self.args)\n\n            self.gamma = tf.placeholder(""float"")\n            self.loss = self.walker_layer()\n            self.loss = self.loss + self.gamma*self.cluster_layer(self.walker_layer)\n            self.loss = self.loss + self.regularizer_layer(self.walker_layer)\n\n            self.batch = tf.Variable(0)\n            self.step = tf.placeholder(""float"")\n\n            self.learning_rate_new = tf.train.polynomial_decay(self.args.initial_learning_rate,\n                                                               self.batch,\n                                                               self.true_step_size,\n                                                               self.args.minimal_learning_rate,\n                                                               self.args.annealing_factor)\n\n            self.train_op = tf.train.AdamOptimizer(self.learning_rate_new).minimize(self.loss,\n                                                                                    global_step=self.batch)\n\n            self.init = tf.global_variables_initializer()\n\n        self.weights = overlap_generator(self.args, self.graph)\n\n    def feed_dict_generator(self, a_random_walk, step, gamma):\n        """"""\n        Method to generate:\n        1. random walk features.\n        2. left and right handside matrices.\n        3. proper time index and overlap vector.\n        """"""\n        index_1, index_2, overlaps = index_generation(self.weights, a_random_walk)\n\n        batch_inputs = batch_input_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        batch_labels = batch_label_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        feed_dict = {self.walker_layer.train_labels: batch_labels,\n                     self.walker_layer.train_inputs: batch_inputs,\n                     self.gamma: gamma,\n                     self.step: float(step),\n                     self.regularizer_layer.edge_indices_left: index_1,\n                     self.regularizer_layer.edge_indices_right: index_2,\n                     self.regularizer_layer.overlap: overlaps}\n        return feed_dict\n\n    def train(self):\n        """"""\n        Method for:\n        1. training the embedding.\n        2. logging.\n        This method is inherited by GEMSEC and DeepWalk variants without an override.\n        """"""\n        self.current_step = 0\n        self.current_gamma = self.args.initial_gamma\n        self.log = log_setup(self.args)\n        with tf.Session(graph=self.computation_graph) as session:\n            self.init.run()\n            print(""Model Initialized."")\n            for repetition in range(self.args.num_of_walks):\n\n                random.shuffle(self.nodes)\n                self.optimization_time = 0\n                self.average_loss = 0\n\n                epoch_printer(repetition)\n\n                for node in tqdm(self.nodes):\n                    self.current_step = self.current_step + 1\n                    self.current_gamma = gamma_incrementer(self.current_step,\n                                                           self.args.initial_gamma,\n                                                           self.args.final_gamma,\n                                                           self.current_gamma,\n                                                           self.true_step_size)\n\n                    feed_dict = self.feed_dict_generator(self.walks[self.current_step-1],\n                                                         self.current_step,\n                                                         self.current_gamma)\n                    start = time.time()\n                    _, loss = session.run([self.train_op, self.loss], feed_dict=feed_dict)\n                    end = time.time()\n                    self.optimization_time = self.optimization_time + (end-start)\n                    self.average_loss = self.average_loss + loss\n\n                print("""")\n                self.average_loss = self.average_loss/self.vocab_size\n                self.final_embeddings = self.walker_layer.embedding_matrix.eval()\n                if ""GEMSEC"" in self.args.model:\n                    self.c_means = self.cluster_layer.cluster_means.eval()\n                    self.modularity_score, assignments = neural_modularity_calculator(self.graph,\n                                                                                      self.final_embeddings,\n                                                                                      self.c_means)\n                else:\n                    self.modularity_score, assignments = classical_modularity_calculator(self.graph,\n                                                                                         self.final_embeddings,\n                                                                                         self.args)\n                self.log = log_updater(self.log, repetition, self.average_loss,\n                                       self.optimization_time, self.modularity_score)\n                tab_printer(self.log)\n        if ""GEMSEC"" in self.args.model:\n            initiate_dump_gemsec(self.log, assignments, self.args,\n                                 self.final_embeddings, self.c_means)\n        else:\n            initiate_dump_dw(self.log, assignments,\n                             self.args, self.final_embeddings)\n\nclass GEMSEC(GEMSECWithRegularization):\n    """"""\n    GEMSEC class.\n    """"""\n    def build(self):\n        """"""\n        Method to create the computational graph.\n        """"""\n        self.computation_graph = tf.Graph()\n        with self.computation_graph.as_default():\n\n            self.walker_layer = DeepWalker(self.args, self.vocab_size, self.degrees)\n            self.cluster_layer = Clustering(self.args)\n\n            self.gamma = tf.placeholder(""float"")\n            self.loss = self.walker_layer()+self.gamma*self.cluster_layer(self.walker_layer)\n\n            self.batch = tf.Variable(0)\n            self.step = tf.placeholder(""float"")\n\n            self.learning_rate_new = tf.train.polynomial_decay(self.args.initial_learning_rate,\n                                                               self.batch,\n                                                               self.true_step_size,\n                                                               self.args.minimal_learning_rate,\n                                                               self.args.annealing_factor)\n\n            self.train_op = tf.train.AdamOptimizer(self.learning_rate_new).minimize(self.loss,\n                                                                                    global_step=self.batch)\n\n            self.init = tf.global_variables_initializer()\n\n    def feed_dict_generator(self, a_random_walk, step, gamma):\n        """"""\n        Method to generate:\n        1. random walk features.\n        2. left and right handside matrices.\n        Proper time index and overlap vector.\n        """"""\n        batch_inputs = batch_input_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        batch_labels = batch_label_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        feed_dict = {self.walker_layer.train_labels: batch_labels,\n                     self.walker_layer.train_inputs: batch_inputs,\n                     self.gamma: gamma,\n                     self.step: float(step)}\n\n        return feed_dict\n\n\nclass DeepWalkWithRegularization(GEMSECWithRegularization):\n    """"""\n    Regularized DeepWalk class.\n    """"""\n    def build(self):\n        """"""\n        Method to create the computational graph and initialize weights.\n        """"""\n        self.computation_graph = tf.Graph()\n        with self.computation_graph.as_default():\n\n            self.walker_layer = DeepWalker(self.args, self.vocab_size, self.degrees)\n            self.regularizer_layer = Regularization(self.args)\n\n            self.gamma = tf.placeholder(""float"")\n            self.loss = self.walker_layer()+self.regularizer_layer(self.walker_layer)\n\n            self.batch = tf.Variable(0)\n            self.step = tf.placeholder(""float"")\n\n            self.learning_rate_new = tf.train.polynomial_decay(self.args.initial_learning_rate,\n                                                               self.batch,\n                                                               self.true_step_size,\n                                                               self.args.minimal_learning_rate,\n                                                               self.args.annealing_factor)\n\n            self.train_op = tf.train.AdamOptimizer(self.learning_rate_new).minimize(self.loss,\n                                                                                    global_step=self.batch)\n\n            self.init = tf.global_variables_initializer()\n\n        self.weights = overlap_generator(self.args, self.graph)\n\n    def feed_dict_generator(self, a_random_walk, step, gamma):\n        """"""\n        Method to generate:\n        1. random walk features.\n        2. left and right handside matrices.\n        3. proper time index and overlap vector.\n        """"""\n        index_1, index_2, overlaps = index_generation(self.weights, a_random_walk)\n\n        batch_inputs = batch_input_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        batch_labels = batch_label_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        feed_dict = {self.walker_layer.train_labels: batch_labels,\n                     self.walker_layer.train_inputs: batch_inputs,\n                     self.gamma: gamma,\n                     self.step: float(step),\n                     self.regularizer_layer.edge_indices_left: index_1,\n                     self.regularizer_layer.edge_indices_right: index_2,\n                     self.regularizer_layer.overlap: overlaps}\n\n        return feed_dict\n\nclass DeepWalk(GEMSECWithRegularization):\n    """"""\n    DeepWalk class.\n    """"""\n    def build(self):\n        """"""\n        Method to create the computational graph and initialize weights.\n        """"""\n        self.computation_graph = tf.Graph()\n        with self.computation_graph.as_default():\n\n            self.walker_layer = DeepWalker(self.args, self.vocab_size, self.degrees)\n\n            self.gamma = tf.placeholder(""float"")\n            self.loss = self.walker_layer()\n\n            self.batch = tf.Variable(0)\n            self.step = tf.placeholder(""float"")\n\n            self.learning_rate_new = tf.train.polynomial_decay(self.args.initial_learning_rate,\n                                                               self.batch,\n                                                               self.true_step_size,\n                                                               self.args.minimal_learning_rate,\n                                                               self.args.annealing_factor)\n\n            self.train_op = tf.train.AdamOptimizer(self.learning_rate_new).minimize(self.loss,\n                                                                                    global_step=self.batch)\n\n            self.init = tf.global_variables_initializer()\n\n    def feed_dict_generator(self, a_random_walk, step, gamma):\n        """"""\n        Method to generate random walk features, gamma and proper time index.\n        """"""\n\n        batch_inputs = batch_input_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        batch_labels = batch_label_generator(a_random_walk,\n                                             self.args.random_walk_length,\n                                             self.args.window_size)\n\n        feed_dict = {self.walker_layer.train_labels: batch_labels,\n                     self.walker_layer.train_inputs: batch_inputs,\n                     self.gamma: gamma,\n                     self.step: float(step)}\n\n        return feed_dict\n'"
src/param_parser.py,0,"b'""""""Parsing the hyperparameters.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters.\n    By default it gives an embedding of the Facebook politicians network.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run GEMSEC."")\n\n    parser.add_argument(""--input"",\n                        nargs=""?"",\n                        default=""./data/politician_edges.csv"",\n\t                help=""Input graph path."")\n\n    parser.add_argument(""--embedding-output"",\n                        nargs=""?"",\n                        default=""./output/embeddings/politician_embedding.csv"",\n\t                help=""Embeddings path."")\n\n    parser.add_argument(""--cluster-mean-output"",\n                        nargs=""?"",\n                        default=""./output/cluster_means/politician_means.csv"",\n\t                help=""Cluster means path."")\n\n    parser.add_argument(""--log-output"",\n                        nargs=""?"",\n                        default=""./output/logs/politician.json"",\n\t                help=""Log path."")\n\n    parser.add_argument(""--assignment-output"",\n                        nargs=""?"",\n                        default=""./output/assignments/politician.json"",\n\t                help=""Log path."")\n\n    parser.add_argument(""--dump-matrices"",\n                        type=bool,\n                        default=True,\n\t                help=""Save the embeddings to disk or not. Default is not."")\n\n    parser.add_argument(""--model"",\n                        nargs=""?"",\n                        default=""GEMSECWithRegularization"",\n\t                help=""The model type."")\n\n    parser.add_argument(""--P"",\n                        type=float,\n                        default=1,\n\t                help=""Return hyperparameter. Default is 1."")\n\n    parser.add_argument(""--Q"",\n                        type=float,\n                        default=1,\n\t                help=""In-out hyperparameter. Default is 1."")\n\n    parser.add_argument(""--walker"",\n                        nargs=""?"",\n                        default=""first"",\n\t                help=""Random walker order. Default is first."")\n\n    parser.add_argument(""--dimensions"",\n                        type=int,\n                        default=16,\n\t                help=""Number of dimensions. Default is 16."")\n\n    parser.add_argument(""--random-walk-length"",\n                        type=int,\n                        default=80,\n\t                help=""Length of random walk per source. Default is 80."")\n\n    parser.add_argument(""--num-of-walks"",\n                        type=int,\n                        default=5,\n\t                help=""Number of random walks per source. Default is 5."")\n\n    parser.add_argument(""--window-size"",\n                        type=int,\n                        default=5,\n                    \thelp=""Window size for proximity statistic extraction. Default is 5."")\n\n    parser.add_argument(""--distortion"",\n                        type=float,\n                        default=0.75,\n\t                help=""Downsampling distortion. Default is 0.75."")\n\n    parser.add_argument(""--negative-sample-number"",\n                        type=int,\n                        default=10,\n\t                help=""Number of negative samples to draw. Default is 10."")\n\n    parser.add_argument(""--initial-learning-rate"",\n                        type=float,\n                        default=0.01,\n\t                help=""Initial learning rate. Default is 0.01."")\n\n    parser.add_argument(""--minimal-learning-rate"",\n                        type=float,\n                        default=0.001,\n\t                help=""Minimal learning rate. Default is 0.001."")\n\n    parser.add_argument(""--annealing-factor"",\n                        type=float,\n                        default=1,\n\t                help=""Annealing factor. Default is 1.0."")\n\n    parser.add_argument(""--initial-gamma"",\n                        type=float,\n                        default=0.1,\n\t                help=""Initial clustering weight. Default is 0.1."")\n\n    parser.add_argument(""--final-gamma"",\n                        type=float,\n                        default=0.5,\n\t                help=""Final clustering weight. Default is 0.5."")\n\n    parser.add_argument(""--lambd"",\n                        type=float,\n                        default=2.0**-4,\n\t                help=""Smoothness regularization penalty. Default is 0.0625."")\n\n    parser.add_argument(""--cluster-number"",\n                        type=int,\n                        default=20,\n\t                help=""Number of clusters. Default is 20."")\n\n    parser.add_argument(""--overlap-weighting"",\n                        nargs=""?"",\n                        default=""normalized_overlap"",\n\t                help=""Weight construction technique for regularization."")\n\n    parser.add_argument(""--regularization-noise"",\n                        type=float,\n                        default=10**-8,\n\t                help=""Uniform noise max and min on the feature vector distance."")\n\n    return parser.parse_args()\n'"
src/print_and_read.py,0,"b'import json\nimport pandas as pd\nimport networkx as nx\nfrom texttable import Texttable\n\ndef graph_reader(input_path):\n    """"""\n    Function to read a csv edge list and transform it to a networkx graph object.\n    """"""    \n    edges = pd.read_csv(input_path)\n    graph = nx.from_edgelist(edges.values.tolist())\n    return graph\n\ndef log_setup(args_in):\n    """"""\n    Function to setup the logging hash table.\n    """"""    \n    log = dict()\n    log[""times""] = []\n    log[""losses""] = []\n    log[""cluster_quality""] = []\n    log[""params""] = vars(args_in)\n    return log\n\ndef json_dumper(data, path):\n    """"""\n    Function to dump the logs and assignments.\n    """"""    \n    with open(path, ""w"") as outfile:\n        json.dump(data, outfile)\n\ndef initiate_dump_gemsec(log, assignments, args, final_embeddings, c_means):\n    """"""\n    Function to dump the logs and assignments for GEMSEC. If the matrix saving boolean is true the embedding is also saved.\n    """"""    \n    json_dumper(log, args.log_output)\n    json_dumper(assignments, args.assignment_output)\n    if args.dump_matrices:\n        final_embeddings = pd.DataFrame(final_embeddings)\n        final_embeddings.to_csv(args.embedding_output, index=None)\n        c_means = pd.DataFrame(c_means)\n        c_means.to_csv(args.cluster_mean_output, index=None)\n\ndef initiate_dump_dw(log, assignments, args, final_embeddings):\n    """"""\n    Function to dump the logs and assignments for DeepWalk. If the matrix saving boolean is true the embedding is also saved.\n    """"""        \n    json_dumper(log, args.log_output)\n    json_dumper(assignments, args.assignment_output)\n    if args.dump_matrices:\n        final_embeddings = pd.DataFrame(final_embeddings)\n        final_embeddings.to_csv(args.embedding_output, index=None)\n\ndef tab_printer(log):\n    """"""\n    Function to print the logs in a nice tabular format.\n    """"""    \n    t = Texttable() \n    t.add_rows([[""Epoch"", log[""losses""][-1][0]]])\n    print(t.draw())\n\n    t = Texttable()\n    t.add_rows([[""Loss"", round(log[""losses""][-1][1],3)]])\n    print(t.draw()) \n\n    t = Texttable()\n    t.add_rows([[""Modularity"", round(log[""cluster_quality""][-1][1],3)]])\n    print(t.draw()) \n\ndef epoch_printer(repetition):\n    """"""\n    Function to print the epoch number.\n    """"""    \n    print("""")\n    print(""Epoch "" + str(repetition+1) + "". initiated."")\n    print("""")\n\ndef log_updater(log, repetition, average_loss, optimization_time, modularity_score):\n    """""" \n    Function to update the log object.\n    """"""    \n    index = repetition + 1\n    log[""losses""] = log[""losses""] + [[int(index), float(average_loss)]]\n    log[""times""] = log[""times""] + [[int(index), float(optimization_time)]]\n    log[""cluster_quality""] = log[""cluster_quality""] + [[int(index), float(modularity_score)]]\n    return log\n'"
