file_path,api_count,code
src/gwnn.py,0,"b'import time\nimport torch\nfrom tqdm import trange\nfrom sklearn.model_selection import train_test_split\nfrom gwnn_layer import SparseGraphWaveletLayer, DenseGraphWaveletLayer\n\nclass GraphWaveletNeuralNetwork(torch.nn.Module):\n    """"""\n    Graph Wavelet Neural Network class.\n    For details see: Graph Wavelet Neural Network.\n    Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, Xueqi Cheng. ICLR, 2019\n    :param args: Arguments object.\n    :param ncount: Number of nodes.\n    :param feature_number: Number of features.\n    :param class_number: Number of classes.\n    :param device: Device used for training.\n    """"""\n    def __init__(self, args, ncount, feature_number, class_number, device):\n        super(GraphWaveletNeuralNetwork, self).__init__()\n        self.args = args\n        self.ncount = ncount\n        self.feature_number = feature_number\n        self.class_number = class_number\n        self.device = device\n        self.setup_layers()\n\n    def setup_layers(self):\n        """"""\n        Setting up a sparse and a dense layer.\n        """"""\n        self.convolution_1 = SparseGraphWaveletLayer(self.feature_number,\n                                                     self.args.filters,\n                                                     self.ncount,\n                                                     self.device)\n\n        self.convolution_2 = DenseGraphWaveletLayer(self.args.filters,\n                                                    self.class_number,\n                                                    self.ncount,\n                                                    self.device)\n\n    def forward(self, phi_indices, phi_values, phi_inverse_indices,\n                phi_inverse_values, feature_indices, feature_values):\n        """"""\n        Forward propagation pass.\n        :param phi_indices: Sparse wavelet matrix index pairs.\n        :param phi_values: Sparse wavelet matrix values.\n        :param phi_inverse_indices: Inverse wavelet matrix index pairs.\n        :param phi_inverse_values: Inverse wavelet matrix values.\n        :param feature_indices: Feature matrix index pairs.\n        :param feature_values: Feature matrix values.\n        :param predictions: Predicted node label vector.\n        """"""\n        deep_features_1 = self.convolution_1(phi_indices,\n                                             phi_values,\n                                             phi_inverse_indices,\n                                             phi_inverse_values,\n                                             feature_indices,\n                                             feature_values,\n                                             self.args.dropout)\n\n        deep_features_2 = self.convolution_2(phi_indices,\n                                             phi_values,\n                                             phi_inverse_indices,\n                                             phi_inverse_values,\n                                             deep_features_1)\n\n        predictions = torch.nn.functional.log_softmax(deep_features_2, dim=1)\n        return predictions\n        \nclass GWNNTrainer(object):\n    """"""\n    Graph Wavelet Neural Network Trainer object.\n    :param args: Arguments object.\n    :param sparsifier: Sparsifier object with sparse wavelet filters.\n    :param features: Sparse feature matrix.\n    :param target: Target vector.\n    """"""\n    def __init__(self, args, sparsifier, features, target):\n        self.args = args\n        self.sparsifier = sparsifier\n        self.features = features\n        self.target = target\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        self.setup_logs()\n        self.setup_features()\n        self.setup_model()\n        self.train_test_split()\n\n    def setup_logs(self):\n        """"""\n        Creating a log for performance measurements.\n        """"""\n        self.logs = dict()\n        self.logs[""parameters""] =  vars(self.args)\n        self.logs[""performance""] = [[""Epoch"", ""Loss""]]\n        self.logs[""training_time""] = [[""Epoch"", ""Seconds""]]\n\n    def update_log(self, loss, epoch):\n        """"""\n        Updating the logs.\n        :param loss:\n        :param epoch:\n        """"""\n        self.epochs.set_description(""GWNN (Loss=%g)"" % round(loss.item(), 4))\n        self.logs[""performance""].append([epoch, round(loss.item(), 4)])\n        self.logs[""training_time""].append([epoch, time.time()-self.time])\n\n    def setup_features(self):\n        """"""\n        Defining PyTorch tensors for sparse matrix multiplications.\n        """"""\n        self.ncount = self.sparsifier.phi_matrices[0].shape[0]\n        self.feature_number = self.features.shape[1]\n        self.class_number = max(self.target)+1\n        self.target = torch.LongTensor(self.target).to(self.device)\n        self.feature_indices = torch.LongTensor([self.features.row, self.features.col])\n        self.feature_indices = self.feature_indices.to(self.device)\n        self.feature_values = torch.FloatTensor(self.features.data).view(-1).to(self.device)\n        self.phi_indices = torch.LongTensor(self.sparsifier.phi_matrices[0].nonzero()).to(self.device)\n        self.phi_values = torch.FloatTensor(self.sparsifier.phi_matrices[0][self.sparsifier.phi_matrices[0].nonzero()])\n        self.phi_values = self.phi_values.view(-1).to(self.device)\n        self.phi_inverse_indices = torch.LongTensor(self.sparsifier.phi_matrices[1].nonzero()).to(self.device)\n        self.phi_inverse_values = torch.FloatTensor(self.sparsifier.phi_matrices[1][self.sparsifier.phi_matrices[1].nonzero()])\n        self.phi_inverse_values = self.phi_inverse_values.view(-1).to(self.device)\n\n    def setup_model(self):\n        """"""\n        Creating a log.\n        """"""\n        self.model = GraphWaveletNeuralNetwork(self.args,\n                                               self.ncount,\n                                               self.feature_number,\n                                               self.class_number,\n                                               self.device)\n        self.model = self.model.to(self.device)\n\n    def train_test_split(self):\n        """"""\n        Train-test split on the nodes.\n        """"""\n        nodes = [x for x in range(self.ncount)]\n\n        train_nodes, test_nodes = train_test_split(nodes,\n                                                   test_size=self.args.test_size,\n                                                   random_state=self.args.seed)\n\n        self.train_nodes = torch.LongTensor(train_nodes) \n        self.test_nodes = torch.LongTensor(test_nodes)\n\n    def fit(self):\n        """"""\n        Fitting a GWNN model.\n        """"""\n        print(""Training.\\n"")\n        self.optimizer = torch.optim.Adam(self.model.parameters(),\n                                          lr=self.args.learning_rate,\n                                          weight_decay=self.args.weight_decay)\n\n        self.model.train()\n        self.epochs = trange(self.args.epochs, desc=""Loss"")\n        for epoch in self.epochs:\n            self.time = time.time()\n            self.optimizer.zero_grad()\n            prediction = self.model(self.phi_indices,\n                                    self.phi_values,\n                                    self.phi_inverse_indices,\n                                    self.phi_inverse_values,\n                                    self.feature_indices,\n                                    self.feature_values)\n\n            loss = torch.nn.functional.nll_loss(prediction[self.train_nodes],\n                                                self.target[self.train_nodes])\n            loss.backward()\n            self.optimizer.step()\n            self.update_log(loss, epoch)\n\n    def score(self):\n        """"""\n        Scoring the test set.\n        """"""\n        print(""\\nScoring.\\n"")\n        self.model.eval()\n        _, prediction = self.model(self.phi_indices,\n                                   self.phi_values,\n                                   self.phi_inverse_indices,\n                                   self.phi_inverse_values,\n                                   self.feature_indices,\n                                   self.feature_values).max(dim=1)\n\n        correct = prediction[self.test_nodes].eq(self.target[self.test_nodes]).sum().item()\n        accuracy = correct/int(self.ncount*self.args.test_size)\n        print(""Test Accuracy: {:.4f}"".format(accuracy))\n        self.logs[""accuracy""] = accuracy\n'"
src/gwnn_layer.py,0,"b'""""""GWNN layers.""""""\n\nimport torch\nfrom torch_sparse import spspmm, spmm\n\nclass GraphWaveletLayer(torch.nn.Module):\n    """"""\n    Abstract Graph Wavelet Layer class.\n    :param in_channels: Number of features.\n    :param out_channels: Number of filters.\n    :param ncount: Number of nodes.\n    :param device: Device to train on.\n    """"""\n    def __init__(self, in_channels, out_channels, ncount, device):\n        super(GraphWaveletLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.ncount = ncount\n        self.device = device\n        self.define_parameters()\n        self.init_parameters()\n\n    def define_parameters(self):\n        """"""\n        Defining diagonal filter matrix (Theta in the paper) and weight matrix.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.in_channels, self.out_channels))\n        self.diagonal_weight_indices = torch.LongTensor([[node for node in range(self.ncount)],\n                                                         [node for node in range(self.ncount)]])\n\n        self.diagonal_weight_indices = self.diagonal_weight_indices.to(self.device)\n        self.diagonal_weight_filter = torch.nn.Parameter(torch.Tensor(self.ncount, 1))\n\n    def init_parameters(self):\n        """"""\n        Initializing the diagonal filter and the weight matrix.\n        """"""\n        torch.nn.init.uniform_(self.diagonal_weight_filter, 0.9, 1.1)\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n\nclass SparseGraphWaveletLayer(GraphWaveletLayer):\n    """"""\n    Sparse Graph Wavelet Layer Class.\n    """"""\n    def forward(self, phi_indices, phi_values, phi_inverse_indices,\n                phi_inverse_values, feature_indices, feature_values, dropout):\n        """"""\n        Forward propagation pass.\n        :param phi_indices: Sparse wavelet matrix index pairs.\n        :param phi_values: Sparse wavelet matrix values.\n        :param phi_inverse_indices: Inverse wavelet matrix index pairs.\n        :param phi_inverse_values: Inverse wavelet matrix values.\n        :param feature_indices: Feature matrix index pairs.\n        :param feature_values: Feature matrix values.\n        :param dropout: Dropout rate.\n        :return dropout_features: Filtered feature matrix extracted.\n        """"""\n        rescaled_phi_indices, rescaled_phi_values = spspmm(phi_indices,\n                                                           phi_values,\n                                                           self.diagonal_weight_indices,\n                                                           self.diagonal_weight_filter.view(-1),\n                                                           self.ncount,\n                                                           self.ncount,\n                                                           self.ncount)\n\n        phi_product_indices, phi_product_values = spspmm(rescaled_phi_indices,\n                                                         rescaled_phi_values,\n                                                         phi_inverse_indices,\n                                                         phi_inverse_values,\n                                                         self.ncount,\n                                                         self.ncount,\n                                                         self.ncount)\n\n        filtered_features = spmm(feature_indices,\n                                 feature_values,\n                                 self.ncount,\n                                 self.in_channels,\n                                 self.weight_matrix)\n\n        localized_features = spmm(phi_product_indices,\n                                  phi_product_values,\n                                  self.ncount,\n                                  self.ncount,\n                                  filtered_features)\n\n        dropout_features = torch.nn.functional.dropout(torch.nn.functional.relu(localized_features),\n                                                       training=self.training,\n                                                       p=dropout)\n        return dropout_features\n\nclass DenseGraphWaveletLayer(GraphWaveletLayer):\n    """"""\n    Dense Graph Wavelet Layer Class.\n    """"""\n    def forward(self, phi_indices, phi_values, phi_inverse_indices, phi_inverse_values, features):\n        """"""\n        Forward propagation pass.\n        :param phi_indices: Sparse wavelet matrix index pairs.\n        :param phi_values: Sparse wavelet matrix values.\n        :param phi_inverse_indices: Inverse wavelet matrix index pairs.\n        :param phi_inverse_values: Inverse wavelet matrix values.\n        :param features: Feature matrix.\n        :return localized_features: Filtered feature matrix extracted.\n        """"""\n        rescaled_phi_indices, rescaled_phi_values = spspmm(phi_indices,\n                                                           phi_values,\n                                                           self.diagonal_weight_indices,\n                                                           self.diagonal_weight_filter.view(-1),\n                                                           self.ncount,\n                                                           self.ncount,\n                                                           self.ncount)\n\n        phi_product_indices, phi_product_values = spspmm(rescaled_phi_indices,\n                                                         rescaled_phi_values,\n                                                         phi_inverse_indices,\n                                                         phi_inverse_values,\n                                                         self.ncount,\n                                                         self.ncount,\n                                                         self.ncount)\n\n        filtered_features = torch.mm(features, self.weight_matrix)\n\n        localized_features = spmm(phi_product_indices,\n                                  phi_product_values,\n                                  self.ncount,\n                                  self.ncount,\n                                  filtered_features)\n\n        return localized_features\n'"
src/main.py,0,"b'""""""Running GWNN.""""""\n\nfrom gwnn import GWNNTrainer\nfrom utils import WaveletSparsifier\nfrom param_parser import parameter_parser\nfrom utils import tab_printer, graph_reader, feature_reader, target_reader, save_logs\n\ndef main():\n    """"""\n    Parsing command line parameters, reading data.\n    Doing sparsification, fitting a GWNN and saving the logs.\n    """"""\n    args = parameter_parser()\n    tab_printer(args)\n    graph = graph_reader(args.edge_path)\n    features = feature_reader(args.features_path)\n    target = target_reader(args.target_path)\n    sparsifier = WaveletSparsifier(graph, args.scale, args.approximation_order, args.tolerance)\n    sparsifier.calculate_all_wavelets()\n    trainer = GWNNTrainer(args, sparsifier, features, target)\n    trainer.fit()\n    trainer.score()\n    save_logs(args, trainer.logs)\n    \nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Parsing the parameters.""""""\n\nimport argparse\n\ndef parameter_parser():\n\n    parser = argparse.ArgumentParser(description=""Run GWNN."")\n\n    parser.add_argument(""--edge-path"",\n                        nargs=""?"",\n                        default=""./input/cora_edges.csv"",\n\t                help=""Edge list csv."")\n\n    parser.add_argument(""--features-path"",\n                        nargs=""?"",\n                        default=""./input/cora_features.json"",\n\t                help=""Feature json."")\n\n    parser.add_argument(""--target-path"",\n                        nargs=""?"",\n                        default=""./input/cora_target.csv"",\n\t                help=""Target classes csv."")\n\n    parser.add_argument(""--log-path"",\n                        nargs=""?"",\n                        default=""./logs/cora_logs.json"",\n\t                help=""Log json."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=200,\n\t                help=""Number of training epochs. Default is 200."")\n\n    parser.add_argument(""--filters"",\n                        type=int,\n                        default=32,\n\t                help=""Filters (neurons) in convolution. Default is 32."")\n\n    parser.add_argument(""--approximation-order"",\n                        type=int,\n                        default=3,\n\t                help=""Order of Chebyshev polynomial. Default is 3."")\n\n    parser.add_argument(""--test-size"",\n                        type=float,\n                        default=0.2,\n\t                help=""Ratio of training samples. Default is 0.2."")\n\n    parser.add_argument(""--dropout"",\n                        type=float,\n                        default=0.5,\n\t                help=""Dropout probability. Default is 0.5."")\n\n    parser.add_argument(""--seed"",\n                        type=int,\n                        default=42,\n\t                help=""Random seed for sklearn pre-training. Default is 42."")\n\n    parser.add_argument(""--tolerance"",\n                        type = float,\n                        default=10**-4,\n\t                help=""Sparsification parameter. Default is 10^-4."")\n\n    parser.add_argument(""--scale"",\n                        type=float,\n                        default=1.0,\n\t                help=""Heat kernel scale length. Default is 1.0."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.01,\n\t                help=""Learning rate. Default is 0.01."")\n\n    parser.add_argument(""--weight-decay"",\n                        type=float,\n                        default=10**-5,\n\t                help=""Adam weight decay. Default is 10^-5."")\n\n    return parser.parse_args()\n'"
src/utils.py,0,"b'""""""GWNN data reading utils.""""""\n\nimport json\nimport pygsp\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom scipy import sparse\nfrom texttable import Texttable\nfrom sklearn.preprocessing import normalize\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef graph_reader(path):\n    """"""\n    Function to create an NX graph object.\n    :param path: Path to the edge list csv.\n    :return graph: NetworkX graph.\n    """"""\n    graph = nx.from_edgelist(pd.read_csv(path).values.tolist())\n    graph.remove_edges_from(nx.selfloop_edges(graph))\n    return graph\n\ndef feature_reader(path):\n    """"""\n    Reading the feature matrix stored as JSON from the disk.\n    :param feature_path: Path to the JSON file.\n    :return features: Feature sparse COO matrix.\n    """"""\n    features = json.load(open(path))\n    index_1 = [int(k) for k, v in features.items() for fet in v]\n    index_2 = [int(fet) for k, v in features.items() for fet in v]\n    values = [1.0]*len(index_1)\n    nodes = [int(k) for k, v in features.items()]\n    node_count = max(nodes)+1\n    feature_count = max(index_2)+1\n    features = sparse.coo_matrix((values, (index_1, index_2)),\n                                 shape=(node_count, feature_count),\n                                 dtype=np.float32)\n    return features\n\ndef target_reader(path):\n    """"""\n    Reading thetarget vector to a numpy column vector.\n    :param path: Path to the target csv.\n    :return target: Target vector.\n    """"""\n    target = np.array(pd.read_csv(path)[""target""])\n    return target\n\ndef save_logs(args, logs):\n    """"""\n    Save the logs at the path.\n    :param args: Arguments objects.\n    :param logs: Log dictionary.\n    """"""\n    with open(args.log_path, ""w"") as f:\n        json.dump(logs, f)\n\nclass WaveletSparsifier(object):\n    """"""\n    Object to sparsify the wavelet coefficients for a graph.\n    """"""\n    def __init__(self, graph, scale, approximation_order, tolerance):\n        """"""\n        :param graph: NetworkX graph object.\n        :param scale: Kernel scale length parameter.\n        :param approximation_order: Chebyshev polynomial order.\n        :param tolerance: Tolerance for sparsification.\n        """"""\n        self.graph = graph\n        self.pygsp_graph = pygsp.graphs.Graph(nx.adjacency_matrix(self.graph))\n        self.pygsp_graph.estimate_lmax()\n        self.scales = [-scale, scale]\n        self.approximation_order = approximation_order\n        self.tolerance = tolerance\n        self.phi_matrices = []\n\n    def calculate_wavelet(self):\n        """"""\n        Creating sparse wavelets.\n        :return remaining_waves: Sparse matrix of attenuated wavelets.\n        """"""\n        impulse = np.eye(self.graph.number_of_nodes(), dtype=int)\n        wavelet_coefficients = pygsp.filters.approximations.cheby_op(self.pygsp_graph,\n                                                                     self.chebyshev,\n                                                                     impulse)\n        wavelet_coefficients[wavelet_coefficients < self.tolerance] = 0\n        ind_1, ind_2 = wavelet_coefficients.nonzero()\n        n_count = self.graph.number_of_nodes()\n        remaining_waves = sparse.csr_matrix((wavelet_coefficients[ind_1, ind_2], (ind_1, ind_2)),\n                                            shape=(n_count, n_count),\n                                            dtype=np.float32)\n        return remaining_waves\n\n    def normalize_matrices(self):\n        """"""\n        Normalizing the wavelet and inverse wavelet matrices.\n        """"""\n        print(""\\nNormalizing the sparsified wavelets.\\n"")\n        for i, phi_matrix in enumerate(self.phi_matrices):\n            self.phi_matrices[i] = normalize(self.phi_matrices[i], norm=\'l1\', axis=1)\n\n    def calculate_density(self):\n        """"""\n        Calculating the density of the sparsified wavelet matrices.\n        """"""\n        wavelet_density = len(self.phi_matrices[0].nonzero()[0])/(self.graph.number_of_nodes()**2)\n        wavelet_density = str(round(100*wavelet_density, 2))\n        inverse_wavelet_density = len(self.phi_matrices[1].nonzero()[0])/(self.graph.number_of_nodes()**2)\n        inverse_wavelet_density = str(round(100*inverse_wavelet_density, 2))\n        print(""Density of wavelets: ""+wavelet_density+""%."")\n        print(""Density of inverse wavelets: ""+inverse_wavelet_density+""%.\\n"")\n\n    def calculate_all_wavelets(self):\n        """"""\n        Graph wavelet coefficient calculation.\n        """"""\n        print(""\\nWavelet calculation and sparsification started.\\n"")\n        for i, scale in enumerate(self.scales):\n            self.heat_filter = pygsp.filters.Heat(self.pygsp_graph,\n                                                  tau=[scale])\n            self.chebyshev = pygsp.filters.approximations.compute_cheby_coeff(self.heat_filter,\n                                                                              m=self.approximation_order)\n            sparsified_wavelets = self.calculate_wavelet()          \n            self.phi_matrices.append(sparsified_wavelets)\n        self.normalize_matrices()\n        self.calculate_density()\n'"
