file_path,api_count,code
src/layers.py,0,"b'""""""Classes for SimGNN modules.""""""\n\nimport torch\n\nclass AttentionModule(torch.nn.Module):\n    """"""\n    SimGNN Attention Module to make a pass on graph.\n    """"""\n    def __init__(self, args):\n        """"""\n        :param args: Arguments object.\n        """"""\n        super(AttentionModule, self).__init__()\n        self.args = args\n        self.setup_weights()\n        self.init_parameters()\n\n    def setup_weights(self):\n        """"""\n        Defining weights.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.args.filters_3,\n                                                             self.args.filters_3))\n\n    def init_parameters(self):\n        """"""\n        Initializing weights.\n        """"""\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n\n    def forward(self, embedding):\n        """"""\n        Making a forward propagation pass to create a graph level representation.\n        :param embedding: Result of the GCN.\n        :return representation: A graph level representation vector.\n        """"""\n        global_context = torch.mean(torch.matmul(embedding, self.weight_matrix), dim=0)\n        transformed_global = torch.tanh(global_context)\n        sigmoid_scores = torch.sigmoid(torch.mm(embedding, transformed_global.view(-1, 1)))\n        representation = torch.mm(torch.t(embedding), sigmoid_scores)\n        return representation\n\nclass TenorNetworkModule(torch.nn.Module):\n    """"""\n    SimGNN Tensor Network module to calculate similarity vector.\n    """"""\n    def __init__(self, args):\n        """"""\n        :param args: Arguments object.\n        """"""\n        super(TenorNetworkModule, self).__init__()\n        self.args = args\n        self.setup_weights()\n        self.init_parameters()\n\n    def setup_weights(self):\n        """"""\n        Defining weights.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.args.filters_3,\n                                                             self.args.filters_3,\n                                                             self.args.tensor_neurons))\n\n        self.weight_matrix_block = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons,\n                                                                   2*self.args.filters_3))\n        self.bias = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons, 1))\n\n    def init_parameters(self):\n        """"""\n        Initializing weights.\n        """"""\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n        torch.nn.init.xavier_uniform_(self.weight_matrix_block)\n        torch.nn.init.xavier_uniform_(self.bias)\n\n    def forward(self, embedding_1, embedding_2):\n        """"""\n        Making a forward propagation pass to create a similarity vector.\n        :param embedding_1: Result of the 1st embedding after attention.\n        :param embedding_2: Result of the 2nd embedding after attention.\n        :return scores: A similarity score vector.\n        """"""\n        scoring = torch.mm(torch.t(embedding_1), self.weight_matrix.view(self.args.filters_3, -1))\n        scoring = scoring.view(self.args.filters_3, self.args.tensor_neurons)\n        scoring = torch.mm(torch.t(scoring), embedding_2)\n        combined_representation = torch.cat((embedding_1, embedding_2))\n        block_scoring = torch.mm(self.weight_matrix_block, combined_representation)\n        scores = torch.nn.functional.relu(scoring + block_scoring + self.bias)\n        return scores\n'"
src/main.py,0,"b'""""""SimGNN runner.""""""\n\nfrom utils import tab_printer\nfrom simgnn import SimGNNTrainer\nfrom param_parser import parameter_parser\n\ndef main():\n    """"""\n    Parsing command line parameters, reading data.\n    Fitting and scoring a SimGNN model.\n    """"""\n    args = parameter_parser()\n    tab_printer(args)\n    trainer = SimGNNTrainer(args)\n    trainer.fit()\n    trainer.score()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Getting params from the command line.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters.\n    The default hyperparameters give a high performance model without grid search.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run SimGNN."")\n\n    parser.add_argument(""--training-graphs"",\n                        nargs=""?"",\n                        default=""./dataset/train/"",\n\t                help=""Folder with training graph pair jsons."")\n\n    parser.add_argument(""--testing-graphs"",\n                        nargs=""?"",\n                        default=""./dataset/test/"",\n\t                help=""Folder with testing graph pair jsons."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=5,\n\t                help=""Number of training epochs. Default is 5."")\n\n    parser.add_argument(""--filters-1"",\n                        type=int,\n                        default=128,\n\t                help=""Filters (neurons) in 1st convolution. Default is 128."")\n\n    parser.add_argument(""--filters-2"",\n                        type=int,\n                        default=64,\n\t                help=""Filters (neurons) in 2nd convolution. Default is 64."")\n\n    parser.add_argument(""--filters-3"",\n                        type=int,\n                        default=32,\n\t                help=""Filters (neurons) in 3rd convolution. Default is 32."")\n\n    parser.add_argument(""--tensor-neurons"",\n                        type=int,\n                        default=16,\n\t                help=""Neurons in tensor network layer. Default is 16."")\n\n    parser.add_argument(""--bottle-neck-neurons"",\n                        type=int,\n                        default=16,\n\t                help=""Bottle neck layer neurons. Default is 16."")\n\n    parser.add_argument(""--batch-size"",\n                        type=int,\n                        default=128,\n\t                help=""Number of graph pairs per batch. Default is 128."")\n\n    parser.add_argument(""--bins"",\n                        type=int,\n                        default=16,\n\t                help=""Similarity score bins. Default is 16."")\n\n    parser.add_argument(""--dropout"",\n                        type=float,\n                        default=0.5,\n\t                help=""Dropout probability. Default is 0.5."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.001,\n\t                help=""Learning rate. Default is 0.001."")\n\n    parser.add_argument(""--weight-decay"",\n                        type=float,\n                        default=5*10**-4,\n\t                help=""Adam weight decay. Default is 5*10^-4."")\n\n    parser.add_argument(""--histogram"",\n                        dest=""histogram"",\n                        action=""store_true"")\n\n    parser.set_defaults(histogram=False)\n\n    return parser.parse_args()\n'"
src/simgnn.py,0,"b'""""""SimGNN class and runner.""""""\n\nimport glob\nimport torch\nimport random\nimport numpy as np\nfrom tqdm import tqdm, trange\nfrom torch_geometric.nn import GCNConv\nfrom layers import AttentionModule, TenorNetworkModule\nfrom utils import process_pair, calculate_loss, calculate_normalized_ged\n\nclass SimGNN(torch.nn.Module):\n    """"""\n    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n    https://arxiv.org/abs/1808.05689\n    """"""\n    def __init__(self, args, number_of_labels):\n        """"""\n        :param args: Arguments object.\n        :param number_of_labels: Number of node labels.\n        """"""\n        super(SimGNN, self).__init__()\n        self.args = args\n        self.number_labels = number_of_labels\n        self.setup_layers()\n\n    def calculate_bottleneck_features(self):\n        """"""\n        Deciding the shape of the bottleneck layer.\n        """"""\n        if self.args.histogram == True:\n            self.feature_count = self.args.tensor_neurons + self.args.bins\n        else:\n            self.feature_count = self.args.tensor_neurons\n\n    def setup_layers(self):\n        """"""\n        Creating the layers.\n        """"""\n        self.calculate_bottleneck_features()\n        self.convolution_1 = GCNConv(self.number_labels, self.args.filters_1)\n        self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)\n        self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)\n        self.attention = AttentionModule(self.args)\n        self.tensor_network = TenorNetworkModule(self.args)\n        self.fully_connected_first = torch.nn.Linear(self.feature_count,\n                                                     self.args.bottle_neck_neurons)\n        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons, 1)\n\n    def calculate_histogram(self, abstract_features_1, abstract_features_2):\n        """"""\n        Calculate histogram from similarity matrix.\n        :param abstract_features_1: Feature matrix for graph 1.\n        :param abstract_features_2: Feature matrix for graph 2.\n        :return hist: Histsogram of similarity scores.\n        """"""\n        scores = torch.mm(abstract_features_1, abstract_features_2).detach()\n        scores = scores.view(-1, 1)\n        hist = torch.histc(scores, bins=self.args.bins)\n        hist = hist/torch.sum(hist)\n        hist = hist.view(1, -1)\n        return hist\n\n    def convolutional_pass(self, edge_index, features):\n        """"""\n        Making convolutional pass.\n        :param edge_index: Edge indices.\n        :param features: Feature matrix.\n        :return features: Absstract feature matrix.\n        """"""\n        features = self.convolution_1(features, edge_index)\n        features = torch.nn.functional.relu(features)\n        features = torch.nn.functional.dropout(features,\n                                               p=self.args.dropout,\n                                               training=self.training)\n\n        features = self.convolution_2(features, edge_index)\n        features = torch.nn.functional.relu(features)\n        features = torch.nn.functional.dropout(features,\n                                               p=self.args.dropout,\n                                               training=self.training)\n\n        features = self.convolution_3(features, edge_index)\n        return features\n\n    def forward(self, data):\n        """"""\n        Forward pass with graphs.\n        :param data: Data dictiyonary.\n        :return score: Similarity score.\n        """"""\n        edge_index_1 = data[""edge_index_1""]\n        edge_index_2 = data[""edge_index_2""]\n        features_1 = data[""features_1""]\n        features_2 = data[""features_2""]\n\n        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1)\n        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2)\n\n        if self.args.histogram == True:\n            hist = self.calculate_histogram(abstract_features_1,\n                                            torch.t(abstract_features_2))\n\n        pooled_features_1 = self.attention(abstract_features_1)\n        pooled_features_2 = self.attention(abstract_features_2)\n        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n        scores = torch.t(scores)\n\n        if self.args.histogram == True:\n            scores = torch.cat((scores, hist), dim=1).view(1, -1)\n\n        scores = torch.nn.functional.relu(self.fully_connected_first(scores))\n        score = torch.sigmoid(self.scoring_layer(scores))\n        return score\n\nclass SimGNNTrainer(object):\n    """"""\n    SimGNN model trainer.\n    """"""\n    def __init__(self, args):\n        """"""\n        :param args: Arguments object.\n        """"""\n        self.args = args\n        self.initial_label_enumeration()\n        self.setup_model()\n\n    def setup_model(self):\n        """"""\n        Creating a SimGNN.\n        """"""\n        self.model = SimGNN(self.args, self.number_of_labels)\n\n    def initial_label_enumeration(self):\n        """"""\n        Collecting the unique node idsentifiers.\n        """"""\n        print(""\\nEnumerating unique labels.\\n"")\n        self.training_graphs = glob.glob(self.args.training_graphs + ""*.json"")\n        self.testing_graphs = glob.glob(self.args.testing_graphs + ""*.json"")\n        graph_pairs = self.training_graphs + self.testing_graphs\n        self.global_labels = set()\n        for graph_pair in tqdm(graph_pairs):\n            data = process_pair(graph_pair)\n            self.global_labels = self.global_labels.union(set(data[""labels_1""]))\n            self.global_labels = self.global_labels.union(set(data[""labels_2""]))\n        self.global_labels = list(self.global_labels)\n        self.global_labels = {val:index  for index, val in enumerate(self.global_labels)}\n        self.number_of_labels = len(self.global_labels)\n\n    def create_batches(self):\n        """"""\n        Creating batches from the training graph list.\n        :return batches: List of lists with batches.\n        """"""\n        random.shuffle(self.training_graphs)\n        batches = []\n        for graph in range(0, len(self.training_graphs), self.args.batch_size):\n            batches.append(self.training_graphs[graph:graph+self.args.batch_size])\n        return batches\n\n    def transfer_to_torch(self, data):\n        """"""\n        Transferring the data to torch and creating a hash table.\n        Including the indices, features and target.\n        :param data: Data dictionary.\n        :return new_data: Dictionary of Torch Tensors.\n        """"""\n        new_data = dict()\n        edges_1 = data[""graph_1""] + [[y, x] for x, y in data[""graph_1""]]\n\n        edges_2 = data[""graph_2""] + [[y, x] for x, y in data[""graph_2""]]\n\n        edges_1 = torch.from_numpy(np.array(edges_1, dtype=np.int64).T).type(torch.long)\n        edges_2 = torch.from_numpy(np.array(edges_2, dtype=np.int64).T).type(torch.long)\n\n        features_1, features_2 = [], []\n\n        for n in data[""labels_1""]:\n            features_1.append([1.0 if self.global_labels[n] == i else 0.0 for i in self.global_labels.values()])\n\n        for n in data[""labels_2""]:\n            features_2.append([1.0 if self.global_labels[n] == i else 0.0 for i in self.global_labels.values()])\n\n        features_1 = torch.FloatTensor(np.array(features_1))\n        features_2 = torch.FloatTensor(np.array(features_2))\n\n        new_data[""edge_index_1""] = edges_1\n        new_data[""edge_index_2""] = edges_2\n\n        new_data[""features_1""] = features_1\n        new_data[""features_2""] = features_2\n\n        norm_ged = data[""ged""]/(0.5*(len(data[""labels_1""])+len(data[""labels_2""])))\n\n        new_data[""target""] = torch.from_numpy(np.exp(-norm_ged).reshape(1, 1)).view(-1).float()\n        return new_data\n\n    def process_batch(self, batch):\n        """"""\n        Forward pass with a batch of data.\n        :param batch: Batch of graph pair locations.\n        :return loss: Loss on the batch.\n        """"""\n        self.optimizer.zero_grad()\n        losses = 0\n        for graph_pair in batch:\n            data = process_pair(graph_pair)\n            data = self.transfer_to_torch(data)\n            target = data[""target""]\n            prediction = self.model(data)\n            losses = losses + torch.nn.functional.mse_loss(data[""target""], prediction)\n        losses.backward(retain_graph=True)\n        self.optimizer.step()\n        loss = losses.item()\n        return loss\n\n    def fit(self):\n        """"""\n        Fitting a model.\n        """"""\n        print(""\\nModel training.\\n"")\n\n        self.optimizer = torch.optim.Adam(self.model.parameters(),\n                                          lr=self.args.learning_rate,\n                                          weight_decay=self.args.weight_decay)\n\n        self.model.train()\n        epochs = trange(self.args.epochs, leave=True, desc=""Epoch"")\n        for epoch in epochs:\n            batches = self.create_batches()\n            self.loss_sum = 0\n            main_index = 0\n            for index, batch in tqdm(enumerate(batches), total=len(batches), desc=""Batches""):\n                loss_score = self.process_batch(batch)\n                main_index = main_index + len(batch)\n                self.loss_sum = self.loss_sum + loss_score * len(batch)\n                loss = self.loss_sum/main_index\n                epochs.set_description(""Epoch (Loss=%g)"" % round(loss, 5))\n\n    def score(self):\n        """"""\n        Scoring on the test set.\n        """"""\n        print(""\\n\\nModel evaluation.\\n"")\n        self.model.eval()\n        self.scores = []\n        self.ground_truth = []\n        for graph_pair in tqdm(self.testing_graphs):\n            data = process_pair(graph_pair)\n            self.ground_truth.append(calculate_normalized_ged(data))\n            data = self.transfer_to_torch(data)\n            target = data[""target""]\n            prediction = self.model(data)\n            self.scores.append(calculate_loss(prediction, target))\n        self.print_evaluation()\n\n    def print_evaluation(self):\n        """"""\n        Printing the error rates.\n        """"""\n        norm_ged_mean = np.mean(self.ground_truth)\n        base_error = np.mean([(n-norm_ged_mean)**2 for n in self.ground_truth])\n        model_error = np.mean(self.scores)\n        print(""\\nBaseline error: "" +str(round(base_error, 5))+""."")\n        print(""\\nModel test error: "" +str(round(model_error, 5))+""."")\n'"
src/utils.py,0,"b'""""""Data processing utilities.""""""\n\nimport json\nimport math\nfrom texttable import Texttable\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef process_pair(path):\n    """"""\n    Reading a json file with a pair of graphs.\n    :param path: Path to a JSON file.\n    :return data: Dictionary with data.\n    """"""\n    data = json.load(open(path))\n    return data\n\ndef calculate_loss(prediction, target):\n    """"""\n    Calculating the squared loss on the normalized GED.\n    :param prediction: Predicted log value of GED.\n    :param target: Factual log transofmed GED.\n    :return score: Squared error.\n    """"""\n    prediction = -math.log(prediction)\n    target = -math.log(target)\n    score = (prediction-target)**2\n    return score\n\ndef calculate_normalized_ged(data):\n    """"""\n    Calculating the normalized GED for a pair of graphs.\n    :param data: Data table.\n    :return norm_ged: Normalized GED score.\n    """"""\n    norm_ged = data[""ged""]/(0.5*(len(data[""labels_1""])+len(data[""labels_2""])))\n    return norm_ged\n'"
