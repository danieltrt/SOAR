file_path,api_count,code
linux/tensorbox/Car_detection.py,8,"b'import tensorflow as tf\nimport os\nimport json\nimport subprocess\nimport sysv_ipc\nimport struct\n\nfrom scipy.misc import imread, imresize\nfrom scipy import misc\n\nfrom train import build_forward\nfrom utils.annolist import AnnotationLib as al\nfrom utils.train_utils import add_rectangles, rescale_boxes\nfrom pymouse import PyMouse\n\nimport cv2\nimport argparse\nimport time\nimport numpy as np\n\ndef get_image_dir(args):\n    weights_iteration = int(args.weights.split(\'-\')[-1])\n    expname = \'_\' + args.expname if args.expname else \'\'\n    image_dir = \'%s/images_%s_%d%s\' % (os.path.dirname(args.weights), os.path.basename(args.test_boxes)[:-5], weights_iteration, expname)\n    return image_dir\n\ndef get_results(args, H):\n    tf.reset_default_graph()\n    x_in = tf.placeholder(tf.float32, name=\'x_in\', shape=[H[\'image_height\'], H[\'image_width\'], 3])\n    if H[\'use_rezoom\']:\n        pred_boxes, pred_logits, pred_confidences, pred_confs_deltas, pred_boxes_deltas = build_forward(H, tf.expand_dims(x_in, 0), \'test\', reuse=None)\n        grid_area = H[\'grid_height\'] * H[\'grid_width\']\n        pred_confidences = tf.reshape(tf.nn.softmax(tf.reshape(pred_confs_deltas, [grid_area * H[\'rnn_len\'], 2])), [grid_area, H[\'rnn_len\'], 2])\n        if H[\'reregress\']:\n            pred_boxes = pred_boxes + pred_boxes_deltas\n    else:\n        pred_boxes, pred_logits, pred_confidences = build_forward(H, tf.expand_dims(x_in, 0), \'test\', reuse=None)\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        saver.restore(sess, args.weights)\n\n        pred_annolist = al.AnnoList()\n\n        data_dir = os.path.dirname(args.test_boxes)\n        image_dir = get_image_dir(args)\n        subprocess.call(\'mkdir -p %s\' % image_dir, shell=True)\n\t\n\tmemory = sysv_ipc.SharedMemory(123463)\n\tmemory2 = sysv_ipc.SharedMemory(123464)\n\tsize = 768, 1024, 3\n\n\tpedal = PyMouse()\n\tpedal.press(1)\n\troad_center = 320\n\twhile True:\n\t    cv2.waitKey(1)\n\t    frameCount = bytearray(memory.read())\n\t    curve = bytearray(memory2.read())\n\t    curve = str(struct.unpack(\'i\',curve)[0])\n\t    m = np.array(frameCount, dtype=np.uint8)\n\t    orig_img = m.reshape(size)\n\t   \n\t    img = imresize(orig_img, (H[""image_height""], H[""image_width""]), interp=\'cubic\')\n\t    feed = {x_in: img}\n\t    (np_pred_boxes, np_pred_confidences) = sess.run([pred_boxes, pred_confidences], feed_dict=feed)\n\t    pred_anno = al.Annotation()\n\t    \n\t    new_img, rects = add_rectangles(H, [img], np_pred_confidences, np_pred_boxes,\n\t\t\t\t\t    use_stitching=True, rnn_len=H[\'rnn_len\'], min_conf=args.min_conf, tau=args.tau, show_suppressed=args.show_suppressed)\n\t    flag = 0\n\t    road_center = 320 + int(curve)\n\t    print(road_center)\n\t    for rect in rects:\n\t\tprint(rect.x1, rect.x2, rect.y2)\n\t\tif (rect.x1 < road_center and rect.x2 > road_center and rect.y2 > 200) and (rect.x2 - rect.x1 > 30):\n\t\t\tflag = 1\n\n\t    if flag is 1:\n\t\tpedal.press(2)\n\t\tprint(""break!"")\n\t    else:\n\t\tpedal.release(2)\n\t\tpedal.press(1)\n\t\tprint(""acceleration!"")\n\t\t\n\t    pred_anno.rects = rects\n\t    pred_anno.imagePath = os.path.abspath(data_dir)\n\t    pred_anno = rescale_boxes((H[""image_height""], H[""image_width""]), pred_anno, orig_img.shape[0], orig_img.shape[1])\n\t    pred_annolist.append(pred_anno)\n\t    \n\t    cv2.imshow(\'.jpg\', new_img)\n\t    \n    return none;\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--weights\', default=\'tensorbox/output/overfeat_rezoom_2017_02_09_13.28/save.ckpt-100000\')\n    parser.add_argument(\'--expname\', default=\'\')\n    parser.add_argument(\'--test_boxes\', default=\'default\')\n    parser.add_argument(\'--gpu\', default=0)\n    parser.add_argument(\'--logdir\', default=\'output\')\n    parser.add_argument(\'--iou_threshold\', default=0.5, type=float)\n    parser.add_argument(\'--tau\', default=0.25, type=float)\n    parser.add_argument(\'--min_conf\', default=0.2, type=float)\n    parser.add_argument(\'--show_suppressed\', default=True, type=bool)\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(args.gpu)\n    hypes_file = \'%s/hypes.json\' % os.path.dirname(args.weights)\n    with open(hypes_file, \'r\') as f:\n        H = json.load(f)\n    expname = args.expname + \'_\' if args.expname else \'\'\n    pred_boxes = \'%s.%s%s\' % (args.weights, expname, os.path.basename(args.test_boxes))\n\n    get_results(args, H)\n\nif __name__ == \'__main__\':\n    main()\n'"
linux/tensorbox/train.py,116,"b'#!/usr/bin/env python\nimport json\nimport cv2\nimport tensorflow.contrib.slim as slim\nimport datetime\nimport random\nimport time\nimport string\nimport argparse\nimport os\nimport threading\nfrom scipy import misc\nimport tensorflow as tf\nimport numpy as np\nfrom distutils.version import LooseVersion\nif LooseVersion(tf.__version__) >= LooseVersion(\'1.0\'):\n    rnn_cell = tf.contrib.rnn\nelse:\n    try:\n        from tensorflow.models.rnn import rnn_cell\n    except ImportError:\n        rnn_cell = tf.nn.rnn_cell\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\n\nrandom.seed(0)\nnp.random.seed(0)\n\nfrom utils import train_utils, googlenet_load, tf_concat\n\n@ops.RegisterGradient(""Hungarian"")\ndef _hungarian_grad(op, *args):\n    return map(array_ops.zeros_like, op.inputs)\n\ndef build_lstm_inner(H, lstm_input):\n    \'\'\'\n    build lstm decoder\n    \'\'\'\n    lstm_cell = rnn_cell.BasicLSTMCell(H[\'lstm_size\'], forget_bias=0.0, state_is_tuple=False)\n    if H[\'num_lstm_layers\'] > 1:\n        lstm = rnn_cell.MultiRNNCell([lstm_cell] * H[\'num_lstm_layers\'], state_is_tuple=False)\n    else:\n        lstm = lstm_cell\n\n    batch_size = H[\'batch_size\'] * H[\'grid_height\'] * H[\'grid_width\']\n    state = tf.zeros([batch_size, lstm.state_size])\n\n    outputs = []\n    with tf.variable_scope(\'RNN\', initializer=tf.random_uniform_initializer(-0.1, 0.1)):\n        for time_step in range(H[\'rnn_len\']):\n            if time_step > 0: tf.get_variable_scope().reuse_variables()\n            output, state = lstm(lstm_input, state)\n            outputs.append(output)\n    return outputs\n\ndef build_overfeat_inner(H, lstm_input):\n    \'\'\'\n    build simple overfeat decoder\n    \'\'\'\n    if H[\'rnn_len\'] > 1:\n        raise ValueError(\'rnn_len > 1 only supported with use_lstm == True\')\n    outputs = []\n    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n    with tf.variable_scope(\'Overfeat\', initializer=initializer):\n        w = tf.get_variable(\'ip\', shape=[H[\'later_feat_channels\'], H[\'lstm_size\']])\n        outputs.append(tf.matmul(lstm_input, w))\n    return outputs\n\ndef deconv(x, output_shape, channels):\n    k_h = 2\n    k_w = 2\n    w = tf.get_variable(\'w_deconv\', initializer=tf.random_normal_initializer(stddev=0.01),\n                        shape=[k_h, k_w, channels[1], channels[0]])\n    y = tf.nn.conv2d_transpose(x, w, output_shape, strides=[1, k_h, k_w, 1], padding=\'VALID\')\n    return y\n\ndef rezoom(H, pred_boxes, early_feat, early_feat_channels, w_offsets, h_offsets):\n    \'\'\'\n    Rezoom into a feature map at multiple interpolation points in a grid.\n    If the predicted object center is at X, len(w_offsets) == 3, and len(h_offsets) == 5,\n    the rezoom grid will look as follows:\n    [o o o]\n    [o o o]\n    [o X o]\n    [o o o]\n    [o o o]\n    Where each letter indexes into the feature map with bilinear interpolation\n    \'\'\'\n\n\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n    indices = []\n    for w_offset in w_offsets:\n        for h_offset in h_offsets:\n            indices.append(train_utils.bilinear_select(H,\n                                                       pred_boxes,\n                                                       early_feat,\n                                                       early_feat_channels,\n                                                       w_offset, h_offset))\n\n    interp_indices = tf_concat(0, indices)\n    rezoom_features = train_utils.interp(early_feat,\n                                         interp_indices,\n                                         early_feat_channels)\n    rezoom_features_r = tf.reshape(rezoom_features,\n                                   [len(w_offsets) * len(h_offsets),\n                                    outer_size,\n                                    H[\'rnn_len\'],\n                                    early_feat_channels])\n    rezoom_features_t = tf.transpose(rezoom_features_r, [1, 2, 0, 3])\n    return tf.reshape(rezoom_features_t,\n                      [outer_size,\n                       H[\'rnn_len\'],\n                       len(w_offsets) * len(h_offsets) * early_feat_channels])\n\ndef build_forward(H, x, phase, reuse):\n    \'\'\'\n    Construct the forward model\n    \'\'\'\n\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n    input_mean = 117.\n    x -= input_mean\n    cnn, early_feat = googlenet_load.model(x, H, reuse)\n    early_feat_channels = H[\'early_feat_channels\']\n    early_feat = early_feat[:, :, :, :early_feat_channels]\n\n    if H[\'deconv\']:\n        size = 3\n        stride = 2\n        pool_size = 5\n\n        with tf.variable_scope(""deconv"", reuse=reuse):\n            w = tf.get_variable(\'conv_pool_w\', shape=[size, size, H[\'later_feat_channels\'], H[\'later_feat_channels\']],\n                                initializer=tf.random_normal_initializer(stddev=0.01))\n            cnn_s = tf.nn.conv2d(cnn, w, strides=[1, stride, stride, 1], padding=\'SAME\')\n            cnn_s_pool = tf.nn.avg_pool(cnn_s[:, :, :, :256], ksize=[1, pool_size, pool_size, 1],\n                                        strides=[1, 1, 1, 1], padding=\'SAME\')\n\n            cnn_s_with_pool = tf_concat(3, [cnn_s_pool, cnn_s[:, :, :, 256:]])\n            cnn_deconv = deconv(cnn_s_with_pool, output_shape=[H[\'batch_size\'], H[\'grid_height\'], H[\'grid_width\'], 256], channels=[H[\'later_feat_channels\'], 256])\n            cnn = tf_concat(3, (cnn_deconv, cnn[:, :, :, 256:]))\n\n    elif H[\'avg_pool_size\'] > 1:\n        pool_size = H[\'avg_pool_size\']\n        cnn1 = cnn[:, :, :, :700]\n        cnn2 = cnn[:, :, :, 700:]\n        cnn2 = tf.nn.avg_pool(cnn2, ksize=[1, pool_size, pool_size, 1],\n                              strides=[1, 1, 1, 1], padding=\'SAME\')\n        cnn = tf_concat(3, [cnn1, cnn2])\n\n    cnn = tf.reshape(cnn,\n                     [H[\'batch_size\'] * H[\'grid_width\'] * H[\'grid_height\'], H[\'later_feat_channels\']])\n    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n    with tf.variable_scope(\'decoder\', reuse=reuse, initializer=initializer):\n        scale_down = 0.01\n        lstm_input = tf.reshape(cnn * scale_down, (H[\'batch_size\'] * grid_size, H[\'later_feat_channels\']))\n        if H[\'use_lstm\']:\n            lstm_outputs = build_lstm_inner(H, lstm_input)\n        else:\n            lstm_outputs = build_overfeat_inner(H, lstm_input)\n\n        pred_boxes = []\n        pred_logits = []\n        for k in range(H[\'rnn_len\']):\n            output = lstm_outputs[k]\n            if phase == \'train\':\n                output = tf.nn.dropout(output, 0.5)\n            box_weights = tf.get_variable(\'box_ip%d\' % k,\n                                          shape=(H[\'lstm_size\'], 4))\n            conf_weights = tf.get_variable(\'conf_ip%d\' % k,\n                                           shape=(H[\'lstm_size\'], H[\'num_classes\']))\n\n            pred_boxes_step = tf.reshape(tf.matmul(output, box_weights) * 50,\n                                         [outer_size, 1, 4])\n\n            pred_boxes.append(pred_boxes_step)\n            pred_logits.append(tf.reshape(tf.matmul(output, conf_weights),\n                                         [outer_size, 1, H[\'num_classes\']]))\n\n        pred_boxes = tf_concat(1, pred_boxes)\n        pred_logits = tf_concat(1, pred_logits)\n        pred_logits_squash = tf.reshape(pred_logits,\n                                        [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n        pred_confidences_squash = tf.nn.softmax(pred_logits_squash)\n        pred_confidences = tf.reshape(pred_confidences_squash,\n                                      [outer_size, H[\'rnn_len\'], H[\'num_classes\']])\n\n        if H[\'use_rezoom\']:\n            pred_confs_deltas = []\n            pred_boxes_deltas = []\n            w_offsets = H[\'rezoom_w_coords\']\n            h_offsets = H[\'rezoom_h_coords\']\n            num_offsets = len(w_offsets) * len(h_offsets)\n            rezoom_features = rezoom(H, pred_boxes, early_feat, early_feat_channels, w_offsets, h_offsets)\n            if phase == \'train\':\n                rezoom_features = tf.nn.dropout(rezoom_features, 0.5)\n            for k in range(H[\'rnn_len\']):\n                delta_features = tf_concat(1, [lstm_outputs[k], rezoom_features[:, k, :] / 1000.])\n                dim = 128\n                delta_weights1 = tf.get_variable(\n                                    \'delta_ip1%d\' % k,\n                                    shape=[H[\'lstm_size\'] + early_feat_channels * num_offsets, dim])\n                # TODO: add dropout here ?\n                ip1 = tf.nn.relu(tf.matmul(delta_features, delta_weights1))\n                if phase == \'train\':\n                    ip1 = tf.nn.dropout(ip1, 0.5)\n                delta_confs_weights = tf.get_variable(\n                                    \'delta_ip2%d\' % k,\n                                    shape=[dim, H[\'num_classes\']])\n                if H[\'reregress\']:\n                    delta_boxes_weights = tf.get_variable(\n                                        \'delta_ip_boxes%d\' % k,\n                                        shape=[dim, 4])\n                    pred_boxes_deltas.append(tf.reshape(tf.matmul(ip1, delta_boxes_weights) * 5,\n                                                        [outer_size, 1, 4]))\n                scale = H.get(\'rezoom_conf_scale\', 50)\n                pred_confs_deltas.append(tf.reshape(tf.matmul(ip1, delta_confs_weights) * scale,\n                                                    [outer_size, 1, H[\'num_classes\']]))\n            pred_confs_deltas = tf_concat(1, pred_confs_deltas)\n            if H[\'reregress\']:\n                pred_boxes_deltas = tf_concat(1, pred_boxes_deltas)\n            return pred_boxes, pred_logits, pred_confidences, pred_confs_deltas, pred_boxes_deltas\n\n    return pred_boxes, pred_logits, pred_confidences\n\ndef build_forward_backward(H, x, phase, boxes, flags):\n    \'\'\'\n    Call build_forward() and then setup the loss functions\n    \'\'\'\n\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n    reuse = {\'train\': None, \'test\': True}[phase]\n    if H[\'use_rezoom\']:\n        (pred_boxes, pred_logits,\n         pred_confidences, pred_confs_deltas, pred_boxes_deltas) = build_forward(H, x, phase, reuse)\n    else:\n        pred_boxes, pred_logits, pred_confidences = build_forward(H, x, phase, reuse)\n    with tf.variable_scope(\'decoder\', reuse={\'train\': None, \'test\': True}[phase]):\n        outer_boxes = tf.reshape(boxes, [outer_size, H[\'rnn_len\'], 4])\n        outer_flags = tf.cast(tf.reshape(flags, [outer_size, H[\'rnn_len\']]), \'int32\')\n        if H[\'use_lstm\']:\n            hungarian_module = tf.load_op_library(\'utils/hungarian/hungarian.so\')\n            assignments, classes, perm_truth, pred_mask = (\n                hungarian_module.hungarian(pred_boxes, outer_boxes, outer_flags, H[\'solver\'][\'hungarian_iou\']))\n        else:\n            classes = tf.reshape(flags, (outer_size, 1))\n            perm_truth = tf.reshape(outer_boxes, (outer_size, 1, 4))\n            pred_mask = tf.reshape(tf.cast(tf.greater(classes, 0), \'float32\'), (outer_size, 1, 1))\n        true_classes = tf.reshape(tf.cast(tf.greater(classes, 0), \'int64\'),\n                                  [outer_size * H[\'rnn_len\']])\n        pred_logit_r = tf.reshape(pred_logits,\n                                  [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n        confidences_loss = (tf.reduce_sum(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred_logit_r, labels=true_classes))\n            ) / outer_size * H[\'solver\'][\'head_weights\'][0]\n        residual = tf.reshape(perm_truth - pred_boxes * pred_mask,\n                              [outer_size, H[\'rnn_len\'], 4])\n        boxes_loss = tf.reduce_sum(tf.abs(residual)) / outer_size * H[\'solver\'][\'head_weights\'][1]\n        if H[\'use_rezoom\']:\n            if H[\'rezoom_change_loss\'] == \'center\':\n                error = (perm_truth[:, :, 0:2] - pred_boxes[:, :, 0:2]) / tf.maximum(perm_truth[:, :, 2:4], 1.)\n                square_error = tf.reduce_sum(tf.square(error), 2)\n                inside = tf.reshape(tf.to_int64(tf.logical_and(tf.less(square_error, 0.2**2), tf.greater(classes, 0))), [-1])\n            elif H[\'rezoom_change_loss\'] == \'iou\':\n                iou = train_utils.iou(train_utils.to_x1y1x2y2(tf.reshape(pred_boxes, [-1, 4])),\n                                      train_utils.to_x1y1x2y2(tf.reshape(perm_truth, [-1, 4])))\n                inside = tf.reshape(tf.to_int64(tf.greater(iou, 0.5)), [-1])\n            else:\n                assert H[\'rezoom_change_loss\'] == False\n                inside = tf.reshape(tf.to_int64((tf.greater(classes, 0))), [-1])\n            new_confs = tf.reshape(pred_confs_deltas, [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n            delta_confs_loss = tf.reduce_sum(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=new_confs, labels=inside)) / outer_size * H[\'solver\'][\'head_weights\'][0] * 0.1\n\n            pred_logits_squash = tf.reshape(new_confs,\n                                            [outer_size * H[\'rnn_len\'], H[\'num_classes\']])\n            pred_confidences_squash = tf.nn.softmax(pred_logits_squash)\n            pred_confidences = tf.reshape(pred_confidences_squash,\n                                      [outer_size, H[\'rnn_len\'], H[\'num_classes\']])\n            loss = confidences_loss + boxes_loss + delta_confs_loss\n            if H[\'reregress\']:\n                delta_residual = tf.reshape(perm_truth - (pred_boxes + pred_boxes_deltas) * pred_mask,\n                                            [outer_size, H[\'rnn_len\'], 4])\n                delta_boxes_loss = (tf.reduce_sum(tf.minimum(tf.square(delta_residual), 10. ** 2)) /\n                               outer_size * H[\'solver\'][\'head_weights\'][1] * 0.03)\n                boxes_loss = delta_boxes_loss\n\n                tf.summary.histogram(phase + \'/delta_hist0_x\', pred_boxes_deltas[:, 0, 0])\n                tf.summary.histogram(phase + \'/delta_hist0_y\', pred_boxes_deltas[:, 0, 1])\n                tf.summary.histogram(phase + \'/delta_hist0_w\', pred_boxes_deltas[:, 0, 2])\n                tf.summary.histogram(phase + \'/delta_hist0_h\', pred_boxes_deltas[:, 0, 3])\n                loss += delta_boxes_loss\n        else:\n            loss = confidences_loss + boxes_loss\n\n    return pred_boxes, pred_confidences, loss, confidences_loss, boxes_loss\n\ndef build(H, q):\n    \'\'\'\n    Build full model for training, including forward / backward passes,\n    optimizers, and summary statistics.\n    \'\'\'\n    arch = H\n    solver = H[""solver""]\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(solver.get(\'gpu\', \'\'))\n\n    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n    gpu_options = tf.GPUOptions()\n    config = tf.ConfigProto(gpu_options=gpu_options)\n\n    learning_rate = tf.placeholder(tf.float32)\n    if solver[\'opt\'] == \'RMS\':\n        opt = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n                                        decay=0.9, epsilon=solver[\'epsilon\'])\n    elif solver[\'opt\'] == \'Adam\':\n        opt = tf.train.AdamOptimizer(learning_rate=learning_rate,\n                                        epsilon=solver[\'epsilon\'])\n    elif solver[\'opt\'] == \'SGD\':\n        opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    else:\n        raise ValueError(\'Unrecognized opt type\')\n    loss, accuracy, confidences_loss, boxes_loss = {}, {}, {}, {}\n    for phase in [\'train\', \'test\']:\n        # generate predictions and losses from forward pass\n        x, confidences, boxes = q[phase].dequeue_many(arch[\'batch_size\'])\n        flags = tf.argmax(confidences, 3)\n\n\n        grid_size = H[\'grid_width\'] * H[\'grid_height\']\n\n        (pred_boxes, pred_confidences,\n         loss[phase], confidences_loss[phase],\n         boxes_loss[phase]) = build_forward_backward(H, x, phase, boxes, flags)\n        pred_confidences_r = tf.reshape(pred_confidences, [H[\'batch_size\'], grid_size, H[\'rnn_len\'], arch[\'num_classes\']])\n        pred_boxes_r = tf.reshape(pred_boxes, [H[\'batch_size\'], grid_size, H[\'rnn_len\'], 4])\n\n\n        # Set up summary operations for tensorboard\n        a = tf.equal(tf.argmax(confidences[:, :, 0, :], 2), tf.argmax(pred_confidences_r[:, :, 0, :], 2))\n        accuracy[phase] = tf.reduce_mean(tf.cast(a, \'float32\'), name=phase+\'/accuracy\')\n\n        if phase == \'train\':\n            global_step = tf.Variable(0, trainable=False)\n\n            tvars = tf.trainable_variables()\n            if H[\'clip_norm\'] <= 0:\n                grads = tf.gradients(loss[\'train\'], tvars)\n            else:\n                grads, norm = tf.clip_by_global_norm(tf.gradients(loss[\'train\'], tvars), H[\'clip_norm\'])\n            train_op = opt.apply_gradients(zip(grads, tvars), global_step=global_step)\n        elif phase == \'test\':\n            moving_avg = tf.train.ExponentialMovingAverage(0.95)\n            smooth_op = moving_avg.apply([accuracy[\'train\'], accuracy[\'test\'],\n                                          confidences_loss[\'train\'], boxes_loss[\'train\'],\n                                          confidences_loss[\'test\'], boxes_loss[\'test\'],\n                                          ])\n\n            for p in [\'train\', \'test\']:\n                tf.summary.scalar(\'%s/accuracy\' % p, accuracy[p])\n                tf.summary.scalar(\'%s/accuracy/smooth\' % p, moving_avg.average(accuracy[p]))\n                tf.summary.scalar(""%s/confidences_loss"" % p, confidences_loss[p])\n                tf.summary.scalar(""%s/confidences_loss/smooth"" % p,\n                    moving_avg.average(confidences_loss[p]))\n                tf.summary.scalar(""%s/regression_loss"" % p, boxes_loss[p])\n                tf.summary.scalar(""%s/regression_loss/smooth"" % p,\n                    moving_avg.average(boxes_loss[p]))\n\n        if phase == \'test\':\n            test_image = x\n            # show ground truth to verify labels are correct\n            test_true_confidences = confidences[0, :, :, :]\n            test_true_boxes = boxes[0, :, :, :]\n\n            # show predictions to visualize training progress\n            test_pred_confidences = pred_confidences_r[0, :, :, :]\n            test_pred_boxes = pred_boxes_r[0, :, :, :]\n\n            def log_image(np_img, np_confidences, np_boxes, np_global_step, pred_or_true):\n\n                merged = train_utils.add_rectangles(H, np_img, np_confidences, np_boxes,\n                                                    use_stitching=True,\n                                                    rnn_len=H[\'rnn_len\'])[0]\n\n                num_images = 10\n                img_path = os.path.join(H[\'save_dir\'], \'%s_%s.jpg\' % ((np_global_step / H[\'logging\'][\'display_iter\']) % num_images, pred_or_true))\n                misc.imsave(img_path, merged)\n                return merged\n\n            pred_log_img = tf.py_func(log_image,\n                                      [test_image, test_pred_confidences, test_pred_boxes, global_step, \'pred\'],\n                                      [tf.float32])\n            true_log_img = tf.py_func(log_image,\n                                      [test_image, test_true_confidences, test_true_boxes, global_step, \'true\'],\n                                      [tf.float32])\n            tf.summary.image(phase + \'/pred_boxes\', pred_log_img, max_outputs=10)\n            tf.summary.image(phase + \'/true_boxes\', true_log_img, max_outputs=10)\n\n    summary_op = tf.summary.merge_all()\n\n    return (config, loss, accuracy, summary_op, train_op,\n            smooth_op, global_step, learning_rate)\n\n\ndef train(H, test_images):\n    \'\'\'\n    Setup computation graph, run 2 prefetch data threads, and then run the main loop\n    \'\'\'\n\n    if not os.path.exists(H[\'save_dir\']): os.makedirs(H[\'save_dir\'])\n\n    ckpt_file = H[\'save_dir\'] + \'/save.ckpt\'\n    with open(H[\'save_dir\'] + \'/hypes.json\', \'w\') as f:\n        json.dump(H, f, indent=4)\n\n    x_in = tf.placeholder(tf.float32)\n    confs_in = tf.placeholder(tf.float32)\n    boxes_in = tf.placeholder(tf.float32)\n    q = {}\n    enqueue_op = {}\n    for phase in [\'train\', \'test\']:\n        dtypes = [tf.float32, tf.float32, tf.float32]\n        grid_size = H[\'grid_width\'] * H[\'grid_height\']\n        shapes = (\n            [H[\'image_height\'], H[\'image_width\'], 3],\n            [grid_size, H[\'rnn_len\'], H[\'num_classes\']],\n            [grid_size, H[\'rnn_len\'], 4],\n            )\n        q[phase] = tf.FIFOQueue(capacity=30, dtypes=dtypes, shapes=shapes)\n        enqueue_op[phase] = q[phase].enqueue((x_in, confs_in, boxes_in))\n\n    def make_feed(d):\n        return {x_in: d[\'image\'], confs_in: d[\'confs\'], boxes_in: d[\'boxes\'],\n                learning_rate: H[\'solver\'][\'learning_rate\']}\n\n    def thread_loop(sess, enqueue_op, phase, gen):\n        for d in gen:\n            sess.run(enqueue_op[phase], feed_dict=make_feed(d))\n\n    (config, loss, accuracy, summary_op, train_op,\n     smooth_op, global_step, learning_rate) = build(H, q)\n\n    saver = tf.train.Saver(max_to_keep=None)\n    writer = tf.summary.FileWriter(\n        logdir=H[\'save_dir\'],\n        flush_secs=10\n    )\n\n    with tf.Session(config=config) as sess:\n        tf.train.start_queue_runners(sess=sess)\n        for phase in [\'train\', \'test\']:\n            # enqueue once manually to avoid thread start delay\n            gen = train_utils.load_data_gen(H, phase, jitter=H[\'solver\'][\'use_jitter\'])\n            d = gen.next()\n            sess.run(enqueue_op[phase], feed_dict=make_feed(d))\n            t = threading.Thread(target=thread_loop,\n                                 args=(sess, enqueue_op, phase, gen))\n            t.daemon = True\n            t.start()\n\n        tf.set_random_seed(H[\'solver\'][\'rnd_seed\'])\n        sess.run(tf.initialize_all_variables())\n        writer.add_graph(sess.graph)\n        weights_str = H[\'solver\'][\'weights\']\n        if len(weights_str) > 0:\n            print(\'Restoring from: %s\' % weights_str)\n            saver.restore(sess, weights_str)\n        else:\n            init_fn = slim.assign_from_checkpoint_fn(\n                  \'%s/data/%s\' % (os.path.dirname(os.path.realpath(__file__)), H[\'slim_ckpt\']),\n                  [x for x in tf.all_variables() if x.name.startswith(H[\'slim_basename\']) and H[\'solver\'][\'opt\'] not in x.name])\n            #init_fn = slim.assign_from_checkpoint_fn(\n                  #\'%s/data/inception_v1.ckpt\' % os.path.dirname(os.path.realpath(__file__)),\n                  #[x for x in tf.all_variables() if x.name.startswith(\'InceptionV1\') and not H[\'solver\'][\'opt\'] in x.name])\n            init_fn(sess)\n\n        # train model for N iterations\n        start = time.time()\n        max_iter = H[\'solver\'].get(\'max_iter\', 10000000)\n        for i in xrange(max_iter):\n            display_iter = H[\'logging\'][\'display_iter\']\n            adjusted_lr = (H[\'solver\'][\'learning_rate\'] *\n                           0.5 ** max(0, (i / H[\'solver\'][\'learning_rate_step\']) - 2))\n            lr_feed = {learning_rate: adjusted_lr}\n\n            if i % display_iter != 0:\n                # train network\n                batch_loss_train, _ = sess.run([loss[\'train\'], train_op], feed_dict=lr_feed)\n            else:\n                # test network every N iterations; log additional info\n                if i > 0:\n                    dt = (time.time() - start) / (H[\'batch_size\'] * display_iter)\n                start = time.time()\n                (train_loss, test_accuracy, summary_str,\n                    _, _) = sess.run([loss[\'train\'], accuracy[\'test\'],\n                                      summary_op, train_op, smooth_op,\n                                     ], feed_dict=lr_feed)\n                writer.add_summary(summary_str, global_step=global_step.eval())\n                print_str = string.join([\n                    \'Step: %d\',\n                    \'lr: %f\',\n                    \'Train Loss: %.2f\',\n                    \'Softmax Test Accuracy: %.1f%%\',\n                    \'Time/image (ms): %.1f\'\n                ], \', \')\n                print(print_str %\n                      (i, adjusted_lr, train_loss,\n                       test_accuracy * 100, dt * 1000 if i > 0 else 0))\n\n            if global_step.eval() % H[\'logging\'][\'save_iter\'] == 0 or global_step.eval() == max_iter - 1:\n                saver.save(sess, ckpt_file, global_step=global_step)\n\n\ndef main():\n    \'\'\'\n    Parse command line arguments and return the hyperparameter dictionary H.\n    H first loads the --hypes hypes.json file and is further updated with\n    additional arguments as needed.\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--weights\', default=None, type=str)\n    parser.add_argument(\'--gpu\', default=None, type=int)\n    parser.add_argument(\'--hypes\', required=True, type=str)\n    parser.add_argument(\'--logdir\', default=\'output\', type=str)\n    args = parser.parse_args()\n    with open(args.hypes, \'r\') as f:\n        H = json.load(f)\n    if args.gpu is not None:\n        H[\'solver\'][\'gpu\'] = args.gpu\n    if len(H.get(\'exp_name\', \'\')) == 0:\n        H[\'exp_name\'] = args.hypes.split(\'/\')[-1].replace(\'.json\', \'\')\n    H[\'save_dir\'] = args.logdir + \'/%s_%s\' % (H[\'exp_name\'],\n        datetime.datetime.now().strftime(\'%Y_%m_%d_%H.%M\'))\n    if args.weights is not None:\n        H[\'solver\'][\'weights\'] = args.weights\n    train(H, test_images=[])\n\nif __name__ == \'__main__\':\n    main()'"
linux/tensorbox/pymouse/__init__.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nThe goal of PyMouse is to have a cross-platform way to control the mouse.\nPyMouse should work on Windows, Mac and any Unix that has xlib.\n\nPyMouse is a part of PyUserInput, along with PyKeyboard, for more information\nabout this project, see:\nhttp://github.com/SavinaRoja/PyUserInput\n\nPyMouse was originally developed by Pepijn de Vos. For the original repository,\nsee:\nhttps://github.com/pepijndevos/PyMouse\n""""""\n\nimport sys\n\nif sys.platform.startswith(\'java\'):\n    from .java_ import PyMouse\n\nelif sys.platform == \'darwin\':\n    from .mac import PyMouse, PyMouseEvent\n\nelif sys.platform == \'win32\':\n    from .windows import PyMouse, PyMouseEvent\n\nelse:\n    from .x11 import PyMouse, PyMouseEvent\n\n'"
linux/tensorbox/pymouse/base.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nThe goal of PyMouse is to have a cross-platform way to control the mouse.\nPyMouse should work on Windows, Mac and any Unix that has xlib.\n\nAs the base file, this provides a rough operational model along with the\nframework to be extended by each platform.\n""""""\n\nfrom threading import Thread\n\n\nclass ScrollSupportError(Exception):\n    pass\n\n\nclass PyMouseMeta(object):\n\n    def press(self, x, y, button=1):\n        """"""\n        Press the mouse on a given x, y and button.\n        Button is defined as 1 = left, 2 = right, 3 = middle.\n        """"""\n\n        raise NotImplementedError\n\n    def release(self, x, y, button=1):\n        """"""\n        Release the mouse on a given x, y and button.\n        Button is defined as 1 = left, 2 = right, 3 = middle.\n        """"""\n\n        raise NotImplementedError\n\n    def click(self, x, y, button=1, n=1):\n        """"""\n        Click a mouse button n times on a given x, y.\n        Button is defined as 1 = left, 2 = right, 3 = middle.\n        """"""\n\n        for i in range(n):\n            self.press(x, y, button)\n            self.release(x, y, button)\n\n    def scroll(self, vertical=None, horizontal=None, depth=None):\n        """"""\n        Generates mouse scrolling events in up to three dimensions: vertical,\n        horizontal, and depth (Mac-only). Values for these arguments may be\n        positive or negative numbers (float or int). Refer to the following:\n            Vertical: + Up, - Down\n            Horizontal: + Right, - Left\n            Depth: + Rise (out of display), - Dive (towards display)\n\n        Dynamic scrolling, which is used Windows and Mac platforms, is not\n        implemented at this time due to an inability to test Mac code. The\n        events generated by this code will thus be discrete units of scrolling\n        ""lines"". The user is advised to take care at all times with scrolling\n        automation as scrolling event consumption is relatively un-standardized.\n\n        Float values will be coerced to integers.\n        """"""\n\n        raise NotImplementedError\n\n    def move(self, x, y):\n        """"""Move the mouse to a given x and y""""""\n\n        raise NotImplementedError\n\n    def drag(self, x, y):\n        """"""Drag the mouse to a given x and y.\n        A Drag is a Move where the mouse key is held down.""""""\n\n        raise NotImplementedError\n\n    def position(self):\n        """"""\n        Get the current mouse position in pixels.\n        Returns a tuple of 2 integers\n        """"""\n\n        raise NotImplementedError\n\n    def screen_size(self):\n        """"""\n        Get the current screen size in pixels.\n        Returns a tuple of 2 integers\n        """"""\n\n        raise NotImplementedError\n\n\nclass PyMouseEventMeta(Thread):\n    def __init__(self, capture=False, capture_move=False):\n        Thread.__init__(self)\n        self.daemon = True\n        self.capture = capture\n        self.capture_move = capture_move\n        self.state = True\n\n    def stop(self):\n        self.state = False\n\n    def click(self, x, y, button, press):\n        """"""Subclass this method with your click event handler""""""\n        pass\n\n    def move(self, x, y):\n        """"""Subclass this method with your move event handler""""""\n        pass\n\n    def scroll(self, x, y, vertical, horizontal):\n        """"""\n        Subclass this method with your scroll event handler\n            Vertical: + Up, - Down\n            Horizontal: + Right, - Left\n        """"""\n        pass\n'"
linux/tensorbox/pymouse/java_.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom java.awt import Robot, Toolkit\nfrom java.awt.event import InputEvent\nfrom java.awt.MouseInfo import getPointerInfo\nfrom .base import PyMouseMeta\n\nr = Robot()\n\nclass PyMouse(PyMouseMeta):\n    def press(self, x, y, button = 1):\n        button_list = [None, InputEvent.BUTTON1_MASK, InputEvent.BUTTON3_MASK, InputEvent.BUTTON2_MASK]\n        self.move(x, y)\n        r.mousePress(button_list[button])\n\n    def release(self, x, y, button = 1):\n        button_list = [None, InputEvent.BUTTON1_MASK, InputEvent.BUTTON3_MASK, InputEvent.BUTTON2_MASK]\n        self.move(x, y)\n        r.mouseRelease(button_list[button])\n    \n    def move(self, x, y):\n        r.mouseMove(x, y)\n\n    def position(self):\n        loc = getPointerInfo().getLocation()\n        return loc.getX, loc.getY\n\n    def screen_size(self):\n        dim = Toolkit.getDefaultToolkit().getScreenSize()\n        return dim.getWidth(), dim.getHeight()\n'"
linux/tensorbox/pymouse/mac.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport Quartz\nfrom AppKit import NSEvent, NSScreen\nfrom .base import PyMouseMeta, PyMouseEventMeta\n\npressID = [None, Quartz.kCGEventLeftMouseDown,\n           Quartz.kCGEventRightMouseDown, Quartz.kCGEventOtherMouseDown]\nreleaseID = [None, Quartz.kCGEventLeftMouseUp,\n             Quartz.kCGEventRightMouseUp, Quartz.kCGEventOtherMouseUp]\n\n\nclass PyMouse(PyMouseMeta):\n\n    def press(self, x, y, button=1):\n        event = Quartz.CGEventCreateMouseEvent(None,\n                                        pressID[button],\n                                        (x, y),\n                                        button - 1)\n        Quartz.CGEventPost(Quartz.kCGHIDEventTap, event)\n\n    def release(self, x, y, button=1):\n        event = Quartz.CGEventCreateMouseEvent(None,\n                                        releaseID[button],\n                                        (x, y),\n                                        button - 1)\n        Quartz.CGEventPost(Quartz.kCGHIDEventTap, event)\n\n    def move(self, x, y):\n        move = Quartz.CGEventCreateMouseEvent(None, Quartz.kCGEventMouseMoved, (x, y), 0)\n        Quartz.CGEventPost(Quartz.kCGHIDEventTap, move)\n\n    def drag(self, x, y):\n        drag = Quartz.CGEventCreateMouseEvent(None, Quartz.kCGEventLeftMouseDragged, (x, y), 0)\n        Quartz.CGEventPost(Quartz.kCGHIDEventTap, drag)\n\n    def position(self):\n        loc = NSEvent.mouseLocation()\n        return loc.x, Quartz.CGDisplayPixelsHigh(0) - loc.y\n\n    def screen_size(self):\n        return NSScreen.mainScreen().frame().size.width, NSScreen.mainScreen().frame().size.height\n\n    def scroll(self, vertical=None, horizontal=None, depth=None):\n        #Local submethod for generating Mac scroll events in one axis at a time\n        def scroll_event(y_move=0, x_move=0, z_move=0, n=1):\n            for _ in range(abs(n)):\n                scrollWheelEvent = Quartz.CGEventCreateScrollWheelEvent(\n                    None,  # No source\n                    Quartz.kCGScrollEventUnitLine,  # Unit of measurement is lines\n                    3,  # Number of wheels(dimensions)\n                    y_move,\n                    x_move,\n                    z_move)\n                Quartz.CGEventPost(Quartz.kCGHIDEventTap, scrollWheelEvent)\n\n        #Execute vertical then horizontal then depth scrolling events\n        if vertical is not None:\n            vertical = int(vertical)\n            if vertical == 0:   # Do nothing with 0 distance\n                pass\n            elif vertical > 0:  # Scroll up if positive\n                scroll_event(y_move=1, n=vertical)\n            else:  # Scroll down if negative\n                scroll_event(y_move=-1, n=abs(vertical))\n        if horizontal is not None:\n            horizontal = int(horizontal)\n            if horizontal == 0:  # Do nothing with 0 distance\n                pass\n            elif horizontal > 0:  # Scroll right if positive\n                scroll_event(x_move=1, n=horizontal)\n            else:  # Scroll left if negative\n                scroll_event(x_move=-1, n=abs(horizontal))\n        if depth is not None:\n            depth = int(depth)\n            if depth == 0:  # Do nothing with 0 distance\n                pass\n            elif vertical > 0:  # Scroll ""out"" if positive\n                scroll_event(z_move=1, n=depth)\n            else:  # Scroll ""in"" if negative\n                scroll_event(z_move=-1, n=abs(depth))\n\n\nclass PyMouseEvent(PyMouseEventMeta):\n    def run(self):\n        tap = Quartz.CGEventTapCreate(\n            Quartz.kCGSessionEventTap,\n            Quartz.kCGHeadInsertEventTap,\n            Quartz.kCGEventTapOptionDefault,\n            Quartz.CGEventMaskBit(Quartz.kCGEventMouseMoved) |\n            Quartz.CGEventMaskBit(Quartz.kCGEventLeftMouseDown) |\n            Quartz.CGEventMaskBit(Quartz.kCGEventLeftMouseUp) |\n            Quartz.CGEventMaskBit(Quartz.kCGEventRightMouseDown) |\n            Quartz.CGEventMaskBit(Quartz.kCGEventRightMouseUp) |\n            Quartz.CGEventMaskBit(Quartz.kCGEventOtherMouseDown) |\n            Quartz.CGEventMaskBit(Quartz.kCGEventOtherMouseUp),\n            self.handler,\n            None)\n\n        loopsource = Quartz.CFMachPortCreateRunLoopSource(None, tap, 0)\n        loop = Quartz.CFRunLoopGetCurrent()\n        Quartz.CFRunLoopAddSource(loop, loopsource, Quartz.kCFRunLoopDefaultMode)\n        Quartz.CGEventTapEnable(tap, True)\n\n        while self.state:\n            Quartz.CFRunLoopRunInMode(Quartz.kCFRunLoopDefaultMode, 5, False)\n\n    def handler(self, proxy, type, event, refcon):\n        (x, y) = Quartz.CGEventGetLocation(event)\n        if type in pressID:\n            self.click(x, y, pressID.index(type), True)\n        elif type in releaseID:\n            self.click(x, y, releaseID.index(type), False)\n        else:\n            self.move(x, y)\n\n        if self.capture:\n            Quartz.CGEventSetType(event, Quartz.kCGEventNull)\n\n        return event\n'"
linux/tensorbox/pymouse/mir.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n'"
linux/tensorbox/pymouse/wayland.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n'"
linux/tensorbox/pymouse/windows.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom ctypes import *\nimport win32api\nimport win32con\nfrom .base import PyMouseMeta, PyMouseEventMeta, ScrollSupportError\nimport pythoncom\nfrom time import sleep\n\nclass POINT(Structure):\n    _fields_ = [(""x"", c_ulong),\n                (""y"", c_ulong)]\n\nclass PyMouse(PyMouseMeta):\n    """"""MOUSEEVENTF_(button and action) constants\n    are defined at win32con, buttonAction is that value""""""\n\n    def press(self, x, y, button=1):\n        buttonAction = 2 ** ((2 * button) - 1)\n        self.move(x, y)\n        win32api.mouse_event(buttonAction, x, y)\n\n    def release(self, x, y, button=1):\n        buttonAction = 2 ** ((2 * button))\n        self.move(x, y)\n        win32api.mouse_event(buttonAction, x, y)\n\n    def scroll(self, vertical=None, horizontal=None, depth=None):\n\n        #Windows supports only vertical and horizontal scrolling\n        if depth is not None:\n            raise ScrollSupportError(\'PyMouse cannot support depth-scrolling \\\nin Windows. This feature is only available on Mac.\')\n\n        #Execute vertical then horizontal scrolling events\n        if vertical is not None:\n            vertical = int(vertical)\n            if vertical == 0:  # Do nothing with 0 distance\n                pass\n            elif vertical > 0:  # Scroll up if positive\n                for _ in range(vertical):\n                    win32api.mouse_event(0x0800, 0, 0, 120, 0)\n            else:  # Scroll down if negative\n                for _ in range(abs(vertical)):\n                    win32api.mouse_event(0x0800, 0, 0, -120, 0)\n        if horizontal is not None:\n            horizontal = int(horizontal)\n            if horizontal == 0:  # Do nothing with 0 distance\n                pass\n            elif horizontal > 0:  # Scroll right if positive\n                for _ in range(horizontal):\n                    win32api.mouse_event(0x01000, 0, 0, 120, 0)\n            else:  # Scroll left if negative\n                for _ in range(abs(horizontal)):\n                    win32api.mouse_event(0x01000, 0, 0, -120, 0)\n\n    def move(self, x, y):\n        windll.user32.SetCursorPos(x, y)\n\n    def drag(self, x, y):\n        self.press(*self.position())\n        #self.move(x, y)\n        self.release(x, y)\n\n    def position(self):\n        pt = POINT()\n        windll.user32.GetCursorPos(byref(pt))\n        return pt.x, pt.y\n\n    def screen_size(self):\n        if windll.user32.GetSystemMetrics(80) == 1:\n            width = windll.user32.GetSystemMetrics(0)\n            height = windll.user32.GetSystemMetrics(1)\n        else:\n            width = windll.user32.GetSystemMetrics(78)\n            height = windll.user32.GetSystemMetrics(79)\n        return width, height\n\nclass PyMouseEvent(PyMouseEventMeta):\n    def __init__(self, capture=False, capture_move=False):\n        import pyHook\n\n        PyMouseEventMeta.__init__(self, capture=capture, capture_move=capture_move)\n        self.hm = pyHook.HookManager()\n\n    def run(self):\n        self.hm.MouseAll = self._action\n        self.hm.HookMouse()\n        while self.state:\n            sleep(0.01)\n            pythoncom.PumpWaitingMessages()\n\n    def stop(self):\n        self.hm.UnhookMouse()\n        self.state = False\n\n    def _action(self, event):\n        import pyHook\n        x, y = event.Position\n\n        if event.Message == pyHook.HookConstants.WM_MOUSEMOVE:\n            self.move(x,y)\n            return not self.capture_move\n\n        elif event.Message == pyHook.HookConstants.WM_LBUTTONDOWN:\n            self.click(x, y, 1, True)\n        elif event.Message == pyHook.HookConstants.WM_LBUTTONUP:\n            self.click(x, y, 1, False)\n        elif event.Message == pyHook.HookConstants.WM_RBUTTONDOWN:\n            self.click(x, y, 2, True)\n        elif event.Message == pyHook.HookConstants.WM_RBUTTONUP:\n            self.click(x, y, 2, False)\n        elif event.Message == pyHook.HookConstants.WM_MBUTTONDOWN:\n            self.click(x, y, 3, True)\n        elif event.Message == pyHook.HookConstants.WM_MBUTTONUP:\n            self.click(x, y, 3, False)\n\n        elif event.Message == pyHook.HookConstants.WM_MOUSEWHEEL:\n            # event.Wheel is -1 when scrolling down, 1 when scrolling up\n            self.scroll(x, y, event.Wheel, 0)\n\n        return not self.capture\n'"
linux/tensorbox/pymouse/x11.py,0,"b'#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom Xlib.display import Display\nfrom Xlib import X\nfrom Xlib.ext.xtest import fake_input\nfrom Xlib.ext import record\nfrom Xlib.protocol import rq\n\nfrom .base import PyMouseMeta, PyMouseEventMeta, ScrollSupportError\n\n\nclass X11Error(Exception):\n    """"""An error that is thrown at the end of a code block managed by a\n    :func:`display_manager` if an *X11* error occurred.\n    """"""\n    pass\n\n\ndef display_manager(display):\n    """"""Traps *X* errors and raises an :class:``X11Error`` at the end if any\n    error occurred.\n\n    This handler also ensures that the :class:`Xlib.display.Display` being\n    managed is sync\'d.\n\n    :param Xlib.display.Display display: The *X* display.\n\n    :return: the display\n    :rtype: Xlib.display.Display\n    """"""\n    from contextlib import contextmanager\n\n    @contextmanager\n    def manager():\n        errors = []\n\n        def handler(*args):\n            errors.append(args)\n\n        old_handler = display.set_error_handler(handler)\n        yield display\n        display.sync()\n        display.set_error_handler(old_handler)\n        if errors:\n            raise X11Error(errors)\n\n    return manager()\n\n\ndef translate_button_code(button):\n    # In X11, the button numbers are:\n    #  leftclick=1, middleclick=2, rightclick=3\n    #  For the purposes of the cross-platform interface of PyMouse, we\n    #  invert the button number values of the right and middle buttons\n    if button in [1, 2, 3]:\n        return (None, 1, 3, 2)[button]\n    else:\n        return button\n\ndef button_code_to_scroll_direction(button):\n    # scrollup=4, scrolldown=5, scrollleft=6, scrollright=7\n    return {\n        4: (1, 0),\n        5: (-1, 0),\n        6: (0, 1),\n        7: (0, -1),\n    }[button]\n\n\nclass PyMouse(PyMouseMeta):\n    def __init__(self, display=None):\n        PyMouseMeta.__init__(self)\n        self.display = Display(display)\n        self.display2 = Display(display)\n\n    def press(self, button=1):\n        #self.move(x, y)\n\n        with display_manager(self.display) as d:\n            fake_input(d, X.ButtonPress, translate_button_code(button))\n\n    def release(self, button=1):\n        #self.move(x, y)\n\n        with display_manager(self.display) as d:\n            fake_input(d, X.ButtonRelease, translate_button_code(button))\n\n    def scroll(self, vertical=None, horizontal=None, depth=None):\n        #Xlib supports only vertical and horizontal scrolling\n        if depth is not None:\n            raise ScrollSupportError(\'PyMouse cannot support depth-scrolling \\\nin X11. This feature is only available on Mac.\')\n\n        #Execute vertical then horizontal scrolling events\n        if vertical is not None:\n            vertical = int(vertical)\n            if vertical == 0:  # Do nothing with 0 distance\n                pass\n            elif vertical > 0:  # Scroll up if positive\n                self.click(*self.position(), button=4, n=vertical)\n            else:  # Scroll down if negative\n                self.click(*self.position(), button=5, n=abs(vertical))\n        if horizontal is not None:\n            horizontal = int(horizontal)\n            if horizontal == 0:  # Do nothing with 0 distance\n                pass\n            elif horizontal > 0:  # Scroll right if positive\n                self.click(*self.position(), button=7, n=horizontal)\n            else:  # Scroll left if negative\n                self.click(*self.position(), button=6, n=abs(horizontal))\n\n    def move(self, x, y):\n        if (x, y) != self.position():\n            with display_manager(self.display) as d:\n                fake_input(d, X.MotionNotify, x=x, y=y)\n\n    def drag(self, x, y):\n        with display_manager(self.display) as d:\n            fake_input(d, X.ButtonPress, 1)\n            fake_input(d, X.MotionNotify, x=x, y=y)\n            fake_input(d, X.ButtonRelease, 1)\n\n    def position(self):\n        coord = self.display.screen().root.query_pointer()._data\n        return coord[""root_x""], coord[""root_y""]\n\n    def screen_size(self):\n        width = self.display.screen().width_in_pixels\n        height = self.display.screen().height_in_pixels\n        return width, height\n\n\nclass PyMouseEvent(PyMouseEventMeta):\n    def __init__(self, capture=False, capture_move=False, display=None):\n        PyMouseEventMeta.__init__(self,\n                                  capture=capture,\n                                  capture_move=capture_move)\n        self.display = Display(display)\n        self.display2 = Display(display)\n        self.ctx = self.display2.record_create_context(\n            0,\n            [record.AllClients],\n            [{\n                    \'core_requests\': (0, 0),\n                    \'core_replies\': (0, 0),\n                    \'ext_requests\': (0, 0, 0, 0),\n                    \'ext_replies\': (0, 0, 0, 0),\n                    \'delivered_events\': (0, 0),\n                    \'device_events\': (X.ButtonPressMask, X.ButtonReleaseMask),\n                    \'errors\': (0, 0),\n                    \'client_started\': False,\n                    \'client_died\': False,\n            }])\n\n    def run(self):\n        try:\n            if self.capture and self.capture_move:\n                capturing = X.ButtonPressMask | X.ButtonReleaseMask | X.PointerMotionMask\n            elif self.capture:\n                capturing = X.ButtonPressMask | X.ButtonReleaseMask\n            elif self.capture_move:\n                capturing = X.PointerMotionMask\n            else:\n                capturing = False\n\n            if capturing:\n                self.display2.screen().root.grab_pointer(True,\n                                                         capturing,\n                                                         X.GrabModeAsync,\n                                                         X.GrabModeAsync,\n                                                         0, 0, X.CurrentTime)\n                self.display.screen().root.grab_pointer(True,\n                                                         capturing,\n                                                         X.GrabModeAsync,\n                                                         X.GrabModeAsync,\n                                                         0, 0, X.CurrentTime)\n\n            self.display2.record_enable_context(self.ctx, self.handler)\n            self.display2.record_free_context(self.ctx)\n        except KeyboardInterrupt:\n            self.stop()\n\n    def stop(self):\n        self.state = False\n        with display_manager(self.display) as d:\n            d.ungrab_pointer(X.CurrentTime)\n            d.record_disable_context(self.ctx)\n        with display_manager(self.display2) as d:\n            d.ungrab_pointer(X.CurrentTime)\n            d.record_disable_context(self.ctx)\n\n    def handler(self, reply):\n        data = reply.data\n        while len(data):\n            event, data = rq.EventField(None).parse_binary_value(data, self.display.display, None, None)\n\n            if event.detail in [4, 5, 6, 7]:\n                if event.type == X.ButtonPress:\n                    self.scroll(event.root_x, event.root_y, *button_code_to_scroll_direction(event.detail))\n            elif event.type == X.ButtonPress:\n                self.click(event.root_x, event.root_y, translate_button_code(event.detail), True)\n            elif event.type == X.ButtonRelease:\n                self.click(event.root_x, event.root_y, translate_button_code(event.detail), False)\n            else:\n                self.move(event.root_x, event.root_y)\n'"
linux/tensorbox/utils/__init__.py,3,"b""\nimport tensorflow as tf\nfrom distutils.version import LooseVersion\n\nTENSORFLOW_VERSION = LooseVersion(tf.__version__)\n\ndef tf_concat(axis, values, **kwargs):\n    if TENSORFLOW_VERSION >= LooseVersion('1.0'):\n        return tf.concat(values, axis, **kwargs)\n    else:\n        return tf.concat(axis, values, **kwargs)\n"""
linux/tensorbox/utils/data_utils.py,0,"b""import os\nimport cv2\nimport re\nimport sys\nimport argparse\nimport numpy as np\nimport copy\nimport annolist.AnnotationLib as al\n\ndef annotation_to_h5(H, a, cell_width, cell_height, max_len):\n    region_size = H['region_size']\n    assert H['region_size'] == H['image_height'] / H['grid_height']\n    assert H['region_size'] == H['image_width'] / H['grid_width']\n    cell_regions = get_cell_grid(cell_width, cell_height, region_size)\n\n    cells_per_image = len(cell_regions)\n\n    box_list = [[] for idx in range(cells_per_image)]\n            \n    for cidx, c in enumerate(cell_regions):\n        box_list[cidx] = [r for r in a.rects if all(r.intersection(c))]\n\n    boxes = np.zeros((1, cells_per_image, 4, max_len, 1), dtype = np.float)\n    box_flags = np.zeros((1, cells_per_image, 1, max_len, 1), dtype = np.float)\n\n    for cidx in xrange(cells_per_image):\n        #assert(cur_num_boxes <= max_len)\n\n        cell_ox = 0.5 * (cell_regions[cidx].x1 + cell_regions[cidx].x2)\n        cell_oy = 0.5 * (cell_regions[cidx].y1 + cell_regions[cidx].y2)\n\n        unsorted_boxes = []\n        for bidx in xrange(min(len(box_list[cidx]), max_len)):\n\n            # relative box position with respect to cell\n            ox = 0.5 * (box_list[cidx][bidx].x1 + box_list[cidx][bidx].x2) - cell_ox\n            oy = 0.5 * (box_list[cidx][bidx].y1 + box_list[cidx][bidx].y2) - cell_oy\n\n            width = abs(box_list[cidx][bidx].x2 - box_list[cidx][bidx].x1)\n            height= abs(box_list[cidx][bidx].y2 - box_list[cidx][bidx].y1)\n            \n            if (abs(ox) < H['focus_size'] * region_size and abs(oy) < H['focus_size'] * region_size and\n                    width < H['biggest_box_px'] and height < H['biggest_box_px']):\n                unsorted_boxes.append(np.array([ox, oy, width, height], dtype=np.float))\n\n        for bidx, box in enumerate(sorted(unsorted_boxes, key=lambda x: x[0]**2 + x[1]**2)):\n            boxes[0, cidx, :, bidx, 0] = box\n            box_flags[0, cidx, 0, bidx, 0] = max(box_list[cidx][bidx].silhouetteID, 1)\n\n    return boxes, box_flags\n\ndef get_cell_grid(cell_width, cell_height, region_size):\n\n    cell_regions = []\n    for iy in xrange(cell_height):\n        for ix in xrange(cell_width):\n            cidx = iy * cell_width + ix\n            ox = (ix + 0.5) * region_size\n            oy = (iy + 0.5) * region_size\n\n            r = al.AnnoRect(ox - 0.5 * region_size, oy - 0.5 * region_size,\n                            ox + 0.5 * region_size, oy + 0.5 * region_size)\n            r.track_id = cidx\n\n            cell_regions.append(r)\n\n\n    return cell_regions\n\ndef annotation_jitter(I, a_in, min_box_width=20, jitter_scale_min=0.9, jitter_scale_max=1.1, jitter_offset=16, target_width=640, target_height=480):\n    a = copy.deepcopy(a_in)\n\n    # MA: sanity check\n    new_rects = []\n    for i in range(len(a.rects)):\n        r = a.rects[i]\n        try:\n            assert(r.x1 < r.x2 and r.y1 < r.y2)\n            new_rects.append(r)\n        except:\n            print('bad rectangle')\n    a.rects = new_rects\n\n\n    if a.rects:\n        cur_min_box_width = min([r.width() for r in a.rects])\n    else:\n        cur_min_box_width = min_box_width / jitter_scale_min\n\n    # don't downscale below min_box_width \n    jitter_scale_min = max(jitter_scale_min, float(min_box_width) / cur_min_box_width)\n\n    # it's always ok to upscale \n    jitter_scale_min = min(jitter_scale_min, 1.0)\n\n    jitter_scale_max = jitter_scale_max\n\n    jitter_scale = np.random.uniform(jitter_scale_min, jitter_scale_max)\n\n    jitter_flip = np.random.random_integers(0, 1)\n\n    if jitter_flip == 1:\n        I = np.fliplr(I)\n\n        for r in a:\n            r.x1 = I.shape[1] - r.x1\n            r.x2 = I.shape[1] - r.x2\n            r.x1, r.x2 = r.x2, r.x1\n\n            for p in r.point:\n                p.x = I.shape[1] - p.x\n\n    I1 = cv2.resize(I, None, fx=jitter_scale, fy=jitter_scale, interpolation = cv2.INTER_CUBIC)\n\n    jitter_offset_x = np.random.random_integers(-jitter_offset, jitter_offset)\n    jitter_offset_y = np.random.random_integers(-jitter_offset, jitter_offset)\n\n\n\n    rescaled_width = I1.shape[1]\n    rescaled_height = I1.shape[0]\n\n    px = round(0.5*(target_width)) - round(0.5*(rescaled_width)) + jitter_offset_x\n    py = round(0.5*(target_height)) - round(0.5*(rescaled_height)) + jitter_offset_y\n\n    I2 = np.zeros((target_height, target_width, 3), dtype=I1.dtype)\n\n    x1 = max(0, px)\n    y1 = max(0, py)\n    x2 = min(rescaled_width, target_width - x1)\n    y2 = min(rescaled_height, target_height - y1)\n\n    I2[0:(y2 - y1), 0:(x2 - x1), :] = I1[y1:y2, x1:x2, :]\n\n    ox1 = round(0.5*rescaled_width) + jitter_offset_x\n    oy1 = round(0.5*rescaled_height) + jitter_offset_y\n\n    ox2 = round(0.5*target_width)\n    oy2 = round(0.5*target_height)\n\n    for r in a:\n        r.x1 = round(jitter_scale*r.x1 - x1)\n        r.x2 = round(jitter_scale*r.x2 - x1)\n\n        r.y1 = round(jitter_scale*r.y1 - y1)\n        r.y2 = round(jitter_scale*r.y2 - y1)\n\n        if r.x1 < 0:\n            r.x1 = 0\n\n        if r.y1 < 0:\n            r.y1 = 0\n\n        if r.x2 >= I2.shape[1]:\n            r.x2 = I2.shape[1] - 1\n\n        if r.y2 >= I2.shape[0]:\n            r.y2 = I2.shape[0] - 1\n\n        for p in r.point:\n            p.x = round(jitter_scale*p.x - x1)\n            p.y = round(jitter_scale*p.y - y1)\n\n        # MA: make sure all points are inside the image\n        r.point = [p for p in r.point if p.x >=0 and p.y >=0 and p.x < I2.shape[1] and p.y < I2.shape[0]]\n\n    new_rects = []\n    for r in a.rects:\n        if r.x1 <= r.x2 and r.y1 <= r.y2:\n            new_rects.append(r)\n        else:\n            pass\n\n    a.rects = new_rects\n\n    return I2, a\n"""
linux/tensorbox/utils/googlenet_load.py,0,"b""from slim_nets import inception_v1 as inception\nfrom slim_nets import resnet_v1 as resnet\nimport tensorflow.contrib.slim as slim\n\ndef model(x, H, reuse, is_training=True):\n    if H['slim_basename'] == 'resnet_v1_101':\n        with slim.arg_scope(resnet.resnet_arg_scope()):\n            _, T = resnet.resnet_v1_101(x,\n                                        is_training=is_training,\n                                        num_classes=1000,\n                                        reuse=reuse)\n    elif H['slim_basename'] == 'InceptionV1':\n        with slim.arg_scope(inception.inception_v1_arg_scope()):\n            _, T = inception.inception_v1(x,\n                                          is_training=is_training,\n                                          num_classes=1001,\n                                          spatial_squeeze=False,\n                                          reuse=reuse)\n    #print '\\n'.join(map(str, [(k, v.op.outputs[0].get_shape()) for k, v in T.iteritems()]))\n\n    coarse_feat = T[H['slim_top_lname']][:, :, :, :H['later_feat_channels']]\n    assert coarse_feat.op.outputs[0].get_shape()[3] == H['later_feat_channels']\n\n    # fine feat can be used to reinspect input\n    attention_lname = H.get('slim_attention_lname', 'Mixed_3b')\n    early_feat = T[attention_lname]\n\n    return coarse_feat, early_feat\n"""
linux/tensorbox/utils/rect.py,0,"b'class Rect(object):\n    def __init__(self, cx, cy, width, height, confidence):\n        self.cx = cx\n        self.cy = cy\n        self.width = width\n        self.height = height\n        self.confidence = confidence\n        self.true_confidence = confidence\n    def overlaps(self, other):\n        if abs(self.cx - other.cx) > (self.width + other.width) / 1.5:\n            return False\n        elif abs(self.cy - other.cy) > (self.height + other.height) / 2.0:\n            return False\n        else:\n            return True\n    def distance(self, other):\n        return sum(map(abs, [self.cx - other.cx, self.cy - other.cy,\n                       self.width - other.width, self.height - other.height]))\n    def intersection(self, other):\n        left = max(self.cx - self.width/2., other.cx - other.width/2.)\n        right = min(self.cx + self.width/2., other.cx + other.width/2.)\n        width = max(right - left, 0)\n        top = max(self.cy - self.height/2., other.cy - other.height/2.)\n        bottom = min(self.cy + self.height/2., other.cy + other.height/2.)\n        height = max(bottom - top, 0)\n        return width * height\n    def area(self):\n        return self.height * self.width\n    def union(self, other):\n        return self.area() + other.area() - self.intersection(other)\n    def iou(self, other):\n        return self.intersection(other) / self.union(other)\n    def __eq__(self, other):\n        return (self.cx == other.cx and \n            self.cy == other.cy and\n            self.width == other.width and\n            self.height == other.height and\n            self.confidence == other.confidence)\n'"
linux/tensorbox/utils/stitch_wrapper.py,0,"b""print('ERROR: stitch_wrapper not yet compiled. Please run `cd /path/to/tensorbox/utils && make`')\n"""
linux/tensorbox/utils/train_utils.py,32,"b'import numpy as np\nimport random\nimport json\nimport os\nimport cv2\nimport itertools\nfrom scipy.misc import imread, imresize\nimport tensorflow as tf\n\nfrom data_utils import (annotation_jitter, annotation_to_h5)\nfrom utils.annolist import AnnotationLib as al\nfrom rect import Rect\nfrom utils import tf_concat\n\ndef rescale_boxes(current_shape, anno, target_height, target_width):\n    x_scale = target_width / float(current_shape[1])\n    y_scale = target_height / float(current_shape[0])\n    for r in anno.rects:\n        assert r.x1 < r.x2\n        r.x1 *= x_scale\n        r.x2 *= x_scale\n        assert r.y1 < r.y2\n        r.y1 *= y_scale\n        r.y2 *= y_scale\n    return anno\n\ndef load_idl_tf(idlfile, H, jitter):\n    """"""Take the idlfile and net configuration and create a generator\n    that outputs a jittered version of a random image from the annolist\n    that is mean corrected.""""""\n\n    annolist = al.parse(idlfile)\n    annos = []\n    for anno in annolist:\n        anno.imageName = os.path.join(\n            os.path.dirname(os.path.realpath(idlfile)), anno.imageName)\n        annos.append(anno)\n    random.seed(0)\n    if H[\'data\'][\'truncate_data\']:\n        annos = annos[:10]\n    for epoch in itertools.count():\n        random.shuffle(annos)\n        for anno in annos:\n            I = imread(anno.imageName)\n\t    #Skip Greyscale images\n            if len(I.shape) < 3:\n                continue\n            if I.shape[2] == 4:\n                I = I[:, :, :3]\n            if I.shape[0] != H[""image_height""] or I.shape[1] != H[""image_width""]:\n                if epoch == 0:\n                    anno = rescale_boxes(I.shape, anno, H[""image_height""], H[""image_width""])\n                I = imresize(I, (H[""image_height""], H[""image_width""]), interp=\'cubic\')\n            if jitter:\n                jitter_scale_min=0.9\n                jitter_scale_max=1.1\n                jitter_offset=16\n                I, anno = annotation_jitter(I,\n                                            anno, target_width=H[""image_width""],\n                                            target_height=H[""image_height""],\n                                            jitter_scale_min=jitter_scale_min,\n                                            jitter_scale_max=jitter_scale_max,\n                                            jitter_offset=jitter_offset)\n\n            boxes, flags = annotation_to_h5(H,\n                                            anno,\n                                            H[""grid_width""],\n                                            H[""grid_height""],\n                                            H[""rnn_len""])\n\n            yield {""image"": I, ""boxes"": boxes, ""flags"": flags}\n\ndef make_sparse(n, d):\n    v = np.zeros((d,), dtype=np.float32)\n    v[n] = 1.\n    return v\n\ndef load_data_gen(H, phase, jitter):\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n\n    data = load_idl_tf(H[""data""][\'%s_idl\' % phase], H, jitter={\'train\': jitter, \'test\': False}[phase])\n\n    for d in data:\n        output = {}\n\n        rnn_len = H[""rnn_len""]\n        flags = d[\'flags\'][0, :, 0, 0:rnn_len, 0]\n        boxes = np.transpose(d[\'boxes\'][0, :, :, 0:rnn_len, 0], (0, 2, 1))\n        assert(flags.shape == (grid_size, rnn_len))\n        assert(boxes.shape == (grid_size, rnn_len, 4))\n\n        output[\'image\'] = d[\'image\']\n        output[\'confs\'] = np.array([[make_sparse(int(detection), d=H[\'num_classes\']) for detection in cell] for cell in flags])\n        output[\'boxes\'] = boxes\n        output[\'flags\'] = flags\n\n        yield output\n\ndef add_rectangles(H, orig_image, confidences, boxes, use_stitching=False, rnn_len=1, min_conf=0.1, show_removed=True, tau=0.25, show_suppressed=True):\n    image = np.copy(orig_image[0])\n    num_cells = H[""grid_height""] * H[""grid_width""]\n    boxes_r = np.reshape(boxes, (-1,\n                                 H[""grid_height""],\n                                 H[""grid_width""],\n                                 rnn_len,\n                                 4))\n    confidences_r = np.reshape(confidences, (-1,\n                                             H[""grid_height""],\n                                             H[""grid_width""],\n                                             rnn_len,\n                                             H[\'num_classes\']))\n    cell_pix_size = H[\'region_size\']\n    all_rects = [[[] for _ in range(H[""grid_width""])] for _ in range(H[""grid_height""])]\n    for n in range(rnn_len):\n        for y in range(H[""grid_height""]):\n            for x in range(H[""grid_width""]):\n                bbox = boxes_r[0, y, x, n, :]\n                abs_cx = int(bbox[0]) + cell_pix_size/2 + cell_pix_size * x\n                abs_cy = int(bbox[1]) + cell_pix_size/2 + cell_pix_size * y\n                w = bbox[2]\n                h = bbox[3]\n                conf = np.max(confidences_r[0, y, x, n, 1:])\n                all_rects[y][x].append(Rect(abs_cx,abs_cy,w,h,conf))\n\n    all_rects_r = [r for row in all_rects for cell in row for r in cell]\n    if use_stitching:\n        from stitch_wrapper import stitch_rects\n        acc_rects = stitch_rects(all_rects, tau)\n    else:\n        acc_rects = all_rects_r\n\n\n    if show_suppressed:\n        pairs = [(all_rects_r, (255, 0, 0))]\n    else:\n        pairs = []\n    pairs.append((acc_rects, (0, 255, 0)))\n    for rect_set, color in pairs:\n        for rect in rect_set:\n            if rect.confidence > min_conf:\n                cv2.rectangle(image,\n                    (rect.cx-int(rect.width/2), rect.cy-int(rect.height/2)),\n                    (rect.cx+int(rect.width/2), rect.cy+int(rect.height/2)),\n                    color,\n                    2)\n\n    rects = []\n    for rect in acc_rects:\n        r = al.AnnoRect()\n        r.x1 = rect.cx - rect.width/2.\n        r.x2 = rect.cx + rect.width/2.\n        r.y1 = rect.cy - rect.height/2.\n        r.y2 = rect.cy + rect.height/2.\n        r.score = rect.true_confidence\n        rects.append(r)\n\n    return image, rects\n\ndef to_x1y1x2y2(box):\n    w = tf.maximum(box[:, 2:3], 1)\n    h = tf.maximum(box[:, 3:4], 1)\n    x1 = box[:, 0:1] - w / 2\n    x2 = box[:, 0:1] + w / 2\n    y1 = box[:, 1:2] - h / 2\n    y2 = box[:, 1:2] + h / 2\n    return tf_concat(1, [x1, y1, x2, y2])\n\ndef intersection(box1, box2):\n    x1_max = tf.maximum(box1[:, 0], box2[:, 0])\n    y1_max = tf.maximum(box1[:, 1], box2[:, 1])\n    x2_min = tf.minimum(box1[:, 2], box2[:, 2])\n    y2_min = tf.minimum(box1[:, 3], box2[:, 3])\n\n    x_diff = tf.maximum(x2_min - x1_max, 0)\n    y_diff = tf.maximum(y2_min - y1_max, 0)\n\n    return x_diff * y_diff\n\ndef area(box):\n    x_diff = tf.maximum(box[:, 2] - box[:, 0], 0)\n    y_diff = tf.maximum(box[:, 3] - box[:, 1], 0)\n    return x_diff * y_diff\n\ndef union(box1, box2):\n    return area(box1) + area(box2) - intersection(box1, box2)\n\ndef iou(box1, box2):\n    return intersection(box1, box2) / union(box1, box2)\n\ndef to_idx(vec, w_shape):\n    \'\'\'\n    vec = (idn, idh, idw)\n    w_shape = [n, h, w, c]\n    \'\'\'\n    return vec[:, 2] + w_shape[2] * (vec[:, 1] + w_shape[1] * vec[:, 0])\n\ndef interp(w, i, channel_dim):\n    \'\'\'\n    Input:\n        w: A 4D block tensor of shape (n, h, w, c)\n        i: A list of 3-tuples [(x_1, y_1, z_1), (x_2, y_2, z_2), ...],\n            each having type (int, float, float)\n\n        The 4D block represents a batch of 3D image feature volumes with c channels.\n        The input i is a list of points  to index into w via interpolation. Direct\n        indexing is not possible due to y_1 and z_1 being float values.\n    Output:\n        A list of the values: [\n            w[x_1, y_1, z_1, :]\n            w[x_2, y_2, z_2, :]\n            ...\n            w[x_k, y_k, z_k, :]\n        ]\n        of the same length == len(i)\n    \'\'\'\n    w_as_vector = tf.reshape(w, [-1, channel_dim]) # gather expects w to be 1-d\n    upper_l = tf.to_int32(tf_concat(1, [i[:, 0:1], tf.floor(i[:, 1:2]), tf.floor(i[:, 2:3])]))\n    upper_r = tf.to_int32(tf_concat(1, [i[:, 0:1], tf.floor(i[:, 1:2]), tf.ceil(i[:, 2:3])]))\n    lower_l = tf.to_int32(tf_concat(1, [i[:, 0:1], tf.ceil(i[:, 1:2]), tf.floor(i[:, 2:3])]))\n    lower_r = tf.to_int32(tf_concat(1, [i[:, 0:1], tf.ceil(i[:, 1:2]), tf.ceil(i[:, 2:3])]))\n\n    upper_l_idx = to_idx(upper_l, tf.shape(w))\n    upper_r_idx = to_idx(upper_r, tf.shape(w))\n    lower_l_idx = to_idx(lower_l, tf.shape(w))\n    lower_r_idx = to_idx(lower_r, tf.shape(w))\n\n    upper_l_value = tf.gather(w_as_vector, upper_l_idx)\n    upper_r_value = tf.gather(w_as_vector, upper_r_idx)\n    lower_l_value = tf.gather(w_as_vector, lower_l_idx)\n    lower_r_value = tf.gather(w_as_vector, lower_r_idx)\n\n    alpha_lr = tf.expand_dims(i[:, 2] - tf.floor(i[:, 2]), 1)\n    alpha_ud = tf.expand_dims(i[:, 1] - tf.floor(i[:, 1]), 1)\n\n    upper_value = (1 - alpha_lr) * upper_l_value + (alpha_lr) * upper_r_value\n    lower_value = (1 - alpha_lr) * lower_l_value + (alpha_lr) * lower_r_value\n    value = (1 - alpha_ud) * upper_value + (alpha_ud) * lower_value\n    return value\n\ndef bilinear_select(H, pred_boxes, early_feat, early_feat_channels, w_offset, h_offset):\n    \'\'\'\n    Function used for rezooming high level feature maps. Uses bilinear interpolation\n    to select all channels at index (x, y) for a high level feature map, where x and y are floats.\n    \'\'\'\n    grid_size = H[\'grid_width\'] * H[\'grid_height\']\n    outer_size = grid_size * H[\'batch_size\']\n\n    fine_stride = 8. # pixels per 60x80 grid cell in 480x640 image\n    coarse_stride = H[\'region_size\'] # pixels per 15x20 grid cell in 480x640 image\n    batch_ids = []\n    x_offsets = []\n    y_offsets = []\n    for n in range(H[\'batch_size\']):\n        for i in range(H[\'grid_height\']):\n            for j in range(H[\'grid_width\']):\n                for k in range(H[\'rnn_len\']):\n                    batch_ids.append([n])\n                    x_offsets.append([coarse_stride / 2. + coarse_stride * j])\n                    y_offsets.append([coarse_stride / 2. + coarse_stride * i])\n\n    batch_ids = tf.constant(batch_ids)\n    x_offsets = tf.constant(x_offsets)\n    y_offsets = tf.constant(y_offsets)\n\n    pred_boxes_r = tf.reshape(pred_boxes, [outer_size * H[\'rnn_len\'], 4])\n    scale_factor = coarse_stride / fine_stride # scale difference between 15x20 and 60x80 features\n\n    pred_x_center = (pred_boxes_r[:, 0:1] + w_offset * pred_boxes_r[:, 2:3] + x_offsets) / fine_stride\n    pred_x_center_clip = tf.clip_by_value(pred_x_center,\n                                     0,\n                                     scale_factor * H[\'grid_width\'] - 1)\n    pred_y_center = (pred_boxes_r[:, 1:2] + h_offset * pred_boxes_r[:, 3:4] + y_offsets) / fine_stride\n    pred_y_center_clip = tf.clip_by_value(pred_y_center,\n                                          0,\n                                          scale_factor * H[\'grid_height\'] - 1)\n\n    interp_indices = tf_concat(1, [tf.to_float(batch_ids), pred_y_center_clip, pred_x_center_clip])\n    return interp_indices\n'"
linux/tensorbox/utils/annolist/AnnoList_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: AnnoList.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'AnnoList.proto\',\n  package=\'protobuf_annolist\',\n  serialized_pb=_b(\'\\n\\x0e\\x41nnoList.proto\\x12\\x11protobuf_annolist\\""B\\n\\tAttribute\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\x0b\\n\\x03val\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04\\x66val\\x18\\x03 \\x01(\\x02\\x12\\x0e\\n\\x06strval\\x18\\x04 \\x01(\\t\\""\\""\\n\\tIdStrPair\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\x05\\x12\\t\\n\\x01s\\x18\\x02 \\x01(\\t\\""j\\n\\rAttributeDesc\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05\\x64type\\x18\\x03 \\x01(\\x05\\x12\\x30\\n\\nval_to_str\\x18\\x04 \\x03(\\x0b\\x32\\x1c.protobuf_annolist.IdStrPair\\"".\\n\\x11\\x41nnoRectAttribute\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0b\\n\\x03val\\x18\\x02 \\x01(\\t\\""\\x98\\x01\\n\\x08\\x41nnoRect\\x12\\n\\n\\x02x1\\x18\\x01 \\x01(\\x02\\x12\\n\\n\\x02y1\\x18\\x02 \\x01(\\x02\\x12\\n\\n\\x02x2\\x18\\x03 \\x01(\\x02\\x12\\n\\n\\x02y2\\x18\\x04 \\x01(\\x02\\x12\\r\\n\\x05score\\x18\\x05 \\x01(\\x02\\x12\\n\\n\\x02id\\x18\\x06 \\x01(\\x05\\x12\\x10\\n\\x08track_id\\x18\\x0b \\x01(\\x05\\x12/\\n\\tattribute\\x18\\x0c \\x03(\\x0b\\x32\\x1c.protobuf_annolist.Attribute\\""o\\n\\nAnnotation\\x12\\x11\\n\\timageName\\x18\\x01 \\x01(\\t\\x12)\\n\\x04rect\\x18\\x02 \\x03(\\x0b\\x32\\x1b.protobuf_annolist.AnnoRect\\x12\\x10\\n\\x08imgWidth\\x18\\x03 \\x01(\\x05\\x12\\x11\\n\\timgHeight\\x18\\x04 \\x01(\\x05\\""w\\n\\x08\\x41nnoList\\x12\\x31\\n\\nannotation\\x18\\x01 \\x03(\\x0b\\x32\\x1d.protobuf_annolist.Annotation\\x12\\x38\\n\\x0e\\x61ttribute_desc\\x18\\x02 \\x03(\\x0b\\x32 .protobuf_annolist.AttributeDescB\\x0c\\x42\\nAnnoListPb\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_ATTRIBUTE = _descriptor.Descriptor(\n  name=\'Attribute\',\n  full_name=\'protobuf_annolist.Attribute\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.Attribute.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val\', full_name=\'protobuf_annolist.Attribute.val\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fval\', full_name=\'protobuf_annolist.Attribute.fval\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'strval\', full_name=\'protobuf_annolist.Attribute.strval\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=37,\n  serialized_end=103,\n)\n\n\n_IDSTRPAIR = _descriptor.Descriptor(\n  name=\'IdStrPair\',\n  full_name=\'protobuf_annolist.IdStrPair\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.IdStrPair.id\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'protobuf_annolist.IdStrPair.s\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=105,\n  serialized_end=139,\n)\n\n\n_ATTRIBUTEDESC = _descriptor.Descriptor(\n  name=\'AttributeDesc\',\n  full_name=\'protobuf_annolist.AttributeDesc\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'protobuf_annolist.AttributeDesc.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.AttributeDesc.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'protobuf_annolist.AttributeDesc.dtype\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val_to_str\', full_name=\'protobuf_annolist.AttributeDesc.val_to_str\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=141,\n  serialized_end=247,\n)\n\n\n_ANNORECTATTRIBUTE = _descriptor.Descriptor(\n  name=\'AnnoRectAttribute\',\n  full_name=\'protobuf_annolist.AnnoRectAttribute\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'protobuf_annolist.AnnoRectAttribute.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val\', full_name=\'protobuf_annolist.AnnoRectAttribute.val\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=249,\n  serialized_end=295,\n)\n\n\n_ANNORECT = _descriptor.Descriptor(\n  name=\'AnnoRect\',\n  full_name=\'protobuf_annolist.AnnoRect\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'x1\', full_name=\'protobuf_annolist.AnnoRect.x1\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y1\', full_name=\'protobuf_annolist.AnnoRect.y1\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x2\', full_name=\'protobuf_annolist.AnnoRect.x2\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y2\', full_name=\'protobuf_annolist.AnnoRect.y2\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'protobuf_annolist.AnnoRect.score\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'protobuf_annolist.AnnoRect.id\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'track_id\', full_name=\'protobuf_annolist.AnnoRect.track_id\', index=6,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attribute\', full_name=\'protobuf_annolist.AnnoRect.attribute\', index=7,\n      number=12, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=298,\n  serialized_end=450,\n)\n\n\n_ANNOTATION = _descriptor.Descriptor(\n  name=\'Annotation\',\n  full_name=\'protobuf_annolist.Annotation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'imageName\', full_name=\'protobuf_annolist.Annotation.imageName\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rect\', full_name=\'protobuf_annolist.Annotation.rect\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'imgWidth\', full_name=\'protobuf_annolist.Annotation.imgWidth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'imgHeight\', full_name=\'protobuf_annolist.Annotation.imgHeight\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=452,\n  serialized_end=563,\n)\n\n\n_ANNOLIST = _descriptor.Descriptor(\n  name=\'AnnoList\',\n  full_name=\'protobuf_annolist.AnnoList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'annotation\', full_name=\'protobuf_annolist.AnnoList.annotation\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'attribute_desc\', full_name=\'protobuf_annolist.AnnoList.attribute_desc\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=565,\n  serialized_end=684,\n)\n\n_ATTRIBUTEDESC.fields_by_name[\'val_to_str\'].message_type = _IDSTRPAIR\n_ANNORECT.fields_by_name[\'attribute\'].message_type = _ATTRIBUTE\n_ANNOTATION.fields_by_name[\'rect\'].message_type = _ANNORECT\n_ANNOLIST.fields_by_name[\'annotation\'].message_type = _ANNOTATION\n_ANNOLIST.fields_by_name[\'attribute_desc\'].message_type = _ATTRIBUTEDESC\nDESCRIPTOR.message_types_by_name[\'Attribute\'] = _ATTRIBUTE\nDESCRIPTOR.message_types_by_name[\'IdStrPair\'] = _IDSTRPAIR\nDESCRIPTOR.message_types_by_name[\'AttributeDesc\'] = _ATTRIBUTEDESC\nDESCRIPTOR.message_types_by_name[\'AnnoRectAttribute\'] = _ANNORECTATTRIBUTE\nDESCRIPTOR.message_types_by_name[\'AnnoRect\'] = _ANNORECT\nDESCRIPTOR.message_types_by_name[\'Annotation\'] = _ANNOTATION\nDESCRIPTOR.message_types_by_name[\'AnnoList\'] = _ANNOLIST\n\nAttribute = _reflection.GeneratedProtocolMessageType(\'Attribute\', (_message.Message,), dict(\n  DESCRIPTOR = _ATTRIBUTE,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.Attribute)\n  ))\n_sym_db.RegisterMessage(Attribute)\n\nIdStrPair = _reflection.GeneratedProtocolMessageType(\'IdStrPair\', (_message.Message,), dict(\n  DESCRIPTOR = _IDSTRPAIR,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.IdStrPair)\n  ))\n_sym_db.RegisterMessage(IdStrPair)\n\nAttributeDesc = _reflection.GeneratedProtocolMessageType(\'AttributeDesc\', (_message.Message,), dict(\n  DESCRIPTOR = _ATTRIBUTEDESC,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AttributeDesc)\n  ))\n_sym_db.RegisterMessage(AttributeDesc)\n\nAnnoRectAttribute = _reflection.GeneratedProtocolMessageType(\'AnnoRectAttribute\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNORECTATTRIBUTE,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoRectAttribute)\n  ))\n_sym_db.RegisterMessage(AnnoRectAttribute)\n\nAnnoRect = _reflection.GeneratedProtocolMessageType(\'AnnoRect\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNORECT,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoRect)\n  ))\n_sym_db.RegisterMessage(AnnoRect)\n\nAnnotation = _reflection.GeneratedProtocolMessageType(\'Annotation\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNOTATION,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.Annotation)\n  ))\n_sym_db.RegisterMessage(Annotation)\n\nAnnoList = _reflection.GeneratedProtocolMessageType(\'AnnoList\', (_message.Message,), dict(\n  DESCRIPTOR = _ANNOLIST,\n  __module__ = \'AnnoList_pb2\'\n  # @@protoc_insertion_point(class_scope:protobuf_annolist.AnnoList)\n  ))\n_sym_db.RegisterMessage(AnnoList)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'B\\nAnnoListPb\'))\n# @@protoc_insertion_point(module_scope)\n'"
linux/tensorbox/utils/annolist/AnnotationLib.py,0,"b'import os\r\n\r\nfrom math import sqrt\r\n\r\nimport gzip\r\nimport json\r\nimport bz2\r\nimport sys\r\nimport numpy as np;\r\n\r\nfrom collections import MutableSequence\r\n\r\n#import AnnoList_pb2\r\nimport PalLib;\r\n\r\nimport xml.dom.minidom\r\nfrom xml.dom.minidom import Node\r\nxml_dom_ext_available=False\r\ntry:\r\n    import xml.dom.ext\r\n    xml_dom_ext_available=True\r\nexcept ImportError:\r\n    pass\r\n\r\n\r\n################################################\r\n#\r\n#  TODO: check distance function\r\n#\r\n################################################\r\n\r\n\r\ndef cmpAnnoRectsByScore(r1, r2):\r\n    return cmp(r1.score, r2.score)\r\n\r\ndef cmpAnnoRectsByScoreDescending(r1, r2):\r\n    return (-1)*cmp(r1.score, r2.score)\r\n\r\ndef cmpDetAnnoRectsByScore(r1, r2):\r\n    return cmp(r1.rect.score, r2.rect.score);\r\n\r\n\r\ndef suffixMatch(fn1, fn2):\r\n    l1 = len(fn1);\r\n    l2 = len(fn2);\r\n            \r\n    if fn1[-l2:] == fn2:\r\n        return True\r\n\r\n    if fn2[-l1:] == fn1:\r\n        return True\r\n\r\n    return False            \r\n\r\nclass AnnoList(MutableSequence):\r\n    """"""Define a list format, which I can customize""""""\r\n    TYPE_INT32 = 5;\r\n    TYPE_FLOAT = 2;\r\n    TYPE_STRING = 9;\r\n\r\n    def __init__(self, data=None):\r\n        super(AnnoList, self).__init__()\r\n\r\n        self.attribute_desc = {};\r\n        self.attribute_val_to_str = {};\r\n\r\n        if not (data is None):\r\n            self._list = list(data)\r\n        else:\r\n            self._list = list()     \r\n\r\n    def add_attribute(self, name, dtype):\r\n        _adesc = AnnoList_pb2.AttributeDesc();\r\n        _adesc.name = name;\r\n        if self.attribute_desc:\r\n            _adesc.id = max((self.attribute_desc[d].id for d in self.attribute_desc)) + 1;\r\n        else:\r\n            _adesc.id = 0;\r\n\r\n        if dtype == int:\r\n            _adesc.dtype = AnnoList.TYPE_INT32;\r\n        elif dtype == float or dtype == np.float32:\r\n            _adesc.dtype = AnnoList.TYPE_FLOAT;\r\n        elif dtype == str:\r\n            _adesc.dtype = AnnoList.TYPE_STRING;\r\n        else:\r\n            print ""unknown attribute type: "", dtype\r\n            assert(False);\r\n    \r\n        #print ""adding attribute: {}, id: {}, type: {}"".format(_adesc.name, _adesc.id, _adesc.dtype);\r\n        self.attribute_desc[name] = _adesc;\r\n\r\n    def add_attribute_val(self, aname, vname, val):\r\n        # add attribute before adding string corresponding to integer value\r\n        assert(aname in self.attribute_desc);\r\n\r\n        # check and add if new \r\n        if all((val_desc.id != val for val_desc in self.attribute_desc[aname].val_to_str)):\r\n            val_desc = self.attribute_desc[aname].val_to_str.add()\r\n            val_desc.id = val;\r\n            val_desc.s = vname;\r\n\r\n        # also add to map for quick access\r\n        if not aname in self.attribute_val_to_str:\r\n            self.attribute_val_to_str[aname] = {};\r\n\r\n        assert(not val in self.attribute_val_to_str[aname]);\r\n        self.attribute_val_to_str[aname][val] = vname;\r\n\r\n\r\n    def attribute_get_value_str(self, aname, val):\r\n        if aname in self.attribute_val_to_str and val in self.attribute_val_to_str[aname]:\r\n            return self.attribute_val_to_str[aname][val];\r\n        else:\r\n            return str(val);\r\n\r\n    def save(self, fname):\r\n        save(fname, self);\r\n\r\n    #MA: list interface   \r\n    def __len__(self):\r\n        return len(self._list)\r\n\r\n    def __getitem__(self, ii):\r\n        if isinstance(ii, slice):\r\n            res = AnnoList();\r\n            res.attribute_desc = self.attribute_desc;\r\n            res._list = self._list[ii]\r\n            return res;\r\n        else:\r\n            return self._list[ii]\r\n\r\n    def __delitem__(self, ii):\r\n        del self._list[ii]\r\n\r\n    def __setitem__(self, ii, val):\r\n        return self._list[ii]\r\n\r\n    def __str__(self):\r\n        return self.__repr__()\r\n\r\n    def __repr__(self):\r\n        return """"""<AnnoList %s>"""""" % self._list\r\n\r\n    def insert(self, ii, val):\r\n        self._list.insert(ii, val)\r\n\r\n    def append(self, val):\r\n        list_idx = len(self._list)\r\n        self.insert(list_idx, val)\r\n\r\n\r\ndef is_compatible_attr_type(protobuf_type, attr_type):\r\n    if protobuf_type == AnnoList.TYPE_INT32:\r\n        return (attr_type == int);\r\n    elif protobuf_type == AnnoList.TYPE_FLOAT:\r\n        return (attr_type == float or attr_type == np.float32);\r\n    elif protobuf_type == AnnoList.TYPE_STRING:\r\n        return (attr_type == str);\r\n    else:\r\n        assert(false);\r\n\r\n\r\ndef protobuf_type_to_python(protobuf_type):\r\n    if protobuf_type == AnnoList.TYPE_INT32:\r\n        return int;\r\n    elif protobuf_type == AnnoList.TYPE_FLOAT:\r\n        return float;\r\n    elif protobuf_type == AnnoList.TYPE_STRING:\r\n        return str;\r\n    else:\r\n        assert(false);\r\n\r\n\r\nclass AnnoPoint(object):\r\n    def __init__(self, x=None, y=None, id=None):\r\n        self.x = x;\r\n        self.y = y;\r\n        self.id = id;\r\n\r\nclass AnnoRect(object):\r\n    def __init__(self, x1=-1, y1=-1, x2=-1, y2=-1):\r\n\r\n        self.x1 = x1\r\n        self.y1 = y1\r\n        self.x2 = x2\r\n        self.y2 = y2\r\n\r\n        self.score = -1.0\r\n        self.scale = -1.0\r\n        self.articulations =[]\r\n        self.viewpoints =[]\r\n        self.d3 = []\r\n\r\n        self.silhouetteID = -1\r\n        self.classID = -1\r\n        self.track_id = -1\r\n\r\n        self.point = [];\r\n        self.at = {};\r\n\r\n    def width(self):\r\n        return abs(self.x2-self.x1)\r\n\r\n    def height(self):\r\n        return abs(self.y2-self.y1)\r\n\r\n    def centerX(self):\r\n        return (self.x1+self.x2)/2.0\r\n\r\n    def centerY(self):\r\n        return (self.y1+self.y2)/2.0\r\n\r\n    def left(self):\r\n        return min(self.x1, self.x2)\r\n\r\n    def right(self):\r\n        return max(self.x1, self.x2)\r\n\r\n    def top(self):\r\n        return min(self.y1, self.y2)\r\n\r\n    def bottom(self):\r\n        return max(self.y1, self.y2)\r\n\r\n    def forceAspectRatio(self, ratio, KeepHeight = False, KeepWidth = False):\r\n        """"""force the Aspect ratio""""""\r\n        if KeepWidth or ((not KeepHeight) and self.width() * 1.0 / self.height() > ratio):\r\n            # extend height\r\n            newHeight = self.width() * 1.0 / ratio\r\n            self.y1 = (self.centerY() - newHeight / 2.0)\r\n            self.y2 = (self.y1 + newHeight)\r\n        else:\r\n            # extend width\r\n            newWidth = self.height() * ratio\r\n            self.x1 = (self.centerX() - newWidth / 2.0)\r\n            self.x2 = (self.x1 + newWidth)\r\n            \r\n    def clipToImage(self, min_x, max_x, min_y, max_y):\r\n        self.x1 = max(min_x, self.x1)\r\n        self.x2 = max(min_x, self.x2)\r\n        self.y1 = max(min_y, self.y1)\r\n        self.y2 = max(min_y, self.y2)\r\n        self.x1 = min(max_x, self.x1)\r\n        self.x2 = min(max_x, self.x2)\r\n        self.y1 = min(max_y, self.y1)\r\n        self.y2 = min(max_y, self.y2)\r\n\r\n    def printContent(self):\r\n        print ""Coords: "", self.x1, self.y1, self.x2, self.y2\r\n        print ""Score: "", self.score\r\n        print ""Articulations: "", self.articulations\r\n        print ""Viewpoints: "", self.viewpoints\r\n        print ""Silhouette: "", self.silhouetteID\r\n\r\n    def ascii(self):\r\n        r = ""(""+str(self.x1)+"", ""+str(self.y1)+"", ""+str(self.x2)+"", ""+str(self.y2)+"")""\r\n        if (self.score!=-1):\r\n            r = r + "":""+str(self.score)\r\n        if (self.silhouetteID !=-1):\r\n            ri = r + ""/""+str(self.silhouetteID)\r\n        return r\r\n\r\n    def writeIDL(self, file):\r\n        file.write("" (""+str(self.x1)+"", ""+str(self.y1)+"", ""+str(self.x2)+"", ""+str(self.y2)+"")"")\r\n        if (self.score!=-1):\r\n            file.write("":""+str(self.score))\r\n        if (self.silhouetteID !=-1):\r\n            file.write(""/""+str(self.silhouetteID))\r\n\r\n    def writeJSON(self):\r\n        jdoc = {""x1"": self.x1, ""x2"": self.x2, ""y1"": self.y1, ""y2"": self.y2}\r\n        \r\n        if (self.score != -1):\r\n            jdoc[""score""] = self.score\r\n        return jdoc\r\n\r\n    def sortCoords(self):\r\n        if (self.x1>self.x2):\r\n            self.x1, self.x2 = self.x2, self.x1\r\n        if (self.y1>self.y2):\r\n            self.y1, self.y2 = self.y2, self.y1\r\n\r\n    def rescale(self, factor):\r\n        self.x1=(self.x1*float(factor))\r\n        self.y1=(self.y1*float(factor))\r\n        self.x2=(self.x2*float(factor))\r\n        self.y2=(self.y2*float(factor))\r\n\r\n    def resize(self, factor, factor_y = None):\r\n        w = self.width()\r\n        h = self.height()\r\n        if factor_y is None:\r\n            factor_y = factor\r\n        centerX = float(self.x1+self.x2)/2.0\r\n        centerY = float(self.y1+self.y2)/2.0\r\n        self.x1 = (centerX - (w/2.0)*factor)\r\n        self.y1 = (centerY - (h/2.0)*factor_y)\r\n        self.x2 = (centerX + (w/2.0)*factor)\r\n        self.y2 = (centerY + (h/2.0)*factor_y)\r\n        \r\n\r\n    def intersection(self, other):\r\n        self.sortCoords()\r\n        other.sortCoords()\r\n        \r\n        if(self.x1 >= other.x2):\r\n            return (0, 0)           \r\n        if(self.x2 <= other.x1):\r\n            return (0, 0)\r\n        if(self.y1 >= other.y2):\r\n            return (0, 0)   \r\n        if(self.y2 <= other.y1):\r\n            return (0, 0)\r\n    \r\n        l = max(self.x1, other.x1);\r\n        t = max(self.y1, other.y1);\r\n        r = min(self.x2, other.x2);\r\n        b = min(self.y2, other.y2);\r\n        return (r - l, b - t)\r\n        \r\n        #Alternate implementation\r\n        #nWidth  = self.x2 - self.x1\r\n        #nHeight = self.y2 - self.y1\r\n        #iWidth  = max(0,min(max(0,other.x2-self.x1),nWidth )-max(0,other.x1-self.x1))\r\n        #iHeight = max(0,min(max(0,other.y2-self.y1),nHeight)-max(0,other.y1-self.y1))\r\n        #return (iWidth, iHeight)\r\n\r\n    def cover(self, other):\r\n        nWidth = self.width()\r\n        nHeight = self.height()\r\n        iWidth, iHeight = self.intersection(other)              \r\n        return float(iWidth * iHeight) / float(nWidth * nHeight)\r\n\r\n    def overlap_pascal(self, other):\r\n        self.sortCoords()\r\n        other.sortCoords()\r\n\r\n        nWidth  = self.x2 - self.x1\r\n        nHeight = self.y2 - self.y1\r\n        iWidth, iHeight = self.intersection(other)\r\n        interSection = iWidth * iHeight\r\n                \r\n        union = self.width() * self.height() + other.width() * other.height() - interSection\r\n                \r\n        overlap = interSection * 1.0 / union\r\n        return overlap\r\n\r\n    def isMatchingPascal(self, other, minOverlap):\r\n        overlap = self.overlap_pascal(other)\r\n        if (overlap >= minOverlap and (self.classID == -1 or other.classID == -1 or self.classID == other.classID)):\r\n            return 1\r\n        else:\r\n            return 0\r\n\r\n    def distance(self, other, aspectRatio=-1, fixWH=\'fixheight\'):\r\n        if (aspectRatio!=-1):\r\n            if (fixWH==\'fixwidth\'):\r\n                dWidth  = float(self.x2 - self.x1)\r\n                dHeight = dWidth / aspectRatio\r\n            elif (fixWH==\'fixheight\'):\r\n                dHeight = float(self.y2 - self.y1)\r\n                dWidth  = dHeight * aspectRatio\r\n        else:\r\n            dWidth  = float(self.x2 - self.x1)\r\n            dHeight = float(self.y2 - self.y1)\r\n\r\n        xdist   = (self.x1 + self.x2 - other.x1 - other.x2) / dWidth\r\n        ydist   = (self.y1 + self.y2 - other.y1 - other.y2) / dHeight\r\n\r\n        return sqrt(xdist*xdist + ydist*ydist)\r\n\r\n    def isMatchingStd(self, other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1):\r\n        cover = other.cover(self)\r\n        overlap = self.cover(other)\r\n        dist = self.distance(other, aspectRatio, fixWH)\r\n\r\n        #if(self.width() == 24 ):\r\n        #print cover, "" "", overlap, "" "", dist\r\n        #print coverThresh, overlapThresh, distThresh\r\n        #print (cover>=coverThresh and overlap>=overlapThresh and dist<=distThresh)\r\n        \r\n        if (cover>=coverThresh and overlap>=overlapThresh and dist<=distThresh and self.classID == other.classID):\r\n            return 1\r\n        else:\r\n            return 0\r\n\r\n    def isMatching(self, other, style, coverThresh, overlapThresh, distThresh, minOverlap, aspectRatio=-1, fixWH=-1):\r\n        #choose matching style\r\n        if (style == 0):\r\n            return self.isMatchingStd(other, coverThresh, overlapThresh, distThresh, aspectRatio=-1, fixWH=-1)\r\n\r\n        if (style == 1):\r\n            return self.isMatchingPascal(other, minOverlap)\r\n\r\n    def addToXML(self, node, doc): # no Silhouette yet\r\n        rect_el = doc.createElement(""annorect"")\r\n        for item in ""x1 y1 x2 y2 score scale track_id"".split():\r\n            coord_el = doc.createElement(item)\r\n            coord_val = doc.createTextNode(str(self.__getattribute__(item)))\r\n            coord_el.appendChild(coord_val)\r\n            rect_el.appendChild(coord_el)\r\n            \r\n        articulation_el = doc.createElement(""articulation"")\r\n        for articulation in self.articulations:\r\n            id_el = doc.createElement(""id"")\r\n            id_val = doc.createTextNode(str(articulation))\r\n            id_el.appendChild(id_val)\r\n            articulation_el.appendChild(id_el)\r\n        if(len(self.articulations) > 0):\r\n            rect_el.appendChild(articulation_el)\r\n            \r\n        viewpoint_el    = doc.createElement(""viewpoint"")\r\n        for viewpoint in self.viewpoints:\r\n            id_el = doc.createElement(""id"")\r\n            id_val = doc.createTextNode(str(viewpoint))\r\n            id_el.appendChild(id_val)\r\n            viewpoint_el.appendChild(id_el)\r\n        if(len(self.viewpoints) > 0):\r\n            rect_el.appendChild(viewpoint_el)\r\n    \r\n        d3_el    = doc.createElement(""D3"")                                      \r\n        for d in self.d3:\r\n            id_el = doc.createElement(""id"")\r\n            id_val = doc.createTextNode(str(d))\r\n            id_el.appendChild(id_val)\r\n            d3_el.appendChild(id_el)\r\n        if(len(self.d3) > 0):\r\n            rect_el.appendChild(d3_el)\r\n                            \r\n        if self.silhouetteID != -1:\r\n            silhouette_el    = doc.createElement(""silhouette"")\r\n            id_el = doc.createElement(""id"")\r\n            id_val = doc.createTextNode(str(self.silhouetteID))\r\n            id_el.appendChild(id_val)\r\n            silhouette_el.appendChild(id_el)\r\n            rect_el.appendChild(silhouette_el)\r\n\r\n        if self.classID != -1:\r\n            class_el    = doc.createElement(""classID"")\r\n            class_val = doc.createTextNode(str(self.classID))\r\n            class_el.appendChild(class_val)\r\n            rect_el.appendChild(class_el)\r\n\r\n        if len(self.point) > 0:\r\n            annopoints_el = doc.createElement(""annopoints"")\r\n\r\n            for p in self.point:\r\n                point_el = doc.createElement(""point"");\r\n                \r\n                point_id_el = doc.createElement(""id"");\r\n                point_id_val = doc.createTextNode(str(p.id));\r\n                point_id_el.appendChild(point_id_val);\r\n                point_el.appendChild(point_id_el);\r\n\r\n                point_x_el = doc.createElement(""x"");\r\n                point_x_val = doc.createTextNode(str(p.x));\r\n                point_x_el.appendChild(point_x_val);\r\n                point_el.appendChild(point_x_el);\r\n\r\n                point_y_el = doc.createElement(""y"");\r\n                point_y_val = doc.createTextNode(str(p.y));\r\n                point_y_el.appendChild(point_y_val);\r\n                point_el.appendChild(point_y_el);\r\n\r\n                annopoints_el.appendChild(point_el);\r\n        \r\n            rect_el.appendChild(annopoints_el);\r\n            \r\n        node.appendChild(rect_el)\r\n\r\n\r\n\r\nclass Annotation(object):\r\n\r\n    def __init__(self):\r\n        self.imageName = """"\r\n        self.imagePath = """"\r\n        self.rects =[]\r\n        self.frameNr = -1\r\n\r\n    def clone_empty(self):\r\n        new = Annotation()\r\n        new.imageName = self.imageName\r\n        new.imagePath = self.imagePath\r\n        new.frameNr   = self.frameNr\r\n        new.rects     = []\r\n        return new\r\n\r\n    def filename(self):\r\n        return os.path.join(self.imagePath, self.imageName)\r\n\r\n    def printContent(self):\r\n        print ""Name: "", self.imageName\r\n        for rect in self.rects:\r\n            rect.printContent()\r\n\r\n    def writeIDL(self, file):\r\n        if (self.frameNr == -1):\r\n            file.write(""\\""""+os.path.join(self.imagePath, self.imageName)+""\\"""")\r\n        else:\r\n            file.write(""\\""""+os.path.join(self.imagePath, self.imageName)+""@%d\\"""" % self.frameNr)\r\n\r\n        if (len(self.rects)>0):\r\n            file.write("":"")\r\n        i=0\r\n        for rect in self.rects:\r\n            rect.writeIDL(file)\r\n            if (i+1<len(self.rects)):\r\n                file.write("","")\r\n            i+=1\r\n\r\n    def writeJSON(self):\r\n        jdoc = {}\r\n        jdoc[\'image_path\'] = os.path.join(self.imagePath, self.imageName)\r\n        jdoc[\'rects\'] = []\r\n        for rect in self.rects:\r\n            jdoc[\'rects\'].append(rect.writeJSON())\r\n        return jdoc\r\n\r\n    def addToXML(self, node, doc): # no frame# yet\r\n        annotation_el = doc.createElement(""annotation"")\r\n        img_el = doc.createElement(""image"")\r\n        name_el = doc.createElement(""name"")             \r\n        name_val = doc.createTextNode(os.path.join(self.imagePath, self.imageName))\r\n        name_el.appendChild(name_val)\r\n        img_el.appendChild(name_el)\r\n        \r\n        if(self.frameNr != -1):\r\n            frame_el = doc.createElement(""frameNr"")\r\n            frame_val = doc.createTextNode(str(self.frameNr))\r\n            frame_el.appendChild(frame_val)\r\n            img_el.appendChild(frame_el)\r\n    \r\n        annotation_el.appendChild(img_el)\r\n        for rect in self.rects:\r\n            rect.addToXML(annotation_el, doc)\r\n        node.appendChild(annotation_el)\r\n\r\n\r\n    def sortByScore(self, dir=""ascending""):\r\n        if (dir==""descending""):\r\n            self.rects.sort(cmpAnnoRectsByScoreDescending)\r\n        else:\r\n            self.rects.sort(cmpAnnoRectsByScore)\r\n\r\n    def __getitem__(self, index):\r\n        return self.rects[index]\r\n\r\nclass detAnnoRect:\r\n    def __init(self):\r\n        self.imageName = """"\r\n        self.frameNr = -1\r\n        self.rect = AnnoRect()\r\n        self.imageIndex = -1\r\n        self.boxIndex = -1\r\n\r\n#####################################################################\r\n### Parsing\r\n\r\ndef parseTii(filename):\r\n\r\n    # MA: this must be some really old code\r\n    assert(False);\r\n    annotations = []\r\n\r\n    #--- parse xml ---#\r\n    doc = xml.dom.minidom.parse(filename)\r\n\r\n    #--- get tags ---#\r\n    for file in doc.getElementsByTagName(""file""):\r\n\r\n        anno = Annotation()\r\n\r\n        for filename in file.getElementsByTagName(""filename""):\r\n            aNode = filename.getAttributeNode(""Src"")\r\n            anno.imageName = aNode.firstChild.data[:-4]+"".png""\r\n\r\n        for objects in file.getElementsByTagName(""objects""):\r\n\r\n            for vehicle in objects.getElementsByTagName(""vehicle""):\r\n\r\n                aNode = vehicle.getAttributeNode(""Type"")\r\n                type = aNode.firstChild.data\r\n\r\n                if (type==""pedestrian""):\r\n\r\n                    rect = AnnoRect()\r\n                    aNode = vehicle.getAttributeNode(""FR"")\r\n                    frontrear = aNode.firstChild.data\r\n                    aNode = vehicle.getAttributeNode(""SD"")\r\n                    side = aNode.firstChild.data\r\n                    if (frontrear == ""1""):\r\n                        orientation=""FR""\r\n                    elif (side == ""1""):\r\n                        orientation=""SD""\r\n                    aNode = vehicle.getAttributeNode( orientation+""_TopLeft_X"")\r\n                    rect.x1 = float(aNode.firstChild.data)\r\n                    aNode = vehicle.getAttributeNode( orientation+""_TopLeft_Y"")\r\n                    rect.y1 = float(aNode.firstChild.data)\r\n                    aNode = vehicle.getAttributeNode( orientation+""_BottomRight_X"")\r\n                    rect.x2 = float(aNode.firstChild.data)\r\n                    aNode = vehicle.getAttributeNode( orientation+""_BottomRight_Y"")\r\n                    rect.y2 = float(aNode.firstChild.data)\r\n                    print ""pedestrian:"", anno.imageName, rect.x1, rect.y1, rect.x2, rect.y2\r\n                    anno.rects.append(rect)\r\n\r\n        annotations.append(anno)\r\n\r\n    return annotations\r\n\r\ndef parseXML(filename):\r\n    filename = os.path.realpath(filename)\r\n\r\n    name, ext = os.path.splitext(filename)\r\n\r\n    annotations = AnnoList([])\r\n\r\n    if(ext == "".al""):\r\n        file = open(filename,\'r\')\r\n        lines = file.read()\r\n        file.close()\r\n\r\n    if(ext == "".gz""):\r\n        zfile = gzip.GzipFile(filename)\r\n        lines = zfile.read()\r\n        zfile.close()\r\n\r\n    if(ext == "".bz2""):\r\n        bfile = bz2.BZ2File(filename)\r\n        lines = bfile.read()\r\n        bfile.close()\r\n\r\n    #--- parse xml ---#\r\n    doc = xml.dom.minidom.parseString(lines)\r\n\r\n    #--- get tags ---#\r\n    for annotation in doc.getElementsByTagName(""annotation""):\r\n        anno = Annotation()\r\n        for image in annotation.getElementsByTagName(""image""):\r\n            for name in image.getElementsByTagName(""name""):\r\n                anno.imageName = name.firstChild.data\r\n\r\n            for fn in image.getElementsByTagName(""frameNr""):\r\n                anno.frameNr = int(fn.firstChild.data)\r\n\r\n        rects = []\r\n        for annoRect in annotation.getElementsByTagName(""annorect""):\r\n            rect = AnnoRect()\r\n\r\n            for x1 in annoRect.getElementsByTagName(""x1""):\r\n                rect.x1 = float(x1.firstChild.data)\r\n\r\n            for y1 in annoRect.getElementsByTagName(""y1""):\r\n                rect.y1 = float(y1.firstChild.data)\r\n\r\n            for x2 in annoRect.getElementsByTagName(""x2""):\r\n                rect.x2 = float(x2.firstChild.data)\r\n\r\n            for y2 in annoRect.getElementsByTagName(""y2""):\r\n                rect.y2 = float(y2.firstChild.data)\r\n\r\n            for scale in annoRect.getElementsByTagName(""scale""):\r\n                rect.scale = float(scale.firstChild.data)\r\n\r\n            for score in annoRect.getElementsByTagName(""score""):\r\n                rect.score = float(score.firstChild.data)\r\n\r\n            for classID in annoRect.getElementsByTagName(""classID""):\r\n                rect.classID = int(classID.firstChild.data)\r\n\r\n            for track_id in annoRect.getElementsByTagName(""track_id""):\r\n                rect.track_id = int(track_id.firstChild.data)\r\n\r\n            for articulation in annoRect.getElementsByTagName(""articulation""):\r\n                for id in articulation.getElementsByTagName(""id""):\r\n                    rect.articulations.append(int(id.firstChild.data))\r\n                #print ""Articulations: "", rect.articulations\r\n\r\n            for viewpoint in annoRect.getElementsByTagName(""viewpoint""):\r\n                for id in viewpoint.getElementsByTagName(""id""):\r\n                    rect.viewpoints.append(int(id.firstChild.data))\r\n                    #print ""Viewpoints: "", rect.viewpoints\r\n                    \r\n            for d in annoRect.getElementsByTagName(""D3""):\r\n                for id in d.getElementsByTagName(""id""):\r\n                    rect.d3.append(float(id.firstChild.data))\r\n\r\n            for silhouette in annoRect.getElementsByTagName(""silhouette""):\r\n                for id in silhouette.getElementsByTagName(""id""):\r\n                    rect.silhouetteID = int(id.firstChild.data)\r\n                #print ""SilhouetteID: "", rect.silhouetteID\r\n\r\n            for annoPoints in annoRect.getElementsByTagName(""annopoints""):                          \r\n                for annoPoint in annoPoints.getElementsByTagName(""point""):\r\n\r\n                    p = AnnoPoint();\r\n                    for annoPointX in annoPoint.getElementsByTagName(""x""):\r\n                        p.x = int(float(annoPointX.firstChild.data));\r\n\r\n                    for annoPointY in annoPoint.getElementsByTagName(""y""):\r\n                        p.y = int(float(annoPointY.firstChild.data));\r\n                        \r\n                    for annoPointId in annoPoint.getElementsByTagName(""id""):\r\n                        p.id = int(annoPointId.firstChild.data);\r\n\r\n                    assert(p.x != None and p.y != None and p.id != None);\r\n                    rect.point.append(p);                                   \r\n\r\n            rects.append(rect)\r\n\r\n        anno.rects = rects\r\n        annotations.append(anno)\r\n\r\n    return annotations\r\n\r\ndef parseJSON(filename):\r\n    filename = os.path.realpath(filename)\r\n    name, ext = os.path.splitext(filename)\r\n    assert ext == \'.json\'\r\n\r\n    annotations = AnnoList([])\r\n    with open(filename, \'r\') as f:\r\n        jdoc = json.load(f)\r\n\r\n    for annotation in jdoc:\r\n        anno = Annotation()\r\n        anno.imageName = annotation[""image_path""]\r\n\r\n        rects = []\r\n        for annoRect in annotation[""rects""]:\r\n            rect = AnnoRect()\r\n\r\n            rect.x1 = annoRect[""x1""]\r\n            rect.x2 = annoRect[""x2""]\r\n            rect.y1 = annoRect[""y1""]\r\n            rect.y2 = annoRect[""y2""]\r\n            if ""score"" in annoRect:\r\n                rect.score = annoRect[""score""]\r\n\r\n            rects.append(rect)\r\n\r\n        anno.rects = rects\r\n        annotations.append(anno)\r\n\r\n    return annotations\r\n    \r\ndef parse(filename, abs_path=False):\r\n    #print ""Parsing: "", filename\r\n    name, ext = os.path.splitext(filename)\r\n    \r\n    if (ext == "".gz"" or ext == "".bz2""):\r\n        name, ext = os.path.splitext(name)\r\n\r\n    if(ext == "".idl""):\r\n        annolist = parseIDL(filename)\r\n    elif(ext == "".al""):\r\n        annolist = parseXML(filename)\r\n    elif(ext == "".pal""):\r\n        annolist = PalLib.pal2al(PalLib.loadPal(filename));\r\n    elif(ext == "".json""):\r\n        annolist = parseJSON(filename)\r\n    else:\r\n        annolist = AnnoList([]);\r\n\r\n    if abs_path:\r\n        basedir = os.path.dirname(os.path.abspath(filename))\r\n        for a in annolist:\r\n            a.imageName = basedir + ""/"" + os.path.basename(a.imageName)\r\n\r\n    return annolist\r\n\r\n\r\ndef parseIDL(filename):\r\n    filename = os.path.realpath(filename)\r\n\r\n    name, ext = os.path.splitext(filename)\r\n\r\n    lines = []\r\n    if(ext == "".idl""):\r\n        file = open(filename,\'r\')\r\n        lines = file.readlines()\r\n        file.close()\r\n\r\n    if(ext == "".gz""):\r\n        zfile = gzip.GzipFile(filename)\r\n        lines = zfile.readlines()\r\n        zfile.close()\r\n\r\n    if(ext == "".bz2""):\r\n        bfile = bz2.BZ2File(filename)\r\n        lines = bfile.readlines()\r\n        bfile.close()\r\n\r\n    annotations = AnnoList([])\r\n\r\n    for line in lines:\r\n        anno = Annotation()\r\n\r\n        ### remove line break\r\n        if (line[-1]==\'\\n\'):\r\n            line = line[:-1]; # remove \'\\n\'\r\n        lineLen = len(line)\r\n        #print line\r\n\r\n        ### get image name\r\n        posImageEnd = line.find(\'\\"":\')\r\n        if (posImageEnd==-1):\r\n            posImageEnd = line.rfind(""\\"""")\r\n        anno.imageName = line[1:posImageEnd]\r\n        #print anno.imageName\r\n\r\n        pos = anno.imageName.rfind(""@"")\r\n        if (pos >= 0):\r\n            anno.frameNr = int(anno.imageName[pos+1:])\r\n            anno.imageName = anno.imageName[:pos]\r\n            if anno.imageName[-1] == ""/"":\r\n                anno.imageName = anno.imageName[:-1]\r\n        else:\r\n            anno.frameNr = -1\r\n\r\n        ### get rect list\r\n        # we split by \',\'. there are 3 commas for each rect and 1 comma seperating the rects\r\n        rectSegs=[]\r\n        if (posImageEnd!=-1 and posImageEnd+4<lineLen):\r\n\r\n            line = line[posImageEnd+3:-1]; # remove ; or .\r\n\r\n            segments = line.split(\',\')\r\n            if (len(segments)%4!=0):\r\n                print ""Parse Errror""\r\n            else:\r\n                for i in range(0,len(segments),4):\r\n                    rectSeg = segments[i]+"",""+segments[i+1]+"",""+segments[i+2]+"",""+segments[i+3]\r\n                    rectSegs.append(rectSeg)\r\n                    #print rectSegs\r\n\r\n            ## parse rect segments\r\n            for rectSeg in rectSegs:\r\n                #print ""RectSeg: "", rectSeg\r\n                rect = AnnoRect()\r\n                posBracket1 = rectSeg.find(\'(\')\r\n                posBracket2 = rectSeg.find(\')\')\r\n                coordinates = rectSeg[posBracket1+1:posBracket2].split(\',\')\r\n                #print coordinates\r\n                #print ""Coordinates: "",coordinates                              \r\n                rect.x1 = float(round(float(coordinates[0].strip())))\r\n                rect.y1 = float(round(float(coordinates[1].strip())))\r\n                rect.x2 = float(round(float(coordinates[2].strip())))\r\n                rect.y2 = float(round(float(coordinates[3].strip())))\r\n                posColon = rectSeg.find(\':\')\r\n                posSlash = rectSeg.find(\'/\')\r\n                if (posSlash!=-1):\r\n                    rect.silhouetteID = int(rectSeg[posSlash+1:])\r\n                else:\r\n                    rectSeg+=""\\n""\r\n                if (posColon!=-1):\r\n                    #print rectSeg[posColon+1:posSlash]\r\n                    rect.score = float(rectSeg[posColon+1:posSlash])\r\n                anno.rects.append(rect)\r\n\r\n        annotations.append(anno)\r\n\r\n    return annotations\r\n\r\n\r\n\r\n    \r\n\r\n#####################################################################\r\n### Saving\r\n\r\ndef save(filename, annotations):\r\n    print ""saving: "", filename;\r\n\r\n    name, ext = os.path.splitext(filename)\r\n\r\n    if (ext == "".gz"" or ext == "".bz2""):\r\n        name, ext = os.path.splitext(name)\r\n\r\n    if(ext == "".idl""):\r\n        return saveIDL(filename, annotations)           \r\n\r\n    elif(ext == \'.json\'):\r\n        return saveJSON(filename, annotations)\r\n\r\n    elif(ext == "".al""):\r\n        return saveXML(filename, annotations)\r\n\r\n    elif(ext == "".pal""):\r\n        return PalLib.savePal(filename, PalLib.al2pal(annotations));\r\n\r\n\r\n    else:\r\n        assert(False);\r\n        return False;\r\n\r\ndef saveIDL(filename, annotations):\r\n    [name, ext] = os.path.splitext(filename)\r\n\r\n    if(ext == "".idl""):\r\n        file = open(filename,\'w\')\r\n\r\n    if(ext == "".gz""):\r\n        file = gzip.GzipFile(filename, \'w\')\r\n\r\n    if(ext == "".bz2""):\r\n        file = bz2.BZ2File(filename, \'w\')\r\n\r\n    i=0\r\n    for annotation in annotations:\r\n        annotation.writeIDL(file)\r\n        if (i+1<len(annotations)):\r\n            file.write("";\\n"")\r\n        else:\r\n            file.write("".\\n"")\r\n        i+=1\r\n\r\n    file.close()\r\n\r\ndef saveJSON(filename, annotations):\r\n    [name, ext] = os.path.splitext(filename)\r\n\r\n    jdoc = []\r\n    for annotation in annotations:\r\n        jdoc.append(annotation.writeJSON())\r\n\r\n    with open(filename, \'w\') as f:\r\n        f.write(json.dumps(jdoc, indent=2, sort_keys=True))\r\n\r\n\r\ndef idlBase(filename):\r\n    if (filename.rfind("".pal"") == len(filename) - 4):\r\n        return (filename[:-4], "".pal"")\r\n\r\n    if (filename.rfind("".json"") == len(filename) - 5):\r\n        return (filename[:-5], "".json"")\r\n\r\n    if (filename.rfind("".idl"") == len(filename) - 4):\r\n        return (filename[:-4], "".idl"")\r\n\r\n    if (filename.rfind("".al"") == len(filename) - 3):\r\n        return (filename[:-3], "".al"")\r\n\r\n    if (filename.rfind("".idl.gz"") == len(filename) - 7):\r\n        return (filename[:-7], "".idl.gz"")\r\n\r\n    if (filename.rfind("".idl.bz2"") == len(filename) - 8):\r\n        return (filename[:-8], "".idl.bz2"")\r\n\r\n    if (filename.rfind("".al.gz"") == len(filename) - 6):\r\n        return (filename[:-6], "".al.gz"")\r\n\r\n    if (filename.rfind("".al.bz2"") == len(filename) - 7):\r\n        return (filename[:-7], "".al.bz2"")\r\n\r\ndef saveXML(filename, annotations):\r\n    document = xml.dom.minidom.Document()\r\n    rootnode = document.createElement(""annotationlist"")\r\n    for anno in annotations:\r\n        anno.addToXML(rootnode, document)\r\n    document.appendChild(rootnode)\r\n    [name, ext] = os.path.splitext(filename)\r\n    if(ext == "".al""):\r\n        writer = open(filename,\'w\')\r\n    elif(ext == "".gz""):\r\n        writer = gzip.GzipFile(filename, \'w\')\r\n    elif(ext == "".bz2""):\r\n        writer = bz2.BZ2File(filename, \'w\')\r\n    else:\r\n        print ""invalid filename - .al(.gz|.bz2) is accepted""\r\n        return\r\n\r\n\r\n    if xml_dom_ext_available:\r\n        xml.dom.ext.PrettyPrint(document, writer)\r\n    else:\r\n        # MA: skip header (currently Matlab\'s loadannotations can\'t deal with the header)\r\n        document.documentElement.writexml(writer);\r\n\r\n        #document.writexml(writer)\r\n\r\n    document.unlink()\r\n\r\n\r\n\r\n\r\n\r\n#####################################################################\r\n### Statistics\r\n\r\ndef getStats(annotations):\r\n    no = 0\r\n    noTiny =0\r\n    noSmall =0\r\n    heights = []\r\n    widths =[]\r\n\r\n    ###--- get all rects ---###\r\n    for anno in annotations:\r\n        no = no + len(anno.rects)\r\n        for rect in anno.rects:\r\n            if (rect.height()<36):\r\n                noTiny=noTiny+1\r\n            if (rect.height()<128):\r\n                noSmall=noSmall+1\r\n            heights.append(rect.height())\r\n            if (rect.width()==0):\r\n                print ""Warning: width=0 in image "", anno.imageName\r\n                widths.append(1)\r\n            else:\r\n                widths.append(rect.width())\r\n                if (float(rect.height())/float(rect.width())<1.5):\r\n                    print ""Degenerated pedestrian annotation: "", anno.imageName\r\n\r\n    ###--- compute average height and variance ---###\r\n    avgHeight = 0\r\n    varHeight = 0\r\n\r\n\r\n    minHeight = 0\r\n    maxHeight = 0\r\n    if len(heights) > 0:\r\n        minHeight = heights[0]\r\n        maxHeight = heights[0]\r\n\r\n    for height in heights:\r\n        avgHeight = avgHeight+height\r\n        if (height > maxHeight):\r\n            maxHeight = height\r\n        if (height < minHeight):\r\n            minHeight = height\r\n\r\n    if (no>0):\r\n        avgHeight = avgHeight/no\r\n    for height in heights:\r\n        varHeight += (height-avgHeight)*(height-avgHeight)\r\n    if (no>1):\r\n        varHeight=float(varHeight)/float(no-1)\r\n\r\n    ###--- compute average width and variance ---###\r\n    avgWidth = 0\r\n    varWidth = 0\r\n    for width in widths:\r\n        avgWidth = avgWidth+width\r\n    if (no>0):\r\n        avgWidth = avgWidth/no\r\n    for width in widths:\r\n        varWidth += (width-avgWidth)*(width-avgWidth)\r\n\r\n    if (no>1):\r\n        varWidth=float(varWidth)/float(no-1)\r\n\r\n    ###--- write statistics ---###\r\n    print ""  Total # rects:"", no\r\n    print ""     avg. Width:"", avgWidth, "" ("", sqrt(varWidth), ""standard deviation )""\r\n    print ""    avg. Height:"", avgHeight, "" ("", sqrt(varHeight), ""standard deviation )""\r\n    print ""     tiny rects:"", noTiny, "" (< 36 pixels)""\r\n    print ""    small rects:"", noSmall, "" (< 128 pixels)""\r\n    print ""    minimum height:"", minHeight\r\n    print ""    maximum height:"", maxHeight\r\n\r\n    ###--- return ---###\r\n    return [widths, heights]\r\n\r\n############################################################\r\n##\r\n##  IDL merging\r\n##\r\n\r\ndef mergeIDL(detIDL, det2IDL, detectionFuse= True, minOverlap = 0.5):\r\n    mergedIDL = []\r\n\r\n    for i,anno in enumerate(detIDL):\r\n        mergedAnno = Annotation()\r\n        mergedAnno.imageName = anno.imageName\r\n        mergedAnno.frameNr = anno.frameNr\r\n        mergedAnno.rects = anno.rects\r\n\r\n        imageFound = False\r\n        filterIndex = -1\r\n        for i,filterAnno in enumerate(det2IDL):\r\n            if (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\r\n                filterIndex = i\r\n                imageFound = True\r\n                break\r\n\r\n        if(not imageFound):\r\n            mergedIDL.append(mergedAnno)\r\n            continue\r\n\r\n        for rect in det2IDL[filterIndex].rects:\r\n            matches = False\r\n\r\n            for frect in anno.rects:\r\n                if rect.overlap_pascal(frect) > minOverlap:\r\n                    matches = True\r\n                    break\r\n\r\n            if (not matches or detectionFuse == False):\r\n                mergedAnno.rects.append(rect)\r\n\r\n        mergedIDL.append(mergedAnno)\r\n\r\n    return mergedIDL\r\n\r\n\r\n############################################################################33\r\n#\r\n# Function to force the aspect ratio of annotations to ratio = width / height\r\n#\r\n#\r\ndef forceAspectRatio(annotations, ratio, KeepHeight = False, KeepWidth = False):\r\n    for anno in annotations:\r\n        for rect in anno.rects:\r\n            rect.forceAspectRatio(ratio, KeepHeight, KeepWidth)\r\n            #Determine which side needs to be extended\r\n#                       if (rect.width() * 1.0 / rect.height() > ratio):\r\n#\r\n#                               #Too wide -> extend height\r\n#                               newHeight = rect.width() * 1.0 / ratio\r\n#                               rect.y1 = int(rect.centerY() - newHeight / 2.0)\r\n#                               rect.y2 = int(rect.y1 + newHeight)\r\n#\r\n#                       else:\r\n#                               #Too short -> extend width\r\n#                               newWidth = rect.height() * ratio\r\n#                               rect.x1 = int(rect.centerX() - newWidth / 2.0)\r\n#                               rect.x2 = int(rect.x1 + newWidth)\r\n\r\n\r\n###################################################################\r\n# Function to greedyly remove subset detIDL from gtIDL\r\n#\r\n# returns two sets\r\n#\r\n# [filteredIDL, missingRecallIDL]\r\n#\r\n# filteredIDL == Rects that were present in both sets\r\n# missingRecallIDL == Rects that were only present in set gtIDL\r\n#\r\n###################################################################\r\ndef extractSubSet(gtIDL, detIDL):\r\n    filteredIDL = []\r\n    missingRecallIDL = []\r\n\r\n    for i,gtAnno in enumerate(gtIDL):\r\n        filteredAnno = Annotation()\r\n        filteredAnno.imageName = gtAnno.imageName\r\n        filteredAnno.frameNr = gtAnno.frameNr\r\n\r\n        missingRecallAnno = Annotation()\r\n        missingRecallAnno.imageName = gtAnno.imageName\r\n        missingRecallAnno.frameNr = gtAnno.frameNr\r\n\r\n        imageFound = False\r\n        filterIndex = -1\r\n        for i,anno in enumerate(detIDL):\r\n            if (suffixMatch(anno.imageName, gtAnno.imageName) and anno.frameNr == gtAnno.frameNr):\r\n                filterIndex = i\r\n                imageFound = True\r\n                break\r\n\r\n        if(not imageFound):\r\n            print ""Image not found "" + gtAnno.imageName + "" !""\r\n            missingRecallIDL.append(gtAnno)\r\n            filteredIDL.append(filteredAnno)\r\n            continue\r\n\r\n        matched = [-1] * len(detIDL[filterIndex].rects)\r\n        for j, rect in enumerate(gtAnno.rects):\r\n            matches = False\r\n\r\n            matchingID = -1\r\n            minCenterPointDist = -1\r\n            for k,frect in enumerate(detIDL[filterIndex].rects):\r\n                minCover = 0.5\r\n                minOverlap = 0.5\r\n                maxDist = 0.5\r\n\r\n                if rect.isMatchingStd(frect, minCover,minOverlap, maxDist):\r\n                    if (matchingID == -1 or rect.distance(frect) < minCenterPointDist):\r\n                        matchingID = k\r\n                        minCenterPointDist = rect.distance(frect)\r\n                        matches = True\r\n\r\n            if (matches):\r\n                #Already matched once check if you are the better match\r\n                if(matched[matchingID] >= 0):\r\n                    #Take the match with the smaller center point distance\r\n                    if(gtAnno.rects[matched[matchingID]].distance(frect) > rect.distance(frect)):\r\n                        missingRecallAnno.rects.append(gtAnno.rects[matched[matchingID]])\r\n                        filteredAnno.rects.remove(gtAnno.rects[matched[matchingID]])\r\n                        filteredAnno.rects.append(rect)\r\n                        matched[matchingID] = j\r\n                    else:\r\n                        missingRecallAnno.rects.append(rect)\r\n                else:\r\n                    #Not matched before.. go on and add the match\r\n                    filteredAnno.rects.append(rect)\r\n                    matched[matchingID] = j\r\n            else:\r\n                missingRecallAnno.rects.append(rect)\r\n\r\n        filteredIDL.append(filteredAnno)\r\n        missingRecallIDL.append(missingRecallAnno)\r\n\r\n    return (filteredIDL     , missingRecallIDL)\r\n\r\n###########################################################\r\n#\r\n#  Function to remove all detections with a too low score\r\n#\r\n#\r\ndef filterMinScore(detections, minScore):\r\n    newDetections = []\r\n    for anno in detections:\r\n        newAnno = Annotation()\r\n        newAnno.frameNr = anno.frameNr\r\n        newAnno.imageName = anno.imageName\r\n        newAnno.imagePath = anno.imagePath\r\n        newAnno.rects = []\r\n\r\n        for rect in anno.rects:\r\n            if(rect.score >= minScore):\r\n                newAnno.rects.append(rect)\r\n\r\n        newDetections.append(newAnno)\r\n    return newDetections\r\n\r\n# foo.idl -> foo-suffix.idl, foo.idl.gz -> foo-suffix.idl.gz etc\r\ndef suffixIdlFileName(filename, suffix):\r\n    exts = ["".idl"", "".idl.gz"", "".idl.bz2""]\r\n    for ext in exts:\r\n        if filename.endswith(ext):\r\n            return filename[0:-len(ext)] + ""-"" + suffix + ext\r\n    raise ValueError(""this does not seem to be a valid filename for an idl-file"")\r\n\r\nif __name__ == ""__main__"":\r\n# test output\r\n    idl = parseIDL(""/tmp/asdf.idl"")\r\n    idl[0].rects[0].articulations = [4,2]\r\n    idl[0].rects[0].viewpoints = [2,3]\r\n    saveXML("""", idl)\r\n\r\n\r\ndef annoAnalyze(detIDL):\r\n    allRects = []\r\n    \r\n    for i,anno in enumerate(detIDL):\r\n        for j in anno.rects:\r\n            newRect = detAnnoRect()\r\n            newRect.imageName = anno.imageName\r\n            newRect.frameNr = anno.frameNr\r\n            newRect.rect = j\r\n            allRects.append(newRect)\r\n\r\n    allRects.sort(cmpDetAnnoRectsByScore)\r\n    \r\n    filteredIDL = AnnoList([])\r\n    for i in allRects:\r\n        a = Annotation()\r\n        a.imageName = i.imageName\r\n        a.frameNr = i.frameNr\r\n        a.rects = []\r\n        a.rects.append(i.rect)\r\n        filteredIDL.append(a)\r\n        \r\n    return filteredIDL\r\n\r\n\r\n'"
linux/tensorbox/utils/annolist/MatPlotter.py,0,"b'import os\nimport sys\nimport string\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom pylab import *\nimport numpy as np\n\nclass MatPlotter:\n    fontsize=15\n    color=0\n    colors=[""r-"", ""b-"", ""k-"", ""c-"", ""m-"", ""y-""]\n    colors+=[x + ""-"" for x in colors]\n    colors+=[""g-"", ""g--""]\n    curFigure=[]\n    legendNames=[]\n    fontsizeLegend=14\n    legendPlace=\'lower right\'\n    legendborderpad = None\n    legendlabelsep = None\n\n\n    def __init__(self, fontsize=15):\n        # self.newFigure()\n        self.fontsize=fontsize\n        self.fontsizeLegend=fontsize - 1\n        pass\n\n    def formatLegend(self, newFontSize = 14, newPlace = \'lower right\', borderpad = None, labelsep = None):\n        self.fontsizeLegend=newFontSize\n        self.legendPlace=newPlace\n        self.legendborderpad = borderpad\n        self.legendlabelsep = labelsep\n\n    def newFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n        return self.newRPCFigure(plotTitle, fsize)\n\n    def newRPCFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n        curFigure = figure(figsize=fsize)\n        self.title = title(plotTitle, fontsize=self.fontsize)\n        #subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n        subplots_adjust(right=0.975, top=0.975)\n\n        #axis(\'equal\')\n        axis([0, 1, 0, 1])\n        xticklocs, xticklabels = xticks(arange(0, 1.01, 0.1))\n        setp(xticklabels, size=self.fontsize)\n        yticklocs, yticklabels = yticks(arange(0, 1.01, 0.1))\n        setp(yticklabels, size=self.fontsize)\n        self.xlabel = xlabel(""1-precision"")\n        self.xlabel.set_size(self.fontsize+2)\n        self.ylabel = ylabel(""recall"")\n        self.ylabel.set_size(self.fontsize+4)\n        grid()\n        hold(True)\n\n    def newFPPIFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n        curFigure = figure(figsize=fsize)\n        self.title = title(plotTitle, fontsize=self.fontsize)\n        subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\n        #axis(\'equal\')\n        axis([0, 100, 0, 1])\n        xticklocs, xticklabels = xticks(arange(0, 100.01, 0.5))\n        setp(xticklabels, size=self.fontsize)\n        yticklocs, yticklabels = yticks(arange(0, 1.01, 0.1))\n        setp(yticklabels, size=self.fontsize)\n        self.xlabel = xlabel(""false positives per image"")\n        self.xlabel.set_size(self.fontsize+2)\n        self.ylabel = ylabel(""recall"")\n        self.ylabel.set_size(self.fontsize+4)\n        grid()\n        hold(True)\n\n\n    def newFreqFigure(self, plotTitle="""", maxX = 10, maxY = 10,fsize=rcParams[\'figure.figsize\']):\n        curFigure = figure(figsize=fsize)\n        self.title = title(plotTitle, fontsize=self.fontsize)\n        subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.1)\n        #axis(\'equal\')\n\n        axis([0, maxX, 0, maxY])\n        xticklocs, xticklabels = xticks(arange(0, maxX + 0.01, maxX * 1.0/ 10))\n        setp(xticklabels, size=self.fontsize)\n        yticklocs, yticklabels = yticks(arange(0, maxY + 0.01, maxY * 1.0/ 10))\n        setp(yticklabels, size=self.fontsize)\n        self.xlabel = xlabel(""False positive / ground truth rect"")\n        self.xlabel.set_size(self.fontsize+2)\n        self.ylabel = ylabel(""True positives / ground truth rect"")\n        self.ylabel.set_size(self.fontsize+4)\n        grid()\n        hold(True)\n\n    def newFPPWFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n        curFigure = figure(figsize=fsize)\n        self.title = title(plotTitle, fontsize=self.fontsize)\n        subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.085)\n\n        self.xlabel = xlabel(""false positive per windows (FPPW)"")\n        self.xlabel.set_size(self.fontsize+2)\n        self.ylabel = ylabel(""miss rate"")\n        self.ylabel.set_size(self.fontsize+4)\n\n        grid()\n        hold(True)\n\n    def newLogFPPIFigure(self, plotTitle="""", fsize=rcParams[\'figure.figsize\']):\n        curFigure = figure(figsize=fsize)\n        self.title = title(plotTitle, fontsize=self.fontsize)\n        subplots_adjust(left=0.085, right=0.975, top=0.975, bottom=0.1)\n\n        #axis(\'equal\')\n\n        self.xlabel = xlabel(""false positives per image"")\n        self.xlabel.set_size(self.fontsize+2)\n        self.ylabel = ylabel(""miss rate"")\n        self.ylabel.set_size(self.fontsize+4)\n        grid()\n        hold(True)\n\n    def loadRPCData(self, fname):\n        self.filename = fname\n        self.prec=[]\n        self.rec=[]\n        self.score=[]\n        self.fppi=[]\n        file = open(fname)\n\n        precScores = []\n        for i in range(1,10,1):\n            precScores.append(100 - i * 10)\n\n        fppiScores=[]\n        for i in range(0, 500, 5):\n            fppiScores.append(i * 1.0 / 100.0)\n\n\n\n        precinfo = []\n        fppiinfo = []\n        eerinfo = []\n        logAvInfo = []\n\n        logAvMR= []\n        self.lamr = 0;\n        self.eer = None;\n        firstLine = True\n        leadingZeroCount = 0\n\n        for line in file.readlines():\n            vals = line.split()\n            #vals=line.split("" "")\n            #for val in vals:\n            #       if val=="""":\n            #               vals.remove(val)\n            self.prec.append(1-float(vals[0]))\n            self.rec.append(float(vals[1]))\n            self.score.append(float(vals[2]))\n\n            if(len(vals)>3):\n                self.fppi.append(float(vals[3]))\n                if firstLine and not float(vals[3]) == 0:\n                    firstLine = False\n\n                    lamrcount = 1\n                    self.lamr = 1 - float(vals[1])\n\n                    lowest_fppi = math.ceil( math.log(float(vals[3]))/ math.log(10) * 10 )\n                    print ""lowest_fppi: "",lowest_fppi;\n\n                    # MA: temporarily commented out\n                    # for i in range(lowest_fppi, 1, 1):\n                    #       logAvMR.append(10** (i * 1.0 / 10))\n\n            #self.score.append(float(vals[2][:-1]))\n            #print 1-self.prec[-1], self.rec[-1], self.score[-1]\n            if (len(self.prec)>1):\n                diff = (1-self.prec[-1]-self.rec[-1]) * (1-self.prec[-2]-self.rec[-2])\n                if ( diff <0):\n                    eerinfo.append( ""EER between: %.03f and %.03f\\tScore:%f"" % (self.rec[-1], self.rec[-2], self.score[-1]))\n                    self.eer = (self.rec[-1]+self.rec[-2]) * 0.5\n                if ( diff == 0 and 1-self.prec[-1]-self.rec[-1]==0):\n                    eerinfo.append( ""EER: %.03f\\tScore:%f"" % (self.rec[-1], self.score[-1]))\n                    self.eer = self.rec[-1]\n\n            #Remove already passed precision\n            if (len(precScores) > 0 and (float(vals[0])) < precScores[0] / 100.0):\n                precinfo.append(""%d percent precision score: %f, recall: %.03f"" % (precScores[0], float(vals[2]), float(vals[1])))\n                while(len(precScores) > 0 and precScores[0]/100.0 > float(vals[0])):\n                    precScores.pop(0)\n\n            #Remove already passed precision\n            if(len(vals) > 3):\n                if (len(fppiScores) > 0 and (float(vals[3])) > fppiScores[0]):\n                    fppiinfo.append(""%f fppi score: %f, recall: %.03f"" % (fppiScores[0], float(vals[2]), float(vals[1])))\n                    while(len(fppiScores) > 0 and fppiScores[0] < float(vals[3])):\n                        fppiScores.pop(0)\n\n                if (len(logAvMR) > 0 and (float(vals[3])) > logAvMR[0]):\n                    while(len(logAvMR) > 0 and logAvMR[0] < float(vals[3])):\n                        logAvInfo.append(""%f fppi, miss rate: %.03f, score: %f"" % (logAvMR[0], 1-float(vals[1]), float(vals[2])) )\n                        self.lamr += 1-float(vals[1])\n                        lamrcount += 1\n                        logAvMR.pop(0)\n\n                lastMR = 1-float(vals[1])\n\n\n        if(len(vals)>3):\n            for i in logAvMR:\n                logAvInfo.append(""%f fppi, miss rate: %.03f, extended"" % (i, lastMR) )\n                self.lamr += lastMR\n                lamrcount += 1\n\n        for i in precinfo:\n            print i;\n        print;\n        for i in fppiinfo:\n            print i;\n        print\n        for i in eerinfo:\n            print i;\n        print\n        print ""Recall at first false positive: %.03f"" % self.rec[0]\n        if(len(vals)>3):\n            print\n            for i in logAvInfo:\n                print i;\n            self.lamr =  self.lamr * 1.0 / lamrcount\n            print ""Log average miss rate in [10^%.01f, 10^0]: %.03f"" % (lowest_fppi / 10.0, self.lamr )\n\n\n\n        print; print\n        file.close()\n\n    def loadFreqData(self, fname):\n        self.filename = fname\n        self.prec=[]\n        self.rec=[]\n        self.score=[]\n        file = open(fname)\n\n        for line in file.readlines():\n            vals = line.split()\n\n            self.prec.append(float(vals[0]))\n            self.rec.append(float(vals[1]))\n            self.score.append(float(vals[2]))\n\n        file.close()\n\n    def loadFPPWData(self, fname):\n        self.loadFreqData(fname)\n\n    def finishPlot(self, axlimits = [0,1.0,0,1.0]):\n        # MA:\n        #self.legend = legend(self.legendNames, self.legendPlace, pad = self.legendborderpad, labelsep = self.legendlabelsep)\n        self.legend = legend(self.legendNames, self.legendPlace)\n\n        lstrings = self.legend.get_texts()\n        setp(lstrings, fontsize=self.fontsizeLegend)\n        #line= plot( [1 - axlimits[0], 0], [axlimits[3], 1 - axlimits[3] ] , \'k\')\n        line= plot( [1, 0], [0, 1] , \'k\')\n\n    def finishFreqPlot(self):\n        self.legend = legend(self.legendNames, self.legendPlace, pad = self.legendborderpad, labelsep = self.legendlabelsep)\n        lstrings = self.legend.get_texts()\n        setp(lstrings, fontsize=self.fontsizeLegend)\n\n\n    def show(self, plotEER = True, axlimits = [0,1.0,0,1.0]):\n        if (plotEER):\n            self.finishPlot(axlimits)\n            axis(axlimits)\n        else:\n            self.finishFreqPlot()\n\n        show()\n\n    def saveCurrentFigure(self, plotEER, filename, axlimits = [0,1.0,0,1.0]):\n        if (plotEER):\n            self.finishPlot(axlimits)\n            axis(axlimits)\n        else:\n            self.finishFreqPlot()\n\n        print ""Saving: "" + filename\n        savefig(filename)\n\n    def plotRFP(self, numImages, fname, line=""r-""):\n        print \'NOT YET IMPLEMENTED\'\n\n    def plotRPC(self, fname, descr=""line"", style=""-1"", axlimits = [0,1.0,0,1.0], linewidth = 2, dashstyle = [], addEER = False ):\n        self.loadRPCData(fname)\n\n        #axis(axlimits);\n        if (style==""-1""):\n            if dashstyle != []:\n                line = plot(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n            else:\n                line = plot(self.prec, self.rec, self.colors[self.color])\n            self.color=self.color+1\n            self.color=self.color % len(self.colors)\n        else:\n            if dashstyle != []:\n                line = plot(self.prec, self.rec, style, dashes = dashstyle)\n            else:\n                line = plot(self.prec, self.rec, style)\n\n        axis(axlimits)\n\n        if addEER and self.eer != None:\n            descr += "" (%.01f%%)"" % (self.eer * 100)\n\n        setp(line, \'linewidth\', linewidth)\n        self.legendNames= self.legendNames+[descr]\n\n    def plotFPPI(self, fname, descr=""line"", style=""-1"", axlimits = [0,2,0,1], linewidth = 2, dashstyle = []):\n        self.loadRPCData(fname)\n\n        if (style==""-1""):\n            if dashstyle != []:\n                line = plot(self.fppi, self.rec, self.colors[self.color], dashes = dashstyle)\n            else:\n                line = plot(self.fppi, self.rec, self.colors[self.color])\n            self.color=self.color+1\n            self.color=self.color % len(self.colors)\n        else:\n            if dashstyle != []:\n                line = plot(self.fppi, self.rec, style, dashes = dashstyle)\n            else:\n                line = plot(self.fppi, self.rec, style)\n\n        axis(axlimits);\n\n        setp(line, \'linewidth\', linewidth)\n        self.legendNames= self.legendNames+[descr]\n\n\n    def plotFreq(self, fname, descr=""line"", style=""-1"", linewidth = 2, dashstyle = []):\n        self.loadFreqData(fname)\n        if (style==""-1""):\n            if dashstyle != []:\n                line = plot(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n            else:\n                line = plot(self.prec, self.rec, self.colors[self.color])\n            self.color=self.color+1\n            self.color=self.color % len(self.colors)\n        else:\n            if dashstyle != []:\n                line = plot(self.prec, self.rec, style, dashes = dashstyle)\n            else:\n                line = plot(self.prec, self.rec, style)\n\n\n        setp(line, \'linewidth\', linewidth)\n        self.legendNames= self.legendNames+[descr]\n\n    def plotFPPW(self, fname, descr=""line"", style=""-1"", axlimits = [5e-6, 1e0, 1e-2, 0.5], linewidth = 2, dashstyle = []):\n        self.loadFPPWData(fname)\n        if (style==""-1""):\n            if dashstyle != []:\n                line = loglog(self.prec, self.rec, self.colors[self.color], dashes = dashstyle)\n            else:\n                line = loglog(self.prec, self.rec, self.colors[self.color])\n            self.color=self.color+1\n            self.color=self.color % len(self.colors)\n        else:\n            if dashstyle != []:\n                line = loglog(self.prec, self.rec, style, dashes = dashstyle)\n            else:\n                line = loglog(self.prec, self.rec, style)\n\n        xticklocs, xticklabels = xticks([1e-5, 1e-4,1e-3, 1e-2, 1e-1, 1e0])\n        setp(xticklabels, size=self.fontsize)\n        yticklocs, yticklabels = yticks(array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5]),\n                                                                        (""0.01"", ""0.02"", ""0.03"", ""0.04"", ""0.05"", ""0.06"", ""0.07"", ""0.08"",""0.09"", ""0.1"", ""0.2"", ""0.3"", ""0.4"", ""0.5""))\n        setp(yticklabels, size=self.fontsize)\n\n        axis(axlimits)\n\n        gca().yaxis.grid(True, \'minor\')\n        setp(line, \'linewidth\', linewidth)\n\n        self.legendNames= self.legendNames+[descr]\n\n    def plotLogFPPI(self, fname, descr=""line"", style=""-1"", axlimits = [5e-3, 1e1, 1e-1, 1], linewidth = 2, dashstyle = [], addlamr = False):\n        self.loadRPCData(fname)\n        if (style==""-1""):\n            if dashstyle != []:\n                line = loglog(self.fppi, [1 - x for x in self.rec], self.colors[self.color], dashes = dashstyle)\n            else:\n                line = loglog(self.fppi, [1 - x for x in self.rec], self.colors[self.color])\n\n            self.color=(self.color+1) % len(self.colors)\n        else:\n            if dashstyle != []:\n                line = loglog(self.fppi, [1 - x for x in self.rec], style, dashes = dashstyle)\n            else:\n                line = loglog(self.fppi, [1 - x for x in self.rec], style)\n\n        gca().yaxis.grid(True, \'minor\')\n\n        m = min(self.fppi)\n        lax = axlimits[0]\n        for i in self.fppi:\n            if(i != m):\n                lax = math.floor(log(i)/math.log(10))\n                leftlabel = math.pow(10, lax)\n                break\n\n        m = max(self.fppi)\n        rightlabel = math.pow(10, math.ceil(log(m)/math.log(10))) + 0.01\n\n        k = leftlabel\n        ticks = [k]\n        while k < rightlabel:\n            k = k * 10\n            ticks.append(k)\n\n        xticklocs, xticklabels = xticks(ticks)\n        setp(xticklabels, size=self.fontsize)\n        yticklocs, yticklabels = yticks(arange(0.1, 1.01, 0.1), (""0.1"", ""0.2"", ""0.3"", ""0.4"", ""0.5"", ""0.6"", ""0.7"", ""0.8"", ""0.9"", ""1.0""))\n        setp(yticklabels, size=self.fontsize)\n\n        axlimits[0] = lax\n        axis(axlimits)\n\n        setp(line, \'linewidth\', linewidth)\n\n        if addlamr:\n            descr += "" (%.01f%%)"" % (self.lamr * 100)\n\n        self.legendNames= self.legendNames+[descr]\n'"
linux/tensorbox/utils/annolist/PalLib.py,0,"b'import sys\n#import AnnoList_pb2\nimport AnnotationLib;\n\nfrom ma_utils import is_number;\n\ndef loadPal(filename):\n    _annolist = AnnoList_pb2.AnnoList();\n\n    f = open(filename, ""rb"");\n    _annolist.ParseFromString(f.read());\n    f.close();\n\n    return _annolist;\n\ndef savePal(filename, _annolist):\n    f = open(filename, ""wb"");\n    f.write(_annolist.SerializeToString());\n    f.close();\n\ndef al2pal(annotations):\n    _annolist = AnnoList_pb2.AnnoList();\n\n    #assert(isinstance(annotations, AnnotationLib.AnnoList));\n\n    # check type of attributes, add missing attributes\n    for a in annotations:\n        for r in a.rects:\n            for k, v in r.at.iteritems():\n                if not k in annotations.attribute_desc:\n                    annotations.add_attribute(k, type(v));\n                else:\n                    assert(AnnotationLib.is_compatible_attr_type(annotations.attribute_desc[k].dtype, type(v)));\n\n    # check attributes values\n    for a in annotations:\n        for r in a.rects:\n            for k, v in r.at.iteritems():\n                if k in annotations.attribute_val_to_str:\n                    # don\'t allow undefined values\n                    if not v in annotations.attribute_val_to_str[k]:\n                        print ""attribute: {}, undefined value: {}"".format(k, v);\n                        assert(False);\n\n    # store attribute descriptions in pal structure\n    for aname, adesc in annotations.attribute_desc.iteritems():\n        _annolist.attribute_desc.extend([adesc]);\n\n    for a in annotations:\n        _a = _annolist.annotation.add();\n        _a.imageName = a.imageName;\n\n        for r in a.rects:\n            _r = _a.rect.add();\n\n            _r.x1 = r.x1;\n            _r.y1 = r.y1;\n            _r.x2 = r.x2;\n            _r.y2 = r.y2;\n\n            _r.score = float(r.score);\n\n            if hasattr(r, \'id\'):\n                _r.id = r.id;\n\n            if hasattr(r, \'track_id\'):\n                _r.track_id = r.track_id;\n\n            if hasattr(r, \'at\'):\n                for k, v in r.at.items():\n                    _at = _r.attribute.add();\n\n                    _at.id = annotations.attribute_desc[k].id;\n\n                    if annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_INT32, type(v)));\n                        _at.val = int(v);\n                    elif annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_FLOAT, type(v)));\n                        _at.fval = float(v);\n                    elif annotations.attribute_desc[k].dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                        assert(AnnotationLib.is_compatible_attr_type(AnnotationLib.AnnoList.TYPE_STRING, type(v)));\n                        _at.strval = str(v);\n                    else:\n                        assert(false);\n\n    return _annolist;\n\ndef pal2al(_annolist):\n    #annotations = [];\n    annotations = AnnotationLib.AnnoList();\n\n    for adesc in _annolist.attribute_desc:\n        annotations.attribute_desc[adesc.name] = adesc;\n        print ""attribute: "", adesc.name, adesc.id\n\n        for valdesc in adesc.val_to_str:\n            annotations.add_attribute_val(adesc.name, valdesc.s, valdesc.id);\n\n    attribute_name_from_id = {adesc.id: aname for aname, adesc in annotations.attribute_desc.iteritems()}\n    attribute_dtype_from_id = {adesc.id: adesc.dtype for aname, adesc in annotations.attribute_desc.iteritems()}\n\n    for _a in _annolist.annotation:\n        anno = AnnotationLib.Annotation()\n\n        anno.imageName = _a.imageName;\n\n        anno.rects = [];\n\n        for _r in _a.rect:\n            rect = AnnotationLib.AnnoRect()\n\n            rect.x1 = _r.x1;\n            rect.x2 = _r.x2;\n            rect.y1 = _r.y1;\n            rect.y2 = _r.y2;\n\n            if _r.HasField(""id""):\n                rect.id = _r.id;\n\n            if _r.HasField(""track_id""):\n                rect.track_id = _r.track_id;\n\n            if _r.HasField(""score""):\n                rect.score = _r.score;\n\n            for _at in _r.attribute:\n                try:\n                    cur_aname = attribute_name_from_id[_at.id];\n                    cur_dtype = attribute_dtype_from_id[_at.id];\n                except KeyError as e:\n                    print ""attribute: "", _at.id\n                    print e\n                    assert(False);\n\n                if cur_dtype == AnnotationLib.AnnoList.TYPE_INT32:\n                    rect.at[cur_aname] = _at.val;\n                elif cur_dtype == AnnotationLib.AnnoList.TYPE_FLOAT:\n                    rect.at[cur_aname] = _at.fval;\n                elif cur_dtype == AnnotationLib.AnnoList.TYPE_STRING:\n                    rect.at[cur_aname] = _at.strval;\n                else:\n                    assert(False);\n\n            anno.rects.append(rect);\n\n        annotations.append(anno);\n\n    return annotations;\n'"
linux/tensorbox/utils/annolist/__init__.py,0,b''
linux/tensorbox/utils/annolist/doRPC.py,0,"b'#!/usr/bin/env python\n\nimport os, sys\nfrom AnnotationLib import *\nfrom optparse import OptionParser\nimport copy\nimport math\n\n# BASED ON WIKIPEDIA VERSION\n# n - number of nodes\n# C - capacity matrix\n# F - flow matrix\n# s - source\n# t - sink\n# sumC - sum over rows of C (too speed up computation)\n\ndef edmonds_karp(n, C, s, t, sumC):\n\n    # Residual capacity from u to v is C[u][v] - F[u][v]\n    F = [[0] * n for i in xrange(n)]\n    while True:\n        P = [-1] * n # Parent table\n        P[s] = s\n        M = [0] * n  # Capacity of path to node\n        M[s] = float(\'infinity\')\n        Q = [s]      # BFS queue\n        while Q:\n            u = Q.pop(0)\n            for v in xrange(n):\n                # There is available capacity,\n        # and v is not seen before in search\n                if C[u][v] - F[u][v] > 0 and P[v] == -1:\n                    P[v] = u\n                    M[v] = min(M[u], C[u][v] - F[u][v])\n                    if v != t:\n                        if(sumC[u] > 0):\n                            Q.append(v)\n                    else:\n                        # Backtrack search, and write flow\n                        while P[v] != v:\n                            u = P[v]\n                            F[u][v] += M[t]\n                            F[v][u] -= M[t]\n                            v = u\n                        Q = None\n                        break\n        if P[t] == -1: # We did not find a path to t\n            return (F)\n\nclass AnnoGraph:\n\n    def __init__(self, anno, det, ignore, style, minCover, minOverlap, maxDistance, ignoreOverlap):\n\n        # setting rects\n        #print anno.imageName\n        self.anno = anno\n        self.det = det\n        self.det.sortByScore(""descending"")\n\n        # generate initial graph\n        self.n = len(det.rects)\n        self.m = len(anno.rects)\n\n        # Number of nodes = number of detections + number of GT + source + sink\n        self.a = self.n + self.m + 2\n\n        # Flow matrix\n        self.F = [[0] * self.a for i in xrange(self.a)]\n        # Capacity matrix\n        self.C = [[0] * self.a for i in xrange(self.a)]\n\n        # Connect source to all detections\n        for i in range(1, self.n + 1):\n            self.C[0][i] = 1\n            self.C[i][0] = 1\n\n        # Connect sink to all GT\n        for i in range(self.n + 1, self.a - 1):\n            self.C[i][self.a - 1] = 1\n            self.C[self.a - 1][i] = 1\n\n        # Overall flow\n        self.full_flow = 0\n        self.ignore_flow = 0\n\n        # match rects / Adjacency matrix\n        self.M = [[] for i in xrange(self.n)]\n        self.match(style, minCover, minOverlap, maxDistance)\n        self.nextN = 0\n\n        # Deactivate All Non Matching detections\n        # Save row sums for capacity matrix\n        self.sumC = []\n        self.sumC.append(self.n)\n        for q in [len(self.M[j]) for j in xrange(len(self.M))]:\n            self.sumC.append(q)\n        for q in [1] * self.m:\n            self.sumC.append(q)\n\n        # Initially no links are active\n        self.sumC_active = []\n        self.sumC_active.append(self.n)\n        for q in [len(self.M[j]) for j in xrange(len(self.M))]:\n            self.sumC_active.append(0)\n        for q in [1] * self.m:\n            self.sumC_active.append(q)\n\n        #\n        self.ignore = [ 0 ] * self.m\n        for ig in ignore.rects:\n            for i, r in enumerate(anno.rects):\n                if(ig.overlap_pascal(r) > ignoreOverlap):\n                    self.ignore[i] = 1\n\n\n    def match(self, style, minCover, minOverlap, maxDistance):\n        for i in xrange(self.n):\n            detRect = self.det.rects[i]\n            for j in xrange(self.m):\n                annoRect = self.anno.rects[j]\n\n                # Bastian Leibe\'s matching style\n                if(style == 0):\n                    assert False;\n                    if detRect.isMatchingStd(annoRect, minCover, minOverlap, maxDistance):\n                        self.M[i].append(self.n + 1 + j)\n\n                # Pascal Matching style\n                if(style == 1):\n                    if (detRect.isMatchingPascal(annoRect, minOverlap)):\n                        self.M[i].append(self.n + 1 + j)\n\n    def decreaseScore(self, score):\n        capacity_change = False\n        for i in xrange(self.nextN, self.n):\n            if (self.det.rects[i].score >= score):\n                capacity_change = self.insertIntoC(i + 1) or capacity_change\n                self.nextN += 1\n            else:\n                break\n\n        if capacity_change:\n            self.F = edmonds_karp(self.a, self.C, 0, self.a - 1, self.sumC_active)\n            self.full_flow = sum([self.F[0][i] for i in xrange(self.a)])\n            self.ignore_flow = sum([self.F[i][self.a - 1] * self.ignore[i - 1 - self.n] for i in range(1 + self.n, 1 + self.n + self.m )])\n\n        return capacity_change\n\n    def addBB(self, rect):\n        self.nextN += 1\n\n        capacity_change = self.insertIntoC(rect.boxIndex + 1)\n\n        if capacity_change:\n            self.F = edmonds_karp(self.a, self.C, 0, self.a - 1, self.sumC_active)\n            self.full_flow = sum([self.F[0][i] for i in xrange(self.a)])\n            self.ignore_flow = sum([self.F[i][self.a - 1] * self.ignore[i - 1 - self.n] for i in range(1 + self.n, 1 + self.n + self.m )])\n\n        return capacity_change\n\n    def     insertIntoC(self, i):\n        #print ""Inserting node"", i, self.det.rects[i-1].score, ""of image"", self.anno.imageName\n\n        for match in self.M[i - 1]:\n            #print ""  match: "", match\n            self.C[i][match] = 1\n            self.C[match][i] = 1\n\n        self.sumC_active[i] = self.sumC[i]\n\n        return self.sumC[i] > 0\n\n    def maxflow(self):\n        return self.full_flow - self.ignore_flow\n\n    def consideredDets(self):\n        return self.nextN - self.ignore_flow\n\n    def ignoredFlow(self):\n        return self.ignore_flow\n\n    def getTruePositives(self):\n        ret = copy.copy(self.anno)\n        ret.rects = []\n        #iterate over GT\n        for i in xrange(self.n + 1, self.a - 1):\n            #Flow to sink > 0\n            if(self.F[i][self.a - 1] > 0 and self.ignore[i - self.n - 1] == 0):\n                #Find associated det\n                for j in xrange(1, self.n + 1):\n                    if(self.F[j][i] > 0):\n                        ret.rects.append(self.det[j - 1])\n                        break\n        return ret\n\n    def getIgnoredTruePositives(self):\n        ret = copy.copy(self.anno)\n        ret.rects = []\n        #iterate over GT\n        for i in xrange(self.n + 1, self.a - 1):\n            #Flow to sink > 0\n            if(self.F[i][self.a - 1] > 0 and self.ignore[i - self.n - 1] == 1):\n                #Find associated det\n                for j in xrange(1, self.n + 1):\n                    if(self.F[j][i] > 0):\n                        ret.rects.append(self.det[j - 1])\n                        break\n        return ret\n\n    def getMissingRecall(self):\n        ret = copy.copy(self.anno)\n        ret.rects = []\n        for i in xrange(self.n + 1, self.a - 1):\n            if(self.F[i][self.a - 1] == 0 and self.ignore[i - self.n - 1] == 0):\n                ret.rects.append(self.anno.rects[i - self.n - 1])\n        return ret\n\n    def getFalsePositives(self):\n        ret = copy.copy(self.det)\n        ret.rects = []\n        for i in xrange(1, self.n + 1):\n            if(self.F[0][i] == 0):\n                ret.rects.append(self.det[i - 1])\n        return ret\n\ndef asort(idlGT, idlDet, minWidth, minHeight, style, minCover, minOverlap, maxDistance, maxWidth=float(\'inf\'), maxHeight=float(\'inf\')):\n    #Asort too small object in ground truth\n\n    for x,anno in enumerate(idlGT):\n\n        imageFound = False\n        filterIndex = -1\n        for i,filterAnno in enumerate(idlDet):\n            if (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\n                filterIndex = i\n                imageFound = True\n                break\n\n        if(not imageFound):\n            continue\n\n        validGTRects = []\n        for j in anno.rects:\n            if (j.width() >= minWidth) and (j.height() >= minHeight) and (j.width() <= maxWidth) and (j.height() <= maxHeight):\n                validGTRects.append(j)\n            else:\n                # Sort out detections that would have matched\n                matchingIndexes = []\n\n                for m,frect in enumerate(idlDet[filterIndex].rects):\n                    if(style == 0):\n                        if (j.isMatchingStd(frect, minCover,minOverlap, maxDistance)):\n                            overlap = j.overlap_pascal(frect)\n                            matchingIndexes.append((m,overlap))\n\n                    if(style == 1):\n                        if(j.isMatchingPascal(frect, minOverlap)):\n                            overlap = j.overlap_pascal(frect)\n                            matchingIndexes.append((m, overlap))\n\n                for m in xrange(len(matchingIndexes) - 1, -1, -1):\n                    matching_rect = idlDet[filterIndex].rects[matchingIndexes[m][0]]\n                    matching_overlap = matchingIndexes[m][1]\n                    better_overlap_found = False\n                    for l in anno.rects:\n                        if l.overlap_pascal(matching_rect) > matching_overlap:\n                            better_overlap_found = True\n\n                    if better_overlap_found:\n                        continue\n\n                    del idlDet[filterIndex].rects[matchingIndexes[m][0]]\n\n        idlGT[x].rects = validGTRects\n\n    #Sort out too small false positives\n    for x,anno in enumerate(idlDet):\n\n        imageFound = False\n        filterIndex = -1\n        for i,filterAnno in enumerate(idlGT):\n            if (suffixMatch(anno.imageName, filterAnno.imageName) and anno.frameNr == filterAnno.frameNr):\n                filterIndex = i\n                imageFound = True\n                break\n\n        if(not imageFound):\n            continue\n\n        validDetRects = []\n        for j in anno.rects:\n            if (j.width() >= minWidth) and (j.height() >= minHeight) and (j.width() <= maxWidth) and (j.height() <= maxHeight):\n                validDetRects.append(j)\n            else:\n                for frect in idlGT[filterIndex].rects:\n\n                    if(style == 0):\n                        if j.isMatchingStd(frect, minCover,minOverlap, maxDistance):\n                            validDetRects.append(j)\n\n                    if(style == 1):\n                        if(j.isMatchingPascal(frect, minOverlap)):\n                            validDetRects.append(j)\n\n\n        idlDet[x].rects = validDetRects\n\n\n# MA: simplified version that does Pascal style matching with one parameter controlling ""intersection-over-union"" matching threshold\ndef comp_prec_recall(annoIDL, detIDL, minOverlap):\n    ignoreIDL = copy.deepcopy(annoIDL)\n    for anno in ignoreIDL:\n        anno.rects = []\n\n    precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minOverlap=minOverlap);\n    return precs, recalls, scores, fppi\n\ndef comp_prec_recall_graphs(annoIDL, detIDL, minOverlap):\n    ignoreIDL = copy.deepcopy(annoIDL)\n    for anno in ignoreIDL:\n        anno.rects = []\n\n    precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minOverlap=minOverlap);\n    return graphs\n\n\n# MA: full version\ndef comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL, minWidth=0, minHeight=0, maxWidth=float(\'inf\'), maxHeight=float(\'inf\'),\n                                matchingStyle=1, minCover=0.5, minOverlap=0.5, maxDistance=0.5, ignoreOverlap=0.9, verbose=False):\n\n    # Asort detections which are too small/too big\n    if verbose:\n        print ""Asorting too large/ too small detections""\n        print ""minWidth:"", minWidth\n        print ""minHeight:"", minHeight\n        print ""maxWidth: "", maxWidth\n        print ""maxHeight: "", maxHeight\n\n    asort(annoIDL, detIDL, minWidth, minHeight, matchingStyle, minCover, minOverlap, maxDistance, maxWidth, maxHeight)\n\n    #Debugging asort\n    #saveIDL(""testGT.idl"", annoIDL)\n    #saveIDL(""testDET.idl"", detIDL)\n\n\n\n    noAnnotations = 0\n    for anno in annoIDL:\n        for j,detAnno in enumerate(detIDL):\n            if (suffixMatch(anno.imageName, detIDL[j].imageName) and anno.frameNr == detIDL[j].frameNr):\n                noAnnotations = noAnnotations + len(anno.rects)\n                break\n\n    if verbose:\n        print ""#Annotations:"", noAnnotations\n\n        ###--- set up graphs ---###\n        print ""Setting up graphs ...""\n\n    graphs = []\n    allRects = []\n    missingFrames = 0\n    for i in xrange(len(annoIDL)):\n\n        imageFound = False\n        filterIndex = -1\n\n        for j, detAnno in enumerate(detIDL):\n            if (suffixMatch(annoIDL[i].imageName, detIDL[j].imageName) and annoIDL[i].frameNr == detIDL[j].frameNr):\n                filterIndex = j\n                imageFound = True\n                break\n\n        if(not imageFound):\n            print ""No annotation/detection pair found for: "" + annoIDL[i].imageName + "" frame: "" + str(annoIDL[i].frameNr)\n            missingFrames += 1\n            continue;\n\n        graphs.append(AnnoGraph(annoIDL[i], detIDL[filterIndex], ignoreIDL[i], matchingStyle, minCover, minOverlap, maxDistance, ignoreOverlap))\n\n        for j,rect in enumerate(detIDL[filterIndex]):\n            newRect = detAnnoRect()\n            newRect.imageName = anno.imageName\n            newRect.frameNr = anno.frameNr\n            newRect.rect = rect\n            newRect.imageIndex = i - missingFrames\n            newRect.boxIndex = j\n            allRects.append(newRect)\n\n    if verbose:\n        print ""missingFrames: "", missingFrames\n        print ""Number of detections on annotated frames: "" , len(allRects)\n\n        ###--- get scores from all rects ---###\n        print ""Sorting scores ...""\n\n    allRects.sort(cmpDetAnnoRectsByScore)\n    allRects.reverse()\n\n    ###--- gradually decrease score ---###\n    if verbose:\n        print ""Gradually decrease score ...""\n\n    lastScore = float(\'infinity\')\n\n    precs = [1.0]\n    recalls = [0.0]\n    #fppi = [ 10**(math.floor(math.log(1.0 / float(len(annoIDL)))/math.log(10) * 10.0) / 10.0) ]\n    fppi = [ 1.0 / float(len(annoIDL)) ]\n    scores = [lastScore]\n\n    numDet = len(allRects)\n    sf = lastsf = 0\n    cd = lastcd = 0\n    iflow = lastiflow = 0\n\n    changed = False\n    firstFP = True\n    for i,nextrect in enumerate(allRects):\n        score = nextrect.rect.score;\n\n        # updating true and false positive counts\n        sf = sf - graphs[nextrect.imageIndex].maxflow()\n        cd = cd - graphs[nextrect.imageIndex].consideredDets()\n        iflow = iflow - graphs[nextrect.imageIndex].ignoredFlow()\n\n        #changed = changed or graphs[nextrect.imageIndex].decreaseScore(score)\n        changed = graphs[nextrect.imageIndex].addBB(nextrect) or changed\n        sf = sf + graphs[nextrect.imageIndex].maxflow()\n        cd = cd + graphs[nextrect.imageIndex].consideredDets()\n        iflow = iflow + graphs[nextrect.imageIndex].ignoredFlow()\n\n        if(firstFP and cd - sf != 0):\n            firstFP = False\n            changed = True\n\n        if (i == numDet - 1 or score != allRects[i + 1].rect.score or firstFP or i == len(allRects)):\n            if(changed or i == numDet - 1 or i == len(allRects)):\n\n                if(lastcd > 0):\n                    scores.append(lastScore)\n                    recalls.append(float(lastsf) / float(noAnnotations - lastiflow))\n                    precs.append(float(lastsf) / float(lastcd))\n                    fppi.append(float(lastcd - lastsf) / float(len(annoIDL)))\n\n                if (cd > 0):\n                    scores.append(score)\n                    recalls.append(float(sf) / float(noAnnotations - iflow))\n                    precs.append(float(sf) / float(cd))\n                    fppi.append(float(cd - sf) / float(len(annoIDL)))\n\n\n            changed = False\n\n        lastScore = score\n        lastsf = sf\n        lastcd = cd\n        lastiflow = iflow\n\n    return precs, recalls, scores, fppi, graphs\n\ndef main():\n\n    parser = OptionParser(usage=""usage: %prog [options] <groundTruthIdl> <detectionIdl>"")\n\n    parser.add_option(""-o"", ""--outFile"",\n              action=""store"", type=""string"", dest=""outFile"")\n    parser.add_option(""-a"", ""--analysisFiles"",\n              action=""store"", type=""string"", dest=""analysisFile"")\n\n    parser.add_option(""-s"", ""--minScore"",\n              action=""store"", type=""float"", dest=""minScore"")\n\n    parser.add_option(""-w"", ""--minWidth"",\n                            action=""store"", type=""int"", dest=""minWidth"", default=0)\n\n    parser.add_option(""-u"", ""--minHeight"",\n                            action=""store"", type=""int"", dest=""minHeight"",default=0)\n    parser.add_option(""--maxWidth"", action=""store"", type=""float"", dest=""maxWidth"", default=float(\'inf\'))\n    parser.add_option(""--maxHeight"", action=""store"", type=""float"", dest=""maxHeight"", default=float(\'inf\'))\n\n    parser.add_option(""-r"", ""--fixAspectRatio"",\n                    action=""store"", type=""float"", dest=""aspectRatio"")\n\n    parser.add_option(""-p"", ""--Pascal-Style"", action=""store_true"", dest=""pascalStyle"")\n    parser.add_option(""-l"", ""--Leibe-Seemann-Matching-Style"", action=""store_true"", dest=""leibeStyle"")\n\n    parser.add_option(""--minCover"", action=""store"", type=""float"", dest=""minCover"", default=0.5)\n    parser.add_option(""--maxDistance"", action=""store"", type=""float"", dest=""maxDistance"", default=0.5)\n    parser.add_option(""--minOverlap"", action=""store"", type=""float"", dest=""minOverlap"", default=0.5)\n\n\n    parser.add_option(""--clipToImageWidth"", action=""store"", type=""float"", dest=""clipWidth"", default= None)\n    parser.add_option(""--clipToImageHeight"", action=""store"", type=""float"", dest=""clipHeight"", default= None)\n\n    parser.add_option(""-d"", ""--dropFirst"", action=""store_true"", dest=""dropFirst"")\n\n    #parser.add_option(""-c"", ""--class"", action=""store"", type=""int"", dest=""classID"", default=-1)\n    parser.add_option(""-c"", ""--class"", action=""store"", type=""int"", dest=""classID"", default = None)\n\n    parser.add_option(""-i"", ""--ignore"", action=""store"", type=""string"", dest=""ignoreFile"")\n    parser.add_option(""--ignoreOverlap"", action=""store"", type=""float"", dest=""ignoreOverlap"", default = 0.9)\n\n    (options, args) = parser.parse_args()\n\n    if (len(args) < 2):\n        print ""Please specify annotation and detection as arguments!""\n        parser.print_help()\n        sys.exit(1)\n\n    annoFile = args[0]\n\n    # First figure out the minimum height and width we are dealing with\n    minWidth =  options.minWidth\n    minHeight = options.minHeight\n    maxWidth =  options.maxWidth\n    maxHeight = options.maxHeight\n\n    print ""Minimum width: %d height: %d"" % (minWidth, minHeight)\n\n    # Load files\n    annoIDL = parse(annoFile)\n    detIDL = []\n    for dets in args[1:]:\n        detIDL += parse(dets)\n\n\n    if options.ignoreFile != None:\n        ignoreIDL = parse(options.ignoreFile)\n    else:\n        ignoreIDL = copy.deepcopy(annoIDL)\n        for anno in ignoreIDL:\n            anno.rects = []\n\n    if(options.classID is not None):\n        for anno in annoIDL:\n            anno.rects = [rect for rect in anno.rects if (rect.classID == options.classID  or rect.classID == -1)]\n        for anno in detIDL:\n            anno.rects = [rect for rect in anno.rects if (rect.classID == options.classID or rect.classID == -1)]\n        for anno in ignoreIDL:\n            anno.rects = [rect for rect in anno.rects if (rect.classID == options.classID or rect.classID == -1)]\n\n    # prevent division by zero when fixing aspect ratio\n    for anno in annoIDL:\n        anno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n    for anno in detIDL:\n        anno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n    for anno in ignoreIDL:\n        anno.rects = [rect for rect in anno.rects if rect.width() > 0 and rect.height() > 0]\n\n\n    # Fix aspect ratio\n    if (not options.aspectRatio == None):\n        forceAspectRatio(annoIDL, options.aspectRatio)\n        forceAspectRatio(detIDL, options.aspectRatio)\n        forceAspectRatio(ignoreIDL, options.aspectRatio)\n\n    # Deselect detections with too low score\n    if (not options.minScore == None):\n        for i,anno in enumerate(detIDL):\n            validRects = []\n            for rect in anno.rects:\n                if (rect.score >= options.minScore):\n                    validRects.append(rect)\n            anno.rects = validRects\n\n    # Clip detections to the image dimensions\n    if(options.clipWidth != None or options.clipHeight != None):\n        min_x = -float(\'inf\')\n        min_y = -float(\'inf\')\n        max_x = float(\'inf\')\n        max_y = float(\'inf\')\n\n        if(options.clipWidth != None):\n            min_x = 0\n            max_x = options.clipWidth\n        if(options.clipHeight != None):\n            min_y = 0\n            max_y = options.clipHeight\n\n        print ""Clipping width: (%.02f-%.02f); clipping height: (%.02f-%.02f)"" % (min_x, max_x, min_y, max_y)\n        for anno in annoIDL:\n            for rect in anno:\n                rect.clipToImage(min_x, max_x, min_y, max_y)\n        for anno in detIDL:\n            for rect in anno:\n                rect.clipToImage(min_x, max_x, min_y, max_y)\n\n    # Setup matching style; standard is Pascal\n    # style\n    matchingStyle = 1\n\n    # Pascal style\n    if (options.pascalStyle == True):\n        matchingStyle = 1\n\n    if (options.leibeStyle == True):\n        matchingStyle = 0\n\n    if (options.pascalStyle and options.leibeStyle):\n        print ""Conflicting matching styles!""\n        sys.exit(1)\n\n    if (options.dropFirst == True):\n        print ""Drop first frame of each sequence...""\n        newIDL = []\n        for i, anno in enumerate(detIDL):\n            if (i > 1 and detIDL[i].frameNr == detIDL[i-1].frameNr + 1 and detIDL[i].frameNr == detIDL[i-2].frameNr + 2 and  detIDL[i].frameNr == detIDL[i-3].frameNr + 3  and detIDL[i].frameNr == detIDL[i-4].frameNr + 4):\n                newIDL.append(anno)\n        detIDL = newIDL\n\n    verbose = True;\n    precs, recalls, scores, fppi, graphs = comp_prec_recall_all_params(annoIDL, detIDL, ignoreIDL,\n                                                               minWidth=options.minWidth, minHeight=options.minHeight,\n                                                               maxWidth=options.maxWidth, maxHeight=options.maxHeight,\n                                                               matchingStyle=matchingStyle,\n                                                               minCover=options.minCover, minOverlap=options.minOverlap,\n                                                               maxDistance=options.maxDistance, ignoreOverlap=options.ignoreOverlap,\n                                                               verbose=verbose);\n\n    ###--- output to file ---###\n    outfilename = options.outFile\n    if outfilename is None:\n        outputDir = os.path.dirname(os.path.abspath(args[1]))\n        outputFile = os.path.basename(os.path.abspath(args[1]))\n        [base, ext] = idlBase(outputFile)\n        #outfilename = outputDir + ""/rpc-"" + base + "".txt""\n\n        outfilename = outputDir + ""/rpc-"" + base + ""_overlap"" + str(options.minOverlap) + "".txt""\n\n    print ""saving:\\n"" + outfilename;\n\n    file = open(outfilename, \'w\')\n    for i in xrange(len(precs)):\n        file.write(str(precs[i])+"" ""+str(recalls[i])+"" ""+str(scores[i])+ "" "" + str(fppi[i])+ ""\\n"")\n    file.close()\n\n    # Extracting failure cases\n    if(options.analysisFile != None):\n\n        anaPrefix = options.analysisFile\n\n        falsePositives = AnnoList([])\n        truePositives = AnnoList([])\n        missingRecall = AnnoList([])\n        ignoredTruePositives = AnnoList([])\n\n        for i in xrange(len(graphs)):\n            falsePositives.append(graphs[i].getFalsePositives())\n            truePositives.append(graphs[i].getTruePositives())\n            truePositives[-1].imageName = falsePositives[-1].imageName\n            truePositives[-1].imagePath = falsePositives[-1].imagePath\n            missingRecall.append(graphs[i].getMissingRecall())\n            missingRecall[-1].imageName = falsePositives[-1].imageName\n            missingRecall[-1].imagePath = falsePositives[-1].imagePath\n            if options.ignoreFile != None:\n                ignoredTruePositives.append(graphs[i].getIgnoredTruePositives())\n\n        #saveIDL(anaPrefix + ""-falsePositives.idl.gz"", falsePositives);\n        falsePositives.save(anaPrefix + ""-falsePositives.pal"")\n\n        sortedFP = annoAnalyze(falsePositives);\n        #saveIDL(anaPrefix + ""-falsePositives-sortedByScore.idl.gz"", sortedFP);\n        #saveIDL(anaPrefix + ""-truePositives.idl.gz"", truePositives);\n\n        # saveIDL(anaPrefix + ""-falsePositives-sortedByScore.idl"", sortedFP);\n        # saveIDL(anaPrefix + ""-truePositives.idl"", truePositives);\n\n        sortedFP.save(anaPrefix + ""-falsePositives-sortedByScore.pal"")\n        truePositives.save(anaPrefix + ""-truePositives.pal"")\n\n        sortedFP = annoAnalyze(truePositives);\n        #saveIDL(anaPrefix + ""-truePositives-sortedByScore.idl.gz"", sortedFP);\n        #saveIDL(anaPrefix + ""-truePositives-sortedByScore.idl"", sortedFP);\n        sortedFP.save(anaPrefix + ""-truePositives-sortedByScore.pal"")\n\n        if options.ignoreFile != None:\n            #saveIDL(anaPrefix + ""-ignoredTruePositives.idl.gz"", ignoredTruePositives)\n            #saveIDL(anaPrefix + ""-ignoredTruePositives.idl"", ignoredTruePositives)\n            ignoredTruePositives.save(anaPrefix + ""-ignoredTruePositives.pal"")\n\n        #saveIDL(anaPrefix + ""-missingRecall.idl.gz"", missingRecall);\n        #saveIDL(anaPrefix + ""-missingRecall.idl"", missingRecall);\n        missingRecall.save(anaPrefix + ""-missingRecall.pal"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
linux/tensorbox/utils/annolist/ma_utils.py,0,b'def is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n'
linux/tensorbox/utils/annolist/plotSimple.py,0,"b'#!/usr/bin/env python\n\nimport sys\nimport os\nimport random\nimport re\nfrom AnnotationLib import *\nfrom MatPlotter import *\nfrom optparse import OptionParser\nfrom copy import deepcopy\nfrom math import sqrt\n\n\ndef main(argv):\n\tparser = OptionParser(usage=""usage: %prog [options] <datafile> [...]"")\n\tparser.add_option(""-o"", ""--output-file"",        action=""store"",\n\t\t\tdest=""output"", type=""str"", help=""outfile. mandatory"")\n\tparser.add_option(""--fppw"", action=""store_true"", dest=""fppw"", help=""False Positives Per Window"")\n\tparser.add_option(""--colors"", action=""store"", dest=""colors"", help=""colors"")\n\tparser.add_option(""--fppi"", action=""store_true"", dest=""fppi"", help=""False Positives Per Image"")\n\tparser.add_option(""--lfppi"", action=""store_true"", dest=""lfppi"", help=""False Positives Per Image(log)"")\n\tparser.add_option(""-c"", ""--components"", action=""store"", dest=""ncomponents"", type=""int"", help=""show n trailing components of the part"", default=3)\n\tparser.add_option(""--cut-trailing"", action=""store"", dest=""cutcomponents"", type=""int"", help=""cut n trailing components of the part (applied after --components)"", default=-1)\n\tparser.add_option(""-t"", ""--title"", action=""store"", dest=""title"", type=""str"", default="""")\n\tparser.add_option(""-f"", ""--fontsize"", action=""store"", dest=""fontsize"", type=""int"", default=12)\n\tparser.add_option(""-l"", ""--legend\'"", action=""store"", dest=""legend"", type=""string"", default=""lr"")\n\t(options, args) = parser.parse_args()\n\tplotter = MatPlotter(options.fontsize)\n\t\n\tposition = ""lower right""\n\tif(options.legend == ""ur""):\n\t\tposition = ""upper right""\n\tif(options.legend == ""ul""):\n\t\tposition = ""upper left""\n\tif(options.legend == ""ll""):\n\t\tposition = ""lower left""\t\n\tplotter.formatLegend(options.fontsize, newPlace = position)\n\t\n\ttitle = options.title\n\tcolors = None\n\tif (options.colors):\n\t\tcolors = options.colors.split()\n\tif (options.fppw):\n\t\tplotter.newFPPWFigure(title)\n\telif (options.lfppi):\n\t\tplotter.newLogFPPIFigure(title)\n\telif (options.fppi):\n\t\tplotter.newFPPIFigure(title)\n\telse:\n\t\tplotter.newFigure(title)\t\t\n\t\t\n\tfor i, filename in enumerate(args):\n\t\tif (os.path.isdir(filename)):\n\t\t\tfilename = os.path.join(filename, ""rpc"", ""result-minh-48"")\n\t\tdisplayname = filename\n\t\tif (options.ncomponents > 0):\n\t\t\tsuffix = None\n\t\t\tfor idx in xrange(options.ncomponents):\n\t\t\t\tdisplayname, last = os.path.split(displayname)\n\t\t\t\tif (suffix):\n\t\t\t\t\tsuffix = os.path.join(last, suffix)\n\t\t\t\telse:\n\t\t\t\t\tsuffix = last\n\t\t\tdisplayname = suffix\n\t\tif (options.cutcomponents > 0):\n\t\t\tfor idx in xrange(options.cutcomponents):\n\t\t\t\tdisplayname, last = os.path.split(displayname)\n#\t\tplusidx = displayname.index(""+"")\n#\t\tdisplayname = displayname[plusidx:]\n\t\tprint ""Plotting: ""+displayname\n\t\tif (options.fppw):\n\t\t\tplotter.plotFPPW(filename, displayname)\n\t\telif (options.lfppi):\n\t\t\tif colors:\n\t\t\t\tplotter.plotLogFPPI(filename, displayname, colors[i])\n\t\t\telse:\n\t\t\t\tplotter.plotLogFPPI(filename, displayname)\n\t\telif (options.fppi):\n\t\t\tplotter.plotFPPI(filename, displayname)\n\t\telse:\t\t\n\t\t\tplotter.plotRPC(filename, displayname)\n\t\n\tplotLine = not (options.fppw or options.lfppi or options.fppi);\t\t\n\t\n\tif (options.output is None):\n\t\tplotter.show(plotLine)\n\telse:\n\t\tplotter.saveCurrentFigure(plotLine, options.output)\n\treturn 0\n\nif __name__ == ""__main__"":\n\tsys.exit(main(sys.argv))\n'"
linux/tensorbox/utils/slim_nets/__init__.py,0,b''
linux/tensorbox/utils/slim_nets/inception_v1.py,53,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\nfrom utils import tf_concat\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf_concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'MaxPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\n\ndef inception_v1_arg_scope(weight_decay=0.00004,\n                           use_batch_norm=True,\n                           batch_norm_var_collection=\'moving_vars\'):\n  """"""Defines the default InceptionV1 arg scope.\n\n  Note: Althougth the original paper didn\'t use batch_norm we found it useful.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_var_collection: The name of the collection for the batch norm\n      variables.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': 0.9997,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n      # collection containing the moving mean and moving variance.\n      \'variables_collections\': {\n          \'beta\': None,\n          \'gamma\': None,\n          \'moving_mean\': [batch_norm_var_collection],\n          \'moving_variance\': [batch_norm_var_collection],\n      }\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
linux/tensorbox/utils/slim_nets/resnet_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom tensorflow.contrib import layers as layers_lib\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\nfrom tensorflow.contrib.framework.python.ops import arg_scope\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.contrib.layers.python.layers import layers\nfrom tensorflow.contrib.layers.python.layers import regularizers\nfrom tensorflow.contrib.layers.python.layers import utils\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return layers.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = tf.contrib.layers.conv2d(inputs, num_outputs, 3, stride=1,\n     padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = tf.contrib.layers.conv2d(inputs, num_outputs, 3, stride=stride,\n     padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return layers_lib.conv2d(\n        inputs,\n        num_outputs,\n        kernel_size,\n        stride=1,\n        rate=rate,\n        padding=\'SAME\',\n        scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = array_ops.pad(\n        inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return layers_lib.conv2d(\n        inputs,\n        num_outputs,\n        kernel_size,\n        stride=stride,\n        rate=rate,\n        padding=\'VALID\',\n        scope=scope)\n\n\n@add_arg_scope\ndef stack_blocks_dense(net,\n                       blocks,\n                       output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with variable_scope.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with variable_scope.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\n\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(\n                net,\n                depth=unit_depth,\n                depth_bottleneck=unit_depth_bottleneck,\n                stride=1,\n                rate=rate)\n            rate *= unit_stride\n\n          else:\n            net = block.unit_fn(\n                net,\n                depth=unit_depth,\n                depth_bottleneck=unit_depth_bottleneck,\n                stride=unit_stride,\n                rate=1)\n            current_stride *= unit_stride\n      net = utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(is_training=True,\n                     weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    is_training: Whether or not we are training the parameters in the batch\n      normalization layers of the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': ops.GraphKeys.UPDATE_OPS,\n  }\n\n  with arg_scope(\n      [layers_lib.conv2d],\n      weights_regularizer=regularizers.l2_regularizer(weight_decay),\n      weights_initializer=initializers.variance_scaling_initializer(),\n      activation_fn=nn_ops.relu,\n      normalizer_fn=layers.batch_norm,\n      normalizer_params=batch_norm_params):\n    with arg_scope([layers.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # tf.contrib.framework.arg_scope([tf.contrib.layers.max_pool2d], padding=\'VALID\').\n      with arg_scope([layers.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
linux/tensorbox/utils/slim_nets/resnet_v1.py,5,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n'"
