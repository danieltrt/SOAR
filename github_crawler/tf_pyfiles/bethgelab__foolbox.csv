file_path,api_count,code
setup.py,0,"b'from setuptools import setup  # type: ignore\nfrom setuptools import find_packages\nfrom os.path import join, dirname\n\nwith open(join(dirname(__file__), ""foolbox/VERSION"")) as f:\n    version = f.read().strip()\n\ntry:\n    # obtain long description from README\n    readme_path = join(dirname(__file__), ""README.rst"")\n    with open(readme_path, encoding=""utf-8"") as f:\n        README = f.read()\n        # remove raw html not supported by PyPI\n        README = ""\\n"".join(README.split(""\\n"")[3:])\nexcept IOError:\n    README = """"\n\n\ninstall_requires = [\n    ""numpy"",\n    ""scipy"",\n    ""setuptools"",\n    ""eagerpy==0.27.0"",\n    ""GitPython>=3.0.7"",\n    ""typing-extensions>=3.7.4.1"",\n]\ntests_require = [""pytest>=5.3.5"", ""pytest-cov>=2.8.1""]\n\n\nsetup(\n    name=""foolbox"",\n    version=version,\n    description=""Foolbox Native is an adversarial attacks library that works natively with PyTorch, TensorFlow and JAX"",\n    long_description=README,\n    long_description_content_type=""text/x-rst"",\n    classifiers=[\n        ""Development Status :: 3 - Alpha"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    keywords="""",\n    author=""Jonas Rauber"",\n    author_email=""git@jonasrauber.de"",\n    url=""https://github.com/bethgelab/foolbox"",\n    license="""",\n    packages=find_packages(),\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=install_requires,\n    extras_require={""testing"": tests_require},\n)\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\nimport foolbox\n\n\n# -- Project information -----------------------------------------------------\n\nproject = ""Foolbox""\ncopyright = ""2020, Jonas Rauber""\nauthor = ""Jonas Rauber""\n\nversion = foolbox.__version__\nrelease = foolbox.__version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.autodoc.typehints"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.napoleon"",\n]\n\n# autodoc_typehints = ""signature""\nautodoc_typehints = ""description""\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store""]\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n# html_theme = ""sphinx_typlog_theme""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n'"
examples/evaluate.py,0,"b'#!/usr/bin/env python3\nimport torchvision.models as models\nimport eagerpy as ep\nfrom foolbox import PyTorchModel, accuracy, samples\nimport foolbox.attacks as fa\nimport numpy as np\n\n\nif __name__ == ""__main__"":\n    # instantiate a model\n    model = models.resnet18(pretrained=True).eval()\n    preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n\n    # get data and test the model\n    # wrapping the tensors with ep.astensors is optional, but it allows\n    # us to work with EagerPy tensors in the following\n    images, labels = ep.astensors(*samples(fmodel, dataset=""imagenet"", batchsize=16))\n    print(""accuracy"")\n    print(accuracy(fmodel, images, labels))\n    print("""")\n\n    attacks = [\n        fa.FGSM(),\n        fa.LinfPGD(),\n        fa.LinfBasicIterativeAttack(),\n        fa.LinfAdditiveUniformNoiseAttack(),\n        fa.LinfDeepFoolAttack(),\n    ]\n\n    epsilons = [\n        0.0,\n        0.0005,\n        0.001,\n        0.0015,\n        0.002,\n        0.003,\n        0.005,\n        0.01,\n        0.02,\n        0.03,\n        0.1,\n        0.3,\n        0.5,\n        1.0,\n    ]\n    print(""epsilons"")\n    print(epsilons)\n    print("""")\n\n    attack_success = np.zeros((len(attacks), len(epsilons), len(images)), dtype=np.bool)\n    for i, attack in enumerate(attacks):\n        _, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n        assert success.shape == (len(epsilons), len(images))\n        success_ = success.numpy()\n        assert success_.dtype == np.bool\n        attack_success[i] = success_\n        print(attack)\n        print(""  "", 1.0 - success_.mean(axis=-1).round(2))\n\n    robust_accuracy = 1.0 - attack_success.max(axis=0).mean(axis=-1)\n    print("""")\n    print(""-"" * 79)\n    print("""")\n    print(""worst case (best attack per-sample)"")\n    print(""  "", robust_accuracy.round(2))\n'"
examples/pytorch_resnet18.py,0,"b'#!/usr/bin/env python3\nimport torchvision.models as models\nimport eagerpy as ep\nfrom foolbox import PyTorchModel, accuracy, samples\nfrom foolbox.attacks import LinfPGD\n\n\nif __name__ == ""__main__"":\n    # instantiate a model\n    model = models.resnet18(pretrained=True).eval()\n    preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n\n    # get data and test the model\n    # wrapping the tensors with ep.astensors is optional, but it allows\n    # us to work with EagerPy tensors in the following\n    images, labels = ep.astensors(*samples(fmodel, dataset=""imagenet"", batchsize=16))\n    print(accuracy(fmodel, images, labels))\n\n    # apply the attack\n    attack = LinfPGD()\n    epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n    advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n\n    # calculate and report the robust accuracy\n    robust_accuracy = 1 - success.float32().mean(axis=-1)\n    for eps, acc in zip(epsilons, robust_accuracy):\n        print(eps, acc.item())\n\n    # we can also manually check this\n    for eps, advs_ in zip(epsilons, advs):\n        print(eps, accuracy(fmodel, advs_, labels))\n        # but then we also need to look at the perturbation sizes\n        # and check if they are smaller than eps\n        print((advs_ - images).norms.linf(axis=(1, 2, 3)).numpy())\n'"
examples/spatial_attack.py,0,"b'#!/usr/bin/env python3\nimport torchvision.models as models\nimport eagerpy as ep\nfrom foolbox import PyTorchModel, accuracy, samples\nimport foolbox.attacks as fa\nimport numpy as np\n\n\nif __name__ == ""__main__"":\n    # instantiate a model\n    model = models.resnet18(pretrained=True).eval()\n    preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n\n    # get data and test the model\n    # wrapping the tensors with ep.astensors is optional, but it allows\n    # us to work with EagerPy tensors in the following\n    images, labels = ep.astensors(*samples(fmodel, dataset=""imagenet"", batchsize=16))\n    print(""accuracy"")\n    print(accuracy(fmodel, images, labels))\n    print("""")\n\n    # attacktrys a combination of specified rotations and translations to an image\n    # stops early if adversarial shifts and translations for all images are found\n    attack = fa.spatial_attack.SpatialAttack(\n        max_translation=6,  # 6px so x in [x-6, x+6] and y in [y-6, y+6]\n        num_translations=6,  # number of translations in x, y.\n        max_rotation=20,  # +- rotation in degrees\n        num_rotations=5,  # number of rotations\n        # max total iterations = num_rotations * num_translations**2\n    )\n\n    xp_, _, success = attack(fmodel, images, labels)\n    print(\n        ""attack success in specified rotation in translation bounds"",\n        success.numpy().astype(np.float32).mean() * 100,\n        "" %"",\n    )\n'"
examples/substituion_model.py,0,"b'#!/usr/bin/env python3\n# mypy: no-disallow-untyped-defs\nimport torchvision.models as models\nimport eagerpy as ep\nfrom foolbox import PyTorchModel, accuracy, samples\nfrom foolbox.attacks import LinfPGD\nfrom foolbox.attacks.base import get_criterion\n\n\nif __name__ == ""__main__"":\n    # instantiate a model\n    model = models.resnet18(pretrained=True).eval()\n    preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n\n    # get data and test the model\n    # wrapping the tensors with ep.astensors is optional, but it allows\n    # us to work with EagerPy tensors in the following\n    images, labels = ep.astensors(*samples(fmodel, dataset=""imagenet"", batchsize=16))\n    print(accuracy(fmodel, images, labels))\n\n    # replace the gradient with the gradient from another model\n    model2 = fmodel  # demo, we just use the same model\n\n    # TODO: this is still a bit annoying because we need\n    # to overwrite run to get the labels\n    class Attack(LinfPGD):\n        def value_and_grad(self, loss_fn, x):\n            val1 = loss_fn(x)\n            loss_fn2 = self.get_loss_fn(model2, self.labels)\n            _, grad2 = ep.value_and_grad(loss_fn2, x)\n            return val1, grad2\n\n        def run(self, model, inputs, criterion, *, epsilon, **kwargs):\n            criterion_ = get_criterion(criterion)\n            self.labels = criterion_.labels\n            return super().run(model, inputs, criterion_, epsilon=epsilon, **kwargs)\n\n    # apply the attack\n    attack = Attack()\n    epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n    advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n\n    # calculate and report the robust accuracy\n    robust_accuracy = 1 - success.float32().mean(axis=-1)\n    for eps, acc in zip(epsilons, robust_accuracy):\n        print(eps, acc.item())\n\n    # we can also manually check this\n    for eps, advs_ in zip(epsilons, advs):\n        print(eps, accuracy(fmodel, advs_, labels))\n        # but then we also need to look at the perturbation sizes\n        # and check if they are smaller than eps\n        print((advs_ - images).norms.linf(axis=(1, 2, 3)).numpy())\n'"
examples/tensorflow_resnet50.py,1,"b'#!/usr/bin/env python3\nimport tensorflow as tf\nimport eagerpy as ep\nfrom foolbox import TensorFlowModel, accuracy, samples\nfrom foolbox.attacks import LinfPGD\n\n\nif __name__ == ""__main__"":\n    # instantiate a model\n    model = tf.keras.applications.ResNet50(weights=""imagenet"")\n    pre = dict(flip_axis=-1, mean=[104.0, 116.0, 123.0])  # RGB to BGR\n    fmodel = TensorFlowModel(model, bounds=(0, 255), preprocessing=pre)\n\n    # get data and test the model\n    # wrapping the tensors with ep.astensors is optional, but it allows\n    # us to work with EagerPy tensors in the following\n    images, labels = ep.astensors(*samples(fmodel, dataset=""imagenet"", batchsize=16))\n    print(accuracy(fmodel, images, labels))\n\n    # apply the attack\n    attack = LinfPGD()\n    epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n    advs, _, success = attack(fmodel, images, labels, epsilons=epsilons)\n\n    # calculate and report the robust accuracy\n    robust_accuracy = 1 - success.float32().mean(axis=-1)\n    for eps, acc in zip(epsilons, robust_accuracy):\n        print(eps, acc.item())\n\n    # we can also manually check this\n    for eps, advs_ in zip(epsilons, advs):\n        print(eps, accuracy(fmodel, advs_, labels))\n        # but then we also need to look at the perturbation sizes\n        # and check if they are smaller than eps\n        print((advs_ - images).norms.linf(axis=(1, 2, 3)).numpy())\n'"
foolbox/__init__.py,0,"b'from os.path import join as _join\nfrom os.path import dirname as _dirname\n\nwith open(_join(_dirname(__file__), ""VERSION"")) as _f:\n    __version__ = _f.read().strip()\n\n# internal modules\n# ----------------\n\nfrom . import devutils  # noqa: F401\nfrom . import tensorboard  # noqa: F401\nfrom . import types  # noqa: F401\n\n# user-facing modules\n# -------------------\n\nfrom .distances import Distance  # noqa: F401\nfrom . import distances  # noqa: F401\n\nfrom .criteria import Criterion  # noqa: F401\nfrom .criteria import Misclassification  # noqa: F401\nfrom .criteria import TargetedMisclassification  # noqa: F401\n\nfrom . import plot  # noqa: F401\n\nfrom .models import Model  # noqa: F401\nfrom .models import PyTorchModel  # noqa: F401\nfrom .models import TensorFlowModel  # noqa: F401\nfrom .models import JAXModel  # noqa: F401\nfrom .models import NumPyModel  # noqa: F401\n\nfrom .utils import accuracy  # noqa: F401\nfrom .utils import samples  # noqa: F401\n\nfrom .attacks import Attack  # noqa: F401\nfrom . import attacks  # noqa: F401\n\nfrom . import zoo  # noqa: F401\n\nfrom . import gradient_estimators  # noqa: F401\n'"
foolbox/criteria.py,0,"b'""""""\n===============================================================================\n:mod:`foolbox.criteria`\n===============================================================================\n\nCriteria are used to define which inputs are adversarial.\nWe provide common criteria for untargeted and targeted adversarial attacks,\ne.g. :class:`Misclassification` and :class:`TargetedMisclassification`.\nNew criteria can easily be implemented by subclassing :class:`Criterion`\nand implementing :meth:`Criterion.__call__`.\n\nCriteria can be combined using a logical and ``criterion1 & criterion2``\nto create a new criterion.\n\n\n:class:`Misclassification`\n===============================================================================\n\n.. code-block:: python\n\n   from foolbox.criteria import Misclassification\n   criterion = Misclassification(labels)\n\n.. autoclass:: Misclassification\n   :members:\n\n\n:class:`TargetedMisclassification`\n===============================================================================\n\n.. code-block:: python\n\n   from foolbox.criteria import TargetedMisclassification\n   criterion = TargetedMisclassification(target_classes)\n\n.. autoclass:: TargetedMisclassification\n   :members:\n\n\n:class:`Criterion`\n===============================================================================\n\n.. autoclass:: Criterion\n   :members:\n   :special-members: __call__\n""""""\nfrom typing import TypeVar, Any\nfrom abc import ABC, abstractmethod\nimport eagerpy as ep\n\n\nT = TypeVar(""T"")\n\n\nclass Criterion(ABC):\n    """"""Abstract base class to implement new criteria.""""""\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        ...\n\n    @abstractmethod\n    def __call__(self, perturbed: T, outputs: T) -> T:\n        """"""Returns a boolean tensor indicating which perturbed inputs are adversarial.\n\n        Args:\n            perturbed: Tensor with perturbed inputs ``(batch, ...)``.\n            outputs: Tensor with model outputs for the perturbed inputs ``(batch, ...)``.\n\n        Returns:\n            A boolean tensor indicating which perturbed inputs are adversarial ``(batch,)``.\n        """"""\n        ...\n\n    def __and__(self, other: ""Criterion"") -> ""Criterion"":\n        return _And(self, other)\n\n\nclass _And(Criterion):\n    def __init__(self, a: Criterion, b: Criterion):\n        super().__init__()\n        self.a = a\n        self.b = b\n\n    def __repr__(self) -> str:\n        return f""{self.a!r} & {self.b!r}""\n\n    def __call__(self, perturbed: T, outputs: T) -> T:\n        args, restore_type = ep.astensors_(perturbed, outputs)\n        a = self.a(*args)\n        b = self.b(*args)\n        is_adv = ep.logical_and(a, b)\n        return restore_type(is_adv)\n\n\nclass Misclassification(Criterion):\n    """"""Considers those perturbed inputs adversarial whose predicted class\n    differs from the label.\n\n    Args:\n        labels: Tensor with labels of the unperturbed inputs ``(batch,)``.\n    """"""\n\n    def __init__(self, labels: Any):\n        super().__init__()\n        self.labels: ep.Tensor = ep.astensor(labels)\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}({self.labels!r})""\n\n    def __call__(self, perturbed: T, outputs: T) -> T:\n        outputs_, restore_type = ep.astensor_(outputs)\n        del perturbed, outputs\n\n        classes = outputs_.argmax(axis=-1)\n        assert classes.shape == self.labels.shape\n        is_adv = classes != self.labels\n        return restore_type(is_adv)\n\n\nclass TargetedMisclassification(Criterion):\n    """"""Considers those perturbed inputs adversarial whose predicted class\n    matches the target class.\n\n    Args:\n        target_classes: Tensor with target classes ``(batch,)``.\n    """"""\n\n    def __init__(self, target_classes: Any):\n        super().__init__()\n        self.target_classes: ep.Tensor = ep.astensor(target_classes)\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}({self.target_classes!r})""\n\n    def __call__(self, perturbed: T, outputs: T) -> T:\n        outputs_, restore_type = ep.astensor_(outputs)\n        del perturbed, outputs\n\n        classes = outputs_.argmax(axis=-1)\n        assert classes.shape == self.target_classes.shape\n        is_adv = classes == self.target_classes\n        return restore_type(is_adv)\n'"
foolbox/devutils.py,0,"b'""""""Internal module with utility functions""""""\nimport eagerpy as ep\n\n\ndef flatten(x: ep.Tensor, keep: int = 1) -> ep.Tensor:\n    return x.flatten(start=keep)\n\n\ndef atleast_kd(x: ep.Tensor, k: int) -> ep.Tensor:\n    shape = x.shape + (1,) * (k - x.ndim)\n    return x.reshape(shape)\n'"
foolbox/distances.py,0,"b'from abc import ABC, abstractmethod\nfrom typing import TypeVar\nimport eagerpy as ep\n\nfrom .devutils import flatten\nfrom .devutils import atleast_kd\n\n\nT = TypeVar(""T"")\n\n\nclass Distance(ABC):\n    @abstractmethod\n    def __call__(self, reference: T, perturbed: T) -> T:\n        ...\n\n    @abstractmethod\n    def clip_perturbation(self, references: T, perturbed: T, epsilon: float) -> T:\n        ...\n\n\nclass LpDistance(Distance):\n    def __init__(self, p: float):\n        self.p = p\n\n    def __repr__(self) -> str:\n        return f""LpDistance({self.p})""\n\n    def __str__(self) -> str:\n        return f""L{self.p} distance""\n\n    def __call__(self, references: T, perturbed: T) -> T:\n        """"""Calculates the distances from references to perturbed using the Lp norm.\n\n        Args:\n            references: A batch of reference inputs.\n            perturbed: A batch of perturbed inputs.\n\n        Returns:\n            A 1D tensor with the distances from references to perturbed.\n        """"""\n        (x, y), restore_type = ep.astensors_(references, perturbed)\n        norms = ep.norms.lp(flatten(y - x), self.p, axis=-1)\n        return restore_type(norms)\n\n    def clip_perturbation(self, references: T, perturbed: T, epsilon: float) -> T:\n        """"""Clips the perturbations to epsilon and returns the new perturbed\n\n        Args:\n            references: A batch of reference inputs.\n            perturbed: A batch of perturbed inputs.\n\n        Returns:\n            A tenosr like perturbed but with the perturbation clipped to epsilon.\n        """"""\n        (x, y), restore_type = ep.astensors_(references, perturbed)\n        p = y - x\n        if self.p == ep.inf:\n            clipped_perturbation = ep.clip(p, -epsilon, epsilon)\n            return restore_type(x + clipped_perturbation)\n        norms = ep.norms.lp(flatten(p), self.p, axis=-1)\n        norms = ep.maximum(norms, 1e-12)  # avoid divsion by zero\n        factor = epsilon / norms\n        factor = ep.minimum(1, factor)  # clipping -> decreasing but not increasing\n        if self.p == 0:\n            if (factor == 1).all():\n                return perturbed\n            raise NotImplementedError(""reducing L0 norms not yet supported"")\n        factor = atleast_kd(factor, x.ndim)\n        clipped_perturbation = factor * p\n        return restore_type(x + clipped_perturbation)\n\n\nl0 = LpDistance(0)\nl1 = LpDistance(1)\nl2 = LpDistance(2)\nlinf = LpDistance(ep.inf)\n'"
foolbox/gradient_estimators.py,0,"b'from typing import Callable, Tuple, Type\nimport eagerpy as ep\nfrom .types import BoundsInput, Bounds\nfrom .attacks.base import Attack\n\n\ndef evolutionary_strategies_gradient_estimator(\n    AttackCls: Type[Attack],\n    *,\n    samples: int,\n    sigma: float,\n    bounds: BoundsInput,\n    clip: bool,\n) -> Type[Attack]:\n\n    if not hasattr(AttackCls, ""value_and_grad""):\n        raise ValueError(\n            ""This attack does not support gradient estimators.""\n        )  # pragma: no cover\n\n    bounds = Bounds(*bounds)\n\n    class GradientEstimator(AttackCls):  # type: ignore\n        def value_and_grad(\n            self, loss_fn: Callable[[ep.Tensor], ep.Tensor], x: ep.Tensor,\n        ) -> Tuple[ep.Tensor, ep.Tensor]:\n            value = loss_fn(x)\n\n            gradient = ep.zeros_like(x)\n            for k in range(samples // 2):\n                noise = ep.normal(x, shape=x.shape)\n\n                pos_theta = x + sigma * noise\n                neg_theta = x - sigma * noise\n\n                if clip:\n                    pos_theta = pos_theta.clip(*bounds)\n                    neg_theta = neg_theta.clip(*bounds)\n\n                pos_loss = loss_fn(pos_theta)\n                neg_loss = loss_fn(neg_theta)\n\n                gradient += (pos_loss - neg_loss) * noise\n\n            gradient /= 2 * sigma * 2 * samples\n\n            return value, gradient\n\n    GradientEstimator.__name__ = AttackCls.__name__ + ""WithESGradientEstimator""\n    GradientEstimator.__qualname__ = AttackCls.__qualname__ + ""WithESGradientEstimator""\n    return GradientEstimator\n\n\nes_gradient_estimator = evolutionary_strategies_gradient_estimator\n'"
foolbox/plot.py,0,"b'from typing import Tuple, Any, Optional\nimport numpy as np\nimport eagerpy as ep\n\n\ndef images(\n    images: Any,\n    *,\n    n: Optional[int] = None,\n    data_format: Optional[str] = None,\n    bounds: Tuple[float, float] = (0, 1),\n    ncols: Optional[int] = None,\n    nrows: Optional[int] = None,\n    figsize: Optional[Tuple[float, float]] = None,\n    scale: float = 1,\n    **kwargs: Any,\n) -> None:\n    import matplotlib.pyplot as plt\n\n    x: ep.Tensor = ep.astensor(images)\n    if x.ndim != 4:\n        raise ValueError(\n            ""expected images to have four dimensions: (N, C, H, W) or (N, H, W, C)""\n        )\n    if n is not None:\n        x = x[:n]\n    if data_format is None:\n        channels_first = x.shape[1] == 1 or x.shape[1] == 3\n        channels_last = x.shape[-1] == 1 or x.shape[-1] == 3\n        if channels_first == channels_last:\n            raise ValueError(""data_format ambigous, please specify it explicitly"")\n    else:\n        channels_first = data_format == ""channels_first""\n        channels_last = data_format == ""channels_last""\n        if not channels_first and not channels_last:\n            raise ValueError(\n                ""expected data_format to be \'channels_first\' or \'channels_last\'""\n            )\n    assert channels_first != channels_last\n    x = x.numpy()\n    if channels_first:\n        x = np.transpose(x, axes=(0, 2, 3, 1))\n    min_, max_ = bounds\n    x = (x - min_) / (max_ - min_)\n\n    if nrows is None and ncols is None:\n        nrows = 1\n    if ncols is None:\n        assert nrows is not None\n        ncols = (len(x) + nrows - 1) // nrows\n    elif nrows is None:\n        nrows = (len(x) + ncols - 1) // ncols\n    if figsize is None:\n        figsize = (ncols * scale, nrows * scale)\n    fig, axes = plt.subplots(\n        ncols=ncols,\n        nrows=nrows,\n        figsize=figsize,\n        squeeze=False,\n        constrained_layout=True,\n        **kwargs,\n    )\n\n    for row in range(nrows):\n        for col in range(ncols):\n            ax = axes[row][col]\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.axis(""off"")\n            i = row * ncols + col\n            if i < len(x):\n                ax.imshow(x[i])\n'"
foolbox/tensorboard.py,0,"b'""""""Internal module for attacks that support logging to TensorBoard""""""\nfrom typing import Union, Callable, TypeVar, Any, cast\nfrom typing_extensions import Literal\nimport eagerpy as ep\nfrom functools import wraps\n\n\nFuncType = Callable[..., None]\nF = TypeVar(""F"", bound=FuncType)\n\n\ndef maybenoop(f: F) -> F:\n    @wraps(f)\n    def wrapper(self: ""TensorBoard"", *args: Any, **kwds: Any) -> None:\n        if self.writer is None:\n            return\n        return f(self, *args, **kwds)\n\n    return cast(F, wrapper)\n\n\nclass TensorBoard:\n    """"""A custom TensorBoard class that accepts EagerPy tensors and that\n    can be disabled by turned into a noop by passing logdir=False.\n\n    This makes it possible to add tensorboard logging without any if\n    statements and without any computational overhead if it\'s disabled.\n    """"""\n\n    def __init__(self, logdir: Union[Literal[False], None, str]):\n        if logdir or (logdir is None):\n            from tensorboardX import SummaryWriter\n\n            self.writer = SummaryWriter(logdir=logdir)\n        else:\n            self.writer = None\n\n    @maybenoop\n    def close(self) -> None:\n        self.writer.close()\n\n    @maybenoop\n    def scalar(self, tag: str, x: Union[int, float], step: int) -> None:\n        self.writer.add_scalar(tag, x, step)\n\n    @maybenoop\n    def mean(self, tag: str, x: ep.Tensor, step: int) -> None:\n        self.writer.add_scalar(tag, x.mean(axis=0).item(), step)\n\n    @maybenoop\n    def probability(self, tag: str, x: ep.Tensor, step: int) -> None:\n        self.writer.add_scalar(tag, x.float32().mean(axis=0).item(), step)\n\n    @maybenoop\n    def conditional_mean(\n        self, tag: str, x: ep.Tensor, cond: ep.Tensor, step: int\n    ) -> None:\n        cond_ = cond.numpy()\n        if ~cond_.any():\n            return\n        x_ = x.numpy()\n        x_ = x_[cond_]\n        self.writer.add_scalar(tag, x_.mean(axis=0).item(), step)\n\n    @maybenoop\n    def probability_ratio(\n        self, tag: str, x: ep.Tensor, y: ep.Tensor, step: int\n    ) -> None:\n        x_ = x.float32().mean(axis=0).item()\n        y_ = y.float32().mean(axis=0).item()\n        if y_ == 0:\n            return\n        self.writer.add_scalar(tag, x_ / y_, step)\n\n    @maybenoop\n    def histogram(\n        self, tag: str, x: ep.Tensor, step: int, *, first: bool = True\n    ) -> None:\n        x = x.numpy()\n        self.writer.add_histogram(tag, x, step)\n        if first:\n            self.writer.add_scalar(tag + ""/0"", x[0].item(), step)\n'"
foolbox/types.py,0,"b'from typing import NewType, NamedTuple, Union, Tuple, Optional, Dict, Any\n\n\nclass Bounds(NamedTuple):\n    lower: float\n    upper: float\n\n\nBoundsInput = Union[Bounds, Tuple[float, float]]\n\nL0 = NewType(""L0"", float)\nL1 = NewType(""L1"", float)\nL2 = NewType(""L2"", float)\nLinf = NewType(""Linf"", float)\n\nPreprocessing = Optional[Dict[str, Any]]\n'"
foolbox/utils.py,0,"b'from typing import Optional, Tuple, Any\nimport eagerpy as ep\nimport warnings\nimport os\nimport numpy as np\n\nfrom .types import Bounds\nfrom .models import Model\n\n\ndef accuracy(fmodel: Model, inputs: Any, labels: Any) -> float:\n    inputs_, labels_ = ep.astensors(inputs, labels)\n    del inputs, labels\n\n    predictions = fmodel(inputs_).argmax(axis=-1)\n    accuracy = (predictions == labels_).float32().mean()\n    return accuracy.item()\n\n\ndef samples(\n    fmodel: Model,\n    dataset: str = ""imagenet"",\n    index: int = 0,\n    batchsize: int = 1,\n    shape: Tuple[int, int] = (224, 224),\n    data_format: Optional[str] = None,\n    bounds: Optional[Bounds] = None,\n) -> Any:\n    if hasattr(fmodel, ""data_format""):\n        if data_format is None:\n            data_format = fmodel.data_format  # type: ignore\n        elif data_format != fmodel.data_format:  # type: ignore\n            raise ValueError(\n                f""data_format ({data_format}) does not match model.data_format ({fmodel.data_format})""  # type: ignore\n            )\n    elif data_format is None:\n        raise ValueError(\n            ""data_format could not be inferred, please specify it explicitly""\n        )\n\n    if bounds is None:\n        bounds = fmodel.bounds\n\n    images, labels = _samples(\n        dataset=dataset,\n        index=index,\n        batchsize=batchsize,\n        shape=shape,\n        data_format=data_format,\n        bounds=bounds,\n    )\n\n    if hasattr(fmodel, ""dummy"") and fmodel.dummy is not None:  # type: ignore\n        images = ep.from_numpy(fmodel.dummy, images).raw  # type: ignore\n        labels = ep.from_numpy(fmodel.dummy, labels).raw  # type: ignore\n    else:\n        warnings.warn(f""unknown model type {type(fmodel)}, returning NumPy arrays"")\n\n    return images, labels\n\n\ndef _samples(\n    dataset: str,\n    index: int,\n    batchsize: int,\n    shape: Tuple[int, int],\n    data_format: str,\n    bounds: Bounds,\n) -> Tuple[Any, Any]:\n    # TODO: this was copied from foolbox v2\n\n    from PIL import Image\n\n    images, labels = [], []\n    basepath = os.path.dirname(__file__)\n    samplepath = os.path.join(basepath, ""data"")\n    files = os.listdir(samplepath)\n\n    if batchsize > 20:\n        warnings.warn(\n            ""samples() has only 20 samples and repeats itself if batchsize > 20""\n        )\n\n    for idx in range(index, index + batchsize):\n        i = idx % 20\n\n        # get filename and label\n        file = [n for n in files if f""{dataset}_{i:02d}_"" in n][0]\n        label = int(file.split(""."")[0].split(""_"")[-1])\n\n        # open file\n        path = os.path.join(samplepath, file)\n        image = Image.open(path)\n\n        if dataset == ""imagenet"":\n            image = image.resize(shape)\n\n        image = np.asarray(image, dtype=np.float32)\n\n        if image.ndim == 2:\n            image = image[..., np.newaxis]\n\n        assert image.ndim == 3\n\n        if data_format == ""channels_first"":\n            image = np.transpose(image, (2, 0, 1))\n\n        images.append(image)\n        labels.append(label)\n\n    images_ = np.stack(images)\n    labels_ = np.array(labels)\n\n    if bounds != (0, 255):\n        images_ = images_ / 255 * (bounds[1] - bounds[0]) + bounds[0]\n    return images_, labels_\n'"
tests/conftest.py,15,"b'from typing import Optional, Callable, Tuple, Dict, Any, List\nimport functools\nimport pytest\nimport eagerpy as ep\n\nimport foolbox\nimport foolbox as fbn\n\nModelAndData = Tuple[fbn.Model, ep.Tensor, ep.Tensor]\n\nmodels: Dict[str, Tuple[Callable[..., ModelAndData], bool]] = {}\nmodels_for_attacks: List[str] = []\n\n\ndef pytest_addoption(parser: Any) -> None:\n    parser.addoption(""--backend"")\n    parser.addoption(""--skipslow"", action=""store_true"")\n\n\n@pytest.fixture(scope=""session"")\ndef dummy(request: Any) -> ep.Tensor:\n    backend: Optional[str] = request.config.option.backend\n    if backend is None or backend == ""none"":\n        pytest.skip()\n        assert False\n    return ep.utils.get_dummy(backend)\n\n\ndef register(\n    backend: str, *, real: bool, attack: bool = True\n) -> Callable[[Callable], Callable]:\n    def decorator(f: Callable[[Any], ModelAndData]) -> Callable[[Any], ModelAndData]:\n        @functools.wraps(f)\n        def model(request: Any) -> ModelAndData:\n            if request.config.option.backend != backend:\n                pytest.skip()\n            return f(request)\n\n        global models\n        global real_models\n\n        models[model.__name__] = (model, real)\n        if attack:\n            models_for_attacks.append(model.__name__)\n        return model\n\n    return decorator\n\n\ndef pytorch_simple_model(\n    device: Any = None, preprocessing: fbn.types.Preprocessing = None\n) -> ModelAndData:\n    import torch\n\n    class Model(torch.nn.Module):\n        def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n            x = torch.mean(x, 3)\n            x = torch.mean(x, 2)\n            return x\n\n    model = Model().eval()\n    bounds = (0, 1)\n    fmodel = fbn.PyTorchModel(\n        model, bounds=bounds, device=device, preprocessing=preprocessing\n    )\n\n    x, _ = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = fmodel(x).argmax(axis=-1)\n    return fmodel, x, y\n\n\n@register(""pytorch"", real=False)\ndef pytorch_simple_model_default(request: Any) -> ModelAndData:\n    return pytorch_simple_model()\n\n\n@register(""pytorch"", real=False)\ndef pytorch_simple_model_default_flip(request: Any) -> ModelAndData:\n    return pytorch_simple_model(preprocessing=dict(flip_axis=-3))\n\n\n@register(""pytorch"", real=False, attack=False)\ndef pytorch_simple_model_default_cpu_native_tensor(request: Any) -> ModelAndData:\n    import torch\n\n    mean = 0.05 * torch.arange(3).float()\n    std = torch.ones(3) * 2\n    return pytorch_simple_model(""cpu"", preprocessing=dict(mean=mean, std=std, axis=-3))\n\n\n@register(""pytorch"", real=False, attack=False)\ndef pytorch_simple_model_default_cpu_eagerpy_tensor(request: Any) -> ModelAndData:\n    mean = 0.05 * ep.torch.arange(3).float32()\n    std = ep.torch.ones(3) * 2\n    return pytorch_simple_model(""cpu"", preprocessing=dict(mean=mean, std=std, axis=-3))\n\n\n@register(""pytorch"", real=False, attack=False)\ndef pytorch_simple_model_string(request: Any) -> ModelAndData:\n    return pytorch_simple_model(""cpu"")\n\n\n@register(""pytorch"", real=False, attack=False)\ndef pytorch_simple_model_object(request: Any) -> ModelAndData:\n    import torch\n\n    return pytorch_simple_model(torch.device(""cpu""))\n\n\n@register(""pytorch"", real=True)\ndef pytorch_mnist(request: Any) -> ModelAndData:\n    fmodel = fbn.zoo.ModelLoader.get().load(\n        ""examples/zoo/mnist/"", module_name=""foolbox_model""\n    )\n    x, y = fbn.samples(fmodel, dataset=""mnist"", batchsize=16)\n    x = ep.astensor(x)\n    y = ep.astensor(y)\n    return fmodel, x, y\n\n\n@register(""pytorch"", real=True)\ndef pytorch_resnet18(request: Any) -> ModelAndData:\n    if request.config.option.skipslow:\n        pytest.skip()\n\n    import torchvision.models as models\n\n    model = models.resnet18(pretrained=True).eval()\n    preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n    fmodel = fbn.PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n\n    x, y = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = ep.astensor(y)\n    return fmodel, x, y\n\n\ndef tensorflow_simple_sequential(\n    device: Optional[str] = None, preprocessing: fbn.types.Preprocessing = None\n) -> ModelAndData:\n    import tensorflow as tf\n\n    with tf.device(device):\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.GlobalAveragePooling2D())\n    bounds = (0, 1)\n    fmodel = fbn.TensorFlowModel(\n        model, bounds=bounds, device=device, preprocessing=preprocessing\n    )\n\n    x, _ = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = fmodel(x).argmax(axis=-1)\n    return fmodel, x, y\n\n\n@register(""tensorflow"", real=False)\ndef tensorflow_simple_sequential_cpu(request: Any) -> ModelAndData:\n    return tensorflow_simple_sequential(""cpu"", None)\n\n\n@register(""tensorflow"", real=False)\ndef tensorflow_simple_sequential_native_tensors(request: Any) -> ModelAndData:\n    import tensorflow as tf\n\n    mean = tf.zeros(1)\n    std = tf.ones(1) * 255.0\n    return tensorflow_simple_sequential(""cpu"", dict(mean=mean, std=std))\n\n\n@register(""tensorflow"", real=False)\ndef tensorflow_simple_sequential_eagerpy_tensors(request: Any) -> ModelAndData:\n    mean = ep.tensorflow.zeros(1)\n    std = ep.tensorflow.ones(1) * 255.0\n    return tensorflow_simple_sequential(""cpu"", dict(mean=mean, std=std))\n\n\n@register(""tensorflow"", real=False)\ndef tensorflow_simple_subclassing(request: Any) -> ModelAndData:\n    import tensorflow as tf\n\n    class Model(tf.keras.Model):  # type: ignore\n        def __init__(self) -> None:\n            super().__init__()\n            self.pool = tf.keras.layers.GlobalAveragePooling2D()\n\n        def call(self, x: tf.Tensor) -> tf.Tensor:  # type: ignore\n            x = self.pool(x)\n            return x\n\n    model = Model()\n    bounds = (0, 1)\n    fmodel = fbn.TensorFlowModel(model, bounds=bounds)\n\n    x, _ = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = fmodel(x).argmax(axis=-1)\n    return fmodel, x, y\n\n\n@register(""tensorflow"", real=False)\ndef tensorflow_simple_functional(request: Any) -> ModelAndData:\n    import tensorflow as tf\n\n    channels = 3\n    h = w = 224\n    data_format = tf.keras.backend.image_data_format()\n    shape = (channels, h, w) if data_format == ""channels_first"" else (h, w, channels)\n    x = x_ = tf.keras.Input(shape=shape)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    model = tf.keras.Model(inputs=x_, outputs=x)\n    bounds = (0, 1)\n    fmodel = fbn.TensorFlowModel(model, bounds=bounds)\n\n    x, _ = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = fmodel(x).argmax(axis=-1)\n    return fmodel, x, y\n\n\n@register(""tensorflow"", real=True)\ndef tensorflow_mobilenetv2(request: Any) -> ModelAndData:\n    if request.config.option.skipslow:\n        pytest.skip()\n\n    import tensorflow as tf\n\n    model = tf.keras.applications.MobileNetV2(weights=""imagenet"")\n    fmodel = fbn.TensorFlowModel(\n        model, bounds=(0, 255), preprocessing=dict(mean=127.5, std=127.5)\n    )\n\n    x, y = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = ep.astensor(y)\n    return fmodel, x, y\n\n\n@register(""tensorflow"", real=True)\ndef tensorflow_resnet50(request: Any) -> ModelAndData:\n    if request.config.option.skipslow:\n        pytest.skip()\n\n    import tensorflow as tf\n\n    if not tf.test.is_gpu_available():\n        pytest.skip(""ResNet50 test too slow without GPU"")\n\n    model = tf.keras.applications.ResNet50(weights=""imagenet"")\n    preprocessing = dict(flip_axis=-1, mean=[104.0, 116.0, 123.0])  # RGB to BGR\n    fmodel = fbn.TensorFlowModel(model, bounds=(0, 255), preprocessing=preprocessing)\n\n    x, y = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n    x = ep.astensor(x)\n    y = ep.astensor(y)\n    return fmodel, x, y\n\n\n@register(""jax"", real=False)\ndef jax_simple_model(request: Any) -> ModelAndData:\n    import jax\n\n    def model(x: Any) -> Any:\n        return jax.numpy.mean(x, axis=(1, 2))\n\n    bounds = (0, 1)\n    fmodel = fbn.JAXModel(model, bounds=bounds)\n\n    x, _ = fbn.samples(\n        fmodel, dataset=""imagenet"", batchsize=16, data_format=""channels_last""\n    )\n    x = ep.astensor(x)\n    y = fmodel(x).argmax(axis=-1)\n    return fmodel, x, y\n\n\n@register(""numpy"", real=False)\ndef numpy_simple_model(request: Any) -> ModelAndData:\n    class Model:\n        def __call__(self, inputs: Any) -> Any:\n            return inputs.mean(axis=(2, 3))\n\n    model = Model()\n    with pytest.raises(ValueError):\n        fbn.NumPyModel(model, bounds=(0, 1), data_format=""foo"")\n\n    fmodel = fbn.NumPyModel(model, bounds=(0, 1))\n    with pytest.raises(ValueError, match=""data_format""):\n        x, _ = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n\n    fmodel = fbn.NumPyModel(model, bounds=(0, 1), data_format=""channels_first"")\n    with pytest.warns(UserWarning, match=""returning NumPy arrays""):\n        x, _ = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n\n    x = ep.astensor(x)\n    y = fmodel(x).argmax(axis=-1)\n    return fmodel, x, y\n\n\n@pytest.fixture(scope=""session"", params=list(models.keys()))\ndef fmodel_and_data_ext(request: Any) -> Tuple[ModelAndData, bool]:\n    global models\n    model_fn, real = models[request.param]\n    model_and_data = model_fn(request)\n    return model_and_data, real\n\n\n@pytest.fixture(scope=""session"", params=models_for_attacks)\ndef fmodel_and_data_ext_for_attacks(request: Any) -> Tuple[ModelAndData, bool]:\n    global models\n    model_fn, real = models[request.param]\n    model_and_data = model_fn(request)\n    return model_and_data, real\n\n\n@pytest.fixture(scope=""session"")\ndef fmodel_and_data(fmodel_and_data_ext: Tuple[ModelAndData, bool]) -> ModelAndData:\n    fmodel_and_data, _ = fmodel_and_data_ext\n    return fmodel_and_data\n'"
tests/test_attacks.py,0,"b'from typing import List, Tuple, Optional\nimport pytest\nimport eagerpy as ep\n\nimport foolbox as fbn\nimport foolbox.attacks as fa\nfrom foolbox.gradient_estimators import es_gradient_estimator\n\nL2 = fbn.types.L2\nLinf = fbn.types.Linf\n\n\nFGSM_GE = es_gradient_estimator(\n    fa.FGSM, samples=100, sigma=0.03, bounds=(0, 1), clip=True\n)\n\n\ndef get_attack_id(x: Tuple[fbn.Attack, bool, bool]) -> str:\n    return repr(x[0])\n\n\n# attack, eps / None, attack_uses_grad, requires_real_model\nattacks: List[Tuple[fbn.Attack, Optional[float], bool, bool]] = [\n    (fa.DDNAttack(init_epsilon=2.0), None, True, False),\n    (fa.InversionAttack(), None, False, False),\n    (\n        fa.InversionAttack(distance=fbn.distances.l2).repeat(3).repeat(2),\n        None,\n        False,\n        False,\n    ),\n    (fa.L2ContrastReductionAttack(), L2(100.0), False, False),\n    (fa.L2ContrastReductionAttack().repeat(3), 100.0, False, False),\n    (\n        fa.BinarySearchContrastReductionAttack(binary_search_steps=15),\n        None,\n        False,\n        False,\n    ),\n    (fa.LinearSearchContrastReductionAttack(steps=20), None, False, False),\n    (fa.L2CarliniWagnerAttack(binary_search_steps=11, steps=5), None, True, False),\n    (\n        fa.L2CarliniWagnerAttack(binary_search_steps=3, steps=20, confidence=2.0),\n        None,\n        True,\n        False,\n    ),\n    (\n        fa.EADAttack(binary_search_steps=10, steps=20, regularization=0),\n        None,\n        True,\n        False,\n    ),\n    (\n        fa.EADAttack(\n            binary_search_steps=10, steps=20, regularization=0, confidence=2.0\n        ),\n        None,\n        True,\n        False,\n    ),\n    (\n        fa.EADAttack(\n            binary_search_steps=3, steps=20, decision_rule=""L1"", regularization=0\n        ),\n        None,\n        True,\n        False,\n    ),\n    (fa.NewtonFoolAttack(steps=20), None, True, False),\n    (fa.VirtualAdversarialAttack(steps=50, xi=1), 10, True, False),\n    (fa.PGD(), Linf(1.0), True, False),\n    (fa.L2PGD(), L2(50.0), True, False),\n    (fa.L1PGD(), 5000.0, True, False),\n    (fa.LinfBasicIterativeAttack(abs_stepsize=0.2), Linf(1.0), True, False),\n    (fa.L2BasicIterativeAttack(), L2(50.0), True, False),\n    (fa.L1BasicIterativeAttack(), 5000.0, True, False),\n    (fa.SparseL1DescentAttack(), 5000.0, True, False),\n    (fa.FGSM(), Linf(100.0), True, False),\n    (FGSM_GE(), Linf(100.0), False, False),\n    (fa.FGM(), L2(100.0), True, False),\n    (fa.L1FastGradientAttack(), 5000.0, True, False),\n    (fa.GaussianBlurAttack(steps=10), None, True, True),\n    (fa.GaussianBlurAttack(steps=10, max_sigma=224.0), None, True, True),\n    (fa.L2DeepFoolAttack(steps=50, loss=""logits""), None, True, False),\n    (fa.L2DeepFoolAttack(steps=50, loss=""crossentropy""), None, True, False),\n    (fa.LinfDeepFoolAttack(steps=50), None, True, False),\n    (fa.BoundaryAttack(steps=50), None, False, False),\n    (\n        fa.BoundaryAttack(\n            steps=110,\n            init_attack=fa.LinearSearchBlendedUniformNoiseAttack(steps=50),\n            update_stats_every_k=1,\n        ),\n        None,\n        False,\n        False,\n    ),\n    (fa.SaltAndPepperNoiseAttack(steps=50), None, True, False),\n    (fa.SaltAndPepperNoiseAttack(steps=50, channel_axis=1), None, True, False),\n    (fa.LinearSearchBlendedUniformNoiseAttack(steps=50), None, False, False),\n    (fa.L2AdditiveGaussianNoiseAttack(), 2500.0, False, False),\n    (fa.LinfAdditiveUniformNoiseAttack(), 10.0, False, False),\n    (\n        fa.L2RepeatedAdditiveGaussianNoiseAttack(check_trivial=False),\n        1000.0,\n        False,\n        False,\n    ),\n    (fa.L2RepeatedAdditiveGaussianNoiseAttack(), 1000.0, False, False),\n    (fa.L2RepeatedAdditiveUniformNoiseAttack(), 1000.0, False, False),\n    (fa.LinfRepeatedAdditiveUniformNoiseAttack(), 3.0, False, False),\n]\n\n\n@pytest.mark.parametrize(""attack_eps_grad_real"", attacks, ids=get_attack_id)\ndef test_untargeted_attacks(\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n    attack_eps_grad_real: Tuple[fbn.Attack, Optional[float], bool, bool],\n) -> None:\n\n    attack, eps, attack_uses_grad, requires_real_model = attack_eps_grad_real\n    (fmodel, x, y), real = fmodel_and_data_ext_for_attacks\n    if requires_real_model and not real:\n        pytest.skip()\n\n    if isinstance(x, ep.NumPyTensor) and attack_uses_grad:\n        pytest.skip()\n\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n    acc = fbn.accuracy(fmodel, x, y)\n    assert acc > 0\n\n    advs, _, _ = attack(fmodel, x, y, epsilons=eps)\n    assert fbn.accuracy(fmodel, advs, y) < acc\n\n\ntargeted_attacks: List[Tuple[fbn.Attack, Optional[float], bool, bool]] = [\n    (\n        fa.L2CarliniWagnerAttack(binary_search_steps=3, steps=20, initial_const=1e1),\n        None,\n        True,\n        False,\n    ),\n    (fa.DDNAttack(init_epsilon=2.0, steps=20), None, True, False),\n    # TODO: targeted EADAttack currently fails repeatedly on MobileNetv2\n    (\n        fa.EADAttack(\n            binary_search_steps=3,\n            steps=20,\n            abort_early=True,\n            regularization=0,\n            initial_const=1e1,\n        ),\n        None,\n        True,\n        False,\n    ),\n    (fa.GenAttack(steps=100, population=6, reduced_dims=(7, 7)), 0.5, False, True),\n]\n\n\n@pytest.mark.parametrize(""attack_eps_grad_real"", targeted_attacks, ids=get_attack_id)\ndef test_targeted_attacks(\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n    attack_eps_grad_real: Tuple[fbn.Attack, Optional[float], bool, bool],\n) -> None:\n\n    attack, eps, attack_uses_grad, requires_real_model = attack_eps_grad_real\n    (fmodel, x, y), real = fmodel_and_data_ext_for_attacks\n    if requires_real_model and not real:\n        pytest.skip()\n\n    if isinstance(x, ep.NumPyTensor) and attack_uses_grad:\n        pytest.skip()\n\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n\n    num_classes = fmodel(x).shape[-1]\n    target_classes = (y + 1) % num_classes\n    criterion = fbn.TargetedMisclassification(target_classes)\n    adv_before_attack = criterion(x, fmodel(x))\n    assert not adv_before_attack.all()\n\n    advs, _, _ = attack(fmodel, x, criterion, epsilons=eps)\n    adv_after_attack = criterion(advs, fmodel(advs))\n    assert adv_after_attack.sum().item() > adv_before_attack.sum().item()\n'"
tests/test_attacks_base.py,0,"b'from typing import Tuple\nimport pytest\nimport eagerpy as ep\nimport foolbox as fbn\n\n\nattacks = [\n    fbn.attacks.InversionAttack(distance=fbn.distances.l2),\n    fbn.attacks.InversionAttack(distance=fbn.distances.l2).repeat(3),\n    fbn.attacks.L2ContrastReductionAttack(),\n    fbn.attacks.L2ContrastReductionAttack().repeat(3),\n]\n\n\n@pytest.mark.parametrize(""attack"", attacks)\ndef test_call_one_epsilon(\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n    attack: fbn.Attack,\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n\n    assert ep.istensor(x)\n    assert ep.istensor(y)\n\n    raw, clipped, success = attack(fmodel, x, y, epsilons=1.0)\n    assert ep.istensor(raw)\n    assert ep.istensor(clipped)\n    assert ep.istensor(success)\n    assert raw.shape == x.shape\n    assert clipped.shape == x.shape\n    assert success.shape == (len(x),)\n\n\ndef test_get_channel_axis() -> None:\n    class Model:\n        data_format = None\n\n    model = Model()\n    model.data_format = ""channels_first""  # type: ignore\n    assert fbn.attacks.base.get_channel_axis(model, 3) == 1  # type: ignore\n    model.data_format = ""channels_last""  # type: ignore\n    assert fbn.attacks.base.get_channel_axis(model, 3) == 2  # type: ignore\n    model.data_format = ""invalid""  # type: ignore\n    with pytest.raises(ValueError):\n        assert fbn.attacks.base.get_channel_axis(model, 3)  # type: ignore\n\n\ndef test_transform_bounds_wrapper_data_format() -> None:\n    class Model(fbn.models.Model):\n        data_format = ""channels_first""\n\n        @property\n        def bounds(self) -> fbn.types.Bounds:\n            return fbn.types.Bounds(0, 1)\n\n        def __call__(self, inputs: fbn.models.base.T) -> fbn.models.base.T:\n            return inputs\n\n    model = Model()\n    wrapped_model = fbn.models.TransformBoundsWrapper(model, (0, 1))\n    assert fbn.attacks.base.get_channel_axis(\n        model, 3\n    ) == fbn.attacks.base.get_channel_axis(wrapped_model, 3)\n'"
tests/test_attacks_raise.py,0,"b'from typing import List, Tuple, Any\nimport pytest\nimport eagerpy as ep\nimport foolbox as fbn\n\nL2 = fbn.types.L2\nLinf = fbn.types.Linf\n\n\ndef test_ead_init_raises() -> None:\n    with pytest.raises(ValueError, match=""invalid decision rule""):\n        fbn.attacks.EADAttack(binary_search_steps=3, steps=20, decision_rule=""invalid"")  # type: ignore\n\n\ndef test_genattack_numpy(request: Any) -> None:\n    class Model:\n        def __call__(self, inputs: Any) -> Any:\n            return inputs.mean(axis=(2, 3))\n\n    model = Model()\n    with pytest.raises(ValueError):\n        fbn.NumPyModel(model, bounds=(0, 1), data_format=""foo"")\n\n    fmodel = fbn.NumPyModel(model, bounds=(0, 1))\n    x, y = ep.astensors(\n        *fbn.samples(\n            fmodel, dataset=""imagenet"", batchsize=16, data_format=""channels_first""\n        )\n    )\n\n    with pytest.raises(ValueError, match=""data_format""):\n        fbn.attacks.GenAttack(reduced_dims=(2, 2)).run(\n            fmodel, x, fbn.TargetedMisclassification(y), epsilon=0.3\n        )\n\n    with pytest.raises(ValueError, match=""channel_axis""):\n        fbn.attacks.GenAttack(channel_axis=2, reduced_dims=(2, 2)).run(\n            fmodel, x, fbn.TargetedMisclassification(y), epsilon=0.3\n        )\n\n\ndef test_deepfool_run_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    if isinstance(x, ep.NumPyTensor):\n        pytest.skip()\n\n    attack = fbn.attacks.L2DeepFoolAttack(loss=""invalid"")  # type: ignore\n    with pytest.raises(ValueError, match=""expected loss to""):\n        attack.run(fmodel, x, y)\n\n\ndef test_blended_noise_attack_run_warns(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    attack = fbn.attacks.LinearSearchBlendedUniformNoiseAttack(directions=1)\n    attack.run(fmodel, x, y)\n\n\ndef test_boundary_attack_run_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n\n    with pytest.raises(ValueError, match=""starting_points are not adversarial""):\n        attack = fbn.attacks.BoundaryAttack()\n        attack.run(fmodel, x, y, starting_points=x)\n\n    if isinstance(x, ep.NumPyTensor):\n        pytest.skip()\n    with pytest.raises(ValueError, match=""init_attack failed for""):\n        attack = fbn.attacks.BoundaryAttack(\n            init_attack=fbn.attacks.DDNAttack(init_epsilon=0.0, steps=1)\n        )\n        attack.run(fmodel, x, y)\n\n\ndef test_newtonfool_run_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    if isinstance(x, ep.NumPyTensor):\n        pytest.skip()\n\n    with pytest.raises(ValueError, match=""unsupported criterion""):\n        attack = fbn.attacks.NewtonFoolAttack()\n        attack.run(fmodel, x, fbn.TargetedMisclassification(y))\n\n    with pytest.raises(ValueError, match=""expected labels to have shape""):\n        attack = fbn.attacks.NewtonFoolAttack(steps=10)\n        attack.run(fmodel, x, ep.concatenate((y, y), 0))\n\n\ndef test_fgsm_run_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    if isinstance(x, ep.NumPyTensor):\n        pytest.skip()\n\n    with pytest.raises(ValueError, match=""unsupported criterion""):\n        attack = fbn.attacks.FGSM()\n        attack.run(fmodel, x, fbn.TargetedMisclassification(y), epsilon=1000)\n\n\ndef test_vat_run_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    if isinstance(x, ep.NumPyTensor):\n        pytest.skip()\n\n    with pytest.raises(ValueError, match=""unsupported criterion""):\n        attack = fbn.attacks.VirtualAdversarialAttack(steps=10)\n        attack.run(fmodel, x, fbn.TargetedMisclassification(y), epsilon=1.0)\n\n    with pytest.raises(ValueError, match=""expected labels to have shape""):\n        attack = fbn.attacks.VirtualAdversarialAttack(steps=10)\n        attack.run(fmodel, x, ep.concatenate((y, y), 0), epsilon=1.0)\n\n\ndef test_blended_noise_init_raises() -> None:\n    with pytest.raises(ValueError, match=""directions must be larger than 0""):\n        fbn.attacks.LinearSearchBlendedUniformNoiseAttack(steps=50, directions=0)\n\n\ndef test_blur_run_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    with pytest.raises(ValueError, match=""to be 1 or 3""):\n        attack = fbn.attacks.GaussianBlurAttack(steps=10, channel_axis=2)\n        attack.run(fmodel, x, y)\n\n\ndef test_blur_numpy(request: Any) -> None:\n    class Model:\n        def __call__(self, inputs: Any) -> Any:\n            return inputs.mean(axis=(2, 3))\n\n    model = Model()\n    with pytest.raises(ValueError):\n        fbn.NumPyModel(model, bounds=(0, 1), data_format=""foo"")\n\n    fmodel = fbn.NumPyModel(model, bounds=(0, 1))\n    x, y = ep.astensors(\n        *fbn.samples(\n            fmodel, dataset=""imagenet"", batchsize=16, data_format=""channels_first""\n        )\n    )\n    with pytest.raises(ValueError, match=""data_format""):\n        fbn.attacks.GaussianBlurAttack()(fmodel, x, y, epsilons=None)\n\n\ndef test_dataset_attack_raises(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool]\n) -> None:\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n\n    attack = fbn.attacks.DatasetAttack()\n\n    # that that running before feed fails properly\n    with pytest.raises(ValueError, match=""feed""):\n        attack.run(fmodel, x, y)\n\n    attack.feed(fmodel, x)\n    attack.run(fmodel, x, y)\n    assert attack.inputs is not None\n    n = len(attack.inputs)\n\n    # test that feed() after run works\n    attack.feed(fmodel, x)\n    attack.run(fmodel, x, y)\n    assert len(attack.inputs) > n\n\n\ntargeted_attacks_raises_exception: List[Tuple[fbn.Attack, bool]] = [\n    (fbn.attacks.EADAttack(), True),\n    (fbn.attacks.DDNAttack(), True),\n    (fbn.attacks.L2CarliniWagnerAttack(), True),\n    (fbn.attacks.GenAttack(), False),\n]\n\n\n@pytest.mark.parametrize(\n    ""attack_exception_text_and_grad"", targeted_attacks_raises_exception\n)\ndef test_targeted_attacks_call_raises_exception(\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n    attack_exception_text_and_grad: Tuple[fbn.Attack, bool],\n) -> None:\n\n    attack, attack_uses_grad = attack_exception_text_and_grad\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n\n    if isinstance(x, ep.NumPyTensor) and attack_uses_grad:\n        pytest.skip()\n\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n\n    num_classes = fmodel(x).shape[-1]\n    target_classes = (y + 1) % num_classes\n    invalid_target_classes = ep.concatenate((target_classes, target_classes), 0)\n    invalid_targeted_criterion = fbn.TargetedMisclassification(invalid_target_classes)\n\n    class DummyCriterion(fbn.Criterion):\n        """"""Criterion without any functionality which is just meant to be\n        rejected by the attacks\n        """"""\n\n        def __repr__(self) -> str:\n            return """"\n\n        def __call__(\n            self, perturbed: fbn.criteria.T, outputs: fbn.criteria.T\n        ) -> fbn.criteria.T:\n            return perturbed\n\n    invalid_criterion = DummyCriterion()\n\n    # check if targeted attack criterion with invalid number of classes is rejected\n    with pytest.raises(ValueError):\n        attack(fmodel, x, invalid_targeted_criterion, epsilons=1000.0)\n\n    # check if only the two valid criteria are accepted\n    with pytest.raises(ValueError):\n        attack(fmodel, x, invalid_criterion, epsilons=1000.0)\n'"
tests/test_binarization_attack.py,0,"b'from typing import Tuple\nimport pytest\nimport eagerpy as ep\n\nfrom foolbox import accuracy, Model\nfrom foolbox.models import ThresholdingWrapper\nfrom foolbox.devutils import flatten\nfrom foolbox.attacks import BinarySearchContrastReductionAttack\nfrom foolbox.attacks import BinarizationRefinementAttack\n\n\ndef test_binarization_attack(\n    fmodel_and_data_ext_for_attacks: Tuple[Tuple[Model, ep.Tensor, ep.Tensor], bool],\n) -> None:\n\n    # get a model with thresholding\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n    fmodel = ThresholdingWrapper(fmodel, threshold=0.5)\n    acc = accuracy(fmodel, x, y)\n    assert acc > 0\n\n    # find some adversarials and check that they are non-trivial\n    attack = BinarySearchContrastReductionAttack(target=0)\n    advs, _, _ = attack(fmodel, x, y, epsilons=None)\n    assert accuracy(fmodel, advs, y) < acc\n\n    # run the refinement attack\n    attack2 = BinarizationRefinementAttack(threshold=0.5, included_in=""upper"")\n    advs2, _, _ = attack2(fmodel, x, y, starting_points=advs, epsilons=None)\n\n    # make sure the predicted classes didn\'t change\n    assert (fmodel(advs).argmax(axis=-1) == fmodel(advs2).argmax(axis=-1)).all()\n\n    # make sure the perturbations didn\'t get larger and some got smaller\n    norms1 = flatten(advs - x).norms.l2(axis=-1)\n    norms2 = flatten(advs2 - x).norms.l2(axis=-1)\n    assert (norms2 <= norms1).all()\n    assert (norms2 < norms1).any()\n\n    # run the refinement attack\n    attack2 = BinarizationRefinementAttack(included_in=""upper"")\n    advs2, _, _ = attack2(fmodel, x, y, starting_points=advs, epsilons=None)\n\n    # make sure the predicted classes didn\'t change\n    assert (fmodel(advs).argmax(axis=-1) == fmodel(advs2).argmax(axis=-1)).all()\n\n    # make sure the perturbations didn\'t get larger and some got smaller\n    norms1 = flatten(advs - x).norms.l2(axis=-1)\n    norms2 = flatten(advs2 - x).norms.l2(axis=-1)\n    assert (norms2 <= norms1).all()\n    assert (norms2 < norms1).any()\n\n    with pytest.raises(ValueError, match=""starting_points""):\n        attack2(fmodel, x, y, epsilons=None)\n\n    attack2 = BinarizationRefinementAttack(included_in=""lower"")\n    with pytest.raises(ValueError, match=""does not match""):\n        attack2(fmodel, x, y, starting_points=advs, epsilons=None)\n\n    attack2 = BinarizationRefinementAttack(included_in=""invalid"")  # type: ignore\n    with pytest.raises(ValueError, match=""expected included_in""):\n        attack2(fmodel, x, y, starting_points=advs, epsilons=None)\n'"
tests/test_brendel_bethge_attack.py,0,"b'from typing import Tuple, Union, List, Any\nimport eagerpy as ep\n\nimport foolbox as fbn\nimport foolbox.attacks as fa\nfrom foolbox.devutils import flatten\nfrom foolbox.attacks.brendel_bethge import BrendelBethgeAttack\nimport pytest\n\n\ndef get_attack_id(x: Tuple[BrendelBethgeAttack, Union[int, float]]) -> str:\n    return repr(x[0])\n\n\nattacks: List[Tuple[fa.Attack, Union[int, float]]] = [\n    (fa.L0BrendelBethgeAttack(steps=20), 0),\n    (fa.L1BrendelBethgeAttack(steps=20), 1),\n    (fa.L2BrendelBethgeAttack(steps=20), 2),\n    (fa.LinfinityBrendelBethgeAttack(steps=20), ep.inf),\n]\n\n\n@pytest.mark.parametrize(""attack_and_p"", attacks, ids=get_attack_id)\ndef test_brendel_bethge_untargeted_attack(\n    request: Any,\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n    attack_and_p: Tuple[BrendelBethgeAttack, Union[int, float]],\n) -> None:\n    if request.config.option.skipslow:\n        pytest.skip()\n\n    (fmodel, x, y), real = fmodel_and_data_ext_for_attacks\n\n    if isinstance(x, ep.NumPyTensor):\n        pytest.skip()\n\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n\n    init_attack = fa.DatasetAttack()\n    init_attack.feed(fmodel, x)\n    init_advs = init_attack.run(fmodel, x, y)\n\n    attack, p = attack_and_p\n    advs = attack.run(fmodel, x, y, starting_points=init_advs)\n\n    init_norms = ep.norms.lp(flatten(init_advs - x), p=p, axis=-1)\n    norms = ep.norms.lp(flatten(advs - x), p=p, axis=-1)\n\n    is_smaller = norms < init_norms\n\n    assert fbn.accuracy(fmodel, advs, y) < fbn.accuracy(fmodel, x, y)\n    assert fbn.accuracy(fmodel, advs, y) <= fbn.accuracy(fmodel, init_advs, y)\n    assert is_smaller.any()\n'"
tests/test_criteria.py,0,"b'from typing import Tuple\nimport foolbox as fbn\nimport eagerpy as ep\n\n\ndef test_correct_unperturbed(\n    fmodel_and_data: Tuple[fbn.Model, ep.Tensor, ep.Tensor]\n) -> None:\n    fmodel, inputs, _ = fmodel_and_data\n    perturbed = inputs\n    logits = fmodel(perturbed)\n    labels = logits.argmax(axis=-1)\n\n    is_adv = fbn.Misclassification(labels)(perturbed, logits)\n    assert not is_adv.any()\n\n    _, num_classes = logits.shape\n    target_classes = (labels + 1) % num_classes\n    is_adv = fbn.TargetedMisclassification(target_classes)(perturbed, logits)\n    assert not is_adv.any()\n\n    combined = fbn.Misclassification(labels) & fbn.Misclassification(labels)\n    is_adv = combined(perturbed, logits)\n    assert not is_adv.any()\n\n\ndef test_wrong_unperturbed(\n    fmodel_and_data: Tuple[fbn.Model, ep.Tensor, ep.Tensor]\n) -> None:\n    fmodel, inputs, _ = fmodel_and_data\n    perturbed = inputs\n    logits = fmodel(perturbed)\n    _, num_classes = logits.shape\n    labels = logits.argmax(axis=-1)\n    labels = (labels + 1) % num_classes\n\n    is_adv = fbn.Misclassification(labels)(perturbed, logits)\n    assert is_adv.all()\n\n    target_classes = (labels + 1) % num_classes\n    is_adv = fbn.TargetedMisclassification(target_classes)(perturbed, logits)\n    if num_classes > 2:\n        assert not is_adv.any()\n    else:\n        assert is_adv.all()\n\n    is_adv = (fbn.Misclassification(labels) & fbn.Misclassification(labels))(\n        perturbed, logits\n    )\n    assert is_adv.all()\n\n    combined = fbn.TargetedMisclassification(labels) & fbn.TargetedMisclassification(\n        target_classes\n    )\n    is_adv = combined(perturbed, logits)\n    assert not is_adv.any()\n\n\ndef test_repr_object() -> None:\n    assert repr(object()).startswith(""<"")\n\n\ndef test_repr_misclassification(dummy: ep.Tensor) -> None:\n    labels = ep.arange(dummy, 10)\n    assert not repr(fbn.Misclassification(labels)).startswith(""<"")\n\n\ndef test_repr_and(dummy: ep.Tensor) -> None:\n    labels = ep.arange(dummy, 10)\n    assert not repr(\n        fbn.Misclassification(labels) & fbn.Misclassification(labels)\n    ).startswith(""<"")\n\n\ndef test_repr_targeted_misclassification(dummy: ep.Tensor) -> None:\n    target_classes = ep.arange(dummy, 10)\n    assert not repr(fbn.TargetedMisclassification(target_classes)).startswith(""<"")\n'"
tests/test_dataset_attack.py,0,"b'from typing import Tuple\nimport pytest\nimport eagerpy as ep\n\nimport foolbox as fbn\n\n\ndef test_dataset_attack(\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n) -> None:\n\n    (fmodel, x, y), _ = fmodel_and_data_ext_for_attacks\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n\n    attack = fbn.attacks.DatasetAttack()\n    attack.feed(fmodel, x)\n\n    assert fbn.accuracy(fmodel, x, y) > 0\n\n    advs, _, success = attack(fmodel, x, y, epsilons=None)\n    assert success.shape == (len(x),)\n    assert success.all()\n    assert fbn.accuracy(fmodel, advs, y) == 0\n\n    with pytest.raises(ValueError, match=""unknown distance""):\n        attack(fmodel, x, y, epsilons=[500.0, 1000.0])\n    attack = fbn.attacks.DatasetAttack(distance=fbn.distances.l2)\n    attack.feed(fmodel, x)\n    advss, _, success = attack(fmodel, x, y, epsilons=[500.0, 1000.0])\n    assert success.shape == (2, len(x))\n    assert success.all()\n    assert fbn.accuracy(fmodel, advss[0], y) == 0\n    assert fbn.accuracy(fmodel, advss[1], y) == 0\n\n    with pytest.raises(TypeError, match=""unexpected keyword argument""):\n        attack(fmodel, x, y, epsilons=None, invalid=True)\n'"
tests/test_devutils.py,0,"b'import pytest\nimport foolbox as fbn\nimport eagerpy as ep\n\n\n@pytest.mark.parametrize(""k"", [1, 2, 3, 4])\ndef test_atleast_kd_1d(dummy: ep.Tensor, k: int) -> None:\n    x = ep.zeros(dummy, (10,))\n    x = fbn.devutils.atleast_kd(x, k)\n    assert x.shape[0] == 10\n    assert x.ndim == k\n\n\n@pytest.mark.parametrize(""k"", [1, 2, 3, 4])\ndef test_atleast_kd_3d(dummy: ep.Tensor, k: int) -> None:\n    x = ep.zeros(dummy, (10, 5, 3))\n    x = fbn.devutils.atleast_kd(x, k)\n    assert x.shape[:3] == (10, 5, 3)\n    assert x.ndim == max(k, 3)\n\n\ndef test_flatten_2d(dummy: ep.Tensor) -> None:\n    x = ep.zeros(dummy, (4, 5))\n    x = fbn.devutils.flatten(x)\n    assert x.shape == (4, 5)\n\n\ndef test_flatten_3d(dummy: ep.Tensor) -> None:\n    x = ep.zeros(dummy, (4, 5, 6))\n    x = fbn.devutils.flatten(x)\n    assert x.shape == (4, 30)\n\n\ndef test_flatten_4d(dummy: ep.Tensor) -> None:\n    x = ep.zeros(dummy, (4, 5, 6, 7))\n    x = fbn.devutils.flatten(x)\n    assert x.shape == (4, 210)\n'"
tests/test_distances.py,0,"b'from typing import Tuple, Any, Dict, Callable, TypeVar\nimport numpy as np\nimport pytest\nimport foolbox as fbn\nimport eagerpy as ep\n\ndistances = {\n    0: fbn.distances.l0,\n    1: fbn.distances.l1,\n    2: fbn.distances.l2,\n    ep.inf: fbn.distances.linf,\n}\n\ndata: Dict[str, Callable[..., Tuple[ep.Tensor, ep.Tensor]]] = {}\n\nFuncType = Callable[..., Tuple[ep.Tensor, ep.Tensor]]\nF = TypeVar(""F"", bound=FuncType)\n\n\ndef register(f: F) -> F:\n    data[f.__name__] = f\n    return f\n\n\n@register\ndef example_4d(dummy: ep.Tensor) -> Tuple[ep.Tensor, ep.Tensor]:\n    reference = ep.full(dummy, (10, 3, 32, 32), 0.2)\n    perturbed = reference + 0.6\n    return reference, perturbed\n\n\n@register\ndef example_batch(dummy: ep.Tensor) -> Tuple[ep.Tensor, ep.Tensor]:\n    x = ep.arange(dummy, 6).float32().reshape((2, 3))\n    x = x / x.max()\n    reference = x\n    perturbed = 1 - x\n    return reference, perturbed\n\n\n@pytest.fixture(scope=""session"", params=list(data.keys()))\ndef reference_perturbed(request: Any, dummy: ep.Tensor) -> Tuple[ep.Tensor, ep.Tensor]:\n    return data[request.param](dummy)\n\n\n@pytest.mark.parametrize(""p"", [0, 1, 2, ep.inf])\ndef test_distance(reference_perturbed: Tuple[ep.Tensor, ep.Tensor], p: float) -> None:\n    reference, perturbed = reference_perturbed\n\n    actual = distances[p](reference, perturbed).numpy()\n\n    diff = perturbed.numpy() - reference.numpy()\n    diff = diff.reshape((len(diff), -1))\n    desired = np.linalg.norm(diff, ord=p, axis=-1)\n\n    np.testing.assert_allclose(actual, desired, rtol=1e-5)\n\n\n@pytest.mark.parametrize(""p"", [0, 1, 2, ep.inf])\ndef test_distance_repr_str(p: float) -> None:\n    assert str(p) in repr(distances[p])\n    assert str(p) in str(distances[p])\n\n\n@pytest.mark.parametrize(""p"", [0, 1, 2, ep.inf])\ndef test_distance_clip(\n    reference_perturbed: Tuple[ep.Tensor, ep.Tensor], p: float\n) -> None:\n    reference, perturbed = reference_perturbed\n\n    ds = distances[p](reference, perturbed).numpy()\n    epsilon = np.median(ds)\n    too_large = ds > epsilon\n\n    desired = np.where(too_large, epsilon, ds)\n\n    perturbed = distances[p].clip_perturbation(reference, perturbed, epsilon)\n    actual = distances[p](reference, perturbed).numpy()\n\n    np.testing.assert_allclose(actual, desired)\n'"
tests/test_evaluate.py,0,"b'from typing import Tuple\nimport pytest\nimport foolbox as fbn\nimport eagerpy as ep\n\n\ndef test_evaluate(fmodel_and_data: Tuple[fbn.Model, ep.Tensor, ep.Tensor]) -> None:\n    pytest.skip()\n    assert False\n    fmodel, x, y = fmodel_and_data  # type: ignore\n\n    attacks = [\n        # L2BasicIterativeAttack,\n        # L2CarliniWagnerAttack,\n        # L2ContrastReductionAttack,\n        # BinarySearchContrastReductionAttack,\n        # LinearSearchContrastReductionAttack,\n    ]\n    epsilons = [0.0, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0]\n\n    acc = fbn.accuracy(fmodel, x, y)\n    assert acc > 0\n\n    _, robust_accuracy = fbn.evaluate_l2(\n        fmodel, x, y, attacks=attacks, epsilons=epsilons\n    )\n    assert robust_accuracy[0] == acc\n    assert robust_accuracy[-1] == 0.0\n'"
tests/test_fetch_weights.py,0,"b'from foolbox.zoo import fetch_weights\nfrom foolbox.zoo.common import home_directory_path, sha256_hash\nfrom foolbox.zoo.weights_fetcher import FOLDER\n\nimport os\nimport pytest\nimport shutil\n\nimport responses\nimport io\nimport zipfile\n\n\n@responses.activate\ndef test_fetch_weights_unzipped() -> None:\n    weights_uri = ""http://localhost:8080/weights.zip""\n    raw_body = _random_body(zipped=False)\n\n    # mock server\n    responses.add(responses.GET, weights_uri, body=raw_body, status=200, stream=True)\n\n    expected_path = _expected_path(weights_uri)\n\n    if os.path.exists(expected_path):\n        shutil.rmtree(expected_path)  # make sure path does not exist already\n\n    file_path = fetch_weights(weights_uri)\n\n    exists_locally = os.path.exists(expected_path)\n    assert exists_locally\n    assert expected_path in file_path\n\n\n@responses.activate\ndef test_fetch_weights_zipped() -> None:\n    weights_uri = ""http://localhost:8080/weights.zip""\n\n    # mock server\n    raw_body = _random_body(zipped=True)\n    responses.add(\n        responses.GET,\n        weights_uri,\n        body=raw_body,\n        status=200,\n        stream=True,\n        content_type=""application/zip"",\n        headers={""Accept-Encoding"": ""gzip, deflate""},\n    )\n\n    expected_path = _expected_path(weights_uri)\n\n    if os.path.exists(expected_path):\n        shutil.rmtree(expected_path)  # make sure path does not exist already\n\n    file_path = fetch_weights(weights_uri, unzip=True)\n\n    exists_locally = os.path.exists(expected_path)\n    assert exists_locally\n    assert expected_path in file_path\n\n\n@responses.activate\ndef test_fetch_weights_returns_404() -> None:\n    weights_uri = ""http://down:8080/weights.zip""\n\n    # mock server\n    responses.add(responses.GET, weights_uri, status=404)\n\n    expected_path = _expected_path(weights_uri)\n\n    if os.path.exists(expected_path):\n        shutil.rmtree(expected_path)  # make sure path does not exist already\n\n    with pytest.raises(RuntimeError):\n        fetch_weights(weights_uri, unzip=False)\n\n\ndef _random_body(zipped: bool = False) -> bytes:\n    if zipped:\n        data = io.BytesIO()\n        with zipfile.ZipFile(data, mode=""w"") as z:\n            z.writestr(""test.txt"", ""no real weights in here :)"")\n        data.seek(0)\n        return data.getvalue()\n    else:\n        raw_body = os.urandom(1024)\n        return raw_body\n\n\ndef _expected_path(weights_uri: str) -> str:\n    hash_digest = sha256_hash(weights_uri)\n    local_path = home_directory_path(FOLDER, hash_digest)\n    return local_path\n'"
tests/test_gen_attack_utils.py,0,"b'import eagerpy as ep\nimport numpy as np\nimport pytest\nfrom typing import Any\n\nfrom foolbox.attacks.gen_attack_utils import rescale_images\n\n\ndef test_rescale_axis(request: Any, dummy: ep.Tensor) -> None:\n    backend = request.config.option.backend\n    if backend == ""numpy"":\n        pytest.skip()\n\n    x_np = np.random.uniform(0.0, 1.0, size=(16, 3, 64, 64))\n    x_np_ep = ep.astensor(x_np)\n    x_up_np_ep = rescale_images(x_np_ep, (16, 3, 128, 128), 1)\n    x_up_np = x_up_np_ep.numpy()\n\n    x = ep.from_numpy(dummy, x_np)\n    x_ep = ep.astensor(x)\n    x_up_ep = rescale_images(x_ep, (16, 3, 128, 128), 1)\n    x_up = x_up_ep.numpy()\n\n    assert np.allclose(x_up_np, x_up)\n\n\ndef test_rescale_axis_nhwc(request: Any, dummy: ep.Tensor) -> None:\n    backend = request.config.option.backend\n    if backend == ""numpy"":\n        pytest.skip()\n\n    x_np = np.random.uniform(0.0, 1.0, size=(16, 64, 64, 3))\n    x_np_ep = ep.astensor(x_np)\n    x_up_np_ep = rescale_images(x_np_ep, (16, 128, 128, 3), -1)\n    x_up_np = x_up_np_ep.numpy()\n\n    x = ep.from_numpy(dummy, x_np)\n    x_ep = ep.astensor(x)\n    x_up_ep = rescale_images(x_ep, (16, 128, 128, 3), -1)\n    x_up = x_up_ep.numpy()\n\n    assert np.allclose(x_up_np, x_up)\n'"
tests/test_models.py,0,"b'from typing import Tuple, Any\nimport pytest\nimport eagerpy as ep\nimport numpy as np\nimport copy\n\nimport foolbox as fbn\n\nModelAndData = Tuple[fbn.Model, ep.Tensor, ep.Tensor]\n\n\ndef test_bounds(fmodel_and_data: ModelAndData) -> None:\n    fmodel, x, y = fmodel_and_data\n    min_, max_ = fmodel.bounds\n    assert min_ < max_\n    assert (x >= min_).all()\n    assert (x <= max_).all()\n\n\ndef test_forward_unwrapped(fmodel_and_data: ModelAndData) -> None:\n    fmodel, x, y = fmodel_and_data\n    logits = ep.astensor(fmodel(x.raw))\n    assert logits.ndim == 2\n    assert len(logits) == len(x) == len(y)\n    _, num_classes = logits.shape\n    assert (y >= 0).all()\n    assert (y < num_classes).all()\n    if hasattr(logits.raw, ""requires_grad""):\n        assert not logits.raw.requires_grad\n\n\ndef test_forward_wrapped(fmodel_and_data: ModelAndData) -> None:\n    fmodel, x, y = fmodel_and_data\n    assert ep.istensor(x)\n    logits = fmodel(x)\n    assert ep.istensor(logits)\n    assert logits.ndim == 2\n    assert len(logits) == len(x) == len(y)\n    _, num_classes = logits.shape\n    assert (y >= 0).all()\n    assert (y < num_classes).all()\n    if hasattr(logits.raw, ""requires_grad""):\n        assert not logits.raw.requires_grad\n\n\ndef test_pytorch_training_warning(request: Any) -> None:\n    backend = request.config.option.backend\n    if backend != ""pytorch"":\n        pytest.skip()\n\n    import torch\n\n    class Model(torch.nn.Module):\n        def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n            return x\n\n    model = Model().train()\n    bounds = (0, 1)\n    with pytest.warns(UserWarning):\n        fbn.PyTorchModel(model, bounds=bounds, device=""cpu"")\n\n\ndef test_pytorch_invalid_model(request: Any) -> None:\n    backend = request.config.option.backend\n    if backend != ""pytorch"":\n        pytest.skip()\n\n    class Model:\n        def forward(self, x: Any) -> Any:\n            return x\n\n    model = Model()\n    bounds = (0, 1)\n    with pytest.raises(ValueError, match=""torch.nn.Module""):\n        fbn.PyTorchModel(model, bounds=bounds)\n\n\n@pytest.mark.parametrize(""bounds"", [(0, 1), (-1.0, 1.0), (0, 255), (-32768, 32767)])\ndef test_transform_bounds(\n    fmodel_and_data: ModelAndData, bounds: fbn.types.BoundsInput\n) -> None:\n    fmodel1, x, y = fmodel_and_data\n    logits1 = fmodel1(x)\n    min1, max1 = fmodel1.bounds\n\n    fmodel2 = fmodel1.transform_bounds(bounds)\n    min2, max2 = fmodel2.bounds\n    x2 = (x - min1) / (max1 - min1) * (max2 - min2) + min2\n    logits2 = fmodel2(x2)\n\n    np.testing.assert_allclose(logits1.numpy(), logits2.numpy(), rtol=1e-4, atol=1e-4)\n\n    # to make sure fmodel1 is not changed in-place\n    logits1b = fmodel1(x)\n    np.testing.assert_allclose(logits1.numpy(), logits1b.numpy(), rtol=2e-6)\n\n    fmodel1c = fmodel2.transform_bounds(fmodel1.bounds)\n    logits1c = fmodel1c(x)\n    np.testing.assert_allclose(logits1.numpy(), logits1c.numpy(), rtol=1e-4, atol=1e-4)\n\n\n@pytest.mark.parametrize(""bounds"", [(0, 1), (-1.0, 1.0), (0, 255), (-32768, 32767)])\ndef test_transform_bounds_inplace(\n    fmodel_and_data: ModelAndData, bounds: fbn.types.BoundsInput\n) -> None:\n    fmodel, x, y = fmodel_and_data\n    fmodel = copy.copy(fmodel)  # to avoid interference with other tests\n\n    if not isinstance(fmodel, fbn.models.base.ModelWithPreprocessing):\n        pytest.skip()\n        assert False\n    logits1 = fmodel(x)\n    min1, max1 = fmodel.bounds\n\n    fmodel.transform_bounds(bounds, inplace=True)\n    min2, max2 = fmodel.bounds\n    x2 = (x - min1) / (max1 - min1) * (max2 - min2) + min2\n    logits2 = fmodel(x2)\n\n    np.testing.assert_allclose(logits1.numpy(), logits2.numpy(), rtol=1e-4, atol=1e-4)\n\n\n@pytest.mark.parametrize(""bounds"", [(0, 1), (-1.0, 1.0), (0, 255), (-32768, 32767)])\n@pytest.mark.parametrize(""manual"", [True, False])\ndef test_transform_bounds_wrapper(\n    fmodel_and_data: ModelAndData, bounds: fbn.types.BoundsInput, manual: bool\n) -> None:\n    fmodel1, x, y = fmodel_and_data\n    fmodel1 = copy.copy(fmodel1)  # to avoid interference with other tests\n\n    logits1 = fmodel1(x)\n    min1, max1 = fmodel1.bounds\n\n    fmodel2: fbn.Model\n    if manual:\n        fmodel2 = fbn.models.TransformBoundsWrapper(fmodel1, bounds)\n    else:\n        if not isinstance(fmodel1, fbn.models.base.ModelWithPreprocessing):\n            pytest.skip()\n            assert False\n        fmodel2 = fmodel1.transform_bounds(bounds, wrapper=True)\n        with pytest.raises(ValueError, match=""cannot both be True""):\n            fmodel1.transform_bounds(bounds, inplace=True, wrapper=True)\n    assert isinstance(fmodel2, fbn.models.TransformBoundsWrapper)\n    min2, max2 = fmodel2.bounds\n    x2 = (x - min1) / (max1 - min1) * (max2 - min2) + min2\n    logits2 = fmodel2(x2)\n\n    np.testing.assert_allclose(logits1.numpy(), logits2.numpy(), rtol=1e-4, atol=1e-4)\n\n    # to make sure fmodel1 is not changed in-place\n    logits1b = fmodel1(x)\n    np.testing.assert_allclose(logits1.numpy(), logits1b.numpy(), rtol=2e-6)\n\n    fmodel1c = fmodel2.transform_bounds(fmodel1.bounds)\n    logits1c = fmodel1c(x)\n    np.testing.assert_allclose(logits1.numpy(), logits1c.numpy(), rtol=1e-4, atol=1e-4)\n\n    # to make sure fmodel2 is not changed in-place\n    logits2b = fmodel2(x2)\n    np.testing.assert_allclose(logits2.numpy(), logits2b.numpy(), rtol=2e-6)\n\n    fmodel2.transform_bounds(fmodel1.bounds, inplace=True)\n    logits1d = fmodel2(x)\n    np.testing.assert_allclose(logits1d.numpy(), logits1.numpy(), rtol=2e-6)\n\n\ndef test_preprocessing(fmodel_and_data: ModelAndData) -> None:\n    fmodel, x, y = fmodel_and_data\n    if not isinstance(fmodel, fbn.models.base.ModelWithPreprocessing):\n        pytest.skip()\n        assert False\n\n    preprocessing = dict(mean=[3, 3, 3], std=[5, 5, 5], axis=-3)\n    fmodel = fbn.models.base.ModelWithPreprocessing(\n        fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n    )\n\n    preprocessing = dict(mean=[3, 3, 3], axis=-3)\n    fmodel = fbn.models.base.ModelWithPreprocessing(\n        fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n    )\n\n    preprocessing = dict(mean=np.array([3, 3, 3]), axis=-3)\n    fmodel = fbn.models.base.ModelWithPreprocessing(\n        fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n    )\n\n    # std -> foo\n    preprocessing = dict(mean=[3, 3, 3], foo=[5, 5, 5], axis=-3)\n    with pytest.raises(ValueError):\n        fmodel = fbn.models.base.ModelWithPreprocessing(\n            fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n        )\n\n    # axis positive\n    preprocessing = dict(mean=[3, 3, 3], std=[5, 5, 5], axis=1)\n    with pytest.raises(ValueError):\n        fmodel = fbn.models.base.ModelWithPreprocessing(\n            fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n        )\n\n    preprocessing = dict(mean=3, std=5)\n    fmodel = fbn.models.base.ModelWithPreprocessing(\n        fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n    )\n\n    # axis with 1D mean\n    preprocessing = dict(mean=3, std=[5, 5, 5], axis=-3)\n    with pytest.raises(ValueError):\n        fmodel = fbn.models.base.ModelWithPreprocessing(\n            fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n        )\n\n    # axis with 1D std\n    preprocessing = dict(mean=[3, 3, 3], std=5, axis=-3)\n    with pytest.raises(ValueError):\n        fmodel = fbn.models.base.ModelWithPreprocessing(\n            fmodel._model, fmodel.bounds, fmodel.dummy, preprocessing\n        )\n'"
tests/test_plot.py,0,"b'import pytest\nimport eagerpy as ep\nimport foolbox as fbn\n\n\ndef test_plot(dummy: ep.Tensor) -> None:\n    # just tests that the calls don\'t throw any errors\n    images = ep.zeros(dummy, (10, 3, 32, 32))\n    fbn.plot.images(images)\n    fbn.plot.images(images, n=3)\n    fbn.plot.images(images, n=3, data_format=""channels_first"")\n    fbn.plot.images(images, nrows=4)\n    fbn.plot.images(images, ncols=3)\n    fbn.plot.images(images, nrows=2, ncols=6)\n    fbn.plot.images(images, nrows=2, ncols=4)\n    with pytest.raises(ValueError):\n        images = ep.zeros(dummy, (10, 3, 3, 3))\n        fbn.plot.images(images)\n    with pytest.raises(ValueError):\n        images = ep.zeros(dummy, (10, 1, 1, 1))\n        fbn.plot.images(images)\n    with pytest.raises(ValueError):\n        images = ep.zeros(dummy, (10, 32, 32))\n        fbn.plot.images(images)\n    with pytest.raises(ValueError):\n        images = ep.zeros(dummy, (10, 3, 32, 32))\n        fbn.plot.images(images, data_format=""foo"")\n'"
tests/test_spatial_attack.py,0,"b'from typing import List, Tuple\nimport pytest\nimport eagerpy as ep\nimport foolbox as fbn\nimport foolbox.attacks as fa\n\n\ndef get_attack_id(x: fbn.Attack) -> str:\n    return repr(x)\n\n\n# attack\nattacks: List[Tuple[fbn.Attack, bool]] = [\n    (fa.SpatialAttack(), False),\n    (fa.SpatialAttack(grid_search=False), False),\n    (fa.SpatialAttack(grid_search=False), True),\n]\n\n\n@pytest.mark.parametrize(""attack_grad_real"", attacks, ids=get_attack_id)\ndef test_spatial_attacks(\n    fmodel_and_data_ext_for_attacks: Tuple[\n        Tuple[fbn.Model, ep.Tensor, ep.Tensor], bool\n    ],\n    attack_grad_real: Tuple[fbn.Attack, bool],\n) -> None:\n\n    attack, repeated = attack_grad_real\n    if repeated:\n        attack = attack.repeat(2)\n    (fmodel, x, y), real = fmodel_and_data_ext_for_attacks\n    if not real:\n        pytest.skip()\n\n    x = (x - fmodel.bounds.lower) / (fmodel.bounds.upper - fmodel.bounds.lower)\n    fmodel = fmodel.transform_bounds((0, 1))\n    acc = fbn.accuracy(fmodel, x, y)\n    assert acc > 0\n    advs, _, _ = attack(fmodel, x, y)  # type: ignore\n    assert fbn.accuracy(fmodel, advs, y) < acc\n'"
tests/test_tensorboard.py,0,"b'from typing import Union, Any\nfrom typing_extensions import Literal\nimport pytest\nimport eagerpy as ep\nimport foolbox as fbn\n\n\n@pytest.mark.parametrize(""logdir"", [False, ""temp""])\ndef test_tensorboard(\n    logdir: Union[Literal[False], None, str], tmp_path: Any, dummy: ep.Tensor\n) -> None:\n    if logdir == ""temp"":\n        logdir = tmp_path\n\n    if logdir:\n        before = len(list(tmp_path.iterdir()))\n\n    tb = fbn.tensorboard.TensorBoard(logdir)\n\n    tb.scalar(""a_scalar"", 5, step=1)\n\n    x = ep.ones(dummy, 10)\n    tb.mean(""a_mean"", x, step=2)\n\n    x = ep.ones(dummy, 10) == ep.arange(dummy, 10)\n    tb.probability(""a_probability"", x, step=2)\n\n    x = ep.arange(dummy, 10).float32()\n    cond = ep.ones(dummy, 10) == (ep.arange(dummy, 10) % 2)\n    tb.conditional_mean(""a_conditional_mean"", x, cond, step=2)\n\n    x = ep.arange(dummy, 10).float32()\n    cond = ep.ones(dummy, 10) == ep.zeros(dummy, 10)\n    tb.conditional_mean(""a_conditional_mean_false"", x, cond, step=2)\n\n    x = ep.ones(dummy, 10) == ep.arange(dummy, 10)\n    y = ep.ones(dummy, 10) == (ep.arange(dummy, 10) % 2)\n    tb.probability_ratio(""a_probability_ratio"", x, y, step=5)\n\n    x = ep.ones(dummy, 10) == (ep.arange(dummy, 10) % 2)\n    y = ep.ones(dummy, 10) == ep.zeros(dummy, 10)\n    tb.probability_ratio(""a_probability_ratio_y_zero"", x, y, step=5)\n\n    x = ep.arange(dummy, 10).float32()\n    tb.histogram(""a_histogram"", x, step=9, first=False)\n    tb.histogram(""a_histogram"", x, step=10, first=True)\n\n    tb.close()\n\n    if logdir:\n        after = len(list(tmp_path.iterdir()))\n        assert after > before  # make sure something has been written\n'"
tests/test_utils.py,0,"b'from typing import Tuple\nimport foolbox as fbn\nimport eagerpy as ep\nimport pytest\n\nModelAndData = Tuple[fbn.Model, ep.Tensor, ep.Tensor]\n\n\ndef test_accuracy(fmodel_and_data: ModelAndData) -> None:\n    fmodel, x, y = fmodel_and_data\n    accuracy = fbn.accuracy(fmodel, x, y)\n    assert 0 <= accuracy <= 1\n    assert accuracy > 0.5\n    y = fmodel(x).argmax(axis=-1)\n    accuracy = fbn.accuracy(fmodel, x, y)\n    assert accuracy == 1\n\n\n@pytest.mark.parametrize(""batchsize"", [1, 8])\n@pytest.mark.parametrize(\n    ""dataset"", [""imagenet"", ""cifar10"", ""cifar100"", ""mnist"", ""fashionMNIST""]\n)\ndef test_samples(fmodel_and_data: ModelAndData, batchsize: int, dataset: str) -> None:\n    fmodel, _, _ = fmodel_and_data\n    if hasattr(fmodel, ""data_format""):\n        data_format = fmodel.data_format  # type: ignore\n        x, y = fbn.samples(fmodel, dataset=dataset, batchsize=batchsize)\n        assert len(x) == len(y) == batchsize\n        assert not ep.istensor(x)\n        assert not ep.istensor(y)\n        x, y = fbn.samples(fmodel, batchsize=batchsize, data_format=data_format)\n        assert len(x) == len(y) == batchsize\n        assert not ep.istensor(x)\n        assert not ep.istensor(y)\n        with pytest.raises(ValueError):\n            data_format = {\n                ""channels_first"": ""channels_last"",\n                ""channels_last"": ""channels_first"",\n            }[data_format]\n            fbn.samples(fmodel, batchsize=batchsize, data_format=data_format)\n    else:\n        x, y = fbn.samples(fmodel, batchsize=batchsize, data_format=""channels_first"")\n        assert len(x) == len(y) == batchsize\n        assert not ep.istensor(x)\n        assert not ep.istensor(y)\n        with pytest.raises(ValueError):\n            fbn.samples(fmodel, batchsize=batchsize)\n\n\n@pytest.mark.parametrize(""batchsize"", [42])\n@pytest.mark.parametrize(""dataset"", [""imagenet""])\ndef test_samples_large_batch(\n    fmodel_and_data: ModelAndData, batchsize: int, dataset: str\n) -> None:\n    fmodel, _, _ = fmodel_and_data\n    data_format = getattr(fmodel, ""data_format"", ""channels_first"")\n    with pytest.warns(UserWarning, match=""only 20 samples""):\n        x, y = fbn.samples(\n            fmodel, dataset=dataset, batchsize=batchsize, data_format=data_format\n        )\n    assert len(x) == len(y) == batchsize\n    assert not ep.istensor(x)\n    assert not ep.istensor(y)\n'"
tests/test_zoo.py,0,"b'from typing import Any\nimport sys\nimport pytest\n\nimport foolbox as fbn\nfrom foolbox.zoo.model_loader import ModelLoader\n\n\n@pytest.fixture(autouse=True)\ndef unload_foolbox_model_module() -> None:\n    # reload foolbox_model from scratch for every run\n    # to ensure atomic tests without side effects\n    module_names = [""foolbox_model"", ""model""]\n    for module_name in module_names:\n        if module_name in sys.modules:\n            del sys.modules[module_name]\n\n\n# test_data = [\n#     # private repo won\'t work on travis\n#     (""https://github.com/bethgelab/AnalysisBySynthesis.git"", (1, 28, 28)),\n#     (""https://github.com/bethgelab/convex_adversarial.git"", (1, 28, 28)),\n#     (""https://github.com/bethgelab/mnist_challenge.git"", 784),\n#     (join(""file://"", dirname(__file__), ""data/model_repo""), (3, 224, 224)),\n# ]\n\nurls = [\n    ""https://github.com/jonasrauber/foolbox-tensorflow-keras-applications"",\n    ""git@github.com:jonasrauber/foolbox-tensorflow-keras-applications.git"",\n]\n\n\n# @pytest.mark.parametrize(""url, dim"", test_data)\n@pytest.mark.parametrize(""url"", urls)\ndef test_loading_model(request: Any, url: str) -> None:\n    backend = request.config.option.backend\n    if backend != ""tensorflow"":\n        pytest.skip()\n\n    # download model\n    try:\n        fmodel = fbn.zoo.get_model(url, name=""MobileNetV2"", overwrite=True)\n    except fbn.zoo.GitCloneError:\n        pytest.skip()\n\n    # download again (test overwriting)\n    try:\n        fmodel = fbn.zoo.get_model(url, name=""MobileNetV2"", overwrite=True)\n    except fbn.zoo.GitCloneError:\n        pytest.skip()\n\n    # create a dummy image\n    # x = np.zeros(dim, dtype=np.float32)\n    # x[:] = np.random.randn(*x.shape)\n    x, y = fbn.samples(fmodel, dataset=""imagenet"", batchsize=16)\n\n    # run the model\n    # logits = model(x)\n    # probabilities = ep.softmax(logits)\n    # predicted_class = np.argmax(logits)\n    assert fbn.accuracy(fmodel, x, y) > 0.9\n\n    # sanity check\n    # assert predicted_class >= 0\n    # assert np.sum(probabilities) >= 0.9999\n\n    # TODO: delete fmodel\n\n\n# @pytest.mark.parametrize(""url, dim"", test_data)\ndef test_loading_invalid_model(request: Any) -> None:\n    url = ""https://github.com/jonasrauber/invalid-url""\n    with pytest.raises(fbn.zoo.GitCloneError):\n        fbn.zoo.get_model(url, name=""MobileNetV2"", overwrite=True)\n\n\ndef test_non_default_module_throws_error() -> None:\n    with pytest.raises(ValueError):\n        ModelLoader.get(key=""other"")\n'"
foolbox/attacks/__init__.py,0,"b'from .base import Attack  # noqa: F401\n\n# FixedEpsilonAttack subclasses\nfrom .contrast import L2ContrastReductionAttack  # noqa: F401\nfrom .virtual_adversarial_attack import VirtualAdversarialAttack  # noqa: F401\nfrom .ddn import DDNAttack  # noqa: F401\nfrom .projected_gradient_descent import (  # noqa: F401\n    L1ProjectedGradientDescentAttack,\n    L2ProjectedGradientDescentAttack,\n    LinfProjectedGradientDescentAttack,\n)\nfrom .basic_iterative_method import (  # noqa: F401\n    L1BasicIterativeAttack,\n    L2BasicIterativeAttack,\n    LinfBasicIterativeAttack,\n)\nfrom .fast_gradient_method import (  # noqa: F401\n    L1FastGradientAttack,\n    L2FastGradientAttack,\n    LinfFastGradientAttack,\n)\nfrom .additive_noise import (  # noqa: F401\n    L2AdditiveGaussianNoiseAttack,\n    L2AdditiveUniformNoiseAttack,\n    LinfAdditiveUniformNoiseAttack,\n    L2RepeatedAdditiveGaussianNoiseAttack,\n    L2RepeatedAdditiveUniformNoiseAttack,\n    LinfRepeatedAdditiveUniformNoiseAttack,\n)\nfrom .sparse_l1_descent_attack import SparseL1DescentAttack  # noqa: F401\n\n# MinimizatonAttack subclasses\nfrom .inversion import InversionAttack  # noqa: F401\nfrom .contrast_min import (  # noqa: F401\n    BinarySearchContrastReductionAttack,\n    LinearSearchContrastReductionAttack,\n)\nfrom .carlini_wagner import L2CarliniWagnerAttack  # noqa: F401\nfrom .newtonfool import NewtonFoolAttack  # noqa: F401\nfrom .ead import EADAttack  # noqa: F401\nfrom .blur import GaussianBlurAttack  # noqa: F401\nfrom .spatial_attack import SpatialAttack  # noqa: F401\nfrom .deepfool import L2DeepFoolAttack, LinfDeepFoolAttack  # noqa: F401\nfrom .saltandpepper import SaltAndPepperNoiseAttack  # noqa: F401\nfrom .blended_noise import LinearSearchBlendedUniformNoiseAttack  # noqa: F401\nfrom .binarization import BinarizationRefinementAttack  # noqa: F401\nfrom .dataset_attack import DatasetAttack  # noqa: F401\nfrom .boundary_attack import BoundaryAttack  # noqa: F401\nfrom .brendel_bethge import (  # noqa: F401\n    L0BrendelBethgeAttack,\n    L1BrendelBethgeAttack,\n    L2BrendelBethgeAttack,\n    LinfinityBrendelBethgeAttack,\n)\nfrom .gen_attack import GenAttack  # noqa: F401\n\n# from .blended_noise import LinearSearchBlendedUniformNoiseAttack  # noqa: F401\n# from .brendel_bethge import (  # noqa: F401\n#     L0BrendelBethgeAttack,\n#     L1BrendelBethgeAttack,\n#     L2BrendelBethgeAttack,\n#     LinfinityBrendelBethgeAttack,\n# )\n# from .additive_noise import L2AdditiveGaussianNoiseAttack  # noqa: F401\n# from .additive_noise import L2AdditiveUniformNoiseAttack  # noqa: F401\n# from .additive_noise import LinfAdditiveUniformNoiseAttack  # noqa: F401\n# from .additive_noise import L2RepeatedAdditiveGaussianNoiseAttack  # noqa: F401\n# from .additive_noise import L2RepeatedAdditiveUniformNoiseAttack  # noqa: F401\n# from .additive_noise import LinfRepeatedAdditiveUniformNoiseAttack  # noqa: F401\n# from .saltandpepper import SaltAndPepperNoiseAttack  # noqa: F401\n\nFGM = L2FastGradientAttack\nFGSM = LinfFastGradientAttack\nL1PGD = L1ProjectedGradientDescentAttack\nL2PGD = L2ProjectedGradientDescentAttack\nLinfPGD = LinfProjectedGradientDescentAttack\nPGD = LinfPGD\n'"
foolbox/attacks/additive_noise.py,0,"b'from typing import Union, Any\nfrom abc import ABC\nfrom abc import abstractmethod\nimport eagerpy as ep\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..distances import l2, linf\n\nfrom .base import FixedEpsilonAttack\nfrom .base import Criterion\nfrom .base import Model\nfrom .base import T\nfrom .base import get_criterion\nfrom .base import get_is_adversarial\nfrom .base import raise_if_kwargs\n\n\nclass BaseAdditiveNoiseAttack(FixedEpsilonAttack, ABC):\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, Any] = None,\n        *,\n        epsilon: float,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, criterion, kwargs\n\n        min_, max_ = model.bounds\n        p = self.sample_noise(x)\n        norms = self.get_norms(p)\n        p = p / atleast_kd(norms, p.ndim)\n        x = x + epsilon * p\n        x = x.clip(min_, max_)\n\n        return restore_type(x)\n\n    @abstractmethod\n    def sample_noise(self, x: ep.Tensor) -> ep.Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_norms(self, p: ep.Tensor) -> ep.Tensor:\n        raise NotImplementedError\n\n\nclass L2Mixin:\n    distance = l2\n\n    def get_norms(self, p: ep.Tensor) -> ep.Tensor:\n        return flatten(p).norms.l2(axis=-1)\n\n\nclass LinfMixin:\n    distance = linf\n\n    def get_norms(self, p: ep.Tensor) -> ep.Tensor:\n        return flatten(p).max(axis=-1)\n\n\nclass GaussianMixin:\n    def sample_noise(self, x: ep.Tensor) -> ep.Tensor:\n        return x.normal(x.shape)\n\n\nclass UniformMixin:\n    def sample_noise(self, x: ep.Tensor) -> ep.Tensor:\n        return x.uniform(x.shape, -1, 1)\n\n\nclass L2AdditiveGaussianNoiseAttack(L2Mixin, GaussianMixin, BaseAdditiveNoiseAttack):\n    """"""Samples Gaussian noise with a fixed L2 size""""""\n\n    pass\n\n\nclass L2AdditiveUniformNoiseAttack(L2Mixin, UniformMixin, BaseAdditiveNoiseAttack):\n    """"""Samples uniform noise with a fixed L2 size""""""\n\n    pass\n\n\nclass LinfAdditiveUniformNoiseAttack(LinfMixin, UniformMixin, BaseAdditiveNoiseAttack):\n    """"""Samples uniform noise with a fixed L-infinity size""""""\n\n    pass\n\n\nclass BaseRepeatedAdditiveNoiseAttack(FixedEpsilonAttack, ABC):\n    def __init__(self, *, repeats: int = 100, check_trivial: bool = True):\n        self.repeats = repeats\n        self.check_trivial = check_trivial\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, Any] = None,\n        *,\n        epsilon: float,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x0, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        is_adversarial = get_is_adversarial(criterion_, model)\n\n        min_, max_ = model.bounds\n\n        result = x0\n        if self.check_trivial:\n            found = is_adversarial(result)\n        else:\n            found = ep.zeros(x0, len(result)).bool()\n\n        for _ in range(self.repeats):\n            if found.all():\n                break\n\n            p = self.sample_noise(x0)\n            norms = self.get_norms(p)\n            p = p / atleast_kd(norms, p.ndim)\n            x = x0 + epsilon * p\n            x = x.clip(min_, max_)\n            is_adv = is_adversarial(x)\n            is_new_adv = ep.logical_and(is_adv, ep.logical_not(found))\n            result = ep.where(atleast_kd(is_new_adv, x.ndim), x, result)\n            found = ep.logical_or(found, is_adv)\n\n        return restore_type(result)\n\n    @abstractmethod\n    def sample_noise(self, x: ep.Tensor) -> ep.Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_norms(self, p: ep.Tensor) -> ep.Tensor:\n        raise NotImplementedError\n\n\nclass L2RepeatedAdditiveGaussianNoiseAttack(\n    L2Mixin, GaussianMixin, BaseRepeatedAdditiveNoiseAttack\n):\n    """"""Repeatedly samples Gaussian noise with a fixed L2 size\n\n    Args:\n        repeats : How often to sample random noise.\n        check_trivial : Check whether original sample is already adversarial.\n    """"""\n\n    pass\n\n\nclass L2RepeatedAdditiveUniformNoiseAttack(\n    L2Mixin, UniformMixin, BaseRepeatedAdditiveNoiseAttack\n):\n    """"""Repeatedly samples uniform noise with a fixed L2 size\n\n    Args:\n        repeats : How often to sample random noise.\n        check_trivial : Check whether original sample is already adversarial.\n    """"""\n\n    pass\n\n\nclass LinfRepeatedAdditiveUniformNoiseAttack(\n    LinfMixin, UniformMixin, BaseRepeatedAdditiveNoiseAttack\n):\n    """"""Repeatedly samples uniform noise with a fixed L-infinity size\n\n    Args:\n        repeats : How often to sample random noise.\n        check_trivial : Check whether original sample is already adversarial.\n    """"""\n\n    pass\n'"
foolbox/attacks/base.py,0,"b'from typing import Callable, TypeVar, Any, Union, Optional, Sequence, List, Tuple, Dict\nfrom typing_extensions import final, overload\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable\nimport eagerpy as ep\n\nfrom ..models import Model\n\nfrom ..criteria import Criterion\nfrom ..criteria import Misclassification\n\nfrom ..devutils import atleast_kd\n\nfrom ..distances import Distance\n\n\nT = TypeVar(""T"")\nCriterionType = TypeVar(""CriterionType"", bound=Criterion)\n\n\n# TODO: support manually specifying early_stop in __call__\n\n\nclass Attack(ABC):\n    @overload\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Sequence[Union[float, None]],\n        **kwargs: Any,\n    ) -> Tuple[List[T], List[T], T]:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[float, None],\n        **kwargs: Any,\n    ) -> Tuple[T, T, T]:\n        ...\n\n    @abstractmethod  # noqa: F811\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[Sequence[Union[float, None]], float, None],\n        **kwargs: Any,\n    ) -> Union[Tuple[List[T], List[T], T], Tuple[T, T, T]]:\n        # in principle, the type of criterion is Union[Criterion, T]\n        # but we want to give subclasses the option to specify the supported\n        # criteria explicitly (i.e. specifying a stricter type constraint)\n        ...\n\n    @abstractmethod\n    def repeat(self, times: int) -> ""Attack"":\n        ...\n\n    def __repr__(self) -> str:\n        args = "", "".join(f""{k.strip(\'_\')}={v}"" for k, v in vars(self).items())\n        return f""{self.__class__.__name__}({args})""\n\n\nclass AttackWithDistance(Attack):\n    @property\n    @abstractmethod\n    def distance(self) -> Distance:\n        ...\n\n    def repeat(self, times: int) -> Attack:\n        return Repeated(self, times)\n\n\nclass Repeated(AttackWithDistance):\n    """"""Repeats the wrapped attack and returns the best result""""""\n\n    def __init__(self, attack: AttackWithDistance, times: int):\n        if times < 1:\n            raise ValueError(f""expected times >= 1, got {times}"")  # pragma: no cover\n\n        self.attack = attack\n        self.times = times\n\n    @property\n    def distance(self) -> Distance:\n        return self.attack.distance\n\n    @overload\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Sequence[Union[float, None]],\n        **kwargs: Any,\n    ) -> Tuple[List[T], List[T], T]:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[float, None],\n        **kwargs: Any,\n    ) -> Tuple[T, T, T]:\n        ...\n\n    def __call__(  # noqa: F811\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[Sequence[Union[float, None]], float, None],\n        **kwargs: Any,\n    ) -> Union[Tuple[List[T], List[T], T], Tuple[T, T, T]]:\n        x, restore_type = ep.astensor_(inputs)\n        del inputs\n\n        criterion = get_criterion(criterion)\n\n        was_iterable = True\n        if not isinstance(epsilons, Iterable):\n            epsilons = [epsilons]\n            was_iterable = False\n\n        N = len(x)\n        K = len(epsilons)\n\n        for i in range(self.times):\n            # run the attack\n            xps, xpcs, success = self.attack(\n                model, x, criterion, epsilons=epsilons, **kwargs\n            )\n            assert len(xps) == K\n            assert len(xpcs) == K\n            for xp in xps:\n                assert xp.shape == x.shape\n            for xpc in xpcs:\n                assert xpc.shape == x.shape\n            assert success.shape == (K, N)\n\n            if i == 0:\n                best_xps = xps\n                best_xpcs = xpcs\n                best_success = success\n                continue\n\n            # TODO: test if stacking the list to a single tensor and\n            # getting rid of the loop is faster\n\n            for k, epsilon in enumerate(epsilons):\n                first = best_success[k].logical_not()\n                assert first.shape == (N,)\n                if epsilon is None:\n                    # if epsilon is None, we need the minimum\n\n                    # TODO: maybe cache some of these distances\n                    # and then remove the else part\n                    closer = self.distance(x, xps[k]) < self.distance(x, best_xps[k])\n                    assert closer.shape == (N,)\n                    new_best = ep.logical_and(success[k], ep.logical_or(closer, first))\n                else:\n                    # for concrete epsilon, we just need a successful one\n                    new_best = ep.logical_and(success[k], first)\n                new_best = atleast_kd(new_best, x.ndim)\n                best_xps[k] = ep.where(new_best, xps[k], best_xps[k])\n                best_xpcs[k] = ep.where(new_best, xpcs[k], best_xpcs[k])\n\n            best_success = ep.logical_or(success, best_success)\n\n        best_xps_ = [restore_type(xp) for xp in best_xps]\n        best_xpcs_ = [restore_type(xpc) for xpc in best_xpcs]\n        if was_iterable:\n            return best_xps_, best_xpcs_, restore_type(best_success)\n        else:\n            assert len(best_xps_) == 1\n            assert len(best_xpcs_) == 1\n            return (\n                best_xps_[0],\n                best_xpcs_[0],\n                restore_type(best_success.squeeze(axis=0)),\n            )\n\n    def repeat(self, times: int) -> ""Repeated"":\n        return Repeated(self.attack, self.times * times)\n\n\nclass FixedEpsilonAttack(AttackWithDistance):\n    """"""Fixed-epsilon attacks try to find adversarials whose perturbation sizes\n    are limited by a fixed epsilon""""""\n\n    @abstractmethod\n    def run(\n        self, model: Model, inputs: T, criterion: Any, *, epsilon: float, **kwargs: Any\n    ) -> T:\n        """"""Runs the attack and returns perturbed inputs.\n\n        The size of the perturbations should be at most epsilon, but this\n        is not guaranteed and the caller should verify this or clip the result.\n        """"""\n        ...\n\n    @overload\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Sequence[Union[float, None]],\n        **kwargs: Any,\n    ) -> Tuple[List[T], List[T], T]:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[float, None],\n        **kwargs: Any,\n    ) -> Tuple[T, T, T]:\n        ...\n\n    @final  # noqa: F811\n    def __call__(  # type: ignore\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[Sequence[Union[float, None]], float, None],\n        **kwargs: Any,\n    ) -> Union[Tuple[List[T], List[T], T], Tuple[T, T, T]]:\n\n        x, restore_type = ep.astensor_(inputs)\n        del inputs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        was_iterable = True\n        if not isinstance(epsilons, Iterable):\n            epsilons = [epsilons]\n            was_iterable = False\n\n        N = len(x)\n        K = len(epsilons)\n\n        # None means: just minimize, no early stopping, no limit on the perturbation size\n        if any(eps is None for eps in epsilons):\n            # TODO: implement a binary search\n            raise NotImplementedError(\n                ""FixedEpsilonAttack subclasses do not yet support None in epsilons""\n            )\n        real_epsilons = [eps for eps in epsilons if eps is not None]\n        del epsilons\n\n        xps = []\n        xpcs = []\n        success = []\n        for epsilon in real_epsilons:\n            xp = self.run(model, x, criterion, epsilon=epsilon, **kwargs)\n\n            # clip to epsilon because we don\'t really know what the attack returns;\n            # alternatively, we could check if the perturbation is at most epsilon,\n            # but then we would need to handle numerical violations;\n            xpc = self.distance.clip_perturbation(x, xp, epsilon)\n            is_adv = is_adversarial(xpc)\n\n            xps.append(xp)\n            xpcs.append(xpc)\n            success.append(is_adv)\n\n        # # TODO: the correction we apply here should make sure that the limits\n        # # are not violated, but this is a hack and we need a better solution\n        # # Alternatively, maybe can just enforce the limits in __call__\n        # xps = [\n        #     self.run(model, x, criterion, epsilon=epsilon, **kwargs)\n        #     for epsilon in real_epsilons\n        # ]\n\n        # is_adv = ep.stack([is_adversarial(xp) for xp in xps])\n        # assert is_adv.shape == (K, N)\n\n        # in_limits = ep.stack(\n        #     [\n        #         self.distance(x, xp) <= epsilon\n        #         for xp, epsilon in zip(xps, real_epsilons)\n        #     ],\n        # )\n        # assert in_limits.shape == (K, N)\n\n        # if not in_limits.all():\n        #     # TODO handle (numerical) violations\n        #     # warn user if run() violated the epsilon constraint\n        #     import pdb\n\n        #     pdb.set_trace()\n\n        # success = ep.logical_and(in_limits, is_adv)\n        # assert success.shape == (K, N)\n\n        success_ = ep.stack(success)\n        assert success_.shape == (K, N)\n\n        xps_ = [restore_type(xp) for xp in xps]\n        xpcs_ = [restore_type(xpc) for xpc in xpcs]\n\n        if was_iterable:\n            return xps_, xpcs_, restore_type(success_)\n        else:\n            assert len(xps_) == 1\n            assert len(xpcs_) == 1\n            return xps_[0], xpcs_[0], restore_type(success_.squeeze(axis=0))\n\n\nclass MinimizationAttack(AttackWithDistance):\n    """"""Minimization attacks try to find adversarials with minimal perturbation sizes""""""\n\n    @abstractmethod\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        """"""Runs the attack and returns perturbed inputs.\n\n        The size of the perturbations should be as small as possible such that\n        the perturbed inputs are still adversarial. In general, this is not\n        guaranteed and the caller has to verify this.\n        """"""\n        ...\n\n    @overload\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Sequence[Union[float, None]],\n        **kwargs: Any,\n    ) -> Tuple[List[T], List[T], T]:\n        ...\n\n    @overload  # noqa: F811\n    def __call__(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[float, None],\n        **kwargs: Any,\n    ) -> Tuple[T, T, T]:\n        ...\n\n    @final  # noqa: F811\n    def __call__(  # type: ignore\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Any,\n        *,\n        epsilons: Union[Sequence[Union[float, None]], float, None],\n        **kwargs: Any,\n    ) -> Union[Tuple[List[T], List[T], T], Tuple[T, T, T]]:\n        x, restore_type = ep.astensor_(inputs)\n        del inputs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        was_iterable = True\n        if not isinstance(epsilons, Iterable):\n            epsilons = [epsilons]\n            was_iterable = False\n\n        N = len(x)\n        K = len(epsilons)\n\n        # None means: just minimize, no early stopping, no limit on the perturbation size\n        if any(eps is None for eps in epsilons):\n            early_stop = None\n        else:\n            early_stop = min(epsilons)\n\n        # run the actual attack\n        xp = self.run(model, x, criterion, early_stop=early_stop, **kwargs)\n\n        xpcs = []\n        success = []\n        for epsilon in epsilons:\n            if epsilon is None:\n                xpc = xp\n            else:\n                xpc = self.distance.clip_perturbation(x, xp, epsilon)\n            is_adv = is_adversarial(xpc)\n\n            xpcs.append(xpc)\n            success.append(is_adv)\n\n        success_ = ep.stack(success)\n        assert success_.shape == (K, N)\n\n        xp_ = restore_type(xp)\n        xpcs_ = [restore_type(xpc) for xpc in xpcs]\n\n        if was_iterable:\n            return [xp_] * K, xpcs_, restore_type(success_)\n        else:\n            assert len(xpcs_) == 1\n            return xp_, xpcs_[0], restore_type(success_.squeeze(axis=0))\n\n\nclass FlexibleDistanceMinimizationAttack(MinimizationAttack):\n    def __init__(self, *, distance: Optional[Distance] = None):\n        self._distance = distance\n\n    @property\n    def distance(self) -> Distance:\n        if self._distance is None:\n            # we delay the error until the distance is needed,\n            # e.g. when __call__ is executed (that way, run\n            # can be used without specifying a distance)\n            raise ValueError(\n                ""unknown distance, please pass `distance` to the attack initializer""\n            )\n        return self._distance\n\n\ndef get_is_adversarial(\n    criterion: Criterion, model: Model\n) -> Callable[[ep.Tensor], ep.Tensor]:\n    def is_adversarial(perturbed: ep.Tensor) -> ep.Tensor:\n        outputs = model(perturbed)\n        return criterion(perturbed, outputs)\n\n    return is_adversarial\n\n\ndef get_criterion(criterion: Union[Criterion, Any]) -> Criterion:\n    if isinstance(criterion, Criterion):\n        return criterion\n    else:\n        return Misclassification(criterion)\n\n\ndef get_channel_axis(model: Model, ndim: int) -> Optional[int]:\n    data_format = getattr(model, ""data_format"", None)\n    if data_format is None:\n        return None\n    if data_format == ""channels_first"":\n        return 1\n    if data_format == ""channels_last"":\n        return ndim - 1\n    raise ValueError(\n        f""unknown data_format, expected \'channels_first\' or \'channels_last\', got {data_format}""\n    )\n\n\ndef raise_if_kwargs(kwargs: Dict[str, Any]) -> None:\n    if kwargs:\n        raise TypeError(\n            f""attack got an unexpected keyword argument \'{next(iter(kwargs.keys()))}\'""\n        )\n'"
foolbox/attacks/basic_iterative_method.py,0,"b'from typing import Optional\n\nfrom .gradient_descent_base import L1BaseGradientDescent\nfrom .gradient_descent_base import L2BaseGradientDescent\nfrom .gradient_descent_base import LinfBaseGradientDescent\n\n\nclass L1BasicIterativeAttack(L1BaseGradientDescent):\n    """"""L1 Basic Iterative Method\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon.\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps.\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float = 0.2,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 10,\n        random_start: bool = False,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n\n\nclass L2BasicIterativeAttack(L2BaseGradientDescent):\n    """"""L2 Basic Iterative Method\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon.\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps.\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float = 0.2,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 10,\n        random_start: bool = False,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n\n\nclass LinfBasicIterativeAttack(LinfBaseGradientDescent):\n    """"""L-infinity Basic Iterative Method\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon.\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps.\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float = 0.2,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 10,\n        random_start: bool = False,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n'"
foolbox/attacks/binarization.py,0,"b'from typing import Union, Optional, Any\nfrom typing_extensions import Literal\nimport eagerpy as ep\nimport numpy as np\n\nfrom ..models import Model\n\nfrom ..criteria import Criterion\n\nfrom ..distances import Distance\n\nfrom .base import FlexibleDistanceMinimizationAttack\nfrom .base import T\nfrom .base import get_is_adversarial\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\n\nclass BinarizationRefinementAttack(FlexibleDistanceMinimizationAttack):\n    """"""For models that preprocess their inputs by binarizing the\n    inputs, this attack can improve adversarials found by other\n    attacks. It does this by utilizing information about the\n    binarization and mapping values to the corresponding value in\n    the clean input or to the right side of the threshold.\n\n    Args:\n        threshold : The threshold used by the models binarization. If none,\n            defaults to (model.bounds()[1] - model.bounds()[0]) / 2.\n        included_in : Whether the threshold value itself belongs to the lower or\n            upper interval.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        distance: Optional[Distance] = None,\n        threshold: Optional[float] = None,\n        included_in: Union[Literal[""lower""], Literal[""upper""]] = ""upper"",\n    ):\n        super().__init__(distance=distance)\n        self.threshold = threshold\n        self.included_in = included_in\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        starting_points: Optional[T] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        if starting_points is None:\n            raise ValueError(""BinarizationRefinementAttack requires starting_points"")\n        (o, x), restore_type = ep.astensors_(inputs, starting_points)\n        del inputs, starting_points, kwargs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        if self.threshold is None:\n            min_, max_ = model.bounds\n            threshold = (min_ + max_) / 2.0\n        else:\n            threshold = self.threshold\n\n        assert o.dtype == x.dtype\n\n        nptype = o.reshape(-1)[0].numpy().dtype.type\n        if nptype not in [np.float16, np.float32, np.float64]:\n            raise ValueError(  # pragma: no cover\n                f""expected dtype to be float16, float32 or float64, found \'{nptype}\'""\n            )\n\n        threshold = nptype(threshold)\n        offset = nptype(1.0)\n\n        if self.included_in == ""lower"":\n            lower_ = threshold\n            upper_ = np.nextafter(threshold, threshold + offset)\n        elif self.included_in == ""upper"":\n            lower_ = np.nextafter(threshold, threshold - offset)\n            upper_ = threshold\n        else:\n            raise ValueError(\n                f""expected included_in to be \'lower\' or \'upper\', found \'{self.included_in}\'""\n            )\n\n        assert lower_ < upper_\n\n        p = ep.full_like(o, ep.nan)\n\n        lower = ep.ones_like(o) * lower_\n        upper = ep.ones_like(o) * upper_\n\n        indices = ep.logical_and(o <= lower, x <= lower)\n        p = ep.where(indices, o, p)\n\n        indices = ep.logical_and(o <= lower, x >= upper)\n        p = ep.where(indices, upper, p)\n\n        indices = ep.logical_and(o >= upper, x <= lower)\n        p = ep.where(indices, lower, p)\n\n        indices = ep.logical_and(o >= upper, x >= upper)\n        p = ep.where(indices, o, p)\n\n        assert not ep.any(ep.isnan(p))\n\n        is_adv1 = is_adversarial(x)\n        is_adv2 = is_adversarial(p)\n        if (is_adv1 != is_adv2).any():\n            raise ValueError(\n                ""The specified threshold does not match what is done by the model.""\n            )\n        return restore_type(p)\n'"
foolbox/attacks/blended_noise.py,0,"b'from typing import Union, Optional, Any\nimport numpy as np\nimport eagerpy as ep\n\nfrom ..devutils import atleast_kd\n\nfrom ..distances import Distance\n\nfrom .base import FlexibleDistanceMinimizationAttack\nfrom .base import Model\nfrom .base import Criterion\nfrom .base import T\nfrom .base import get_is_adversarial\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\nimport warnings\n\n\nclass LinearSearchBlendedUniformNoiseAttack(FlexibleDistanceMinimizationAttack):\n    """"""Blends the input with a uniform noise input until it is misclassified.\n\n    Args:\n        distance : Distance measure for which minimal adversarial examples are searched.\n        directions : Number of random directions in which the perturbation is searched.\n        steps : Number of blending steps between the original image and the random\n            directions.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        distance: Optional[Distance] = None,\n        directions: int = 1000,\n        steps: int = 1000,\n    ):\n        super().__init__(distance=distance)\n        self.directions = directions\n        self.steps = steps\n\n        if directions <= 0:\n            raise ValueError(""directions must be larger than 0"")\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, Any] = None,\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        is_adversarial = get_is_adversarial(criterion_, model)\n\n        min_, max_ = model.bounds\n\n        N = len(x)\n\n        for j in range(self.directions):\n            # random noise inputs tend to be classified into the same class,\n            # so we might need to make very many draws if the original class\n            # is that one\n            random_ = ep.uniform(x, x.shape, min_, max_)\n            is_adv_ = atleast_kd(is_adversarial(random_), x.ndim)\n\n            if j == 0:\n                random = random_\n                is_adv = is_adv_\n            else:\n                random = ep.where(is_adv, random, random_)\n                is_adv = is_adv.logical_or(is_adv_)\n\n            if is_adv.all():\n                break\n\n        if not is_adv.all():\n            warnings.warn(\n                f""{self.__class__.__name__} failed to draw sufficient random""\n                f"" inputs that are adversarial ({is_adv.sum()} / {N}).""\n            )\n\n        x0 = x\n\n        epsilons = np.linspace(0, 1, num=self.steps + 1, dtype=np.float32)\n        best = ep.ones(x, (N,))\n\n        for epsilon in epsilons:\n            x = (1 - epsilon) * x0 + epsilon * random\n            # TODO: due to limited floating point precision, clipping can be required\n            is_adv = is_adversarial(x)\n\n            epsilon = epsilon.item()\n\n            best = ep.minimum(ep.where(is_adv, epsilon, 1.0), best)\n\n            if (best < 1).all():\n                break\n\n        best = atleast_kd(best, x0.ndim)\n        x = (1 - best) * x0 + best * random\n\n        return restore_type(x)\n'"
foolbox/attacks/blur.py,0,"b'from typing import Union, Optional, Any\nimport numpy as np\nfrom scipy.ndimage.filters import gaussian_filter\nimport eagerpy as ep\n\nfrom ..devutils import atleast_kd\n\nfrom ..models import Model\n\nfrom ..criteria import Criterion\n\nfrom ..distances import Distance\n\nfrom .base import FlexibleDistanceMinimizationAttack\nfrom .base import T\nfrom .base import get_is_adversarial\nfrom .base import get_criterion\nfrom .base import get_channel_axis\nfrom .base import raise_if_kwargs\n\n\nclass GaussianBlurAttack(FlexibleDistanceMinimizationAttack):\n    """"""Blurs the inputs using a Gaussian filter with linearly\n    increasing standard deviation.\n\n    Args:\n        steps : Number of sigma values tested between 0 and max_sigma.\n        channel_axis : Index of the channel axis in the input data.\n        max_sigma : Maximally allowed sigma value of the Gaussian blur.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        distance: Optional[Distance] = None,\n        steps: int = 1000,\n        channel_axis: Optional[int] = None,\n        max_sigma: Optional[float] = None,\n    ):\n        super().__init__(distance=distance)\n        self.steps = steps\n        self.channel_axis = channel_axis\n        self.max_sigma = max_sigma\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        if x.ndim != 4:\n            raise NotImplementedError(\n                ""only implemented for inputs with two spatial dimensions (and one channel and one batch dimension)""\n            )\n\n        if self.channel_axis is None:\n            channel_axis = get_channel_axis(model, x.ndim)\n        else:\n            channel_axis = self.channel_axis % x.ndim\n\n        if channel_axis is None:\n            raise ValueError(\n                ""cannot infer the data_format from the model, please specify""\n                "" channel_axis when initializing the attack""\n            )\n\n        max_sigma: float\n        if self.max_sigma is None:\n            if channel_axis == 1:\n                h, w = x.shape[2:4]\n            elif channel_axis == 3:\n                h, w = x.shape[1:3]\n            else:\n                raise ValueError(\n                    ""expected \'channel_axis\' to be 1 or 3, got {channel_axis}""\n                )\n            max_sigma = max(h, w)\n        else:\n            max_sigma = self.max_sigma\n\n        min_, max_ = model.bounds\n\n        x0 = x\n        x0_ = x0.numpy()\n\n        result = x0\n        found = is_adversarial(x0)\n\n        epsilon = 0.0\n        stepsize = 1.0 / self.steps\n        for _ in range(self.steps):\n            # TODO: reduce the batch size to the ones that haven\'t been sucessful\n\n            epsilon += stepsize\n\n            sigmas = [epsilon * max_sigma] * x0.ndim\n            sigmas[0] = 0\n            sigmas[channel_axis] = 0\n\n            # TODO: once we can implement gaussian_filter in eagerpy, avoid converting from numpy\n            x_ = gaussian_filter(x0_, sigmas)\n            x_ = np.clip(x_, min_, max_)\n            x = ep.from_numpy(x0, x_)\n\n            is_adv = is_adversarial(x)\n            new_adv = ep.logical_and(is_adv, found.logical_not())\n            result = ep.where(atleast_kd(new_adv, x.ndim), x, result)\n            found = ep.logical_or(new_adv, found)\n\n            if found.all():\n                break\n\n        return restore_type(result)\n'"
foolbox/attacks/boundary_attack.py,0,"b'from typing import Union, Tuple, Optional, Any\nfrom typing_extensions import Literal\nimport numpy as np\nimport eagerpy as ep\nimport logging\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..types import Bounds\n\nfrom ..models import Model\n\nfrom ..criteria import Criterion\n\nfrom ..distances import l2\n\nfrom ..tensorboard import TensorBoard\n\nfrom .blended_noise import LinearSearchBlendedUniformNoiseAttack\n\nfrom .base import MinimizationAttack\nfrom .base import T\nfrom .base import get_criterion\nfrom .base import get_is_adversarial\nfrom .base import raise_if_kwargs\n\n\nclass BoundaryAttack(MinimizationAttack):\n    """"""A powerful adversarial attack that requires neither gradients\n    nor probabilities.\n\n    This is the reference implementation for the attack. [#Bren18]_\n\n    Notes:\n        Differences to the original reference implementation:\n        * We do not perform internal operations with float64\n        * The samples within a batch can currently influence each other a bit\n        * We don\'t perform the additional convergence confirmation\n        * The success rate tracking changed a bit\n        * Some other changes due to batching and merged loops\n\n    Args:\n        init_attack : Attack to use to find a starting points. Defaults to\n            LinearSearchBlendedUniformNoiseAttack. Only used if starting_points is None.\n        steps : Maximum number of steps to run. Might converge and stop before that.\n        spherical_step : Initial step size for the orthogonal (spherical) step.\n        source_step : Initial step size for the step towards the target.\n        source_step_convergance : Sets the threshold of the stop criterion:\n            if source_step becomes smaller than this value during the attack,\n            the attack has converged and will stop.\n        step_adaptation : Factor by which the step sizes are multiplied or divided.\n        tensorboard : The log directory for TensorBoard summaries. If False, TensorBoard\n            summaries will be disabled (default). If None, the logdir will be\n            runs/CURRENT_DATETIME_HOSTNAME.\n        update_stats_every_k :\n\n    References:\n        .. [#Bren18] Wieland Brendel (*), Jonas Rauber (*), Matthias Bethge,\n           ""Decision-Based Adversarial Attacks: Reliable Attacks\n           Against Black-Box Machine Learning Models"",\n           https://arxiv.org/abs/1712.04248\n    """"""\n\n    distance = l2\n\n    def __init__(\n        self,\n        init_attack: Optional[MinimizationAttack] = None,\n        steps: int = 25000,\n        spherical_step: float = 1e-2,\n        source_step: float = 1e-2,\n        source_step_convergance: float = 1e-7,\n        step_adaptation: float = 1.5,\n        tensorboard: Union[Literal[False], None, str] = False,\n        update_stats_every_k: int = 10,\n    ):\n        if init_attack is not None and not isinstance(init_attack, MinimizationAttack):\n            raise NotImplementedError\n        self.init_attack = init_attack\n        self.steps = steps\n        self.spherical_step = spherical_step\n        self.source_step = source_step\n        self.source_step_convergance = source_step_convergance\n        self.step_adaptation = step_adaptation\n        self.tensorboard = tensorboard\n        self.update_stats_every_k = update_stats_every_k\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        starting_points: Optional[T] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        originals, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        if starting_points is None:\n            init_attack: MinimizationAttack\n            if self.init_attack is None:\n                init_attack = LinearSearchBlendedUniformNoiseAttack(steps=50)\n                logging.info(\n                    f""Neither starting_points nor init_attack given. Falling""\n                    f"" back to {init_attack!r} for initialization.""\n                )\n            else:\n                init_attack = self.init_attack\n            # TODO: use call and support all types of attacks (once early_stop is\n            # possible in __call__)\n            best_advs = init_attack.run(\n                model, originals, criterion, early_stop=early_stop\n            )\n        else:\n            best_advs = ep.astensor(starting_points)\n\n        is_adv = is_adversarial(best_advs)\n        if not is_adv.all():\n            failed = is_adv.logical_not().float32().sum()\n            if starting_points is None:\n                raise ValueError(\n                    f""init_attack failed for {failed} of {len(is_adv)} inputs""\n                )\n            else:\n                raise ValueError(\n                    f""{failed} of {len(is_adv)} starting_points are not adversarial""\n                )\n        del starting_points\n\n        tb = TensorBoard(logdir=self.tensorboard)\n\n        N = len(originals)\n        ndim = originals.ndim\n        spherical_steps = ep.ones(originals, N) * self.spherical_step\n        source_steps = ep.ones(originals, N) * self.source_step\n\n        tb.scalar(""batchsize"", N, 0)\n\n        # create two queues for each sample to track success rates\n        # (used to update the hyper parameters)\n        stats_spherical_adversarial = ArrayQueue(maxlen=100, N=N)\n        stats_step_adversarial = ArrayQueue(maxlen=30, N=N)\n\n        bounds = model.bounds\n\n        for step in range(1, self.steps + 1):\n            converged = source_steps < self.source_step_convergance\n            if converged.all():\n                break  # pragma: no cover\n            converged = atleast_kd(converged, ndim)\n\n            # TODO: performance: ignore those that have converged\n            # (we could select the non-converged ones, but we currently\n            # cannot easily invert this in the end using EagerPy)\n\n            unnormalized_source_directions = originals - best_advs\n            source_norms = ep.norms.l2(flatten(unnormalized_source_directions), axis=-1)\n            source_directions = unnormalized_source_directions / atleast_kd(\n                source_norms, ndim\n            )\n\n            # only check spherical candidates every k steps\n            check_spherical_and_update_stats = step % self.update_stats_every_k == 0\n\n            candidates, spherical_candidates = draw_proposals(\n                bounds,\n                originals,\n                best_advs,\n                unnormalized_source_directions,\n                source_directions,\n                source_norms,\n                spherical_steps,\n                source_steps,\n            )\n            candidates.dtype == originals.dtype\n            spherical_candidates.dtype == spherical_candidates.dtype\n\n            is_adv = is_adversarial(candidates)\n\n            spherical_is_adv: Optional[ep.Tensor]\n            if check_spherical_and_update_stats:\n                spherical_is_adv = is_adversarial(spherical_candidates)\n                stats_spherical_adversarial.append(spherical_is_adv)\n                # TODO: algorithm: the original implementation ignores those samples\n                # for which spherical is not adversarial and continues with the\n                # next iteration -> we estimate different probabilities (conditional vs. unconditional)\n                # TODO: thoughts: should we always track this because we compute it anyway\n                stats_step_adversarial.append(is_adv)\n            else:\n                spherical_is_adv = None\n\n            # in theory, we are closer per construction\n            # but limited numerical precision might break this\n            distances = ep.norms.l2(flatten(originals - candidates), axis=-1)\n            closer = distances < source_norms\n            is_best_adv = ep.logical_and(is_adv, closer)\n            is_best_adv = atleast_kd(is_best_adv, ndim)\n\n            cond = converged.logical_not().logical_and(is_best_adv)\n            best_advs = ep.where(cond, candidates, best_advs)\n\n            tb.probability(""converged"", converged, step)\n            tb.scalar(""updated_stats"", check_spherical_and_update_stats, step)\n            tb.histogram(""norms"", source_norms, step)\n            tb.probability(""is_adv"", is_adv, step)\n            if spherical_is_adv is not None:\n                tb.probability(""spherical_is_adv"", spherical_is_adv, step)\n            tb.histogram(""candidates/distances"", distances, step)\n            tb.probability(""candidates/closer"", closer, step)\n            tb.probability(""candidates/is_best_adv"", is_best_adv, step)\n            tb.probability(""new_best_adv_including_converged"", is_best_adv, step)\n            tb.probability(""new_best_adv"", cond, step)\n\n            if check_spherical_and_update_stats:\n                full = stats_spherical_adversarial.isfull()\n                tb.probability(""spherical_stats/full"", full, step)\n                if full.any():\n                    probs = stats_spherical_adversarial.mean()\n                    cond1 = ep.logical_and(probs > 0.5, full)\n                    spherical_steps = ep.where(\n                        cond1, spherical_steps * self.step_adaptation, spherical_steps\n                    )\n                    source_steps = ep.where(\n                        cond1, source_steps * self.step_adaptation, source_steps\n                    )\n                    cond2 = ep.logical_and(probs < 0.2, full)\n                    spherical_steps = ep.where(\n                        cond2, spherical_steps / self.step_adaptation, spherical_steps\n                    )\n                    source_steps = ep.where(\n                        cond2, source_steps / self.step_adaptation, source_steps\n                    )\n                    stats_spherical_adversarial.clear(ep.logical_or(cond1, cond2))\n                    tb.conditional_mean(\n                        ""spherical_stats/isfull/success_rate/mean"", probs, full, step\n                    )\n                    tb.probability_ratio(\n                        ""spherical_stats/isfull/too_linear"", cond1, full, step\n                    )\n                    tb.probability_ratio(\n                        ""spherical_stats/isfull/too_nonlinear"", cond2, full, step\n                    )\n\n                full = stats_step_adversarial.isfull()\n                tb.probability(""step_stats/full"", full, step)\n                if full.any():\n                    probs = stats_step_adversarial.mean()\n                    # TODO: algorithm: changed the two values because we are currently tracking p(source_step_sucess)\n                    # instead of p(source_step_success | spherical_step_sucess) that was tracked before\n                    cond1 = ep.logical_and(probs > 0.25, full)\n                    source_steps = ep.where(\n                        cond1, source_steps * self.step_adaptation, source_steps\n                    )\n                    cond2 = ep.logical_and(probs < 0.1, full)\n                    source_steps = ep.where(\n                        cond2, source_steps / self.step_adaptation, source_steps\n                    )\n                    stats_step_adversarial.clear(ep.logical_or(cond1, cond2))\n                    tb.conditional_mean(\n                        ""step_stats/isfull/success_rate/mean"", probs, full, step\n                    )\n                    tb.probability_ratio(\n                        ""step_stats/isfull/success_rate_too_high"", cond1, full, step\n                    )\n                    tb.probability_ratio(\n                        ""step_stats/isfull/success_rate_too_low"", cond2, full, step\n                    )\n\n            tb.histogram(""spherical_step"", spherical_steps, step)\n            tb.histogram(""source_step"", source_steps, step)\n        tb.close()\n        return restore_type(best_advs)\n\n\nclass ArrayQueue:\n    def __init__(self, maxlen: int, N: int):\n        # we use NaN as an indicator for missing data\n        self.data = np.full((maxlen, N), np.nan)\n        self.next = 0\n        # used to infer the correct framework because this class uses NumPy\n        self.tensor: Optional[ep.Tensor] = None\n\n    @property\n    def maxlen(self) -> int:\n        return int(self.data.shape[0])\n\n    @property\n    def N(self) -> int:\n        return int(self.data.shape[1])\n\n    def append(self, x: ep.Tensor) -> None:\n        if self.tensor is None:\n            self.tensor = x\n        x = x.numpy()\n        assert x.shape == (self.N,)\n        self.data[self.next] = x\n        self.next = (self.next + 1) % self.maxlen\n\n    def clear(self, dims: ep.Tensor) -> None:\n        if self.tensor is None:\n            self.tensor = dims  # pragma: no cover\n        dims = dims.numpy()\n        assert dims.shape == (self.N,)\n        assert dims.dtype == np.bool\n        self.data[:, dims] = np.nan\n\n    def mean(self) -> ep.Tensor:\n        assert self.tensor is not None\n        result = np.nanmean(self.data, axis=0)\n        return ep.from_numpy(self.tensor, result)\n\n    def isfull(self) -> ep.Tensor:\n        assert self.tensor is not None\n        result = ~np.isnan(self.data).any(axis=0)\n        return ep.from_numpy(self.tensor, result)\n\n\ndef draw_proposals(\n    bounds: Bounds,\n    originals: ep.Tensor,\n    perturbed: ep.Tensor,\n    unnormalized_source_directions: ep.Tensor,\n    source_directions: ep.Tensor,\n    source_norms: ep.Tensor,\n    spherical_steps: ep.Tensor,\n    source_steps: ep.Tensor,\n) -> Tuple[ep.Tensor, ep.Tensor]:\n    # remember the actual shape\n    shape = originals.shape\n    assert perturbed.shape == shape\n    assert unnormalized_source_directions.shape == shape\n    assert source_directions.shape == shape\n\n    # flatten everything to (batch, size)\n    originals = flatten(originals)\n    perturbed = flatten(perturbed)\n    unnormalized_source_directions = flatten(unnormalized_source_directions)\n    source_directions = flatten(source_directions)\n    N, D = originals.shape\n\n    assert source_norms.shape == (N,)\n    assert spherical_steps.shape == (N,)\n    assert source_steps.shape == (N,)\n\n    # draw from an iid Gaussian (we can share this across the whole batch)\n    eta = ep.normal(perturbed, (D, 1))\n\n    # make orthogonal (source_directions are normalized)\n    eta = eta.T - ep.matmul(source_directions, eta) * source_directions\n    assert eta.shape == (N, D)\n\n    # rescale\n    norms = ep.norms.l2(eta, axis=-1)\n    assert norms.shape == (N,)\n    eta = eta * atleast_kd(spherical_steps * source_norms / norms, eta.ndim)\n\n    # project on the sphere using Pythagoras\n    distances = atleast_kd((spherical_steps.square() + 1).sqrt(), eta.ndim)\n    directions = eta - unnormalized_source_directions\n    spherical_candidates = originals + directions / distances\n\n    # clip\n    min_, max_ = bounds\n    spherical_candidates = spherical_candidates.clip(min_, max_)\n\n    # step towards the original inputs\n    new_source_directions = originals - spherical_candidates\n    assert new_source_directions.ndim == 2\n    new_source_directions_norms = ep.norms.l2(flatten(new_source_directions), axis=-1)\n\n    # length if spherical_candidates would be exactly on the sphere\n    lengths = source_steps * source_norms\n\n    # length including correction for numerical deviation from sphere\n    lengths = lengths + new_source_directions_norms - source_norms\n\n    # make sure the step size is positive\n    lengths = ep.maximum(lengths, 0)\n\n    # normalize the length\n    lengths = lengths / new_source_directions_norms\n    lengths = atleast_kd(lengths, new_source_directions.ndim)\n\n    candidates = spherical_candidates + lengths * new_source_directions\n\n    # clip\n    candidates = candidates.clip(min_, max_)\n\n    # restore shape\n    candidates = candidates.reshape(shape)\n    spherical_candidates = spherical_candidates.reshape(shape)\n    return candidates, spherical_candidates\n'"
foolbox/attacks/brendel_bethge.py,0,"b'# mypy: allow-untyped-defs, no-strict-optional\n\nfrom typing import Union, Optional, Tuple, Any\nfrom typing_extensions import Literal\nfrom abc import ABC\nfrom abc import abstractmethod\nimport numpy as np\nimport eagerpy as ep\nimport logging\nimport warnings\nfrom ..devutils import flatten\nfrom . import LinearSearchBlendedUniformNoiseAttack\nfrom ..tensorboard import TensorBoard\nfrom .base import Model\nfrom .base import MinimizationAttack\nfrom .base import get_is_adversarial\nfrom .base import get_criterion\nfrom .base import T\nfrom ..criteria import Misclassification, TargetedMisclassification\nfrom .base import raise_if_kwargs\nfrom ..distances import l0, l1, l2, linf\n\n\ntry:\n    from numba import jitclass  # type: ignore\nexcept (ModuleNotFoundError, ImportError) as e:  # pragma: no cover\n    # delay the error until the attack is initialized\n    NUMBA_IMPORT_ERROR = e\n\n    def jitclass(*args, **kwargs):\n        def decorator(c):\n            return c\n\n        return decorator\n\n\nelse:\n    NUMBA_IMPORT_ERROR = None\n\n\nEPS = 1e-10\n\n\nclass Optimizer(object):  # pragma: no cover\n    """""" Base class for the trust-region optimization. If feasible, this optimizer solves the problem\n\n        min_delta distance(x0, x + delta) s.t. ||delta||_2 <= r AND delta^T b = c AND min_ <= x + delta <= max_\n\n        where x0 is the original sample, x is the current optimisation state, r is the trust-region radius,\n        b is the current estimate of the normal vector of the decision boundary, c is the estimated distance of x\n        to the trust region and [min_, max_] are the value constraints of the input. The function distance(.,.)\n        is the distance measure to be optimised (e.g. L2, L1, L0).\n\n    """"""\n\n    def __init__(self):\n        self.bfgsb = BFGSB()  # a box-constrained BFGS solver\n\n    def solve(self, x0, x, b, min_, max_, c, r):\n        x0, x, b = x0.astype(np.float64), x.astype(np.float64), b.astype(np.float64)\n        cmax, cmaxnorm = self._max_logit_diff(x, b, min_, max_, c)\n\n        if np.abs(cmax) < np.abs(c):\n            # problem not solvable (boundary cannot be reached)\n            if np.sqrt(cmaxnorm) < r:\n                # make largest possible step towards boundary while staying within bounds\n                _delta = self.optimize_boundary_s_t_trustregion(\n                    x0, x, b, min_, max_, c, r\n                )\n            else:\n                # make largest possible step towards boundary while staying within trust region\n                _delta = self.optimize_boundary_s_t_trustregion(\n                    x0, x, b, min_, max_, c, r\n                )\n        else:\n            if cmaxnorm < r:\n                # problem is solvable\n                # proceed with standard optimization\n                _delta = self.optimize_distance_s_t_boundary_and_trustregion(\n                    x0, x, b, min_, max_, c, r\n                )\n            else:\n                # problem might not be solvable\n                bnorm = np.linalg.norm(b)\n                minnorm = self._minimum_norm_to_boundary(x, b, min_, max_, c, bnorm)\n\n                if minnorm <= r:\n                    # problem is solvable, proceed with standard optimization\n                    _delta = self.optimize_distance_s_t_boundary_and_trustregion(\n                        x0, x, b, min_, max_, c, r\n                    )\n                else:\n                    # problem not solvable (boundary cannot be reached)\n                    # make largest step towards boundary within trust region\n                    _delta = self.optimize_boundary_s_t_trustregion(\n                        x0, x, b, min_, max_, c, r\n                    )\n\n        return _delta\n\n    def _max_logit_diff(self, x, b, _ell, _u, c):\n        """""" Tests whether the (estimated) boundary can be reached within trust region. """"""\n        N = x.shape[0]\n        cmax = 0.0\n        norm = 0.0\n\n        if c > 0:\n            for n in range(N):\n                if b[n] > 0:\n                    cmax += b[n] * (_u - x[n])\n                    norm += (_u - x[n]) ** 2\n                else:\n                    cmax += b[n] * (_ell - x[n])\n                    norm += (x[n] - _ell) ** 2\n        else:\n            for n in range(N):\n                if b[n] > 0:\n                    cmax += b[n] * (_ell - x[n])\n                    norm += (x[n] - _ell) ** 2\n                else:\n                    cmax += b[n] * (_u - x[n])\n                    norm += (_u - x[n]) ** 2\n\n        return cmax, np.sqrt(norm)\n\n    def _minimum_norm_to_boundary(self, x, b, _ell, _u, c, bnorm):\n        """""" Computes the minimum norm necessary to reach the boundary. More precisely, we aim to solve the\n            following optimization problem\n\n                min ||delta||_2^2 s.t. lower <= x + delta <= upper AND b.dot(delta) = c\n\n            Lets forget about the box constraints for a second, i.e.\n\n                min ||delta||_2^2 s.t. b.dot(delta) = c\n\n            The dual of this problem is quite straight-forward to solve,\n\n                g(lambda, delta) = ||delta||_2^2 + lambda * (c - b.dot(delta))\n\n            The minimum of this Lagrangian is delta^* = lambda * b / 2, and so\n\n                inf_delta g(lambda, delta) = lambda^2 / 4 ||b||_2^2 + lambda * c\n\n            and so the optimal lambda, which maximizes inf_delta g(lambda, delta), is given by\n\n                lambda^* = 2c / ||b||_2^2\n\n            which in turn yields the optimal delta:\n\n                delta^* = c * b / ||b||_2^2\n\n            To take into account the box-constraints we perform a binary search over lambda and apply the box\n            constraint in each step.\n        """"""\n        N = x.shape[0]\n\n        lambda_lower = 2 * c / bnorm ** 2\n        lambda_upper = (\n            np.sign(c) * np.inf\n        )  # optimal initial point (if box-constraints are neglected)\n        _lambda = lambda_lower\n        k = 0\n\n        # perform a binary search over lambda\n        while True:\n            # compute _c = b.dot([- _lambda * b / 2]_clip)\n            k += 1\n            _c = 0\n            norm = 0\n\n            if c > 0:\n                for n in range(N):\n                    lam_step = _lambda * b[n] / 2\n                    if b[n] > 0:\n                        max_step = _u - x[n]\n                        delta_step = min(max_step, lam_step)\n                        _c += b[n] * delta_step\n                        norm += delta_step ** 2\n                    else:\n                        max_step = _ell - x[n]\n                        delta_step = max(max_step, lam_step)\n                        _c += b[n] * delta_step\n                        norm += delta_step ** 2\n            else:\n                for n in range(N):\n                    lam_step = _lambda * b[n] / 2\n                    if b[n] > 0:\n                        max_step = _ell - x[n]\n                        delta_step = max(max_step, lam_step)\n                        _c += b[n] * delta_step\n                        norm += delta_step ** 2\n                    else:\n                        max_step = _u - x[n]\n                        delta_step = min(max_step, lam_step)\n                        _c += b[n] * delta_step\n                        norm += delta_step ** 2\n\n            # adjust lambda\n            if np.abs(_c) < np.abs(c):\n                # increase absolute value of lambda\n                if np.isinf(lambda_upper):\n                    _lambda *= 2\n                else:\n                    lambda_lower = _lambda\n                    _lambda = (lambda_upper - lambda_lower) / 2 + lambda_lower\n            else:\n                # decrease lambda\n                lambda_upper = _lambda\n                _lambda = (lambda_upper - lambda_lower) / 2 + lambda_lower\n\n            # stopping condition\n            if 0.999 * np.abs(c) - EPS < np.abs(_c) < 1.001 * np.abs(c) + EPS:\n                break\n\n        return np.sqrt(norm)\n\n    def optimize_distance_s_t_boundary_and_trustregion(\n        self, x0, x, b, min_, max_, c, r\n    ):\n        """""" Find the solution to the optimization problem\n\n            min_delta ||dx - delta||_p^p s.t. ||delta||_2^2 <= r^2 AND b^T delta = c AND min_ <= x + delta <= max_\n        """"""\n        params0 = np.array([0.0, 0.0])\n        bounds = np.array([(-np.inf, np.inf), (0, np.inf)])\n        args = (x0, x, b, min_, max_, c, r)\n\n        qk = self.bfgsb.solve(self.fun_and_jac, params0, bounds, args)\n        return self._get_final_delta(\n            qk[0], qk[1], x0, x, b, min_, max_, c, r, touchup=True\n        )\n\n    def optimize_boundary_s_t_trustregion_fun_and_jac(\n        self, params, x0, x, b, min_, max_, c, r\n    ):\n        N = x0.shape[0]\n        s = -np.sign(c)\n        _mu = params[0]\n        t = 1 / (2 * _mu + EPS)\n\n        g = -_mu * r ** 2\n        grad_mu = -(r ** 2)\n\n        for n in range(N):\n            d = -s * b[n] * t\n\n            if d < min_ - x[n]:\n                d = min_ - x[n]\n            elif d > max_ - x[n]:\n                d = max_ - x[n]\n            else:\n                grad_mu += (b[n] + 2 * _mu * d) * (b[n] / (2 * _mu ** 2 + EPS))\n\n            grad_mu += d ** 2\n            g += (b[n] + _mu * d) * d\n\n        return -g, -np.array([grad_mu])\n\n    def safe_div(self, nominator, denominator):\n        if np.abs(denominator) > EPS:\n            return nominator / denominator\n        elif denominator >= 0:\n            return nominator / EPS\n        else:\n            return -nominator / EPS\n\n    def optimize_boundary_s_t_trustregion(self, x0, x, b, min_, max_, c, r):\n        """""" Find the solution to the optimization problem\n\n            min_delta sign(c) b^T delta s.t. ||delta||_2^2 <= r^2 AND min_ <= x + delta <= max_\n\n            Note: this optimization problem is independent of the Lp norm being optimized.\n\n            Lagrangian: g(delta) = sign(c) b^T delta + mu * (||delta||_2^2 - r^2)\n            Optimal delta: delta = - sign(c) * b / (2 * mu)\n        """"""\n        params0 = np.array([1.0])\n        args = (x0, x, b, min_, max_, c, r)\n        bounds = np.array([(0, np.inf)])\n\n        qk = self.bfgsb.solve(\n            self.optimize_boundary_s_t_trustregion_fun_and_jac, params0, bounds, args\n        )\n\n        _delta = self.safe_div(-b, 2 * qk[0])\n\n        for n in range(x0.shape[0]):\n            if _delta[n] < min_ - x[n]:\n                _delta[n] = min_ - x[n]\n            elif _delta[n] > max_ - x[n]:\n                _delta[n] = max_ - x[n]\n\n        return _delta\n\n\nclass BrendelBethgeAttack(MinimizationAttack, ABC):\n    """"""Base class for the Brendel & Bethge adversarial attack [#Bren19]_, a powerful\n    gradient-based adversarial attack that follows the adversarial boundary\n    (the boundary between the space of adversarial and non-adversarial images as\n    defined by the adversarial criterion) to find the minimum distance to the\n    clean image.\n\n    This is the reference implementation of the Brendel & Bethge attack.\n\n    Implementation differs from the attack used in the paper in two ways:\n    * The initial binary search is always using the full 10 steps (for ease of implementation).\n    * The adaptation of the trust region over the course of optimisation is less\n      greedy but is more robust, reliable and simpler (decay every K steps)\n\n    Args:\n        init_attack : Attack to use to find a starting points. Defaults to\n            LinearSearchBlendedUniformNoiseAttack. Only used if starting_points is None.\n        overshoot : If 1 the attack tries to return exactly to the adversarial boundary\n            in each iteration. For higher values the attack tries to overshoot\n            over the boundary to ensure that the perturbed sample in each iteration\n            is adversarial.\n        steps : Maximum number of iterations to run. Might converge and stop\n            before that.\n        lr : Trust region radius, behaves similar to a learning rate. Smaller values\n            decrease the step size in each iteration and ensure that the attack\n            follows the boundary more faithfully.\n        lr_decay : The trust region lr is multiplied with lr_decay in regular intervals (see\n            lr_reduction_interval).\n        lr_num_decay : Number of learning rate decays in regular intervals of\n            length steps / lr_num_decay.\n        momentum : Averaging of the boundary estimation over multiple steps. A momentum of\n            zero would always take the current estimate while values closer to one\n            average over a larger number of iterations.\n        tensorboard : The log directory for TensorBoard summaries. If False, TensorBoard\n            summaries will be disabled (default). If None, the logdir will be\n            runs/CURRENT_DATETIME_HOSTNAME.\n        binary_search_steps : Number of binary search steps used to find the adversarial boundary\n            between the starting point and the clean image.\n\n    References:\n        .. [#Bren19] Wieland Brendel, Jonas Rauber, Matthias K\xc3\xbcmmerer,\n            Ivan Ustyuzhaninov, Matthias Bethge,\n            ""Accurate, reliable and fast robustness evaluation"",\n            33rd Conference on Neural Information Processing Systems (2019)\n            https://arxiv.org/abs/1907.01003\n    """"""\n\n    def __init__(\n        self,\n        init_attack: Optional[MinimizationAttack] = None,\n        overshoot: float = 1.1,\n        steps: int = 1000,\n        lr: float = 1e-3,\n        lr_decay: float = 0.5,\n        lr_num_decay: int = 20,\n        momentum: float = 0.8,\n        tensorboard: Union[Literal[False], None, str] = False,\n        binary_search_steps: int = 10,\n    ):\n\n        if NUMBA_IMPORT_ERROR is not None:\n            raise NUMBA_IMPORT_ERROR  # pragma: no cover\n\n        self.init_attack = init_attack\n        self.overshoot = overshoot\n        self.steps = steps\n        self.lr = lr\n        self.lr_decay = lr_decay\n        self.lr_num_decay = lr_num_decay\n        self.momentum = momentum\n        self.tensorboard = tensorboard\n        self.binary_search_steps = binary_search_steps\n\n        self._optimizer: Optimizer = self.instantiate_optimizer()\n\n    def run(  # noqa: C901\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[TargetedMisclassification, Misclassification, T],\n        *,\n        starting_points: Optional[ep.Tensor] = None,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        """"""Applies the Brendel & Bethge attack.\n\n        Parameters\n        ----------\n        inputs : Tensor that matches model type\n            The original clean inputs.\n        labels : Integer tensor that matches model type\n            The reference labels for the inputs.\n        criterion : Callable\n            A callable that returns true if the given logits of perturbed\n            inputs should be considered adversarial w.r.t. to the given labels\n            and unperturbed inputs.\n        starting_point : Tensor of same type and shape as inputs\n            Adversarial inputs to use as a starting points, in particular\n            for targeted attacks.\n        """"""\n        raise_if_kwargs(kwargs)\n        del kwargs\n\n        tb = TensorBoard(logdir=self.tensorboard)\n\n        originals, restore_type = ep.astensor_(inputs)\n        del inputs\n\n        criterion_ = get_criterion(criterion)\n        del criterion\n        is_adversarial = get_is_adversarial(criterion_, model)\n\n        if isinstance(criterion_, Misclassification):\n            targeted = False\n            classes = criterion_.labels\n        elif isinstance(criterion_, TargetedMisclassification):\n            targeted = True\n            classes = criterion_.target_classes\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        if starting_points is None:\n            init_attack: MinimizationAttack\n            if self.init_attack is None:\n                init_attack = LinearSearchBlendedUniformNoiseAttack()\n                logging.info(\n                    f""Neither starting_points nor init_attack given. Falling""\n                    f"" back to {init_attack!r} for initialization.""\n                )\n            else:\n                init_attack = self.init_attack\n            # TODO: use call and support all types of attacks (once early_stop is\n            # possible in __call__)\n            starting_points = init_attack.run(model, originals, criterion_)\n\n        best_advs = ep.astensor(starting_points)\n        assert is_adversarial(best_advs).all()\n\n        # perform binary search to find adversarial boundary\n        # TODO: Implement more efficient search with breaking condition\n        N = len(originals)\n        rows = range(N)\n        bounds = model.bounds\n        min_, max_ = bounds\n\n        x0 = originals\n        x0_np_flatten = x0.numpy().reshape((N, -1))\n        x1 = best_advs\n\n        lower_bound = ep.zeros(x0, shape=(N,))\n        upper_bound = ep.ones(x0, shape=(N,))\n\n        for _ in range(self.binary_search_steps):\n            epsilons = (lower_bound + upper_bound) / 2\n            mid_points = self.mid_points(x0, x1, epsilons, bounds)\n            is_advs = is_adversarial(mid_points)\n            lower_bound = ep.where(is_advs, lower_bound, epsilons)\n            upper_bound = ep.where(is_advs, epsilons, upper_bound)\n\n        starting_points = self.mid_points(x0, x1, upper_bound, bounds)\n\n        tb.scalar(""batchsize"", N, 0)\n\n        # function to compute logits_diff and gradient\n        def loss_fun(x):\n            logits = model(x)\n\n            if targeted:\n                c_minimize = best_other_classes(logits, classes)\n                c_maximize = classes\n            else:\n                c_minimize = classes\n                c_maximize = best_other_classes(logits, classes)\n\n            logits_diffs = logits[rows, c_minimize] - logits[rows, c_maximize]\n            assert logits_diffs.shape == (N,)\n\n            return logits_diffs.sum(), logits_diffs\n\n        value_and_grad = ep.value_and_grad_fn(x0, loss_fun, has_aux=True)\n\n        def logits_diff_and_grads(x) -> Tuple[Any, Any]:\n            _, logits_diffs, boundary = value_and_grad(x)\n            return logits_diffs.numpy(), boundary.numpy().copy()\n\n        x = starting_points\n        lrs = self.lr * np.ones(N)\n        lr_reduction_interval = min(1, int(self.steps / self.lr_num_decay))\n        converged = np.zeros(N, dtype=np.bool)\n        rate_normalization = np.prod(x.shape) * (max_ - min_)\n        original_shape = x.shape\n        _best_advs = best_advs.numpy()\n\n        for step in range(1, self.steps + 1):\n            if converged.all():\n                break  # pragma: no cover\n\n            # get logits and local boundary geometry\n            # TODO: only perform forward pass on non-converged samples\n            logits_diffs, _boundary = logits_diff_and_grads(x)\n\n            # record optimal adversarials\n            distances = self.norms(originals - x)\n            source_norms = self.norms(originals - best_advs)\n\n            closer = distances < source_norms\n            is_advs = logits_diffs < 0\n            closer = closer.logical_and(ep.from_numpy(x, is_advs))\n\n            x_np_flatten = x.numpy().reshape((N, -1))\n\n            if closer.any():\n                _best_advs = best_advs.numpy().copy()\n                _closer = closer.numpy().flatten()\n                for idx in np.arange(N)[_closer]:\n                    _best_advs[idx] = x_np_flatten[idx].reshape(original_shape[1:])\n\n            best_advs = ep.from_numpy(x, _best_advs)\n\n            # denoise estimate of boundary using a short history of the boundary\n            if step == 1:\n                boundary = _boundary\n            else:\n                boundary = (1 - self.momentum) * _boundary + self.momentum * boundary\n\n            # learning rate adaptation\n            if (step + 1) % lr_reduction_interval == 0:\n                lrs *= self.lr_decay\n\n            # compute optimal step within trust region depending on metric\n            x = x.reshape((N, -1))\n            region = lrs * rate_normalization\n\n            # we aim to slight overshoot over the boundary to stay within the adversarial region\n            corr_logits_diffs = np.where(\n                -logits_diffs < 0,\n                -self.overshoot * logits_diffs,\n                -(2 - self.overshoot) * logits_diffs,\n            )\n\n            # employ solver to find optimal step within trust region\n            # for each sample\n            deltas, k = [], 0\n\n            for sample in range(N):\n                if converged[sample]:\n                    # don\'t perform optimisation on converged samples\n                    deltas.append(\n                        np.zeros_like(x0_np_flatten[sample])\n                    )  # pragma: no cover\n                else:\n                    _x0 = x0_np_flatten[sample]\n                    _x = x_np_flatten[sample]\n                    _b = boundary[k].flatten()\n                    _c = corr_logits_diffs[k]\n                    r = region[sample]\n\n                    delta = self._optimizer.solve(  # type: ignore\n                        _x0, _x, _b, bounds[0], bounds[1], _c, r\n                    )\n                    deltas.append(delta)\n\n                    k += 1  # idx of masked sample\n\n            deltas = np.stack(deltas)\n            deltas = ep.from_numpy(x, deltas.astype(np.float32))  # type: ignore\n\n            # add step to current perturbation\n            x = (x + ep.astensor(deltas)).reshape(original_shape)\n\n            tb.probability(""converged"", converged, step)\n            tb.histogram(""norms"", source_norms, step)\n            tb.histogram(""candidates/distances"", distances, step)\n\n        tb.close()\n\n        return restore_type(best_advs)\n\n    @abstractmethod\n    def instantiate_optimizer(self) -> Optimizer:\n        raise NotImplementedError\n\n    @abstractmethod\n    def norms(self, x: ep.Tensor) -> ep.Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def mid_points(\n        self,\n        x0: ep.Tensor,\n        x1: ep.Tensor,\n        epsilons: ep.Tensor,\n        bounds: Tuple[float, float],\n    ) -> ep.Tensor:\n        raise NotImplementedError\n\n\ndef best_other_classes(logits: ep.Tensor, exclude: ep.Tensor) -> ep.Tensor:\n    other_logits = logits - ep.onehot_like(logits, exclude, value=np.inf)\n    return other_logits.argmax(axis=-1)\n\n\nclass L2BrendelBethgeAttack(BrendelBethgeAttack):\n    """"""L2 variant of the Brendel & Bethge adversarial attack. [#Bren19]_\n    This is a powerful gradient-based adversarial attack that follows the\n    adversarial boundary (the boundary between the space of adversarial and\n    non-adversarial images as defined by the adversarial criterion) to find\n    the minimum distance to the clean image.\n\n    This is the reference implementation of the Brendel & Bethge attack.\n\n    References:\n        .. [#Bren19] Wieland Brendel, Jonas Rauber, Matthias K\xc3\xbcmmerer,\n           Ivan Ustyuzhaninov, Matthias Bethge,\n           ""Accurate, reliable and fast robustness evaluation"",\n           33rd Conference on Neural Information Processing Systems (2019)\n           https://arxiv.org/abs/1907.01003\n   """"""\n\n    distance = l2\n\n    def instantiate_optimizer(self):\n        if len(L2Optimizer._ctor.signatures) == 0:\n            # optimiser is not yet compiled, give user a warning/notice\n            warnings.warn(\n                ""At the first initialisation the optimizer needs to be compiled. This may take between 20 to 60 seconds.""\n            )\n\n        return L2Optimizer()\n\n    def norms(self, x: ep.Tensor) -> ep.Tensor:\n        return flatten(x).norms.l2(axis=-1)\n\n    def mid_points(\n        self, x0: ep.Tensor, x1: ep.Tensor, epsilons: ep.Tensor, bounds\n    ) -> ep.Tensor:\n        # returns a point between x0 and x1 where\n        # epsilon = 0 returns x0 and epsilon = 1\n        # returns x1\n\n        # get epsilons in right shape for broadcasting\n        epsilons = epsilons.reshape(epsilons.shape + (1,) * (x0.ndim - 1))\n        return epsilons * x1 + (1 - epsilons) * x0\n\n\nclass LinfinityBrendelBethgeAttack(BrendelBethgeAttack):\n    """"""L-infinity variant of the Brendel & Bethge adversarial attack. [#Bren19]_\n    This is a powerful gradient-based adversarial attack that follows the\n    adversarial boundary (the boundary between the space of adversarial and\n    non-adversarial images as defined by the adversarial criterion) to find\n    the minimum distance to the clean image.\n\n    This is the reference implementation of the Brendel & Bethge attack.\n\n    References:\n        .. [#Bren19] Wieland Brendel, Jonas Rauber, Matthias K\xc3\xbcmmerer,\n           Ivan Ustyuzhaninov, Matthias Bethge,\n           ""Accurate, reliable and fast robustness evaluation"",\n           33rd Conference on Neural Information Processing Systems (2019)\n           https://arxiv.org/abs/1907.01003\n   """"""\n\n    distance = linf\n\n    def instantiate_optimizer(self):\n        return LinfOptimizer()\n\n    def norms(self, x: ep.Tensor) -> ep.Tensor:\n        return flatten(x).norms.linf(axis=-1)\n\n    def mid_points(\n        self,\n        x0: ep.Tensor,\n        x1: ep.Tensor,\n        epsilons: ep.Tensor,\n        bounds: Tuple[float, float],\n    ):\n        # returns a point between x0 and x1 where\n        # epsilon = 0 returns x0 and epsilon = 1\n        delta = x1 - x0\n        min_, max_ = bounds\n        s = max_ - min_\n        # get epsilons in right shape for broadcasting\n        epsilons = epsilons.reshape(epsilons.shape + (1,) * (x0.ndim - 1))\n\n        clipped_delta = ep.where(delta < -epsilons * s, -epsilons * s, delta)\n        clipped_delta = ep.where(\n            clipped_delta > epsilons * s, epsilons * s, clipped_delta\n        )\n        return x0 + clipped_delta\n\n\nclass L1BrendelBethgeAttack(BrendelBethgeAttack):\n    """"""L1 variant of the Brendel & Bethge adversarial attack. [#Bren19]_\n    This is a powerful gradient-based adversarial attack that follows the\n    adversarial boundary (the boundary between the space of adversarial and\n    non-adversarial images as defined by the adversarial criterion) to find\n    the minimum distance to the clean image.\n\n    This is the reference implementation of the Brendel & Bethge attack.\n\n    References:\n        .. [#Bren19] Wieland Brendel, Jonas Rauber, Matthias K\xc3\xbcmmerer,\n           Ivan Ustyuzhaninov, Matthias Bethge,\n           ""Accurate, reliable and fast robustness evaluation"",\n           33rd Conference on Neural Information Processing Systems (2019)\n           https://arxiv.org/abs/1907.01003\n   """"""\n\n    distance = l1\n\n    def instantiate_optimizer(self):\n        return L1Optimizer()\n\n    def norms(self, x: ep.Tensor) -> ep.Tensor:\n        return flatten(x).norms.l1(axis=-1)\n\n    def mid_points(\n        self,\n        x0: ep.Tensor,\n        x1: ep.Tensor,\n        epsilons: ep.Tensor,\n        bounds: Tuple[float, float],\n    ) -> ep.Tensor:\n        # returns a point between x0 and x1 where\n        # epsilon = 0 returns x0 and epsilon = 1\n        # returns x1\n\n        # get epsilons in right shape for broadcasting\n        epsilons = epsilons.reshape(epsilons.shape + (1,) * (x0.ndim - 1))\n\n        threshold = (bounds[1] - bounds[0]) * (1 - epsilons)\n        mask = (x1 - x0).abs() > threshold\n        new_x = ep.where(\n            mask, x0 + (x1 - x0).sign() * ((x1 - x0).abs() - threshold), x0\n        )\n        return new_x\n\n\nclass L0BrendelBethgeAttack(BrendelBethgeAttack):\n    """"""L0 variant of the Brendel & Bethge adversarial attack. [#Bren19]_\n    This is a powerful gradient-based adversarial attack that follows the\n    adversarial boundary (the boundary between the space of adversarial and\n    non-adversarial images as defined by the adversarial criterion) to find\n    the minimum distance to the clean image.\n\n    This is the reference implementation of the Brendel & Bethge attack.\n\n    References:\n        .. [#Bren19] Wieland Brendel, Jonas Rauber, Matthias K\xc3\xbcmmerer,\n           Ivan Ustyuzhaninov, Matthias Bethge,\n           ""Accurate, reliable and fast robustness evaluation"",\n           33rd Conference on Neural Information Processing Systems (2019)\n           https://arxiv.org/abs/1907.01003\n   """"""\n\n    distance = l0\n\n    def instantiate_optimizer(self):\n        return L0Optimizer()\n\n    def norms(self, x: ep.Tensor) -> ep.Tensor:\n        return (flatten(x).abs() > 1e-4).sum(axis=-1)\n\n    def mid_points(\n        self,\n        x0: ep.Tensor,\n        x1: ep.Tensor,\n        epsilons: ep.Tensor,\n        bounds: Tuple[float, float],\n    ):\n        # returns a point between x0 and x1 where\n        # epsilon = 0 returns x0 and epsilon = 1\n        # returns x1\n\n        # get epsilons in right shape for broadcasting\n        epsilons = epsilons.reshape(epsilons.shape + (1,) * (x0.ndim - 1))\n\n        threshold = (bounds[1] - bounds[0]) * epsilons\n        mask = ep.abs(x1 - x0) < threshold\n        new_x = ep.where(mask, x1, x0)\n        return new_x\n\n\n@jitclass(spec=[])\nclass BFGSB(object):\n    def __init__(self):\n        pass\n\n    def solve(\n        self, fun_and_jac, q0, bounds, args, ftol=1e-10, pgtol=-1e-5, maxiter=None\n    ):\n        N = q0.shape[0]\n\n        if maxiter is None:\n            maxiter = N * 200\n\n        l = bounds[:, 0]  # noqa: E741\n        u = bounds[:, 1]\n\n        func_calls = 0\n\n        old_fval, gfk = fun_and_jac(q0, *args)\n        func_calls += 1\n\n        k = 0\n        Hk = np.eye(N)\n\n        # Sets the initial step guess to dx ~ 1\n        qk = q0\n        old_old_fval = old_fval + np.linalg.norm(gfk) / 2\n\n        # gnorm = np.amax(np.abs(gfk))\n        _gfk = gfk\n\n        # Compare with implementation BFGS-B implementation\n        # in https://github.com/andrewhooker/PopED/blob/master/R/bfgsb_min.R\n\n        while k < maxiter:\n            # check if projected gradient is still large enough\n            pg_norm = 0\n            for v in range(N):\n                if _gfk[v] < 0:\n                    gv = max(qk[v] - u[v], _gfk[v])\n                else:\n                    gv = min(qk[v] - l[v], _gfk[v])\n\n                if pg_norm < np.abs(gv):\n                    pg_norm = np.abs(gv)\n\n            if pg_norm < pgtol:\n                break\n\n            # get cauchy point\n            x_cp = self._cauchy_point(qk, l, u, _gfk.copy(), Hk)\n            qk1 = self._subspace_min(qk, l, u, x_cp, _gfk.copy(), Hk)\n            pk = qk1 - qk\n\n            (\n                alpha_k,\n                fc,\n                gc,\n                old_fval,\n                old_old_fval,\n                gfkp1,\n                fnev,\n            ) = self._line_search_wolfe(\n                fun_and_jac, qk, pk, _gfk, old_fval, old_old_fval, l, u, args\n            )\n            func_calls += fnev\n\n            if alpha_k is None:\n                break\n\n            if np.abs(old_fval - old_old_fval) <= (ftol + ftol * np.abs(old_fval)):\n                break\n\n            qkp1 = self._project(qk + alpha_k * pk, l, u)\n\n            if gfkp1 is None:\n                _, gfkp1 = fun_and_jac(qkp1, *args)\n\n            sk = qkp1 - qk\n            qk = qkp1\n\n            yk = np.zeros_like(qk)\n            for k3 in range(N):\n                yk[k3] = gfkp1[k3] - _gfk[k3]\n\n                if np.abs(yk[k3]) < 1e-4:\n                    yk[k3] = -1e-4\n\n            _gfk = gfkp1\n\n            k += 1\n\n            # update inverse Hessian matrix\n            Hk_sk = Hk.dot(sk)\n\n            sk_yk = 0\n            sk_Hk_sk = 0\n            for v in range(N):\n                sk_yk += sk[v] * yk[v]\n                sk_Hk_sk += sk[v] * Hk_sk[v]\n\n            if np.abs(sk_yk) >= 1e-8:\n                rhok = 1.0 / sk_yk\n            else:\n                rhok = 100000.0\n\n            if np.abs(sk_Hk_sk) >= 1e-8:\n                rsk_Hk_sk = 1.0 / sk_Hk_sk\n            else:\n                rsk_Hk_sk = 100000.0\n\n            for v in range(N):\n                for w in range(N):\n                    Hk[v, w] += yk[v] * yk[w] * rhok - Hk_sk[v] * Hk_sk[w] * rsk_Hk_sk\n\n        return qk\n\n    def _cauchy_point(self, x, l, u, g, B):  # noqa: E741\n        # finds the cauchy point for q(x)=x\'Gx+x\'d s$t. l<=x<=u\n        # g=G*x+d #gradient of q(x)\n        # converted from r-code: https://github.com/andrewhooker/PopED/blob/master/R/cauchy_point.R\n        n = x.shape[0]\n        t = np.zeros_like(x)\n        d = np.zeros_like(x)\n\n        for i in range(n):\n            if g[i] < 0:\n                t[i] = (x[i] - u[i]) / g[i]\n            elif g[i] > 0:\n                t[i] = (x[i] - l[i]) / g[i]\n            elif g[i] == 0:\n                t[i] = np.inf\n\n            if t[i] == 0:\n                d[i] = 0\n            else:\n                d[i] = -g[i]\n\n        ts = t.copy()\n        ts = ts[ts != 0]\n        ts = np.sort(ts)\n\n        df = g.dot(d)\n        d2f = d.dot(B.dot(d))\n\n        if d2f < 1e-10:\n            return x\n\n        dt_min = -df / d2f\n        t_old = 0\n        i = 0\n        z = np.zeros_like(x)\n\n        while i < ts.shape[0] and dt_min >= (ts[i] - t_old):\n            ind = ts[i] < t\n            d[~ind] = 0\n            z = z + (ts[i] - t_old) * d\n            df = g.dot(d) + d.dot(B.dot(z))\n            d2f = d.dot(B.dot(d))\n            dt_min = df / (d2f + 1e-8)\n            t_old = ts[i]\n            i += 1\n\n        dt_min = max(dt_min, 0)\n        t_old = t_old + dt_min\n        x_cp = x - t_old * g\n        temp = x - t * g\n        x_cp[t_old > t] = temp[t_old > t]\n\n        return x_cp\n\n    def _subspace_min(self, x, l, u, x_cp, d, G):  # noqa: E741\n        # converted from r-code: https://github.com/andrewhooker/PopED/blob/master/R/subspace_min.R\n        n = x.shape[0]\n        Z = np.eye(n)\n        fixed = (x_cp <= l + 1e-8) + (x_cp >= u - 1e8)\n\n        if np.all(fixed):\n            x = x_cp\n            return x\n\n        Z = Z[:, ~fixed]\n        rgc = Z.T.dot(d + G.dot(x_cp - x))\n        rB = Z.T.dot(G.dot(Z)) + 1e-10 * np.eye(Z.shape[1])\n        d[~fixed] = np.linalg.solve(rB, rgc)\n        d[~fixed] = -d[~fixed]\n        alpha = 1\n        temp1 = alpha\n\n        for i in np.arange(n)[~fixed]:\n            dk = d[i]\n            if dk < 0:\n                temp2 = l[i] - x_cp[i]\n                if temp2 >= 0:\n                    temp1 = 0\n                else:\n                    if dk * alpha < temp2:\n                        temp1 = temp2 / dk\n                    else:\n                        temp2 = u[i] - x_cp[i]\n            else:\n                temp2 = u[i] - x_cp[i]\n                if temp1 <= 0:\n                    temp1 = 0\n                else:\n                    if dk * alpha > temp2:\n                        temp1 = temp2 / dk\n\n            alpha = min(temp1, alpha)\n\n        return x_cp + alpha * Z.dot(d[~fixed])\n\n    def _project(self, q, l, u):  # noqa: E741\n        N = q.shape[0]\n        for k in range(N):\n            if q[k] < l[k]:\n                q[k] = l[k]\n            elif q[k] > u[k]:\n                q[k] = u[k]\n\n        return q\n\n    def _line_search_armijo(\n        self,\n        fun_and_jac,\n        pt,\n        dpt,\n        func_calls,\n        m,\n        gk,\n        l,  # noqa: E741\n        u,\n        x0,\n        x,\n        b,\n        min_,\n        max_,\n        c,\n        r,\n    ):\n        ls_rho = 0.6\n        ls_c = 1e-4\n        ls_alpha = 1\n\n        t = m * ls_c\n\n        for k2 in range(100):\n            ls_pt = self._project(pt + ls_alpha * dpt, l, u)\n\n            gkp1, dgkp1 = fun_and_jac(ls_pt, x0, x, b, min_, max_, c, r)\n            func_calls += 1\n\n            if gk - gkp1 >= ls_alpha * t:\n                break\n            else:\n                ls_alpha *= ls_rho\n\n        return ls_alpha, ls_pt, gkp1, dgkp1, func_calls\n\n    def _line_search_wolfe(  # noqa: C901\n        self,\n        fun_and_jac,\n        xk,\n        pk,\n        gfk,\n        old_fval,\n        old_old_fval,\n        l,  # noqa: #E741\n        u,\n        args,\n    ):\n        """"""Find alpha that satisfies strong Wolfe conditions.\n        Uses the line search algorithm to enforce strong Wolfe conditions\n        Wright and Nocedal, \'Numerical Optimization\', 1999, pg. 59-60\n        For the zoom phase it uses an algorithm by\n        Outputs: (alpha0, gc, fc)\n        """"""\n        c1 = 1e-4\n        c2 = 0.9\n        N = xk.shape[0]\n        _ls_fc = 0\n        _ls_ingfk = None\n\n        alpha0 = 0\n        phi0 = old_fval\n\n        derphi0 = 0\n        for v in range(N):\n            derphi0 += gfk[v] * pk[v]\n\n        if derphi0 == 0:\n            derphi0 = 1e-8\n        elif np.abs(derphi0) < 1e-8:\n            derphi0 = np.sign(derphi0) * 1e-8\n\n        alpha1 = min(1.0, 1.01 * 2 * (phi0 - old_old_fval) / derphi0)\n\n        if alpha1 == 0:\n            # This shouldn\'t happen. Perhaps the increment has slipped below\n            # machine precision?  For now, set the return variables skip the\n            # useless while loop, and raise warnflag=2 due to possible imprecision.\n            # print(""Slipped below machine precision."")\n            alpha_star = None\n            fval_star = old_fval\n            old_fval = old_old_fval\n            fprime_star = None\n\n        _xkp1 = self._project(xk + alpha1 * pk, l, u)\n        phi_a1, _ls_ingfk = fun_and_jac(_xkp1, *args)\n        _ls_fc += 1\n        # derphi_a1 = phiprime(alpha1)  evaluated below\n\n        phi_a0 = phi0\n        derphi_a0 = derphi0\n\n        i = 1\n        maxiter = 10\n        while 1:  # bracketing phase\n            # print(""   (ls) in while loop: "", alpha1, alpha0)\n            if alpha1 == 0:\n                break\n            if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or (\n                (phi_a1 >= phi_a0) and (i > 1)\n            ):\n                # inlining zoom for performance reasons\n                #                 alpha0, alpha1, phi_a0, phi_a1, derphi_a0, phi0, derphi0, pk, xk\n                # zoom signature: (a_lo, a_hi, phi_lo, phi_hi, derphi_lo, phi0, derphi0, pk, xk)\n                # INLINE START\n                k = 0\n                delta1 = 0.2  # cubic interpolant check\n                delta2 = 0.1  # quadratic interpolant check\n                phi_rec = phi0\n                a_rec = 0\n                a_hi = alpha1\n                a_lo = alpha0\n                phi_lo = phi_a0\n                phi_hi = phi_a1\n                derphi_lo = derphi_a0\n                while 1:\n                    # interpolate to find a trial step length between a_lo and a_hi\n                    # Need to choose interpolation here.  Use cubic interpolation and then if the\n                    #  result is within delta * dalpha or outside of the interval bounded by a_lo or a_hi\n                    #  then use quadratic interpolation, if the result is still too close, then use bisection\n\n                    dalpha = a_hi - a_lo\n                    if dalpha < 0:\n                        a, b = a_hi, a_lo\n                    else:\n                        a, b = a_lo, a_hi\n\n                    # minimizer of cubic interpolant\n                    #    (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n                    #      if the result is too close to the end points (or out of the interval)\n                    #         then use quadratic interpolation with phi_lo, derphi_lo and phi_hi\n                    #      if the result is stil too close to the end points (or out of the interval)\n                    #         then use bisection\n\n                    if k > 0:\n                        cchk = delta1 * dalpha\n                        a_j = self._cubicmin(\n                            a_lo, phi_lo, derphi_lo, a_hi, phi_hi, a_rec, phi_rec\n                        )\n                    if (\n                        (k == 0)\n                        or (a_j is None)\n                        or (a_j > b - cchk)\n                        or (a_j < a + cchk)\n                    ):\n                        qchk = delta2 * dalpha\n                        a_j = self._quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n                        if (a_j is None) or (a_j > b - qchk) or (a_j < a + qchk):\n                            a_j = a_lo + 0.5 * dalpha\n\n                    # Check new value of a_j\n                    _xkp1 = self._project(xk + a_j * pk, l, u)\n                    # if _xkp1[1] < 0:\n                    #     _xkp1[1] = 0\n                    phi_aj, _ls_ingfk = fun_and_jac(_xkp1, *args)\n\n                    derphi_aj = 0\n                    for v in range(N):\n                        derphi_aj += _ls_ingfk[v] * pk[v]\n\n                    if (phi_aj > phi0 + c1 * a_j * derphi0) or (phi_aj >= phi_lo):\n                        phi_rec = phi_hi\n                        a_rec = a_hi\n                        a_hi = a_j\n                        phi_hi = phi_aj\n                    else:\n                        if abs(derphi_aj) <= -c2 * derphi0:\n                            a_star = a_j\n                            val_star = phi_aj\n                            valprime_star = _ls_ingfk\n                            break\n                        if derphi_aj * (a_hi - a_lo) >= 0:\n                            phi_rec = phi_hi\n                            a_rec = a_hi\n                            a_hi = a_lo\n                            phi_hi = phi_lo\n                        else:\n                            phi_rec = phi_lo\n                            a_rec = a_lo\n                        a_lo = a_j\n                        phi_lo = phi_aj\n                        derphi_lo = derphi_aj\n                    k += 1\n                    if k > maxiter:\n                        a_star = a_j\n                        val_star = phi_aj\n                        valprime_star = None\n                        break\n\n                alpha_star = a_star\n                fval_star = val_star\n                fprime_star = valprime_star\n                fnev = k\n                ## INLINE END\n\n                _ls_fc += fnev\n                break\n\n            i += 1\n            if i > maxiter:\n                break\n\n            _xkp1 = self._project(xk + alpha1 * pk, l, u)\n            _, _ls_ingfk = fun_and_jac(_xkp1, *args)\n            derphi_a1 = 0\n            for v in range(N):\n                derphi_a1 += _ls_ingfk[v] * pk[v]\n            _ls_fc += 1\n            if abs(derphi_a1) <= -c2 * derphi0:\n                alpha_star = alpha1\n                fval_star = phi_a1\n                fprime_star = _ls_ingfk\n                break\n\n            if derphi_a1 >= 0:\n                # alpha_star, fval_star, fprime_star, fnev, _ls_ingfk = _zoom(\n                #     alpha1, alpha0, phi_a1, phi_a0, derphi_a1, phi0, derphi0, pk, xk\n                # )\n                #\n                # INLINE START\n                maxiter = 10\n                k = 0\n                delta1 = 0.2  # cubic interpolant check\n                delta2 = 0.1  # quadratic interpolant check\n                phi_rec = phi0\n                a_rec = 0\n                a_hi = alpha0\n                a_lo = alpha1\n                phi_lo = phi_a1\n                phi_hi = phi_a0\n                derphi_lo = derphi_a1\n                while 1:\n                    # interpolate to find a trial step length between a_lo and a_hi\n                    # Need to choose interpolation here.  Use cubic interpolation and then if the\n                    #  result is within delta * dalpha or outside of the interval bounded by a_lo or a_hi\n                    #  then use quadratic interpolation, if the result is still too close, then use bisection\n\n                    dalpha = a_hi - a_lo\n                    if dalpha < 0:\n                        a, b = a_hi, a_lo\n                    else:\n                        a, b = a_lo, a_hi\n\n                    # minimizer of cubic interpolant\n                    #    (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n                    #      if the result is too close to the end points (or out of the interval)\n                    #         then use quadratic interpolation with phi_lo, derphi_lo and phi_hi\n                    #      if the result is stil too close to the end points (or out of the interval)\n                    #         then use bisection\n\n                    if k > 0:\n                        cchk = delta1 * dalpha\n                        a_j = self._cubicmin(\n                            a_lo, phi_lo, derphi_lo, a_hi, phi_hi, a_rec, phi_rec\n                        )\n                    if (\n                        (k == 0)\n                        or (a_j is None)\n                        or (a_j > b - cchk)\n                        or (a_j < a + cchk)\n                    ):\n                        qchk = delta2 * dalpha\n                        a_j = self._quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n                        if (a_j is None) or (a_j > b - qchk) or (a_j < a + qchk):\n                            a_j = a_lo + 0.5 * dalpha\n\n                    # Check new value of a_j\n                    _xkp1 = self._project(xk + a_j * pk, l, u)\n                    phi_aj, _ls_ingfk = fun_and_jac(_xkp1, *args)\n                    derphi_aj = 0\n                    for v in range(N):\n                        derphi_aj += _ls_ingfk[v] * pk[v]\n                    if (phi_aj > phi0 + c1 * a_j * derphi0) or (phi_aj >= phi_lo):\n                        phi_rec = phi_hi\n                        a_rec = a_hi\n                        a_hi = a_j\n                        phi_hi = phi_aj\n                    else:\n                        if abs(derphi_aj) <= -c2 * derphi0:\n                            a_star = a_j\n                            val_star = phi_aj\n                            valprime_star = _ls_ingfk\n                            break\n                        if derphi_aj * (a_hi - a_lo) >= 0:\n                            phi_rec = phi_hi\n                            a_rec = a_hi\n                            a_hi = a_lo\n                            phi_hi = phi_lo\n                        else:\n                            phi_rec = phi_lo\n                            a_rec = a_lo\n                        a_lo = a_j\n                        phi_lo = phi_aj\n                        derphi_lo = derphi_aj\n                    k += 1\n                    if k > maxiter:\n                        a_star = a_j\n                        val_star = phi_aj\n                        valprime_star = None\n                        break\n\n                alpha_star = a_star\n                fval_star = val_star\n                fprime_star = valprime_star\n                fnev = k\n                ## INLINE END\n\n                _ls_fc += fnev\n                break\n\n            alpha2 = 2 * alpha1  # increase by factor of two on each iteration\n            i = i + 1\n            alpha0 = alpha1\n            alpha1 = alpha2\n            phi_a0 = phi_a1\n            _xkp1 = self._project(xk + alpha1 * pk, l, u)\n            phi_a1, _ls_ingfk = fun_and_jac(_xkp1, *args)\n            _ls_fc += 1\n            derphi_a0 = derphi_a1\n\n            # stopping test if lower function not found\n            if i > maxiter:\n                alpha_star = alpha1\n                fval_star = phi_a1\n                fprime_star = None\n                break\n\n        return alpha_star, _ls_fc, _ls_fc, fval_star, old_fval, fprime_star, _ls_fc\n\n    def _cubicmin(self, a, fa, fpa, b, fb, c, fc):\n        # finds the minimizer for a cubic polynomial that goes through the\n        #  points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.\n        #\n        # if no minimizer can be found return None\n        #\n        # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n\n        C = fpa\n        db = b - a\n        dc = c - a\n        if (db == 0) or (dc == 0) or (b == c):\n            return None\n        denom = (db * dc) ** 2 * (db - dc)\n        A = dc ** 2 * (fb - fa - C * db) - db ** 2 * (fc - fa - C * dc)\n        B = -(dc ** 3) * (fb - fa - C * db) + db ** 3 * (fc - fa - C * dc)\n\n        A /= denom\n        B /= denom\n        radical = B * B - 3 * A * C\n        if radical < 0:\n            return None\n        if A == 0:\n            return None\n        xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n        return xmin\n\n    def _quadmin(self, a, fa, fpa, b, fb):\n        # finds the minimizer for a quadratic polynomial that goes through\n        #  the points (a,fa), (b,fb) with derivative at a of fpa\n        # f(x) = B*(x-a)^2 + C*(x-a) + D\n        D = fa\n        C = fpa\n        db = b - a * 1.0\n        if db == 0:\n            return None\n        B = (fb - D - C * db) / (db * db)\n        if B <= 0:\n            return None\n        xmin = a - C / (2.0 * B)\n        return xmin\n\n\nif NUMBA_IMPORT_ERROR is None:\n    spec = [(""bfgsb"", BFGSB.class_type.instance_type)]  # type: ignore\nelse:\n    spec = []  # pragma: no cover\n\n\n@jitclass(spec=spec)\nclass L2Optimizer(Optimizer):\n    def optimize_distance_s_t_boundary_and_trustregion(  # noqa: C901\n        self, x0, x, b, min_, max_, c, r\n    ):\n        """""" Solves the L2 trust region problem\n\n        min ||x0 - x - delta||_2 s.t. b^top delta = c\n                                    & ell <= x + delta <= u\n                                    & ||delta||_2 <= r\n\n        This is a specialised solver that does not use the generic BFGS-B solver.\n        Instead, this active-set solver computes the active set of indices (those that\n        do not hit the bounds) and then computes that optimal step size in the direction\n        of the boundary and the direction of the original sample over the active indices.\n\n        Parameters\n        ----------\n        x0 : `numpy.ndarray`\n            The original image against which we minimize the perturbation\n            (flattened).\n        x : `numpy.ndarray`\n            The current perturbation (flattened).\n        b : `numpy.ndarray`\n            Normal vector of the local decision boundary (flattened).\n        min_ : float\n            Lower bound on the pixel values.\n        max_ : float\n            Upper bound on the pixel values.\n        c : float\n            Logit difference between the ground truth class of x0 and the\n            leading class different from the ground truth.\n        r : float\n            Size of the trust region.\n        """"""\n        N = x0.shape[0]\n        clamp_c = 0\n        clamp_norm = 0\n        ck = c\n        rk = r\n        masked_values = 0\n\n        mask = np.zeros(N, dtype=np.uint8)\n        delta = np.empty_like(x0)\n        dx = x0 - x\n\n        for k in range(20):\n            # inner optimization that solves subproblem\n            bnorm = 1e-8\n            bdotDx = 0\n\n            for i in range(N):\n                if mask[i] == 0:\n                    bnorm += b[i] * b[i]\n                    bdotDx += b[i] * dx[i]\n\n            bdotDx = bdotDx / bnorm\n            ck_bnorm = ck / bnorm\n            b_scale = -bdotDx + ck / bnorm\n            new_masked_values = 0\n            delta_norm = 0\n            descent_norm = 0\n            boundary_step_norm = 0\n\n            # make optimal step towards boundary AND minimum\n            for i in range(N):\n                if mask[i] == 0:\n                    delta[i] = dx[i] + b[i] * b_scale\n                    boundary_step_norm = (\n                        boundary_step_norm + b[i] * ck_bnorm * b[i] * ck_bnorm\n                    )\n                    delta_norm = delta_norm + delta[i] * delta[i]\n                    descent_norm = descent_norm + (dx[i] - b[i] * bdotDx) * (\n                        dx[i] - b[i] * bdotDx\n                    )\n\n            # check of step to boundary is already larger than trust region\n            if boundary_step_norm > rk * rk:\n                for i in range(N):\n                    if mask[i] == 0:\n                        delta[i] = b[i] * ck_bnorm\n            else:\n                # check if combined step to large and correct step to minimum if necessary\n                if delta_norm > rk * rk:\n                    region_correct = np.sqrt(rk * rk - boundary_step_norm)\n                    region_correct = region_correct / (np.sqrt(descent_norm) + 1e-8)\n                    b_scale = -region_correct * bdotDx + ck / bnorm\n\n                    for i in range(N):\n                        if mask[i] == 0:\n                            delta[i] = region_correct * dx[i] + b[i] * b_scale\n\n            for i in range(N):\n                if mask[i] == 0:\n                    if x[i] + delta[i] <= min_:\n                        mask[i] = 1\n                        delta[i] = min_ - x[i]\n                        new_masked_values = new_masked_values + 1\n                        clamp_norm = clamp_norm + delta[i] * delta[i]\n                        clamp_c = clamp_c + b[i] * delta[i]\n\n                    if x[i] + delta[i] >= max_:\n                        mask[i] = 1\n                        delta[i] = max_ - x[i]\n                        new_masked_values = new_masked_values + 1\n                        clamp_norm = clamp_norm + delta[i] * delta[i]\n                        clamp_c = clamp_c + b[i] * delta[i]\n\n            # should no additional variable get out of bounds, stop optimization\n            if new_masked_values == 0:\n                break\n\n            masked_values = masked_values + new_masked_values\n\n            if clamp_norm < r * r:\n                rk = np.sqrt(r * r - clamp_norm)\n            else:\n                rk = 0\n\n            ck = c - clamp_c\n\n            if masked_values == N:\n                break\n\n        return delta\n\n    def fun_and_jac(self, params, x0, x, b, min_, max_, c, r):\n        # we need to compute the loss function\n        # g = distance + mu * (norm_d - r ** 2) + lam * (b_dot_d - c)\n        # and its derivative d g / d lam and d g / d mu\n        lam, mu = params\n\n        N = x0.shape[0]\n\n        g = 0\n        d_g_d_lam = 0\n        d_g_d_mu = 0\n\n        distance = 0\n        b_dot_d = 0\n        d_norm = 0\n\n        t = 1 / (2 * mu + 2)\n\n        for n in range(N):\n            dx = x0[n] - x[n]\n            bn = b[n]\n            xn = x[n]\n\n            d = (2 * dx - lam * bn) * t\n\n            if d + xn > max_:\n                d = max_ - xn\n            elif d + xn < min_:\n                d = min_ - xn\n            else:\n                prefac = 2 * (d - dx) + 2 * mu * d + lam * bn\n                d_g_d_lam -= prefac * bn * t\n                d_g_d_mu -= prefac * 2 * d * t\n\n            distance += (d - dx) ** 2\n            b_dot_d += bn * d\n            d_norm += d ** 2\n\n            g += (dx - d) ** 2 + mu * d ** 2 + lam * bn * d\n            d_g_d_lam += bn * d\n            d_g_d_mu += d ** 2\n\n        g += -mu * r ** 2 - lam * c\n        d_g_d_lam -= c\n        d_g_d_mu -= r ** 2\n\n        return -g, -np.array([d_g_d_lam, d_g_d_mu])\n\n    def _get_final_delta(self, lam, mu, x0, x, b, min_, max_, c, r, touchup=True):\n        delta = np.empty_like(x0)\n        N = x0.shape[0]\n\n        t = 1 / (2 * mu + 2)\n\n        for n in range(N):\n            d = (2 * (x0[n] - x[n]) - lam * b[n]) * t\n\n            if d + x[n] > max_:\n                d = max_ - x[n]\n            elif d + x[n] < min_:\n                d = min_ - x[n]\n\n            delta[n] = d\n\n        return delta\n\n    def _distance(self, x0, x):\n        return np.linalg.norm(x0 - x) ** 2\n\n\n@jitclass(spec=spec)\nclass L1Optimizer(Optimizer):\n    def fun_and_jac(self, params, x0, x, b, min_, max_, c, r):\n        lam, mu = params\n        # arg min_delta ||delta - dx||_1 + lam * b^T delta + mu * ||delta||_2^2  s.t.  min <= delta + x <= max\n        N = x0.shape[0]\n\n        g = 0\n        d_g_d_lam = 0\n        d_g_d_mu = 0\n\n        if mu > 0:\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                t = 1 / (2 * mu)\n                u = -lam * bn * t - dx\n\n                if np.abs(u) - t < 0:\n                    # value and grad = 0\n                    d = dx\n                else:\n                    d = np.sign(u) * (np.abs(u) - t) + dx\n\n                    if d + x[n] < min_:\n                        d = min_ - x[n]\n                    elif d + x[n] > max_:\n                        d = max_ - x[n]\n                    else:\n                        prefac = np.sign(d - dx) + 2 * mu * d + lam * bn\n                        d_g_d_lam -= prefac * bn * t\n                        d_g_d_mu -= prefac * 2 * d * t\n\n                g += np.abs(dx - d) + mu * d ** 2 + lam * bn * d\n                d_g_d_lam += bn * d\n                d_g_d_mu += d ** 2\n        else:  # mu == 0\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                if np.abs(lam * bn) < 1:\n                    d = dx\n                elif np.sign(lam * bn) < 0:\n                    d = max_ - x[n]\n                else:\n                    d = min_ - x[n]\n\n                g += np.abs(dx - d) + mu * d ** 2 + lam * bn * d\n                d_g_d_lam += bn * d\n                d_g_d_mu += d ** 2\n\n        g += -mu * r ** 2 - lam * c\n        d_g_d_lam -= c\n        d_g_d_mu -= r ** 2\n\n        return -g, -np.array([d_g_d_lam, d_g_d_mu])\n\n    def _get_final_delta(self, lam, mu, x0, x, b, min_, max_, c, r, touchup=True):\n        delta = np.empty_like(x0)\n        N = x0.shape[0]\n\n        b_dot_d = 0\n        norm_d = 0\n        distance = 0\n\n        if mu > 0:\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                t = 1 / (2 * mu)\n                u = -lam * bn * t - dx\n\n                if np.abs(u) - t < 0:\n                    # value and grad = 0\n                    d = dx\n                else:\n                    d = np.sign(u) * (np.abs(u) - t) + dx\n\n                    if d + x[n] < min_:\n                        # grad = 0\n                        d = min_ - x[n]\n                    elif d + x[n] > max_:\n                        # grad = 0\n                        d = max_ - x[n]\n\n                delta[n] = d\n                b_dot_d += b[n] * d\n                norm_d += d ** 2\n                distance += np.abs(d - dx)\n        else:  # mu == 0\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                if np.abs(lam * bn) < 1:\n                    d = dx\n                elif np.sign(lam * bn) < 0:\n                    d = max_ - x[n]\n                else:\n                    d = min_ - x[n]\n\n                delta[n] = d\n                b_dot_d += b[n] * d\n                norm_d += d ** 2\n                distance += np.abs(d - dx)\n\n        if touchup:\n            # search for the one index that (a) we can modify to match boundary constraint, (b) stays within our\n            # trust region and (c) minimize the distance to the original image\n            dc = c - b_dot_d\n            k = 0\n            min_distance = np.inf\n            min_distance_idx = 0\n            for n in range(N):\n                if np.abs(b[n]) > 0:\n                    dx = x0[n] - x[n]\n                    old_d = delta[n]\n                    new_d = old_d + dc / b[n]\n\n                    if (\n                        x[n] + new_d <= max_\n                        and x[n] + new_d >= min_\n                        and norm_d - old_d ** 2 + new_d ** 2 <= r ** 2\n                    ):\n                        # conditions (a) and (b) are fulfilled\n                        if k == 0:\n                            min_distance = (\n                                distance - np.abs(old_d - dx) + np.abs(new_d - dx)\n                            )\n                            min_distance_idx = n\n                            k += 1\n                        else:\n                            new_distance = (\n                                distance - np.abs(old_d - dx) + np.abs(new_d - dx)\n                            )\n                            if min_distance > new_distance:\n                                min_distance = new_distance\n                                min_distance_idx = n\n\n            if k > 0:\n                # touchup successful\n                idx = min_distance_idx\n                old_d = delta[idx]\n\n                new_d = old_d + dc / b[idx]\n                delta[idx] = new_d\n\n        return delta\n\n    def _distance(self, x0, x):\n        return np.abs(x0 - x).sum()\n\n\n@jitclass(spec=spec)\nclass LinfOptimizer(Optimizer):\n    def optimize_distance_s_t_boundary_and_trustregion(\n        self, x0, x, b, min_, max_, c, r\n    ):\n        """""" Find the solution to the optimization problem\n\n            min_delta ||dx - delta||_p^p s.t. ||delta||_2^2 <= r^2 AND b^T delta = c AND min_ <= x + delta <= max_\n        """"""\n        params0 = np.array([0.0, 0.0])\n        bounds = np.array([(-np.inf, np.inf), (0, np.inf)])\n\n        return self.binary_search(params0, bounds, x0, x, b, min_, max_, c, r)\n\n    def binary_search(\n        self, q0, bounds, x0, x, b, min_, max_, c, r, etol=1e-6, maxiter=1000\n    ):\n        # perform binary search over epsilon\n        epsilon = (max_ - min_) / 2.0\n        eps_low = min_\n        eps_high = max_\n        func_calls = 0\n\n        bnorm = np.linalg.norm(b)\n        lambda0 = 2 * c / bnorm ** 2\n\n        k = 0\n\n        while eps_high - eps_low > etol:\n            fun, nfev, _lambda0 = self.fun(\n                epsilon, x0, x, b, min_, max_, c, r, lambda0=lambda0\n            )\n            func_calls += nfev\n            if fun > -np.inf:\n                # decrease epsilon\n                eps_high = epsilon\n                lambda0 = _lambda0\n            else:\n                # increase epsilon\n                eps_low = epsilon\n\n            k += 1\n            epsilon = (eps_high - eps_low) / 2.0 + eps_low\n\n            if k > 20:\n                break\n\n        delta = self._get_final_delta(\n            lambda0, eps_high, x0, x, b, min_, max_, c, r, touchup=True\n        )\n        return delta\n\n    def _Linf_bounds(self, x0, epsilon, ell, u):\n        N = x0.shape[0]\n        _ell = np.empty_like(x0)\n        _u = np.empty_like(x0)\n        for i in range(N):\n            nx, px = x0[i] - epsilon, x0[i] + epsilon\n            if nx > ell:\n                _ell[i] = nx\n            else:\n                _ell[i] = ell\n\n            if px < u:\n                _u[i] = px\n            else:\n                _u[i] = u\n\n        return _ell, _u\n\n    def fun(self, epsilon, x0, x, b, ell, u, c, r, lambda0=None):\n        """""" Computes the minimum norm necessary to reach the boundary. More precisely, we aim to solve the\n            following optimization problem\n\n                min ||delta||_2^2 s.t. lower <= x + delta <= upper AND b.dot(delta) = c\n\n            Lets forget about the box constraints for a second, i.e.\n\n                min ||delta||_2^2 s.t. b.dot(delta) = c\n\n            The dual of this problem is quite straight-forward to solve,\n\n                g(lambda, delta) = ||delta||_2^2 + lambda * (c - b.dot(delta))\n\n            The minimum of this Lagrangian is delta^* = lambda * b / 2, and so\n\n                inf_delta g(lambda, delta) = lambda^2 / 4 ||b||_2^2 + lambda * c\n\n            and so the optimal lambda, which maximizes inf_delta g(lambda, delta), is given by\n\n                lambda^* = 2c / ||b||_2^2\n\n            which in turn yields the optimal delta:\n\n                delta^* = c * b / ||b||_2^2\n\n            To take into account the box-constraints we perform a binary search over lambda and apply the box\n            constraint in each step.\n        """"""\n        N = x.shape[0]\n\n        # new box constraints\n        _ell, _u = self._Linf_bounds(x0, epsilon, ell, u)\n\n        # initialize lambda\n        _lambda = lambda0\n\n        # compute delta and determine active set\n        k = 0\n\n        lambda_max, lambda_min = 1e10, -1e10\n\n        # check whether problem is actually solvable (i.e. check whether boundary constraint can be reached)\n        max_c = 0\n        min_c = 0\n\n        for n in range(N):\n            if b[n] > 0:\n                max_c += b[n] * (_u[n] - x[n])\n                min_c += b[n] * (_ell[n] - x[n])\n            else:\n                max_c += b[n] * (_ell[n] - x[n])\n                min_c += b[n] * (_u[n] - x[n])\n\n        if c > max_c or c < min_c:\n            return -np.inf, k, _lambda\n\n        while True:\n            k += 1\n            _c = 0\n            norm = 0\n            _active_bnorm = 0\n\n            for n in range(N):\n                lam_step = _lambda * b[n] / 2\n                if lam_step + x[n] < _ell[n]:\n                    delta_step = _ell[n] - x[n]\n                elif lam_step + x[n] > _u[n]:\n                    delta_step = _u[n] - x[n]\n                else:\n                    delta_step = lam_step\n                    _active_bnorm += b[n] ** 2\n\n                _c += b[n] * delta_step\n                norm += delta_step ** 2\n\n            if 0.9999 * np.abs(c) - EPS < np.abs(_c) < 1.0001 * np.abs(c) + EPS:\n                if norm > r ** 2:\n                    return -np.inf, k, _lambda\n                else:\n                    return -epsilon, k, _lambda\n            else:\n                # update lambda according to active variables\n                if _c > c:\n                    lambda_max = _lambda\n                else:\n                    lambda_min = _lambda\n                #\n                if _active_bnorm == 0:\n                    # update is stepping out of feasible region, fallback to binary search\n                    _lambda = (lambda_max - lambda_min) / 2 + lambda_min\n                else:\n                    _lambda += 2 * (c - _c) / _active_bnorm\n\n                dlambda = lambda_max - lambda_min\n                if (\n                    _lambda > lambda_max - 0.1 * dlambda\n                    or _lambda < lambda_min + 0.1 * dlambda\n                ):\n                    # update is stepping out of feasible region, fallback to binary search\n                    _lambda = (lambda_max - lambda_min) / 2 + lambda_min\n\n    def _get_final_delta(self, lam, eps, x0, x, b, min_, max_, c, r, touchup=True):\n        N = x.shape[0]\n        delta = np.empty_like(x0)\n\n        # new box constraints\n        _ell, _u = self._Linf_bounds(x0, eps, min_, max_)\n\n        for n in range(N):\n            lam_step = lam * b[n] / 2\n            if lam_step + x[n] < _ell[n]:\n                delta[n] = _ell[n] - x[n]\n            elif lam_step + x[n] > _u[n]:\n                delta[n] = _u[n] - x[n]\n            else:\n                delta[n] = lam_step\n\n        return delta\n\n    def _distance(self, x0, x):\n        return np.abs(x0 - x).max()\n\n\n@jitclass(spec=spec)\nclass L0Optimizer(Optimizer):\n    def optimize_distance_s_t_boundary_and_trustregion(\n        self, x0, x, b, min_, max_, c, r\n    ):\n        """""" Find the solution to the optimization problem\n\n            min_delta ||dx - delta||_p^p s.t. ||delta||_2^2 <= r^2 AND b^T delta = c AND min_ <= x + delta <= max_\n        """"""\n        params0 = np.array([0.0, 0.0])\n        bounds = np.array([(-np.inf, np.inf), (0, np.inf)])\n\n        return self.minimize(params0, bounds, x0, x, b, min_, max_, c, r)\n\n    def minimize(\n        self,\n        q0,\n        bounds,\n        x0,\n        x,\n        b,\n        min_,\n        max_,\n        c,\n        r,\n        ftol=1e-9,\n        xtol=-1e-5,\n        maxiter=1000,\n    ):\n        # First check whether solution can be computed without trust region\n        delta, delta_norm = self.minimize_without_trustregion(\n            x0, x, b, c, r, min_, max_\n        )\n\n        if delta_norm <= r:\n            return delta\n        else:\n            # perform Nelder-Mead optimization\n            args = (x0, x, b, min_, max_, c, r)\n\n            results = self._nelder_mead_algorithm(\n                q0, bounds, args=args, tol_f=ftol, tol_x=xtol, max_iter=maxiter\n            )\n\n            delta = self._get_final_delta(\n                results[0], results[1], x0, x, b, min_, max_, c, r, touchup=True\n            )\n\n        return delta\n\n    def minimize_without_trustregion(self, x0, x, b, c, r, ell, u):\n        # compute maximum direction to b.dot(delta) within box-constraints\n        delta = x0 - x\n        total = np.empty_like(x0)\n        total_b = np.empty_like(x0)\n        bdotdelta = b.dot(delta)\n        delta_bdotdelta = c - bdotdelta\n\n        for k in range(x0.shape[0]):\n            if b[k] > 0 and delta_bdotdelta > 0:\n                total_b[k] = (u - x0[k]) * b[k]  # pos\n                total[k] = u - x0[k]\n            elif b[k] > 0 and delta_bdotdelta < 0:\n                total_b[k] = (ell - x0[k]) * b[k]  # neg\n                total[k] = ell - x0[k]\n            elif b[k] < 0 and delta_bdotdelta > 0:\n                total_b[k] = (ell - x0[k]) * b[k]  # pos\n                total[k] = ell - x0[k]\n            else:\n                total_b[k] = (u - x0[k]) * b[k]  # neg\n                total[k] = u - x0[k]\n\n        b_argsort = np.argsort(np.abs(total_b))[::-1]\n\n        for idx in b_argsort:\n            if np.abs(c - bdotdelta) > np.abs(total_b[idx]):\n                delta[idx] += total[idx]\n                bdotdelta += total_b[idx]\n            else:\n                delta[idx] += (c - bdotdelta) / (b[idx] + 1e-20)\n                break\n\n        delta_norm = np.linalg.norm(delta)\n\n        return delta, delta_norm\n\n    def _nelder_mead_algorithm(\n        self,\n        q0,\n        bounds,\n        args=(),\n        \xcf\x81=1.0,\n        \xcf\x87=2.0,\n        \xce\xb3=0.5,\n        \xcf\x83=0.5,\n        tol_f=1e-8,\n        tol_x=1e-8,\n        max_iter=1000,\n    ):\n        """"""\n        Implements the Nelder-Mead algorithm described in Lagarias et al. (1998)\n        modified to maximize instead of minimizing.\n\n        Parameters\n        ----------\n        vertices : ndarray(float, ndim=2)\n            Initial simplex with shape (n+1, n) to be modified in-place.\n\n        args : tuple, optional\n            Extra arguments passed to the objective function.\n\n        \xcf\x81 : scalar(float), optional(default=1.)\n            Reflection parameter. Must be strictly greater than 0.\n\n        \xcf\x87 : scalar(float), optional(default=2.)\n            Expansion parameter. Must be strictly greater than max(1, \xcf\x81).\n\n        \xce\xb3 : scalar(float), optional(default=0.5)\n            Contraction parameter. Must be stricly between 0 and 1.\n\n        \xcf\x83 : scalar(float), optional(default=0.5)\n            Shrinkage parameter. Must be strictly between 0 and 1.\n\n        tol_f : scalar(float), optional(default=1e-10)\n            Tolerance to be used for the function value convergence test.\n\n        tol_x : scalar(float), optional(default=1e-10)\n            Tolerance to be used for the function domain convergence test.\n\n        max_iter : scalar(float), optional(default=1000)\n            The maximum number of allowed iterations.\n\n        Returns\n        ----------\n        x : Approximate solution\n\n        """"""\n        vertices = self._initialize_simplex(q0)\n        n = vertices.shape[1]\n        self._check_params(\xcf\x81, \xcf\x87, \xce\xb3, \xcf\x83, bounds, n)\n\n        nit = 0\n\n        \xcf\x81\xce\xb3 = \xcf\x81 * \xce\xb3\n        \xcf\x81\xcf\x87 = \xcf\x81 * \xcf\x87\n        \xcf\x83_n = \xcf\x83 ** n\n\n        f_val = np.empty(n + 1, dtype=np.float64)\n        for i in range(n + 1):\n            f_val[i] = self._neg_bounded_fun(bounds, vertices[i], args=args)\n\n        # Step 1: Sort\n        sort_ind = f_val.argsort()\n        LV_ratio = 1\n\n        # Compute centroid\n        x_bar = vertices[sort_ind[:n]].sum(axis=0) / n\n\n        while True:\n            shrink = False\n\n            # Check termination\n            fail = nit >= max_iter\n\n            best_val_idx = sort_ind[0]\n            worst_val_idx = sort_ind[n]\n\n            term_f = f_val[worst_val_idx] - f_val[best_val_idx] < tol_f\n\n            # Linearized volume ratio test (see [2])\n            term_x = LV_ratio < tol_x\n\n            if term_x or term_f or fail:\n                break\n\n            # Step 2: Reflection\n            x_r = x_bar + \xcf\x81 * (x_bar - vertices[worst_val_idx])\n            f_r = self._neg_bounded_fun(bounds, x_r, args=args)\n\n            if f_r >= f_val[best_val_idx] and f_r < f_val[sort_ind[n - 1]]:\n                # Accept reflection\n                vertices[worst_val_idx] = x_r\n                LV_ratio *= \xcf\x81\n\n            # Step 3: Expansion\n            elif f_r < f_val[best_val_idx]:\n                x_e = x_bar + \xcf\x87 * (x_r - x_bar)\n                f_e = self._neg_bounded_fun(bounds, x_e, args=args)\n                if f_e < f_r:  # Greedy minimization\n                    vertices[worst_val_idx] = x_e\n                    LV_ratio *= \xcf\x81\xcf\x87\n                else:\n                    vertices[worst_val_idx] = x_r\n                    LV_ratio *= \xcf\x81\n\n            # Step 4 & 5: Contraction and Shrink\n            else:\n                # Step 4: Contraction\n                if f_r < f_val[worst_val_idx]:  # Step 4.a: Outside Contraction\n                    x_c = x_bar + \xce\xb3 * (x_r - x_bar)\n                    LV_ratio_update = \xcf\x81\xce\xb3\n                else:  # Step 4.b: Inside Contraction\n                    x_c = x_bar - \xce\xb3 * (x_r - x_bar)\n                    LV_ratio_update = \xce\xb3\n\n                f_c = self._neg_bounded_fun(bounds, x_c, args=args)\n                if f_c < min(f_r, f_val[worst_val_idx]):  # Accept contraction\n                    vertices[worst_val_idx] = x_c\n                    LV_ratio *= LV_ratio_update\n\n                # Step 5: Shrink\n                else:\n                    shrink = True\n                    for i in sort_ind[1:]:\n                        vertices[i] = vertices[best_val_idx] + \xcf\x83 * (\n                            vertices[i] - vertices[best_val_idx]\n                        )\n                        f_val[i] = self._neg_bounded_fun(bounds, vertices[i], args=args)\n\n                    sort_ind[1:] = f_val[sort_ind[1:]].argsort() + 1\n\n                    x_bar = (\n                        vertices[best_val_idx]\n                        + \xcf\x83 * (x_bar - vertices[best_val_idx])\n                        + (vertices[worst_val_idx] - vertices[sort_ind[n]]) / n\n                    )\n\n                    LV_ratio *= \xcf\x83_n\n\n            if not shrink:  # Nonshrink ordering rule\n                f_val[worst_val_idx] = self._neg_bounded_fun(\n                    bounds, vertices[worst_val_idx], args=args\n                )\n\n                for i, j in enumerate(sort_ind):\n                    if f_val[worst_val_idx] < f_val[j]:\n                        sort_ind[i + 1 :] = sort_ind[i:-1]\n                        sort_ind[i] = worst_val_idx\n                        break\n\n                x_bar += (vertices[worst_val_idx] - vertices[sort_ind[n]]) / n\n\n            nit += 1\n\n        return vertices[sort_ind[0]]\n\n    def _initialize_simplex(self, x0):\n        """"""\n        Generates an initial simplex for the Nelder-Mead method.\n\n        Parameters\n        ----------\n        x0 : ndarray(float, ndim=1)\n            Initial guess. Array of real elements of size (n,), where \xe2\x80\x98n\xe2\x80\x99 is the\n            number of independent variables.\n\n        bounds: ndarray(float, ndim=2)\n            Sequence of (min, max) pairs for each element in x0.\n\n        Returns\n        ----------\n        vertices : ndarray(float, ndim=2)\n            Initial simplex with shape (n+1, n).\n\n        """"""\n        n = x0.size\n\n        vertices = np.empty((n + 1, n), dtype=np.float64)\n\n        # Broadcast x0 on row dimension\n        vertices[:] = x0\n\n        nonzdelt = 0.05\n        zdelt = 0.00025\n\n        for i in range(n):\n            # Generate candidate coordinate\n            if vertices[i + 1, i] != 0.0:\n                vertices[i + 1, i] *= 1 + nonzdelt\n            else:\n                vertices[i + 1, i] = zdelt\n\n        return vertices\n\n    def _check_params(self, \xcf\x81, \xcf\x87, \xce\xb3, \xcf\x83, bounds, n):\n        """"""\n        Checks whether the parameters for the Nelder-Mead algorithm are valid.\n        JIT-compiled in `nopython` mode using Numba.\n\n        Parameters\n        ----------\n        \xcf\x81 : scalar(float)\n            Reflection parameter. Must be strictly greater than 0.\n\n        \xcf\x87 : scalar(float)\n            Expansion parameter. Must be strictly greater than max(1, \xcf\x81).\n\n        \xce\xb3 : scalar(float)\n            Contraction parameter. Must be stricly between 0 and 1.\n\n        \xcf\x83 : scalar(float)\n            Shrinkage parameter. Must be strictly between 0 and 1.\n\n        bounds: ndarray(float, ndim=2)\n            Sequence of (min, max) pairs for each element in x.\n\n        n : scalar(int)\n            Number of independent variables.\n\n        """"""\n        if \xcf\x81 < 0:\n            raise ValueError(""\xcf\x81 must be strictly greater than 0."")\n        if \xcf\x87 < 1:\n            raise ValueError(""\xcf\x87 must be strictly greater than 1."")\n        if \xcf\x87 < \xcf\x81:\n            raise ValueError(""\xcf\x87 must be strictly greater than \xcf\x81."")\n        if \xce\xb3 < 0 or \xce\xb3 > 1:\n            raise ValueError(""\xce\xb3 must be strictly between 0 and 1."")\n        if \xcf\x83 < 0 or \xcf\x83 > 1:\n            raise ValueError(""\xcf\x83 must be strictly between 0 and 1."")\n\n        if not (bounds.shape == (0, 2) or bounds.shape == (n, 2)):\n            raise ValueError(""The shape of `bounds` is not valid."")\n        if (np.atleast_2d(bounds)[:, 0] > np.atleast_2d(bounds)[:, 1]).any():\n            raise ValueError(""Lower bounds must be greater than upper bounds."")\n\n    def _check_bounds(self, x, bounds):\n        """"""\n        Checks whether `x` is within `bounds`. JIT-compiled in `nopython` mode\n        using Numba.\n\n        Parameters\n        ----------\n        x : ndarray(float, ndim=1)\n            1-D array with shape (n,) of independent variables.\n\n        bounds: ndarray(float, ndim=2)\n            Sequence of (min, max) pairs for each element in x.\n\n        Returns\n        ----------\n        bool\n            `True` if `x` is within `bounds`, `False` otherwise.\n\n        """"""\n        if bounds.shape == (0, 2):\n            return True\n        else:\n            return (np.atleast_2d(bounds)[:, 0] <= x).all() and (\n                x <= np.atleast_2d(bounds)[:, 1]\n            ).all()\n\n    def _neg_bounded_fun(self, bounds, x, args=()):\n        """"""\n        Wrapper for bounding and taking the negative of `fun` for the\n        Nelder-Mead algorithm. JIT-compiled in `nopython` mode using Numba.\n\n        Parameters\n        ----------\n        bounds: ndarray(float, ndim=2)\n            Sequence of (min, max) pairs for each element in x.\n\n        x : ndarray(float, ndim=1)\n            1-D array with shape (n,) of independent variables at which `fun` is\n            to be evaluated.\n\n        args : tuple, optional\n            Extra arguments passed to the objective function.\n\n        Returns\n        ----------\n        scalar\n            `-fun(x, *args)` if x is within `bounds`, `np.inf` otherwise.\n\n        """"""\n        if self._check_bounds(x, bounds):\n            return -self.fun(x, *args)\n        else:\n            return np.inf\n\n    def fun(self, params, x0, x, b, min_, max_, c, r):\n        # arg min_delta ||delta - dx||_0 + lam * b^T delta + mu * ||delta||_2^2  s.t.  min <= delta + x <= max\n        lam, mu = params\n        N = x0.shape[0]\n\n        g = -mu * r ** 2 - lam * c\n\n        if mu > 0:\n            t = 1 / (2 * mu)\n\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n\n                case1 = lam * bn * dx + mu * dx ** 2\n\n                optd = -lam * bn * t\n                if optd < min_ - x[n]:\n                    optd = min_ - x[n]\n                elif optd > max_ - x[n]:\n                    optd = max_ - x[n]\n\n                case2 = 1 + lam * bn * optd + mu * optd ** 2\n\n                if case1 <= case2:\n                    g += mu * dx ** 2 + lam * bn * dx\n                else:\n                    g += 1 + mu * optd ** 2 + lam * bn * optd\n        else:\n            # arg min_delta ||delta - dx||_0 + lam * b^T delta\n            # case delta[n] = dx[n]: lam * b[n] * dx[n]\n            # case delta[n] != dx[n]: lam * b[n] * [min_ - x[n], max_ - x[n]]\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                case1 = lam * bn * dx\n                case2 = 1 + lam * bn * (min_ - x[n])\n                case3 = 1 + lam * bn * (max_ - x[n])\n                if case1 <= case2 and case1 <= case3:\n                    g += mu * dx ** 2 + lam * bn * dx\n                elif case2 < case3:\n                    g += 1 + mu * (min_ - x[n]) ** 2 + lam * bn * (min_ - x[n])\n                else:\n                    g += 1 + mu * (max_ - x[n]) ** 2 + lam * bn * (max_ - x[n])\n\n        return g\n\n    def _get_final_delta(self, lam, mu, x0, x, b, min_, max_, c, r, touchup=True):\n        if touchup:\n            delta = self.__get_final_delta(lam, mu, x0, x, b, min_, max_, c, r)\n            if delta is not None:\n                return delta\n            else:\n                # fallback\n                params = [\n                    (lam + 1e-5, mu),\n                    (lam, mu + 1e-5),\n                    (lam - 1e-5, mu),\n                    (lam, mu - 1e-5),\n                    (lam + 1e-5, mu + 1e-5),\n                    (lam - 1e-5, mu - 1e-5),\n                    (lam + 1e-5, mu - 1e-5),\n                    (lam - 1e-5, mu + 1e-5),\n                ]\n                for param in params:\n                    delta = self.__get_final_delta(\n                        param[0], param[1], x0, x, b, min_, max_, c, r\n                    )\n                    if delta is not None:\n                        return delta\n\n                # 2nd fallback\n                return self.__get_final_delta(\n                    lam, mu, x0, x, b, min_, max_, c, r, False\n                )\n        else:\n            return self.__get_final_delta(lam, mu, x0, x, b, min_, max_, c, r, False)\n\n    def __get_final_delta(self, lam, mu, x0, x, b, min_, max_, c, r, touchup=True):\n        delta = np.empty_like(x0)\n        N = x0.shape[0]\n\n        b_dot_d = 0\n        norm_d = 0\n        distance = 0\n\n        if mu > 0:\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                t = 1 / (2 * mu)\n\n                case1 = lam * bn * dx + mu * dx ** 2\n\n                optd = -lam * bn * t\n                if optd < min_ - x[n]:\n                    optd = min_ - x[n]\n                elif optd > max_ - x[n]:\n                    optd = max_ - x[n]\n\n                case2 = 1 + lam * bn * optd + mu * optd ** 2\n\n                if case1 <= case2:\n                    d = dx\n                else:\n                    d = optd\n                    distance += 1\n\n                delta[n] = d\n                b_dot_d += bn * d\n                norm_d += d ** 2\n        else:  # mu == 0\n            for n in range(N):\n                dx = x0[n] - x[n]\n                bn = b[n]\n                case1 = lam * bn * dx\n                case2 = 1 + lam * bn * (min_ - x[n])\n                case3 = 1 + lam * bn * (max_ - x[n])\n                if case1 <= case2 and case1 <= case3:\n                    d = dx\n                elif case2 < case3:\n                    d = min_ - x[n]\n                    distance += 1\n                else:\n                    d = max_ - x[n]\n                    distance += 1\n\n                delta[n] = d\n                norm_d += d ** 2\n                b_dot_d += bn * d\n\n        if touchup:\n            # search for the one index that\n            # (a) we can modify to match boundary constraint\n            # (b) stays within our trust region and\n            # (c) minimize the distance to the original image.\n            dc = c - b_dot_d\n            k = 0\n            min_distance = np.inf\n            min_norm = np.inf\n            min_distance_idx = 0\n            for n in range(N):\n                if np.abs(b[n]) > 0:\n                    dx = x0[n] - x[n]\n                    old_d = delta[n]\n                    new_d = old_d + dc / b[n]\n\n                    if (\n                        x[n] + new_d <= max_\n                        and x[n] + new_d >= min_\n                        and norm_d - old_d ** 2 + new_d ** 2 <= r ** 2\n                    ):\n                        # conditions (a) and (b) are fulfilled\n                        if k == 0:\n                            min_distance = (\n                                distance\n                                - (np.abs(old_d - dx) > 1e-10)\n                                + (np.abs(new_d - dx) > 1e-10)\n                            )\n                            min_distance_idx = n\n                            min_norm = norm_d - old_d ** 2 + new_d ** 2\n                            k += 1\n                        else:\n                            new_distance = (\n                                distance\n                                - (np.abs(old_d - dx) > 1e-10)\n                                + (np.abs(new_d - dx) > 1e-10)\n                            )\n                            if (\n                                min_distance > new_distance\n                                or min_distance == new_distance\n                                and min_norm > norm_d - old_d ** 2 + new_d ** 2\n                            ):\n                                min_distance = new_distance\n                                min_norm = norm_d - old_d ** 2 + new_d ** 2\n                                min_distance_idx = n\n\n            if k > 0:\n                # touchup successful\n                idx = min_distance_idx\n                old_d = delta[idx]\n\n                new_d = old_d + dc / b[idx]\n                delta[idx] = new_d\n\n                return delta\n            else:\n                return None\n\n        return delta\n\n    def _distance(self, x0, x):\n        return np.sum(np.abs(x - x0) > EPS)\n'"
foolbox/attacks/carlini_wagner.py,0,"b'from typing import Union, Tuple, Any, Optional\nfrom functools import partial\nimport numpy as np\nimport eagerpy as ep\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..types import Bounds\n\nfrom ..models import Model\n\nfrom ..distances import l2\n\nfrom ..criteria import Misclassification\nfrom ..criteria import TargetedMisclassification\n\nfrom .base import MinimizationAttack\nfrom .base import T\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\n\nclass L2CarliniWagnerAttack(MinimizationAttack):\n    """"""Implementation of the Carlini & Wagner L2 Attack. [#Carl16]_\n\n    Args:\n        binary_search_steps : Number of steps to perform in the binary search\n            over the const c.\n        steps : Number of optimization steps within each binary search step.\n        stepsize : Stepsize to update the examples.\n        confidence : Confidence required for an example to be marked as adversarial.\n            Controls the gap between example and decision boundary.\n        initial_const : Initial value of the const c with which the binary search starts.\n        abort_early : Stop inner search as soons as an adversarial example has been found.\n            Does not affect the binary search over the const c.\n\n    References:\n        .. [#Carl16] Nicholas Carlini, David Wagner, ""Towards evaluating the robustness of\n            neural networks. In 2017 ieee symposium on security and privacy""\n            https://arxiv.org/abs/1608.04644\n    """"""\n\n    distance = l2\n\n    def __init__(\n        self,\n        binary_search_steps: int = 9,\n        steps: int = 10000,\n        stepsize: float = 1e-2,\n        confidence: float = 0,\n        initial_const: float = 1e-3,\n        abort_early: bool = True,\n    ):\n        self.binary_search_steps = binary_search_steps\n        self.steps = steps\n        self.stepsize = stepsize\n        self.confidence = confidence\n        self.initial_const = initial_const\n        self.abort_early = abort_early\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Misclassification, TargetedMisclassification, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        N = len(x)\n\n        if isinstance(criterion_, Misclassification):\n            targeted = False\n            classes = criterion_.labels\n            change_classes_logits = self.confidence\n        elif isinstance(criterion_, TargetedMisclassification):\n            targeted = True\n            classes = criterion_.target_classes\n            change_classes_logits = -self.confidence\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        def is_adversarial(perturbed: ep.Tensor, logits: ep.Tensor) -> ep.Tensor:\n            if change_classes_logits != 0:\n                logits += ep.onehot_like(logits, classes, value=change_classes_logits)\n            return criterion_(perturbed, logits)\n\n        if classes.shape != (N,):\n            name = ""target_classes"" if targeted else ""labels""\n            raise ValueError(\n                f""expected {name} to have shape ({N},), got {classes.shape}""\n            )\n\n        bounds = model.bounds\n        to_attack_space = partial(_to_attack_space, bounds=bounds)\n        to_model_space = partial(_to_model_space, bounds=bounds)\n\n        x_attack = to_attack_space(x)\n        reconstsructed_x = to_model_space(x_attack)\n\n        rows = range(N)\n\n        def loss_fun(\n            delta: ep.Tensor, consts: ep.Tensor\n        ) -> Tuple[ep.Tensor, Tuple[ep.Tensor, ep.Tensor]]:\n            assert delta.shape == x_attack.shape\n            assert consts.shape == (N,)\n\n            x = to_model_space(x_attack + delta)\n            logits = model(x)\n\n            if targeted:\n                c_minimize = best_other_classes(logits, classes)\n                c_maximize = classes  # target_classes\n            else:\n                c_minimize = classes  # labels\n                c_maximize = best_other_classes(logits, classes)\n\n            is_adv_loss = logits[rows, c_minimize] - logits[rows, c_maximize]\n            assert is_adv_loss.shape == (N,)\n\n            is_adv_loss = is_adv_loss + self.confidence\n            is_adv_loss = ep.maximum(0, is_adv_loss)\n            is_adv_loss = is_adv_loss * consts\n\n            squared_norms = flatten(x - reconstsructed_x).square().sum(axis=-1)\n            loss = is_adv_loss.sum() + squared_norms.sum()\n            return loss, (x, logits)\n\n        loss_aux_and_grad = ep.value_and_grad_fn(x, loss_fun, has_aux=True)\n\n        consts = self.initial_const * np.ones((N,))\n        lower_bounds = np.zeros((N,))\n        upper_bounds = np.inf * np.ones((N,))\n\n        best_advs = ep.zeros_like(x)\n        best_advs_norms = ep.full(x, (N,), ep.inf)\n\n        # the binary search searches for the smallest consts that produce adversarials\n        for binary_search_step in range(self.binary_search_steps):\n            if (\n                binary_search_step == self.binary_search_steps - 1\n                and self.binary_search_steps >= 10\n            ):\n                # in the last binary search step, repeat the search once\n                consts = np.minimum(upper_bounds, 1e10)\n\n            # create a new optimizer find the delta that minimizes the loss\n            delta = ep.zeros_like(x_attack)\n            optimizer = AdamOptimizer(delta)\n\n            # tracks whether adv with the current consts was found\n            found_advs = np.full((N,), fill_value=False)\n            loss_at_previous_check = np.inf\n\n            consts_ = ep.from_numpy(x, consts.astype(np.float32))\n\n            for step in range(self.steps):\n                loss, (perturbed, logits), gradient = loss_aux_and_grad(delta, consts_)\n                delta += optimizer(gradient, self.stepsize)\n\n                if self.abort_early and step % (np.ceil(self.steps / 10)) == 0:\n                    # after each tenth of the overall steps, check progress\n                    if not (loss <= 0.9999 * loss_at_previous_check):\n                        break  # stop Adam if there has been no progress\n                    loss_at_previous_check = loss\n\n                found_advs_iter = is_adversarial(perturbed, logits)\n                found_advs = np.logical_or(found_advs, found_advs_iter.numpy())\n\n                norms = flatten(perturbed - x).norms.l2(axis=-1)\n                closer = norms < best_advs_norms\n                new_best = ep.logical_and(closer, found_advs_iter)\n\n                new_best_ = atleast_kd(new_best, best_advs.ndim)\n                best_advs = ep.where(new_best_, perturbed, best_advs)\n                best_advs_norms = ep.where(new_best, norms, best_advs_norms)\n\n            upper_bounds = np.where(found_advs, consts, upper_bounds)\n            lower_bounds = np.where(found_advs, lower_bounds, consts)\n\n            consts_exponential_search = consts * 10\n            consts_binary_search = (lower_bounds + upper_bounds) / 2\n            consts = np.where(\n                np.isinf(upper_bounds), consts_exponential_search, consts_binary_search\n            )\n\n        return restore_type(best_advs)\n\n\nclass AdamOptimizer:\n    def __init__(self, x: ep.Tensor):\n        self.m = ep.zeros_like(x)\n        self.v = ep.zeros_like(x)\n        self.t = 0\n\n    def __call__(\n        self,\n        gradient: ep.Tensor,\n        stepsize: float,\n        beta1: float = 0.9,\n        beta2: float = 0.999,\n        epsilon: float = 1e-8,\n    ) -> ep.Tensor:\n        self.t += 1\n\n        self.m = beta1 * self.m + (1 - beta1) * gradient\n        self.v = beta2 * self.v + (1 - beta2) * gradient ** 2\n\n        bias_correction_1 = 1 - beta1 ** self.t\n        bias_correction_2 = 1 - beta2 ** self.t\n\n        m_hat = self.m / bias_correction_1\n        v_hat = self.v / bias_correction_2\n\n        return -stepsize * m_hat / (ep.sqrt(v_hat) + epsilon)\n\n\ndef best_other_classes(logits: ep.Tensor, exclude: ep.Tensor) -> ep.Tensor:\n    other_logits = logits - ep.onehot_like(logits, exclude, value=ep.inf)\n    return other_logits.argmax(axis=-1)\n\n\ndef _to_attack_space(x: ep.Tensor, *, bounds: Bounds) -> ep.Tensor:\n    min_, max_ = bounds\n    a = (min_ + max_) / 2\n    b = (max_ - min_) / 2\n    x = (x - a) / b  # map from [min_, max_] to [-1, +1]\n    x = x * 0.999999  # from [-1, +1] to approx. (-1, +1)\n    x = x.arctanh()  # from (-1, +1) to (-inf, +inf)\n    return x\n\n\ndef _to_model_space(x: ep.Tensor, *, bounds: Bounds) -> ep.Tensor:\n    min_, max_ = bounds\n    x = x.tanh()  # from (-inf, +inf) to (-1, +1)\n    a = (min_ + max_) / 2\n    b = (max_ - min_) / 2\n    x = x * b + a  # map from (-1, +1) to (min_, max_)\n    return x\n'"
foolbox/attacks/contrast.py,0,"b'from typing import Union, Any\nimport eagerpy as ep\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..criteria import Criterion\n\nfrom ..distances import l2\n\nfrom ..models import Model\n\nfrom .base import FixedEpsilonAttack\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\nclass L2ContrastReductionAttack(FixedEpsilonAttack):\n    """"""Reduces the contrast of the input using a perturbation of the given size\n\n    Args:\n        target : Target relative to the bounds from 0 (min) to 1 (max)\n            towards which the contrast is reduced\n    """"""\n\n    distance = l2\n\n    def __init__(self, *, target: float = 0.5):\n        self.target = target\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, Any] = None,\n        *,\n        epsilon: float,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, criterion, kwargs\n\n        min_, max_ = model.bounds\n        target = min_ + self.target * (max_ - min_)\n\n        direction = target - x\n        norms = ep.norms.l2(flatten(direction), axis=-1)\n        scale = epsilon / atleast_kd(norms, direction.ndim)\n        scale = ep.minimum(scale, 1)\n\n        x = x + scale * direction\n        x = x.clip(min_, max_)\n        return restore_type(x)\n'"
foolbox/attacks/contrast_min.py,0,"b'from typing import Union, Any, Optional\nimport eagerpy as ep\n\nfrom ..devutils import atleast_kd\n\nfrom ..models import Model\n\nfrom ..criteria import Criterion\n\nfrom ..distances import Distance\n\nfrom .base import FlexibleDistanceMinimizationAttack\nfrom .base import T\nfrom .base import get_is_adversarial\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\n\nclass BinarySearchContrastReductionAttack(FlexibleDistanceMinimizationAttack):\n    """"""Reduces the contrast of the input using a binary search to find the\n    smallest adversarial perturbation\n\n    Args:\n        distance : Distance measure for which minimal adversarial examples are searched.\n        binary_search_steps : Number of iterations in the binary search.\n            This controls the precision of the results.\n        target : Target relative to the bounds from 0 (min) to 1 (max)\n            towards which the contrast is reduced\n    """"""\n\n    def __init__(\n        self,\n        *,\n        distance: Optional[Distance] = None,\n        binary_search_steps: int = 15,\n        target: float = 0.5,\n    ):\n        super().__init__(distance=distance)\n        self.binary_search_steps = binary_search_steps\n        self.target = target\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        min_, max_ = model.bounds\n        target = min_ + self.target * (max_ - min_)\n        direction = target - x\n\n        lower_bound = ep.zeros(x, len(x))\n        upper_bound = ep.ones(x, len(x))\n        epsilons = lower_bound\n        for _ in range(self.binary_search_steps):\n            eps = atleast_kd(epsilons, x.ndim)\n            is_adv = is_adversarial(x + eps * direction)\n            lower_bound = ep.where(is_adv, lower_bound, epsilons)\n            upper_bound = ep.where(is_adv, epsilons, upper_bound)\n            epsilons = (lower_bound + upper_bound) / 2\n\n        epsilons = upper_bound\n        eps = atleast_kd(epsilons, x.ndim)\n        xp = x + eps * direction\n        return restore_type(xp)\n\n\nclass LinearSearchContrastReductionAttack(FlexibleDistanceMinimizationAttack):\n    """"""Reduces the contrast of the input using a linear search to find the\n    smallest adversarial perturbation""""""\n\n    def __init__(\n        self,\n        *,\n        distance: Optional[Distance] = None,\n        steps: int = 1000,\n        target: float = 0.5,\n    ):\n        super().__init__(distance=distance)\n        self.steps = steps\n        self.target = target\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        min_, max_ = model.bounds\n        target = min_ + self.target * (max_ - min_)\n        direction = target - x\n\n        best = ep.ones(x, len(x))\n\n        epsilon = 0.0\n        stepsize = 1.0 / self.steps\n        for _ in range(self.steps):\n            # TODO: reduce the batch size to the ones that have not yet been sucessful\n\n            is_adv = is_adversarial(x + epsilon * direction)\n            is_best_adv = ep.logical_and(is_adv, best == 1)\n            best = ep.where(is_best_adv, epsilon, best)\n\n            if (best < 1).all():\n                break  # pragma: no cover\n\n            epsilon += stepsize\n\n        eps = atleast_kd(best, x.ndim)\n        xp = x + eps * direction\n        return restore_type(xp)\n'"
foolbox/attacks/dataset_attack.py,0,"b'from typing import Union, Optional, Any, List\nimport numpy as np\nimport eagerpy as ep\n\nfrom ..devutils import atleast_kd\n\nfrom ..models import Model\n\nfrom ..distances import Distance\n\nfrom ..criteria import Criterion\n\nfrom .base import FlexibleDistanceMinimizationAttack\nfrom .base import T\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\n\nclass DatasetAttack(FlexibleDistanceMinimizationAttack):\n    """"""Draws randomly from the given dataset until adversarial examples for all\n    inputs have been found.\n\n    To pass data form the dataset to this attack, call :meth:`feed()`.\n    :meth:`feed()` can be called several times and should only be called with\n    batches that are small enough that they can be passed through the model.\n\n    Args:\n        distance : Distance measure for which minimal adversarial examples are searched.\n    """"""\n\n    def __init__(self, *, distance: Optional[Distance] = None):\n        super().__init__(distance=distance)\n        self.raw_inputs: List[ep.Tensor] = []\n        self.raw_outputs: List[ep.Tensor] = []\n        self.inputs: Optional[ep.Tensor] = None\n        self.outputs: Optional[ep.Tensor] = None\n\n    def feed(self, model: Model, inputs: Any) -> None:\n        x = ep.astensor(inputs)\n        del inputs\n\n        self.raw_inputs.append(x)\n        self.raw_outputs.append(model(x))\n\n    def process_raw(self) -> None:\n        raw_inputs = self.raw_inputs\n        raw_outputs = self.raw_outputs\n        assert len(raw_inputs) == len(raw_outputs)\n        assert (self.inputs is None) == (self.outputs is None)\n\n        if self.inputs is None:\n            if len(raw_inputs) == 0:\n                raise ValueError(\n                    ""DatasetAttack can only be called after data has been provided using \'feed()\'""\n                )\n        elif self.inputs is not None:\n            assert self.outputs is not None\n            raw_inputs = [self.inputs] + raw_inputs\n            raw_outputs = [self.outputs] + raw_outputs\n\n        self.inputs = ep.concatenate(raw_inputs, axis=0)\n        self.outputs = ep.concatenate(raw_outputs, axis=0)\n        self.raw_inputs = []\n        self.raw_outputs = []\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        self.process_raw()\n        assert self.inputs is not None\n        assert self.outputs is not None\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n\n        result = x\n        found = criterion(x, model(x))\n\n        dataset_size = len(self.inputs)\n        batch_size = len(x)\n\n        while not found.all():\n            indices = np.random.randint(0, dataset_size, size=(batch_size,))\n\n            xp = self.inputs[indices]\n            yp = self.outputs[indices]\n            is_adv = criterion(xp, yp)\n\n            new_found = ep.logical_and(is_adv, found.logical_not())\n            result = ep.where(atleast_kd(new_found, result.ndim), xp, result)\n            found = ep.logical_or(found, new_found)\n\n        return restore_type(result)\n'"
foolbox/attacks/ddn.py,0,"b'from typing import Union, Tuple, Optional, Any\nimport math\nimport eagerpy as ep\n\nfrom ..models import Model\n\nfrom ..criteria import Misclassification, TargetedMisclassification\n\nfrom ..distances import l2\n\nfrom ..devutils import atleast_kd, flatten\n\nfrom .base import MinimizationAttack\nfrom .base import get_criterion\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\ndef normalize_gradient_l2_norms(grad: ep.Tensor) -> ep.Tensor:\n    norms = ep.norms.l2(flatten(grad), -1)\n\n    # remove zero gradients\n    grad = ep.where(\n        atleast_kd(norms == 0, grad.ndim), ep.normal(grad, shape=grad.shape), grad\n    )\n    # calculate norms again for previously vanishing elements\n    norms = ep.norms.l2(flatten(grad), -1)\n\n    norms = ep.maximum(norms, 1e-12)  # avoid division by zero\n    factor = 1 / norms\n    factor = atleast_kd(factor, grad.ndim)\n    return grad * factor\n\n\nclass DDNAttack(MinimizationAttack):\n    """"""The Decoupled Direction and Norm L2 adversarial attack. [#Rony18]_\n\n    Args:\n        init_epsilon : Initial value for the norm/epsilon ball.\n        steps : Number of steps for the optimization.\n        gamma : Factor by which the norm will be modified: new_norm = norm * (1 + or - gamma).\n\n    References:\n        .. [#Rony18] J\xc3\xa9r\xc3\xb4me Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed,\n            Robert Sabourin, Eric Granger, ""Decoupling Direction and Norm for\n            Efficient Gradient-Based L2 Adversarial Attacks and Defenses"",\n            https://arxiv.org/abs/1811.09600\n    """"""\n\n    distance = l2\n\n    def __init__(\n        self, *, init_epsilon: float = 1.0, steps: int = 10, gamma: float = 0.05,\n    ):\n        self.init_epsilon = init_epsilon\n        self.steps = steps\n        self.gamma = gamma\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Misclassification, TargetedMisclassification, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        N = len(x)\n\n        if isinstance(criterion_, Misclassification):\n            targeted = False\n            classes = criterion_.labels\n        elif isinstance(criterion_, TargetedMisclassification):\n            targeted = True\n            classes = criterion_.target_classes\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        if classes.shape != (N,):\n            name = ""target_classes"" if targeted else ""labels""\n            raise ValueError(\n                f""expected {name} to have shape ({N},), got {classes.shape}""\n            )\n\n        stepsize = 1.0\n        min_, max_ = model.bounds\n\n        def loss_fn(\n            inputs: ep.Tensor, labels: ep.Tensor\n        ) -> Tuple[ep.Tensor, ep.Tensor]:\n            logits = model(inputs)\n\n            sign = -1.0 if targeted else 1.0\n            loss = sign * ep.crossentropy(logits, labels).sum()\n\n            return loss, logits\n\n        grad_and_logits = ep.value_and_grad_fn(x, loss_fn, has_aux=True)\n\n        delta = ep.zeros_like(x)\n\n        epsilon = self.init_epsilon * ep.ones(x, len(x))\n        worst_norm = ep.norms.l2(flatten(ep.maximum(x - min_, max_ - x)), -1)\n\n        best_l2 = worst_norm\n        best_delta = delta\n        adv_found = ep.zeros(x, len(x)).bool()\n\n        for i in range(self.steps):\n            # perform cosine annealing of LR starting from 1.0 to 0.01\n            stepsize = (\n                0.01 + (stepsize - 0.01) * (1 + math.cos(math.pi * i / self.steps)) / 2\n            )\n\n            x_adv = x + delta\n\n            _, logits, gradients = grad_and_logits(x_adv, classes)\n            gradients = normalize_gradient_l2_norms(gradients)\n            is_adversarial = criterion_(x_adv, logits)\n\n            l2 = ep.norms.l2(flatten(delta), axis=-1)\n            is_smaller = l2 <= best_l2\n\n            is_both = ep.logical_and(is_adversarial, is_smaller)\n            adv_found = ep.logical_or(adv_found, is_adversarial)\n            best_l2 = ep.where(is_both, l2, best_l2)\n\n            best_delta = ep.where(atleast_kd(is_both, x.ndim), delta, best_delta)\n\n            # do step\n            delta = delta + stepsize * gradients\n\n            epsilon = epsilon * ep.where(\n                is_adversarial, 1.0 - self.gamma, 1.0 + self.gamma\n            )\n            epsilon = ep.minimum(epsilon, worst_norm)\n\n            # project to epsilon ball\n            delta *= atleast_kd(epsilon / ep.norms.l2(flatten(delta), -1), x.ndim)\n\n            # clip to valid bounds\n            delta = ep.clip(x + delta, *model.bounds) - x\n\n        x_adv = x + best_delta\n\n        return restore_type(x_adv)\n'"
foolbox/attacks/deepfool.py,0,"b'from typing import Union, Optional, Tuple, Any, Callable\nfrom typing_extensions import Literal\nimport eagerpy as ep\nimport logging\nfrom abc import ABC\nfrom abc import abstractmethod\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..models import Model\n\nfrom ..criteria import Criterion\n\nfrom ..distances import l2, linf\n\nfrom .base import MinimizationAttack\nfrom .base import T\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\n\nclass DeepFoolAttack(MinimizationAttack, ABC):\n    """"""A simple and fast gradient-based adversarial attack.\n\n    Implements the `DeepFool`_ attack.\n\n    Args:\n        steps : Maximum number of steps to perform.\n        candidates : Limit on the number of the most likely classes that should\n            be considered. A small value is usually sufficient and much faster.\n        overshoot : How much to overshoot the boundary.\n        loss  Loss function to use inside the update function.\n\n\n    .. _DeepFool:\n            Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard,\n            ""DeepFool: a simple and accurate method to fool deep neural\n            networks"", https://arxiv.org/abs/1511.04599\n\n    """"""\n\n    def __init__(\n        self,\n        *,\n        steps: int = 50,\n        candidates: Optional[int] = 10,\n        overshoot: float = 0.02,\n        loss: Union[Literal[""logits""], Literal[""crossentropy""]] = ""logits"",\n    ):\n        self.steps = steps\n        self.candidates = candidates\n        self.overshoot = overshoot\n        self.loss = loss\n\n    def _get_loss_fn(\n        self, model: Model, classes: ep.Tensor,\n    ) -> Callable[[ep.Tensor, int], Tuple[ep.Tensor, Tuple[ep.Tensor, ep.Tensor]]]:\n\n        N = len(classes)\n        rows = range(N)\n        i0 = classes[:, 0]\n\n        if self.loss == ""logits"":\n\n            def loss_fun(\n                x: ep.Tensor, k: int\n            ) -> Tuple[ep.Tensor, Tuple[ep.Tensor, ep.Tensor]]:\n                logits = model(x)\n                ik = classes[:, k]\n                l0 = logits[rows, i0]\n                lk = logits[rows, ik]\n                loss = lk - l0\n                return loss.sum(), (loss, logits)\n\n        elif self.loss == ""crossentropy"":\n\n            def loss_fun(\n                x: ep.Tensor, k: int\n            ) -> Tuple[ep.Tensor, Tuple[ep.Tensor, ep.Tensor]]:\n                logits = model(x)\n                ik = classes[:, k]\n                l0 = -ep.crossentropy(logits, i0)\n                lk = -ep.crossentropy(logits, ik)\n                loss = lk - l0\n                return loss.sum(), (loss, logits)\n\n        else:\n            raise ValueError(\n                f""expected loss to be \'logits\' or \'crossentropy\', got \'{self.loss}\'""\n            )\n\n        return loss_fun\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n\n        min_, max_ = model.bounds\n\n        logits = model(x)\n        classes = logits.argsort(axis=-1).flip(axis=-1)\n        if self.candidates is None:\n            candidates = logits.shape[-1]  # pragma: no cover\n        else:\n            candidates = min(self.candidates, logits.shape[-1])\n            if not candidates >= 2:\n                raise ValueError(  # pragma: no cover\n                    f""expected the model output to have atleast 2 classes, got {logits.shape[-1]}""\n                )\n            logging.info(f""Only testing the top-{candidates} classes"")\n            classes = classes[:, :candidates]\n\n        N = len(x)\n        rows = range(N)\n\n        loss_fun = self._get_loss_fn(model, classes)\n        loss_aux_and_grad = ep.value_and_grad_fn(x, loss_fun, has_aux=True)\n\n        x0 = x\n        p_total = ep.zeros_like(x)\n        for _ in range(self.steps):\n            # let\'s first get the logits using k = 1 to see if we are done\n            diffs = [loss_aux_and_grad(x, 1)]\n            _, (_, logits), _ = diffs[0]\n\n            is_adv = criterion(x, logits)\n            if is_adv.all():\n                break\n\n            # then run all the other k\'s as well\n            # we could avoid repeated forward passes and only repeat\n            # the backward pass, but this cannot currently be done in eagerpy\n            diffs += [loss_aux_and_grad(x, k) for k in range(2, candidates)]\n\n            # we don\'t need the logits\n            diffs_ = [(losses, grad) for _, (losses, _), grad in diffs]\n            losses = ep.stack([lo for lo, _ in diffs_], axis=1)\n            grads = ep.stack([g for _, g in diffs_], axis=1)\n            assert losses.shape == (N, candidates - 1)\n            assert grads.shape == (N, candidates - 1) + x0.shape[1:]\n\n            # calculate the distances\n            distances = self.get_distances(losses, grads)\n            assert distances.shape == (N, candidates - 1)\n\n            # determine the best directions\n            best = distances.argmin(axis=1)\n            distances = distances[rows, best]\n            losses = losses[rows, best]\n            grads = grads[rows, best]\n            assert distances.shape == (N,)\n            assert losses.shape == (N,)\n            assert grads.shape == x0.shape\n\n            # apply perturbation\n            distances = distances + 1e-4  # for numerical stability\n            p_step = self.get_perturbations(distances, grads)\n            assert p_step.shape == x0.shape\n\n            p_total += p_step\n            # don\'t do anything for those that are already adversarial\n            x = ep.where(\n                atleast_kd(is_adv, x.ndim), x, x0 + (1.0 + self.overshoot) * p_total\n            )\n            x = ep.clip(x, min_, max_)\n\n        return restore_type(x)\n\n    @abstractmethod\n    def get_distances(self, losses: ep.Tensor, grads: ep.Tensor) -> ep.Tensor:\n        ...\n\n    @abstractmethod\n    def get_perturbations(self, distances: ep.Tensor, grads: ep.Tensor) -> ep.Tensor:\n        ...\n\n\nclass L2DeepFoolAttack(DeepFoolAttack):\n    """"""A simple and fast gradient-based adversarial attack.\n\n    Implements the DeepFool L2 attack. [#Moos15]_\n\n    Args:\n        steps : Maximum number of steps to perform.\n        candidates : Limit on the number of the most likely classes that should\n            be considered. A small value is usually sufficient and much faster.\n        overshoot : How much to overshoot the boundary.\n        loss  Loss function to use inside the update function.\n\n    References:\n        .. [#Moos15] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard,\n            ""DeepFool: a simple and accurate method to fool deep neural\n            networks"", https://arxiv.org/abs/1511.04599\n\n    """"""\n\n    distance = l2\n\n    def get_distances(self, losses: ep.Tensor, grads: ep.Tensor) -> ep.Tensor:\n        return abs(losses) / (flatten(grads, keep=2).norms.l2(axis=-1) + 1e-8)\n\n    def get_perturbations(self, distances: ep.Tensor, grads: ep.Tensor) -> ep.Tensor:\n        return (\n            atleast_kd(\n                distances / (flatten(grads).norms.l2(axis=-1) + 1e-8), grads.ndim,\n            )\n            * grads\n        )\n\n\nclass LinfDeepFoolAttack(DeepFoolAttack):\n    """"""A simple and fast gradient-based adversarial attack.\n\n        Implements the `DeepFool`_ L-Infinity attack.\n\n        Args:\n            steps : Maximum number of steps to perform.\n            candidates : Limit on the number of the most likely classes that should\n                be considered. A small value is usually sufficient and much faster.\n            overshoot : How much to overshoot the boundary.\n            loss  Loss function to use inside the update function.\n\n\n        .. _DeepFool:\n                Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard,\n                ""DeepFool: a simple and accurate method to fool deep neural\n                networks"", https://arxiv.org/abs/1511.04599\n\n        """"""\n\n    distance = linf\n\n    def get_distances(self, losses: ep.Tensor, grads: ep.Tensor) -> ep.Tensor:\n        return abs(losses) / (flatten(grads, keep=2).abs().sum(axis=-1) + 1e-8)\n\n    def get_perturbations(self, distances: ep.Tensor, grads: ep.Tensor) -> ep.Tensor:\n        return atleast_kd(distances, grads.ndim) * grads.sign()\n'"
foolbox/attacks/ead.py,0,"b'from typing import Union, Tuple, Any, Optional\nfrom typing_extensions import Literal\n\nimport math\n\nimport eagerpy as ep\n\nfrom ..models import Model\n\nfrom ..criteria import Misclassification, TargetedMisclassification\n\nfrom ..distances import l1\n\nfrom ..devutils import atleast_kd, flatten\n\nfrom .base import MinimizationAttack\nfrom .base import get_criterion\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\nclass EADAttack(MinimizationAttack):\n    """"""Implementation of the EAD Attack with EN Decision Rule. [#Chen18]_\n\n    Args:\n        binary_search_steps : Number of steps to perform in the binary search\n            over the const c.\n        steps : Number of optimization steps within each binary search step.\n        initial_stepsize : Initial stepsize to update the examples.\n        confidence : Confidence required for an example to be marked as adversarial.\n            Controls the gap between example and decision boundary.\n        initial_const : Initial value of the const c with which the binary search starts.\n        regularization : Controls the L1 regularization.\n        decision_rule : Rule according to which the best adversarial examples are selected.\n            They either minimize the L1 or ElasticNet distance.\n        abort_early : Stop inner search as soons as an adversarial example has been found.\n            Does not affect the binary search over the const c.\n\n    References:\n        .. [#Chen18] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh,\n        ""EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples"",\n        https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16893\n    """"""\n\n    distance = l1\n\n    def __init__(\n        self,\n        binary_search_steps: int = 9,\n        steps: int = 10000,\n        initial_stepsize: float = 1e-2,\n        confidence: float = 0.0,\n        initial_const: float = 1e-3,\n        regularization: float = 1e-2,\n        decision_rule: Union[Literal[""EN""], Literal[""L1""]] = ""EN"",\n        abort_early: bool = True,\n    ):\n        if decision_rule not in (""EN"", ""L1""):\n            raise ValueError(""invalid decision rule"")\n\n        self.binary_search_steps = binary_search_steps\n        self.steps = steps\n        self.confidence = confidence\n        self.initial_stepsize = initial_stepsize\n        self.regularization = regularization\n        self.initial_const = initial_const\n        self.abort_early = abort_early\n        self.decision_rule = decision_rule\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Misclassification, TargetedMisclassification, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        N = len(x)\n\n        if isinstance(criterion_, Misclassification):\n            targeted = False\n            classes = criterion_.labels\n            change_classes_logits = self.confidence\n        elif isinstance(criterion_, TargetedMisclassification):\n            targeted = True\n            classes = criterion_.target_classes\n            change_classes_logits = -self.confidence\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        def is_adversarial(perturbed: ep.Tensor, logits: ep.Tensor) -> ep.Tensor:\n            if change_classes_logits != 0:\n                logits += ep.onehot_like(logits, classes, value=change_classes_logits)\n            return criterion_(perturbed, logits)\n\n        if classes.shape != (N,):\n            name = ""target_classes"" if targeted else ""labels""\n            raise ValueError(\n                f""expected {name} to have shape ({N},), got {classes.shape}""\n            )\n\n        min_, max_ = model.bounds\n        rows = range(N)\n\n        def loss_fun(y_k: ep.Tensor, consts: ep.Tensor) -> Tuple[ep.Tensor, ep.Tensor]:\n            assert y_k.shape == x.shape\n            assert consts.shape == (N,)\n\n            logits = model(y_k)\n\n            if targeted:\n                c_minimize = _best_other_classes(logits, classes)\n                c_maximize = classes\n            else:\n                c_minimize = classes\n                c_maximize = _best_other_classes(logits, classes)\n\n            is_adv_loss = logits[rows, c_minimize] - logits[rows, c_maximize]\n            assert is_adv_loss.shape == (N,)\n\n            is_adv_loss = is_adv_loss + self.confidence\n            is_adv_loss = ep.maximum(0, is_adv_loss)\n            is_adv_loss = is_adv_loss * consts\n\n            squared_norms = flatten(y_k - x).square().sum(axis=-1)\n            loss = is_adv_loss.sum() + squared_norms.sum()\n            return loss, logits\n\n        loss_aux_and_grad = ep.value_and_grad_fn(x, loss_fun, has_aux=True)\n\n        consts = self.initial_const * ep.ones(x, (N,))\n        lower_bounds = ep.zeros(x, (N,))\n        upper_bounds = ep.inf * ep.ones(x, (N,))\n\n        best_advs = ep.zeros_like(x)\n        best_advs_norms = ep.ones(x, (N,)) * ep.inf\n\n        # the binary search searches for the smallest consts that produce adversarials\n        for binary_search_step in range(self.binary_search_steps):\n            if (\n                binary_search_step == self.binary_search_steps - 1\n                and self.binary_search_steps >= 10\n            ):\n                # in the last iteration, repeat the search once\n                consts = ep.minimum(upper_bounds, 1e10)\n\n            # create a new optimizer find the delta that minimizes the loss\n            x_k = x\n            y_k = x\n\n            found_advs = ep.full(\n                x, (N,), value=False\n            ).bool()  # found adv with the current consts\n            loss_at_previous_check = ep.inf\n\n            for iteration in range(self.steps):\n                # square-root learning rate decay\n                stepsize = self.initial_stepsize * (1.0 - iteration / self.steps) ** 0.5\n\n                loss, logits, gradient = loss_aux_and_grad(y_k, consts)\n\n                x_k_old = x_k\n                x_k = _project_shrinkage_thresholding(\n                    y_k - stepsize * gradient, x, self.regularization, min_, max_\n                )\n                y_k = x_k + iteration / (iteration + 3.0) * (x_k - x_k_old)\n\n                if self.abort_early and iteration % (math.ceil(self.steps / 10)) == 0:\n                    # after each tenth of the iterations, check progress\n                    if not loss.item() <= 0.9999 * loss_at_previous_check:\n                        break  # stop optimization if there has been no progress\n                    loss_at_previous_check = loss.item()\n\n                found_advs_iter = is_adversarial(x_k, logits)\n\n                best_advs, best_advs_norms = _apply_decision_rule(\n                    self.decision_rule,\n                    self.regularization,\n                    best_advs,\n                    best_advs_norms,\n                    x_k,\n                    x,\n                    found_advs_iter,\n                )\n\n                found_advs = ep.logical_or(found_advs, found_advs_iter)\n\n            upper_bounds = ep.where(found_advs, consts, upper_bounds)\n            lower_bounds = ep.where(found_advs, lower_bounds, consts)\n\n            consts_exponential_search = consts * 10\n            consts_binary_search = (lower_bounds + upper_bounds) / 2\n            consts = ep.where(\n                ep.isinf(upper_bounds), consts_exponential_search, consts_binary_search\n            )\n\n        return restore_type(best_advs)\n\n\ndef _best_other_classes(logits: ep.Tensor, exclude: ep.Tensor) -> ep.Tensor:\n    other_logits = logits - ep.onehot_like(logits, exclude, value=ep.inf)\n    return other_logits.argmax(axis=-1)\n\n\ndef _apply_decision_rule(\n    decision_rule: Union[Literal[""EN""], Literal[""L1""]],\n    beta: float,\n    best_advs: ep.Tensor,\n    best_advs_norms: ep.Tensor,\n    x_k: ep.Tensor,\n    x: ep.Tensor,\n    found_advs: ep.Tensor,\n) -> Tuple[ep.Tensor, ep.Tensor]:\n    if decision_rule == ""EN"":\n        norms = beta * flatten(x_k - x).abs().sum(axis=-1) + flatten(\n            x_k - x\n        ).square().sum(axis=-1)\n    else:\n        # decision rule = L1\n        norms = flatten(x_k - x).abs().sum(axis=-1)\n\n    new_best = ep.logical_and(norms < best_advs_norms, found_advs)\n    new_best_kd = atleast_kd(new_best, best_advs.ndim)\n    best_advs = ep.where(new_best_kd, x_k, best_advs)\n    best_advs_norms = ep.where(new_best, norms, best_advs_norms)\n\n    return best_advs, best_advs_norms\n\n\ndef _project_shrinkage_thresholding(\n    z: ep.Tensor, x0: ep.Tensor, regularization: float, min_: float, max_: float\n) -> ep.Tensor:\n    """"""Performs the element-wise projected shrinkage-thresholding\n    operation""""""\n\n    upper_mask = z - x0 > regularization\n    lower_mask = z - x0 < -regularization\n\n    projection = ep.where(upper_mask, ep.minimum(z - regularization, max_), x0)\n    projection = ep.where(lower_mask, ep.maximum(z + regularization, min_), projection)\n\n    return projection\n'"
foolbox/attacks/fast_gradient_method.py,0,"b'from .gradient_descent_base import L1BaseGradientDescent\nfrom .gradient_descent_base import L2BaseGradientDescent\nfrom .gradient_descent_base import LinfBaseGradientDescent\n\n\nclass L1FastGradientAttack(L1BaseGradientDescent):\n    """"""Fast Gradient Method (FGM) using the L1 norm\n\n    Args:\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n    """"""\n\n    def __init__(self, *, random_start: bool = False):\n        super().__init__(\n            rel_stepsize=1.0, steps=1, random_start=random_start,\n        )\n\n\nclass L2FastGradientAttack(L2BaseGradientDescent):\n    """"""Fast Gradient Method (FGM)\n\n    Args:\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n    """"""\n\n    def __init__(self, *, random_start: bool = False):\n        super().__init__(\n            rel_stepsize=1.0, steps=1, random_start=random_start,\n        )\n\n\nclass LinfFastGradientAttack(LinfBaseGradientDescent):\n    """"""Fast Gradient Sign Method (FGSM)\n\n    Args:\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n    """"""\n\n    def __init__(self, *, random_start: bool = False):\n        super().__init__(\n            rel_stepsize=1.0, steps=1, random_start=random_start,\n        )\n'"
foolbox/attacks/gen_attack.py,0,"b'from typing import Optional, Any, Tuple, Union\nimport numpy as np\nimport eagerpy as ep\n\nfrom ..devutils import atleast_kd\n\nfrom ..models import Model\n\nfrom ..criteria import TargetedMisclassification\n\nfrom ..distances import linf\n\nfrom .base import FixedEpsilonAttack\nfrom .base import T\nfrom .base import get_channel_axis\nfrom .base import raise_if_kwargs\nimport math\n\nfrom .gen_attack_utils import rescale_images\n\n\nclass GenAttack(FixedEpsilonAttack):\n    """"""A black-box algorithm for L-infinity adversarials. [#Alz18]_\n\n    This attack is performs a genetic search in order to find an adversarial\n    perturbation in a black-box scenario in as few queries as possible.\n\n    References:\n        .. [#Alz18] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang,\n           Cho-Jui Hsieh, Mani Srivastava,\n           ""GenAttack: Practical Black-box Attacks with Gradient-Free\n           Optimization"",\n           https://arxiv.org/abs/1805.11090\n\n    """"""\n\n    def __init__(\n        self,\n        *,\n        steps: int = 1000,\n        population: int = 10,\n        mutation_probability: float = 0.10,\n        mutation_range: float = 0.15,\n        sampling_temperature: float = 0.3,\n        channel_axis: Optional[int] = None,\n        reduced_dims: Optional[Tuple[int, int]] = None,\n    ):\n        self.steps = steps\n        self.population = population\n        self.min_mutation_probability = mutation_probability\n        self.min_mutation_range = mutation_range\n        self.sampling_temperature = sampling_temperature\n        self.channel_axis = channel_axis\n        self.reduced_dims = reduced_dims\n\n    distance = linf\n\n    def apply_noise(\n        self,\n        x: ep.TensorType,\n        noise: ep.TensorType,\n        epsilon: float,\n        channel_axis: Optional[int],\n    ) -> ep.TensorType:\n        if noise.shape != x.shape and channel_axis is not None:\n            # upscale noise\n\n            noise = rescale_images(noise, x.shape, channel_axis)\n\n        return ep.clip(noise + x, -epsilon, +epsilon)\n\n    def choice(\n        self, a: int, size: Union[int, ep.TensorType], replace: bool, p: ep.TensorType\n    ) -> Any:\n        p = p.numpy()\n        x = np.random.choice(a, size, replace, p)\n        return x\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: TargetedMisclassification,\n        *,\n        epsilon: float,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        N = len(x)\n\n        if isinstance(criterion, TargetedMisclassification):\n            classes = criterion.target_classes\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        if classes.shape != (N,):\n            raise ValueError(\n                f""expected target_classes to have shape ({N},), got {classes.shape}""\n            )\n\n        noise_shape: Union[Tuple[int, int, int, int], Tuple[int, ...]]\n        channel_axis: Optional[int] = None\n        if self.reduced_dims is not None:\n            if x.ndim != 4:\n                raise NotImplementedError(\n                    ""only implemented for inputs with two spatial dimensions""\n                    "" (and one channel and one batch dimension)""\n                )\n\n            if self.channel_axis is None:\n                maybe_axis = get_channel_axis(model, x.ndim)\n                if maybe_axis is None:\n                    raise ValueError(\n                        ""cannot infer the data_format from the model, please""\n                        "" specify channel_axis when initializing the attack""\n                    )\n                else:\n                    channel_axis = maybe_axis\n            else:\n                channel_axis = self.channel_axis % x.ndim\n\n            if channel_axis == 1:\n                noise_shape = (x.shape[1], *self.reduced_dims)\n            elif channel_axis == 3:\n                noise_shape = (*self.reduced_dims, x.shape[3])\n            else:\n                raise ValueError(\n                    ""expected \'channel_axis\' to be 1 or 3, got {channel_axis}""\n                )\n        else:\n            noise_shape = x.shape[1:]  # pragma: no cover\n\n        def is_adversarial(logits: ep.TensorType) -> ep.TensorType:\n            return ep.argmax(logits, 1) == classes\n\n        num_plateaus = ep.zeros(x, len(x))\n        mutation_probability = (\n            ep.ones_like(num_plateaus) * self.min_mutation_probability\n        )\n        mutation_range = ep.ones_like(num_plateaus) * self.min_mutation_range\n\n        noise_pops = ep.uniform(\n            x, (N, self.population, *noise_shape), -epsilon, epsilon\n        )\n\n        def calculate_fitness(logits: ep.TensorType) -> ep.TensorType:\n            first = logits[range(N), classes]\n            second = ep.log(ep.exp(logits).sum(1) - first)\n\n            return first - second\n\n        n_its_wo_change = ep.zeros(x, (N,))\n        for step in range(self.steps):\n            fitness_l, is_adv_l = [], []\n\n            for i in range(self.population):\n                it = self.apply_noise(x, noise_pops[:, i], epsilon, channel_axis)\n                logits = model(it)\n                f = calculate_fitness(logits)\n                a = is_adversarial(logits)\n                fitness_l.append(f)\n                is_adv_l.append(a)\n\n            fitness = ep.stack(fitness_l)\n            is_adv = ep.stack(is_adv_l, 1)\n            elite_idxs = ep.argmax(fitness, 0)\n\n            elite_noise = noise_pops[range(N), elite_idxs]\n            is_adv = is_adv[range(N), elite_idxs]\n\n            # early stopping\n            if is_adv.all():\n                return restore_type(  # pragma: no cover\n                    self.apply_noise(x, elite_noise, epsilon, channel_axis)\n                )\n\n            probs = ep.softmax(fitness / self.sampling_temperature, 0)\n            parents_idxs = np.stack(\n                [\n                    self.choice(\n                        self.population,\n                        2 * self.population - 2,\n                        replace=True,\n                        p=probs[:, i],\n                    )\n                    for i in range(N)\n                ],\n                1,\n            )\n\n            mutations = [\n                ep.uniform(\n                    x,\n                    noise_shape,\n                    -mutation_range[i].item() * epsilon,\n                    mutation_range[i].item() * epsilon,\n                )\n                for i in range(N)\n            ]\n\n            new_noise_pops = [elite_noise]\n            for i in range(0, self.population - 1):\n                parents_1 = noise_pops[range(N), parents_idxs[2 * i]]\n                parents_2 = noise_pops[range(N), parents_idxs[2 * i + 1]]\n\n                # calculate crossover\n                p = probs[parents_idxs[2 * i], range(N)] / (\n                    probs[parents_idxs[2 * i], range(N)]\n                    + probs[parents_idxs[2 * i + 1], range(N)]\n                )\n                p = atleast_kd(p, x.ndim)\n                p = ep.tile(p, (1, *noise_shape))\n\n                crossover_mask = ep.uniform(p, p.shape, 0, 1) < p\n                children = ep.where(crossover_mask, parents_1, parents_2)\n\n                # calculate mutation\n                mutation_mask = ep.uniform(children, children.shape)\n                mutation_mask = mutation_mask <= atleast_kd(\n                    mutation_probability, children.ndim\n                )\n                children = ep.where(mutation_mask, children + mutations[i], children)\n\n                # project back to epsilon range\n                children = ep.clip(children, -epsilon, epsilon)\n\n                new_noise_pops.append(children)\n\n            noise_pops = ep.stack(new_noise_pops, 1)\n\n            # increase num_plateaus if fitness does not improve\n            # for 100 consecutive steps\n            n_its_wo_change = ep.where(\n                elite_idxs == 0, n_its_wo_change + 1, ep.zeros_like(n_its_wo_change)\n            )\n            num_plateaus = ep.where(\n                n_its_wo_change >= 100, num_plateaus + 1, num_plateaus\n            )\n            n_its_wo_change = ep.where(\n                n_its_wo_change >= 100, ep.zeros_like(n_its_wo_change), n_its_wo_change\n            )\n\n            mutation_probability = ep.maximum(\n                self.min_mutation_probability,\n                0.5 * ep.exp(math.log(0.9) * ep.ones_like(num_plateaus) * num_plateaus),\n            )\n            mutation_range = ep.maximum(\n                self.min_mutation_range,\n                0.5 * ep.exp(math.log(0.9) * ep.ones_like(num_plateaus) * num_plateaus),\n            )\n\n        return restore_type(self.apply_noise(x, elite_noise, epsilon, channel_axis))\n'"
foolbox/attacks/gen_attack_utils.py,0,"b'from typing import Union, List, Tuple\nimport eagerpy as ep\n\n\ndef rescale_jax(x: ep.JAXTensor, target_shape: List[int]) -> ep.JAXTensor:\n    # img must be in channel_last format\n\n    # modified according to https://github.com/google/jax/issues/862\n    import jax.numpy as np\n\n    img = x.raw\n\n    resize_rates = (target_shape[1] / x.shape[1], target_shape[2] / x.shape[2])\n\n    def interpolate_bilinear(  # type: ignore\n        im: np.ndarray, rows: np.ndarray, cols: np.ndarray\n    ) -> np.ndarray:\n        # based on http://stackoverflow.com/a/12729229\n        col_lo = np.floor(cols).astype(int)\n        col_hi = col_lo + 1\n        row_lo = np.floor(rows).astype(int)\n        row_hi = row_lo + 1\n\n        def cclip(cols: np.ndarray) -> np.ndarray:  # type: ignore\n            return np.clip(cols, 0, ncols - 1)\n\n        def rclip(rows: np.ndarray) -> np.ndarray:  # type: ignore\n            return np.clip(rows, 0, nrows - 1)\n\n        nrows, ncols = im.shape[-3:-1]\n\n        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n\n        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n\n        return wa * Ia + wb * Ib + wc * Ic + wd * Id\n\n    nrows, ncols = img.shape[-3:-1]\n    deltas = (0.5 / resize_rates[0], 0.5 / resize_rates[1])\n\n    rows = np.linspace(deltas[0], nrows - deltas[0], np.int32(resize_rates[0] * nrows))\n    cols = np.linspace(deltas[1], ncols - deltas[1], np.int32(resize_rates[1] * ncols))\n    rows_grid, cols_grid = np.meshgrid(rows - 0.5, cols - 0.5, indexing=""ij"")\n\n    img_resize_vec = interpolate_bilinear(img, rows_grid.flatten(), cols_grid.flatten())\n    img_resize = img_resize_vec.reshape(\n        img.shape[:-3] + (len(rows), len(cols)) + img.shape[-1:]\n    )\n\n    return ep.JAXTensor(img_resize)\n\n\ndef rescale_numpy(x: ep.NumPyTensor, target_shape: List[int]) -> ep.NumPyTensor:\n    import numpy as np\n\n    img = x.raw\n\n    resize_rates = (target_shape[1] / x.shape[1], target_shape[2] / x.shape[2])\n\n    def interpolate_bilinear(  # type: ignore\n        im: np.ndarray, rows: np.ndarray, cols: np.ndarray\n    ) -> np.ndarray:\n        # based on http://stackoverflow.com/a/12729229\n        col_lo = np.floor(cols).astype(int)\n        col_hi = col_lo + 1\n        row_lo = np.floor(rows).astype(int)\n        row_hi = row_lo + 1\n\n        def cclip(cols: np.ndarray) -> np.ndarray:  # type: ignore\n            return np.clip(cols, 0, ncols - 1)\n\n        def rclip(rows: np.ndarray) -> np.ndarray:  # type: ignore\n            return np.clip(rows, 0, nrows - 1)\n\n        nrows, ncols = im.shape[-3:-1]\n\n        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n\n        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n\n        return wa * Ia + wb * Ib + wc * Ic + wd * Id\n\n    nrows, ncols = img.shape[-3:-1]\n    deltas = (0.5 / resize_rates[0], 0.5 / resize_rates[1])\n\n    rows = np.linspace(deltas[0], nrows - deltas[0], np.int32(resize_rates[0] * nrows))\n    cols = np.linspace(deltas[1], ncols - deltas[1], np.int32(resize_rates[1] * ncols))\n    rows_grid, cols_grid = np.meshgrid(rows - 0.5, cols - 0.5, indexing=""ij"")\n\n    img_resize_vec = interpolate_bilinear(img, rows_grid.flatten(), cols_grid.flatten())\n    img_resize = img_resize_vec.reshape(\n        img.shape[:-3] + (len(rows), len(cols)) + img.shape[-1:]\n    )\n\n    return ep.NumPyTensor(img_resize)\n\n\ndef rescale_tensorflow(\n    x: ep.TensorFlowTensor, target_shape: List[int]\n) -> ep.TensorFlowTensor:\n    import tensorflow\n\n    img = x.raw\n\n    img_resized = tensorflow.image.resize(img, size=target_shape[1:-1])\n\n    return ep.TensorFlowTensor(img_resized)\n\n\ndef rescale_pytorch(x: ep.PyTorchTensor, target_shape: List[int]) -> ep.PyTorchTensor:\n    import torch\n\n    img = x.raw\n\n    img_resized = torch.nn.functional.interpolate(\n        img, size=target_shape[2:], mode=""bilinear"", align_corners=False\n    )\n\n    return ep.PyTorchTensor(img_resized)\n\n\ndef swap_axes(x: ep.TensorType, dim0: int, dim1: int) -> ep.TensorType:\n    assert dim0 < x.ndim\n    assert dim1 < x.ndim\n\n    axes = list(range(x.ndim))\n    axes[dim0] = dim1\n    axes[dim1] = dim0\n\n    return ep.transpose(x, tuple(axes))\n\n\ndef rescale_images(\n    x: ep.TensorType, target_shape: Union[Tuple[int, ...], List[int]], channel_axis: int\n) -> ep.TensorType:\n    target_shape = list(target_shape)\n\n    if channel_axis < 0:\n        channel_axis = x.ndim - 1 + channel_axis\n\n    if isinstance(x, ep.PyTorchTensor):\n        if channel_axis != 1:\n            x = swap_axes(x, channel_axis, 1)  # type: ignore\n\n            target_shape[channel_axis], target_shape[1] = (\n                target_shape[1],\n                target_shape[channel_axis],\n            )\n\n        x = rescale_pytorch(x, target_shape)  # type: ignore\n\n        if channel_axis != 1:\n            x = swap_axes(x, channel_axis, 1)  # type: ignore\n\n    elif isinstance(x, ep.TensorFlowTensor):\n        if channel_axis != x.ndim - 1:\n            x = swap_axes(x, channel_axis, x.ndim - 1)  # type: ignore\n\n            target_shape[channel_axis], target_shape[x.ndim - 1] = (\n                target_shape[x.ndim - 1],\n                target_shape[channel_axis],\n            )\n\n        x = rescale_tensorflow(x, target_shape)  # type: ignore\n\n        if channel_axis != x.ndim - 1:\n            x = swap_axes(x, channel_axis, x.ndim - 1)  # type: ignore\n\n    elif isinstance(x, ep.NumPyTensor):\n        if channel_axis != x.ndim - 1:\n            x = swap_axes(x, channel_axis, x.ndim - 1)  # type: ignore\n\n            target_shape[channel_axis], target_shape[x.ndim - 1] = (\n                target_shape[x.ndim - 1],\n                target_shape[channel_axis],\n            )\n\n        x = rescale_numpy(x, target_shape)  # type: ignore\n        if channel_axis != x.ndim - 1:\n            x = swap_axes(x, channel_axis, x.ndim - 1)  # type: ignore\n\n    elif isinstance(x, ep.JAXTensor):\n        if channel_axis != x.ndim - 1:\n            x = swap_axes(x, channel_axis, x.ndim - 1)  # type: ignore\n\n            target_shape[channel_axis], target_shape[x.ndim - 1] = (\n                target_shape[x.ndim - 1],\n                target_shape[channel_axis],\n            )\n\n        x = rescale_jax(x, target_shape)  # type: ignore\n        if channel_axis != x.ndim - 1:\n            x = swap_axes(x, channel_axis, x.ndim - 1)  # type: ignore\n\n    return x\n'"
foolbox/attacks/gradient_descent_base.py,0,"b'from typing import Union, Any, Optional, Callable, Tuple\nfrom abc import ABC, abstractmethod\nimport eagerpy as ep\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..types import Bounds\n\nfrom ..models.base import Model\n\nfrom ..criteria import Misclassification\n\nfrom ..distances import l1, l2, linf\n\nfrom .base import FixedEpsilonAttack\nfrom .base import T\nfrom .base import get_criterion\nfrom .base import raise_if_kwargs\n\n\nclass BaseGradientDescent(FixedEpsilonAttack, ABC):\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float,\n        abs_stepsize: Optional[float] = None,\n        steps: int,\n        random_start: bool,\n    ):\n        self.rel_stepsize = rel_stepsize\n        self.abs_stepsize = abs_stepsize\n        self.steps = steps\n        self.random_start = random_start\n\n    def get_loss_fn(\n        self, model: Model, labels: ep.Tensor\n    ) -> Callable[[ep.Tensor], ep.Tensor]:\n        # can be overridden by users\n        def loss_fn(inputs: ep.Tensor) -> ep.Tensor:\n            logits = model(inputs)\n            return ep.crossentropy(logits, labels).sum()\n\n        return loss_fn\n\n    def value_and_grad(\n        # can be overridden by users\n        self,\n        loss_fn: Callable[[ep.Tensor], ep.Tensor],\n        x: ep.Tensor,\n    ) -> Tuple[ep.Tensor, ep.Tensor]:\n        return ep.value_and_grad(loss_fn, x)\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Misclassification, T],\n        *,\n        epsilon: float,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x0, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        if not isinstance(criterion_, Misclassification):\n            raise ValueError(""unsupported criterion"")\n\n        labels = criterion_.labels\n        loss_fn = self.get_loss_fn(model, labels)\n\n        if self.abs_stepsize is None:\n            stepsize = self.rel_stepsize * epsilon\n        else:\n            stepsize = self.abs_stepsize\n\n        x = x0\n\n        if self.random_start:\n            x = self.get_random_start(x0, epsilon)\n            x = ep.clip(x, *model.bounds)\n        else:\n            x = x0\n\n        for _ in range(self.steps):\n            _, gradients = self.value_and_grad(loss_fn, x)\n            gradients = self.normalize(gradients, x=x, bounds=model.bounds)\n            x = x + stepsize * gradients\n            x = self.project(x, x0, epsilon)\n            x = ep.clip(x, *model.bounds)\n\n        return restore_type(x)\n\n    @abstractmethod\n    def get_random_start(self, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        ...\n\n    @abstractmethod\n    def normalize(\n        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n    ) -> ep.Tensor:\n        ...\n\n    @abstractmethod\n    def project(self, x: ep.Tensor, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        ...\n\n\ndef clip_lp_norms(x: ep.Tensor, *, norm: float, p: float) -> ep.Tensor:\n    assert 0 < p < ep.inf\n    norms = flatten(x).norms.lp(p=p, axis=-1)\n    norms = ep.maximum(norms, 1e-12)  # avoid divsion by zero\n    factor = ep.minimum(1, norm / norms)  # clipping -> decreasing but not increasing\n    factor = atleast_kd(factor, x.ndim)\n    return x * factor\n\n\ndef normalize_lp_norms(x: ep.Tensor, *, p: float) -> ep.Tensor:\n    assert 0 < p < ep.inf\n    norms = flatten(x).norms.lp(p=p, axis=-1)\n    norms = ep.maximum(norms, 1e-12)  # avoid divsion by zero\n    factor = 1 / norms\n    factor = atleast_kd(factor, x.ndim)\n    return x * factor\n\n\ndef uniform_l1_n_balls(dummy: ep.Tensor, batch_size: int, n: int) -> ep.Tensor:\n    # https://mathoverflow.net/a/9188\n    u = ep.uniform(dummy, (batch_size, n))\n    v = u.sort(axis=-1)\n    vp = ep.concatenate([ep.zeros(v, (batch_size, 1)), v[:, : n - 1]], axis=-1)\n    assert v.shape == vp.shape\n    x = v - vp\n    sign = ep.uniform(dummy, (batch_size, n), low=-1.0, high=1.0).sign()\n    return sign * x\n\n\ndef uniform_l2_n_spheres(dummy: ep.Tensor, batch_size: int, n: int) -> ep.Tensor:\n    x = ep.normal(dummy, (batch_size, n + 1))\n    r = x.norms.l2(axis=-1, keepdims=True)\n    s = x / r\n    return s\n\n\ndef uniform_l2_n_balls(dummy: ep.Tensor, batch_size: int, n: int) -> ep.Tensor:\n    """"""Sampling from the n-ball\n\n    Implementation of the algorithm proposed by Voelker et al. [#Voel17]_\n\n    References:\n        .. [#Voel17] Voelker et al., 2017, Efficiently sampling vectors and coordinates\n            from the n-sphere and n-ball\n            http://compneuro.uwaterloo.ca/files/publications/voelker.2017.pdf\n    """"""\n    s = uniform_l2_n_spheres(dummy, batch_size, n + 1)\n    b = s[:, :n]\n    return b\n\n\nclass L1BaseGradientDescent(BaseGradientDescent):\n    distance = l1\n\n    def get_random_start(self, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        batch_size, n = flatten(x0).shape\n        r = uniform_l1_n_balls(x0, batch_size, n).reshape(x0.shape)\n        return x0 + epsilon * r\n\n    def normalize(\n        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n    ) -> ep.Tensor:\n        return normalize_lp_norms(gradients, p=1)\n\n    def project(self, x: ep.Tensor, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        return x0 + clip_lp_norms(x - x0, norm=epsilon, p=1)\n\n\nclass L2BaseGradientDescent(BaseGradientDescent):\n    distance = l2\n\n    def get_random_start(self, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        batch_size, n = flatten(x0).shape\n        r = uniform_l2_n_balls(x0, batch_size, n).reshape(x0.shape)\n        return x0 + epsilon * r\n\n    def normalize(\n        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n    ) -> ep.Tensor:\n        return normalize_lp_norms(gradients, p=2)\n\n    def project(self, x: ep.Tensor, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        return x0 + clip_lp_norms(x - x0, norm=epsilon, p=2)\n\n\nclass LinfBaseGradientDescent(BaseGradientDescent):\n    distance = linf\n\n    def get_random_start(self, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        return x0 + ep.uniform(x0, x0.shape, -epsilon, epsilon)\n\n    def normalize(\n        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n    ) -> ep.Tensor:\n        return gradients.sign()\n\n    def project(self, x: ep.Tensor, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        return x0 + ep.clip(x - x0, -epsilon, epsilon)\n'"
foolbox/attacks/inversion.py,0,"b'from typing import Union, Any, Optional\nimport eagerpy as ep\n\nfrom ..criteria import Criterion\n\nfrom ..models import Model\n\nfrom .base import FlexibleDistanceMinimizationAttack\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\nclass InversionAttack(FlexibleDistanceMinimizationAttack):\n    """"""Creates ""negative images"" by inverting the pixel values. [#Hos16]_\n\n    References:\n        .. [#Hos16] Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal, Radha Poovendran,\n               ""On the Limitation of Convolutional Neural Networks in Recognizing\n               Negative Images"",\n               https://arxiv.org/abs/1607.02533\n    """"""\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Criterion, Any] = None,\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, criterion, kwargs\n\n        min_, max_ = model.bounds\n        x = min_ + max_ - x\n        return restore_type(x)\n'"
foolbox/attacks/newtonfool.py,0,"b'from typing import Union, Tuple, Any, Optional\nimport eagerpy as ep\n\nfrom ..models import Model\n\nfrom ..criteria import Misclassification\n\nfrom ..distances import l2\n\nfrom ..devutils import atleast_kd, flatten\n\nfrom .base import MinimizationAttack\nfrom .base import get_criterion\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\nclass NewtonFoolAttack(MinimizationAttack):\n    """"""Implementation of the NewtonFool Attack. [#Jang17]_\n\n    Args:\n        steps : Number of update steps to perform.\n        step_size : Size of each update step.\n\n    References:\n        .. [#Jang17] Uyeong Jang et al., ""Objective Metrics and Gradient Descent\n            Algorithms for Adversarial Examples in Machine Learning"",\n            https://dl.acm.org/citation.cfm?id=3134635\n    """"""\n\n    distance = l2\n\n    def __init__(self, steps: int = 100, stepsize: float = 0.01):\n        self.steps = steps\n        self.stepsize = stepsize\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Misclassification, T],\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        N = len(x)\n\n        if isinstance(criterion_, Misclassification):\n            classes = criterion_.labels\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        if classes.shape != (N,):\n            raise ValueError(\n                f""expected labels to have shape ({N},), got {classes.shape}""\n            )\n\n        min_, max_ = model.bounds\n\n        x_l2_norm = flatten(x.square()).sum(1)\n\n        def loss_fun(x: ep.Tensor) -> Tuple[ep.Tensor, Tuple[ep.Tensor, ep.Tensor]]:\n            logits = model(x)\n            scores = ep.softmax(logits)\n            pred_scores = scores[range(N), classes]\n            loss = pred_scores.sum()\n            return loss, (scores, pred_scores)\n\n        for i in range(self.steps):\n            # (1) get the scores and gradients\n            _, (scores, pred_scores), gradients = ep.value_aux_and_grad(loss_fun, x)\n\n            pred = scores.argmax(-1)\n            num_classes = scores.shape[-1]\n\n            # (2) calculate gradient norm\n            gradients_l2_norm = flatten(gradients.square()).sum(1)\n\n            # (3) calculate delta\n            a = self.stepsize * x_l2_norm * gradients_l2_norm\n            b = pred_scores - 1.0 / num_classes\n\n            delta = ep.minimum(a, b)\n\n            # (4) stop the attack if an adversarial example has been found\n            # this is not described in the paper but otherwise once the prob. drops\n            # below chance level the likelihood is not decreased but increased\n            is_not_adversarial = (pred == classes).float32()\n            delta *= is_not_adversarial\n\n            # (5) calculate & apply current perturbation\n            a = atleast_kd(delta / gradients_l2_norm.square(), gradients.ndim)\n            x -= a * gradients\n\n            x = ep.clip(x, min_, max_)\n\n        return restore_type(x)\n'"
foolbox/attacks/projected_gradient_descent.py,0,"b'from typing import Optional\n\nfrom .gradient_descent_base import L1BaseGradientDescent\nfrom .gradient_descent_base import L2BaseGradientDescent\nfrom .gradient_descent_base import LinfBaseGradientDescent\n\n\nclass L1ProjectedGradientDescentAttack(L1BaseGradientDescent):\n    """"""L1 Projected Gradient Descent\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps to perform.\n        random_start : Whether the perturbation is initialized randomly or starts at zero.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float = 0.025,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 50,\n        random_start: bool = True,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n\n\nclass L2ProjectedGradientDescentAttack(L2BaseGradientDescent):\n    """"""L2 Projected Gradient Descent\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps to perform.\n        random_start : Whether the perturbation is initialized randomly or starts at zero.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float = 0.025,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 50,\n        random_start: bool = True,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n\n\nclass LinfProjectedGradientDescentAttack(LinfBaseGradientDescent):\n    """"""Linf Projected Gradient Descent\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon (defaults to 0.01 / 0.3).\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps to perform.\n        random_start : Whether the perturbation is initialized randomly or starts at zero.\n    """"""\n\n    def __init__(\n        self,\n        *,\n        rel_stepsize: float = 0.01 / 0.3,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 40,\n        random_start: bool = True,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n'"
foolbox/attacks/saltandpepper.py,0,"b'from typing import Optional, Any\nimport eagerpy as ep\n\nfrom ..criteria import Misclassification\n\nfrom ..distances import l2\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom .base import MinimizationAttack\nfrom .base import get_is_adversarial\nfrom .base import get_channel_axis\n\nfrom ..models.base import Model\nfrom .base import get_criterion\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\nclass SaltAndPepperNoiseAttack(MinimizationAttack):\n    """"""Increases the amount of salt and pepper noise until the input is misclassified.\n\n    Args:\n        steps : The number of steps to run.\n        across_channels : Whether the noise should be the same across all channels.\n        channel_axis : The axis across which the noise should be the same\n            (if across_channels is True). If None, will be automatically inferred\n            from the model if possible.\n    """"""\n\n    distance = l2\n\n    def __init__(\n        self,\n        steps: int = 1000,\n        across_channels: bool = True,\n        channel_axis: Optional[int] = None,\n    ):\n        self.steps = steps\n        self.across_channels = across_channels\n        self.channel_axis = channel_axis\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Misclassification,\n        *,\n        early_stop: Optional[float] = None,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x0, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        is_adversarial = get_is_adversarial(criterion_, model)\n\n        N = len(x0)\n        shape = list(x0.shape)\n\n        if self.across_channels and x0.ndim > 2:\n            if self.channel_axis is None:\n                channel_axis = get_channel_axis(model, x0.ndim)\n            else:\n                channel_axis = self.channel_axis % x0.ndim\n            if channel_axis is not None:\n                shape[channel_axis] = 1\n\n        min_, max_ = model.bounds\n        r = max_ - min_\n\n        result = x0\n        is_adv = is_adversarial(result)\n        best_advs_norms = ep.where(is_adv, ep.zeros(x0, N), ep.full(x0, N, ep.inf))\n        min_probability = ep.zeros(x0, N)\n        max_probability = ep.ones(x0, N)\n        stepsizes = max_probability / self.steps\n        p = stepsizes\n\n        for step in range(self.steps):\n            # add salt and pepper\n            u = ep.uniform(x0, tuple(shape))\n            p_ = atleast_kd(p, x0.ndim)\n            salt = (u >= 1 - p_ / 2).astype(x0.dtype) * r\n            pepper = -(u < p_ / 2).astype(x0.dtype) * r\n            x = x0 + salt + pepper\n            x = ep.clip(x, min_, max_)\n\n            # check if we found new best adversarials\n            norms = flatten(x).norms.l2(axis=-1)\n            closer = norms < best_advs_norms\n            is_adv = is_adversarial(x)  # TODO: ignore those that are not closer anyway\n            is_best_adv = ep.logical_and(is_adv, closer)\n\n            # update results and search space\n            result = ep.where(atleast_kd(is_best_adv, x.ndim), x, result)\n            best_advs_norms = ep.where(is_best_adv, norms, best_advs_norms)\n            min_probability = ep.where(is_best_adv, 0.5 * p, min_probability)\n            # we set max_probability a bit higher than p because the relationship\n            # between p and norms is not strictly monotonic\n            max_probability = ep.where(\n                is_best_adv, ep.minimum(p * 1.2, 1.0), max_probability\n            )\n            remaining = self.steps - step\n            stepsizes = ep.where(\n                is_best_adv, (max_probability - min_probability) / remaining, stepsizes\n            )\n            reset = p == max_probability\n            p = ep.where(ep.logical_or(is_best_adv, reset), min_probability, p)\n            p = ep.minimum(p + stepsizes, max_probability)\n\n        return restore_type(result)\n'"
foolbox/attacks/sparse_l1_descent_attack.py,0,"b'from typing import Optional\nimport eagerpy as ep\nimport numpy as np\n\nfrom ..devutils import flatten\nfrom ..devutils import atleast_kd\n\nfrom ..types import Bounds\n\nfrom .gradient_descent_base import L1BaseGradientDescent\nfrom .gradient_descent_base import normalize_lp_norms\n\n\nclass SparseL1DescentAttack(L1BaseGradientDescent):\n    """"""Sparse L1 Descent Attack [#Tra19]_.\n\n    Args:\n        rel_stepsize: Stepsize relative to epsilon.\n        abs_stepsize: If given, it takes precedence over rel_stepsize.\n        steps : Number of update steps.\n        random_start : Controls whether to randomly start within allowed epsilon ball.\n\n    References:\n        .. [#Tra19] Florian Tram\xc3\xa8r, Dan Boneh, ""Adversarial Training and\n        Robustness for Multiple Perturbations""\n        https://arxiv.org/abs/1904.13000\n    """"""\n\n    def normalize(\n        self, gradients: ep.Tensor, *, x: ep.Tensor, bounds: Bounds\n    ) -> ep.Tensor:\n        bad_pos = ep.logical_or(\n            ep.logical_and(x == bounds.lower, gradients < 0),\n            ep.logical_and(x == bounds.upper, gradients > 0),\n        )\n        gradients = ep.where(bad_pos, ep.zeros_like(gradients), gradients)\n\n        abs_gradients = gradients.abs()\n        quantiles = np.quantile(\n            flatten(abs_gradients).numpy(), q=self.quantile, axis=-1\n        )\n        keep = abs_gradients >= atleast_kd(\n            ep.from_numpy(gradients, quantiles), gradients.ndim\n        )\n        e = ep.where(keep, gradients.sign(), ep.zeros_like(gradients))\n        return normalize_lp_norms(e, p=1)\n\n    def project(self, x: ep.Tensor, x0: ep.Tensor, epsilon: float) -> ep.Tensor:\n        # based on https://github.com/ftramer/MultiRobustness/blob/ad41b63235d13b1b2a177c5f270ab9afa74eee69/pgd_attack.py#L110\n        delta = flatten(x - x0)\n        norms = delta.norms.l1(axis=-1)\n        if (norms <= epsilon).all():\n            return x\n\n        n, d = delta.shape\n        abs_delta = abs(delta)\n        mu = -ep.sort(-abs_delta, axis=-1)\n        cumsums = mu.cumsum(axis=-1)\n        js = 1.0 / ep.arange(x, 1, d + 1).astype(x.dtype)\n        temp = mu - js * (cumsums - epsilon)\n        guarantee_first = ep.arange(x, d).astype(x.dtype) / d\n        # guarantee_first are small values (< 1) that we add to the boolean\n        # tensor (only 0 and 1) to break the ties and always return the first\n        # argmin, i.e. the first value where the boolean tensor is 0\n        # (otherwise, this is not guaranteed on GPUs, see e.g. PyTorch)\n        rho = ep.argmin((temp > 0).astype(x.dtype) + guarantee_first, axis=-1)\n        theta = 1.0 / (1 + rho.astype(x.dtype)) * (cumsums[range(n), rho] - epsilon)\n        delta = delta.sign() * ep.maximum(abs_delta - theta[..., ep.newaxis], 0)\n        delta = delta.reshape(x.shape)\n        return x0 + delta\n\n    def __init__(\n        self,\n        *,\n        quantile: float = 0.99,\n        rel_stepsize: float = 0.2,\n        abs_stepsize: Optional[float] = None,\n        steps: int = 10,\n        random_start: bool = False,\n    ):\n        super().__init__(\n            rel_stepsize=rel_stepsize,\n            abs_stepsize=abs_stepsize,\n            steps=steps,\n            random_start=random_start,\n        )\n        if not 0 <= quantile <= 1:\n            raise ValueError(f""quantile needs to be between 0 and 1, got {quantile}"")\n        self.quantile = quantile\n'"
foolbox/attacks/spatial_attack.py,0,"b'from typing import Union, Any, Tuple, Generator\nimport eagerpy as ep\nimport numpy as np\n\nfrom ..devutils import atleast_kd\n\nfrom ..criteria import Criterion\n\nfrom .base import Model\nfrom .base import T\nfrom .base import get_is_adversarial\nfrom .base import get_criterion\nfrom .base import Attack\nfrom .spatial_attack_transformations import rotate_and_shift\nfrom .base import raise_if_kwargs\n\n\nclass SpatialAttack(Attack):\n    """"""Adversarially chosen rotations and translations. [#Engs]\n    This implementation is based on the reference implementation by\n    Madry et al.: https://github.com/MadryLab/adversarial_spatial\n\n    References:\n    .. [#Engs] Logan Engstrom*, Brandon Tran*, Dimitris Tsipras*,\n           Ludwig Schmidt, Aleksander M\xc4\x85dry: ""A Rotation and a\n           Translation Suffice: Fooling CNNs with Simple Transformations"",\n           http://arxiv.org/abs/1712.02779\n    """"""\n\n    def __init__(\n        self,\n        max_translation: float = 3,\n        max_rotation: float = 30,\n        num_translations: int = 5,\n        num_rotations: int = 5,\n        grid_search: bool = True,\n        random_steps: int = 100,\n    ):\n\n        self.max_trans = max_translation\n        self.max_rot = max_rotation\n\n        self.grid_search = grid_search\n\n        # grid search true\n        self.num_trans = num_translations\n        self.num_rots = num_rotations\n\n        # grid search false\n        self.random_steps = random_steps\n\n    def __call__(  # type: ignore\n        self, model: Model, inputs: T, criterion: Any, **kwargs: Any,\n    ) -> Tuple[T, T, T]:\n        x, restore_type = ep.astensor_(inputs)\n        del inputs\n        criterion = get_criterion(criterion)\n\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        if x.ndim != 4:\n            raise NotImplementedError(\n                ""only implemented for inputs with two spatial dimensions (and one channel and one batch dimension)""\n            )\n\n        xp = self.run(model, x, criterion)\n        success = is_adversarial(xp)\n\n        xp_ = restore_type(xp)\n        return xp_, xp_, restore_type(success)  # twice to match API\n\n    def run(\n        self, model: Model, inputs: T, criterion: Union[Criterion, T], **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n\n        x, restore_type = ep.astensor_(inputs)\n        del inputs, kwargs\n\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n\n        found = is_adversarial(x)\n        results = x\n\n        def grid_search_generator() -> Generator[Any, Any, Any]:\n            dphis = np.linspace(-self.max_rot, self.max_rot, self.num_rots)\n            dxs = np.linspace(-self.max_trans, self.max_trans, self.num_trans)\n            dys = np.linspace(-self.max_trans, self.max_trans, self.num_trans)\n            for dphi in dphis:\n                for dx in dxs:\n                    for dy in dys:\n                        yield dphi, dx, dy\n\n        def random_search_generator() -> Generator[Any, Any, Any]:\n            dphis = np.random.uniform(-self.max_rot, self.max_rot, self.random_steps)\n            dxs = np.random.uniform(-self.max_trans, self.max_trans, self.random_steps)\n            dys = np.random.uniform(-self.max_trans, self.max_trans, self.random_steps)\n            for dphi, dx, dy in zip(dphis, dxs, dys):\n                yield dphi, dx, dy\n\n        gen = grid_search_generator() if self.grid_search else random_search_generator()\n        for dphi, dx, dy in gen:\n            # TODO: reduce the batch size to the ones that haven\'t been successful\n\n            x_p = rotate_and_shift(x, translation=(dx, dy), rotation=dphi)\n            is_adv = is_adversarial(x_p)\n            new_adv = ep.logical_and(is_adv, found.logical_not())\n\n            results = ep.where(atleast_kd(new_adv, x_p.ndim), x_p, results)\n            found = ep.logical_or(new_adv, found)\n            if found.all():\n                break  # all images in batch misclassified\n        return restore_type(results)\n\n    def repeat(self, times: int) -> Attack:\n        if self.grid_search:\n            raise ValueError(\n                ""repeat is not supported if attack is deterministic""\n            )  # attack is deterministic\n        else:\n            random_steps = self.random_steps * times\n            return SpatialAttack(\n                max_translation=self.max_trans,\n                max_rotation=self.max_rot,\n                num_translations=self.num_trans,\n                num_rotations=self.num_rots,\n                grid_search=self.grid_search,\n                random_steps=random_steps,\n            )\n'"
foolbox/attacks/spatial_attack_transformations.py,44,"b'from typing import Tuple, Any\nimport numpy as np\nimport math\nfrom eagerpy import astensor, Tensor\nfrom eagerpy.tensor import TensorFlowTensor, PyTorchTensor\n\n\ndef rotate_and_shift(\n    inputs: Tensor,\n    translation: Tuple[float, float] = (0.0, 0.0),\n    rotation: float = 0.0,\n) -> Any:\n    rotation = rotation * math.pi / 180.0\n    if isinstance(inputs, TensorFlowTensor):\n        transformed_tensor = transform_tf(inputs, translation, rotation)\n    elif isinstance(inputs, PyTorchTensor):\n        transformed_tensor = transform_pt(inputs, translation, rotation)\n    else:\n        raise NotImplementedError()\n\n    return transformed_tensor\n\n\ndef transform_pt(\n    x_e: Tensor, translation: Tuple[float, float] = (0.0, 0.0), rotation: float = 0.0,\n) -> Any:\n    import torch\n\n    # x_e shape: (bs, nch, x, y)\n    # rotation in rad, translation in pixel\n    # angles: scalar or Tensor with (bs,)\n    bs = x_e.shape[0]\n    theta = np.zeros((2, 3)).astype(np.float32)\n    theta[0, :] = [np.cos(rotation), -np.sin(rotation), translation[0]]\n    theta[1, :] = [np.sin(rotation), np.cos(rotation), translation[1]]\n    theta = np.tile(theta[None], (bs, 1, 1)).reshape(bs, 2, 3)\n    # convert from pixels to relative translation, (bs, n_ch, x, y)\n    theta[:, 0, 2] /= x_e.shape[2] / 2.0\n    theta[:, 1, 2] /= x_e.shape[3] / 2.0\n\n    # to pt\n    x = x_e.raw\n    theta = torch.tensor(theta, device=x.device)\n\n    assert len(x.shape) == 4\n    assert theta.shape[1:] == (2, 3)\n\n    bs, _, n_x, n_y, = x.shape\n\n    def create_meshgrid(x: torch.Tensor) -> torch.Tensor:\n        space_x = torch.linspace(-1, 1, n_x, device=x.device)\n        space_y = torch.linspace(-1, 1, n_y, device=x.device)\n        meshgrid = torch.meshgrid([space_x, space_y])  # type: ignore\n        ones = torch.ones(meshgrid[0].shape, device=x.device)\n        gridder = torch.stack([meshgrid[1], meshgrid[0], ones], dim=2)\n        grid = gridder[None, ...].repeat(bs, 1, 1, 1)[..., None]\n        return grid\n\n    meshgrid = create_meshgrid(x)\n    theta = theta[:, None, None, :, :].repeat(1, n_x, n_y, 1, 1)\n    new_coords = torch.matmul(theta, meshgrid)\n    new_coords = new_coords.squeeze_(-1)\n\n    # align_corners=True to match tf implementation\n    transformed_images = torch.nn.functional.grid_sample(  # type: ignore\n        x, new_coords, mode=""bilinear"", padding_mode=""zeros"", align_corners=True\n    )\n    return astensor(transformed_images)\n\n\n# adapted adapted from\n# https://github.com/kevinzakka/spatial-transformer-network/blob/master/stn/transformer.py\n# state @375f990 on 3 Jun 2018\ndef transform_tf(\n    x_e: Tensor, translation: Tuple[float, float] = (0.0, 0.0), rotation: float = 0.0,\n) -> Any:\n    """"""\n    Input\n    - x: Ep tensor of shape (bs, n_x, n_y, C).\n    - translation: tuple of x, y translation in pixels\n    - rotation: rotation in rad\n\n    Returns\n    - out_fmap: transformed input feature map. Tensor of size (bs, n_x, n_y, C).\n    Notes\n\n    References:\n    [#Jade]: \'Spatial Transformer Networks\', Jaderberg et. al,\n         (https://arxiv.org/abs/1506.02025)\n    """"""\n    import tensorflow as tf\n\n    bs = x_e.shape[0]\n    theta = np.zeros((2, 3)).astype(np.float32)\n\n    theta[0, :] = [np.cos(rotation), -np.sin(rotation), translation[0]]\n    theta[1, :] = [np.sin(rotation), np.cos(rotation), translation[1]]\n    theta = np.tile(theta[None], (bs, 1, 1)).reshape(bs, 2, 3)\n    # convert from pixels to relative translation (bs, x, y, n_ch)\n    theta[:, 0, 2] /= x_e.shape[1] / 2.0\n    theta[:, 1, 2] /= x_e.shape[2] / 2.0\n\n    # to tf\n    theta = tf.convert_to_tensor(theta)\n    x = x_e.raw\n\n    # grab input dimensions\n    assert theta.shape[1:] == (2, 3)\n    assert len(x.shape) == 4\n    bs = tf.shape(x)[0]\n    n_x = tf.shape(x)[1]  # height matrix\n    n_y = tf.shape(x)[2]  # width matrix\n\n    def get_pixel_value(img: Any, x: Any, y: Any) -> Any:\n        """"""\n        Utility function to get pixel value for coordinate\n        vectors x and y from a  4D tensor image.\n\n        Args:\n        - img: tensor of shape (bs, n_x, n_y, C)\n        - x: flattened tensor of shape (bs*n_x*n_y,)\n        - y: flattened tensor of shape (bs*n_x*n_y,)\n\n        Returns:\n        - output: tensor of shape (bs, n_x, n_y, C)\n        """"""\n        batch_idx = tf.range(0, bs)\n        batch_idx = tf.reshape(batch_idx, (bs, 1, 1))\n        b = tf.tile(batch_idx, (1, n_x, n_y))\n        indices = tf.stack([b, y, x], 3)\n        return tf.gather_nd(img, indices)\n\n    def bilinear_sampler(img: Any, x: Any, y: Any) -> Any:\n        """"""\n        Performs bilinear sampling of the input images according to the\n        normalized coordinates provided by the sampling grid. Note that\n        the sampling is done identically for each channel of the input.\n        To test if the function works properly, output image should be\n        identical to input image when theta is initialized to identity\n        transform.\n\n        Args:\n        - img: batch of images in (bs, n_x, n_y, C) layout.\n        - grid: x, y which is the output of affine_grid_generator.\n\n        Returns:\n        - out: interpolated images according to grids. Same size as grid.\n        """"""\n        max_y = tf.cast(n_x - 1, ""int32"")\n        max_x = tf.cast(n_y - 1, ""int32"")\n\n        # rescale x and y to [0, n_y-1/n_x-1]\n        x = tf.cast(x, ""float32"")\n        y = tf.cast(y, ""float32"")\n        x = 0.5 * ((x + 1.0) * tf.cast(max_x, ""float32""))\n        y = 0.5 * ((y + 1.0) * tf.cast(max_y, ""float32""))\n\n        # grab 4 nearest corner points for each (x_i, y_i)\n        x0 = tf.cast(tf.floor(x), ""int32"")\n        x1 = x0 + 1\n        y0 = tf.cast(tf.floor(y), ""int32"")\n        y1 = y0 + 1\n\n        # clip to range [0, n_x-1/n_y-1] to not violate img boundaries\n        min_val = 0\n        x0 = tf.clip_by_value(x0, min_val, max_x)\n        x1 = tf.clip_by_value(x1, min_val, max_x)\n        y0 = tf.clip_by_value(y0, min_val, max_y)\n        y1 = tf.clip_by_value(y1, min_val, max_y)\n\n        # get pixel value at corner coords\n        Ia = get_pixel_value(img, x0, y0)\n        Ib = get_pixel_value(img, x0, y1)\n        Ic = get_pixel_value(img, x1, y0)\n        Id = get_pixel_value(img, x1, y1)\n\n        # recast as float for delta calculation\n        x0 = tf.cast(x0, ""float32"")\n        x1 = tf.cast(x1, ""float32"")\n        y0 = tf.cast(y0, ""float32"")\n        y1 = tf.cast(y1, ""float32"")\n\n        # calculate deltas\n        wa = (x1 - x) * (y1 - y)\n        wb = (x1 - x) * (y - y0)\n        wc = (x - x0) * (y1 - y)\n        wd = (x - x0) * (y - y0)\n\n        # add dimension for addition\n        wa = tf.expand_dims(wa, axis=3)\n        wb = tf.expand_dims(wb, axis=3)\n        wc = tf.expand_dims(wc, axis=3)\n        wd = tf.expand_dims(wd, axis=3)\n\n        # compute output\n        out = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n\n        return out\n\n    def affine_grid_generator(height: Any, width: Any, theta: Any) -> Any:\n        """"""\n        This function returns a sampling grid, which when\n        used with the bilinear sampler on the input feature\n        map, will create an output feature map that is an\n        affine transformation [1] of the input feature map.\n\n        Args:\n        - height: desired height of grid/output. Used\n          to downsample or upsample.\n        - width: desired width of grid/output. Used\n          to downsample or upsample.\n        - theta: affine transform matrices of shape (num_batch, 2, 3).\n          For each image in the batch, we have 6 theta parameters of\n          the form (2x3) that define the affine transformation T.\n\n        Returns:\n        - normalized grid (-1, 1) of shape (num_batch, 2, n_x, n_y).\n          The 2nd dimension has 2 components: (x, y) which are the\n          sampling points of the original image for each point in the\n          target image.\n        Note\n        ----\n        [1]: the affine transformation allows cropping, translation,\n             and isotropic scaling.\n        """"""\n        num_batch = tf.shape(theta)[0]\n\n        # create normalized 2D grid\n        x_l = tf.linspace(-1.0, 1.0, width)\n        y_l = tf.linspace(-1.0, 1.0, height)\n        x_t, y_t = tf.meshgrid(x_l, y_l)\n\n        # flatten\n        x_t_flat = tf.reshape(x_t, [-1])\n        y_t_flat = tf.reshape(y_t, [-1])\n\n        # reshape to [x_t, y_t , 1] - (homogeneous form)\n        ones = tf.ones_like(x_t_flat)\n        sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n\n        # repeat grid num_batch times\n        sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n        sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n\n        # cast to float32 (required for matmul)\n        theta = tf.cast(theta, ""float32"")\n        sampling_grid = tf.cast(sampling_grid, ""float32"")\n\n        # transform the sampling grid - batch multiply\n        batch_grids = tf.matmul(theta, sampling_grid)\n        # batch grid has shape (num_batch, 2, n_x*n_y)\n\n        # reshape to (num_batch, n_x, n_y, 2)\n        batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n\n        return batch_grids\n\n    batch_grids = affine_grid_generator(n_x, n_y, theta)\n\n    x_s = batch_grids[:, 0, :, :]\n    y_s = batch_grids[:, 1, :, :]\n\n    # sample input with grid to get output\n    transformed_images = bilinear_sampler(x, x_s, y_s)\n\n    return astensor(transformed_images)\n'"
foolbox/attacks/virtual_adversarial_attack.py,0,"b'from typing import Union, Any\nimport eagerpy as ep\n\nfrom ..models import Model\n\nfrom ..criteria import Misclassification\n\nfrom ..distances import l2\n\nfrom ..devutils import flatten, atleast_kd\n\nfrom .base import FixedEpsilonAttack\nfrom .base import get_criterion\nfrom .base import T\nfrom .base import raise_if_kwargs\n\n\nclass VirtualAdversarialAttack(FixedEpsilonAttack):\n    """"""Second-order gradient-based attack on the logits. [#Miy15]_\n    The attack calculate an untargeted adversarial perturbation by performing a\n    approximated second order optimization step on the KL divergence between\n    the unperturbed predictions and the predictions for the adversarial\n    perturbation. This attack was originally introduced as the\n    Virtual Adversarial Training [#Miy15]_ method.\n\n    Args:\n        steps : Number of update steps.\n        xi : L2 distance between original image and first adversarial proposal.\n\n\n    References:\n        .. [#Miy15] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae,\n            Shin Ishii, ""Distributional Smoothing with Virtual Adversarial Training"",\n            https://arxiv.org/abs/1507.00677\n    """"""\n\n    distance = l2\n\n    def __init__(self, steps: int, xi: float = 1e-6):\n        self.steps = steps\n        self.xi = xi\n\n    def run(\n        self,\n        model: Model,\n        inputs: T,\n        criterion: Union[Misclassification, T],\n        *,\n        epsilon: float,\n        **kwargs: Any,\n    ) -> T:\n        raise_if_kwargs(kwargs)\n        x, restore_type = ep.astensor_(inputs)\n        criterion_ = get_criterion(criterion)\n        del inputs, criterion, kwargs\n\n        N = len(x)\n\n        if isinstance(criterion_, Misclassification):\n            classes = criterion_.labels\n        else:\n            raise ValueError(""unsupported criterion"")\n\n        if classes.shape != (N,):\n            raise ValueError(\n                f""expected labels to have shape ({N},), got {classes.shape}""\n            )\n\n        bounds = model.bounds\n\n        def loss_fun(delta: ep.Tensor, logits: ep.Tensor) -> ep.Tensor:\n            assert x.shape[0] == logits.shape[0]\n            assert delta.shape == x.shape\n\n            x_hat = x + delta\n            logits_hat = model(x_hat)\n            loss = ep.kl_div_with_logits(logits, logits_hat).sum()\n\n            return loss\n\n        value_and_grad = ep.value_and_grad_fn(x, loss_fun, has_aux=False)\n\n        clean_logits = model(x)\n\n        # start with random vector as search vector\n        d = ep.normal(x, shape=x.shape, mean=0, stddev=1)\n        for it in range(self.steps):\n            # normalize proposal to be unit vector\n            d = d * self.xi / atleast_kd(ep.norms.l2(flatten(d), axis=-1), x.ndim)\n\n            # use gradient of KL divergence as new search vector\n            _, grad = value_and_grad(d, clean_logits)\n            d = grad\n\n            # rescale search vector\n            d = (bounds[1] - bounds[0]) * d\n\n            if ep.any(ep.norms.l2(flatten(d), axis=-1) < 1e-64):\n                raise RuntimeError(  # pragma: no cover\n                    ""Gradient vanished; this can happen if xi is too small.""\n                )\n\n        final_delta = epsilon / atleast_kd(ep.norms.l2(flatten(d), axis=-1), d.ndim) * d\n        x_adv = ep.clip(x + final_delta, *bounds)\n        return restore_type(x_adv)\n'"
foolbox/models/__init__.py,0,b'from .base import Model  # noqa: F401\nfrom .base import TransformBoundsWrapper  # noqa: F401\n\nfrom .pytorch import PyTorchModel  # noqa: F401\nfrom .tensorflow import TensorFlowModel  # noqa: F401\nfrom .jax import JAXModel  # noqa: F401\nfrom .numpy import NumPyModel  # noqa: F401\n\nfrom .wrappers import ThresholdingWrapper  # noqa: F401\n'
foolbox/models/base.py,0,"b'from typing import TypeVar, Callable, Optional, Tuple, Any\nfrom abc import ABC, abstractmethod\nimport copy\nimport eagerpy as ep\n\nfrom ..types import Bounds, BoundsInput, Preprocessing\nfrom ..devutils import atleast_kd\n\n\nT = TypeVar(""T"")\nPreprocessArgs = Tuple[Optional[ep.Tensor], Optional[ep.Tensor], Optional[int]]\n\n\nclass Model(ABC):\n    @property\n    @abstractmethod\n    def bounds(self) -> Bounds:\n        ...\n\n    @abstractmethod  # noqa: F811\n    def __call__(self, inputs: T) -> T:\n        """"""Passes inputs through the model and returns the model\'s output""""""\n        ...\n\n    def transform_bounds(self, bounds: BoundsInput) -> ""Model"":\n        """"""Returns a new model with the desired bounds and updates the preprocessing accordingly""""""\n        # subclasses can provide more efficient implementations\n        return TransformBoundsWrapper(self, bounds)\n\n\nclass TransformBoundsWrapper(Model):\n    def __init__(self, model: Model, bounds: BoundsInput):\n        self._model = model\n        self._bounds = Bounds(*bounds)\n\n    @property\n    def bounds(self) -> Bounds:\n        return self._bounds\n\n    def __call__(self, inputs: T) -> T:\n        x, restore_type = ep.astensor_(inputs)\n        y = self._preprocess(x)\n        z = self._model(y)\n        return restore_type(z)\n\n    def transform_bounds(self, bounds: BoundsInput, inplace: bool = False) -> Model:\n        if inplace:\n            self._bounds = Bounds(*bounds)\n            return self\n        else:\n            # using the wrapped model instead of self to avoid\n            # unnessary sequences of wrappers\n            return TransformBoundsWrapper(self._model, bounds)\n\n    def _preprocess(self, inputs: ep.TensorType) -> ep.TensorType:\n        if self.bounds == self._model.bounds:\n            return inputs\n\n        # from bounds to (0, 1)\n        min_, max_ = self.bounds\n        x = (inputs - min_) / (max_ - min_)\n\n        # from (0, 1) to wrapped model bounds\n        min_, max_ = self._model.bounds\n        return x * (max_ - min_) + min_\n\n    @property\n    def data_format(self) -> Any:\n        return getattr(self._model, ""data_format"", None)\n\n\nModelType = TypeVar(""ModelType"", bound=""ModelWithPreprocessing"")\n\n\nclass ModelWithPreprocessing(Model):\n    def __init__(  # type: ignore\n        self,\n        model: Callable[..., ep.types.NativeTensor],\n        bounds: BoundsInput,\n        dummy: ep.Tensor,\n        preprocessing: Preprocessing = None,\n    ):\n        if not callable(model):\n            raise ValueError(""expected model to be callable"")  # pragma: no cover\n\n        self._model = model\n        self._bounds = Bounds(*bounds)\n        self._dummy = dummy\n        self._preprocess_args = self._process_preprocessing(preprocessing)\n\n    @property\n    def bounds(self) -> Bounds:\n        return self._bounds\n\n    @property\n    def dummy(self) -> ep.Tensor:\n        return self._dummy\n\n    def __call__(self, inputs: T) -> T:\n        x, restore_type = ep.astensor_(inputs)\n        y = self._preprocess(x)\n        z = ep.astensor(self._model(y.raw))\n        return restore_type(z)\n\n    def transform_bounds(\n        self, bounds: BoundsInput, inplace: bool = False, wrapper: bool = False,\n    ) -> Model:\n        """"""Returns a new model with the desired bounds and updates the preprocessing accordingly""""""\n        # more efficient than the base class implementation because it avoids the additional wrapper\n\n        if wrapper:\n            if inplace:\n                raise ValueError(""inplace and wrapper cannot both be True"")\n            return super().transform_bounds(bounds)\n\n        if self.bounds == bounds:\n            if inplace:\n                return self\n            else:\n                return copy.copy(self)\n\n        a, b = self.bounds\n        c, d = bounds\n        f = (d - c) / (b - a)\n\n        mean, std, flip_axis = self._preprocess_args\n\n        if mean is None:\n            mean = ep.zeros(self._dummy, 1)\n        mean = f * (mean - a) + c\n\n        if std is None:\n            std = ep.ones(self._dummy, 1)\n        std = f * std\n\n        if inplace:\n            model = self\n        else:\n            model = copy.copy(self)\n        model._bounds = Bounds(*bounds)\n        model._preprocess_args = (mean, std, flip_axis)\n        return model\n\n    def _preprocess(self, inputs: ep.Tensor) -> ep.Tensor:\n        mean, std, flip_axis = self._preprocess_args\n        x = inputs\n        if flip_axis is not None:\n            x = x.flip(axis=flip_axis)\n        if mean is not None:\n            x = x - mean\n        if std is not None:\n            x = x / std\n        assert x.dtype == inputs.dtype\n        return x\n\n    def _process_preprocessing(self, preprocessing: Preprocessing) -> PreprocessArgs:\n        if preprocessing is None:\n            preprocessing = dict()\n\n        unsupported = set(preprocessing.keys()) - {""mean"", ""std"", ""axis"", ""flip_axis""}\n        if len(unsupported) > 0:\n            raise ValueError(f""unknown preprocessing key: {unsupported.pop()}"")\n\n        mean = preprocessing.get(""mean"", None)\n        std = preprocessing.get(""std"", None)\n        axis = preprocessing.get(""axis"", None)\n        flip_axis = preprocessing.get(""flip_axis"", None)\n\n        def to_tensor(x: Any) -> Optional[ep.Tensor]:\n            if x is None:\n                return None\n            if isinstance(x, ep.Tensor):\n                return x\n            try:\n                y = ep.astensor(x)  # might raise ValueError\n                if not isinstance(y, type(self._dummy)):\n                    raise ValueError\n                return y\n            except ValueError:\n                return ep.from_numpy(self._dummy, x)\n\n        mean_ = to_tensor(mean)\n        std_ = to_tensor(std)\n\n        def apply_axis(x: Optional[ep.Tensor], axis: int) -> Optional[ep.Tensor]:\n            if x is None:\n                return None\n            if x.ndim != 1:\n                raise ValueError(f""non-None axis requires a 1D tensor, got {x.ndim}D"")\n            if axis >= 0:\n                raise ValueError(\n                    ""expected axis to be None or negative, -1 refers to the last axis""\n                )\n            return atleast_kd(x, -axis)\n\n        if axis is not None:\n            mean_ = apply_axis(mean_, axis)\n            std_ = apply_axis(std_, axis)\n\n        return mean_, std_, flip_axis\n'"
foolbox/models/jax.py,0,"b'from typing import Any\nimport eagerpy as ep\n\nfrom ..types import BoundsInput, Preprocessing\n\nfrom .base import ModelWithPreprocessing\n\n\nclass JAXModel(ModelWithPreprocessing):\n    def __init__(\n        self, model: Any, bounds: BoundsInput, preprocessing: Preprocessing = None\n    ):\n        dummy = ep.jax.numpy.zeros(0)\n        super().__init__(model, bounds=bounds, dummy=dummy, preprocessing=preprocessing)\n'"
foolbox/models/numpy.py,0,"b'from typing import TypeVar, Callable, Optional\nimport eagerpy as ep\n\nfrom ..types import Bounds\nfrom ..types import BoundsInput\n\nfrom .base import Model\n\n\nT = TypeVar(""T"")\n\n\nclass NumPyModel(Model):\n    def __init__(\n        self, model: Callable, bounds: BoundsInput, data_format: Optional[str] = None\n    ):\n        self._model = model\n        self._bounds = Bounds(*bounds)\n        if data_format is not None:\n            if data_format not in [""channels_first"", ""channels_last""]:\n                raise ValueError(\n                    f""expected data_format to be \'channels_first\' or \'channels_last\', got {data_format}""\n                )\n        self._data_format = data_format\n\n    @property\n    def bounds(self) -> Bounds:\n        return self._bounds\n\n    def __call__(self, inputs: T) -> T:\n        x, restore_type = ep.astensor_(inputs)\n        y = self._model(x.numpy())\n        z = ep.from_numpy(x, y)\n        return restore_type(z)\n\n    @property\n    def data_format(self) -> str:\n        if self._data_format is None:\n            raise AttributeError(  # AttributeError -> hasattr returns False\n                ""please specify data_format when initializing the NumPyModel""\n            )\n        return self._data_format\n'"
foolbox/models/pytorch.py,0,"b'from typing import Any, cast\nimport warnings\nimport eagerpy as ep\n\nfrom ..types import BoundsInput, Preprocessing\n\nfrom .base import ModelWithPreprocessing\n\n\ndef get_device(device: Any) -> Any:\n    import torch\n\n    if device is None:\n        return torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    if isinstance(device, str):\n        return torch.device(device)\n    return device\n\n\nclass PyTorchModel(ModelWithPreprocessing):\n    def __init__(\n        self,\n        model: Any,\n        bounds: BoundsInput,\n        device: Any = None,\n        preprocessing: Preprocessing = None,\n    ):\n        import torch\n\n        if not isinstance(model, torch.nn.Module):\n            raise ValueError(""expected model to be a torch.nn.Module instance"")\n\n        if model.training:\n            with warnings.catch_warnings():\n                warnings.simplefilter(""always"")\n                warnings.warn(\n                    ""The PyTorch model is in training mode and therefore might""\n                    "" not be deterministic. Call the eval() method to set it in""\n                    "" evaluation mode if this is not intended.""\n                )\n\n        device = get_device(device)\n        model = model.to(device)\n        dummy = ep.torch.zeros(0, device=device)\n\n        # we need to make sure the output only requires_grad if the input does\n        def _model(x: torch.Tensor) -> torch.Tensor:\n            with torch.set_grad_enabled(x.requires_grad):\n                result = cast(torch.Tensor, model(x))\n            return result\n\n        super().__init__(\n            _model, bounds=bounds, dummy=dummy, preprocessing=preprocessing\n        )\n\n        self.data_format = ""channels_first""\n        self.device = device\n'"
foolbox/models/tensorflow.py,4,"b'from typing import cast, Any\nimport eagerpy as ep\n\nfrom ..types import BoundsInput, Preprocessing\n\nfrom .base import ModelWithPreprocessing\n\n\ndef get_device(device: Any) -> Any:\n    import tensorflow as tf\n\n    if device is None:\n        device = tf.device(""/GPU:0"" if tf.test.is_gpu_available() else ""/CPU:0"")\n    if isinstance(device, str):\n        device = tf.device(device)\n    return device\n\n\nclass TensorFlowModel(ModelWithPreprocessing):\n    def __init__(\n        self,\n        model: Any,\n        bounds: BoundsInput,\n        device: Any = None,\n        preprocessing: Preprocessing = None,\n    ):\n        import tensorflow as tf\n\n        if not tf.executing_eagerly():\n            raise ValueError(\n                ""TensorFlowModel requires TensorFlow Eager Mode""\n            )  # pragma: no cover\n\n        device = get_device(device)\n        with device:\n            dummy = ep.tensorflow.zeros(0)\n        super().__init__(model, bounds, dummy, preprocessing=preprocessing)\n\n        self.device = device\n\n    @property\n    def data_format(self) -> str:\n        import tensorflow as tf\n\n        return cast(str, tf.keras.backend.image_data_format())\n'"
foolbox/models/wrappers.py,0,"b'import eagerpy as ep\n\nfrom ..types import Bounds\n\nfrom .base import Model\nfrom .base import T\n\n\nclass ThresholdingWrapper(Model):\n    def __init__(self, model: Model, threshold: float):\n        self._model = model\n        self._threshold = threshold\n\n    @property\n    def bounds(self) -> Bounds:\n        return self._model.bounds\n\n    def __call__(self, inputs: T) -> T:\n        min_, max_ = self._model.bounds\n        x, restore_type = ep.astensor_(inputs)\n        y = ep.where(x < self._threshold, min_, max_).astype(x.dtype)\n        z = self._model(y)\n        return restore_type(z)\n'"
foolbox/zoo/__init__.py,0,b'from .zoo import get_model  # noqa: F401\nfrom .weights_fetcher import fetch_weights  # noqa: F401\nfrom .git_cloner import GitCloneError  # noqa: F401\nfrom .model_loader import ModelLoader  # noqa: F401\n'
foolbox/zoo/common.py,0,"b'import hashlib\nimport os\n\n\ndef sha256_hash(git_uri: str) -> str:\n    m = hashlib.sha256()\n    m.update(git_uri.encode())\n    return m.hexdigest()\n\n\ndef home_directory_path(folder: str, hash_digest: str) -> str:\n    # does this work on all operating systems?\n    home = os.path.expanduser(""~"")\n    return os.path.join(home, folder, hash_digest)\n'"
foolbox/zoo/git_cloner.py,0,"b'import os\nimport shutil\nfrom git import Repo\nimport logging\nfrom .common import sha256_hash, home_directory_path\n\nFOLDER = "".foolbox_zoo""\n\n\nclass GitCloneError(RuntimeError):\n    pass\n\n\ndef clone(git_uri: str, overwrite: bool = False) -> str:\n    """"""Clones a remote git repository to a local path.\n\n    Args:\n        git_uri: The URI to the git repository to be cloned.\n        overwrite: Whether or not to overwrite the local path.\n\n    Returns:\n        The generated local path where the repository has been cloned to.\n    """"""\n    hash_digest = sha256_hash(git_uri)\n    local_path = home_directory_path(FOLDER, hash_digest)\n    exists_locally = os.path.exists(local_path)\n\n    if exists_locally and overwrite:\n        # TODO: ideally we would just pull the latest changes instead of cloning again\n        shutil.rmtree(local_path, ignore_errors=True)\n        exists_locally = False\n\n    if not exists_locally:\n        _clone_repo(git_uri, local_path)\n    else:\n        logging.info(  # pragma: no cover\n            ""Git repository already exists locally.""\n        )  # pragma: no cover\n\n    return local_path\n\n\ndef _clone_repo(git_uri: str, local_path: str) -> None:\n    logging.info(""Cloning repo %s to %s"", git_uri, local_path)\n    try:\n        Repo.clone_from(git_uri, local_path)\n    except Exception as e:\n        logging.exception(""Failed to clone repository"", e)\n        raise GitCloneError(""Failed to clone repository"")\n    logging.info(""Cloned repo successfully."")\n'"
foolbox/zoo/model_loader.py,0,"b'from typing import Any, cast, Optional\nfrom types import ModuleType\nimport sys\nimport importlib\nimport abc\nfrom abc import abstractmethod\n\nfrom ..models import Model\n\n\nclass ModelLoader(abc.ABC):\n    @abstractmethod\n    def load(\n        self, path: str, module_name: str = ""foolbox_model"", **kwargs: Any\n    ) -> Model:\n        """"""Loads a model from a local path, to which a git repository has been previously cloned to.\n\n        Args:\n            path: The path to the local repository containing the code.\n            module_name: The name of the module to import.\n            kwargs: Additional parameters for the loaded model.\n\n        Returns:\n            A foolbox-wrapped model.\n        """"""\n        ...\n\n    @staticmethod\n    def get(key: Optional[str] = None) -> ""ModelLoader"":\n        if key is None:\n            return DefaultLoader()\n        else:\n            raise ValueError(f""No model loader for: {key}"")\n\n    @staticmethod\n    def _import_module(path: str, module_name: str = ""foolbox_model"") -> ModuleType:\n        sys.path.insert(0, path)\n        module = importlib.import_module(module_name)\n        print(""imported module: {}"".format(module))\n        return module\n\n\nclass DefaultLoader(ModelLoader):\n    def load(\n        self, path: str, module_name: str = ""foolbox_model"", **kwargs: Any\n    ) -> Model:\n        module = super()._import_module(path, module_name=module_name)\n        model = module.create(**kwargs)  # type: ignore\n        return cast(Model, model)\n'"
foolbox/zoo/weights_fetcher.py,0,"b'import requests\nimport shutil\nimport zipfile\nimport tarfile\nimport os\nimport logging\n\nfrom .common import sha256_hash, home_directory_path\n\nFOLDER = "".foolbox_zoo/weights""\n\n\ndef fetch_weights(weights_uri: str, unzip: bool = False) -> str:\n    """"""Provides utilities to download and extract packages\n    containing model weights when creating foolbox-zoo compatible\n    repositories, if the weights are not part of the repository itself.\n\n    Examples\n    --------\n\n    Download and unzip weights:\n\n    >>> from foolbox import zoo\n    >>> url = \'https://github.com/MadryLab/mnist_challenge_models/raw/master/secret.zip\'  # noqa F501\n    >>> weights_path = zoo.fetch_weights(url, unzip=True)\n\n    Args:\n        weights_uri: The URI to fetch the weights from.\n        unzip: Should be `True` if the file to be downloaded is a zipped package.\n\n    Returns:\n        Local path where the weights have been downloaded and potentially unzipped to.\n    """"""\n    assert weights_uri is not None\n    hash_digest = sha256_hash(weights_uri)\n    local_path = home_directory_path(FOLDER, hash_digest)\n    exists_locally = os.path.exists(local_path)\n\n    filename = _filename_from_uri(weights_uri)\n    file_path = os.path.join(local_path, filename)\n\n    if exists_locally:\n        logging.info(""Weights already stored locally."")  # pragma: no cover\n    else:\n        _download(file_path, weights_uri, local_path)\n\n    if unzip:\n        file_path = _extract(local_path, filename)\n\n    return file_path\n\n\ndef _filename_from_uri(url: str) -> str:\n    # get last part of the URI, i.e. file-name\n    filename = url.split(""/"")[-1]\n    # remove query params if exist\n    filename = filename.split(""?"")[0]\n    return filename\n\n\ndef _download(file_path: str, url: str, directory: str) -> None:\n    logging.info(""Downloading weights: %s to %s"", url, file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    # first check ETag or If-Modified-Since header or similar\n    # to check whether updated weights are available?\n    r = requests.get(url, stream=True)\n    if r.status_code == 200:\n        with open(file_path, ""wb"") as f:\n            r.raw.decode_content = True\n            shutil.copyfileobj(r.raw, f)\n    else:\n        raise RuntimeError(""Failed to fetch weights from %s"", url)\n\n\ndef _extract(directory: str, filename: str) -> str:\n    file_path = os.path.join(directory, filename)\n    extracted_folder = filename.rsplit(""."", 1)[0]\n    extracted_folder = os.path.join(directory, extracted_folder)\n\n    if not os.path.exists(extracted_folder):\n        logging.info(""Extracting weights package to %s"", extracted_folder)\n        os.makedirs(extracted_folder)\n        if "".zip"" in file_path:\n            zip_ref = zipfile.ZipFile(file_path, ""r"")\n            zip_ref.extractall(extracted_folder)\n            zip_ref.close()\n        elif "".tar.gz"" in file_path:  # pragma: no cover\n            tar_ref = tarfile.TarFile.open(file_path, ""r"")\n            tar_ref.extractall(extracted_folder)\n            tar_ref.close()\n    else:\n        logging.info(\n            ""Extraced folder already exists: %s"", extracted_folder\n        )  # pragma: no cover\n\n    return extracted_folder\n'"
foolbox/zoo/zoo.py,0,"b'from typing import Any\n\nfrom ..models import Model\n\nfrom .git_cloner import clone\nfrom .model_loader import ModelLoader\n\n\ndef get_model(\n    url: str, module_name: str = ""foolbox_model"", overwrite: bool = False, **kwargs: Any\n) -> Model:\n    """"""Download a Foolbox-compatible model from the given Git repository URL.\n\n    Examples\n    --------\n\n    Instantiate a model:\n\n    >>> from foolbox import zoo\n    >>> url = ""https://github.com/bveliqi/foolbox-zoo-dummy.git""\n    >>> model = zoo.get_model(url)  # doctest: +SKIP\n\n    Only works with a foolbox-zoo compatible repository.\n    I.e. models need to have a `foolbox_model.py` file\n    with a `create()`-function, which returns a foolbox-wrapped model.\n\n    Using the kwargs parameter it is possible to input an arbitrary number\n    of parameters to this methods call. These parameters are forwarded to\n    the instantiated model.\n\n    Example repositories:\n\n        - https://github.com/jonasrauber/foolbox-tensorflow-keras-applications\n        - https://github.com/bethgelab/AnalysisBySynthesis\n        - https://github.com/bethgelab/mnist_challenge\n        - https://github.com/bethgelab/cifar10_challenge\n        - https://github.com/bethgelab/convex_adversarial\n        - https://github.com/wielandbrendel/logit-pairing-foolbox.git\n        - https://github.com/bethgelab/defensive-distillation.git\n\n    Args:\n        url: URL to the git repository.\n        module_name: The name of the module to import.\n        kwargs: Optional set of parameters that will be used by the to be instantiated model.\n\n    Returns:\n        A Foolbox-wrapped model instance.\n    """"""\n    repo_path = clone(url, overwrite=overwrite)\n    loader = ModelLoader.get()\n    model = loader.load(repo_path, module_name=module_name, **kwargs)\n    return model\n'"
examples/zoo/mnist/foolbox_model.py,0,"b'#!/usr/bin/env python3\nimport torch\nimport torch.nn as nn\nimport os\nfrom foolbox.models import PyTorchModel\nfrom foolbox.utils import accuracy, samples\n\n\ndef create() -> PyTorchModel:\n    model = nn.Sequential(\n        nn.Conv2d(1, 32, 3),\n        nn.ReLU(),\n        nn.Conv2d(32, 64, 3),\n        nn.ReLU(),\n        nn.MaxPool2d(2),\n        nn.Dropout2d(0.25),\n        nn.Flatten(),  # type: ignore\n        nn.Linear(9216, 128),\n        nn.ReLU(),\n        nn.Dropout2d(0.5),\n        nn.Linear(128, 10),\n    )\n    path = os.path.join(os.path.dirname(os.path.realpath(__file__)), ""mnist_cnn.pth"")\n    model.load_state_dict(torch.load(path))  # type: ignore\n    model.eval()\n    preprocessing = dict(mean=0.1307, std=0.3081)\n    fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n    return fmodel\n\n\nif __name__ == ""__main__"":\n    # test the model\n    fmodel = create()\n    images, labels = samples(fmodel, dataset=""mnist"", batchsize=20)\n    print(accuracy(fmodel, images, labels))\n'"
