file_path,api_count,code
data_helpers.py,0,"b'import numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for datasets.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels():\n    """"""\n    Loads polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(""./data/rt-polarity.pos"", ""r"", encoding=\'latin-1\').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(""./data/rt-polarity.neg"", ""r"", encoding=\'latin-1\').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    x_text = [s.split("" "") for s in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\n    """"""\n    Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.\n    """"""\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences\n\n\ndef build_vocab(sentences):\n    """"""\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    """"""\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    vocabulary_inv = list(sorted(vocabulary_inv))\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n    """"""\n    Maps sentences and labels to vectors based on a vocabulary.\n    """"""\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return [x, y]\n\n\ndef load_data():\n    """"""\n    Loads and preprocessed data for the dataset.\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n    """"""\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\n    return [x, y, vocabulary, vocabulary_inv]\n'"
model.py,0,"b'from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Dropout, Concatenate\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom data_helpers import load_data\n\nprint(\'Loading data\')\nx, y, vocabulary, vocabulary_inv = load_data()\n\n# x.shape -> (10662, 56)\n# y.shape -> (10662, 2)\n# len(vocabulary) -> 18765\n# len(vocabulary_inv) -> 18765\n\nX_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n\n# X_train.shape -> (8529, 56)\n# y_train.shape -> (8529, 2)\n# X_test.shape -> (2133, 56)\n# y_test.shape -> (2133, 2)\n\n\nsequence_length = x.shape[1] # 56\nvocabulary_size = len(vocabulary_inv) # 18765\nembedding_dim = 256\nfilter_sizes = [3,4,5]\nnum_filters = 512\ndrop = 0.5\n\nepochs = 100\nbatch_size = 30\n\n# this returns a tensor\nprint(""Creating Model..."")\ninputs = Input(shape=(sequence_length,), dtype=\'int32\')\nembedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\nreshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n\nconv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding=\'valid\', kernel_initializer=\'normal\', activation=\'relu\')(reshape)\nconv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding=\'valid\', kernel_initializer=\'normal\', activation=\'relu\')(reshape)\nconv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding=\'valid\', kernel_initializer=\'normal\', activation=\'relu\')(reshape)\n\nmaxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding=\'valid\')(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding=\'valid\')(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding=\'valid\')(conv_2)\n\nconcatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\nflatten = Flatten()(concatenated_tensor)\ndropout = Dropout(drop)(flatten)\noutput = Dense(units=2, activation=\'softmax\')(dropout)\n\n# this creates a model that includes\nmodel = Model(inputs=inputs, outputs=output)\n\ncheckpoint = ModelCheckpoint(\'weights.{epoch:03d}-{val_acc:.4f}.hdf5\', monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'auto\')\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\nmodel.compile(optimizer=adam, loss=\'binary_crossentropy\', metrics=[\'accuracy\'])\nprint(""Traning Model..."")\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training\n\n\n\n'"
