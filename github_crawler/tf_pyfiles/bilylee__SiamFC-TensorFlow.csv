file_path,api_count,code
configuration.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright @ 2017 bily     Huazhong University of Science and Technology\n#\n\n""""""Default configurations of model specification, training and tracking\n\nFor most of the time, DO NOT modify the configurations within this file.\nUse the configurations here as the default configurations and only update\nthem following the examples in the `experiments` directory.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\n\nLOG_DIR = \'Logs/SiamFC\'  # where checkpoints, logs are saved\nRUN_NAME = \'SiamFC-3s-color-scratch\'  # identifier of the experiment\n\nMODEL_CONFIG = {\n  \'z_image_size\': 127,  # Exemplar image size\n\n  \'embed_config\': {\'embedding_name\': \'convolutional_alexnet\',\n                   \'embedding_checkpoint_file\': None,  # mat file path of the pretrained embedding model.\n                   \'train_embedding\': True,\n                   \'init_method\': \'kaiming_normal\',\n                   \'use_bn\': True,\n                   \'bn_scale\': True,\n                   \'bn_momentum\': 0.05,\n                   \'bn_epsilon\': 1e-6,\n                   \'embedding_feature_num\': 256,\n                   \'weight_decay\': 5e-4,\n                   \'stride\': 8, },\n\n  \'adjust_response_config\': {\'train_bias\': True,\n                             \'scale\': 1e-3, },\n}\n\nTRAIN_CONFIG = {\n  \'train_dir\': osp.join(LOG_DIR, \'track_model_checkpoints\', RUN_NAME),\n\n  \'seed\': 123,  # fix seed for reproducing experiments\n\n  \'train_data_config\': {\'input_imdb\': \'data/train_imdb.pickle\',\n                        \'preprocessing_name\': \'siamese_fc_color\',\n                        \'num_examples_per_epoch\': 5.32e4,\n                        \'epoch\': 50,\n                        \'batch_size\': 8,\n                        \'max_frame_dist\': 100,  # Maximum distance between any two random frames draw from videos.\n                        \'prefetch_threads\': 4,\n                        \'prefetch_capacity\': 15 * 8, },  # The maximum elements number in the data loading queue\n\n  \'validation_data_config\': {\'input_imdb\': \'data/validation_imdb.pickle\',\n                             \'preprocessing_name\': \'None\',\n                             \'batch_size\': 8,\n                             \'max_frame_dist\': 100,  # Maximum distance between any two random frames draw from videos.\n                             \'prefetch_threads\': 1,\n                             \'prefetch_capacity\': 15 * 8, },  # The maximum elements number in the data loading queue\n\n  # Configurations for generating groundtruth maps\n  \'gt_config\': {\'rPos\': 16,\n                \'rNeg\': 0, },\n\n  # Optimizer for training the model.\n  \'optimizer_config\': {\'optimizer\': \'MOMENTUM\',  # SGD and MOMENTUM are supported\n                       \'momentum\': 0.9,\n                       \'use_nesterov\': False, },\n\n  # Learning rate configs\n  \'lr_config\': {\'policy\': \'exponential\',\n                \'initial_lr\': 0.01,\n                \'num_epochs_per_decay\': 1,\n                \'lr_decay_factor\': 0.8685113737513527,\n                \'staircase\': True, },\n\n  # If not None, clip gradients to this value.\n  \'clip_gradients\': None,\n\n  # Frequency at which loss and global step are logged\n  \'log_every_n_steps\': 10,\n\n  # Frequency to save model\n  \'save_model_every_n_step\': 5.32e4 // 8,  # save model every epoch\n\n  # How many model checkpoints to keep. No limit if None.\n  \'max_checkpoints_to_keep\': None,\n}\n\nTRACK_CONFIG = {\n  # Directory for saving log files during tracking.\n  \'log_dir\': osp.join(LOG_DIR, \'track_model_inference\', RUN_NAME),\n\n  # Logging level of inference, use 1 for detailed inspection. 0 for speed.\n  \'log_level\': 0,\n\n  \'x_image_size\': 255,  # Search image size during tracking\n\n  # Configurations for upsampling score maps\n  \'upsample_method\': \'bicubic\',\n  \'upsample_factor\': 16,\n\n  # Configurations for searching scales\n  \'num_scales\': 3,  # Number of scales to search\n  \'scale_step\': 1.0375,  # Scale changes between different scale search\n  \'scale_damp\': 0.59,  # Damping factor for scale update\n  \'scale_penalty\': 0.9745,  # Score penalty for scale change\n\n  # Configurations for penalizing large displacement from the center\n  \'window_influence\': 0.176,\n\n  \'include_first\': False, # If track the first frame\n}\n'"
siamese_model.py,49,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Construct the computational graph of siamese model for training. """"""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport tensorflow as tf\n\nfrom datasets.dataloader import DataLoader\nfrom embeddings.convolutional_alexnet import convolutional_alexnet_arg_scope, convolutional_alexnet\nfrom metrics.track_metrics import center_dist_error, center_score_error\nfrom utils.train_utils import construct_gt_score_maps, load_mat_model\n\nslim = tf.contrib.slim\n\n\nclass SiameseModel:\n  def __init__(self, model_config, train_config, mode=\'train\'):\n    self.model_config = model_config\n    self.train_config = train_config\n    self.mode = mode\n    assert mode in [\'train\', \'validation\', \'inference\']\n\n    if self.mode == \'train\':\n      self.data_config = self.train_config[\'train_data_config\']\n    elif self.mode == \'validation\':\n      self.data_config = self.train_config[\'validation_data_config\']\n\n    self.dataloader = None\n    self.exemplars = None\n    self.instances = None\n    self.response = None\n    self.batch_loss = None\n    self.total_loss = None\n    self.init_fn = None\n    self.global_step = None\n\n  def is_training(self):\n    """"""Returns true if the model is built for training mode""""""\n    return self.mode == \'train\'\n\n  def build_inputs(self):\n    """"""Input fetching and batching\n\n    Outputs:\n      self.exemplars: image batch of shape [batch, hz, wz, 3]\n      self.instances: image batch of shape [batch, hx, wx, 3]\n    """"""\n    if self.mode in [\'train\', \'validation\']:\n      with tf.device(""/cpu:0""):  # Put data loading and preprocessing in CPU is substantially faster\n        self.dataloader = DataLoader(self.data_config, self.is_training())\n        self.dataloader.build()\n        exemplars, instances = self.dataloader.get_one_batch()\n\n        exemplars = tf.to_float(exemplars)\n        instances = tf.to_float(instances)\n    else:\n      self.examplar_feed = tf.placeholder(shape=[None, None, None, 3],\n                                          dtype=tf.uint8,\n                                          name=\'examplar_input\')\n      self.instance_feed = tf.placeholder(shape=[None, None, None, 3],\n                                          dtype=tf.uint8,\n                                          name=\'instance_input\')\n      exemplars = tf.to_float(self.examplar_feed)\n      instances = tf.to_float(self.instance_feed)\n\n    self.exemplars = exemplars\n    self.instances = instances\n\n  def build_image_embeddings(self, reuse=False):\n    """"""Builds the image model subgraph and generates image embeddings\n\n    Inputs:\n      self.exemplars: A tensor of shape [batch, hz, wz, 3]\n      self.instances: A tensor of shape [batch, hx, wx, 3]\n\n    Outputs:\n      self.exemplar_embeds: A Tensor of shape [batch, hz_embed, wz_embed, embed_dim]\n      self.instance_embeds: A Tensor of shape [batch, hx_embed, wx_embed, embed_dim]\n    """"""\n    config = self.model_config[\'embed_config\']\n    arg_scope = convolutional_alexnet_arg_scope(config,\n                                                trainable=config[\'train_embedding\'],\n                                                is_training=self.is_training())\n\n    @functools.wraps(convolutional_alexnet)\n    def embedding_fn(images, reuse=False):\n      with slim.arg_scope(arg_scope):\n        return convolutional_alexnet(images, reuse=reuse)\n\n    self.exemplar_embeds, _ = embedding_fn(self.exemplars, reuse=reuse)\n    self.instance_embeds, _ = embedding_fn(self.instances, reuse=True)\n\n  def build_template(self):\n    # The template is simply the feature of the exemplar image in SiamFC.\n    self.templates = self.exemplar_embeds\n\n  def build_detection(self, reuse=False):\n    with tf.variable_scope(\'detection\', reuse=reuse):\n      def _translation_match(x, z):  # translation match for one example within a batch\n        x = tf.expand_dims(x, 0)  # [1, in_height, in_width, in_channels]\n        z = tf.expand_dims(z, -1)  # [filter_height, filter_width, in_channels, 1]\n        return tf.nn.conv2d(x, z, strides=[1, 1, 1, 1], padding=\'VALID\', name=\'translation_match\')\n\n      output = tf.map_fn(lambda x: _translation_match(x[0], x[1]),\n                         (self.instance_embeds, self.templates),\n                         dtype=self.instance_embeds.dtype)\n      output = tf.squeeze(output, [1, 4])  # of shape e.g., [8, 15, 15]\n\n      # Adjust score, this is required to make training possible.\n      config = self.model_config[\'adjust_response_config\']\n      bias = tf.get_variable(\'biases\', [1],\n                             dtype=tf.float32,\n                             initializer=tf.constant_initializer(0.0, dtype=tf.float32),\n                             trainable=config[\'train_bias\'])\n      response = config[\'scale\'] * output + bias\n      self.response = response\n\n  def build_loss(self):\n    response = self.response\n    response_size = response.get_shape().as_list()[1:3]  # [height, width]\n\n    gt = construct_gt_score_maps(response_size,\n                                 self.data_config[\'batch_size\'],\n                                 self.model_config[\'embed_config\'][\'stride\'],\n                                 self.train_config[\'gt_config\'])\n\n    with tf.name_scope(\'Loss\'):\n      loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=response,\n                                                     labels=gt)\n\n      with tf.name_scope(\'Balance_weights\'):\n        n_pos = tf.reduce_sum(tf.to_float(tf.equal(gt[0], 1)))\n        n_neg = tf.reduce_sum(tf.to_float(tf.equal(gt[0], 0)))\n        w_pos = 0.5 / n_pos\n        w_neg = 0.5 / n_neg\n        class_weights = tf.where(tf.equal(gt, 1),\n                                 w_pos * tf.ones_like(gt),\n                                 tf.ones_like(gt))\n        class_weights = tf.where(tf.equal(gt, 0),\n                                 w_neg * tf.ones_like(gt),\n                                 class_weights)\n        loss = loss * class_weights\n\n      # Note that we use reduce_sum instead of reduce_mean since the loss has\n      # already been normalized by class_weights in spatial dimension.\n      loss = tf.reduce_sum(loss, [1, 2])\n\n      batch_loss = tf.reduce_mean(loss, name=\'batch_loss\')\n      tf.losses.add_loss(batch_loss)\n\n      total_loss = tf.losses.get_total_loss()\n      self.batch_loss = batch_loss\n      self.total_loss = total_loss\n\n      tf.summary.image(\'exemplar\', self.exemplars, family=self.mode)\n      tf.summary.image(\'instance\', self.instances, family=self.mode)\n\n      mean_batch_loss, update_op1 = tf.metrics.mean(batch_loss)\n      mean_total_loss, update_op2 = tf.metrics.mean(total_loss)\n      with tf.control_dependencies([update_op1, update_op2]):\n        tf.summary.scalar(\'batch_loss\', mean_batch_loss, family=self.mode)\n        tf.summary.scalar(\'total_loss\', mean_total_loss, family=self.mode)\n\n      if self.mode == \'train\':\n        tf.summary.image(\'GT\', tf.reshape(gt[0], [1] + response_size + [1]), family=\'GT\')\n      tf.summary.image(\'Response\', tf.expand_dims(tf.sigmoid(response), -1), family=self.mode)\n      tf.summary.histogram(\'Response\', self.response, family=self.mode)\n\n      # Two more metrics to monitor the performance of training\n      tf.summary.scalar(\'center_score_error\', center_score_error(response), family=self.mode)\n      tf.summary.scalar(\'center_dist_error\', center_dist_error(response), family=self.mode)\n\n  def setup_global_step(self):\n    global_step = tf.Variable(\n      initial_value=0,\n      name=\'global_step\',\n      trainable=False,\n      collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n\n    self.global_step = global_step\n\n  def setup_embedding_initializer(self):\n    """"""Sets up the function to restore embedding variables from checkpoint.""""""\n    embed_config = self.model_config[\'embed_config\']\n    if embed_config[\'embedding_checkpoint_file\']:\n      # Restore Siamese FC models from .mat model files\n      initialize = load_mat_model(embed_config[\'embedding_checkpoint_file\'],\n                                  \'convolutional_alexnet/\', \'detection/\')\n\n      def restore_fn(sess):\n        tf.logging.info(""Restoring embedding variables from checkpoint file %s"",\n                        embed_config[\'embedding_checkpoint_file\'])\n        sess.run([initialize])\n\n      self.init_fn = restore_fn\n\n  def build(self, reuse=False):\n    """"""Creates all ops for training and evaluation""""""\n    with tf.name_scope(self.mode):\n      self.build_inputs()\n      self.build_image_embeddings(reuse=reuse)\n      self.build_template()\n      self.build_detection(reuse=reuse)\n      self.setup_embedding_initializer()\n\n      if self.mode in [\'train\', \'validation\']:\n        self.build_loss()\n\n      if self.is_training():\n        self.setup_global_step()\n'"
train_siamese_model.py,21,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Train the model""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport os.path as osp\nimport random\nimport time\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\nfrom sacred import Experiment\nfrom sacred.observers import FileStorageObserver\n\nimport configuration\nimport siamese_model\nfrom utils.misc_utils import auto_select_gpu, mkdir_p, save_cfgs\n\nex = Experiment(configuration.RUN_NAME)\nex.observers.append(FileStorageObserver.create(osp.join(configuration.LOG_DIR, \'sacred\')))\n\n\n@ex.config\ndef configurations():\n  # Add configurations for current script, for more details please see the documentation of `sacred`.\n  # REFER: http://sacred.readthedocs.io/en/latest/index.html\n  model_config = configuration.MODEL_CONFIG\n  train_config = configuration.TRAIN_CONFIG\n  track_config = configuration.TRACK_CONFIG\n\n\ndef _configure_learning_rate(train_config, global_step):\n  lr_config = train_config[\'lr_config\']\n\n  num_batches_per_epoch = \\\n    int(train_config[\'train_data_config\'][\'num_examples_per_epoch\'] / train_config[\'train_data_config\'][\'batch_size\'])\n\n  lr_policy = lr_config[\'policy\']\n  if lr_policy == \'piecewise_constant\':\n    lr_boundaries = [int(e * num_batches_per_epoch) for e in lr_config[\'lr_boundaries\']]\n    return tf.train.piecewise_constant(global_step,\n                                       lr_boundaries,\n                                       lr_config[\'lr_values\'])\n  elif lr_policy == \'exponential\':\n    decay_steps = int(num_batches_per_epoch) * lr_config[\'num_epochs_per_decay\']\n    return tf.train.exponential_decay(lr_config[\'initial_lr\'],\n                                      global_step,\n                                      decay_steps=decay_steps,\n                                      decay_rate=lr_config[\'lr_decay_factor\'],\n                                      staircase=lr_config[\'staircase\'])\n  elif lr_policy == \'cosine\':\n    T_total = train_config[\'train_data_config\'][\'epoch\'] * num_batches_per_epoch\n    return 0.5 * lr_config[\'initial_lr\'] * (1 + tf.cos(np.pi * tf.to_float(global_step) / T_total))\n  else:\n    raise ValueError(\'Learning rate policy [%s] was not recognized\', lr_policy)\n\n\ndef _configure_optimizer(train_config, learning_rate):\n  optimizer_config = train_config[\'optimizer_config\']\n  optimizer_name = optimizer_config[\'optimizer\'].upper()\n  if optimizer_name == \'MOMENTUM\':\n    optimizer = tf.train.MomentumOptimizer(\n      learning_rate,\n      momentum=optimizer_config[\'momentum\'],\n      use_nesterov=optimizer_config[\'use_nesterov\'],\n      name=\'Momentum\')\n  elif optimizer_name == \'SGD\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', optimizer_config[\'optimizer\'])\n  return optimizer\n\n\n@ex.automain\ndef main(model_config, train_config, track_config):\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = auto_select_gpu()\n\n  # Create training directory which will be used to save: configurations, model files, TensorBoard logs\n  train_dir = train_config[\'train_dir\']\n  if not osp.isdir(train_dir):\n    logging.info(\'Creating training directory: %s\', train_dir)\n    mkdir_p(train_dir)\n\n  g = tf.Graph()\n  with g.as_default():\n    # Set fixed seed for reproducible experiments\n    random.seed(train_config[\'seed\'])\n    np.random.seed(train_config[\'seed\'])\n    tf.set_random_seed(train_config[\'seed\'])\n\n    # Build the training and validation model\n    model = siamese_model.SiameseModel(model_config, train_config, mode=\'train\')\n    model.build()\n    model_va = siamese_model.SiameseModel(model_config, train_config, mode=\'validation\')\n    model_va.build(reuse=True)\n\n    # Save configurations for future reference\n    save_cfgs(train_dir, model_config, train_config, track_config)\n\n    learning_rate = _configure_learning_rate(train_config, model.global_step)\n    optimizer = _configure_optimizer(train_config, learning_rate)\n    tf.summary.scalar(\'learning_rate\', learning_rate)\n\n    # Set up the training ops\n    opt_op = tf.contrib.layers.optimize_loss(\n      loss=model.total_loss,\n      global_step=model.global_step,\n      learning_rate=learning_rate,\n      optimizer=optimizer,\n      clip_gradients=train_config[\'clip_gradients\'],\n      learning_rate_decay_fn=None,\n      summaries=[\'learning_rate\'])\n\n    with tf.control_dependencies([opt_op]):\n      train_op = tf.no_op(name=\'train\')\n\n    saver = tf.train.Saver(tf.global_variables(),\n                           max_to_keep=train_config[\'max_checkpoints_to_keep\'])\n\n    summary_writer = tf.summary.FileWriter(train_dir, g)\n    summary_op = tf.summary.merge_all()\n\n    global_variables_init_op = tf.global_variables_initializer()\n    local_variables_init_op = tf.local_variables_initializer()\n    g.finalize()  # Finalize graph to avoid adding ops by mistake\n\n    # Dynamically allocate GPU memory\n    gpu_options = tf.GPUOptions(allow_growth=True)\n    sess_config = tf.ConfigProto(gpu_options=gpu_options)\n\n    sess = tf.Session(config=sess_config)\n    model_path = tf.train.latest_checkpoint(train_config[\'train_dir\'])\n\n    if not model_path:\n      sess.run(global_variables_init_op)\n      sess.run(local_variables_init_op)\n      start_step = 0\n\n      if model_config[\'embed_config\'][\'embedding_checkpoint_file\']:\n        model.init_fn(sess)\n    else:\n      logging.info(\'Restore from last checkpoint: {}\'.format(model_path))\n      sess.run(local_variables_init_op)\n      saver.restore(sess, model_path)\n      start_step = tf.train.global_step(sess, model.global_step.name) + 1\n\n    # Training loop\n    data_config = train_config[\'train_data_config\']\n    total_steps = int(data_config[\'epoch\'] *\n                      data_config[\'num_examples_per_epoch\'] /\n                      data_config[\'batch_size\'])\n    logging.info(\'Train for {} steps\'.format(total_steps))\n    for step in range(start_step, total_steps):\n      start_time = time.time()\n      _, loss, batch_loss = sess.run([train_op, model.total_loss, model.batch_loss])\n      duration = time.time() - start_time\n\n      if step % 10 == 0:\n        examples_per_sec = data_config[\'batch_size\'] / float(duration)\n        time_remain = data_config[\'batch_size\'] * (total_steps - step) / examples_per_sec\n        m, s = divmod(time_remain, 60)\n        h, m = divmod(m, 60)\n        format_str = (\'%s: step %d, total loss = %.2f, batch loss = %.2f (%.1f examples/sec; %.3f \'\n                      \'sec/batch; %dh:%02dm:%02ds remains)\')\n        logging.info(format_str % (datetime.now(), step, loss, batch_loss,\n                                   examples_per_sec, duration, h, m, s))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      if step % train_config[\'save_model_every_n_step\'] == 0 or (step + 1) == total_steps:\n        checkpoint_path = osp.join(train_config[\'train_dir\'], \'model.ckpt\')\n        saver.save(sess, checkpoint_path, global_step=step)\n'"
benchmarks/__init__.py,0,b''
benchmarks/run_SiamFC.py,5,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\nr""""""Support integration with OTB benchmark""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport sys\nimport time\n\nimport tensorflow as tf\n\n# Code root absolute path\nCODE_ROOT = \'/path/to/SiamFC-TensorFlow\'\n\n# Checkpoint for evaluation\nCHECKPOINT = \'/path/to/SiamFC-TensorFlow/Logs/SiamFC/track_model_checkpoints/SiamFC-3s-color-pretrained\'\n\nsys.path.insert(0, CODE_ROOT)\n\nfrom utils.misc_utils import auto_select_gpu, load_cfgs\nfrom inference import inference_wrapper\nfrom inference.tracker import Tracker\nfrom utils.infer_utils import Rectangle\n\n# Set GPU\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = auto_select_gpu()\ntf.logging.set_verbosity(tf.logging.DEBUG)\n\n\ndef run_SiamFC(seq, rp, bSaveImage):\n  checkpoint_path = CHECKPOINT\n  logging.info(\'Evaluating {}...\'.format(checkpoint_path))\n\n  # Read configurations from json\n  model_config, _, track_config = load_cfgs(checkpoint_path)\n\n  track_config[\'log_level\'] = 0  # Skip verbose logging for speed\n\n  # Build the inference graph.\n  g = tf.Graph()\n  with g.as_default():\n    model = inference_wrapper.InferenceWrapper()\n    restore_fn = model.build_graph_from_config(model_config, track_config, checkpoint_path)\n  g.finalize()\n\n  gpu_options = tf.GPUOptions(allow_growth=True)\n  sess_config = tf.ConfigProto(gpu_options=gpu_options)\n\n  with tf.Session(graph=g, config=sess_config) as sess:\n    # Load the model from checkpoint.\n    restore_fn(sess)\n\n    tracker = Tracker(model, model_config, track_config)\n\n    tic = time.clock()\n    frames = seq.s_frames\n    init_rect = seq.init_rect\n    x, y, width, height = init_rect  # OTB format\n    init_bb = Rectangle(x - 1, y - 1, width, height)\n\n    trajectory_py = tracker.track(sess, init_bb, frames)\n    trajectory = [Rectangle(val.x + 1, val.y + 1, val.width, val.height) for val in\n                  trajectory_py]  # x, y add one to match OTB format\n    duration = time.clock() - tic\n\n    result = dict()\n    result[\'res\'] = trajectory\n    result[\'type\'] = \'rect\'\n    result[\'fps\'] = round(seq.len / duration, 3)\n    return result\n'"
datasets/__init__.py,0,b'\n'
datasets/dataloader.py,10,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nimport tensorflow as tf\n\nfrom datasets.sampler import Sampler\nfrom datasets.transforms import Compose, RandomGray, RandomCrop, CenterCrop, RandomStretch\nfrom datasets.vid import VID\nfrom utils.misc_utils import get\n\n\nclass DataLoader(object):\n  def __init__(self, config, is_training):\n    self.config = config\n    self.is_training = is_training\n\n    preprocess_name = get(config, \'preprocessing_name\', None)\n    logging.info(\'preproces -- {}\'.format(preprocess_name))\n\n    if preprocess_name == \'siamese_fc_color\':\n      self.v_transform = None\n      # TODO: use a single operation (tf.image.crop_and_resize) to achieve all transformations ?\n      self.z_transform = Compose([RandomStretch(),\n                                  CenterCrop((255 - 8, 255 - 8)),\n                                  RandomCrop(255 - 2 * 8),\n                                  CenterCrop((127, 127))])\n      self.x_transform = Compose([RandomStretch(),\n                                  CenterCrop((255 - 8, 255 - 8)),\n                                  RandomCrop(255 - 2 * 8), ])\n    elif preprocess_name == \'siamese_fc_gray\':\n      self.v_transform = RandomGray()\n      self.z_transform = Compose([RandomStretch(),\n                                  CenterCrop((255 - 8, 255 - 8)),\n                                  RandomCrop(255 - 2 * 8),\n                                  CenterCrop((127, 127))])\n      self.x_transform = Compose([RandomStretch(),\n                                  CenterCrop((255 - 8, 255 - 8)),\n                                  RandomCrop(255 - 2 * 8), ])\n    elif preprocess_name == \'None\':\n      self.v_transform = None\n      self.z_transform = CenterCrop((127, 127))\n      self.x_transform = CenterCrop((255, 255))\n    else:\n      raise ValueError(\'Preprocessing name {} was not recognized.\'.format(preprocess_name))\n\n    self.dataset_py = VID(config[\'input_imdb\'], config[\'max_frame_dist\'])\n    self.sampler = Sampler(self.dataset_py, shuffle=is_training)\n\n  def build(self):\n    self.build_dataset()\n    self.build_iterator()\n\n  def build_dataset(self):\n    def sample_generator():\n      for video_id in self.sampler:\n        sample = self.dataset_py[video_id]\n        yield sample\n\n    def transform_fn(video):\n      exemplar_file = tf.read_file(video[0])\n      instance_file = tf.read_file(video[1])\n      exemplar_image = tf.image.decode_jpeg(exemplar_file, channels=3, dct_method=""INTEGER_ACCURATE"")\n      instance_image = tf.image.decode_jpeg(instance_file, channels=3, dct_method=""INTEGER_ACCURATE"")\n\n      if self.v_transform is not None:\n        video = tf.stack([exemplar_image, instance_image])\n        video = self.v_transform(video)\n        exemplar_image = video[0]\n        instance_image = video[1]\n\n      if self.z_transform is not None:\n        exemplar_image = self.z_transform(exemplar_image)\n\n      if self.x_transform is not None:\n        instance_image = self.x_transform(instance_image)\n\n      return exemplar_image, instance_image\n\n    dataset = tf.data.Dataset.from_generator(sample_generator,\n                                             output_types=(tf.string),\n                                             output_shapes=(tf.TensorShape([2])))\n    dataset = dataset.map(transform_fn, num_parallel_calls=self.config[\'prefetch_threads\'])\n    dataset = dataset.prefetch(self.config[\'prefetch_capacity\'])\n    dataset = dataset.repeat()\n    dataset = dataset.batch(self.config[\'batch_size\'])\n    self.dataset_tf = dataset\n\n  def build_iterator(self):\n    self.iterator = self.dataset_tf.make_one_shot_iterator()\n\n  def get_one_batch(self):\n    return self.iterator.get_next()\n'"
datasets/sampler.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Dataset Sampler""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\nclass Sampler(object):\n  def __init__(self, data_source, shuffle=True):\n    self.data_source = data_source\n    self.shuffle = shuffle\n\n  def __iter__(self):\n    data_idxs = np.arange(len(self.data_source))\n    if self.shuffle:\n      np.random.shuffle(data_idxs)\n\n    for idx in data_idxs:\n      yield idx\n\n\nif __name__ == \'__main__\':\n  x = [1, 2, 3]\n  sampler = Sampler(x, shuffle=True)\n  p = 0\n  for xx in sampler:\n    print(x[xx])\n    p += 1\n    if p == 10: break\n'"
datasets/transforms.py,15,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n\n""""""Various transforms for video and image augmentation""""""\n\nimport numbers\n\nimport tensorflow as tf\n\n\nclass Compose(object):\n  """"""Composes several transforms together.""""""\n\n  def __init__(self, transforms):\n    self.transforms = transforms\n\n  def __call__(self, example):\n    for t in self.transforms:\n      example = t(example)\n    return example\n\n\nclass RandomGray(object):\n  def __init__(self, gray_ratio=0.25):\n    self.gray_ratio = gray_ratio\n\n  def __call__(self, img_sequence):\n    def rgb_to_gray():\n      gray_images = tf.image.rgb_to_grayscale(img_sequence)\n      return tf.concat([gray_images] * 3, axis=3)\n\n    def identity():\n      return tf.identity(img_sequence)\n\n    return tf.cond(tf.less(tf.random_uniform([], 0, 1), self.gray_ratio), rgb_to_gray, identity)\n\n\nclass RandomStretch(object):\n  def __init__(self, max_stretch=0.05, interpolation=\'bilinear\'):\n    self.max_stretch = max_stretch\n    self.interpolation = interpolation\n\n  def __call__(self, img):\n    scale = 1.0 + tf.random_uniform([], -self.max_stretch, self.max_stretch)\n    img_shape = tf.shape(img)\n    ts = tf.to_int32(tf.round(tf.to_float(img_shape[:2]) * scale))\n    resize_method_map = {\'bilinear\': tf.image.ResizeMethod.BILINEAR,\n                         \'bicubic\': tf.image.ResizeMethod.BICUBIC}\n    return tf.image.resize_images(img, ts, method=resize_method_map[self.interpolation])\n\n\nclass CenterCrop(object):\n  def __init__(self, size):\n    if isinstance(size, numbers.Number):\n      self.size = (int(size), int(size))\n    else:\n      self.size = size\n\n  def __call__(self, img):\n    th, tw = self.size\n    return tf.image.resize_image_with_crop_or_pad(img, th, tw)\n\n\nclass RandomCrop(object):\n  def __init__(self, size):\n    if isinstance(size, numbers.Number):\n      self.size = (int(size), int(size))\n    else:\n      self.size = size\n\n  def __call__(self, img):\n    img_shape = tf.shape(img)\n    th, tw = self.size\n\n    y1 = tf.random_uniform([], 0, img_shape[0] - th, dtype=tf.int32)\n    x1 = tf.random_uniform([], 0, img_shape[1] - tw, dtype=tf.int32)\n\n    return tf.image.crop_to_bounding_box(img, y1, x1, th, tw)\n'"
datasets/vid.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""VID Dataset""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pickle\n\nimport numpy as np\n\n\ndef downsample(n_in, n_out, max_frame_dist=1):\n  # Get a list of frame distance between consecutive frames\n  max_frame_dist = np.minimum(n_in, max_frame_dist)\n  possible_frame_dist = range(1, max_frame_dist + 1)\n  frame_dist = np.random.choice(possible_frame_dist, n_out - 1)\n  end_to_start_frame_dist = np.sum(frame_dist)\n\n  # Check frame dist boundary\n  possible_max_start_idx = n_in - 1 - end_to_start_frame_dist\n  if possible_max_start_idx < 0:\n    n_extra = - possible_max_start_idx\n    while n_extra > 0:\n      for idx, dist in enumerate(frame_dist):\n        if dist > 1:\n          frame_dist[idx] = dist - 1\n          n_extra -= 1\n          if n_extra == 0: break\n\n  # Get frame dist\n  end_to_start_frame_dist = np.sum(frame_dist)\n  possible_max_start_idx = n_in - 1 - end_to_start_frame_dist\n  start_idx = np.random.choice(possible_max_start_idx + 1, 1)\n  out_idxs = np.cumsum(np.concatenate((start_idx, frame_dist)))\n  return out_idxs\n\n\ndef upsample(n_in, n_out):\n  n_more = n_out - n_in\n  in_idxs = range(n_in)\n  more_idxs = np.random.choice(in_idxs, n_more)\n  out_idxs = sorted(list(in_idxs) + list(more_idxs))\n  return out_idxs\n\n\nclass VID:\n  def __init__(self, imdb_path, max_frame_dist, epoch_size=None):\n    with open(imdb_path, \'rb\') as f:\n      imdb = pickle.load(f)\n\n    self.videos = imdb[\'videos\']\n    self.time_steps = 2\n    self.max_frame_dist = max_frame_dist\n\n    if epoch_size is None:\n      self.epoch_size = len(self.videos)\n    else:\n      self.epoch_size = int(epoch_size)\n\n  def __getitem__(self, index):\n    img_ids = self.videos[index % len(self.videos)]\n    n_frames = len(img_ids)\n\n    if n_frames < self.time_steps:\n      out_idxs = upsample(n_frames, self.time_steps)\n    elif n_frames == self.time_steps:\n      out_idxs = range(n_frames)\n    else:\n      out_idxs = downsample(n_frames, self.time_steps, self.max_frame_dist)\n\n    video = []\n    for j, frame_idx in enumerate(out_idxs):\n      img_path = img_ids[frame_idx]\n      video.append(img_path.encode(\'utf-8\'))\n    return video\n\n  def __len__(self):\n    return self.epoch_size\n'"
embeddings/__init__.py,0,b''
embeddings/convolutional_alexnet.py,12,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Contains definitions of the network in [1].\n\n  [1] Bertinetto, L., et al. (2016).\n      ""Fully-Convolutional Siamese Networks for Object Tracking.""\n      arXiv preprint arXiv:1606.09549.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nimport tensorflow as tf\n\nfrom utils.misc_utils import get\n\nslim = tf.contrib.slim\n\n\ndef convolutional_alexnet_arg_scope(embed_config,\n                                    trainable=True,\n                                    is_training=False):\n  """"""Defines the default arg scope.\n\n  Args:\n    embed_config: A dictionary which contains configurations for the embedding function.\n    trainable: If the weights in the embedding function is trainable.\n    is_training: If the embedding function is built for training.\n\n  Returns:\n    An `arg_scope` to use for the convolutional_alexnet models.\n  """"""\n  # Only consider the model to be in training mode if it\'s trainable.\n  # This is vital for batch_norm since moving_mean and moving_variance\n  # will get updated even if not trainable.\n  is_model_training = trainable and is_training\n\n  if get(embed_config, \'use_bn\', True):\n    batch_norm_scale = get(embed_config, \'bn_scale\', True)\n    batch_norm_decay = 1 - get(embed_config, \'bn_momentum\', 3e-4)\n    batch_norm_epsilon = get(embed_config, \'bn_epsilon\', 1e-6)\n    batch_norm_params = {\n      ""scale"": batch_norm_scale,\n      # Decay for the moving averages.\n      ""decay"": batch_norm_decay,\n      # Epsilon to prevent 0s in variance.\n      ""epsilon"": batch_norm_epsilon,\n      ""trainable"": trainable,\n      ""is_training"": is_model_training,\n      # Collection containing the moving mean and moving variance.\n      ""variables_collections"": {\n        ""beta"": None,\n        ""gamma"": None,\n        ""moving_mean"": [""moving_vars""],\n        ""moving_variance"": [""moving_vars""],\n      },\n      \'updates_collections\': None,  # Ensure that updates are done within a frame\n    }\n    normalizer_fn = slim.batch_norm\n  else:\n    batch_norm_params = {}\n    normalizer_fn = None\n\n  weight_decay = get(embed_config, \'weight_decay\', 5e-4)\n  if trainable:\n    weights_regularizer = slim.l2_regularizer(weight_decay)\n  else:\n    weights_regularizer = None\n\n  init_method = get(embed_config, \'init_method\', \'kaiming_normal\')\n  if is_model_training:\n    logging.info(\'embedding init method -- {}\'.format(init_method))\n  if init_method == \'kaiming_normal\':\n    # The same setting as siamese-fc\n    initializer = slim.variance_scaling_initializer(factor=2.0, mode=\'FAN_OUT\', uniform=False)\n  else:\n    initializer = slim.xavier_initializer()\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=weights_regularizer,\n      weights_initializer=initializer,\n      padding=\'VALID\',\n      trainable=trainable,\n      activation_fn=tf.nn.relu,\n      normalizer_fn=normalizer_fn,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.batch_norm], is_training=is_model_training) as arg_sc:\n        return arg_sc\n\n\ndef convolutional_alexnet(inputs, reuse=None, scope=\'convolutional_alexnet\'):\n  """"""Defines the feature extractor of SiamFC.\n\n  Args:\n    inputs: a Tensor of shape [batch, h, w, c].\n    reuse: if the weights in the embedding function are reused.\n    scope: the variable scope of the computational graph.\n\n  Returns:\n    net: the computed features of the inputs.\n    end_points: the intermediate outputs of the embedding function.\n  """"""\n  with tf.variable_scope(scope, \'convolutional_alexnet\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = inputs\n      net = slim.conv2d(net, 96, [11, 11], 2, scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      with tf.variable_scope(\'conv2\'):\n        b1, b2 = tf.split(net, 2, 3)\n        b1 = slim.conv2d(b1, 128, [5, 5], scope=\'b1\')\n        # The original implementation has bias terms for all convolution, but\n        # it actually isn\'t necessary if the convolution layer is followed by a batch\n        # normalization layer since batch norm will subtract the mean.\n        b2 = slim.conv2d(b2, 128, [5, 5], scope=\'b2\')\n        net = tf.concat([b1, b2], 3)\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], 1, scope=\'conv3\')\n      with tf.variable_scope(\'conv4\'):\n        b1, b2 = tf.split(net, 2, 3)\n        b1 = slim.conv2d(b1, 192, [3, 3], 1, scope=\'b1\')\n        b2 = slim.conv2d(b2, 192, [3, 3], 1, scope=\'b2\')\n        net = tf.concat([b1, b2], 3)\n      # Conv 5 with only convolution, has bias\n      with tf.variable_scope(\'conv5\'):\n        with slim.arg_scope([slim.conv2d],\n                            activation_fn=None, normalizer_fn=None):\n          b1, b2 = tf.split(net, 2, 3)\n          b1 = slim.conv2d(b1, 128, [3, 3], 1, scope=\'b1\')\n          b2 = slim.conv2d(b2, 128, [3, 3], 1, scope=\'b2\')\n        net = tf.concat([b1, b2], 3)\n      # Convert end_points_collection into a dictionary of end_points.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      return net, end_points\n\n\nconvolutional_alexnet.stride = 8\n'"
experiments/SiamFC-3s-color-pretrained.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Load pretrained color model in the SiamFC paper and save it in the TensorFlow format""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom configuration import LOG_DIR\nfrom scripts.convert_pretrained_model import ex\n\nif __name__ == \'__main__\':\n  RUN_NAME = \'SiamFC-3s-color-pretrained\'\n  ex.run(config_updates={\'model_config\': {\'embed_config\': {\'embedding_checkpoint_file\': \'assets/2016-08-17.net.mat\',\n                                                           \'train_embedding\': False, },\n                                          },\n                         \'train_config\': {\'train_dir\': osp.join(LOG_DIR, \'track_model_checkpoints\', RUN_NAME), },\n                         \'track_config\': {\'log_dir\': osp.join(LOG_DIR, \'track_model_inference\', RUN_NAME), }\n                         },\n         options={\'--name\': RUN_NAME,\n                  \'--force\': True,\n                  \'--enforce_clean\': False,\n                  })\n'"
experiments/SiamFC-3s-color-scratch.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Train the color model in the SiamFC paper from scratch""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom configuration import LOG_DIR\nfrom train_siamese_model import ex\n\nif __name__ == \'__main__\':\n  RUN_NAME = \'SiamFC-3s-color-scratch\'\n  ex.run(config_updates={\'train_config\': {\'train_dir\': osp.join(LOG_DIR, \'track_model_checkpoints\', RUN_NAME), },\n                         \'track_config\': {\'log_dir\': osp.join(LOG_DIR, \'track_model_inference\', RUN_NAME), }\n                         },\n         options={\'--name\': RUN_NAME,\n                  \'--force\': True,\n                  \'--enforce_clean\': False,\n                  })\n'"
experiments/SiamFC-3s-gray-pretrained.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Load pretrained color model in the SiamFC paper and save it in the TensorFlow format""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom configuration import LOG_DIR\nfrom scripts.convert_pretrained_model import ex\n\nif __name__ == \'__main__\':\n  RUN_NAME = \'SiamFC-3s-gray-pretrained\'\n  ex.run(\n    config_updates={\'model_config\': {\'embed_config\': {\'embedding_checkpoint_file\': \'assets/2016-08-17_gray025.net.mat\',\n                                                      \'train_embedding\': False, },\n                                     },\n                    \'train_config\': {\'train_dir\': osp.join(LOG_DIR, \'track_model_checkpoints\', RUN_NAME), },\n                    \'track_config\': {\'log_dir\': osp.join(LOG_DIR, \'track_model_inference\', RUN_NAME), }\n                    },\n    options={\'--name\': RUN_NAME,\n             \'--force\': True,\n             \'--enforce_clean\': False,\n             })\n'"
experiments/SiamFC-3s-gray-scratch.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Train the color model in the SiamFC paper from scratch""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom configuration import LOG_DIR\nfrom train_siamese_model import ex\n\nif __name__ == \'__main__\':\n  RUN_NAME = \'SiamFC-3s-gray-scratch\'\n  ex.run(config_updates={\'train_config\': {\'train_data_config\': {\'preprocessing_name\': \'siamese_fc_gray\', },\n                                          \'train_dir\': osp.join(LOG_DIR, \'track_model_checkpoints\', RUN_NAME), },\n                         \'track_config\': {\'log_dir\': osp.join(LOG_DIR, \'track_model_inference\', RUN_NAME), }\n                         },\n         options={\'--name\': RUN_NAME,\n                  \'--force\': True,\n                  \'--enforce_clean\': False,\n                  })\n'"
inference/__init__.py,0,b''
inference/inference_wrapper.py,49,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Model Wrapper class for performing inference with a SiameseModel""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport logging\nimport os\nimport os.path as osp\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom embeddings.convolutional_alexnet import convolutional_alexnet_arg_scope, convolutional_alexnet\nfrom utils.infer_utils import get_exemplar_images\nfrom utils.misc_utils import get_center\n\nslim = tf.contrib.slim\n\n\nclass InferenceWrapper():\n  """"""Model wrapper class for performing inference with a siamese model.""""""\n\n  def __init__(self):\n    self.image = None\n    self.target_bbox_feed = None\n    self.search_images = None\n    self.embeds = None\n    self.templates = None\n    self.init = None\n    self.model_config = None\n    self.track_config = None\n    self.response_up = None\n\n  def build_graph_from_config(self, model_config, track_config, checkpoint_path):\n    """"""Build the inference graph and return a restore function.""""""\n    self.build_model(model_config, track_config)\n    ema = tf.train.ExponentialMovingAverage(0)\n    variables_to_restore = ema.variables_to_restore(moving_avg_variables=[])\n\n    # Filter out State variables\n    variables_to_restore_filterd = {}\n    for key, value in variables_to_restore.items():\n      if key.split(\'/\')[1] != \'State\':\n        variables_to_restore_filterd[key] = value\n\n    saver = tf.train.Saver(variables_to_restore_filterd)\n\n    if osp.isdir(checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n      if not checkpoint_path:\n        raise ValueError(""No checkpoint file found in: {}"".format(checkpoint_path))\n\n    def _restore_fn(sess):\n      logging.info(""Loading model from checkpoint: %s"", checkpoint_path)\n      saver.restore(sess, checkpoint_path)\n      logging.info(""Successfully loaded checkpoint: %s"", os.path.basename(checkpoint_path))\n\n    return _restore_fn\n\n  def build_model(self, model_config, track_config):\n    self.model_config = model_config\n    self.track_config = track_config\n\n    self.build_inputs()\n    self.build_search_images()\n    self.build_template()\n    self.build_detection()\n    self.build_upsample()\n    self.dumb_op = tf.no_op(\'dumb_operation\')\n\n  def build_inputs(self):\n    filename = tf.placeholder(tf.string, [], name=\'filename\')\n    image_file = tf.read_file(filename)\n    image = tf.image.decode_jpeg(image_file, channels=3, dct_method=""INTEGER_ACCURATE"")\n    image = tf.to_float(image)\n    self.image = image\n    self.target_bbox_feed = tf.placeholder(dtype=tf.float32,\n                                           shape=[4],\n                                           name=\'target_bbox_feed\')  # center\'s y, x, height, width\n\n  def build_search_images(self):\n    """"""Crop search images from the input image based on the last target position\n\n    1. The input image is scaled such that the area of target&context takes up to (scale_factor * z_image_size) ^ 2\n    2. Crop an image patch as large as x_image_size centered at the target center.\n    3. If the cropped image region is beyond the boundary of the input image, mean values are padded.\n    """"""\n    model_config = self.model_config\n    track_config = self.track_config\n\n    size_z = model_config[\'z_image_size\']\n    size_x = track_config[\'x_image_size\']\n    context_amount = 0.5\n\n    num_scales = track_config[\'num_scales\']\n    scales = np.arange(num_scales) - get_center(num_scales)\n    assert np.sum(scales) == 0, \'scales should be symmetric\'\n    search_factors = [track_config[\'scale_step\'] ** x for x in scales]\n\n    frame_sz = tf.shape(self.image)\n    target_yx = self.target_bbox_feed[0:2]\n    target_size = self.target_bbox_feed[2:4]\n    avg_chan = tf.reduce_mean(self.image, axis=(0, 1), name=\'avg_chan\')\n\n    # Compute base values\n    base_z_size = target_size\n    base_z_context_size = base_z_size + context_amount * tf.reduce_sum(base_z_size)\n    base_s_z = tf.sqrt(tf.reduce_prod(base_z_context_size))  # Canonical size\n    base_scale_z = tf.div(tf.to_float(size_z), base_s_z)\n    d_search = (size_x - size_z) / 2.0\n    base_pad = tf.div(d_search, base_scale_z)\n    base_s_x = base_s_z + 2 * base_pad\n    base_scale_x = tf.div(tf.to_float(size_x), base_s_x)\n\n    boxes = []\n    for factor in search_factors:\n      s_x = factor * base_s_x\n      frame_sz_1 = tf.to_float(frame_sz[0:2] - 1)\n      topleft = tf.div(target_yx - get_center(s_x), frame_sz_1)\n      bottomright = tf.div(target_yx + get_center(s_x), frame_sz_1)\n      box = tf.concat([topleft, bottomright], axis=0)\n      boxes.append(box)\n    boxes = tf.stack(boxes)\n\n    scale_xs = []\n    for factor in search_factors:\n      scale_x = base_scale_x / factor\n      scale_xs.append(scale_x)\n    self.scale_xs = tf.stack(scale_xs)\n\n    # Note we use different padding values for each image\n    # while the original implementation uses only the average value\n    # of the first image for all images.\n    image_minus_avg = tf.expand_dims(self.image - avg_chan, 0)\n    image_cropped = tf.image.crop_and_resize(image_minus_avg, boxes,\n                                             box_ind=tf.zeros((track_config[\'num_scales\']), tf.int32),\n                                             crop_size=[size_x, size_x])\n    self.search_images = image_cropped + avg_chan\n\n  def get_image_embedding(self, images, reuse=None):\n    config = self.model_config[\'embed_config\']\n    arg_scope = convolutional_alexnet_arg_scope(config,\n                                                trainable=config[\'train_embedding\'],\n                                                is_training=False)\n\n    @functools.wraps(convolutional_alexnet)\n    def embedding_fn(images, reuse=False):\n      with slim.arg_scope(arg_scope):\n        return convolutional_alexnet(images, reuse=reuse)\n\n    embed, _ = embedding_fn(images, reuse)\n\n    return embed\n\n  def build_template(self):\n    model_config = self.model_config\n    track_config = self.track_config\n\n    # Exemplar image lies at the center of the search image in the first frame\n    exemplar_images = get_exemplar_images(self.search_images, [model_config[\'z_image_size\'],\n                                                               model_config[\'z_image_size\']])\n    templates = self.get_image_embedding(exemplar_images)\n    center_scale = int(get_center(track_config[\'num_scales\']))\n    center_template = tf.identity(templates[center_scale])\n    templates = tf.stack([center_template for _ in range(track_config[\'num_scales\'])])\n\n    with tf.variable_scope(\'target_template\'):\n      # Store template in Variable such that we don\'t have to feed this template every time.\n      with tf.variable_scope(\'State\'):\n        state = tf.get_variable(\'exemplar\',\n                                initializer=tf.zeros(templates.get_shape().as_list(), dtype=templates.dtype),\n                                trainable=False)\n        with tf.control_dependencies([templates]):\n          self.init = tf.assign(state, templates, validate_shape=True)\n        self.templates = state\n\n  def build_detection(self):\n    self.embeds = self.get_image_embedding(self.search_images, reuse=True)\n    with tf.variable_scope(\'detection\'):\n      def _translation_match(x, z):\n        x = tf.expand_dims(x, 0)  # [batch, in_height, in_width, in_channels]\n        z = tf.expand_dims(z, -1)  # [filter_height, filter_width, in_channels, out_channels]\n        return tf.nn.conv2d(x, z, strides=[1, 1, 1, 1], padding=\'VALID\', name=\'translation_match\')\n\n      output = tf.map_fn(\n        lambda x: _translation_match(x[0], x[1]),\n        (self.embeds, self.templates), dtype=self.embeds.dtype)  # of shape [3, 1, 17, 17, 1]\n      output = tf.squeeze(output, [1, 4])  # of shape e.g. [3, 17, 17]\n\n      bias = tf.get_variable(\'biases\', [1],\n                             dtype=tf.float32,\n                             initializer=tf.constant_initializer(0.0, dtype=tf.float32),\n                             trainable=False)\n      response = self.model_config[\'adjust_response_config\'][\'scale\'] * output + bias\n      self.response = response\n\n  def build_upsample(self):\n    """"""Upsample response to obtain finer target position""""""\n    with tf.variable_scope(\'upsample\'):\n      response = tf.expand_dims(self.response, 3)\n      up_method = self.track_config[\'upsample_method\']\n      methods = {\'bilinear\': tf.image.ResizeMethod.BILINEAR,\n                 \'bicubic\': tf.image.ResizeMethod.BICUBIC}\n      up_method = methods[up_method]\n      response_spatial_size = self.response.get_shape().as_list()[1:3]\n      up_size = [s * self.track_config[\'upsample_factor\'] for s in response_spatial_size]\n      response_up = tf.image.resize_images(response,\n                                           up_size,\n                                           method=up_method,\n                                           align_corners=True)\n      response_up = tf.squeeze(response_up, [3])\n      self.response_up = response_up\n\n  def initialize(self, sess, input_feed):\n    image_path, target_bbox = input_feed\n    scale_xs, _ = sess.run([self.scale_xs, self.init],\n                           feed_dict={\'filename:0\': image_path,\n                                      ""target_bbox_feed:0"": target_bbox, })\n    return scale_xs\n\n  def inference_step(self, sess, input_feed):\n    image_path, target_bbox = input_feed\n    log_level = self.track_config[\'log_level\']\n    image_cropped_op = self.search_images if log_level > 0 else self.dumb_op\n    image_cropped, scale_xs, response_output = sess.run(\n      fetches=[image_cropped_op, self.scale_xs, self.response_up],\n      feed_dict={\n        ""filename:0"": image_path,\n        ""target_bbox_feed:0"": target_bbox, })\n\n    output = {\n      \'image_cropped\': image_cropped,\n      \'scale_xs\': scale_xs,\n      \'response\': response_output}\n    return output, None\n'"
inference/tracker.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n""""""Class for tracking using a track model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os.path as osp\n\nimport numpy as np\nimport cv2\nfrom cv2 import imwrite\n\nfrom utils.infer_utils import convert_bbox_format, Rectangle\nfrom utils.misc_utils import get_center, get\n\n\nclass TargetState(object):\n  """"""Represent the target state.""""""\n\n  def __init__(self, bbox, search_pos, scale_idx):\n    self.bbox = bbox  # (cx, cy, w, h) in the original image\n    self.search_pos = search_pos  # target center position in the search image\n    self.scale_idx = scale_idx  # scale index in the searched scales\n\n\nclass Tracker(object):\n  """"""Tracker based on the siamese model.""""""\n\n  def __init__(self, siamese_model, model_config, track_config):\n    self.siamese_model = siamese_model\n    self.model_config = model_config\n    self.track_config = track_config\n\n    self.num_scales = track_config[\'num_scales\']\n    logging.info(\'track num scales -- {}\'.format(self.num_scales))\n    scales = np.arange(self.num_scales) - get_center(self.num_scales)\n    self.search_factors = [self.track_config[\'scale_step\'] ** x for x in scales]\n\n    self.x_image_size = track_config[\'x_image_size\']  # Search image size\n    self.window = None  # Cosine window\n    self.log_level = track_config[\'log_level\']\n\n  def track(self, sess, first_bbox, frames, logdir=\'/tmp\'):\n    """"""Runs tracking on a single image sequence.""""""\n    # Get initial target bounding box and convert to center based\n    bbox = convert_bbox_format(first_bbox, \'center-based\')\n\n    # Feed in the first frame image to set initial state.\n    bbox_feed = [bbox.y, bbox.x, bbox.height, bbox.width]\n    input_feed = [frames[0], bbox_feed]\n    frame2crop_scale = self.siamese_model.initialize(sess, input_feed)\n\n    # Storing target state\n    original_target_height = bbox.height\n    original_target_width = bbox.width\n    search_center = np.array([get_center(self.x_image_size),\n                              get_center(self.x_image_size)])\n    current_target_state = TargetState(bbox=bbox,\n                                       search_pos=search_center,\n                                       scale_idx=int(get_center(self.num_scales)))\n\n    include_first = get(self.track_config, \'include_first\', False)\n    logging.info(\'Tracking include first -- {}\'.format(include_first))\n\n    # Run tracking loop\n    reported_bboxs = []\n    for i, filename in enumerate(frames):\n      if i > 0 or include_first:  # We don\'t really want to process the first image unless intended to do so.\n        bbox_feed = [current_target_state.bbox.y, current_target_state.bbox.x,\n                     current_target_state.bbox.height, current_target_state.bbox.width]\n        input_feed = [filename, bbox_feed]\n\n        outputs, metadata = self.siamese_model.inference_step(sess, input_feed)\n        search_scale_list = outputs[\'scale_xs\']\n        response = outputs[\'response\']\n        response_size = response.shape[1]\n\n        # Choose the scale whole response map has the highest peak\n        if self.num_scales > 1:\n          response_max = np.max(response, axis=(1, 2))\n          penalties = self.track_config[\'scale_penalty\'] * np.ones((self.num_scales))\n          current_scale_idx = int(get_center(self.num_scales))\n          penalties[current_scale_idx] = 1.0\n          response_penalized = response_max * penalties\n          best_scale = np.argmax(response_penalized)\n        else:\n          best_scale = 0\n\n        response = response[best_scale]\n\n        with np.errstate(all=\'raise\'):  # Raise error if something goes wrong\n          response = response - np.min(response)\n          response = response / np.sum(response)\n\n        if self.window is None:\n          window = np.dot(np.expand_dims(np.hanning(response_size), 1),\n                          np.expand_dims(np.hanning(response_size), 0))\n          self.window = window / np.sum(window)  # normalize window\n        window_influence = self.track_config[\'window_influence\']\n        response = (1 - window_influence) * response + window_influence * self.window\n\n        # Find maximum response\n        r_max, c_max = np.unravel_index(response.argmax(),\n                                        response.shape)\n\n        # Convert from crop-relative coordinates to frame coordinates\n        p_coor = np.array([r_max, c_max])\n        # displacement from the center in instance final representation ...\n        disp_instance_final = p_coor - get_center(response_size)\n        # ... in instance feature space ...\n        upsample_factor = self.track_config[\'upsample_factor\']\n        disp_instance_feat = disp_instance_final / upsample_factor\n        # ... Avoid empty position ...\n        r_radius = int(response_size / upsample_factor / 2)\n        disp_instance_feat = np.maximum(np.minimum(disp_instance_feat, r_radius), -r_radius)\n        # ... in instance input ...\n        disp_instance_input = disp_instance_feat * self.model_config[\'embed_config\'][\'stride\']\n        # ... in instance original crop (in frame coordinates)\n        disp_instance_frame = disp_instance_input / search_scale_list[best_scale]\n        # Position within frame in frame coordinates\n        y = current_target_state.bbox.y\n        x = current_target_state.bbox.x\n        y += disp_instance_frame[0]\n        x += disp_instance_frame[1]\n\n        # Target scale damping and saturation\n        target_scale = current_target_state.bbox.height / original_target_height\n        search_factor = self.search_factors[best_scale]\n        scale_damp = self.track_config[\'scale_damp\']  # damping factor for scale update\n        target_scale *= ((1 - scale_damp) * 1.0 + scale_damp * search_factor)\n        target_scale = np.maximum(0.2, np.minimum(5.0, target_scale))\n\n        # Some book keeping\n        height = original_target_height * target_scale\n        width = original_target_width * target_scale\n        current_target_state.bbox = Rectangle(x, y, width, height)\n        current_target_state.scale_idx = best_scale\n        current_target_state.search_pos = search_center + disp_instance_input\n\n        assert 0 <= current_target_state.search_pos[0] < self.x_image_size, \\\n          \'target position in feature space should be no larger than input image size\'\n        assert 0 <= current_target_state.search_pos[1] < self.x_image_size, \\\n          \'target position in feature space should be no larger than input image size\'\n\n        if self.log_level > 0:\n          np.save(osp.join(logdir, \'num_frames.npy\'), [i + 1])\n\n          # Select the image with the highest score scale and convert it to uint8\n          image_cropped = outputs[\'image_cropped\'][best_scale].astype(np.uint8)\n          # Note that imwrite in cv2 assumes the image is in BGR format.\n          # However, the cropped image returned by TensorFlow is RGB.\n          # Therefore, we convert color format using cv2.cvtColor\n          imwrite(osp.join(logdir, \'image_cropped{}.jpg\'.format(i)),\n                  cv2.cvtColor(image_cropped, cv2.COLOR_RGB2BGR))\n\n          np.save(osp.join(logdir, \'best_scale{}.npy\'.format(i)), [best_scale])\n          np.save(osp.join(logdir, \'response{}.npy\'.format(i)), response)\n\n          y_search, x_search = current_target_state.search_pos\n          search_scale = search_scale_list[best_scale]\n          target_height_search = height * search_scale\n          target_width_search = width * search_scale\n          bbox_search = Rectangle(x_search, y_search, target_width_search, target_height_search)\n          bbox_search = convert_bbox_format(bbox_search, \'top-left-based\')\n          np.save(osp.join(logdir, \'bbox{}.npy\'.format(i)),\n                  [bbox_search.x, bbox_search.y, bbox_search.width, bbox_search.height])\n\n      reported_bbox = convert_bbox_format(current_target_state.bbox, \'top-left-based\')\n      reported_bboxs.append(reported_bbox)\n    return reported_bboxs\n'"
metrics/__init__.py,0,b''
metrics/track_metrics.py,33,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops.metrics_impl import _confusion_matrix_at_thresholds\n\n\ndef _auc(labels, predictions, weights=None, num_thresholds=200,\n         metrics_collections=None, updates_collections=None,\n         curve=\'ROC\', name=None, summation_method=\'trapezoidal\'):\n  """"""Computes the approximate AUC via a Riemann sum.\n\n  Modified version of tf.metrics.auc. Add support for AUC computation\n  of the recall curve.\n  """"""\n  with tf.variable_scope(\n      name, \'auc\', (labels, predictions, weights)):\n    if curve != \'ROC\' and curve != \'PR\' and curve != \'R\':\n      raise ValueError(\'curve must be either ROC, PR or R, %s unknown\' %\n                       (curve))\n    kepsilon = 1e-7  # to account for floating point imprecisions\n    thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n                  for i in range(num_thresholds - 2)]\n    thresholds = [0.0 - kepsilon] + thresholds + [1.0 + kepsilon]\n\n    values, update_ops = _confusion_matrix_at_thresholds(\n      labels, predictions, thresholds, weights)\n\n    # Add epsilons to avoid dividing by 0.\n    epsilon = 1.0e-6\n\n    def compute_auc(tp, fn, tn, fp, name):\n      """"""Computes the roc-auc or pr-auc based on confusion counts.""""""\n      rec = tf.div(tp + epsilon, tp + fn + epsilon)\n      if curve == \'ROC\':\n        fp_rate = tf.div(fp, fp + tn + epsilon)\n        x = fp_rate\n        y = rec\n      elif curve == \'R\':  # recall auc\n        x = tf.linspace(1., 0., num_thresholds)\n        y = rec\n      else:  # curve == \'PR\'.\n        prec = tf.div(tp + epsilon, tp + fp + epsilon)\n        x = rec\n        y = prec\n      if summation_method == \'trapezoidal\':\n        return tf.reduce_sum(\n          tf.multiply(x[:num_thresholds - 1] - x[1:],\n                      (y[:num_thresholds - 1] + y[1:]) / 2.),\n          name=name)\n      elif summation_method == \'minoring\':\n        return tf.reduce_sum(\n          tf.multiply(x[:num_thresholds - 1] - x[1:],\n                      tf.minimum(y[:num_thresholds - 1], y[1:])),\n          name=name)\n      elif summation_method == \'majoring\':\n        return tf.reduce_sum(\n          tf.multiply(x[:num_thresholds - 1] - x[1:],\n                      tf.maximum(y[:num_thresholds - 1], y[1:])),\n          name=name)\n      else:\n        raise ValueError(\'Invalid summation_method: %s\' % summation_method)\n\n    # sum up the areas of all the trapeziums\n    auc_value = compute_auc(\n      values[\'tp\'], values[\'fn\'], values[\'tn\'], values[\'fp\'], \'value\')\n    update_op = compute_auc(\n      update_ops[\'tp\'], update_ops[\'fn\'], update_ops[\'tn\'], update_ops[\'fp\'],\n      \'update_op\')\n\n    if metrics_collections:\n      ops.add_to_collections(metrics_collections, auc_value)\n\n    if updates_collections:\n      ops.add_to_collections(updates_collections, update_op)\n\n    return auc_value, update_op\n\n\ndef get_center_index(response):\n  """"""Get the index of the center in the response map""""""\n  shape = tf.shape(response)\n  c1 = tf.to_int32((shape[1] - 1) / 2)\n  c2 = tf.to_int32((shape[2] - 1) / 2)\n  return c1, c2\n\n\ndef center_score_error(response):\n  """"""Center score error.\n\n  The error is low when the center of the response map is classified as target.\n  """"""\n  with tf.name_scope(\'CS-err\'):\n    r, c = get_center_index(response)\n    center_score = response[:, r, c]\n    mean, update_op = tf.metrics.mean(tf.to_float(center_score < 0))\n    with tf.control_dependencies([update_op]):\n      mean = tf.identity(mean)\n    return mean\n\n\ndef get_maximum_index(response):\n  """"""Get the index of the maximum value in the response map""""""\n  response_shape = response.get_shape().as_list()\n  response_spatial_size = response_shape[-2:]  # e.g. [29, 29]\n  length = response_spatial_size[0] * response_spatial_size[1]\n\n  # Get maximum response index (note index starts from zero)\n  ind_max = tf.argmax(tf.reshape(response, [-1, length]), 1)\n  ind_row = tf.div(ind_max, response_spatial_size[1])\n  ind_col = tf.mod(ind_max, response_spatial_size[1])\n  return ind_row, ind_col\n\n\ndef center_dist_error(response):\n  """"""Center distance error.\n\n  The error is low when the maximum response is at the center of the response map.\n  """"""\n  with tf.name_scope(\'CD-err\'):\n    radius_in_pixel = 50.\n    total_stride = 8.\n    num_thresholds = 100\n    radius_in_response = radius_in_pixel / total_stride\n\n    gt_r, gt_c = get_center_index(response)\n    max_r, max_c = get_maximum_index(response)\n    gt_r = tf.to_float(gt_r)\n    gt_c = tf.to_float(gt_c)\n    max_r = tf.to_float(max_r)\n    max_c = tf.to_float(max_c)\n    distances = tf.sqrt((gt_r - max_r) ** 2 + (gt_c - max_c) ** 2)\n\n    # We cast distances as prediction accuracies in the range [0, 1] where 0 means fail and\n    # 1 means success. In this way, we can readily use streaming_auc to compute area\n    # under curve.\n    dist_norm = distances / radius_in_response\n    dist_norm = tf.minimum(dist_norm, 1.)\n    predictions = 1. - dist_norm\n    labels = tf.ones_like(predictions)\n\n    auc, update_op = _auc(labels, predictions, num_thresholds=num_thresholds, curve=\'R\')\n    with tf.control_dependencies([update_op]):\n      err = 1. - auc\n    return err\n'"
scripts/__init__.py,0,b''
scripts/build_VID2015_imdb.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Save the paths of crops from the ImageNet VID 2015 dataset in pickle format""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os\nimport os.path as osp\nimport pickle\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom utils.misc_utils import sort_nicely\n\n\nclass Config:\n  ### Dataset\n  # directory where curated dataset is stored\n  dataset_dir = \'data/ILSVRC2015-VID-Curation\'\n  save_dir = \'data/\'\n\n  # percentage of all videos for validation\n  validation_ratio = 0.1\n\n\nclass DataIter:\n  """"""Container for dataset of one iteration""""""\n  pass\n\n\nclass Dataset:\n  def __init__(self, config):\n    self.config = config\n\n  def _get_unique_trackids(self, video_dir):\n    """"""Get unique trackids within video_dir""""""\n    x_image_paths = glob.glob(video_dir + \'/*.crop.x.jpg\')\n    trackids = [os.path.basename(path).split(\'.\')[1] for path in x_image_paths]\n    unique_trackids = set(trackids)\n    return unique_trackids\n\n  def dataset_iterator(self, video_dirs):\n    video_num = len(video_dirs)\n    iter_size = 150\n    iter_num = int(np.ceil(video_num / float(iter_size)))\n    for iter_ in range(iter_num):\n      iter_start = iter_ * iter_size\n      iter_videos = video_dirs[iter_start: iter_start + iter_size]\n\n      data_iter = DataIter()\n      num_videos = len(iter_videos)\n      instance_videos = []\n      for index in range(num_videos):\n        print(\'Processing {}/{}...\'.format(iter_start + index, video_num))\n        video_dir = iter_videos[index]\n        trackids = self._get_unique_trackids(video_dir)\n\n        for trackid in trackids:\n          instance_image_paths = glob.glob(video_dir + \'/*\' + trackid + \'.crop.x.jpg\')\n\n          # sort image paths by frame number\n          instance_image_paths = sort_nicely(instance_image_paths)\n\n          # get image absolute path\n          instance_image_paths = [os.path.abspath(p) for p in instance_image_paths]\n          instance_videos.append(instance_image_paths)\n      data_iter.num_videos = len(instance_videos)\n      data_iter.instance_videos = instance_videos\n      yield data_iter\n\n  def get_all_video_dirs(self):\n    ann_dir = os.path.join(self.config.dataset_dir, \'Data\', \'VID\')\n    all_video_dirs = []\n\n    # We have already combined all training and validation videos in ILSVRC2015 and put them in the `train` directory.\n    # The file structure is like:\n    # train\n    #    |- a\n    #    |- b\n    #    |_ c\n    #       |- ILSVRC2015_train_00024001\n    #       |- ILSVRC2015_train_00024002\n    #       |_ ILSVRC2015_train_00024003\n    #               |- 000045.00.crop.x.jpg\n    #               |- 000046.00.crop.x.jpg\n    #               |- ...\n    train_dirs = os.listdir(os.path.join(ann_dir, \'train\'))\n    for dir_ in train_dirs:\n      train_sub_dir = os.path.join(ann_dir, \'train\', dir_)\n      video_names = os.listdir(train_sub_dir)\n      train_video_dirs = [os.path.join(train_sub_dir, name) for name in video_names]\n      all_video_dirs = all_video_dirs + train_video_dirs\n\n    return all_video_dirs\n\n\ndef main():\n  # Get the data.\n  config = Config()\n  dataset = Dataset(config)\n  all_video_dirs = dataset.get_all_video_dirs()\n  num_validation = int(len(all_video_dirs) * config.validation_ratio)\n\n  ### validation\n  validation_dirs = all_video_dirs[:num_validation]\n  validation_imdb = dict()\n  validation_imdb[\'videos\'] = []\n  for i, data_iter in enumerate(dataset.dataset_iterator(validation_dirs)):\n    validation_imdb[\'videos\'] += data_iter.instance_videos\n  validation_imdb[\'n_videos\'] = len(validation_imdb[\'videos\'])\n  validation_imdb[\'image_shape\'] = (255, 255, 3)\n\n  ### train\n  train_dirs = all_video_dirs[num_validation:]\n  train_imdb = dict()\n  train_imdb[\'videos\'] = []\n  for i, data_iter in enumerate(dataset.dataset_iterator(train_dirs)):\n    train_imdb[\'videos\'] += data_iter.instance_videos\n  train_imdb[\'n_videos\'] = len(train_imdb[\'videos\'])\n  train_imdb[\'image_shape\'] = (255, 255, 3)\n\n  if not tf.gfile.IsDirectory(config.save_dir):\n    tf.logging.info(\'Creating training directory: %s\', config.save_dir)\n    tf.gfile.MakeDirs(config.save_dir)\n\n  with open(os.path.join(config.save_dir, \'validation_imdb.pickle\'), \'wb\') as f:\n    pickle.dump(validation_imdb, f)\n  with open(os.path.join(config.save_dir, \'train_imdb.pickle\'), \'wb\') as f:\n    pickle.dump(train_imdb, f)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
scripts/convert_pretrained_model.py,15,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Convert the matlab-pretrained model into TensorFlow format""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport os.path as osp\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nimport configuration\nimport siamese_model\nfrom utils.misc_utils import auto_select_gpu, save_cfgs\n\n# Set GPU\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = auto_select_gpu()\ntf.logging.set_verbosity(tf.logging.DEBUG)\n\nfrom sacred import Experiment\n\nex = Experiment(configuration.RUN_NAME)\n\n\n@ex.config\ndef configurations():\n  # Add configurations for current script, for more details please see the documentation of `sacred`.\n  model_config = configuration.MODEL_CONFIG\n  train_config = configuration.TRAIN_CONFIG\n  track_config = configuration.TRACK_CONFIG\n\n\n@ex.automain\ndef main(model_config, train_config, track_config):\n  # Create training directory\n  train_dir = train_config[\'train_dir\']\n  if not tf.gfile.IsDirectory(train_dir):\n    tf.logging.info(\'Creating training directory: %s\', train_dir)\n    tf.gfile.MakeDirs(train_dir)\n\n  # Build the Tensorflow graph\n  g = tf.Graph()\n  with g.as_default():\n    # Set fixed seed\n    np.random.seed(train_config[\'seed\'])\n    tf.set_random_seed(train_config[\'seed\'])\n\n    # Build the model\n    model = siamese_model.SiameseModel(model_config, train_config, mode=\'inference\')\n    model.build()\n\n    # Save configurations for future reference\n    save_cfgs(train_dir, model_config, train_config, track_config)\n\n    saver = tf.train.Saver(tf.global_variables(),\n                           max_to_keep=train_config[\'max_checkpoints_to_keep\'])\n\n    # Dynamically allocate GPU memory\n    gpu_options = tf.GPUOptions(allow_growth=True)\n    sess_config = tf.ConfigProto(gpu_options=gpu_options)\n\n    sess = tf.Session(config=sess_config)\n    model_path = tf.train.latest_checkpoint(train_config[\'train_dir\'])\n\n    if not model_path:\n      # Initialize all variables\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      start_step = 0\n\n      # Load pretrained embedding model if needed\n      if model_config[\'embed_config\'][\'embedding_checkpoint_file\']:\n        model.init_fn(sess)\n\n    else:\n      logging.info(\'Restore from last checkpoint: {}\'.format(model_path))\n      sess.run(tf.local_variables_initializer())\n      saver.restore(sess, model_path)\n      start_step = tf.train.global_step(sess, model.global_step.name) + 1\n\n    checkpoint_path = osp.join(train_config[\'train_dir\'], \'model.ckpt\')\n    saver.save(sess, checkpoint_path, global_step=start_step)\n'"
scripts/download_assets.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\nimport os.path as osp\nimport sys\nimport zipfile\n\nimport six.moves.urllib as urllib\n\nCURRENT_DIR = osp.dirname(__file__)\nROOT_DIR = osp.join(CURRENT_DIR, \'..\')\nsys.path.append(ROOT_DIR)\n\nfrom utils.misc_utils import mkdir_p\n\n\ndef download_or_skip(download_url, save_path):\n  if not osp.exists(save_path):\n    print(\'Downloading: {}\'.format(download_url))\n    opener = urllib.request.URLopener()\n    opener.retrieve(download_url, save_path)\n  else:\n    print(\'File {} exists, skip downloading.\'.format(save_path))\n\n\nif __name__ == \'__main__\':\n  assets_dir = osp.join(ROOT_DIR, \'assets\')\n\n  # Make assets directory\n  mkdir_p(assets_dir)\n\n  # Download the pretrained color model\n  download_base = \'https://www.robots.ox.ac.uk/~luca/stuff/siam-fc_nets/\'\n  model_name = \'2016-08-17.net.mat\'\n  download_or_skip(download_base + model_name, osp.join(assets_dir, model_name))\n\n  # Download the pretrained gray model\n  download_base = \'https://www.robots.ox.ac.uk/~luca/stuff/siam-fc_nets/\'\n  model_name = \'2016-08-17_gray025.net.mat\'\n  download_or_skip(download_base + model_name, osp.join(assets_dir, model_name))\n\n  # Download one test sequence\n  download_base = ""http://cvlab.hanyang.ac.kr/tracker_benchmark/seq_new/""\n  seq_name = \'KiteSurf.zip\'\n  download_or_skip(download_base + seq_name, osp.join(assets_dir, seq_name))\n\n  # Unzip the test sequence\n  with zipfile.ZipFile(osp.join(assets_dir, seq_name), \'r\') as zip_ref:\n    zip_ref.extractall(assets_dir)\n'"
scripts/preprocess_VID_data.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport sys\nimport xml.etree.ElementTree as ET\nfrom glob import glob\nfrom multiprocessing.pool import ThreadPool\n\nimport cv2\nfrom cv2 import imread, imwrite\n\nCURRENT_DIR = osp.dirname(__file__)\nROOT_DIR = osp.join(CURRENT_DIR, '..')\nsys.path.append(ROOT_DIR)\n\nfrom utils.infer_utils import get_crops, Rectangle, convert_bbox_format\nfrom utils.misc_utils import mkdir_p\n\n\ndef get_track_save_directory(save_dir, split, subdir, video):\n  subdir_map = {'ILSVRC2015_VID_train_0000': 'a',\n                'ILSVRC2015_VID_train_0001': 'b',\n                'ILSVRC2015_VID_train_0002': 'c',\n                'ILSVRC2015_VID_train_0003': 'd',\n                '': 'e'}\n  return osp.join(save_dir, 'Data', 'VID', split, subdir_map[subdir], video)\n\n\ndef process_split(root_dir, save_dir, split, subdir='', ):\n  data_dir = osp.join(root_dir, 'Data', 'VID', split)\n  anno_dir = osp.join(root_dir, 'Annotations', 'VID', split, subdir)\n  video_names = os.listdir(anno_dir)\n\n  for idx, video in enumerate(video_names):\n    print('{split}-{subdir} ({idx}/{total}): Processing {video}...'.format(split=split, subdir=subdir,\n                                                                           idx=idx, total=len(video_names),\n                                                                           video=video))\n    video_path = osp.join(anno_dir, video)\n    xml_files = glob(osp.join(video_path, '*.xml'))\n\n    for xml in xml_files:\n      tree = ET.parse(xml)\n      root = tree.getroot()\n\n      folder = root.find('folder').text\n      filename = root.find('filename').text\n\n      # Read image\n      img_file = osp.join(data_dir, folder, filename + '.JPEG')\n      img = None\n\n      # Get all object bounding boxes\n      bboxs = []\n      for object in root.iter('object'):\n        bbox = object.find('bndbox')\n        xmax = float(bbox.find('xmax').text)\n        xmin = float(bbox.find('xmin').text)\n        ymax = float(bbox.find('ymax').text)\n        ymin = float(bbox.find('ymin').text)\n        width = xmax - xmin + 1\n        height = ymax - ymin + 1\n        bboxs.append([xmin, ymin, width, height])\n\n      for idx, object in enumerate(root.iter('object')):\n        id = object.find('trackid').text\n        class_name = object.find('name').text\n\n        track_save_dir = get_track_save_directory(save_dir, 'train', subdir, video)\n        mkdir_p(track_save_dir)\n        savename = osp.join(track_save_dir, '{}.{:02d}.crop.x.jpg'.format(filename, int(id)))\n        if osp.isfile(savename): continue  # skip existing images\n\n        if img is None:\n          img = imread(img_file)\n\n        # Get crop\n        target_box = convert_bbox_format(Rectangle(*bboxs[idx]), 'center-based')\n        crop, _ = get_crops(img, target_box,\n                            size_z=127, size_x=255,\n                            context_amount=0.5, )\n\n        imwrite(savename, crop, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n\n\nif __name__ == '__main__':\n  vid_dir = osp.join(ROOT_DIR, 'data/ILSVRC2015')\n\n  # Or, you could save the actual curated data to a disk with sufficient space\n  # then create a soft link in `data/ILSVRC2015-VID-Curation`\n  save_dir = 'data/ILSVRC2015-VID-Curation'\n\n  pool = ThreadPool(processes=5)\n\n  one_work = lambda a, b: process_split(vid_dir, save_dir, a, b)\n\n  results = []\n  results.append(pool.apply_async(one_work, ['val', '']))\n  results.append(pool.apply_async(one_work, ['train', 'ILSVRC2015_VID_train_0000']))\n  results.append(pool.apply_async(one_work, ['train', 'ILSVRC2015_VID_train_0001']))\n  results.append(pool.apply_async(one_work, ['train', 'ILSVRC2015_VID_train_0002']))\n  results.append(pool.apply_async(one_work, ['train', 'ILSVRC2015_VID_train_0003']))\n  ans = [res.get() for res in results]\n"""
scripts/run_tracking.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\nr""""""Generate tracking results for videos using Siamese Model""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport os.path as osp\nimport sys\nfrom glob import glob\n\nimport tensorflow as tf\nfrom sacred import Experiment\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom inference import inference_wrapper\nfrom inference.tracker import Tracker\nfrom utils.infer_utils import Rectangle\nfrom utils.misc_utils import auto_select_gpu, mkdir_p, sort_nicely, load_cfgs\n\nex = Experiment()\n\n\n@ex.config\ndef configs():\n  checkpoint = \'Logs/SiamFC/track_model_checkpoints/SiamFC-3s-color-pretrained\'\n  input_files = \'assets/KiteSurf\'\n\n\n@ex.automain\ndef main(checkpoint, input_files):\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = auto_select_gpu()\n\n  model_config, _, track_config = load_cfgs(checkpoint)\n  track_config[\'log_level\'] = 1\n\n  g = tf.Graph()\n  with g.as_default():\n    model = inference_wrapper.InferenceWrapper()\n    restore_fn = model.build_graph_from_config(model_config, track_config, checkpoint)\n  g.finalize()\n\n  if not osp.isdir(track_config[\'log_dir\']):\n    logging.info(\'Creating inference directory: %s\', track_config[\'log_dir\'])\n    mkdir_p(track_config[\'log_dir\'])\n\n  video_dirs = []\n  for file_pattern in input_files.split("",""):\n    video_dirs.extend(glob(file_pattern))\n  logging.info(""Running tracking on %d videos matching %s"", len(video_dirs), input_files)\n\n  gpu_options = tf.GPUOptions(allow_growth=True)\n  sess_config = tf.ConfigProto(gpu_options=gpu_options)\n\n  with tf.Session(graph=g, config=sess_config) as sess:\n    restore_fn(sess)\n\n    tracker = Tracker(model, model_config=model_config, track_config=track_config)\n\n    for video_dir in video_dirs:\n      if not osp.isdir(video_dir):\n        logging.warning(\'{} is not a directory, skipping...\'.format(video_dir))\n        continue\n\n      video_name = osp.basename(video_dir)\n      video_log_dir = osp.join(track_config[\'log_dir\'], video_name)\n      mkdir_p(video_log_dir)\n\n      filenames = sort_nicely(glob(video_dir + \'/img/*.jpg\'))\n      first_line = open(video_dir + \'/groundtruth_rect.txt\').readline()\n      bb = [int(v) for v in first_line.strip().split(\',\')]\n      init_bb = Rectangle(bb[0] - 1, bb[1] - 1, bb[2], bb[3])  # 0-index in python\n\n      trajectory = tracker.track(sess, init_bb, filenames, video_log_dir)\n      with open(osp.join(video_log_dir, \'track_rect.txt\'), \'w\') as f:\n        for region in trajectory:\n          rect_str = \'{},{},{},{}\\n\'.format(region.x + 1, region.y + 1,\n                                            region.width, region.height)\n          f.write(rect_str)\n'"
scripts/show_tracking.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\nimport os.path as osp\nimport sys\n\nfrom sacred import Experiment\n\nex = Experiment()\n\nimport numpy as np\nfrom matplotlib.pyplot import imread, Rectangle\n\nCURRENT_DIR = osp.dirname(__file__)\nsys.path.append(osp.join(CURRENT_DIR, \'..\'))\n\nfrom utils.videofig import videofig\n\n\ndef readbbox(file):\n  with open(file, \'r\') as f:\n    lines = f.readlines()\n    bboxs = [[float(val) for val in line.strip().replace(\' \', \',\').replace(\'\\t\', \',\').split(\',\')] for line in lines]\n  return bboxs\n\n\ndef create_bbox(bbox, color):\n  return Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n                   fill=False,  # remove background\\n"",\n                   edgecolor=color)\n\n\ndef set_bbox(artist, bbox):\n  artist.set_xy((bbox[0], bbox[1]))\n  artist.set_width(bbox[2])\n  artist.set_height(bbox[3])\n\n\n@ex.config\ndef configs():\n  videoname = \'KiteSurf\'\n  runname = \'SiamFC-3s-color-pretrained\'\n  data_dir = \'assets/\'\n  track_log_dir = \'Logs/SiamFC/track_model_inference/{}/{}\'.format(runname, videoname)\n\n\n@ex.automain\ndef main(videoname, data_dir, track_log_dir):\n  track_log_dir = osp.join(track_log_dir)\n  video_data_dir = osp.join(data_dir, videoname)\n  te_bboxs = readbbox(osp.join(track_log_dir, \'track_rect.txt\'))\n  gt_bboxs = readbbox(osp.join(video_data_dir, \'groundtruth_rect.txt\'))\n  num_frames = len(gt_bboxs)\n\n  def redraw_fn(ind, axes):\n    ind += 1\n    input_ = imread(osp.join(track_log_dir, \'image_cropped{}.jpg\'.format(ind)))\n    response = np.load(osp.join(track_log_dir, \'response{}.npy\'.format(ind)))\n    org_img = imread(osp.join(data_dir, videoname, \'img\', \'{:04d}.jpg\'.format(ind + 1)))\n    gt_bbox = gt_bboxs[ind]\n    te_bbox = te_bboxs[ind]\n\n    bbox = np.load(osp.join(track_log_dir, \'bbox{}.npy\'.format(ind)))\n\n    if not redraw_fn.initialized:\n      ax1, ax2, ax3 = axes\n      redraw_fn.im1 = ax1.imshow(input_)\n      redraw_fn.im2 = ax2.imshow(response)\n      redraw_fn.im3 = ax3.imshow(org_img)\n\n      redraw_fn.bb1 = create_bbox(bbox, color=\'red\')\n      redraw_fn.bb2 = create_bbox(gt_bbox, color=\'green\')\n      redraw_fn.bb3 = create_bbox(te_bbox, color=\'red\')\n\n      ax1.add_patch(redraw_fn.bb1)\n      ax3.add_patch(redraw_fn.bb2)\n      ax3.add_patch(redraw_fn.bb3)\n\n      redraw_fn.text = ax3.text(0.03, 0.97, \'F:{}\'.format(ind), fontdict={\'size\': 10, },\n                                ha=\'left\', va=\'top\',\n                                bbox={\'facecolor\': \'red\', \'alpha\': 0.7},\n                                transform=ax3.transAxes)\n\n      redraw_fn.initialized = True\n    else:\n      redraw_fn.im1.set_array(input_)\n      redraw_fn.im2.set_array(response)\n      redraw_fn.im3.set_array(org_img)\n      set_bbox(redraw_fn.bb1, bbox)\n      set_bbox(redraw_fn.bb2, gt_bbox)\n      set_bbox(redraw_fn.bb3, te_bbox)\n      redraw_fn.text.set_text(\'F: {}\'.format(ind))\n\n  redraw_fn.initialized = False\n\n  videofig(int(num_frames) - 1, redraw_fn,\n           grid_specs={\'nrows\': 2, \'ncols\': 2, \'wspace\': 0, \'hspace\': 0},\n           layout_specs=[\'[0, 0]\', \'[0, 1]\', \'[1, :]\'])\n'"
tests/test_converted_model.py,7,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Tests for track model""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\nimport numpy as np\nimport scipy.io as sio\nimport tensorflow as tf\nfrom scipy.misc import imread  # Only pillow 2.x is compatible with matlab 2016R\n\nCURRENT_DIR = osp.dirname(__file__)\nPARENT_DIR = osp.join(CURRENT_DIR, \'..\')\nsys.path.append(PARENT_DIR)\n\nimport siamese_model\nimport configuration\nfrom utils.misc_utils import load_cfgs\n\n\ndef test_load_embedding_from_mat():\n  """"""Test if the embedding model loaded from .mat\n     produces the same features as the original MATLAB implementation""""""\n  matpath = osp.join(PARENT_DIR, \'assets/2016-08-17.net.mat\')\n  test_im = osp.join(CURRENT_DIR, \'01.jpg\')\n  gt_feat = osp.join(CURRENT_DIR, \'result.mat\')\n\n  model_config = configuration.MODEL_CONFIG\n  model_config[\'embed_config\'][\'embedding_name\'] = \'convolutional_alexnet\'\n  model_config[\'embed_config\'][\'embedding_checkpoint_file\'] = matpath  # For SiameseFC\n  model_config[\'embed_config\'][\'train_embedding\'] = False\n\n  g = tf.Graph()\n  with g.as_default():\n    model = siamese_model.SiameseModel(model_config, configuration.TRAIN_CONFIG, mode=\'inference\')\n    model.build()\n\n    with tf.Session() as sess:\n      # Initialize models\n      init = tf.global_variables_initializer()\n      sess.run(init)\n\n      # Load model here\n      model.init_fn(sess)\n\n      # Load image\n      im = imread(test_im)\n      im_batch = np.expand_dims(im, 0)\n\n      # Feed image\n      feature = sess.run([model.exemplar_embeds], feed_dict={model.examplar_feed: im_batch})\n\n      # Compare with features computed from original source code\n      ideal_feature = sio.loadmat(gt_feat)[\'r\'][\'z_features\'][0][0]\n      diff = feature - ideal_feature\n      diff = np.sqrt(np.mean(np.square(diff)))\n      print(\'Feature computation difference: {}\'.format(diff))\n      print(\'You should get something like: 0.00892720464617\')\n\n\ndef test_load_embedding_from_converted_TF_model():\n  """"""Test if the embedding model loaded from converted TensorFlow checkpoint\n     produces the same features as the original implementation""""""\n  checkpoint = osp.join(PARENT_DIR, \'Logs/SiamFC/track_model_checkpoints/SiamFC-3s-color-pretrained\')\n  test_im = osp.join(CURRENT_DIR, \'01.jpg\')\n  gt_feat = osp.join(CURRENT_DIR, \'result.mat\')\n\n  if not osp.exists(checkpoint):\n    raise Exception(\'SiamFC-3s-color-pretrained is not generated yet.\')\n  model_config, train_config, track_config = load_cfgs(checkpoint)\n\n  # Build the model\n  g = tf.Graph()\n  with g.as_default():\n    model = siamese_model.SiameseModel(model_config, train_config, mode=\'inference\')\n    model.build()\n\n    with tf.Session() as sess:\n      # Load model here\n      saver = tf.train.Saver(tf.global_variables())\n      if osp.isdir(checkpoint):\n        model_path = tf.train.latest_checkpoint(checkpoint)\n      else:\n        model_path = checkpoint\n\n      saver.restore(sess, model_path)\n\n      # Load image\n      im = imread(test_im)\n      im_batch = np.expand_dims(im, 0)\n\n      # Feed image\n      feature = sess.run([model.exemplar_embeds], feed_dict={model.examplar_feed: im_batch})\n\n      # Compare with features computed from original source code\n      ideal_feature = sio.loadmat(gt_feat)[\'r\'][\'z_features\'][0][0]\n      diff = feature - ideal_feature\n      diff = np.sqrt(np.mean(np.square(diff)))\n      print(\'Feature computation difference: {}\'.format(diff))\n      print(\'You should get something like: 0.00892720464617\')\n\n\ndef test():\n  test_load_embedding_from_mat()\n  test_load_embedding_from_converted_TF_model()\n\n\nif __name__ == \'__main__\':\n  test()\n'"
utils/__init__.py,0,b''
utils/infer_utils.py,7,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""\nInference Utilities\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport numpy as np\nimport tensorflow as tf\nfrom cv2 import resize\n\nfrom utils.misc_utils import get_center\n\nRectangle = collections.namedtuple(\'Rectangle\', [\'x\', \'y\', \'width\', \'height\'])\n\n\ndef im2rgb(im):\n  if len(im.shape) != 3:\n    im = np.stack([im, im, im], -1)\n  return im\n\n\ndef convert_bbox_format(bbox, to):\n  x, y, target_width, target_height = bbox.x, bbox.y, bbox.width, bbox.height\n  if to == \'top-left-based\':\n    x -= get_center(target_width)\n    y -= get_center(target_height)\n  elif to == \'center-based\':\n    y += get_center(target_height)\n    x += get_center(target_width)\n  else:\n    raise ValueError(""Bbox format: {} was not recognized"".format(to))\n  return Rectangle(x, y, target_width, target_height)\n\n\ndef get_exemplar_images(images, exemplar_size, targets_pos=None):\n  """"""Crop exemplar image from input images""""""\n  with tf.name_scope(\'get_exemplar_image\'):\n    batch_size, x_height, x_width = images.get_shape().as_list()[:3]\n    z_height, z_width = exemplar_size\n\n    if targets_pos is None:\n      target_pos_single = [[get_center(x_height), get_center(x_width)]]\n      targets_pos_ = tf.tile(target_pos_single, [batch_size, 1])\n    else:\n      targets_pos_ = targets_pos\n\n    # convert to top-left corner based coordinates\n    top = tf.to_int32(tf.round(targets_pos_[:, 0] - get_center(z_height)))\n    bottom = tf.to_int32(top + z_height)\n    left = tf.to_int32(tf.round(targets_pos_[:, 1] - get_center(z_width)))\n    right = tf.to_int32(left + z_width)\n\n    def _slice(x):\n      f, t, l, b, r = x\n      c = f[t:b, l:r]\n      return c\n\n    exemplar_img = tf.map_fn(_slice, (images, top, left, bottom, right), dtype=images.dtype)\n    exemplar_img.set_shape([batch_size, z_height, z_width, 3])\n    return exemplar_img\n\n\ndef get_crops(im, bbox, size_z, size_x, context_amount):\n  """"""Obtain image sub-window, padding with avg channel if area goes outside of border\n\n  Adapted from https://github.com/bertinetto/siamese-fc/blob/master/ILSVRC15-curation/save_crops.m#L46\n\n  Args:\n    im: Image ndarray\n    bbox: Named tuple (x, y, width, height) x, y corresponds to the crops center\n    size_z: Target + context size\n    size_x: The resultant crop size\n    context_amount: The amount of context\n\n  Returns:\n    image crop: Image ndarray\n  """"""\n  cy, cx, h, w = bbox.y, bbox.x, bbox.height, bbox.width\n  wc_z = w + context_amount * (w + h)\n  hc_z = h + context_amount * (w + h)\n  s_z = np.sqrt(wc_z * hc_z)\n  scale_z = size_z / s_z\n\n  d_search = (size_x - size_z) / 2\n  pad = d_search / scale_z\n  s_x = s_z + 2 * pad\n  scale_x = size_x / s_x\n\n  image_crop_x, _, _, _, _ = get_subwindow_avg(im, [cy, cx],\n                                               [size_x, size_x],\n                                               [np.round(s_x), np.round(s_x)])\n\n  return image_crop_x, scale_x\n\n\ndef get_subwindow_avg(im, pos, model_sz, original_sz):\n  # avg_chans = np.mean(im, axis=(0, 1)) # This version is 3x slower\n  avg_chans = [np.mean(im[:, :, 0]), np.mean(im[:, :, 1]), np.mean(im[:, :, 2])]\n  if not original_sz:\n    original_sz = model_sz\n  sz = original_sz\n  im_sz = im.shape\n  # make sure the size is not too small\n  assert im_sz[0] > 2 and im_sz[1] > 2\n  c = [get_center(s) for s in sz]\n\n  # check out-of-bounds coordinates, and set them to avg_chans\n  context_xmin = np.int(np.round(pos[1] - c[1]))\n  context_xmax = np.int(context_xmin + sz[1] - 1)\n  context_ymin = np.int(np.round(pos[0] - c[0]))\n  context_ymax = np.int(context_ymin + sz[0] - 1)\n  left_pad = np.int(np.maximum(0, -context_xmin))\n  top_pad = np.int(np.maximum(0, -context_ymin))\n  right_pad = np.int(np.maximum(0, context_xmax - im_sz[1] + 1))\n  bottom_pad = np.int(np.maximum(0, context_ymax - im_sz[0] + 1))\n\n  context_xmin = context_xmin + left_pad\n  context_xmax = context_xmax + left_pad\n  context_ymin = context_ymin + top_pad\n  context_ymax = context_ymax + top_pad\n  if top_pad > 0 or bottom_pad > 0 or left_pad > 0 or right_pad > 0:\n    R = np.pad(im[:, :, 0], ((top_pad, bottom_pad), (left_pad, right_pad)),\n               \'constant\', constant_values=(avg_chans[0]))\n    G = np.pad(im[:, :, 1], ((top_pad, bottom_pad), (left_pad, right_pad)),\n               \'constant\', constant_values=(avg_chans[1]))\n    B = np.pad(im[:, :, 2], ((top_pad, bottom_pad), (left_pad, right_pad)),\n               \'constant\', constant_values=(avg_chans[2]))\n\n    im = np.stack((R, G, B), axis=2)\n\n  im_patch_original = im[context_ymin:context_ymax + 1,\n                      context_xmin:context_xmax + 1, :]\n  if not (model_sz[0] == original_sz[0] and model_sz[1] == original_sz[1]):\n    im_patch = resize(im_patch_original, tuple(model_sz))\n  else:\n    im_patch = im_patch_original\n  return im_patch, left_pad, top_pad, right_pad, bottom_pad\n'"
utils/misc_utils.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Miscellaneous Utilities.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport errno\nimport json\nimport logging\nimport os\nimport re\nimport sys\nfrom os import path as osp\n\ntry:\n  import pynvml  # nvidia-ml provides utility for NVIDIA management\n\n  HAS_NVML = True\nexcept:\n  HAS_NVML = False\n\n\ndef auto_select_gpu():\n  """"""Select gpu which has largest free memory""""""\n  if HAS_NVML:\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    largest_free_mem = 0\n    largest_free_idx = 0\n    for i in range(deviceCount):\n      handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n      info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n      if info.free > largest_free_mem:\n        largest_free_mem = info.free\n        largest_free_idx = i\n    pynvml.nvmlShutdown()\n    largest_free_mem = largest_free_mem / 1024. / 1024.  # Convert to MB\n\n    idx_to_gpu_id = {}\n    for i in range(deviceCount):\n      idx_to_gpu_id[i] = \'{}\'.format(i)\n\n    gpu_id = idx_to_gpu_id[largest_free_idx]\n    logging.info(\'Using largest free memory GPU {} with free memory {}MB\'.format(gpu_id, largest_free_mem))\n    return gpu_id\n  else:\n    logging.info(\'nvidia-ml-py is not installed, automatically select gpu is disabled!\')\n    return \'0\'\n\n\ndef get_center(x):\n  return (x - 1.) / 2.\n\n\ndef get(config, key, default):\n  """"""Get value in config by key, use default if key is not set\n\n  This little function is useful for dynamical experimental settings.\n  For example, we can add a new configuration without worrying compatibility with older versions.\n  You can also achieve this by just calling config.get(key, default), but add a warning is even better : )\n  """"""\n  val = config.get(key)\n  if val is None:\n    logging.warning(\'{} is not explicitly specified, using default value: {}\'.format(key, default))\n    val = default\n  return val\n\n\ndef mkdir_p(path):\n  """"""mimic the behavior of mkdir -p in bash""""""\n  try:\n    os.makedirs(path)\n  except OSError as exc:  # Python >2.5\n    if exc.errno == errno.EEXIST and os.path.isdir(path):\n      pass\n    else:\n      raise\n\n\ndef tryfloat(s):\n  try:\n    return float(s)\n  except:\n    return s\n\n\ndef alphanum_key(s):\n  """""" Turn a string into a list of string and number chunks.\n      ""z23a"" -> [""z"", 23, ""a""]\n  """"""\n  return [tryfloat(c) for c in re.split(\'([0-9.]+)\', s)]\n\n\ndef sort_nicely(l):\n  """"""Sort the given list in the way that humans expect.""""""\n  return sorted(l, key=alphanum_key)\n\n\nclass Tee(object):\n  """"""Mimic the behavior of tee in bash\n\n  From: http://web.archive.org/web/20141016185743/https://mail.python.org/pipermail/python-list/2007-May/460639.html\n  Usage:\n    tee=Tee(\'logfile\', \'w\')\n    print \'abcdefg\'\n    print \'another line\'\n    tee.close()\n    print \'screen only\'\n    del tee # should do nothing\n  """"""\n\n  def __init__(self, name, mode):\n    self.file = open(name, mode)\n    self.stdout = sys.stdout\n    sys.stdout = self\n\n  def close(self):\n    if self.stdout is not None:\n      sys.stdout = self.stdout\n      self.stdout = None\n    if self.file is not None:\n      self.file.close()\n      self.file = None\n\n  def write(self, data):\n    self.file.write(data)\n    self.stdout.write(data)\n\n  def flush(self):\n    self.file.flush()\n    self.stdout.flush()\n\n  def __del__(self):\n    self.close()\n\n\ndef save_cfgs(train_dir, model_config, train_config, track_config):\n  """"""Save all configurations in JSON format for future reference""""""\n  with open(osp.join(train_dir, \'model_config.json\'), \'w\') as f:\n    json.dump(model_config, f, indent=2)\n  with open(osp.join(train_dir, \'train_config.json\'), \'w\') as f:\n    json.dump(train_config, f, indent=2)\n  with open(osp.join(train_dir, \'track_config.json\'), \'w\') as f:\n    json.dump(track_config, f, indent=2)\n\n\ndef load_cfgs(checkpoint):\n  if osp.isdir(checkpoint):\n    train_dir = checkpoint\n  else:\n    train_dir = osp.dirname(checkpoint)\n\n  with open(osp.join(train_dir, \'model_config.json\'), \'r\') as f:\n    model_config = json.load(f)\n  with open(osp.join(train_dir, \'train_config.json\'), \'r\') as f:\n    train_config = json.load(f)\n  with open(osp.join(train_dir, \'track_config.json\'), \'r\') as f:\n    track_config = json.load(f)\n  return model_config, train_config, track_config\n'"
utils/train_utils.py,16,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Utilities for model construction""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import io as sio\n\nfrom utils.misc_utils import get_center\n\n\ndef construct_gt_score_maps(response_size, batch_size, stride, gt_config=None):\n  """"""Construct a batch of groundtruth score maps\n\n  Args:\n    response_size: A list or tuple with two elements [ho, wo]\n    batch_size: An integer e.g., 16\n    stride: Embedding stride e.g., 8\n    gt_config: Configurations for groundtruth generation\n\n  Return:\n    A float tensor of shape [batch_size] + response_size\n  """"""\n  with tf.name_scope(\'construct_gt\'):\n    ho = response_size[0]\n    wo = response_size[1]\n    y = tf.cast(tf.range(0, ho), dtype=tf.float32) - get_center(ho)\n    x = tf.cast(tf.range(0, wo), dtype=tf.float32) - get_center(wo)\n    [Y, X] = tf.meshgrid(y, x)\n\n    def _logistic_label(X, Y, rPos, rNeg):\n      # dist_to_center = tf.sqrt(tf.square(X) + tf.square(Y))  # L2 metric\n      dist_to_center = tf.abs(X) + tf.abs(Y)  # Block metric\n      Z = tf.where(dist_to_center <= rPos,\n                   tf.ones_like(X),\n                   tf.where(dist_to_center < rNeg,\n                            0.5 * tf.ones_like(X),\n                            tf.zeros_like(X)))\n      return Z\n\n    rPos = gt_config[\'rPos\'] / stride\n    rNeg = gt_config[\'rNeg\'] / stride\n    gt = _logistic_label(X, Y, rPos, rNeg)\n\n    # Duplicate a batch of maps\n    gt_expand = tf.reshape(gt, [1] + response_size)\n    gt = tf.tile(gt_expand, [batch_size, 1, 1])\n    return gt\n\n\ndef get_params_from_mat(matpath):\n  """"""Get parameter from .mat file into parms(dict)""""""\n\n  def squeeze(vars_):\n    # Matlab save some params with shape (*, 1)\n    # However, we don\'t need the trailing dimension in TensorFlow.\n    if isinstance(vars_, (list, tuple)):\n      return [np.squeeze(v, 1) for v in vars_]\n    else:\n      return np.squeeze(vars_, 1)\n\n  netparams = sio.loadmat(matpath)[""net""][""params""][0][0]\n  params = dict()\n\n  for i in range(netparams.size):\n    param = netparams[0][i]\n    name = param[""name""][0]\n    value = param[""value""]\n    value_size = param[""value""].shape[0]\n\n    match = re.match(r""([a-z]+)([0-9]+)([a-z]+)"", name, re.I)\n    if match:\n      items = match.groups()\n    elif name == \'adjust_f\':\n      params[\'detection/weights\'] = squeeze(value)\n      continue\n    elif name == \'adjust_b\':\n      params[\'detection/biases\'] = squeeze(value)\n      continue\n    else:\n      raise Exception(\'unrecognized layer params\')\n\n    op, layer, types = items\n    layer = int(layer)\n    if layer in [1, 3]:\n      if op == \'conv\':  # convolution\n        if types == \'f\':\n          params[\'conv%d/weights\' % layer] = value\n        elif types == \'b\':\n          value = squeeze(value)\n          params[\'conv%d/biases\' % layer] = value\n      elif op == \'bn\':  # batch normalization\n        if types == \'x\':\n          m, v = squeeze(np.split(value, 2, 1))\n          params[\'conv%d/BatchNorm/moving_mean\' % layer] = m\n          params[\'conv%d/BatchNorm/moving_variance\' % layer] = np.square(v)\n        elif types == \'m\':\n          value = squeeze(value)\n          params[\'conv%d/BatchNorm/gamma\' % layer] = value\n        elif types == \'b\':\n          value = squeeze(value)\n          params[\'conv%d/BatchNorm/beta\' % layer] = value\n      else:\n        raise Exception\n    elif layer in [2, 4]:\n      if op == \'conv\' and types == \'f\':\n        b1, b2 = np.split(value, 2, 3)\n      else:\n        b1, b2 = np.split(value, 2, 0)\n      if op == \'conv\':\n        if types == \'f\':\n          params[\'conv%d/b1/weights\' % layer] = b1\n          params[\'conv%d/b2/weights\' % layer] = b2\n        elif types == \'b\':\n          b1, b2 = squeeze(np.split(value, 2, 0))\n          params[\'conv%d/b1/biases\' % layer] = b1\n          params[\'conv%d/b2/biases\' % layer] = b2\n      elif op == \'bn\':\n        if types == \'x\':\n          m1, v1 = squeeze(np.split(b1, 2, 1))\n          m2, v2 = squeeze(np.split(b2, 2, 1))\n          params[\'conv%d/b1/BatchNorm/moving_mean\' % layer] = m1\n          params[\'conv%d/b2/BatchNorm/moving_mean\' % layer] = m2\n          params[\'conv%d/b1/BatchNorm/moving_variance\' % layer] = np.square(v1)\n          params[\'conv%d/b2/BatchNorm/moving_variance\' % layer] = np.square(v2)\n        elif types == \'m\':\n          params[\'conv%d/b1/BatchNorm/gamma\' % layer] = squeeze(b1)\n          params[\'conv%d/b2/BatchNorm/gamma\' % layer] = squeeze(b2)\n        elif types == \'b\':\n          params[\'conv%d/b1/BatchNorm/beta\' % layer] = squeeze(b1)\n          params[\'conv%d/b2/BatchNorm/beta\' % layer] = squeeze(b2)\n      else:\n        raise Exception\n\n    elif layer in [5]:\n      if op == \'conv\' and types == \'f\':\n        b1, b2 = np.split(value, 2, 3)\n      else:\n        b1, b2 = squeeze(np.split(value, 2, 0))\n      assert op == \'conv\', \'layer5 contains only convolution\'\n      if types == \'f\':\n        params[\'conv%d/b1/weights\' % layer] = b1\n        params[\'conv%d/b2/weights\' % layer] = b2\n      elif types == \'b\':\n        params[\'conv%d/b1/biases\' % layer] = b1\n        params[\'conv%d/b2/biases\' % layer] = b2\n\n  return params\n\n\ndef load_mat_model(matpath, embed_scope, detection_scope=None):\n  """"""Restore SiameseFC models from .mat model files""""""\n  params = get_params_from_mat(matpath)\n\n  assign_ops = []\n\n  def _assign(ref_name, params, scope=embed_scope):\n    var_in_model = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                     scope + ref_name)[0]\n    var_in_mat = params[ref_name]\n    op = tf.assign(var_in_model, var_in_mat)\n    assign_ops.append(op)\n\n  for l in range(1, 6):\n    if l in [1, 3]:\n      _assign(\'conv%d/weights\' % l, params)\n      # _assign(\'conv%d/biases\' % l, params)\n      _assign(\'conv%d/BatchNorm/beta\' % l, params)\n      _assign(\'conv%d/BatchNorm/gamma\' % l, params)\n      _assign(\'conv%d/BatchNorm/moving_mean\' % l, params)\n      _assign(\'conv%d/BatchNorm/moving_variance\' % l, params)\n    elif l in [2, 4]:\n      # Branch 1\n      _assign(\'conv%d/b1/weights\' % l, params)\n      # _assign(\'conv%d/b1/biases\' % l, params)\n      _assign(\'conv%d/b1/BatchNorm/beta\' % l, params)\n      _assign(\'conv%d/b1/BatchNorm/gamma\' % l, params)\n      _assign(\'conv%d/b1/BatchNorm/moving_mean\' % l, params)\n      _assign(\'conv%d/b1/BatchNorm/moving_variance\' % l, params)\n      # Branch 2\n      _assign(\'conv%d/b2/weights\' % l, params)\n      # _assign(\'conv%d/b2/biases\' % l, params)\n      _assign(\'conv%d/b2/BatchNorm/beta\' % l, params)\n      _assign(\'conv%d/b2/BatchNorm/gamma\' % l, params)\n      _assign(\'conv%d/b2/BatchNorm/moving_mean\' % l, params)\n      _assign(\'conv%d/b2/BatchNorm/moving_variance\' % l, params)\n    elif l in [5]:\n      # Branch 1\n      _assign(\'conv%d/b1/weights\' % l, params)\n      _assign(\'conv%d/b1/biases\' % l, params)\n      # Branch 2\n      _assign(\'conv%d/b2/weights\' % l, params)\n      _assign(\'conv%d/b2/biases\' % l, params)\n    else:\n      raise Exception(\'layer number must below 5\')\n\n  if detection_scope:\n    _assign(detection_scope + \'biases\', params, scope=\'\')\n\n  initialize = tf.group(*assign_ops)\n  return initialize\n'"
utils/videofig.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright \xc2\xa9 2017 bily     Huazhong University of Science and Technology\n#\n# Distributed under terms of the MIT license.\n\n""""""Figure with horizontal scrollbar and play capabilities\n\nFor latest version, go to https://github.com/bilylee/videofig\n\nBasic usage\n-----------\nCreates a figure with a horizontal scrollbar and shortcuts to scroll automatically.\nThe scroll range is 0 to NUM_FRAMES - 1. The function REDRAW_FUN(F, AXES) is called to\nredraw at scroll position F (for example, REDRAW_FUNC can show the frame F of a video)\nusing AXES for drawing. F is an integer, AXES is a instance of [Axes class](https://matplotlib.org/api/axes_api.html)\n\nThis can be used not only to play and analyze standard videos, but it also lets you place\nany custom Matplotlib plots and graphics on top.\n\nThe keyboard shortcuts are:\n  Enter(Return) -- play/pause video (25 frames-per-second default).\n  Backspace -- play/pause video 5 times slower.\n  Right/left arrow keys -- advance/go back one frame.\n  Page down/page up -- advance/go back 30 frames.\n  Home/end -- go to first/last frame of video.\n\nAdvanced usage\n--------------\nvideofig(NUM_FRAMES, REDRAW_FUNC, FPS, BIG_SCROLL)\nAlso specifies the speed of the play function (frames-per-second) and\nthe frame step of page up/page down (or empty for defaults).\n\nvideofig(NUM_FRAMES, REDRAW_FUNC, FPS, BIG_SCROLL, KEY_FUNC)\nAlso calls KEY_FUNC(KEY) with any keys that weren\'t processed, so you\ncan add more shortcut keys (or empty for none).\n\nExample 1: Plot a dynamic sine wave\n---------\n  import numpy as np\n\n  def redraw_fn(f, axes):\n    amp = float(f) / 3000\n    f0 = 3\n    t = np.arange(0.0, 1.0, 0.001)\n    s = amp * np.sin(2 * np.pi * f0 * t)\n    if not redraw_fn.initialized:\n      redraw_fn.l, = axes.plot(t, s, lw=2, color=\'red\')\n      redraw_fn.initialized = True\n    else:\n      redraw_fn.l.set_ydata(s)\n\n  redraw_fn.initialized = False\n\n  videofig(100, redraw_fn)\n\nExample 2: Show images in a custom directory\n---------\n  import os\n  import glob\n  from scipy.misc import imread\n\n  img_dir = \'YOUR-IMAGE-DIRECTORY\'\n  img_files = glob.glob(os.path.join(video_dir, \'*.jpg\'))\n\n  def redraw_fn(f, axes):\n    img_file = img_files[f]\n    img = imread(img_file)\n    if not redraw_fn.initialized:\n      redraw_fn.im = axes.imshow(img, animated=True)\n      redraw_fn.initialized = True\n    else:\n      redraw_fn.im.set_array(img)\n  redraw_fn.initialized = False\n\n  videofig(len(img_files), redraw_fn, play_fps=30)\n\nExample 3: Show images together with object bounding boxes\n----------\n  import os\n  import glob\n  from scipy.misc import imread\n  from matplotlib.pyplot import Rectangle\n\n  video_dir = \'YOUR-VIDEO-DIRECTORY\'\n\n  img_files = glob.glob(os.path.join(video_dir, \'*.jpg\'))\n  box_files = glob.glob(os.path.join(video_dir, \'*.txt\'))\n\n  def redraw_fn(f, axes):\n    img = imread(img_files[f])\n    box = bbread(box_files[f])  # Define your own bounding box reading utility\n    x, y, w, h = box\n    if not redraw_fn.initialized:\n      im = axes.imshow(img, animated=True)\n      bb = Rectangle((x, y), w, h,\n                     fill=False,  # remove background\n                     edgecolor=""red"")\n      axes.add_patch(bb)\n      redraw_fn.im = im\n      redraw_fn.bb = bb\n      redraw_fn.initialized = True\n    else:\n      redraw_fn.im.set_array(img)\n      redraw_fn.bb.set_xy((x, y))\n      redraw_fn.bb.set_width(w)\n      redraw_fn.bb.set_height(h)\n  redraw_fn.initialized = False\n\n  videofig(len(img_files), redraw_fn, play_fps=30)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib import pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.widgets import Slider\n\n\ndef videofig(num_frames, redraw_func, play_fps=25, big_scroll=30, key_func=None,\n             grid_specs=None, layout_specs=None, figname=None, *args):\n  """"""Figure with horizontal scrollbar and play capabilities\n\n  This script is mainly inspired by the elegant work of Jo\xc3\xa3o Filipe Henriques\n    https://www.mathworks.com/matlabcentral/fileexchange/29544-figure-to-play-and-analyze-videos-with-custom-plots-on-top?focused=5172704&tab=function\n\n  :param num_frames: an integer, number of frames in a sequence\n  :param redraw_func: callable with signature redraw_func(f, axes)\n                      used to draw a new frame at position f using axes, which is a instance of Axes class in matplotlib \n  :param play_fps: an integer, number of frames per second, used to control the play speed\n  :param big_scroll: an integer, big scroll number used when pressed page down or page up keys. \n  :param key_func: optional callable which signature key_func(key), used to provide custom key shortcuts.\n  :param grid_specs: optional dictionary, used to specify the gridspec of the main drawing pane. \n                     For example, grid_specs = {\'nrows\': 2, \'ncols\': 2} will create a gridspec with 2 rows and 2 cols.\n  :param layout_specs: optional list, used to specify the layout of the gridspec of the main drawing pane.\n                     For example, layout_specs = [\'[:, 0]\', \'[:, 1]\'] means:\n                        gs = ... Some code to create the main drawing pane gridspec ...\n                        ax1 = plt.subplot(gs[:, 0])\n                        ax2 = plt.subplot(gs[:, 1])\n  :param args: other optional arguments\n  :return: None\n  """"""\n  # Check arguments\n  check_int_scalar(num_frames, \'num_frames\')\n  check_callback(redraw_func, \'redraw_func\')\n  check_int_scalar(play_fps, \'play_fps\')\n  check_int_scalar(big_scroll, \'big_scroll\')\n  if key_func:\n    check_callback(key_func, \'key_func\')\n\n  # Initialize figure\n  if not figname:\n    figname = \'VideoPlayer\'\n  fig_handle = plt.figure(num=figname)\n\n  # We use GridSpec to support embedding multiple plots in the main drawing pane.\n  # A nested grid demo can be found at https://matplotlib.org/users/plotting/examples/demo_gridspec06.py\n  # Construct outer grid, which contains the main drawing pane and slider ball\n  outer_grid = gridspec.GridSpec(2, 1,\n                                 left=0, bottom=0, right=1, top=1,\n                                 height_ratios=[97, 3], wspace=0.0, hspace=0.0)\n\n  # Construct inner grid in main drawing pane\n  if grid_specs is None:\n    grid_specs = {\'nrows\': 1, \'ncols\': 1}  # We will have only one axes by default\n\n  inner_grid = gridspec.GridSpecFromSubplotSpec(subplot_spec=outer_grid[0], **grid_specs)\n  if layout_specs:\n    # eval() can\'t work properly in list comprehension which is inside a function.\n    # Refer to: http://bugs.python.org/issue5242\n    # Maybe I should find another way to implement layout_specs without using eval()...\n    axes_handle = []\n    for spec in layout_specs:\n      axes_handle.append(plt.Subplot(fig_handle, eval(\'inner_grid\' + spec)))\n  else:\n    num_inner_plots = grid_specs[\'nrows\'] * grid_specs[\'ncols\']\n    axes_handle = [plt.Subplot(fig_handle, inner_grid[i]) for i in range(num_inner_plots)]\n\n  for ax in axes_handle: fig_handle.add_subplot(ax)\n\n  if len(axes_handle) == 1:\n    axes_handle = axes_handle[0]\n    axes_handle.set_axis_off()\n\n  # Build scrollbar\n  scroll_axes_handle = plt.Subplot(fig_handle, outer_grid[1])\n  scroll_axes_handle.set_facecolor(\'lightgoldenrodyellow\')\n  fig_handle.add_subplot(scroll_axes_handle)\n  scroll_handle = Slider(scroll_axes_handle, \'\', 0.0, num_frames - 1, valinit=0.0)\n\n  def draw_new(_):\n    redraw_func(int(scroll_handle.val), axes_handle)\n    fig_handle.canvas.draw_idle()\n\n  def scroll(new_f):\n    new_f = min(max(new_f, 0), num_frames - 1)  # clip in the range of [0, num_frames - 1]\n    cur_f = scroll_handle.val\n\n    # Stop player at the end of the sequence\n    if new_f == (num_frames - 1):\n      play.running = False\n\n    if cur_f != new_f:\n      # move scroll bar to new position\n      scroll_handle.set_val(new_f)\n\n    return axes_handle\n\n  def play(period):\n    play.running ^= True  # Toggle state\n    if play.running:\n      frame_idxs = range(int(scroll_handle.val), num_frames)\n      play.anim = FuncAnimation(fig_handle, scroll, frame_idxs,\n                                interval=1000 * period, repeat=False)\n      plt.draw()\n    else:\n      play.anim.event_source.stop()\n\n  # Set initial player state\n  play.running = False\n\n  def key_press(event):\n    key = event.key\n    f = scroll_handle.val\n    if key == \'left\':\n      scroll(f - 1)\n    elif key == \'right\':\n      scroll(f + 1)\n    elif key == \'pageup\':\n      scroll(f - big_scroll)\n    elif key == \'pagedown\':\n      scroll(f + big_scroll)\n    elif key == \'home\':\n      scroll(0)\n    elif key == \'end\':\n      scroll(num_frames - 1)\n    elif key == \'enter\':\n      play(1 / play_fps)\n    elif key == \'backspace\':\n      play(5 / play_fps)\n    else:\n      if key_func:\n        key_func(key)\n\n  # Register events\n  scroll_handle.on_changed(draw_new)\n  fig_handle.canvas.mpl_connect(\'key_press_event\', key_press)\n\n  # Draw initial frame\n  redraw_func(0, axes_handle)\n\n  # Start playing\n  play(1 / play_fps)\n\n  # plt.show() has to be put in the end of the function,\n  # otherwise, the program simply won\'t work, weird...\n  plt.show()\n\n\ndef check_int_scalar(a, name):\n  assert isinstance(a, int), \'{} must be a int scalar, instead of {}\'.format(name, type(name))\n\n\ndef check_callback(a, name):\n  # Check http://stackoverflow.com/questions/624926/how-to-detect-whether-a-python-variable-is-a-function\n  # for more details about python function type detection.\n  assert callable(a), \'{} must be callable, instead of {}\'.format(name, type(name))\n\n\nif __name__ == \'__main__\':\n  import argparse\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""-i"", ""--imgdir"", type=str, default=None, help=""directory containing JPEG images"")\n  args = parser.parse_args()\n\n  if args.imgdir:\n    import os\n    import glob\n    from scipy.misc import imread\n\n    img_files = sorted(glob.glob(os.path.join(args.imgdir, \'*.jpg\')))\n\n\n    def redraw_fn(f, axes):\n      img_file = img_files[f]\n      img = imread(img_file)\n      if not redraw_fn.initialized:\n        redraw_fn.im = axes.imshow(img, animated=True)\n        redraw_fn.initialized = True\n      else:\n        redraw_fn.im.set_array(img)\n\n\n    redraw_fn.initialized = False\n    videofig(len(img_files), redraw_fn, play_fps=30)\n\n  else:\n    import numpy as np\n\n\n    def redraw_fn(f, axes):\n      amp = float(f) / 3000\n      f0 = 3\n      t = np.arange(0.0, 1.0, 0.001)\n      s = amp * np.sin(2 * np.pi * f0 * t)\n      if not redraw_fn.initialized:\n        redraw_fn.l, = axes.plot(t, s, lw=2, color=\'red\')\n        redraw_fn.initialized = True\n      else:\n        redraw_fn.l.set_ydata(s)\n\n\n    redraw_fn.initialized = False\n    videofig(100, redraw_fn)\n'"
