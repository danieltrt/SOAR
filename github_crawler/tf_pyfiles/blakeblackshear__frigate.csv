file_path,api_count,code
benchmark.py,0,"b'import os\nfrom statistics import mean\nimport multiprocessing as mp\nimport numpy as np\nimport datetime\nfrom frigate.edgetpu import ObjectDetector, EdgeTPUProcess, RemoteObjectDetector, load_labels\n\nmy_frame = np.expand_dims(np.full((300,300,3), 1, np.uint8), axis=0)\nlabels = load_labels(\'/labelmap.txt\')\n\n######\n# Minimal same process runner\n######\n# object_detector = ObjectDetector()\n# tensor_input = np.expand_dims(np.full((300,300,3), 0, np.uint8), axis=0)\n\n# start = datetime.datetime.now().timestamp()\n\n# frame_times = []\n# for x in range(0, 1000):\n#   start_frame = datetime.datetime.now().timestamp()\n\n#   tensor_input[:] = my_frame\n#   detections = object_detector.detect_raw(tensor_input)\n#   parsed_detections = []\n#   for d in detections:\n#       if d[1] < 0.4:\n#           break\n#       parsed_detections.append((\n#           labels[int(d[0])],\n#           float(d[1]),\n#           (d[2], d[3], d[4], d[5])\n#       ))\n#   frame_times.append(datetime.datetime.now().timestamp()-start_frame)\n\n# duration = datetime.datetime.now().timestamp()-start\n# print(f""Processed for {duration:.2f} seconds."")\n# print(f""Average frame processing time: {mean(frame_times)*1000:.2f}ms"")\n\n######\n# Separate process runner\n######\ndef start(id, num_detections, detection_queue):\n  object_detector = RemoteObjectDetector(str(id), \'/labelmap.txt\', detection_queue)\n  start = datetime.datetime.now().timestamp()\n\n  frame_times = []\n  for x in range(0, num_detections):\n    start_frame = datetime.datetime.now().timestamp()\n    detections = object_detector.detect(my_frame)\n    frame_times.append(datetime.datetime.now().timestamp()-start_frame)\n\n  duration = datetime.datetime.now().timestamp()-start\n  print(f""{id} - Processed for {duration:.2f} seconds."")\n  print(f""{id} - Average frame processing time: {mean(frame_times)*1000:.2f}ms"")\n\nedgetpu_process = EdgeTPUProcess()\n\n# start(1, 1000, edgetpu_process.detect_lock, edgetpu_process.detect_ready, edgetpu_process.frame_ready)\n\n####\n# Multiple camera processes\n####\ncamera_processes = []\nfor x in range(0, 10):\n  camera_process = mp.Process(target=start, args=(x, 100, edgetpu_process.detection_queue))\n  camera_process.daemon = True\n  camera_processes.append(camera_process)\n\nstart = datetime.datetime.now().timestamp()\n\nfor p in camera_processes:\n  p.start()\n\nfor p in camera_processes:\n  p.join()\n\nduration = datetime.datetime.now().timestamp()-start\nprint(f""Total - Processed for {duration:.2f} seconds."")'"
detect_objects.py,0,"b'import os\nimport sys\nimport traceback\nimport signal\nimport cv2\nimport time\nimport datetime\nimport queue\nimport yaml\nimport threading\nimport multiprocessing as mp\nimport subprocess as sp\nimport numpy as np\nimport logging\nfrom flask import Flask, Response, make_response, jsonify, request\nimport paho.mqtt.client as mqtt\n\nfrom frigate.video import track_camera, get_ffmpeg_input, get_frame_shape, CameraCapture, start_or_restart_ffmpeg\nfrom frigate.object_processing import TrackedObjectProcessor\nfrom frigate.util import EventsPerSecond\nfrom frigate.edgetpu import EdgeTPUProcess\n\nFRIGATE_VARS = {k: v for k, v in os.environ.items() if k.startswith(\'FRIGATE_\')}\n\nwith open(\'/config/config.yml\') as f:\n    CONFIG = yaml.safe_load(f)\n\nMQTT_HOST = CONFIG[\'mqtt\'][\'host\']\nMQTT_PORT = CONFIG.get(\'mqtt\', {}).get(\'port\', 1883)\nMQTT_TOPIC_PREFIX = CONFIG.get(\'mqtt\', {}).get(\'topic_prefix\', \'frigate\')\nMQTT_USER = CONFIG.get(\'mqtt\', {}).get(\'user\')\nMQTT_PASS = CONFIG.get(\'mqtt\', {}).get(\'password\')\nif not MQTT_PASS is None:\n    MQTT_PASS = MQTT_PASS.format(**FRIGATE_VARS)\nMQTT_CLIENT_ID = CONFIG.get(\'mqtt\', {}).get(\'client_id\', \'frigate\')\n\n# Set the default FFmpeg config\nFFMPEG_CONFIG = CONFIG.get(\'ffmpeg\', {})\nFFMPEG_DEFAULT_CONFIG = {\n    \'global_args\': FFMPEG_CONFIG.get(\'global_args\', \n        [\'-hide_banner\',\'-loglevel\',\'panic\']),\n    \'hwaccel_args\': FFMPEG_CONFIG.get(\'hwaccel_args\',\n        []),\n    \'input_args\': FFMPEG_CONFIG.get(\'input_args\',\n        [\'-avoid_negative_ts\', \'make_zero\',\n         \'-fflags\', \'nobuffer\',\n         \'-flags\', \'low_delay\',\n         \'-strict\', \'experimental\',\n         \'-fflags\', \'+genpts+discardcorrupt\',\n         \'-vsync\', \'drop\',\n         \'-rtsp_transport\', \'tcp\',\n         \'-stimeout\', \'5000000\',\n         \'-use_wallclock_as_timestamps\', \'1\']),\n    \'output_args\': FFMPEG_CONFIG.get(\'output_args\',\n        [\'-f\', \'rawvideo\',\n         \'-pix_fmt\', \'rgb24\'])\n}\n\nGLOBAL_OBJECT_CONFIG = CONFIG.get(\'objects\', {})\n\nWEB_PORT = CONFIG.get(\'web_port\', 5000)\nDEBUG = (CONFIG.get(\'debug\', \'0\') == \'1\')\n\ndef start_plasma_store():\n    plasma_cmd = [\'plasma_store\', \'-m\', \'400000000\', \'-s\', \'/tmp/plasma\']\n    plasma_process = sp.Popen(plasma_cmd, stdout=sp.DEVNULL, stderr=sp.DEVNULL)\n    time.sleep(1)\n    rc = plasma_process.poll()\n    if rc is not None:\n        return None\n    return plasma_process\n\nclass CameraWatchdog(threading.Thread):\n    def __init__(self, camera_processes, config, tflite_process, tracked_objects_queue, plasma_process):\n        threading.Thread.__init__(self)\n        self.camera_processes = camera_processes\n        self.config = config\n        self.tflite_process = tflite_process\n        self.tracked_objects_queue = tracked_objects_queue\n        self.plasma_process = plasma_process\n\n    def run(self):\n        time.sleep(10)\n        while True:\n            # wait a bit before checking\n            time.sleep(10)\n\n            now = datetime.datetime.now().timestamp()\n            \n            # check the plasma process\n            rc = self.plasma_process.poll()\n            if rc != None:\n                print(f""plasma_process exited unexpectedly with {rc}"")\n                self.plasma_process = start_plasma_store()\n\n            # check the detection process\n            detection_start = self.tflite_process.detection_start.value\n            if (detection_start > 0.0 and \n                now - detection_start > 10):\n                print(""Detection appears to be stuck. Restarting detection process"")\n                self.tflite_process.start_or_restart()\n            elif not self.tflite_process.detect_process.is_alive():\n                print(""Detection appears to have stopped. Restarting detection process"")\n                self.tflite_process.start_or_restart()\n\n            # check the camera processes\n            for name, camera_process in self.camera_processes.items():\n                process = camera_process[\'process\']\n                if not process.is_alive():\n                    print(f""Track process for {name} is not alive. Starting again..."")\n                    camera_process[\'process_fps\'].value = 0.0\n                    camera_process[\'detection_fps\'].value = 0.0\n                    camera_process[\'read_start\'].value = 0.0\n                    process = mp.Process(target=track_camera, args=(name, self.config[name], GLOBAL_OBJECT_CONFIG, camera_process[\'frame_queue\'],\n                        camera_process[\'frame_shape\'], self.tflite_process.detection_queue, self.tracked_objects_queue, \n                        camera_process[\'process_fps\'], camera_process[\'detection_fps\'],\n                        camera_process[\'read_start\'], camera_process[\'detection_frame\']))\n                    process.daemon = True\n                    camera_process[\'process\'] = process\n                    process.start()\n                    print(f""Track process started for {name}: {process.pid}"")\n                \n                if not camera_process[\'capture_thread\'].is_alive():\n                    frame_shape = camera_process[\'frame_shape\']\n                    frame_size = frame_shape[0] * frame_shape[1] * frame_shape[2]\n                    ffmpeg_process = start_or_restart_ffmpeg(camera_process[\'ffmpeg_cmd\'], frame_size)\n                    camera_capture = CameraCapture(name, ffmpeg_process, frame_shape, camera_process[\'frame_queue\'], \n                        camera_process[\'take_frame\'], camera_process[\'camera_fps\'], camera_process[\'detection_frame\'])\n                    camera_capture.start()\n                    camera_process[\'ffmpeg_process\'] = ffmpeg_process\n                    camera_process[\'capture_thread\'] = camera_capture\n                elif now - camera_process[\'capture_thread\'].current_frame > 5:\n                    print(f""No frames received from {name} in 5 seconds. Exiting ffmpeg..."")\n                    ffmpeg_process = camera_process[\'ffmpeg_process\']\n                    ffmpeg_process.terminate()\n                    try:\n                        print(""Waiting for ffmpeg to exit gracefully..."")\n                        ffmpeg_process.communicate(timeout=30)\n                    except sp.TimeoutExpired:\n                        print(""FFmpeg didnt exit. Force killing..."")\n                        ffmpeg_process.kill()\n                        ffmpeg_process.communicate()\n\ndef main():\n    # connect to mqtt and setup last will\n    def on_connect(client, userdata, flags, rc):\n        print(""On connect called"")\n        if rc != 0:\n            if rc == 3:\n                print (""MQTT Server unavailable"")\n            elif rc == 4:\n                print (""MQTT Bad username or password"")\n            elif rc == 5:\n                print (""MQTT Not authorized"")\n            else:\n                print (""Unable to connect to MQTT: Connection refused. Error code: "" + str(rc))\n        # publish a message to signal that the service is running\n        client.publish(MQTT_TOPIC_PREFIX+\'/available\', \'online\', retain=True)\n    client = mqtt.Client(client_id=MQTT_CLIENT_ID)\n    client.on_connect = on_connect\n    client.will_set(MQTT_TOPIC_PREFIX+\'/available\', payload=\'offline\', qos=1, retain=True)\n    if not MQTT_USER is None:\n        client.username_pw_set(MQTT_USER, password=MQTT_PASS)\n    client.connect(MQTT_HOST, MQTT_PORT, 60)\n    client.loop_start()\n\n    plasma_process = start_plasma_store()\n\n    ##\n    # Setup config defaults for cameras\n    ##\n    for name, config in CONFIG[\'cameras\'].items():\n        config[\'snapshots\'] = {\n            \'show_timestamp\': config.get(\'snapshots\', {}).get(\'show_timestamp\', True)\n        }\n\n    # Queue for cameras to push tracked objects to\n    tracked_objects_queue = mp.SimpleQueue()\n    \n    # Start the shared tflite process\n    tflite_process = EdgeTPUProcess()\n\n    # start the camera processes\n    camera_processes = {}\n    for name, config in CONFIG[\'cameras\'].items():\n        # Merge the ffmpeg config with the global config\n        ffmpeg = config.get(\'ffmpeg\', {})\n        ffmpeg_input = get_ffmpeg_input(ffmpeg[\'input\'])\n        ffmpeg_global_args = ffmpeg.get(\'global_args\', FFMPEG_DEFAULT_CONFIG[\'global_args\'])\n        ffmpeg_hwaccel_args = ffmpeg.get(\'hwaccel_args\', FFMPEG_DEFAULT_CONFIG[\'hwaccel_args\'])\n        ffmpeg_input_args = ffmpeg.get(\'input_args\', FFMPEG_DEFAULT_CONFIG[\'input_args\'])\n        ffmpeg_output_args = ffmpeg.get(\'output_args\', FFMPEG_DEFAULT_CONFIG[\'output_args\'])\n        ffmpeg_cmd = ([\'ffmpeg\'] +\n                ffmpeg_global_args +\n                ffmpeg_hwaccel_args +\n                ffmpeg_input_args +\n                [\'-i\', ffmpeg_input] +\n                ffmpeg_output_args +\n                [\'pipe:\'])\n        \n        if \'width\' in config and \'height\' in config:\n            frame_shape = (config[\'height\'], config[\'width\'], 3)\n        else:\n            frame_shape = get_frame_shape(ffmpeg_input)\n\n        frame_size = frame_shape[0] * frame_shape[1] * frame_shape[2]\n        take_frame = config.get(\'take_frame\', 1)\n\n        detection_frame = mp.Value(\'d\', 0.0)\n\n        ffmpeg_process = start_or_restart_ffmpeg(ffmpeg_cmd, frame_size)\n        frame_queue = mp.SimpleQueue()\n        camera_fps = EventsPerSecond()\n        camera_fps.start()\n        camera_capture = CameraCapture(name, ffmpeg_process, frame_shape, frame_queue, take_frame, camera_fps, detection_frame)\n        camera_capture.start()\n\n        camera_processes[name] = {\n            \'camera_fps\': camera_fps,\n            \'take_frame\': take_frame,\n            \'process_fps\': mp.Value(\'d\', 0.0),\n            \'detection_fps\': mp.Value(\'d\', 0.0),\n            \'detection_frame\': detection_frame,\n            \'read_start\': mp.Value(\'d\', 0.0),\n            \'ffmpeg_process\': ffmpeg_process,\n            \'ffmpeg_cmd\': ffmpeg_cmd,\n            \'frame_queue\': frame_queue,\n            \'frame_shape\': frame_shape,\n            \'capture_thread\': camera_capture\n        }\n\n        camera_process = mp.Process(target=track_camera, args=(name, config, GLOBAL_OBJECT_CONFIG, frame_queue, frame_shape,\n            tflite_process.detection_queue, tracked_objects_queue, camera_processes[name][\'process_fps\'], \n            camera_processes[name][\'detection_fps\'], \n            camera_processes[name][\'read_start\'], camera_processes[name][\'detection_frame\']))\n        camera_process.daemon = True\n        camera_processes[name][\'process\'] = camera_process\n\n    for name, camera_process in camera_processes.items():\n        camera_process[\'process\'].start()\n        print(f""Camera_process started for {name}: {camera_process[\'process\'].pid}"")\n    \n    object_processor = TrackedObjectProcessor(CONFIG[\'cameras\'], client, MQTT_TOPIC_PREFIX, tracked_objects_queue)\n    object_processor.start()\n    \n    camera_watchdog = CameraWatchdog(camera_processes, CONFIG[\'cameras\'], tflite_process, tracked_objects_queue, plasma_process)\n    camera_watchdog.start()\n\n    # create a flask app that encodes frames a mjpeg on demand\n    app = Flask(__name__)\n    log = logging.getLogger(\'werkzeug\')\n    log.setLevel(logging.ERROR)\n\n    @app.route(\'/\')\n    def ishealthy():\n        # return a healh\n        return ""Frigate is running. Alive and healthy!""\n\n    @app.route(\'/debug/stack\')\n    def processor_stack():\n        frame = sys._current_frames().get(object_processor.ident, None)\n        if frame:\n            return ""<br>"".join(traceback.format_stack(frame)), 200\n        else:\n            return ""no frame found"", 200\n\n    @app.route(\'/debug/print_stack\')\n    def print_stack():\n        pid = int(request.args.get(\'pid\', 0))\n        if pid == 0:\n            return ""missing pid"", 200\n        else:\n            os.kill(pid, signal.SIGUSR1)\n            return ""check logs"", 200\n\n    @app.route(\'/debug/stats\')\n    def stats():\n        stats = {}\n\n        total_detection_fps = 0\n\n        for name, camera_stats in camera_processes.items():\n            total_detection_fps += camera_stats[\'detection_fps\'].value\n            capture_thread = camera_stats[\'capture_thread\']\n            stats[name] = {\n                \'camera_fps\': round(capture_thread.fps.eps(), 2),\n                \'process_fps\': round(camera_stats[\'process_fps\'].value, 2),\n                \'skipped_fps\': round(capture_thread.skipped_fps.eps(), 2),\n                \'detection_fps\': round(camera_stats[\'detection_fps\'].value, 2),\n                \'read_start\': camera_stats[\'read_start\'].value,\n                \'pid\': camera_stats[\'process\'].pid,\n                \'ffmpeg_pid\': camera_stats[\'ffmpeg_process\'].pid,\n                \'frame_info\': {\n                    \'read\': capture_thread.current_frame,\n                    \'detect\': camera_stats[\'detection_frame\'].value,\n                    \'process\': object_processor.camera_data[name][\'current_frame_time\']\n                }\n            }\n        \n        stats[\'coral\'] = {\n            \'fps\': round(total_detection_fps, 2),\n            \'inference_speed\': round(tflite_process.avg_inference_speed.value*1000, 2),\n            \'detection_start\': tflite_process.detection_start.value,\n            \'pid\': tflite_process.detect_process.pid\n        }\n\n        rc = camera_watchdog.plasma_process.poll()\n        stats[\'plasma_store_rc\'] = rc\n\n        return jsonify(stats)\n\n    @app.route(\'/<camera_name>/<label>/best.jpg\')\n    def best(camera_name, label):\n        if camera_name in CONFIG[\'cameras\']:\n            best_frame = object_processor.get_best(camera_name, label)\n            if best_frame is None:\n                best_frame = np.zeros((720,1280,3), np.uint8)\n            best_frame = cv2.cvtColor(best_frame, cv2.COLOR_RGB2BGR)\n            ret, jpg = cv2.imencode(\'.jpg\', best_frame)\n            response = make_response(jpg.tobytes())\n            response.headers[\'Content-Type\'] = \'image/jpg\'\n            return response\n        else:\n            return ""Camera named {} not found"".format(camera_name), 404\n\n    @app.route(\'/<camera_name>\')\n    def mjpeg_feed(camera_name):\n        fps = int(request.args.get(\'fps\', \'3\'))\n        height = int(request.args.get(\'h\', \'360\'))\n        if camera_name in CONFIG[\'cameras\']:\n            # return a multipart response\n            return Response(imagestream(camera_name, fps, height),\n                            mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n        else:\n            return ""Camera named {} not found"".format(camera_name), 404\n\n    def imagestream(camera_name, fps, height):\n        while True:\n            # max out at specified FPS\n            time.sleep(1/fps)\n            frame = object_processor.get_current_frame(camera_name)\n            if frame is None:\n                frame = np.zeros((height,int(height*16/9),3), np.uint8)\n\n            width = int(height*frame.shape[1]/frame.shape[0])\n\n            frame = cv2.resize(frame, dsize=(width, height), interpolation=cv2.INTER_LINEAR)\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n\n            ret, jpg = cv2.imencode(\'.jpg\', frame)\n            yield (b\'--frame\\r\\n\'\n                b\'Content-Type: image/jpeg\\r\\n\\r\\n\' + jpg.tobytes() + b\'\\r\\n\\r\\n\')\n\n    app.run(host=\'0.0.0.0\', port=WEB_PORT, debug=False)\n\n    object_processor.join()\n    \n    plasma_process.terminate()\n\nif __name__ == \'__main__\':\n    main()\n'"
frigate/edgetpu.py,0,"b'import os\nimport datetime\nimport hashlib\nimport multiprocessing as mp\nimport numpy as np\nimport pyarrow.plasma as plasma\nimport tflite_runtime.interpreter as tflite\nfrom tflite_runtime.interpreter import load_delegate\nfrom frigate.util import EventsPerSecond, listen\n\ndef load_labels(path, encoding=\'utf-8\'):\n  """"""Loads labels from file (with or without index numbers).\n  Args:\n    path: path to label file.\n    encoding: label file encoding.\n  Returns:\n    Dictionary mapping indices to labels.\n  """"""\n  with open(path, \'r\', encoding=encoding) as f:\n    lines = f.readlines()\n    if not lines:\n        return {}\n\n    if lines[0].split(\' \', maxsplit=1)[0].isdigit():\n        pairs = [line.split(\' \', maxsplit=1) for line in lines]\n        return {int(index): label.strip() for index, label in pairs}\n    else:\n        return {index: line.strip() for index, line in enumerate(lines)}\n\nclass ObjectDetector():\n    def __init__(self):\n        edge_tpu_delegate = None\n        try:\n            edge_tpu_delegate = load_delegate(\'libedgetpu.so.1.0\')\n        except ValueError:\n            print(""No EdgeTPU detected. Falling back to CPU."")\n        \n        if edge_tpu_delegate is None:\n            self.interpreter = tflite.Interpreter(\n                model_path=\'/cpu_model.tflite\')\n        else:\n            self.interpreter = tflite.Interpreter(\n                model_path=\'/edgetpu_model.tflite\',\n                experimental_delegates=[edge_tpu_delegate])\n        \n        self.interpreter.allocate_tensors()\n\n        self.tensor_input_details = self.interpreter.get_input_details()\n        self.tensor_output_details = self.interpreter.get_output_details()\n    \n    def detect_raw(self, tensor_input):\n        self.interpreter.set_tensor(self.tensor_input_details[0][\'index\'], tensor_input)\n        self.interpreter.invoke()\n        boxes = np.squeeze(self.interpreter.get_tensor(self.tensor_output_details[0][\'index\']))\n        label_codes = np.squeeze(self.interpreter.get_tensor(self.tensor_output_details[1][\'index\']))\n        scores = np.squeeze(self.interpreter.get_tensor(self.tensor_output_details[2][\'index\']))\n\n        detections = np.zeros((20,6), np.float32)\n        for i, score in enumerate(scores):\n            detections[i] = [label_codes[i], score, boxes[i][0], boxes[i][1], boxes[i][2], boxes[i][3]]\n        \n        return detections\n\ndef run_detector(detection_queue, avg_speed, start):\n    print(f""Starting detection process: {os.getpid()}"")\n    listen()\n    plasma_client = plasma.connect(""/tmp/plasma"")\n    object_detector = ObjectDetector()\n\n    while True:\n        object_id_str = detection_queue.get()\n        object_id_hash = hashlib.sha1(str.encode(object_id_str))\n        object_id = plasma.ObjectID(object_id_hash.digest())\n        object_id_out = plasma.ObjectID(hashlib.sha1(str.encode(f""out-{object_id_str}"")).digest())\n        input_frame = plasma_client.get(object_id, timeout_ms=0)\n\n        if input_frame is plasma.ObjectNotAvailable:\n            continue\n\n        # detect and put the output in the plasma store\n        start.value = datetime.datetime.now().timestamp()\n        plasma_client.put(object_detector.detect_raw(input_frame), object_id_out)\n        duration = datetime.datetime.now().timestamp()-start.value\n        start.value = 0.0\n\n        avg_speed.value = (avg_speed.value*9 + duration)/10\n        \nclass EdgeTPUProcess():\n    def __init__(self):\n        self.detection_queue = mp.SimpleQueue()\n        self.avg_inference_speed = mp.Value(\'d\', 0.01)\n        self.detection_start = mp.Value(\'d\', 0.0)\n        self.detect_process = None\n        self.start_or_restart()\n\n    def start_or_restart(self):\n        self.detection_start.value = 0.0\n        if (not self.detect_process is None) and self.detect_process.is_alive():\n            self.detect_process.terminate()\n            print(""Waiting for detection process to exit gracefully..."")\n            self.detect_process.join(timeout=30)\n            if self.detect_process.exitcode is None:\n                print(""Detection process didnt exit. Force killing..."")\n                self.detect_process.kill()\n                self.detect_process.join()\n        self.detect_process = mp.Process(target=run_detector, args=(self.detection_queue, self.avg_inference_speed, self.detection_start))\n        self.detect_process.daemon = True\n        self.detect_process.start()\n\nclass RemoteObjectDetector():\n    def __init__(self, name, labels, detection_queue):\n        self.labels = load_labels(labels)\n        self.name = name\n        self.fps = EventsPerSecond()\n        self.plasma_client = plasma.connect(""/tmp/plasma"")\n        self.detection_queue = detection_queue\n    \n    def detect(self, tensor_input, threshold=.4):\n        detections = []\n\n        now = f""{self.name}-{str(datetime.datetime.now().timestamp())}""\n        object_id_frame = plasma.ObjectID(hashlib.sha1(str.encode(now)).digest())\n        object_id_detections = plasma.ObjectID(hashlib.sha1(str.encode(f""out-{now}"")).digest())\n        self.plasma_client.put(tensor_input, object_id_frame)\n        self.detection_queue.put(now)\n        raw_detections = self.plasma_client.get(object_id_detections, timeout_ms=10000)\n\n        if raw_detections is plasma.ObjectNotAvailable:\n            self.plasma_client.delete([object_id_frame])\n            return detections\n\n        for d in raw_detections:\n            if d[1] < threshold:\n                break\n            detections.append((\n                self.labels[int(d[0])],\n                float(d[1]),\n                (d[2], d[3], d[4], d[5])\n            ))\n        self.plasma_client.delete([object_id_frame, object_id_detections])\n        self.fps.update()\n        return detections'"
frigate/motion.py,0,"b'import cv2\nimport imutils\nimport numpy as np\n\nclass MotionDetector():\n    def __init__(self, frame_shape, mask, resize_factor=4):\n        self.resize_factor = resize_factor\n        self.motion_frame_size = (int(frame_shape[0]/resize_factor), int(frame_shape[1]/resize_factor))\n        self.avg_frame = np.zeros(self.motion_frame_size, np.float)\n        self.avg_delta = np.zeros(self.motion_frame_size, np.float)\n        self.motion_frame_count = 0\n        self.frame_counter = 0\n        resized_mask = cv2.resize(mask, dsize=(self.motion_frame_size[1], self.motion_frame_size[0]), interpolation=cv2.INTER_LINEAR)\n        self.mask = np.where(resized_mask==[0])\n\n    def detect(self, frame):\n        motion_boxes = []\n\n        # resize frame\n        resized_frame = cv2.resize(frame, dsize=(self.motion_frame_size[1], self.motion_frame_size[0]), interpolation=cv2.INTER_LINEAR)\n\n        # convert to grayscale\n        gray = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n\n        # mask frame\n        gray[self.mask] = [255]\n\n        # it takes ~30 frames to establish a baseline\n        # dont bother looking for motion\n        if self.frame_counter < 30:\n            self.frame_counter += 1\n        else:\n            # compare to average\n            frameDelta = cv2.absdiff(gray, cv2.convertScaleAbs(self.avg_frame))\n\n            # compute the average delta over the past few frames\n            # the alpha value can be modified to configure how sensitive the motion detection is.\n            # higher values mean the current frame impacts the delta a lot, and a single raindrop may\n            # register as motion, too low and a fast moving person wont be detected as motion\n            # this also assumes that a person is in the same location across more than a single frame\n            cv2.accumulateWeighted(frameDelta, self.avg_delta, 0.2)\n\n            # compute the threshold image for the current frame\n            current_thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n\n            # black out everything in the avg_delta where there isnt motion in the current frame\n            avg_delta_image = cv2.convertScaleAbs(self.avg_delta)\n            avg_delta_image[np.where(current_thresh==[0])] = [0]\n\n            # then look for deltas above the threshold, but only in areas where there is a delta\n            # in the current frame. this prevents deltas from previous frames from being included\n            thresh = cv2.threshold(avg_delta_image, 25, 255, cv2.THRESH_BINARY)[1]\n\n            # dilate the thresholded image to fill in holes, then find contours\n            # on thresholded image\n            thresh = cv2.dilate(thresh, None, iterations=2)\n            cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            cnts = imutils.grab_contours(cnts)\n\n            # loop over the contours\n            for c in cnts:\n                # if the contour is big enough, count it as motion\n                contour_area = cv2.contourArea(c)\n                if contour_area > 100:\n                    x, y, w, h = cv2.boundingRect(c)\n                    motion_boxes.append((x*self.resize_factor, y*self.resize_factor, (x+w)*self.resize_factor, (y+h)*self.resize_factor))\n        \n        if len(motion_boxes) > 0:\n            self.motion_frame_count += 1\n            # TODO: this really depends on FPS\n            if self.motion_frame_count >= 10:\n                # only average in the current frame if the difference persists for at least 3 frames\n                cv2.accumulateWeighted(gray, self.avg_frame, 0.2)\n        else:\n            # when no motion, just keep averaging the frames together\n            cv2.accumulateWeighted(gray, self.avg_frame, 0.2)\n            self.motion_frame_count = 0\n\n        return motion_boxes'"
frigate/object_processing.py,0,"b'import json\nimport hashlib\nimport datetime\nimport time\nimport copy\nimport cv2\nimport threading\nimport numpy as np\nfrom collections import Counter, defaultdict\nimport itertools\nimport pyarrow.plasma as plasma\nimport matplotlib.pyplot as plt\nfrom frigate.util import draw_box_with_label, PlasmaManager\nfrom frigate.edgetpu import load_labels\n\nPATH_TO_LABELS = \'/labelmap.txt\'\n\nLABELS = load_labels(PATH_TO_LABELS)\ncmap = plt.cm.get_cmap(\'tab10\', len(LABELS.keys()))\n\nCOLOR_MAP = {}\nfor key, val in LABELS.items():\n    COLOR_MAP[val] = tuple(int(round(255 * c)) for c in cmap(key)[:3])\n\nclass TrackedObjectProcessor(threading.Thread):\n    def __init__(self, config, client, topic_prefix, tracked_objects_queue):\n        threading.Thread.__init__(self)\n        self.config = config\n        self.client = client\n        self.topic_prefix = topic_prefix\n        self.tracked_objects_queue = tracked_objects_queue\n        self.camera_data = defaultdict(lambda: {\n            \'best_objects\': {},\n            \'object_status\': defaultdict(lambda: defaultdict(lambda: \'OFF\')),\n            \'tracked_objects\': {},\n            \'current_frame\': np.zeros((720,1280,3), np.uint8),\n            \'current_frame_time\': 0.0,\n            \'object_id\': None\n        })\n        self.plasma_client = PlasmaManager()\n        \n    def get_best(self, camera, label):\n        if label in self.camera_data[camera][\'best_objects\']:\n            return self.camera_data[camera][\'best_objects\'][label][\'frame\']\n        else:\n            return None\n    \n    def get_current_frame(self, camera):\n        return self.camera_data[camera][\'current_frame\']\n\n    def run(self):\n        while True:\n            camera, frame_time, tracked_objects = self.tracked_objects_queue.get()\n\n            config = self.config[camera]\n            best_objects = self.camera_data[camera][\'best_objects\']\n            current_object_status = self.camera_data[camera][\'object_status\']\n            self.camera_data[camera][\'tracked_objects\'] = tracked_objects\n            self.camera_data[camera][\'current_frame_time\'] = frame_time\n\n            ###\n            # Draw tracked objects on the frame\n            ###\n            current_frame = self.plasma_client.get(f""{camera}{frame_time}"")\n\n            if not current_frame is plasma.ObjectNotAvailable:\n                # draw the bounding boxes on the frame\n                for obj in tracked_objects.values():\n                    thickness = 2\n                    color = COLOR_MAP[obj[\'label\']]\n                    \n                    if obj[\'frame_time\'] != frame_time:\n                        thickness = 1\n                        color = (255,0,0)\n\n                    # draw the bounding boxes on the frame\n                    box = obj[\'box\']\n                    draw_box_with_label(current_frame, box[0], box[1], box[2], box[3], obj[\'label\'], f""{int(obj[\'score\']*100)}% {int(obj[\'area\'])}"", thickness=thickness, color=color)\n                    # draw the regions on the frame\n                    region = obj[\'region\']\n                    cv2.rectangle(current_frame, (region[0], region[1]), (region[2], region[3]), (0,255,0), 1)\n                \n                if config[\'snapshots\'][\'show_timestamp\']:\n                    time_to_show = datetime.datetime.fromtimestamp(frame_time).strftime(""%m/%d/%Y %H:%M:%S"")\n                    cv2.putText(current_frame, time_to_show, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, fontScale=.8, color=(255, 255, 255), thickness=2)\n\n                ###\n                # Set the current frame\n                ###\n                self.camera_data[camera][\'current_frame\'] = current_frame\n\n                # delete the previous frame from the plasma store and update the object id\n                if not self.camera_data[camera][\'object_id\'] is None:\n                    self.plasma_client.delete(self.camera_data[camera][\'object_id\'])\n                self.camera_data[camera][\'object_id\'] = f""{camera}{frame_time}""\n            \n            ###\n            # Maintain the highest scoring recent object and frame for each label\n            ###\n            for obj in tracked_objects.values():\n                # if the object wasn\'t seen on the current frame, skip it\n                if obj[\'frame_time\'] != frame_time:\n                    continue\n                if obj[\'label\'] in best_objects:\n                    now = datetime.datetime.now().timestamp()\n                    # if the object is a higher score than the current best score \n                    # or the current object is more than 1 minute old, use the new object\n                    if obj[\'score\'] > best_objects[obj[\'label\']][\'score\'] or (now - best_objects[obj[\'label\']][\'frame_time\']) > 60:\n                        obj[\'frame\'] = np.copy(self.camera_data[camera][\'current_frame\'])\n                        best_objects[obj[\'label\']] = obj\n                else:\n                    obj[\'frame\'] = np.copy(self.camera_data[camera][\'current_frame\'])\n                    best_objects[obj[\'label\']] = obj\n\n            ###\n            # Report over MQTT\n            ###\n            # count objects with more than 2 entries in history by type\n            obj_counter = Counter()\n            for obj in tracked_objects.values():\n                if len(obj[\'history\']) > 1:\n                    obj_counter[obj[\'label\']] += 1\n                    \n            # report on detected objects\n            for obj_name, count in obj_counter.items():\n                new_status = \'ON\' if count > 0 else \'OFF\'\n                if new_status != current_object_status[obj_name]:\n                    current_object_status[obj_name] = new_status\n                    self.client.publish(f""{self.topic_prefix}/{camera}/{obj_name}"", new_status, retain=False)\n                    # send the best snapshot over mqtt\n                    best_frame = cv2.cvtColor(best_objects[obj_name][\'frame\'], cv2.COLOR_RGB2BGR)\n                    ret, jpg = cv2.imencode(\'.jpg\', best_frame)\n                    if ret:\n                        jpg_bytes = jpg.tobytes()\n                        self.client.publish(f""{self.topic_prefix}/{camera}/{obj_name}/snapshot"", jpg_bytes, retain=True)\n\n            # expire any objects that are ON and no longer detected\n            expired_objects = [obj_name for obj_name, status in current_object_status.items() if status == \'ON\' and not obj_name in obj_counter]\n            for obj_name in expired_objects:\n                current_object_status[obj_name] = \'OFF\'\n                self.client.publish(f""{self.topic_prefix}/{camera}/{obj_name}"", \'OFF\', retain=False)\n                # send updated snapshot over mqtt\n                best_frame = cv2.cvtColor(best_objects[obj_name][\'frame\'], cv2.COLOR_RGB2BGR)\n                ret, jpg = cv2.imencode(\'.jpg\', best_frame)\n                if ret:\n                    jpg_bytes = jpg.tobytes()\n                    self.client.publish(f""{self.topic_prefix}/{camera}/{obj_name}/snapshot"", jpg_bytes, retain=True)\n'"
frigate/objects.py,0,"b'import time\nimport datetime\nimport threading\nimport cv2\nimport itertools\nimport copy\nimport numpy as np\nimport multiprocessing as mp\nfrom collections import defaultdict\nfrom scipy.spatial import distance as dist\nfrom frigate.util import draw_box_with_label, calculate_region\n\nclass ObjectTracker():\n    def __init__(self, max_disappeared):\n        self.tracked_objects = {}\n        self.disappeared = {}\n        self.max_disappeared = max_disappeared\n\n    def register(self, index, obj):\n        id = f""{obj[\'frame_time\']}-{index}""\n        obj[\'id\'] = id\n        obj[\'top_score\'] = obj[\'score\']\n        self.add_history(obj)\n        self.tracked_objects[id] = obj\n        self.disappeared[id] = 0\n\n    def deregister(self, id):\n        del self.tracked_objects[id]\n        del self.disappeared[id]\n    \n    def update(self, id, new_obj):\n        self.disappeared[id] = 0\n        self.tracked_objects[id].update(new_obj)\n        self.add_history(self.tracked_objects[id])\n        if self.tracked_objects[id][\'score\'] > self.tracked_objects[id][\'top_score\']:\n            self.tracked_objects[id][\'top_score\'] = self.tracked_objects[id][\'score\']\n    \n    def add_history(self, obj):\n        entry = {\n            \'score\': obj[\'score\'],\n            \'box\': obj[\'box\'],\n            \'region\': obj[\'region\'],\n            \'centroid\': obj[\'centroid\'],\n            \'frame_time\': obj[\'frame_time\']\n        }\n        if \'history\' in obj:\n            obj[\'history\'].append(entry)\n        else:\n            obj[\'history\'] = [entry]\n\n    def match_and_update(self, frame_time, new_objects):\n        # group by name\n        new_object_groups = defaultdict(lambda: [])\n        for obj in new_objects:\n            new_object_groups[obj[0]].append({\n                \'label\': obj[0],\n                \'score\': obj[1],\n                \'box\': obj[2],\n                \'area\': obj[3],\n                \'region\': obj[4],\n                \'frame_time\': frame_time\n            })\n        \n        # update any tracked objects with labels that are not\n        # seen in the current objects and deregister if needed\n        for obj in list(self.tracked_objects.values()):\n            if not obj[\'label\'] in new_object_groups:\n                if self.disappeared[obj[\'id\']] >= self.max_disappeared:\n                    self.deregister(obj[\'id\'])\n                else:\n                    self.disappeared[obj[\'id\']] += 1\n        \n        if len(new_objects) == 0:\n            return\n        \n        # track objects for each label type\n        for label, group in new_object_groups.items():\n            current_objects = [o for o in self.tracked_objects.values() if o[\'label\'] == label]\n            current_ids = [o[\'id\'] for o in current_objects]\n            current_centroids = np.array([o[\'centroid\'] for o in current_objects])\n\n            # compute centroids of new objects\n            for obj in group:\n                centroid_x = int((obj[\'box\'][0]+obj[\'box\'][2]) / 2.0)\n                centroid_y = int((obj[\'box\'][1]+obj[\'box\'][3]) / 2.0)\n                obj[\'centroid\'] = (centroid_x, centroid_y)\n\n            if len(current_objects) == 0:\n                for index, obj in enumerate(group):\n                    self.register(index, obj)\n                return\n            \n            new_centroids = np.array([o[\'centroid\'] for o in group])\n\n            # compute the distance between each pair of tracked\n            # centroids and new centroids, respectively -- our\n            # goal will be to match each new centroid to an existing\n            # object centroid\n            D = dist.cdist(current_centroids, new_centroids)\n\n            # in order to perform this matching we must (1) find the\n            # smallest value in each row and then (2) sort the row\n            # indexes based on their minimum values so that the row\n            # with the smallest value is at the *front* of the index\n            # list\n            rows = D.min(axis=1).argsort()\n\n            # next, we perform a similar process on the columns by\n            # finding the smallest value in each column and then\n            # sorting using the previously computed row index list\n            cols = D.argmin(axis=1)[rows]\n\n            # in order to determine if we need to update, register,\n            # or deregister an object we need to keep track of which\n            # of the rows and column indexes we have already examined\n            usedRows = set()\n            usedCols = set()\n\n            # loop over the combination of the (row, column) index\n            # tuples\n            for (row, col) in zip(rows, cols):\n                # if we have already examined either the row or\n                # column value before, ignore it\n                if row in usedRows or col in usedCols:\n                    continue\n\n                # otherwise, grab the object ID for the current row,\n                # set its new centroid, and reset the disappeared\n                # counter\n                objectID = current_ids[row]\n                self.update(objectID, group[col])\n\n                # indicate that we have examined each of the row and\n                # column indexes, respectively\n                usedRows.add(row)\n                usedCols.add(col)\n\n            # compute the column index we have NOT yet examined\n            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n\n            # in the event that the number of object centroids is\n\t\t\t# equal or greater than the number of input centroids\n\t\t\t# we need to check and see if some of these objects have\n\t\t\t# potentially disappeared\n            if D.shape[0] >= D.shape[1]:\n                for row in unusedRows:\n                    id = current_ids[row]\n\n                    if self.disappeared[id] >= self.max_disappeared:\n                        self.deregister(id)\n                    else:\n                        self.disappeared[id] += 1\n            # if the number of input centroids is greater\n            # than the number of existing object centroids we need to\n            # register each new input centroid as a trackable object\n            else:\n                for col in unusedCols:\n                    self.register(col, group[col])\n'"
frigate/util.py,0,"b'import datetime\nimport time\nimport signal\nimport traceback\nimport collections\nimport numpy as np\nimport cv2\nimport threading\nimport matplotlib.pyplot as plt\nimport hashlib\nimport pyarrow.plasma as plasma\n\ndef draw_box_with_label(frame, x_min, y_min, x_max, y_max, label, info, thickness=2, color=None, position=\'ul\'):\n    if color is None:\n        color = (0,0,255)\n    display_text = ""{}: {}"".format(label, info)\n    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, thickness)\n    font_scale = 0.5\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    # get the width and height of the text box\n    size = cv2.getTextSize(display_text, font, fontScale=font_scale, thickness=2)\n    text_width = size[0][0]\n    text_height = size[0][1]\n    line_height = text_height + size[1]\n    # set the text start position\n    if position == \'ul\':\n        text_offset_x = x_min\n        text_offset_y = 0 if y_min < line_height else y_min - (line_height+8)\n    elif position == \'ur\':\n        text_offset_x = x_max - (text_width+8)\n        text_offset_y = 0 if y_min < line_height else y_min - (line_height+8)\n    elif position == \'bl\':\n        text_offset_x = x_min\n        text_offset_y = y_max\n    elif position == \'br\':\n        text_offset_x = x_max - (text_width+8)\n        text_offset_y = y_max\n    # make the coords of the box with a small padding of two pixels\n    textbox_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y + line_height))\n    cv2.rectangle(frame, textbox_coords[0], textbox_coords[1], color, cv2.FILLED)\n    cv2.putText(frame, display_text, (text_offset_x, text_offset_y + line_height - 3), font, fontScale=font_scale, color=(0, 0, 0), thickness=2)\n\ndef calculate_region(frame_shape, xmin, ymin, xmax, ymax, multiplier=2):    \n    # size is larger than longest edge\n    size = int(max(xmax-xmin, ymax-ymin)*multiplier)\n    # if the size is too big to fit in the frame\n    if size > min(frame_shape[0], frame_shape[1]):\n        size = min(frame_shape[0], frame_shape[1])\n\n    # x_offset is midpoint of bounding box minus half the size\n    x_offset = int((xmax-xmin)/2.0+xmin-size/2.0)\n    # if outside the image\n    if x_offset < 0:\n        x_offset = 0\n    elif x_offset > (frame_shape[1]-size):\n        x_offset = (frame_shape[1]-size)\n\n    # y_offset is midpoint of bounding box minus half the size\n    y_offset = int((ymax-ymin)/2.0+ymin-size/2.0)\n    # if outside the image\n    if y_offset < 0:\n        y_offset = 0\n    elif y_offset > (frame_shape[0]-size):\n        y_offset = (frame_shape[0]-size)\n\n    return (x_offset, y_offset, x_offset+size, y_offset+size)\n\ndef intersection(box_a, box_b):\n    return (\n        max(box_a[0], box_b[0]),\n        max(box_a[1], box_b[1]),\n        min(box_a[2], box_b[2]),\n        min(box_a[3], box_b[3])\n    )\n\ndef area(box):\n    return (box[2]-box[0] + 1)*(box[3]-box[1] + 1)\n    \ndef intersection_over_union(box_a, box_b):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    intersect = intersection(box_a, box_b)\n\n    # compute the area of intersection rectangle\n    inter_area = max(0, intersect[2] - intersect[0] + 1) * max(0, intersect[3] - intersect[1] + 1)\n\n    if inter_area == 0:\n        return 0.0\n    \n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    box_a_area = (box_a[2] - box_a[0] + 1) * (box_a[3] - box_a[1] + 1)\n    box_b_area = (box_b[2] - box_b[0] + 1) * (box_b[3] - box_b[1] + 1)\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = inter_area / float(box_a_area + box_b_area - inter_area)\n\n    # return the intersection over union value\n    return iou\n\ndef clipped(obj, frame_shape):\n    # if the object is within 5 pixels of the region border, and the region is not on the edge\n    # consider the object to be clipped\n    box = obj[2]\n    region = obj[4]\n    if ((region[0] > 5 and box[0]-region[0] <= 5) or \n        (region[1] > 5 and box[1]-region[1] <= 5) or\n        (frame_shape[1]-region[2] > 5 and region[2]-box[2] <= 5) or\n        (frame_shape[0]-region[3] > 5 and region[3]-box[3] <= 5)):\n        return True\n    else:\n        return False\n\nclass EventsPerSecond:\n    def __init__(self, max_events=1000):\n        self._start = None\n        self._max_events = max_events\n        self._timestamps = []\n    \n    def start(self):\n        self._start = datetime.datetime.now().timestamp()\n\n    def update(self):\n        self._timestamps.append(datetime.datetime.now().timestamp())\n        # truncate the list when it goes 100 over the max_size\n        if len(self._timestamps) > self._max_events+100:\n            self._timestamps = self._timestamps[(1-self._max_events):]\n\n    def eps(self, last_n_seconds=10):\n\t\t# compute the (approximate) events in the last n seconds\n        now = datetime.datetime.now().timestamp()\n        seconds = min(now-self._start, last_n_seconds)\n        return len([t for t in self._timestamps if t > (now-last_n_seconds)]) / seconds\n\ndef print_stack(sig, frame):\n    traceback.print_stack(frame)\n\ndef listen():\n    signal.signal(signal.SIGUSR1, print_stack)\n\nclass PlasmaManager:\n    def __init__(self):\n        self.connect()\n    \n    def connect(self):\n        while True:\n            try:\n                self.plasma_client = plasma.connect(""/tmp/plasma"")\n                return\n            except:\n                print(f""TrackedObjectProcessor: unable to connect plasma client"")\n                time.sleep(10)\n\n    def get(self, name, timeout_ms=0):\n        object_id = plasma.ObjectID(hashlib.sha1(str.encode(name)).digest())\n        while True:\n            try:\n                return self.plasma_client.get(object_id, timeout_ms=timeout_ms)\n            except:\n                self.connect()\n                time.sleep(1)\n\n    def put(self, name, obj):\n        object_id = plasma.ObjectID(hashlib.sha1(str.encode(name)).digest())\n        while True:\n            try:\n                self.plasma_client.put(obj, object_id)\n                return\n            except Exception as e:\n                print(f""Failed to put in plasma: {e}"")\n                self.connect()\n                time.sleep(1)\n\n    def delete(self, name):\n        object_id = plasma.ObjectID(hashlib.sha1(str.encode(name)).digest())\n        while True:\n            try:\n                self.plasma_client.delete([object_id])\n                return\n            except:\n                self.connect()\n                time.sleep(1)'"
frigate/video.py,0,"b'import os\nimport time\nimport datetime\nimport cv2\nimport queue\nimport threading\nimport ctypes\nimport pyarrow.plasma as plasma\nimport multiprocessing as mp\nimport subprocess as sp\nimport numpy as np\nimport copy\nimport itertools\nimport json\nfrom collections import defaultdict\nfrom frigate.util import draw_box_with_label, area, calculate_region, clipped, intersection_over_union, intersection, EventsPerSecond, listen, PlasmaManager\nfrom frigate.objects import ObjectTracker\nfrom frigate.edgetpu import RemoteObjectDetector\nfrom frigate.motion import MotionDetector\n\ndef get_frame_shape(source):\n    ffprobe_cmd = "" "".join([\n        \'ffprobe\',\n        \'-v\',\n        \'panic\',\n        \'-show_error\',\n        \'-show_streams\',\n        \'-of\',\n        \'json\',\n        \'""\'+source+\'""\'\n    ])\n    print(ffprobe_cmd)\n    p = sp.Popen(ffprobe_cmd, stdout=sp.PIPE, shell=True)\n    (output, err) = p.communicate()\n    p_status = p.wait()\n    info = json.loads(output)\n    print(info)\n\n    video_info = [s for s in info[\'streams\'] if s[\'codec_type\'] == \'video\'][0]\n\n    if video_info[\'height\'] != 0 and video_info[\'width\'] != 0:\n        return (video_info[\'height\'], video_info[\'width\'], 3)\n    \n    # fallback to using opencv if ffprobe didnt succeed\n    video = cv2.VideoCapture(source)\n    ret, frame = video.read()\n    frame_shape = frame.shape\n    video.release()\n    return frame_shape\n\ndef get_ffmpeg_input(ffmpeg_input):\n    frigate_vars = {k: v for k, v in os.environ.items() if k.startswith(\'FRIGATE_\')}\n    return ffmpeg_input.format(**frigate_vars)\n\ndef filtered(obj, objects_to_track, object_filters, mask):\n    object_name = obj[0]\n\n    if not object_name in objects_to_track:\n        return True\n    \n    if object_name in object_filters:\n        obj_settings = object_filters[object_name]\n\n        # if the min area is larger than the\n        # detected object, don\'t add it to detected objects\n        if obj_settings.get(\'min_area\',-1) > obj[3]:\n            return True\n        \n        # if the detected object is larger than the\n        # max area, don\'t add it to detected objects\n        if obj_settings.get(\'max_area\', 24000000) < obj[3]:\n            return True\n\n        # if the score is lower than the threshold, skip\n        if obj_settings.get(\'threshold\', 0) > obj[1]:\n            return True\n    \n        # compute the coordinates of the object and make sure\n        # the location isnt outside the bounds of the image (can happen from rounding)\n        y_location = min(int(obj[2][3]), len(mask)-1)\n        x_location = min(int((obj[2][2]-obj[2][0])/2.0)+obj[2][0], len(mask[0])-1)\n\n        # if the object is in a masked location, don\'t add it to detected objects\n        if mask[y_location][x_location] == [0]:\n            return True\n        \n        return False\n\ndef create_tensor_input(frame, region):\n    cropped_frame = frame[region[1]:region[3], region[0]:region[2]]\n\n    # Resize to 300x300 if needed\n    if cropped_frame.shape != (300, 300, 3):\n        cropped_frame = cv2.resize(cropped_frame, dsize=(300, 300), interpolation=cv2.INTER_LINEAR)\n    \n    # Expand dimensions since the model expects images to have shape: [1, 300, 300, 3]\n    return np.expand_dims(cropped_frame, axis=0)\n\ndef start_or_restart_ffmpeg(ffmpeg_cmd, frame_size, ffmpeg_process=None):\n    if not ffmpeg_process is None:\n        print(""Terminating the existing ffmpeg process..."")\n        ffmpeg_process.terminate()\n        try:\n            print(""Waiting for ffmpeg to exit gracefully..."")\n            ffmpeg_process.communicate(timeout=30)\n        except sp.TimeoutExpired:\n            print(""FFmpeg didnt exit. Force killing..."")\n            ffmpeg_process.kill()\n            ffmpeg_process.communicate()\n        ffmpeg_process = None\n\n    print(""Creating ffmpeg process..."")\n    print("" "".join(ffmpeg_cmd))\n    process = sp.Popen(ffmpeg_cmd, stdout = sp.PIPE, stdin = sp.DEVNULL, bufsize=frame_size*10, start_new_session=True)\n    return process\n\nclass CameraCapture(threading.Thread):\n    def __init__(self, name, ffmpeg_process, frame_shape, frame_queue, take_frame, fps, detection_frame):\n        threading.Thread.__init__(self)\n        self.name = name\n        self.frame_shape = frame_shape\n        self.frame_size = frame_shape[0] * frame_shape[1] * frame_shape[2]\n        self.frame_queue = frame_queue\n        self.take_frame = take_frame\n        self.fps = fps\n        self.skipped_fps = EventsPerSecond()\n        self.plasma_client = PlasmaManager()\n        self.ffmpeg_process = ffmpeg_process\n        self.current_frame = 0\n        self.last_frame = 0\n        self.detection_frame = detection_frame\n\n    def run(self):\n        frame_num = 0\n        self.skipped_fps.start()\n        while True:\n            if self.ffmpeg_process.poll() != None:\n                print(f""{self.name}: ffmpeg process is not running. exiting capture thread..."")\n                break\n\n            frame_bytes = self.ffmpeg_process.stdout.read(self.frame_size)\n            self.current_frame = datetime.datetime.now().timestamp()\n\n            if len(frame_bytes) == 0:\n                print(f""{self.name}: ffmpeg didnt return a frame. something is wrong."")\n                continue\n\n            self.fps.update()\n\n            frame_num += 1\n            if (frame_num % self.take_frame) != 0:\n                self.skipped_fps.update()\n                continue\n\n            # if the detection process is more than 1 second behind, skip this frame\n            if self.detection_frame.value > 0.0 and (self.last_frame - self.detection_frame.value) > 1:\n                self.skipped_fps.update()\n                continue\n\n            # put the frame in the plasma store\n            self.plasma_client.put(f""{self.name}{self.current_frame}"",\n                    np\n                        .frombuffer(frame_bytes, np.uint8)\n                        .reshape(self.frame_shape)\n                )\n            # add to the queue\n            self.frame_queue.put(self.current_frame)\n            self.last_frame = self.current_frame\n\ndef track_camera(name, config, global_objects_config, frame_queue, frame_shape, detection_queue, detected_objects_queue, fps, detection_fps, read_start, detection_frame):\n    print(f""Starting process for {name}: {os.getpid()}"")\n    listen()\n\n    detection_frame.value = 0.0\n\n    # Merge the tracked object config with the global config\n    camera_objects_config = config.get(\'objects\', {})    \n    # combine tracked objects lists\n    objects_to_track = set().union(global_objects_config.get(\'track\', [\'person\', \'car\', \'truck\']), camera_objects_config.get(\'track\', []))\n    # merge object filters\n    global_object_filters = global_objects_config.get(\'filters\', {})\n    camera_object_filters = camera_objects_config.get(\'filters\', {})\n    objects_with_config = set().union(global_object_filters.keys(), camera_object_filters.keys())\n    object_filters = {}\n    for obj in objects_with_config:\n        object_filters[obj] = {**global_object_filters.get(obj, {}), **camera_object_filters.get(obj, {})}\n\n    frame = np.zeros(frame_shape, np.uint8)\n\n    # load in the mask for object detection\n    if \'mask\' in config:\n        mask = cv2.imread(""/config/{}"".format(config[\'mask\']), cv2.IMREAD_GRAYSCALE)\n    else:\n        mask = None\n\n    if mask is None:\n        mask = np.zeros((frame_shape[0], frame_shape[1], 1), np.uint8)\n        mask[:] = 255\n\n    motion_detector = MotionDetector(frame_shape, mask, resize_factor=6)\n    object_detector = RemoteObjectDetector(name, \'/labelmap.txt\', detection_queue)\n\n    object_tracker = ObjectTracker(10)\n\n    plasma_client = PlasmaManager()\n    avg_wait = 0.0\n    fps_tracker = EventsPerSecond()\n    fps_tracker.start()\n    object_detector.fps.start()\n    while True:\n        read_start.value = datetime.datetime.now().timestamp()\n        frame_time = frame_queue.get()\n        duration = datetime.datetime.now().timestamp()-read_start.value\n        read_start.value = 0.0\n        avg_wait = (avg_wait*99+duration)/100\n        detection_frame.value = frame_time\n        \n        # Get frame from plasma store\n        frame = plasma_client.get(f""{name}{frame_time}"")\n\n        if frame is plasma.ObjectNotAvailable:\n            continue\n\n        fps_tracker.update()\n        fps.value = fps_tracker.eps()\n        detection_fps.value = object_detector.fps.eps()\n        \n        # look for motion\n        motion_boxes = motion_detector.detect(frame)\n\n        tracked_objects = object_tracker.tracked_objects.values()\n\n        # merge areas of motion that intersect with a known tracked object into a single area to look at\n        areas_of_interest = []\n        used_motion_boxes = []\n        for obj in tracked_objects:\n            x_min, y_min, x_max, y_max = obj[\'box\']\n            for m_index, motion_box in enumerate(motion_boxes):\n                if area(intersection(obj[\'box\'], motion_box))/area(motion_box) > .5:\n                    used_motion_boxes.append(m_index)\n                    x_min = min(obj[\'box\'][0], motion_box[0])\n                    y_min = min(obj[\'box\'][1], motion_box[1])\n                    x_max = max(obj[\'box\'][2], motion_box[2])\n                    y_max = max(obj[\'box\'][3], motion_box[3])\n            areas_of_interest.append((x_min, y_min, x_max, y_max))\n        unused_motion_boxes = set(range(0, len(motion_boxes))).difference(used_motion_boxes)\n        \n        # compute motion regions\n        motion_regions = [calculate_region(frame_shape, motion_boxes[i][0], motion_boxes[i][1], motion_boxes[i][2], motion_boxes[i][3], 1.2)\n            for i in unused_motion_boxes]\n        \n        # compute tracked object regions\n        object_regions = [calculate_region(frame_shape, a[0], a[1], a[2], a[3], 1.2)\n            for a in areas_of_interest]\n        \n        # merge regions with high IOU\n        merged_regions = motion_regions+object_regions\n        while True:\n            max_iou = 0.0\n            max_indices = None\n            region_indices = range(len(merged_regions))\n            for a, b in itertools.combinations(region_indices, 2):\n                iou = intersection_over_union(merged_regions[a], merged_regions[b])\n                if iou > max_iou:\n                    max_iou = iou\n                    max_indices = (a, b)\n            if max_iou > 0.1:\n                a = merged_regions[max_indices[0]]\n                b = merged_regions[max_indices[1]]\n                merged_regions.append(calculate_region(frame_shape,\n                    min(a[0], b[0]),\n                    min(a[1], b[1]),\n                    max(a[2], b[2]),\n                    max(a[3], b[3]),\n                    1\n                ))\n                del merged_regions[max(max_indices[0], max_indices[1])]\n                del merged_regions[min(max_indices[0], max_indices[1])]\n            else:\n                break\n\n        # resize regions and detect\n        detections = []\n        for region in merged_regions:\n\n            tensor_input = create_tensor_input(frame, region)\n\n            region_detections = object_detector.detect(tensor_input)\n\n            for d in region_detections:\n                box = d[2]\n                size = region[2]-region[0]\n                x_min = int((box[1] * size) + region[0])\n                y_min = int((box[0] * size) + region[1])\n                x_max = int((box[3] * size) + region[0])\n                y_max = int((box[2] * size) + region[1])\n                det = (d[0],\n                    d[1],\n                    (x_min, y_min, x_max, y_max),\n                    (x_max-x_min)*(y_max-y_min),\n                    region)\n                if filtered(det, objects_to_track, object_filters, mask):\n                    continue\n                detections.append(det)\n\n        #########\n        # merge objects, check for clipped objects and look again up to N times\n        #########\n        refining = True\n        refine_count = 0\n        while refining and refine_count < 4:\n            refining = False\n\n            # group by name\n            detected_object_groups = defaultdict(lambda: [])\n            for detection in detections:\n                detected_object_groups[detection[0]].append(detection)\n\n            selected_objects = []\n            for group in detected_object_groups.values():\n\n                # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n                boxes = [(o[2][0], o[2][1], o[2][2]-o[2][0], o[2][3]-o[2][1])\n                    for o in group]\n                confidences = [o[1] for o in group]\n                idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n\n                for index in idxs:\n                    obj = group[index[0]]\n                    if clipped(obj, frame_shape):\n                        box = obj[2]\n                        # calculate a new region that will hopefully get the entire object\n                        region = calculate_region(frame_shape, \n                            box[0], box[1],\n                            box[2], box[3])\n                        \n                        tensor_input = create_tensor_input(frame, region)\n                        # run detection on new region\n                        refined_detections = object_detector.detect(tensor_input)\n                        for d in refined_detections:\n                            box = d[2]\n                            size = region[2]-region[0]\n                            x_min = int((box[1] * size) + region[0])\n                            y_min = int((box[0] * size) + region[1])\n                            x_max = int((box[3] * size) + region[0])\n                            y_max = int((box[2] * size) + region[1])\n                            det = (d[0],\n                                d[1],\n                                (x_min, y_min, x_max, y_max),\n                                (x_max-x_min)*(y_max-y_min),\n                                region)\n                            if filtered(det, objects_to_track, object_filters, mask):\n                                continue\n                            selected_objects.append(det)\n\n                        refining = True\n                    else:\n                        selected_objects.append(obj)\n                \n            # set the detections list to only include top, complete objects\n            # and new detections\n            detections = selected_objects\n\n            if refining:\n                refine_count += 1\n        \n        # now that we have refined our detections, we need to track objects\n        object_tracker.match_and_update(frame_time, detections)\n\n        # add to the queue\n        detected_objects_queue.put((name, frame_time, object_tracker.tracked_objects))\n\n    print(f""{name}: exiting subprocess"")'"
