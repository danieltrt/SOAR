file_path,api_count,code
setup.py,0,"b'from setuptools import setup\n\n# import ``__version__`` from code base\nexec(open(\'edward/version.py\').read())\n\nsetup(\n    name=\'edward\',\n    version=__version__,\n    description=\'A library for probabilistic modeling, inference, and \'\n                \'criticism\',\n    author=\'Dustin Tran\',\n    author_email=""dustin@cs.columbia.edu"",\n    packages=[\'edward\', \'edward.criticisms\', \'edward.inferences\',\n              \'edward.models\', \'edward.util\', \'edward.inferences.conjugacy\'],\n    install_requires=[\'numpy>=1.7\',\n                      \'six>=1.10.0\'],\n    extras_require={\n        \'tensorflow\': [\'tensorflow>=1.2.0rc0\'],\n        \'tensorflow with gpu\': [\'tensorflow-gpu>=1.2.0rc0\'],\n        \'neural networks\': [\'keras>=2.0.0\', \'prettytensor>=0.7.4\'],\n        \'datasets\': [\'observations>=0.1.2\'],\n        \'notebooks\': [\'jupyter>=1.0.0\'],\n        \'visualization\': [\'matplotlib>=1.3\',\n                          \'pillow>=3.4.2\',\n                          \'seaborn>=0.3.1\']},\n    tests_require=[\'pytest\', \'pytest-pep8\'],\n    url=\'http://edwardlib.org\',\n    keywords=\'machine learning statistics probabilistic programming tensorflow\',\n    license=\'Apache License 2.0\',\n    classifiers=[\'Intended Audience :: Developers\',\n                 \'Intended Audience :: Education\',\n                 \'Intended Audience :: Science/Research\',\n                 \'License :: OSI Approved :: Apache Software License\',\n                 \'Operating System :: POSIX :: Linux\',\n                 \'Operating System :: MacOS :: MacOS X\',\n                 \'Operating System :: Microsoft :: Windows\',\n                 \'Programming Language :: Python :: 2.7\',\n                 \'Programming Language :: Python :: 3.4\'],\n)\n'"
docs/generate_api_navbar_and_symbols.py,0,"b'""""""Autogenerate navbar and convert {{symbol}}s to a format for the parser.\n\nAll pages in src_dir/api/ must be an element in PAGES. Otherwise the\npage will have no navbar.\n\nThe order of the navbar is given by the order of PAGES.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport shutil\nimport re\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--src_dir\', type=str)\nparser.add_argument(\'--out_dir\', type=str)\nargs = parser.parse_args()\n\n# Note we don\'t strictly need the \'parent_pages\' field. We can\n# technically infer them based on the other pages\' \'child_pages\'. It\n# is denoted only for convenience.\nPAGES = [\n    {\n        \'page\': \'index.tex\',\n        \'title\': \'Home\',\n        \'parent_pages\': [],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'data.tex\',\n        \'title\': \'Data\',\n        \'parent_pages\': [],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'model.tex\',\n        \'title\': \'Model\',\n        \'parent_pages\': [],\n        \'child_pages\': [\n            \'model-compositionality.tex\',\n            \'model-development.tex\',\n        ],\n    },\n    {\n        \'page\': \'model-compositionality.tex\',\n        \'title\': \'Compositionality\',\n        \'parent_pages\': [\n            \'model.tex\'\n        ],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'model-development.tex\',\n        \'title\': \'Development\',\n        \'parent_pages\': [\n            \'model.tex\'\n        ],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'inference.tex\',\n        \'title\': \'Inference\',\n        \'parent_pages\': [],\n        \'child_pages\': [\n            \'inference-classes.tex\',\n            \'inference-compositionality.tex\',\n            \'inference-data-subsampling.tex\',\n            \'inference-development.tex\',\n        ],\n    },\n    {\n        \'page\': \'inference-classes.tex\',\n        \'title\': \'Classes\',\n        \'parent_pages\': [\n            \'inference.tex\'\n        ],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'inference-compositionality.tex\',\n        \'title\': \'Compositionality\',\n        \'parent_pages\': [\n            \'inference.tex\'\n        ],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'inference-data-subsampling.tex\',\n        \'title\': \'Data Subsampling\',\n        \'parent_pages\': [\n            \'inference.tex\'\n        ],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'inference-development.tex\',\n        \'title\': \'Development\',\n        \'parent_pages\': [\n            \'inference.tex\'\n        ],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'criticism.tex\',\n        \'title\': \'Criticism\',\n        \'parent_pages\': [],\n        \'child_pages\': [],\n    },\n    {\n        \'page\': \'reference.tex\',\n        \'title\': \'Reference\',\n        \'parent_pages\': [],\n        \'child_pages\': [],\n    },\n]\n\n\ndef generate_navbar(page_data):\n  """"""Return a string. It is the navigation bar for ``page_data``.""""""\n  def generate_top_navbar():\n    # Create top of navbar. (Note this can be cached and not run within a loop.)\n    top_navbar = """"""\\\\begin{abstract}\n\\section{API and Documentation}\n\\\\begin{lstlisting}[raw=html]\n<div class=""row"" style=""padding-bottom: 5%"">\n<div class=""row"" style=""padding-bottom: 1%"">""""""\n    for page_data in PAGES:\n      title = page_data[\'title\']\n      page_name = page_data[\'page\']\n      parent_pages = page_data[\'parent_pages\']\n      if len(parent_pages) == 0 and \\\n              page_name not in [\'index.tex\', \'reference.tex\']:\n        top_navbar += \'\\n\'\n        top_navbar += \'<a class=""button3"" href=""/api/\'\n        top_navbar += page_name.replace(\'.tex\', \'\')\n        top_navbar += \'"">\'\n        top_navbar += title\n        top_navbar += \'</a>\'\n\n    top_navbar += \'\\n\'\n    top_navbar += \'</div>\'\n    return top_navbar\n\n  page_name = page_data[\'page\']\n  title = page_data[\'title\']\n  parent_pages = page_data[\'parent_pages\']\n  child_pages = page_data[\'child_pages\']\n\n  navbar = generate_top_navbar()\n  # Create bottom of navbar if there are child pages for that section.\n  if len(child_pages) > 0 or len(parent_pages) > 0:\n    if len(parent_pages) > 0:\n      parent = parent_pages[0]\n      parent_page = [page_data for page_data in PAGES\n                     if page_data[\'page\'] == parent][0]\n      pgs = parent_page[\'child_pages\']\n    else:\n      pgs = child_pages\n\n    navbar += \'\\n\'\n    navbar += \'<div class=""row"">\'\n    for child_page in pgs:\n      navbar += \'\\n\'\n      navbar += \'<a class=""button4"" href=""/api/\'\n      navbar += child_page.replace(\'.tex\', \'\')\n      navbar += \'"">\'\n      navbar += [page_data for page_data in PAGES\n                 if page_data[\'page\'] == child_page][0][\'title\']\n      navbar += \'</a>\'\n\n    navbar += \'\\n\'\n    navbar += \'</div>\'\n\n  navbar += \'\\n\'\n  navbar += """"""</div>\n\\end{lstlisting}\n\\end{abstract}""""""\n\n  # Set primary button in navbar. If a child page, set primary buttons\n  # for both top and bottom of navbar.\n  search_term = \'"" href=""/api/\' + page_name.replace(\'.tex\', \'\') + \'"">\'\n  navbar = navbar.replace(search_term, \' button-primary\' + search_term)\n  if len(parent_pages) > 0:\n    parent = parent_pages[0]\n    search_term = \'"" href=""/api/\' + parent.replace(\'.tex\', \'\') + \'"">\'\n    navbar = navbar.replace(search_term, \' button-primary\' + search_term)\n\n  return navbar\n\n\ndef generate_models():\n  import edward.models as module\n  from edward.models import RandomVariable\n  objs = [getattr(module, name) for name in dir(module)]\n  objs = [obj for obj in objs\n          if (isinstance(obj, type) and\n              issubclass(obj, RandomVariable) and\n              obj != RandomVariable\n              )\n          ]\n  objs = sorted(objs, key=lambda cls: cls.__name__)\n\n  links = [(\'@{{ed.models.{}}}\').format(cls.__name__) for cls in objs]\n  return \'\\n\\item\'.join(links)\n\n\ndef generate_criticisms():\n  import edward.criticisms as module\n  objs = [getattr(module, name) for name in dir(module)]\n  objs = [obj for obj in objs\n          if (hasattr(obj, \'__call__\') or\n              isinstance(obj, type))\n          ]\n  objs = sorted(objs, key=lambda cls: cls.__name__)\n\n  links = [(\'@{{ed.criticisms.{}}}\').format(cls.__name__) for cls in objs]\n  return \'\\n\\item\'.join(links)\n\n\ndef generate_util():\n  import edward.util as module\n  objs = [getattr(module, name) for name in dir(module)]\n  objs = [obj for obj in objs\n          if (hasattr(obj, \'__call__\') or\n              isinstance(obj, type))\n          ]\n  objs = sorted(objs, key=lambda cls: cls.__name__)\n\n  links = [(\'@{{ed.util.{}}}\').format(cls.__name__) for cls in objs]\n  return \'\\n\\item\'.join(links)\n\n\ndef get_tensorflow_version():\n  import tensorflow\n  return str(getattr(tensorflow, \'__version__\', \'<unknown verison>\'))\n\n\nprint(""Starting autogeneration."")\nsrc_dir = os.path.expanduser(args.src_dir)\nout_dir = os.path.expanduser(args.out_dir)\nshutil.copytree(src_dir, out_dir)\n\nfor page_data in PAGES:\n  page_name = page_data[\'page\']\n  path = os.path.join(out_dir, \'api\', page_name)\n  print(path)\n\n  # Generate navigation bar.\n  navbar = generate_navbar(page_data)\n\n  # Insert autogenerated content into page.\n  document = open(path).read()\n  assert \'{{navbar}}\' in document, \\\n         (""File found for "" + path + "" but missing {{navbar}} tag."")\n  document = document.replace(\'{{navbar}}\', navbar)\n\n  if \'{{models}}\' in document:\n    document = document.replace(\'{{models}}\', generate_models())\n\n  if \'{{criticisms}}\' in document:\n    document = document.replace(\'{{criticisms}}\', generate_criticisms())\n\n  if \'{{util}}\' in document:\n    document = document.replace(\'{{util}}\', generate_util())\n\n  if \'{{tensorflow_version}}\' in document:\n    document = document.replace(\'{{tensorflow_version}}\',\n                                get_tensorflow_version())\n\n  subdir = os.path.dirname(path)\n  if not os.path.exists(subdir):\n    os.makedirs(subdir)\n\n  # Write to file.\n  open(path, \'w\').write(document)\n'"
docs/generate_api_toc.py,0,"b'""""""Take generated TOC YAML file and format it into template.pandoc.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport yaml\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--src_dir\', type=str)\nparser.add_argument(\'--yaml_dir\', type=str)\nparser.add_argument(\'--out_dir\', type=str)\nargs = parser.parse_args()\n\nsrc_dir = os.path.expanduser(args.src_dir)\nyaml_dir = os.path.expanduser(args.yaml_dir)\nout_dir = os.path.expanduser(args.out_dir)\n\nwith open(yaml_dir) as f:\n  data_map = yaml.safe_load(f)\n\ntoc = \'\'\nfor entry in data_map[\'toc\']:\n  title = entry[\'title\']\n  if title == \'ed\':\n    continue\n\n  section = entry[\'section\']\n  assert section[0][\'title\'] == \'Overview\'\n  path = section[0][\'path\']\n  toc += \'<a class=""button u-full-width"" href=""{}"">{}</a>\'.format(path, title)\n  toc += \'\\n\'\n\ndocument = open(src_dir).read()\ndocument = document.replace(\'{{toc}}\', toc)\nopen(out_dir, \'w\').write(document)\n'"
docs/pandoc-code2raw.py,0,"b'#!/usr/bin/env python\n\n""""""Pandoc filter to insert arbitrary raw output markup\nas Code/CodeBlocks with an attribute raw=<outputformat>.\n\nEspecially useful for inserting LaTeX code which pandoc will\notherwise mangle:\n\n    ````{raw=latex}\n    \\let\\Begin\\begin\n    \\let\\End\\end\n    ````\nor for making HTML opaque to pandoc, which will otherwise\nshow the text between tags in other output formats,\n\nor for allowing Markdown in the arguments of LaTeX commands\nor the contents of LaTeX environments\n\n    `\\textsf{`{raw=latex}<span class=sans>San Seriffe</span>`}`{raw=latex}\n\n    ````{raw=latex}\n    \\begin{center}\n    ````\n    This is *centered*!\n    ````{raw=latex}\n    \\end{center}\n    ````\n\nor for header-includes in metadata:\n\n    ---\n    header-includes: |\n      ````{raw=latex}\n      \\usepackage{pgfpages}\n      \\pgfpagesuselayout{2 on 1}[a4paper]\n      ````\n    ...\n\nSee <https://github.com/jgm/pandoc/issues/2139>\n""""""\n\nfrom pandocfilters import RawInline, RawBlock, toJSONFilter\nfrom pandocattributes import PandocAttributes\n\nraw4code = {\'Code\': RawInline, \'CodeBlock\': RawBlock}\n\n\ndef code2raw(key, val, format, meta):\n    if key not in raw4code:\n        return None\n    attrs = PandocAttributes(val[0], format=\'pandoc\')\n    raw = attrs.kvs.get(\'raw\', None)\n    if raw:\n        # if raw != format:     # but what if we output markdown?\n        #     return []\n        return raw4code[key](raw, val[-1])\n    else:\n        return None\n\n\nif __name__ == ""__main__"":\n    toJSONFilter(code2raw)\n'"
docs/strip_p_in_li.py,0,"b'""""""Strip paragraphs in lists in pandoc\'s html output.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\n\nfrom bs4 import BeautifulSoup\n\npaths = (""build/*.html"", ""build/api/*.html"", ""build/tutorials/*.html"")\nfilenames = []\n\nfor path in paths:\n  filenames.extend(glob.glob(path))\n\nfor filename in filenames:\n  soup = BeautifulSoup(open(filename), \'html.parser\')\n  all_li = soup.find_all(\'li\')\n  if all_li:\n    for list_item in all_li:\n      if list_item.p is not None:\n        list_item.p.unwrap()\n    html = str(soup)\n    html = html.replace(\'border=""1""\', \'\')\n    with open(filename, \'wb\') as file:\n      file.write(html)\n'"
edward/__init__.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom edward import criticisms\nfrom edward import inferences\nfrom edward import models\nfrom edward import util\n\n# Direct imports for convenience\nfrom edward.criticisms import (\n    evaluate, ppc, ppc_density_plot, ppc_stat_hist_plot)\nfrom edward.inferences import (\n    Inference, MonteCarlo, VariationalInference,\n    HMC, MetropolisHastings, SGLD, SGHMC,\n    KLpq, KLqp, ReparameterizationKLqp, ReparameterizationKLKLqp,\n    ReparameterizationEntropyKLqp, ReplicaExchangeMC, ScoreKLqp, ScoreKLKLqp,\n    ScoreEntropyKLqp, ScoreRBKLqp, WakeSleep, GANInference, BiGANInference,\n    WGANInference, ImplicitKLqp, MAP, Laplace, complete_conditional, Gibbs)\nfrom edward.models import RandomVariable\nfrom edward.util import (\n    check_data, check_latent_vars, copy, dot,\n    get_ancestors, get_blanket, get_children, get_control_variate_coef,\n    get_descendants, get_parents, get_session, get_siblings, get_variables,\n    is_independent, Progbar, random_variables, rbf, set_seed,\n    to_simplex, transform)\nfrom edward.version import __version__, VERSION\n\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n# Export modules and constants.\n_allowed_symbols = [\n    'criticisms',\n    'inferences',\n    'models',\n    'util',\n    'evaluate',\n    'ppc',\n    'ppc_density_plot',\n    'ppc_stat_hist_plot',\n    'Inference',\n    'MonteCarlo',\n    'VariationalInference',\n    'HMC',\n    'MetropolisHastings',\n    'SGLD',\n    'SGHMC',\n    'KLpq',\n    'KLqp',\n    'ReparameterizationKLqp',\n    'ReparameterizationKLKLqp',\n    'ReparameterizationEntropyKLqp',\n    'ScoreKLqp',\n    'ScoreKLKLqp',\n    'ScoreEntropyKLqp',\n    'ScoreRBKLqp',\n    'WakeSleep',\n    'GANInference',\n    'BiGANInference',\n    'WGANInference',\n    'ImplicitKLqp',\n    'MAP',\n    'Laplace',\n    'complete_conditional',\n    'Gibbs',\n    'RandomVariable',\n    'check_data',\n    'check_latent_vars',\n    'copy',\n    'dot',\n    'get_ancestors',\n    'get_blanket',\n    'get_children',\n    'get_control_variate_coef',\n    'get_descendants',\n    'get_parents',\n    'get_session',\n    'get_siblings',\n    'get_variables',\n    'is_independent',\n    'Progbar',\n    'random_variables',\n    'ReplicaExchangeMC',\n    'rbf',\n    'set_seed',\n    'to_simplex',\n    'transform',\n    '__version__',\n    'VERSION',\n]\n\n# Remove all extra symbols that don't have a docstring or are not explicitly\n# referenced in the whitelist.\nremove_undocumented(__name__, _allowed_symbols, [\n    criticisms, inferences, models, util\n])\n"""
edward/version.py,0,"b""__version__ = '1.3.5'\nVERSION = __version__\n"""
examples/bayesian_linear_regression.py,14,"b'""""""Bayesian linear regression using stochastic gradient Hamiltonian\nMonte Carlo.\n\nThis version visualizes additional fits of the model.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/supervised-regression\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, Empirical\n\ntf.flags.DEFINE_integer(""N"", default=40, help=""Number of data points."")\ntf.flags.DEFINE_integer(""D"", default=1, help=""Number of features."")\ntf.flags.DEFINE_integer(""T"", default=5000, help=""Number of posterior samples."")\ntf.flags.DEFINE_integer(""nburn"", default=100,\n                        help=""Number of burn-in samples."")\ntf.flags.DEFINE_integer(""stride"", default=10,\n                        help=""Frequency with which to plots samples."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N, noise_std=0.5):\n  X = np.concatenate([np.linspace(0, 2, num=N / 2),\n                      np.linspace(6, 8, num=N / 2)])\n  y = 2.0 * X + 10 * np.random.normal(0, noise_std, size=N)\n  X = X.reshape((N, 1))\n  return X, y\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  X_train, y_train = build_toy_dataset(FLAGS.N)\n  X_test, y_test = build_toy_dataset(FLAGS.N)\n\n  # MODEL\n  X = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.D])\n  w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n  b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n  y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(FLAGS.N))\n\n  # INFERENCE\n  qw = Empirical(params=tf.get_variable(""qw/params"", [FLAGS.T, FLAGS.D]))\n  qb = Empirical(params=tf.get_variable(""qb/params"", [FLAGS.T, 1]))\n\n  inference = ed.SGHMC({w: qw, b: qb}, data={X: X_train, y: y_train})\n  inference.run(step_size=1e-3)\n\n  # CRITICISM\n\n  # Plot posterior samples.\n  sns.jointplot(qb.params.eval()[FLAGS.nburn:FLAGS.T:FLAGS.stride],\n                qw.params.eval()[FLAGS.nburn:FLAGS.T:FLAGS.stride])\n  plt.show()\n\n  # Posterior predictive checks.\n  y_post = ed.copy(y, {w: qw, b: qb})\n  # This is equivalent to\n  # y_post = Normal(loc=ed.dot(X, qw) + qb, scale=tf.ones(FLAGS.N))\n\n  print(""Mean squared error on test data:"")\n  print(ed.evaluate(\'mean_squared_error\', data={X: X_test, y_post: y_test}))\n\n  print(""Displaying prior predictive samples."")\n  n_prior_samples = 10\n\n  w_prior = w.sample(n_prior_samples).eval()\n  b_prior = b.sample(n_prior_samples).eval()\n\n  plt.scatter(X_train, y_train)\n\n  inputs = np.linspace(-1, 10, num=400)\n  for ns in range(n_prior_samples):\n      output = inputs * w_prior[ns] + b_prior[ns]\n      plt.plot(inputs, output)\n\n  plt.show()\n\n  print(""Displaying posterior predictive samples."")\n  n_posterior_samples = 10\n\n  w_post = qw.sample(n_posterior_samples).eval()\n  b_post = qb.sample(n_posterior_samples).eval()\n\n  plt.scatter(X_train, y_train)\n\n  inputs = np.linspace(-1, 10, num=400)\n  for ns in range(n_posterior_samples):\n      output = inputs * w_post[ns] + b_post[ns]\n      plt.plot(inputs, output)\n\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/bayesian_linear_regression_implicitklqp.py,17,"b'""""""Bayesian linear regression. Inference uses data subsampling and\nscales the log-likelihood.\n\nOne local optima is an inferred posterior mean of about [-5.0 5.0].\nThis implies there is some weird symmetry happening; this result can\nbe obtained by initializing the first coordinate to be negative.\nSimilar occurs for the second coordinate.\n\nNote as with all GAN-style training, the algorithm is not stable. It\nis recommended to monitor training and halt manually according to some\ncriterion (e.g., prediction accuracy on validation test, quality of\nsamples).\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/supervised-regression\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\ntf.flags.DEFINE_integer(""N"", default=500, help=""Number of data points."")\ntf.flags.DEFINE_integer(""M"", default=50, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""D"", default=2, help=""Number of features."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N, w, noise_std=0.1):\n  D = len(w)\n  x = np.random.randn(N, D)\n  y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n  return x, y\n\n\ndef generator(arrays, batch_size):\n  """"""Generate batches, one with respect to each array\'s first axis.""""""\n  starts = [0] * len(arrays)  # pointers to where we are in iteration\n  while True:\n    batches = []\n    for i, array in enumerate(arrays):\n      start = starts[i]\n      stop = start + batch_size\n      diff = stop - array.shape[0]\n      if diff <= 0:\n        batch = array[start:stop]\n        starts[i] += batch_size\n      else:\n        batch = np.concatenate((array[start:], array[:diff]))\n        starts[i] = diff\n      batches.append(batch)\n    yield batches\n\n\ndef main(_):\n  def ratio_estimator(data, local_vars, global_vars):\n    """"""Takes as input a dict of data x, local variable samples z, and\n    global variable samples beta; outputs real values of shape\n    (x.shape[0] + z.shape[0],). In this example, there are no local\n    variables.\n    """"""\n    # data[y] has shape (M,); global_vars[w] has shape (D,)\n    # we concatenate w to each data point y, so input has shape (M, 1 + D)\n    input = tf.concat([\n        tf.reshape(data[y], [FLAGS.M, 1]),\n        tf.tile(tf.reshape(global_vars[w], [1, FLAGS.D]), [FLAGS.M, 1])], 1)\n    hidden = tf.layers.dense(input, 64, activation=tf.nn.relu)\n    output = tf.layers.dense(hidden, 1, activation=None)\n    return output\n\n  ed.set_seed(42)\n\n  # DATA\n  w_true = np.ones(FLAGS.D) * 5.0\n  X_train, y_train = build_toy_dataset(FLAGS.N, w_true)\n  X_test, y_test = build_toy_dataset(FLAGS.N, w_true)\n  data = generator([X_train, y_train], FLAGS.M)\n\n  # MODEL\n  X = tf.placeholder(tf.float32, [FLAGS.M, FLAGS.D])\n  y_ph = tf.placeholder(tf.float32, [FLAGS.M])\n  w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n  y = Normal(loc=ed.dot(X, w), scale=tf.ones(FLAGS.M))\n\n  # INFERENCE\n  qw = Normal(loc=tf.get_variable(""qw/loc"", [FLAGS.D]) + 1.0,\n              scale=tf.nn.softplus(tf.get_variable(""qw/scale"", [FLAGS.D])))\n\n  inference = ed.ImplicitKLqp(\n      {w: qw}, data={y: y_ph},\n      discriminator=ratio_estimator, global_vars={w: qw})\n  inference.initialize(n_iter=5000, n_print=100,\n                       scale={y: float(FLAGS.N) / FLAGS.M})\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n\n  for _ in range(inference.n_iter):\n    X_batch, y_batch = next(data)\n    for _ in range(5):\n      info_dict_d = inference.update(\n          variables=""Disc"", feed_dict={X: X_batch, y_ph: y_batch})\n\n    info_dict = inference.update(\n        variables=""Gen"", feed_dict={X: X_batch, y_ph: y_batch})\n    info_dict[\'loss_d\'] = info_dict_d[\'loss_d\']\n    info_dict[\'t\'] = info_dict[\'t\'] // 6  # say set of 6 updates is 1 iteration\n\n    t = info_dict[\'t\']\n    inference.print_progress(info_dict)\n    if t == 1 or t % inference.n_print == 0:\n      # Check inferred posterior parameters.\n      mean, std = sess.run([qw.mean(), qw.stddev()])\n      print(""\\nInferred mean & std:"")\n      print(mean)\n      print(std)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/bayesian_logistic_regression.py,16,"b'""""""Bayesian logistic regression using Hamiltonian Monte Carlo.\n\nWe visualize the fit.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal, Empirical\n\ntf.flags.DEFINE_integer(""N"", default=40, help=""Number of data points."")\ntf.flags.DEFINE_integer(""D"", default=1, help=""Number of features."")\ntf.flags.DEFINE_integer(""T"", default=5000, help=""Number of posterior samples."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N, noise_std=0.1):\n  D = 1\n  X = np.linspace(-6, 6, num=N)\n  y = np.tanh(X) + np.random.normal(0, noise_std, size=N)\n  y[y < 0.5] = 0\n  y[y >= 0.5] = 1\n  X = (X - 4.0) / 4.0\n  X = X.reshape((N, D))\n  return X, y\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  X_train, y_train = build_toy_dataset(FLAGS.N)\n\n  # MODEL\n  X = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.D])\n  w = Normal(loc=tf.zeros(FLAGS.D), scale=3.0 * tf.ones(FLAGS.D))\n  b = Normal(loc=tf.zeros([]), scale=3.0 * tf.ones([]))\n  y = Bernoulli(logits=ed.dot(X, w) + b)\n\n  # INFERENCE\n  qw = Empirical(params=tf.get_variable(""qw/params"", [FLAGS.T, FLAGS.D]))\n  qb = Empirical(params=tf.get_variable(""qb/params"", [FLAGS.T]))\n\n  inference = ed.HMC({w: qw, b: qb}, data={X: X_train, y: y_train})\n  inference.initialize(n_print=10, step_size=0.6)\n\n  # Alternatively, use variational inference.\n  # qw_loc = tf.get_variable(""qw_loc"", [FLAGS.D])\n  # qw_scale = tf.nn.softplus(tf.get_variable(""qw_scale"", [FLAGS.D]))\n  # qb_loc = tf.get_variable(""qb_loc"", []) + 10.0\n  # qb_scale = tf.nn.softplus(tf.get_variable(""qb_scale"", []))\n\n  # qw = Normal(loc=qw_loc, scale=qw_scale)\n  # qb = Normal(loc=qb_loc, scale=qb_scale)\n\n  # inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n  # inference.initialize(n_print=10, n_iter=600)\n\n  tf.global_variables_initializer().run()\n\n  # Set up figure.\n  fig = plt.figure(figsize=(8, 8), facecolor=\'white\')\n  ax = fig.add_subplot(111, frameon=False)\n  plt.ion()\n  plt.show(block=False)\n\n  # Build samples from inferred posterior.\n  n_samples = 50\n  inputs = np.linspace(-5, 3, num=400, dtype=np.float32).reshape((400, 1))\n  probs = tf.stack([tf.sigmoid(ed.dot(inputs, qw.sample()) + qb.sample())\n                    for _ in range(n_samples)])\n\n  for t in range(inference.n_iter):\n    info_dict = inference.update()\n    inference.print_progress(info_dict)\n\n    if t % inference.n_print == 0:\n      outputs = probs.eval()\n\n      # Plot data and functions\n      plt.cla()\n      ax.plot(X_train[:], y_train, \'bx\')\n      for s in range(n_samples):\n        ax.plot(inputs[:], outputs[s], alpha=0.2)\n\n      ax.set_xlim([-5, 3])\n      ax.set_ylim([-0.5, 1.5])\n      plt.draw()\n      plt.pause(1.0 / 60.0)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/bayesian_nn.py,36,"b'""""""Bayesian neural network using variational inference\n(see, e.g., Blundell et al. (2015); Kucukelbir et al. (2016)).\n\nInspired by autograd\'s Bayesian neural network example.\nThis example prettifies some of the tensor naming for visualization in\nTensorBoard. To view TensorBoard, run `tensorboard --logdir=log`.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/bayesian-neural-network\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\ntf.flags.DEFINE_integer(""N"", default=40, help=""Number of data points."")\ntf.flags.DEFINE_integer(""D"", default=1, help=""Number of features."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N=40, noise_std=0.1):\n  D = 1\n  X = np.concatenate([np.linspace(0, 2, num=N / 2),\n                      np.linspace(6, 8, num=N / 2)])\n  y = np.cos(X) + np.random.normal(0, noise_std, size=N)\n  X = (X - 4.0) / 4.0\n  X = X.reshape((N, D))\n  return X, y\n\n\ndef main(_):\n  def neural_network(X):\n    h = tf.tanh(tf.matmul(X, W_0) + b_0)\n    h = tf.tanh(tf.matmul(h, W_1) + b_1)\n    h = tf.matmul(h, W_2) + b_2\n    return tf.reshape(h, [-1])\n  ed.set_seed(42)\n\n  # DATA\n  X_train, y_train = build_toy_dataset(FLAGS.N)\n\n  # MODEL\n  with tf.name_scope(""model""):\n    W_0 = Normal(loc=tf.zeros([FLAGS.D, 10]), scale=tf.ones([FLAGS.D, 10]),\n                 name=""W_0"")\n    W_1 = Normal(loc=tf.zeros([10, 10]), scale=tf.ones([10, 10]), name=""W_1"")\n    W_2 = Normal(loc=tf.zeros([10, 1]), scale=tf.ones([10, 1]), name=""W_2"")\n    b_0 = Normal(loc=tf.zeros(10), scale=tf.ones(10), name=""b_0"")\n    b_1 = Normal(loc=tf.zeros(10), scale=tf.ones(10), name=""b_1"")\n    b_2 = Normal(loc=tf.zeros(1), scale=tf.ones(1), name=""b_2"")\n\n    X = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.D], name=""X"")\n    y = Normal(loc=neural_network(X), scale=0.1 * tf.ones(FLAGS.N), name=""y"")\n\n  # INFERENCE\n  with tf.variable_scope(""posterior""):\n    with tf.variable_scope(""qW_0""):\n      loc = tf.get_variable(""loc"", [FLAGS.D, 10])\n      scale = tf.nn.softplus(tf.get_variable(""scale"", [FLAGS.D, 10]))\n      qW_0 = Normal(loc=loc, scale=scale)\n    with tf.variable_scope(""qW_1""):\n      loc = tf.get_variable(""loc"", [10, 10])\n      scale = tf.nn.softplus(tf.get_variable(""scale"", [10, 10]))\n      qW_1 = Normal(loc=loc, scale=scale)\n    with tf.variable_scope(""qW_2""):\n      loc = tf.get_variable(""loc"", [10, 1])\n      scale = tf.nn.softplus(tf.get_variable(""scale"", [10, 1]))\n      qW_2 = Normal(loc=loc, scale=scale)\n    with tf.variable_scope(""qb_0""):\n      loc = tf.get_variable(""loc"", [10])\n      scale = tf.nn.softplus(tf.get_variable(""scale"", [10]))\n      qb_0 = Normal(loc=loc, scale=scale)\n    with tf.variable_scope(""qb_1""):\n      loc = tf.get_variable(""loc"", [10])\n      scale = tf.nn.softplus(tf.get_variable(""scale"", [10]))\n      qb_1 = Normal(loc=loc, scale=scale)\n    with tf.variable_scope(""qb_2""):\n      loc = tf.get_variable(""loc"", [1])\n      scale = tf.nn.softplus(tf.get_variable(""scale"", [1]))\n      qb_2 = Normal(loc=loc, scale=scale)\n\n  inference = ed.KLqp({W_0: qW_0, b_0: qb_0,\n                       W_1: qW_1, b_1: qb_1,\n                       W_2: qW_2, b_2: qb_2}, data={X: X_train, y: y_train})\n  inference.run(logdir=\'log\')\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/beta_bernoulli.py,4,"b'""""""A simple coin flipping example. Inspired by Stan\'s toy example.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Beta, Empirical\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n  # MODEL\n  p = Beta(1.0, 1.0)\n  x = Bernoulli(probs=p, sample_shape=10)\n\n  # INFERENCE\n  qp = Empirical(params=tf.get_variable(\n      ""qp/params"", [1000], initializer=tf.constant_initializer(0.5)))\n\n  proposal_p = Beta(3.0, 9.0)\n\n  inference = ed.MetropolisHastings({p: qp}, {p: proposal_p}, data={x: x_data})\n  inference.run()\n\n  # CRITICISM\n  # exact posterior has mean 0.25 and std 0.12\n  sess = ed.get_session()\n  mean, stddev = sess.run([qp.mean(), qp.stddev()])\n  print(""Inferred posterior mean:"")\n  print(mean)\n  print(""Inferred posterior stddev:"")\n  print(stddev)\n\n  x_post = ed.copy(x, {p: qp})\n  tx_rep, tx = ed.ppc(\n      lambda xs, zs: tf.reduce_mean(tf.cast(xs[x_post], tf.float32)),\n      data={x_post: x_data})\n  ed.ppc_stat_hist_plot(\n      tx[0], tx_rep, stat_name=r\'$T \\equiv$mean\', bins=10)\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/beta_bernoulli_conjugate.py,2,"b'""""""A simple coin flipping example that exploits conjugacy.\n\nInspired by Stan\'s toy example.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Beta\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n  # MODEL\n  p = Beta(1.0, 1.0)\n  x = Bernoulli(probs=p, sample_shape=10)\n\n  # COMPLETE CONDITIONAL\n  p_cond = ed.complete_conditional(p)\n\n  sess = ed.get_session()\n\n  print(\'p(probs | x) type:\', p_cond.parameters[\'name\'])\n  param_vals = sess.run({key: val for\n                         key, val in six.iteritems(p_cond.parameters)\n                         if isinstance(val, tf.Tensor)}, {x: x_data})\n  print(\'parameters:\')\n  for key, val in six.iteritems(param_vals):\n    print(\'%s:\\t%.3f\' % (key, val))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/bigan.py,23,"b'""""""Adversarially Learned Inference (Dumoulin et al., 2017), aka\nBidirectional Generative Adversarial Networks (Donahue et al., 2017),\nfor joint learning of generator and inference networks for MNIST.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom observations import mnist\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""out_dir"", default=""/tmp/out"", help="""")\ntf.flags.DEFINE_integer(""M"", default=100, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""d"", default=50, help=""Latent dimension."")\ntf.flags.DEFINE_float(""leak"", default=0.2,\n                      help=""Leak parameter for leakyReLU."")\ntf.flags.DEFINE_integer(""hidden_units"", default=300, help="""")\ntf.flags.DEFINE_float(""encoder_variance"", default=0.01,\n                      help=""Set to 0 for deterministic encoder."")\n\nFLAGS = tf.flags.FLAGS\nif not os.path.exists(FLAGS.out_dir):\n  os.makedirs(FLAGS.out_dir)\n\n\ndef generator(array, batch_size):\n  """"""Generate batch with respect to array\'s first axis.""""""\n  start = 0  # pointer to where we are in iteration\n  while True:\n    stop = start + batch_size\n    diff = stop - array.shape[0]\n    if diff <= 0:\n      batch = array[start:stop]\n      start += batch_size\n    else:\n      batch = np.concatenate((array[start:], array[:diff]))\n      start = diff\n    batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n    batch = np.random.binomial(1, batch)  # binarize images\n    yield batch\n\n\ndef leakyrelu(x, alpha=FLAGS.leak):\n  return tf.maximum(x, alpha * x)\n\n\ndef gen_latent(x, hidden_units):\n  net = tf.layers.dense(x, hidden_units, activation=leakyrelu)\n  net = tf.layers.dense(net, FLAGS.d, activation=None)\n  return (net + np.sqrt(FLAGS.encoder_variance) *\n          np.random.normal(0.0, 1.0, [FLAGS.M, FLAGS.d]))\n\n\ndef gen_data(z, hidden_units):\n  net = tf.layers.dense(z, hidden_units, activation=leakyrelu)\n  net = tf.layers.dense(net, 784, activation=tf.sigmoid)\n  return net\n\n\ndef discriminative_network(x, y):\n  # Discriminator must output probability in logits\n  net = tf.concat([x, y], 1)\n  net = tf.layers.dense(net, FLAGS.hidden_units, activation=leakyrelu)\n  net = tf.layers.dense(net, 1, activation=None)\n  return net\n\n\ndef plot(samples):\n  fig = plt.figure(figsize=(4, 4))\n  plt.title(str(samples))\n  gs = gridspec.GridSpec(4, 4)\n  gs.update(wspace=0.05, hspace=0.05)\n\n  for i, sample in enumerate(samples):\n    ax = plt.subplot(gs[i])\n    plt.axis(\'off\')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_aspect(\'equal\')\n    plt.imshow(sample.reshape(28, 28), cmap=\'Greys_r\')\n\n  return fig\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA. MNIST batches are fed at training time.\n  (x_train, _), (x_test, _) = mnist(FLAGS.data_dir)\n  x_train_generator = generator(x_train, FLAGS.M)\n  x_ph = tf.placeholder(tf.float32, [FLAGS.M, 784])\n  z_ph = tf.placeholder(tf.float32, [FLAGS.M, FLAGS.d])\n\n  # MODEL\n  with tf.variable_scope(""Gen""):\n    xf = gen_data(z_ph, FLAGS.hidden_units)\n    zf = gen_latent(x_ph, FLAGS.hidden_units)\n\n  # INFERENCE:\n  optimizer = tf.train.AdamOptimizer()\n  optimizer_d = tf.train.AdamOptimizer()\n  inference = ed.BiGANInference(\n      latent_vars={zf: z_ph}, data={xf: x_ph},\n      discriminator=discriminative_network)\n\n  inference.initialize(\n      optimizer=optimizer, optimizer_d=optimizer_d, n_iter=100000, n_print=3000)\n\n  sess = ed.get_session()\n  init_op = tf.global_variables_initializer()\n  sess.run(init_op)\n\n  idx = np.random.randint(FLAGS.M, size=16)\n  i = 0\n  for t in range(inference.n_iter):\n    if t % inference.n_print == 1:\n\n      samples = sess.run(xf, feed_dict={z_ph: z_batch})\n      samples = samples[idx, ]\n      fig = plot(samples)\n      plt.savefig(os.path.join(FLAGS.out_dir, \'{}{}.png\').format(\n          \'Generated\', str(i).zfill(3)), bbox_inches=\'tight\')\n      plt.close(fig)\n\n      fig = plot(x_batch[idx, ])\n      plt.savefig(os.path.join(FLAGS.out_dir, \'{}{}.png\').format(\n          \'Base\', str(i).zfill(3)), bbox_inches=\'tight\')\n      plt.close(fig)\n\n      zsam = sess.run(zf, feed_dict={x_ph: x_batch})\n      reconstructions = sess.run(xf, feed_dict={z_ph: zsam})\n      reconstructions = reconstructions[idx, ]\n      fig = plot(reconstructions)\n      plt.savefig(os.path.join(FLAGS.out_dir, \'{}{}.png\').format(\n          \'Reconstruct\', str(i).zfill(3)), bbox_inches=\'tight\')\n      plt.close(fig)\n\n      i += 1\n\n    x_batch = next(x_train_generator)\n    z_batch = np.random.normal(0, 1, [FLAGS.M, FLAGS.d])\n\n    info_dict = inference.update(feed_dict={x_ph: x_batch, z_ph: z_batch})\n    inference.print_progress(info_dict)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/cox_process.py,12,"b'""""""A Cox process model for spatial analysis\n(Cox, 1955; Miller et al., 2014).\n\nThe data set is a N x V matrix. There are N NBA players, X =\n{(x_1, ..., x_N)}, where each x_n has a set of V counts. x_{n, v} is\nthe number of attempted basketball shots for the nth NBA player at\nlocation v.\n\nWe model a latent intensity function for each data point. Let K be the\nN x V x V covariance matrix applied to the data set X with fixed\nkernel hyperparameters, where a slice K_n is the V x V covariance\nmatrix over counts for a data point x_n.\n\nFor n = 1, ..., N,\n  p(f_n) = N(f_n | 0, K_n),\n  p(x_n | f_n) = \\prod_{v=1}^V p(x_{n,v} | f_{n,v}),\n    where p(x_{n,v} | f_{n, v}) = Poisson(x_{n,v} | exp(f_{n,v})).\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import MultivariateNormalTriL, Normal, Poisson\nfrom edward.util import rbf\nfrom scipy.stats import multivariate_normal, poisson\n\ntf.flags.DEFINE_integer(""N"", default=308, help=""Number of NBA players."")\ntf.flags.DEFINE_integer(""V"", default=2, help=""Number of shot locations."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N, V):\n  """"""A simulator mimicking the data set from 2015-2016 NBA season with\n  308 NBA players and ~150,000 shots.""""""\n  L = np.tril(np.random.normal(2.5, 0.1, size=[V, V]))\n  K = np.matmul(L, L.T)\n  x = np.zeros([N, V])\n  for n in range(N):\n    f_n = multivariate_normal.rvs(cov=K, size=1)\n    for v in range(V):\n      x[n, v] = poisson.rvs(mu=np.exp(f_n[v]), size=1)\n\n  return x\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_data = build_toy_dataset(FLAGS.N, FLAGS.V)\n\n  # MODEL\n  x_ph = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.V])\n\n  # Form (N, V, V) covariance, one matrix per data point.\n  K = tf.stack([rbf(tf.reshape(xn, [FLAGS.V, 1])) + tf.diag([1e-6, 1e-6])\n                for xn in tf.unstack(x_ph)])\n  f = MultivariateNormalTriL(loc=tf.zeros([FLAGS.N, FLAGS.V]),\n                             scale_tril=tf.cholesky(K))\n  x = Poisson(rate=tf.exp(f))\n\n  # INFERENCE\n  qf = Normal(\n      loc=tf.get_variable(""qf/loc"", [FLAGS.N, FLAGS.V]),\n      scale=tf.nn.softplus(tf.get_variable(""qf/scale"", [FLAGS.N, FLAGS.V])))\n\n  inference = ed.KLqp({f: qf}, data={x: x_data, x_ph: x_data})\n  inference.run(n_iter=5000)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/deep_exponential_family.py,30,"b'# -*- coding: utf-8 -*-\n""""""Sparse Gamma deep exponential family (Ranganath et al., 2015). We\napply it as a topic model on the collection of NIPS 2011 conference\npapers.\n\nThe loss function can sometimes erroneously output a negative value or\nNaN. This happens when the samples from the variational approximation\nare numerically zero, which causes Gamma log probs to output inf.\n\nWith default settings (in particular, with log normal variational\napproximation), it takes ~62s per epoch on a Titan X (Pascal).\nFollowing results are on epoch 12.\n\n10000/10000 [100%] \xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88 Elapsed: 62s\nNegative log-likelihood <= -1060649.607\nPerplexity <= 0.205\nTopic 0: let distribution set strategy distributions given learning\n    information use property\nTopic 1: functions problem risk function submodular cut level\n    clustering sets performance\nTopic 2: action value learning regret reward actions algorithm optimal\n    state return\nTopic 3: posterior stochastic approach information based using prior\n    mean divergence since\nTopic 4: player inference game propagation experts static query expert\n    base variables\nTopic 5: algorithm set loss weak algorithms optimal submodular online\n    cost setting\nTopic 6: sparse sparsity norm solution learning penalty greedy\n    structure wise regularization\nTopic 7: learning training linear kernel using coding accuracy\n    performance dataset based\nTopic 8: object categories image features examples classes images\n    class objects visual\nTopic 9: data manifold matrix points dimensional point low linear\n    gradient optimization\n\nA Gamma variational approximation produces worse results, which is\nlikely due to the high variance in stochastic gradients. It takes ~2\nminutes per epoch on a Titan X (Pascal). Following results are on\nepoch 12.\n\nNegative log-likelihood <= 3738025.615\nPerplexity <= 266.623\nTopic 0: reasons posterior tion using similar tools university input\n    computed refers\nTopic 1: expected since much related rate defined optimization vector\n    thus neurons\nTopic 2: large linear given table shown true drop classification\n    constraints current\nTopic 3: proposed processing estimated better values gaussian form\n    test true setting\nTopic 4: see methods local several rate processing general vector\n    enables section\nTopic 5: thus case methods image dataset models different instead new\n    respectively\nTopic 6: based consider samples step object see kernel since problem\n    training\nTopic 7: approaches linear computing show gaussian data expected\n    analysis well proof\nTopic 8: fig point kernel bayesian solution applications results\n    follows regression computer\nTopic 9: conference optimization training pages maximum learning\n    dataset performance state inference\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom datetime import datetime\nfrom edward.models import Gamma, Poisson, Normal, PointMass, \\\n    TransformedDistribution\nfrom edward.util import Progbar\nfrom observations import nips\n\ntf.flags.DEFINE_string(""data_dir"", default=""~/data"", help="""")\ntf.flags.DEFINE_string(""logdir"", default=""~/log/def/"", help="""")\ntf.flags.DEFINE_list(""K"", default=[100, 30, 15],\n                     help=""Number of components per layer."")\ntf.flags.DEFINE_string(""q"", default=""lognormal"",\n                       help=""Choice of q; \'lognormal\' or \'gamma\'."")\ntf.flags.DEFINE_float(""shape"", default=0.1, help=""Gamma shape parameter."")\ntf.flags.DEFINE_float(""lr"", default=1e-4, help=""Learning rate step-size."")\n\nFLAGS = tf.flags.FLAGS\nFLAGS.data_dir = os.path.expanduser(FLAGS.data_dir)\nFLAGS.logdir = os.path.expanduser(FLAGS.logdir)\ntimestamp = datetime.strftime(datetime.utcnow(), ""%Y%m%d_%H%M%S"")\nFLAGS.logdir += timestamp + \'_\' + \'_\'.join([str(ks) for ks in FLAGS.K]) + \\\n    \'_q_\' + str(FLAGS.q) + \'_lr_\' + str(FLAGS.lr)\n\n\ndef pointmass_q(shape, name=None):\n  with tf.variable_scope(name, default_name=""pointmass_q""):\n    min_mean = 1e-3\n    mean = tf.get_variable(""mean"", shape)\n    rv = PointMass(tf.maximum(tf.nn.softplus(mean), min_mean))\n    return rv\n\n\ndef gamma_q(shape, name=None):\n  # Parameterize Gamma q\'s via shape and scale, with softplus unconstraints.\n  with tf.variable_scope(name, default_name=""gamma_q""):\n    min_shape = 1e-3\n    min_scale = 1e-5\n    shape = tf.get_variable(\n        ""shape"", shape,\n        initializer=tf.random_normal_initializer(mean=0.5, stddev=0.1))\n    scale = tf.get_variable(\n        ""scale"", shape, initializer=tf.random_normal_initializer(stddev=0.1))\n    rv = Gamma(tf.maximum(tf.nn.softplus(shape), min_shape),\n               tf.maximum(1.0 / tf.nn.softplus(scale), 1.0 / min_scale))\n    return rv\n\n\ndef lognormal_q(shape, name=None):\n  with tf.variable_scope(name, default_name=""lognormal_q""):\n    min_scale = 1e-5\n    loc = tf.get_variable(""loc"", shape)\n    scale = tf.get_variable(\n        ""scale"", shape, initializer=tf.random_normal_initializer(stddev=0.1))\n    rv = TransformedDistribution(\n        distribution=Normal(loc, tf.maximum(tf.nn.softplus(scale), min_scale)),\n        bijector=tf.contrib.distributions.bijectors.Exp())\n    return rv\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_train, metadata = nips(FLAGS.data_dir)\n  documents = metadata[\'columns\']\n  words = metadata[\'rows\']\n\n  # Subset to documents in 2011 and words appearing in at least two\n  # documents and have a total word count of at least 10.\n  doc_idx = [i for i, document in enumerate(documents)\n             if document.startswith(\'2011\')]\n  documents = [documents[doc] for doc in doc_idx]\n  x_train = x_train[:, doc_idx]\n  word_idx = np.logical_and(np.sum(x_train != 0, 1) >= 2,\n                            np.sum(x_train, 1) >= 10)\n  words = [word for word, idx in zip(words, word_idx) if idx]\n  x_train = x_train[word_idx, :]\n  x_train = x_train.T\n\n  N = x_train.shape[0]  # number of documents\n  D = x_train.shape[1]  # vocabulary size\n\n  # MODEL\n  W2 = Gamma(0.1, 0.3, sample_shape=[FLAGS.K[2], FLAGS.K[1]])\n  W1 = Gamma(0.1, 0.3, sample_shape=[FLAGS.K[1], FLAGS.K[0]])\n  W0 = Gamma(0.1, 0.3, sample_shape=[FLAGS.K[0], D])\n\n  z3 = Gamma(0.1, 0.1, sample_shape=[N, FLAGS.K[2]])\n  z2 = Gamma(FLAGS.shape, FLAGS.shape / tf.matmul(z3, W2))\n  z1 = Gamma(FLAGS.shape, FLAGS.shape / tf.matmul(z2, W1))\n  x = Poisson(tf.matmul(z1, W0))\n\n  # INFERENCE\n  qW2 = pointmass_q(W2.shape)\n  qW1 = pointmass_q(W1.shape)\n  qW0 = pointmass_q(W0.shape)\n  if FLAGS.q == \'gamma\':\n    qz3 = gamma_q(z3.shape)\n    qz2 = gamma_q(z2.shape)\n    qz1 = gamma_q(z1.shape)\n  else:\n    qz3 = lognormal_q(z3.shape)\n    qz2 = lognormal_q(z2.shape)\n    qz1 = lognormal_q(z1.shape)\n\n  # We apply variational EM with E-step over local variables\n  # and M-step to point estimate the global weight matrices.\n  inference_e = ed.KLqp({z1: qz1, z2: qz2, z3: qz3},\n                        data={x: x_train, W0: qW0, W1: qW1, W2: qW2})\n  inference_m = ed.MAP({W0: qW0, W1: qW1, W2: qW2},\n                       data={x: x_train, z1: qz1, z2: qz2, z3: qz3})\n\n  optimizer_e = tf.train.RMSPropOptimizer(FLAGS.lr)\n  optimizer_m = tf.train.RMSPropOptimizer(FLAGS.lr)\n  kwargs = {\'optimizer\': optimizer_e,\n            \'n_print\': 100,\n            \'logdir\': FLAGS.logdir,\n            \'log_timestamp\': False}\n  if FLAGS.q == \'gamma\':\n    kwargs[\'n_samples\'] = 30\n  inference_e.initialize(**kwargs)\n  inference_m.initialize(optimizer=optimizer_m)\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n\n  n_epoch = 20\n  n_iter_per_epoch = 10000\n  for epoch in range(n_epoch):\n    print(""Epoch {}"".format(epoch))\n    nll = 0.0\n\n    pbar = Progbar(n_iter_per_epoch)\n    for t in range(1, n_iter_per_epoch + 1):\n      pbar.update(t)\n      info_dict_e = inference_e.update()\n      info_dict_m = inference_m.update()\n      nll += info_dict_e[\'loss\']\n\n    # Compute perplexity averaged over a number of training iterations.\n    # The model\'s negative log-likelihood of data is upper bounded by\n    # the variational objective.\n    nll /= n_iter_per_epoch\n    perplexity = np.exp(nll / np.sum(x_train))\n    print(""Negative log-likelihood <= {:0.3f}"".format(nll))\n    print(""Perplexity <= {:0.3f}"".format(perplexity))\n\n    # Print top 10 words for first 10 topics.\n    qW0_vals = sess.run(qW0)\n    for k in range(10):\n      top_words_idx = qW0_vals[k, :].argsort()[-10:][::-1]\n      top_words = "" "".join([words[i] for i in top_words_idx])\n      print(""Topic {}: {}"".format(k, top_words))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/dirichlet_categorical.py,7,"b'""""""Dirichlet-Categorical with variational inference.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Categorical, Dirichlet\n\ntf.flags.DEFINE_integer(""N"", default=1000, help="""")\ntf.flags.DEFINE_integer(""K"", default=4, help="""")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef main(_):\n  # DATA\n  pi_true = np.random.dirichlet(np.array([20.0, 30.0, 10.0, 10.0]))\n  z_data = np.array([np.random.choice(FLAGS.K, 1, p=pi_true)[0]\n                     for n in range(FLAGS.N)])\n  print(""pi: {}"".format(pi_true))\n\n  # MODEL\n  pi = Dirichlet(tf.ones(4))\n  z = Categorical(probs=pi, sample_shape=FLAGS.N)\n\n  # INFERENCE\n  qpi = Dirichlet(tf.nn.softplus(\n      tf.get_variable(""qpi/concentration"", [FLAGS.K])))\n\n  inference = ed.KLqp({pi: qpi}, data={z: z_data})\n  inference.run(n_iter=1500, n_samples=30)\n\n  sess = ed.get_session()\n  print(""Inferred pi: {}"".format(sess.run(qpi.mean())))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/factor_analysis.py,16,"b'""""""Logistic factor analysis on MNIST. Using Monte Carlo EM, with HMC\nfor the E-step and MAP for the M-step. We fit to just one data\npoint in MNIST.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport os\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Empirical, Normal\nfrom observations import mnist\nfrom scipy.misc import imsave\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""out_dir"", default=""/tmp/out"", help="""")\ntf.flags.DEFINE_integer(""N"", default=1, help=""Number of data points."")\ntf.flags.DEFINE_integer(""d"", default=10, help=""Number of latent dimensions."")\ntf.flags.DEFINE_integer(""n_iter_per_epoch"", default=5000, help="""")\ntf.flags.DEFINE_integer(""n_epoch"", default=20, help="""")\n\nFLAGS = tf.flags.FLAGS\nif not os.path.exists(FLAGS.out_dir):\n  os.makedirs(FLAGS.out_dir)\n\nFLAGS = tf.flags.FLAGS\n\n\ndef generative_network(z):\n  """"""Generative network to parameterize generative model. It takes\n  latent variables as input and outputs the likelihood parameters.\n\n  logits = neural_network(z)\n  """"""\n  net = tf.layers.dense(z, 28 * 28, activation=None)\n  net = tf.reshape(net, [FLAGS.N, -1])\n  return net\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  (x_train, _), (x_test, _) = mnist(FLAGS.data_dir)\n  x_train = x_train[:FLAGS.N]\n\n  # MODEL\n  z = Normal(loc=tf.zeros([FLAGS.N, FLAGS.d]),\n             scale=tf.ones([FLAGS.N, FLAGS.d]))\n  logits = generative_network(z)\n  x = Bernoulli(logits=logits)\n\n  # INFERENCE\n  T = FLAGS.n_iter_per_epoch * FLAGS.n_epoch\n  qz = Empirical(params=tf.get_variable(""qz/params"", [T, FLAGS.N, FLAGS.d]))\n\n  inference_e = ed.HMC({z: qz}, data={x: x_train})\n  inference_e.initialize()\n\n  inference_m = ed.MAP(data={x: x_train, z: qz.params[inference_e.t]})\n  optimizer = tf.train.AdamOptimizer(0.01, epsilon=1.0)\n  inference_m.initialize(optimizer=optimizer)\n\n  tf.global_variables_initializer().run()\n\n  for _ in range(FLAGS.n_epoch - 1):\n    avg_loss = 0.0\n    for _ in range(FLAGS.n_iter_per_epoch):\n      info_dict_e = inference_e.update()\n      info_dict_m = inference_m.update()\n      avg_loss += info_dict_m[\'loss\']\n      inference_e.print_progress(info_dict_e)\n\n    # Print a lower bound to the average marginal likelihood for an\n    # image.\n    avg_loss = avg_loss / FLAGS.n_iter_per_epoch\n    avg_loss = avg_loss / FLAGS.N\n    print(""\\nlog p(x) >= {:0.3f}"".format(avg_loss))\n\n    # Prior predictive check.\n    images = x.eval()\n    for m in range(FLAGS.N):\n      imsave(os.path.join(FLAGS.out_dir, \'%d.png\') % m,\n             images[m].reshape(28, 28))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/gan_synthetic_data.py,21,"b'""""""Generative adversarial network for toy Gaussian data\n(Goodfellow et al., 2014).\n\nInspired by a blog post by Eric Jang.\n\nNote there are several common failure modes, such as\n(1) saturation of either discriminative or generative network;\n(2) the generator running into a local optima that produces a Gaussian\nsomewhere around -1 rather than at the true data; and\n(3) mode collapse around the true Gaussian, where the variance is\nseverely underestimated.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/gan\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom scipy.stats import norm\n\ntf.flags.DEFINE_integer(""M"", default=12, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""H"", default=4, help=""Number of hidden units."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef next_batch(N):\n  samples = np.random.normal(4.0, 0.5, N)\n  samples.sort()\n  return samples\n\n\ndef generative_network(eps):\n  net = tf.layers.dense(eps, FLAGS.H, activation=tf.nn.relu)\n  net = tf.layers.dense(net, 1, activation=None)\n  return net\n\n\ndef discriminative_network(x):\n  """"""Outputs probability in logits.""""""\n  net = tf.layers.dense(x, FLAGS.H * 2, activation=tf.tanh)\n  net = tf.layers.dense(net, FLAGS.H * 2, activation=tf.tanh)\n  net = tf.layers.dense(net, FLAGS.H * 2, activation=tf.tanh)\n  net = tf.layers.dense(net, 1, activation=None)\n  return net\n\n\ndef get_samples(x_ph, num_points=10000, num_bins=100):\n  """"""Return a tuple (db, pd, pg), where\n  + db is the discriminator\'s decision boundary;\n  + pd is a histogram of samples from the data distribution;\n  + pg is a histogram of samples from the generative model.\n  """"""\n  sess = ed.get_session()\n  bins = np.linspace(-8, 8, num_bins)\n\n  # Decision boundary\n  with tf.variable_scope(""Disc"", reuse=True):\n    p_true = tf.sigmoid(discriminative_network(x_ph))\n\n  xs = np.linspace(-8, 8, num_points)\n  db = np.zeros((num_points, 1))\n  for i in range(num_points // FLAGS.M):\n    db[FLAGS.M * i:FLAGS.M * (i + 1)] = sess.run(\n        p_true, {x_ph: np.reshape(xs[FLAGS.M * i:FLAGS.M * (i + 1)],\n                                  (FLAGS.M, 1))})\n\n  # Data samples\n  d = next_batch(num_points)\n  pd, _ = np.histogram(d, bins=bins, density=True)\n\n  # Generated samples\n  eps_ph = tf.placeholder(tf.float32, [FLAGS.M, 1])\n  with tf.variable_scope(""Gen"", reuse=True):\n    G = generative_network(eps_ph)\n\n  epss = np.linspace(-8, 8, num_points)\n  g = np.zeros((num_points, 1))\n  for i in range(num_points // FLAGS.M):\n    g[FLAGS.M * i:FLAGS.M * (i + 1)] = sess.run(\n        G, {eps_ph: np.reshape(epss[FLAGS.M * i:FLAGS.M * (i + 1)],\n                               (FLAGS.M, 1))})\n  pg, _ = np.histogram(g, bins=bins, density=True)\n\n  return db, pd, pg\n\n\ndef main(_):\n  sns.set(color_codes=True)\n  ed.set_seed(42)\n\n  # DATA. We use a placeholder to represent a minibatch. During\n  # inference, we generate data on the fly and feed `x_ph`.\n  x_ph = tf.placeholder(tf.float32, [FLAGS.M, 1])\n\n  # MODEL\n  with tf.variable_scope(""Gen""):\n    eps = tf.linspace(-8.0, 8.0, FLAGS.M) + 0.01 * tf.random_normal([FLAGS.M])\n    eps = tf.reshape(eps, [-1, 1])\n    x = generative_network(eps)\n\n  # INFERENCE\n  optimizer = tf.train.GradientDescentOptimizer(0.03)\n  optimizer_d = tf.train.GradientDescentOptimizer(0.03)\n\n  inference = ed.GANInference(\n      data={x: x_ph}, discriminator=discriminative_network)\n  inference.initialize(\n      optimizer=optimizer, optimizer_d=optimizer_d)\n  tf.global_variables_initializer().run()\n\n  for _ in range(inference.n_iter):\n    x_data = next_batch(FLAGS.M).reshape([FLAGS.M, 1])\n    info_dict = inference.update(feed_dict={x_ph: x_data})\n    inference.print_progress(info_dict)\n\n  # CRITICISM\n  db, pd, pg = get_samples(x_ph)\n  db_x = np.linspace(-8, 8, len(db))\n  p_x = np.linspace(-8, 8, len(pd))\n  f, ax = plt.subplots(1)\n  ax.plot(db_x, db, label=""Decision boundary"")\n  ax.set_ylim(0, 1)\n  plt.plot(p_x, pd, label=""Real data"")\n  plt.plot(p_x, pg, label=""Generated data"")\n  plt.title(""1D Generative Adversarial Network"")\n  plt.xlabel(""Data values"")\n  plt.ylabel(""Probability density"")\n  plt.legend()\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/gan_wasserstein.py,17,"b'""""""Wasserstein generative adversarial network for MNIST (Arjovsky et\nal., 2017). It modifies GANs (Goodfellow et al., 2014) to optimize\nunder the Wasserstein distance.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/gan\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom edward.models import Uniform\nfrom observations import mnist\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""out_dir"", default=""/tmp/out"", help="""")\ntf.flags.DEFINE_integer(""M"", default=128, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""d"", default=10, help=""Latent dimension."")\n\nFLAGS = tf.flags.FLAGS\nif not os.path.exists(FLAGS.out_dir):\n  os.makedirs(FLAGS.out_dir)\n\n\ndef generator(array, batch_size):\n  """"""Generate batch with respect to array\'s first axis.""""""\n  start = 0  # pointer to where we are in iteration\n  while True:\n    stop = start + batch_size\n    diff = stop - array.shape[0]\n    if diff <= 0:\n      batch = array[start:stop]\n      start += batch_size\n    else:\n      batch = np.concatenate((array[start:], array[:diff]))\n      start = diff\n    batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n    batch = np.random.binomial(1, batch)  # binarize images\n    yield batch\n\n\ndef generative_network(eps):\n  net = tf.layers.dense(eps, 128, activation=tf.nn.relu)\n  net = tf.layers.dense(net, 784, activation=tf.sigmoid)\n  return net\n\n\ndef discriminative_network(x):\n  net = tf.layers.dense(x, 128, activation=tf.nn.relu)\n  net = tf.layers.dense(net, 1, activation=None)\n  return net\n\n\ndef plot(samples):\n  fig = plt.figure(figsize=(4, 4))\n  gs = gridspec.GridSpec(4, 4)\n  gs.update(wspace=0.05, hspace=0.05)\n\n  for i, sample in enumerate(samples):\n    ax = plt.subplot(gs[i])\n    plt.axis(\'off\')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_aspect(\'equal\')\n    plt.imshow(sample.reshape(28, 28), cmap=\'Greys_r\')\n\n  return fig\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA. MNIST batches are fed at training time.\n  (x_train, _), (x_test, _) = mnist(FLAGS.data_dir)\n  x_train_generator = generator(x_train, FLAGS.M)\n  x_ph = tf.placeholder(tf.float32, [FLAGS.M, 784])\n\n  # MODEL\n  with tf.variable_scope(""Gen""):\n    eps = Uniform(low=tf.zeros([FLAGS.M, FLAGS.d]) - 1.0,\n                  high=tf.ones([FLAGS.M, FLAGS.d]))\n    x = generative_network(eps)\n\n  # INFERENCE\n  optimizer = tf.train.RMSPropOptimizer(learning_rate=5e-5)\n  optimizer_d = tf.train.RMSPropOptimizer(learning_rate=5e-5)\n\n  inference = ed.WGANInference(\n      data={x: x_ph}, discriminator=discriminative_network)\n  inference.initialize(\n      optimizer=optimizer, optimizer_d=optimizer_d,\n      n_iter=15000, n_print=1000, clip=0.01, penalty=None)\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n\n  idx = np.random.randint(FLAGS.M, size=16)\n  i = 0\n  for t in range(inference.n_iter):\n    if t % inference.n_print == 0:\n      samples = sess.run(x)\n      samples = samples[idx, ]\n\n      fig = plot(samples)\n      plt.savefig(os.path.join(FLAGS.out_dir, \'{}.png\').format(\n          str(i).zfill(3)), bbox_inches=\'tight\')\n      plt.close(fig)\n      i += 1\n\n    x_batch = next(x_train_generator)\n    for _ in range(5):\n      inference.update(feed_dict={x_ph: x_batch}, variables=""Disc"")\n\n    info_dict = inference.update(feed_dict={x_ph: x_batch}, variables=""Gen"")\n    # note: not printing discriminative objective; `info_dict` above\n    # does not store it since updating only ""Gen""\n    info_dict[\'t\'] = info_dict[\'t\'] // 6  # say set of 6 updates is 1 iteration\n    inference.print_progress(info_dict)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/gan_wasserstein_synthetic.py,21,"b'""""""Wasserstein generative adversarial network for toy Gaussian data\n(Arjovsky et al., 2017). A gradient penalty is used to approximate the\n1-Lipschitz functional family in the Wasserstein distance (Gulrajani\net al., 2017).\n\nInspired by a blog post by Eric Jang.\n\nNote there are several common failure modes, such as\n(1) saturation of either discriminative or generative network;\n(2) mode collapse around the true Gaussian, where the variance is\nseverely underestimated.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/gan\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom scipy.stats import norm\n\ntf.flags.DEFINE_integer(""M"", default=12, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""H"", default=4, help=""Number of hidden units."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef next_batch(N):\n  samples = np.random.normal(4.0, 0.5, N)\n  samples.sort()\n  return samples\n\n\ndef generative_network(eps):\n  net = tf.layers.dense(eps, FLAGS.H, activation=tf.nn.relu)\n  net = tf.layers.dense(net, 1, activation=None)\n  return net\n\n\ndef discriminative_network(x):\n  """"""Outputs probability in logits.""""""\n  net = tf.layers.dense(x, FLAGS.H * 2, activation=tf.tanh)\n  net = tf.layers.dense(net, FLAGS.H * 2, activation=tf.tanh)\n  net = tf.layers.dense(net, FLAGS.H * 2, activation=tf.tanh)\n  net = tf.layers.dense(net, 1, activation=None)\n  return net\n\n\ndef get_samples(x_ph, num_points=10000, num_bins=100):\n  """"""Return a tuple (db, pd, pg), where\n  + db is the discriminator\'s decision boundary;\n  + pd is a histogram of samples from the data distribution;\n  + pg is a histogram of samples from the generative model.\n  """"""\n  sess = ed.get_session()\n  bins = np.linspace(-8, 8, num_bins)\n\n  # Decision boundary\n  with tf.variable_scope(""Disc"", reuse=True):\n    p_true = tf.sigmoid(discriminative_network(x_ph))\n\n  xs = np.linspace(-8, 8, num_points)\n  db = np.zeros((num_points, 1))\n  for i in range(num_points // FLAGS.M):\n    db[FLAGS.M * i:FLAGS.M * (i + 1)] = sess.run(\n        p_true, {x_ph: np.reshape(xs[FLAGS.M * i:FLAGS.M * (i + 1)],\n                                  (FLAGS.M, 1))})\n\n  # Data samples\n  d = next_batch(num_points)\n  pd, _ = np.histogram(d, bins=bins, density=True)\n\n  # Generated samples\n  eps_ph = tf.placeholder(tf.float32, [FLAGS.M, 1])\n  with tf.variable_scope(""Gen"", reuse=True):\n    G = generative_network(eps_ph)\n\n  epss = np.linspace(-8, 8, num_points)\n  g = np.zeros((num_points, 1))\n  for i in range(num_points // FLAGS.M):\n    g[FLAGS.M * i:FLAGS.M * (i + 1)] = sess.run(\n        G, {eps_ph: np.reshape(epss[FLAGS.M * i:FLAGS.M * (i + 1)],\n                               (FLAGS.M, 1))})\n  pg, _ = np.histogram(g, bins=bins, density=True)\n\n  return db, pd, pg\n\n\ndef main(_):\n  sns.set(color_codes=True)\n  ed.set_seed(42)\n\n  # DATA. We use a placeholder to represent a minibatch. During\n  # inference, we generate data on the fly and feed `x_ph`.\n  x_ph = tf.placeholder(tf.float32, [FLAGS.M, 1])\n\n  # MODEL\n  with tf.variable_scope(""Gen""):\n    eps = tf.linspace(-8.0, 8.0, FLAGS.M) + 0.01 * tf.random_normal([FLAGS.M])\n    eps = tf.reshape(eps, [FLAGS.M, 1])\n    x = generative_network(eps)\n\n  # INFERENCE\n  optimizer = tf.train.GradientDescentOptimizer(0.03)\n  optimizer_d = tf.train.GradientDescentOptimizer(0.03)\n\n  inference = ed.WGANInference(\n      data={x: x_ph}, discriminator=discriminative_network)\n  inference.initialize(\n      optimizer=optimizer, optimizer_d=optimizer_d, penalty=0.1,\n      n_iter=1000)\n  tf.global_variables_initializer().run()\n\n  for _ in range(inference.n_iter):\n    x_data = next_batch(FLAGS.M).reshape([FLAGS.M, 1])\n    for _ in range(5):\n      info_dict_d = inference.update(feed_dict={x_ph: x_data}, variables=""Disc"")\n\n    info_dict = inference.update(feed_dict={x_ph: x_data}, variables=""Gen"")\n    info_dict[\'t\'] = info_dict[\'t\'] // 6  # say set of 6 updates is 1 iteration\n    info_dict[\'loss_d\'] = info_dict_d[\'loss_d\']  # get disc loss from update\n    inference.print_progress(info_dict)\n\n  # CRITICISM\n  db, pd, pg = get_samples(x_ph)\n  db_x = np.linspace(-8, 8, len(db))\n  p_x = np.linspace(-8, 8, len(pd))\n  f, ax = plt.subplots(1)\n  ax.plot(db_x, db, label=""Decision boundary"")\n  ax.set_ylim(0, 1)\n  plt.plot(p_x, pd, label=""Real data"")\n  plt.plot(p_x, pg, label=""Generated data"")\n  plt.title(""1D Generative Adversarial Network"")\n  plt.xlabel(""Data values"")\n  plt.ylabel(""Probability density"")\n  plt.legend()\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/invgamma_normal_mh.py,9,"b'""""""InverseGamma-Normal with Metropolis-Hastings.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import InverseGamma, Normal, Empirical\n\ntf.flags.DEFINE_integer(""N"", default=1000, help=""Number of data points."")\ntf.flags.DEFINE_float(""loc"", default=7.0, help="""")\ntf.flags.DEFINE_float(""scale"", default=0.7, help="""")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef main(_):\n  # Data generation (known mean)\n  xn_data = np.random.normal(FLAGS.loc, FLAGS.scale, FLAGS.N)\n  print(""scale: {}"".format(FLAGS.scale))\n\n  # Prior definition\n  alpha = 0.5\n  beta = 0.7\n\n  # Posterior inference\n  # Probabilistic model\n  ig = InverseGamma(alpha, beta)\n  xn = Normal(FLAGS.loc, tf.sqrt(ig), sample_shape=FLAGS.N)\n\n  # Inference\n  qig = Empirical(params=tf.get_variable(\n      ""qig/params"", [1000], initializer=tf.constant_initializer(0.5)))\n  proposal_ig = InverseGamma(2.0, 2.0)\n  inference = ed.MetropolisHastings({ig: qig},\n                                    {ig: proposal_ig}, data={xn: xn_data})\n  inference.run()\n\n  sess = ed.get_session()\n  print(""Inferred scale: {}"".format(sess.run(tf.sqrt(qig.mean()))))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/irt.py,26,"b'""""""Bayesian Item Response Theory (IRT) Mixed Effects Model\nusing variational inference.\n\nSimulates data and fits y ~ 1 + (1|student) + (1|question)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom edward.models import Normal, Bernoulli\nfrom scipy.special import expit\n\ntf.flags.DEFINE_integer(""n_students"", default=50000, help="""")\ntf.flags.DEFINE_integer(""n_questions"", default=2000, help="""")\ntf.flags.DEFINE_integer(""n_obs"", default=200000, help="""")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(n_students, n_questions, n_obs,\n                      sigma_students=1.0, sigma_questions=1.5, loc=0.0):\n  student_etas = np.random.normal(0.0, sigma_students,\n                                  size=n_students)\n  question_etas = np.random.normal(0.0, sigma_questions,\n                                   size=n_questions)\n\n  student_ids = np.random.choice(range(n_students), n_obs)\n  question_ids = np.random.choice(range(n_questions), n_obs)\n\n  logits = student_etas[student_ids] + question_etas[question_ids] + loc\n  outcomes = np.random.binomial(1, expit(logits), n_obs)\n\n  data = pd.DataFrame({\'question_id\': question_ids,\n                       \'student_id\': student_ids,\n                       \'outcomes\': outcomes})\n\n  return data, student_etas, question_etas\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  data, true_s_etas, true_q_etas = build_toy_dataset(\n      FLAGS.n_students, FLAGS.n_questions, FLAGS.n_obs)\n  obs = data[\'outcomes\'].values\n  student_ids = data[\'student_id\'].values.astype(int)\n  question_ids = data[\'question_id\'].values.astype(int)\n\n  # MODEL\n  lnvar_students = Normal(loc=0.0, scale=1.0)\n  lnvar_questions = Normal(loc=0.0, scale=1.0)\n\n  sigma_students = tf.sqrt(tf.exp(lnvar_students))\n  sigma_questions = tf.sqrt(tf.exp(lnvar_questions))\n\n  overall_mu = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n\n  student_etas = Normal(loc=0.0, scale=sigma_students,\n                        sample_shape=FLAGS.n_students)\n  question_etas = Normal(loc=0.0, scale=sigma_questions,\n                         sample_shape=FLAGS.n_questions)\n\n  observation_logodds = (tf.gather(student_etas, student_ids) +\n                         tf.gather(question_etas, question_ids) +\n                         overall_mu)\n  outcomes = Bernoulli(logits=observation_logodds)\n\n  # INFERENCE\n  qstudents = Normal(\n      loc=tf.get_variable(""qstudents/loc"", [FLAGS.n_students]),\n      scale=tf.nn.softplus(\n          tf.get_variable(""qstudents/scale"", [FLAGS.n_students])))\n  qquestions = Normal(\n      loc=tf.get_variable(""qquestions/loc"", [FLAGS.n_questions]),\n      scale=tf.nn.softplus(\n          tf.get_variable(""qquestions/scale"", [FLAGS.n_questions])))\n  qlnvarstudents = Normal(\n      loc=tf.get_variable(""qlnvarstudents/loc"", []),\n      scale=tf.nn.softplus(\n          tf.get_variable(""qlnvarstudents/scale"", [])))\n  qlnvarquestions = Normal(\n      loc=tf.get_variable(""qlnvarquestions/loc"", []),\n      scale=tf.nn.softplus(\n          tf.get_variable(""qlnvarquestions/scale"", [])))\n  qmu = Normal(\n      loc=tf.get_variable(""qmu/loc"", [1]),\n      scale=tf.nn.softplus(\n          tf.get_variable(""qmu/scale"", [1])))\n\n  latent_vars = {\n      overall_mu: qmu,\n      lnvar_students: qlnvarstudents,\n      lnvar_questions: qlnvarquestions,\n      student_etas: qstudents,\n      question_etas: qquestions\n  }\n  data = {outcomes: obs}\n  inference = ed.KLqp(latent_vars, data)\n  inference.initialize(n_print=2, n_iter=50)\n\n  qstudents_mean = qstudents.mean()\n  qquestions_mean = qquestions.mean()\n\n  tf.global_variables_initializer().run()\n\n  f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n  ax1.set_ylim([-3.0, 3.0])\n  ax2.set_ylim([-3.0, 3.0])\n  ax1.set_xlim([-3.0, 3.0])\n  ax2.set_xlim([-3.0, 3.0])\n\n  for t in range(inference.n_iter):\n    info_dict = inference.update()\n    inference.print_progress(info_dict)\n\n    if t % inference.n_print == 0:\n      # CRITICISM\n      ax1.clear()\n      ax2.clear()\n      ax1.set_ylim([-3.0, 3.0])\n      ax2.set_ylim([-3.0, 3.0])\n      ax1.set_xlim([-3.0, 3.0])\n      ax2.set_xlim([-3.0, 3.0])\n\n      ax1.set_title(\'Student Intercepts\')\n      ax2.set_title(\'Question Intercepts\')\n      ax1.set_xlabel(\'True Student Random Intercepts\')\n      ax1.set_ylabel(\'Estimated Student Random Intercepts\')\n      ax2.set_xlabel(\'True Question Random Intercepts\')\n      ax2.set_ylabel(\'Estimated Question Random Intercepts\')\n\n      ax1.scatter(true_s_etas, qstudents_mean.eval(), s=0.05)\n      ax2.scatter(true_q_etas, qquestions_mean.eval(), s=0.05)\n      plt.draw()\n      plt.pause(2.0 / 60.0)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/iwvi.py,15,"b'""""""A demo of how to develop new inference algorithms in Edward. Here\nwe implement importance-weighted variational inference. We test it on\nlogistic regression.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences import VariationalInference\nfrom edward.models import Bernoulli, Normal, RandomVariable\nfrom edward.util import copy\nfrom scipy.special import expit\n\n\ndef reduce_logmeanexp(input_tensor, axis=None, keep_dims=False):\n  logsumexp = tf.reduce_logsumexp(input_tensor, axis, keep_dims)\n  input_tensor = tf.convert_to_tensor(input_tensor)\n  n = input_tensor.shape.as_list()\n  if axis is None:\n    n = tf.cast(tf.reduce_prod(n), logsumexp.dtype)\n  else:\n    n = tf.cast(tf.reduce_prod(n[axis]), logsumexp.dtype)\n\n  return -tf.log(n) + logsumexp\n\n\nclass IWVI(VariationalInference):\n  """"""Importance-weighted variational inference.\n\n  Uses importance sampling to produce an improved lower bound on the\n  log marginal likelihood. It is the core idea behind\n  importance-weighted autoencoders (Burda et al. (2016)).\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(IWVI, self).__init__(*args, **kwargs)\n\n  def initialize(self, K=5, *args, **kwargs):\n    """"""Initialization.\n\n    Args:\n      K: int. Number of importance samples.\n    """"""\n    self.K = K\n    return super(IWVI, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    """"""Build loss function. Its automatic differentiation\n    is a stochastic gradient of\n\n    $-\\mathbb{E}_{q(z^1; \\lambda), ..., q(z^K; \\lambda)} [\n      \\log 1/K \\sum_{k=1}^K p(x, z^k)/q(z^k; \\lambda) ]$\n\n    based on the reparameterization trick.\n    """"""\n    # Form vector of K log importance weights.\n    log_w = []\n    for k in range(self.K):\n      scope = \'inference_\' + str(id(self)) + \'/\' + str(k)\n      z_sample = {}\n      q_log_prob = 0.0\n      for z, qz in six.iteritems(self.latent_vars):\n        # Copy q(z) to obtain new set of posterior samples.\n        qz_copy = copy(qz, scope=scope)\n        z_sample[z] = qz_copy\n        q_log_prob += tf.reduce_sum(qz_copy.log_prob(qz_copy))\n\n      p_log_prob = 0.0\n      for z in six.iterkeys(self.latent_vars):\n        # Copy p(z), swapping its conditioning set with samples\n        # from variational distribution.\n        z_copy = copy(z, z_sample, scope=scope)\n        p_log_prob += tf.reduce_sum(z_copy.log_prob(z_sample[z]))\n\n      for x, qx in six.iteritems(self.data):\n        if isinstance(x, RandomVariable):\n          # Copy p(x | z), swapping its conditioning set with samples\n          # from variational distribution.\n          x_copy = copy(x, z_sample, scope=scope)\n          p_log_prob += tf.reduce_sum(x_copy.log_prob(qx))\n\n      log_w += [p_log_prob - q_log_prob]\n\n    loss = -reduce_logmeanexp(log_w)\n    grads = tf.gradients(loss, [v._ref() for v in var_list])\n    grads_and_vars = list(zip(grads, var_list))\n    return loss, grads_and_vars\n\n\ndef main(_):\n  ed.set_seed(42)\n  N = 5000  # number of data points\n  D = 10  # number of features\n\n  # DATA\n  w_true = np.random.randn(D)\n  X_data = np.random.randn(N, D)\n  p = expit(np.dot(X_data, w_true))\n  y_data = np.array([np.random.binomial(1, i) for i in p])\n\n  # MODEL\n  X = tf.placeholder(tf.float32, [N, D])\n  w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n  y = Bernoulli(logits=ed.dot(X, w))\n\n  # INFERENCE\n  qw = Normal(loc=tf.get_variable(""qw/loc"", [D]),\n              scale=tf.nn.softplus(tf.get_variable(""qw/scale"", [D])))\n\n  inference = IWVI({w: qw}, data={X: X_data, y: y_data})\n  inference.run(K=5, n_iter=1000)\n\n  # CRITICISM\n  print(""Mean squared error in true values to inferred posterior mean:"")\n  print(tf.reduce_mean(tf.square(w_true - qw.mean())).eval())\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/lstm.py,47,"b'""""""LSTM language model on text8.\n\nDefault hyperparameters achieve ~78.4 NLL at epoch 50, ~76.1423 NLL at\nepoch 200; ~13s/epoch on Titan X (Pascal).\n\nSamples after 200 epochs:\n```\ne the classmaker was cut apart rome the charts sometimes known a\nhemical place baining examples of equipment accepted manner clas\nuetean meeting sought to exist as this waiting an excerpt for of\nerally enjoyed a film writer of unto one two volunteer humphrey\ny captured by the saughton river goodness where stones were nota\n```\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport os\nimport string\nimport tensorflow as tf\n\nfrom datetime import datetime\nfrom edward.models import Categorical\nfrom edward.util import Progbar\nfrom observations import text8\n\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""log_dir"", default=""/tmp/log"", help="""")\ntf.flags.DEFINE_integer(""n_epoch"", default=200, help="""")\ntf.flags.DEFINE_integer(""batch_size"", default=128, help="""")\ntf.flags.DEFINE_integer(""hidden_size"", default=512, help="""")\ntf.flags.DEFINE_integer(""timesteps"", default=64, help="""")\ntf.flags.DEFINE_float(""lr"", default=5e-3, help="""")\n\nFLAGS = tf.flags.FLAGS\n\ntimestamp = datetime.strftime(datetime.utcnow(), ""%Y%m%d_%H%M%S"")\nhyperparam_str = \'_\'.join([\n    var + \'_\' + str(eval(var)).replace(\'.\', \'_\')\n    for var in [\'FLAGS.batch_size\', \'FLAGS.hidden_size\',\n                \'FLAGS.timesteps\', \'FLAGS.lr\']])\nFLAGS.log_dir = os.path.join(FLAGS.log_dir, timestamp + \'_\' + hyperparam_str)\nif not os.path.exists(FLAGS.log_dir):\n  os.makedirs(FLAGS.log_dir)\n\n\ndef lstm_cell(x, h, c, name=None, reuse=False):\n  """"""LSTM returning hidden state and content cell at a specific timestep.""""""\n  nin = x.shape[-1].value\n  nout = h.shape[-1].value\n  with tf.variable_scope(name, default_name=""lstm"",\n                         values=[x, h, c], reuse=reuse):\n    wx = tf.get_variable(""kernel/input"", [nin, nout * 4],\n                         dtype=tf.float32,\n                         initializer=tf.orthogonal_initializer(1.0))\n    wh = tf.get_variable(""kernel/hidden"", [nout, nout * 4],\n                         dtype=tf.float32,\n                         initializer=tf.orthogonal_initializer(1.0))\n    b = tf.get_variable(""bias"", [nout * 4],\n                        dtype=tf.float32,\n                        initializer=tf.constant_initializer(0.0))\n\n  z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n  i, f, o, u = tf.split(z, 4, axis=1)\n  i = tf.sigmoid(i)\n  f = tf.sigmoid(f + 1.0)\n  o = tf.sigmoid(o)\n  u = tf.tanh(u)\n  c = f * c + i * u\n  h = o * tf.tanh(c)\n  return h, c\n\n\ndef generator(input, batch_size, timesteps, encoder):\n  """"""Generate batch with respect to input (a list). Encode its\n  strings to integers, returning an array of shape [batch_size, timesteps].\n  """"""\n  while True:\n    imb = np.random.randint(0, len(input) - timesteps, batch_size)\n    encoded = np.asarray(\n        [[encoder[c] for c in input[i:(i + timesteps)]] for i in imb],\n        dtype=np.int32)\n    yield encoded\n\n\ndef language_model(input, vocab_size):\n  """"""Form p(x[0], ..., x[timesteps - 1]),\n\n  \\prod_{t=0}^{timesteps - 1} p(x[t] | x[:t]),\n\n  To calculate the probability, we call log_prob on\n  x = [x[0], ..., x[timesteps - 1]] given\n  `input` = [0, x[0], ..., x[timesteps - 2]].\n\n  We implement this separately from the generative model so the\n  forward pass, e.g., embedding/dense layers, can be parallelized.\n\n  [batch_size, timesteps] -> [batch_size, timesteps]\n  """"""\n  x = tf.one_hot(input, depth=vocab_size, dtype=tf.float32)\n  h = tf.fill(tf.stack([tf.shape(x)[0], FLAGS.hidden_size]), 0.0)\n  c = tf.fill(tf.stack([tf.shape(x)[0], FLAGS.hidden_size]), 0.0)\n  hs = []\n  reuse = None\n  for t in range(FLAGS.timesteps):\n    if t > 0:\n      reuse = True\n    xt = x[:, t, :]\n    h, c = lstm_cell(xt, h, c, name=""lstm"", reuse=reuse)\n    hs.append(h)\n\n  h = tf.stack(hs, 1)\n  logits = tf.layers.dense(h, vocab_size, name=""dense"")\n  output = Categorical(logits=logits)\n  return output\n\n\ndef language_model_gen(batch_size, vocab_size):\n  """"""Generate x ~ prod p(x_t | x_{<t}). Output [batch_size, timesteps].\n  """"""\n  # Initialize data input randomly.\n  x = tf.random_uniform([FLAGS.batch_size], 0, vocab_size, dtype=tf.int32)\n  h = tf.zeros([FLAGS.batch_size, FLAGS.hidden_size])\n  c = tf.zeros([FLAGS.batch_size, FLAGS.hidden_size])\n  xs = []\n  for _ in range(FLAGS.timesteps):\n    x = tf.one_hot(x, depth=vocab_size, dtype=tf.float32)\n    h, c = lstm_cell(x, h, c, name=""lstm"")\n    logits = tf.layers.dense(h, vocab_size, name=""dense"")\n    x = Categorical(logits=logits).value()\n    xs.append(x)\n\n  xs = tf.cast(tf.stack(xs, 1), tf.int32)\n  return xs\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_train, _, x_test = text8(FLAGS.data_dir)\n  vocab = string.ascii_lowercase + \' \'\n  vocab_size = len(vocab)\n  encoder = dict(zip(vocab, range(vocab_size)))\n  decoder = {v: k for k, v in encoder.items()}\n\n  data = generator(x_train, FLAGS.batch_size, FLAGS.timesteps, encoder)\n\n  # MODEL\n  x_ph = tf.placeholder(tf.int32, [None, FLAGS.timesteps])\n  with tf.variable_scope(""language_model""):\n    # Shift input sequence to right by 1, [0, x[0], ..., x[timesteps - 2]].\n    x_ph_shift = tf.pad(x_ph, [[0, 0], [1, 0]])[:, :-1]\n    x = language_model(x_ph_shift, vocab_size)\n\n  with tf.variable_scope(""language_model"", reuse=True):\n    x_gen = language_model_gen(5, vocab_size)\n\n  imb = range(0, len(x_test) - FLAGS.timesteps, FLAGS.timesteps)\n  encoded_x_test = np.asarray(\n      [[encoder[c] for c in x_test[i:(i + FLAGS.timesteps)]] for i in imb],\n      dtype=np.int32)\n  test_size = encoded_x_test.shape[0]\n  print(""Test set shape: {}"".format(encoded_x_test.shape))\n  test_nll = -tf.reduce_sum(x.log_prob(x_ph))\n\n  # INFERENCE\n  inference = ed.MAP({}, {x: x_ph})\n\n  optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.lr)\n  inference.initialize(optimizer=optimizer,\n                       logdir=FLAGS.log_dir,\n                       log_timestamp=False)\n\n  print(""Number of sets of parameters: {}"".format(\n      len(tf.trainable_variables())))\n  print(""Number of parameters: {}"".format(\n      np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])))\n  for v in tf.trainable_variables():\n    print(v)\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n\n  # Double n_epoch and print progress every half an epoch.\n  n_iter_per_epoch = len(x_train) // (FLAGS.batch_size * FLAGS.timesteps * 2)\n  epoch = 0.0\n  for _ in range(FLAGS.n_epoch * 2):\n    epoch += 0.5\n    print(""Epoch: {0}"".format(epoch))\n    avg_nll = 0.0\n\n    pbar = Progbar(n_iter_per_epoch)\n    for t in range(1, n_iter_per_epoch + 1):\n      pbar.update(t)\n      x_batch = next(data)\n      info_dict = inference.update({x_ph: x_batch})\n      avg_nll += info_dict[\'loss\']\n\n    # Print average bits per character over epoch.\n    avg_nll /= (n_iter_per_epoch * FLAGS.batch_size * FLAGS.timesteps *\n                np.log(2))\n    print(""Train average bits/char: {:0.8f}"".format(avg_nll))\n\n    # Print per-data point log-likelihood on test set.\n    avg_nll = 0.0\n    for start in range(0, test_size, batch_size):\n      end = min(test_size, start + batch_size)\n      x_batch = encoded_x_test[start:end]\n      avg_nll += sess.run(test_nll, {x_ph: x_batch})\n\n    avg_nll /= test_size\n    print(""Test average NLL: {:0.8f}"".format(avg_nll))\n\n    # Generate samples from model.\n    samples = sess.run(x_gen)\n    samples = [\'\'.join([decoder[c] for c in sample]) for sample in samples]\n    print(""Samples:"")\n    for sample in samples:\n      print(sample)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/mixture_gaussian_gibbs.py,2,"b'""""""Mixture of Gaussians, with block Gibbs for inference.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom time import time\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import (\n    Dirichlet, Categorical, InverseGamma, ParamMixture, Normal)\n\n\ndef main(_):\n  # Generate data\n  true_mu = np.array([-1.0, 0.0, 1.0], np.float32) * 10\n  true_sigmasq = np.array([1.0**2, 2.0**2, 3.0**2], np.float32)\n  true_pi = np.array([0.2, 0.3, 0.5], np.float32)\n  N = 10000\n  K = len(true_mu)\n  true_z = np.random.choice(np.arange(K), size=N, p=true_pi)\n  x_data = true_mu[true_z] + np.random.randn(N) * np.sqrt(true_sigmasq[true_z])\n\n  # Prior hyperparameters\n  pi_alpha = np.ones(K, dtype=np.float32)\n  mu_sigma = np.std(true_mu)\n  sigmasq_alpha = 1.0\n  sigmasq_beta = 2.0\n\n  # Model\n  pi = Dirichlet(pi_alpha)\n  mu = Normal(0.0, mu_sigma, sample_shape=K)\n  sigmasq = InverseGamma(sigmasq_alpha, sigmasq_beta, sample_shape=K)\n  x = ParamMixture(pi, {\'loc\': mu, \'scale\': tf.sqrt(sigmasq)}, Normal,\n                   sample_shape=N)\n  z = x.cat\n\n  # Conditionals\n  mu_cond = ed.complete_conditional(mu)\n  sigmasq_cond = ed.complete_conditional(sigmasq)\n  pi_cond = ed.complete_conditional(pi)\n  z_cond = ed.complete_conditional(z)\n\n  sess = ed.get_session()\n\n  # Initialize randomly\n  pi_est, mu_est, sigmasq_est, z_est = sess.run([pi, mu, sigmasq, z])\n\n  print(\'Initial parameters:\')\n  print(\'pi:\', pi_est)\n  print(\'mu:\', mu_est)\n  print(\'sigmasq:\', sigmasq_est)\n  print()\n\n  # Gibbs sampler\n  cond_dict = {pi: pi_est, mu: mu_est, sigmasq: sigmasq_est,\n               z: z_est, x: x_data}\n  t0 = time()\n  T = 500\n  for t in range(T):\n    z_est = sess.run(z_cond, cond_dict)\n    cond_dict[z] = z_est\n    pi_est, mu_est = sess.run([pi_cond, mu_cond], cond_dict)\n    cond_dict[pi] = pi_est\n    cond_dict[mu] = mu_est\n    sigmasq_est = sess.run(sigmasq_cond, cond_dict)\n    cond_dict[sigmasq] = sigmasq_est\n  print(\'took %.3f seconds to run %d iterations\' % (time() - t0, T))\n\n  print()\n  print(\'Final sample for parameters::\')\n  print(\'pi:\', pi_est)\n  print(\'mu:\', mu_est)\n  print(\'sigmasq:\', sigmasq_est)\n  print()\n\n  print()\n  print(\'True parameters:\')\n  print(\'pi:\', true_pi)\n  print(\'mu:\', true_mu)\n  print(\'sigmasq:\', true_sigmasq)\n  print()\n\n  plt.figure(figsize=[10, 10])\n  plt.subplot(2, 1, 1)\n  plt.hist(x_data, 50)\n  plt.title(\'Empirical Distribution of $x$\')\n  plt.xlabel(\'$x$\')\n  plt.ylabel(\'frequency\')\n  xl = plt.xlim()\n  plt.subplot(2, 1, 2)\n  plt.hist(sess.run(x, {pi: pi_est, mu: mu_est, sigmasq: sigmasq_est}), 50)\n  plt.title(""Predictive distribution $p(x \\mid \\mathrm{inferred }\\ ""\n            ""\\pi, \\mu, \\sigma^2)$"")\n  plt.xlabel(\'$x$\')\n  plt.ylabel(\'frequency\')\n  plt.xlim(xl)\n  plt.show()\n\nif __name__ == ""__main__"":\n  plt.style.use(\'ggplot\')\n  tf.app.run()\n'"
examples/mixture_gaussian_mh.py,25,"b'""""""Mixture of Gaussians.\n\nPerform inference with Metropolis-Hastings. It utterly fails. This is\nbecause we are proposing a sample in a high-dimensional space. The\nacceptance ratio is so small that it is unlikely we\'ll ever accept a\nproposed sample. A Gibbs-like extension (""MH within Gibbs""), which\ndoes a separate MH in each dimension, may succeed.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/unsupervised\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.models import (\n    Categorical, Dirichlet, Empirical, InverseGamma, Normal)\nfrom scipy.stats import norm\n\ntf.flags.DEFINE_integer(""N"", default=500, help=""Number of data points."")\ntf.flags.DEFINE_integer(""K"", default=2, help=""Number of components."")\ntf.flags.DEFINE_integer(""D"", default=2, help=""Dimensionality of data."")\ntf.flags.DEFINE_integer(""T"", default=5000, help=""Number of posterior samples."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N):\n  pi = np.array([0.4, 0.6])\n  mus = [[1, 1], [-1, -1]]\n  stds = [[0.1, 0.1], [0.1, 0.1]]\n  x = np.zeros((N, 2))\n  for n in range(N):\n    k = np.argmax(np.random.multinomial(1, pi))\n    x[n, :] = np.random.multivariate_normal(mus[k], np.diag(stds[k]))\n\n  return x\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_data = build_toy_dataset(FLAGS.N)\n\n  # MODEL\n  pi = Dirichlet(concentration=tf.ones(FLAGS.K))\n  mu = Normal(0.0, 1.0, sample_shape=[FLAGS.K, FLAGS.D])\n  sigma = InverseGamma(concentration=1.0, rate=1.0,\n                       sample_shape=[FLAGS.K, FLAGS.D])\n  c = Categorical(logits=tf.log(pi) - tf.log(1.0 - pi), sample_shape=FLAGS.N)\n  x = Normal(loc=tf.gather(mu, c), scale=tf.gather(sigma, c))\n\n  # INFERENCE\n  qpi = Empirical(params=tf.get_variable(\n      ""qpi/params"",\n      [FLAGS.T, FLAGS.K],\n      initializer=tf.constant_initializer(1.0 / FLAGS.K)))\n  qmu = Empirical(params=tf.get_variable(""qmu/params"",\n                                         [FLAGS.T, FLAGS.K, FLAGS.D],\n                                         initializer=tf.zeros_initializer()))\n  qsigma = Empirical(params=tf.get_variable(""qsigma/params"",\n                                            [FLAGS.T, FLAGS.K, FLAGS.D],\n                                            initializer=tf.ones_initializer()))\n  qc = Empirical(params=tf.get_variable(""qc/params"",\n                                        [FLAGS.T, FLAGS.N],\n                                        initializer=tf.zeros_initializer(),\n                                        dtype=tf.int32))\n\n  gpi = Dirichlet(concentration=tf.constant([1.4, 1.6]))\n  gmu = Normal(loc=tf.constant([[1.0, 1.0], [-1.0, -1.0]]),\n               scale=tf.constant([[0.5, 0.5], [0.5, 0.5]]))\n  gsigma = InverseGamma(concentration=tf.constant([[1.1, 1.1], [1.1, 1.1]]),\n                        rate=tf.constant([[1.0, 1.0], [1.0, 1.0]]))\n  gc = Categorical(logits=tf.zeros([FLAGS.N, FLAGS.K]))\n\n  inference = ed.MetropolisHastings(\n      latent_vars={pi: qpi, mu: qmu, sigma: qsigma, c: qc},\n      proposal_vars={pi: gpi, mu: gmu, sigma: gsigma, c: gc},\n      data={x: x_data})\n\n  inference.initialize()\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n\n  for _ in range(inference.n_iter):\n    info_dict = inference.update()\n    inference.print_progress(info_dict)\n\n    t = info_dict[\'t\']\n    if t == 1 or t % inference.n_print == 0:\n      qpi_mean, qmu_mean = sess.run([qpi.mean(), qmu.mean()])\n      print("""")\n      print(""Inferred membership probabilities:"")\n      print(qpi_mean)\n      print(""Inferred cluster means:"")\n      print(qmu_mean)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/normal.py,6,"b'""""""Correlated normal posterior. Inference with Hamiltonian Monte Carlo.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom matplotlib import pyplot as plt\nfrom edward.models import Empirical, MultivariateNormalTriL\n\n\ndef mvn_plot_contours(z, label=False, ax=None):\n  """"""Plot the contours of 2-d Normal or MultivariateNormal object.\n  Scale the axes to show 3 standard deviations.\n  """"""\n  sess = ed.get_session()\n  mu = sess.run(z.parameters[\'loc\'])\n  mu_x, mu_y = mu\n  Sigma = sess.run(z.parameters[\'scale_tril\'])\n  sigma_x, sigma_y = np.sqrt(Sigma[0, 0]), np.sqrt(Sigma[1, 1])\n  xmin, xmax = mu_x - 3 * sigma_x, mu_x + 3 * sigma_x\n  ymin, ymax = mu_y - 3 * sigma_y, mu_y + 3 * sigma_y\n  xs = np.linspace(xmin, xmax, num=100)\n  ys = np.linspace(ymin, ymax, num=100)\n  X, Y = np.meshgrid(xs, ys)\n  T = tf.cast(np.c_[X.flatten(), Y.flatten()], dtype=tf.float32)\n  Z = sess.run(tf.exp(z.log_prob(T))).reshape((len(xs), len(ys)))\n  if ax is None:\n    fig, ax = plt.subplots()\n  cs = ax.contour(X, Y, Z)\n  if label:\n    plt.clabel(cs, inline=1, fontsize=10)\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # MODEL\n  z = MultivariateNormalTriL(\n      loc=tf.ones(2),\n      scale_tril=tf.cholesky(tf.constant([[1.0, 0.8], [0.8, 1.0]])))\n\n  # INFERENCE\n  qz = Empirical(params=tf.get_variable(""qz/params"", [1000, 2]))\n\n  inference = ed.HMC({z: qz})\n  inference.run()\n\n  # CRITICISM\n  sess = ed.get_session()\n  mean, stddev = sess.run([qz.mean(), qz.stddev()])\n  print(""Inferred posterior mean:"")\n  print(mean)\n  print(""Inferred posterior stddev:"")\n  print(stddev)\n\n  fig, ax = plt.subplots()\n  trace = sess.run(qz.params)\n  ax.scatter(trace[:, 0], trace[:, 1], marker=""."")\n  mvn_plot_contours(z, ax=ax)\n  plt.show()\n\nif __name__ == ""__main__"":\n  plt.style.use(""ggplot"")\n  tf.app.run()\n'"
examples/normal_normal.py,3,"b'""""""Normal-normal model using Hamiltonian Monte Carlo.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Empirical, Normal\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  x_data = np.array([0.0] * 50)\n\n  # MODEL: Normal-Normal with known variance\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n  # INFERENCE\n  qmu = Empirical(params=tf.get_variable(""qmu/params"", [1000],\n                                         initializer=tf.zeros_initializer()))\n\n  # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n  inference = ed.HMC({mu: qmu}, data={x: x_data})\n  inference.run()\n\n  # CRITICISM\n  sess = ed.get_session()\n  mean, stddev = sess.run([qmu.mean(), qmu.stddev()])\n  print(""Inferred posterior mean:"")\n  print(mean)\n  print(""Inferred posterior stddev:"")\n  print(stddev)\n\n  # Check convergence with visual diagnostics.\n  samples = sess.run(qmu.params)\n\n  # Plot histogram.\n  plt.hist(samples, bins=\'auto\')\n  plt.show()\n\n  # Trace plot.\n  plt.plot(samples)\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/normal_sgld.py,4,"b'""""""Correlated normal posterior. Inference with stochastic gradient\nLangevin dynamics.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Empirical, MultivariateNormalTriL\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # MODEL\n  z = MultivariateNormalTriL(\n      loc=tf.ones(2),\n      scale_tril=tf.cholesky(tf.constant([[1.0, 0.8], [0.8, 1.0]])))\n\n  # INFERENCE\n  qz = Empirical(params=tf.get_variable(""qz/params"", [2000, 2]))\n\n  inference = ed.SGLD({z: qz})\n  inference.run(step_size=5.0)\n\n  # CRITICISM\n  sess = ed.get_session()\n  mean, stddev = sess.run([qz.mean(), qz.stddev()])\n  print(""Inferred posterior mean:"")\n  print(mean)\n  print(""Inferred posterior stddev:"")\n  print(stddev)\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/pp_dirichlet_process.py,8,"b'""""""Dirichlet process.\n\nWe implement sample generation from a Dirichlet process (with no base\ndistribution) via its stick breaking construction. It is a streamlined\nimplementation of the `DirichletProcess` random variable in Edward.\n\nReferences\n----------\nhttps://probmods.org/chapters/12-non-parametric-models.html#infinite-discrete-distributions-the-dirichlet-processes\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Beta, DirichletProcess, Exponential, Normal\n\n\ndef dirichlet_process(alpha):\n  """"""Demo of stochastic while loop for stick breaking construction.""""""\n  def cond(k, beta_k):\n    # End while loop (return False) when flip is heads.\n    flip = Bernoulli(beta_k)\n    return tf.cast(1 - flip, tf.bool)\n\n  def body(k, beta_k):\n    beta_k = Beta(1.0, alpha)\n    return k + 1, beta_k\n\n  k = tf.constant(0)\n  beta_k = Beta(1.0, alpha)\n  stick_num, stick_beta = tf.while_loop(cond, body, loop_vars=[k, beta_k])\n  return stick_num\n\n\ndef main(_):\n  dp = dirichlet_process(10.0)\n\n  # The number of sticks broken is dynamic, changing across evaluations.\n  sess = tf.Session()\n  print(sess.run(dp))\n  print(sess.run(dp))\n\n  # Demo of the DirichletProcess random variable in Edward.\n  base = Normal(0.0, 1.0)\n\n  # Highly concentrated DP.\n  alpha = 1.0\n  dp = DirichletProcess(alpha, base)\n  x = dp.sample(1000)\n  samples = sess.run(x)\n  plt.hist(samples, bins=100, range=(-3.0, 3.0))\n  plt.title(""DP({0}, N(0, 1))"".format(alpha))\n  plt.show()\n\n  # More spread out DP.\n  alpha = 50.0\n  dp = DirichletProcess(alpha, base)\n  x = dp.sample(1000)\n  samples = sess.run(x)\n  plt.hist(samples, bins=100, range=(-3.0, 3.0))\n  plt.title(""DP({0}, N(0, 1))"".format(alpha))\n  plt.show()\n\n  # States persist across calls to sample() in a DP.\n  alpha = 1.0\n  dp = DirichletProcess(alpha, base)\n  x = dp.sample(50)\n  y = dp.sample(75)\n  samples_x, samples_y = sess.run([x, y])\n  plt.subplot(211)\n  plt.hist(samples_x, bins=100, range=(-3.0, 3.0))\n  plt.title(""DP({0}, N(0, 1)) across two calls to sample()"".format(alpha))\n  plt.subplot(212)\n  plt.hist(samples_y, bins=100, range=(-3.0, 3.0))\n  plt.show()\n\n  # `theta` is the distribution indirectly returned by the DP.\n  # Fetching theta is the same as fetching the Dirichlet process.\n  dp = DirichletProcess(alpha, base)\n  theta = Normal(0.0, 1.0, value=tf.cast(dp, tf.float32))\n  print(sess.run([dp, theta]))\n  print(sess.run([dp, theta]))\n\n  # DirichletProcess can also take in non-scalar concentrations and bases.\n  alpha = tf.constant([0.1, 0.6, 0.4])\n  base = Exponential(rate=tf.ones([5, 2]))\n  dp = DirichletProcess(alpha, base)\n  print(dp)\n\nif __name__ == ""__main__"":\n  plt.style.use(\'ggplot\')\n  tf.app.run()\n'"
examples/pp_dynamic_shape.py,3,"b'""""""Dynamic shapes.\n\nWe build a random variable whose size depends on a sample from another\nrandom variable.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Exponential, Dirichlet, Gamma\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # Prior on scalar hyperparameter to Dirichlet.\n  alpha = Gamma(1.0, 1.0)\n\n  # Prior on size of Dirichlet.\n  n = 1 + tf.cast(Exponential(0.5), tf.int32)\n\n  # Build a vector of ones whose size is n; multiply it by alpha.\n  p = Dirichlet(tf.ones([n]) * alpha)\n\n  sess = ed.get_session()\n  print(sess.run(p))\n  # [ 0.01012419  0.02939712  0.05036638  0.51287931  0.31020424  0.0485355\n  #   0.0384932 ]\n  print(sess.run(p))\n  # [ 0.12836078  0.23335715  0.63828212]\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/pp_persistent_randomness.py,2,"b'""""""Persistent randomness.\n\nOur language defines random variables. They enable memoization in the\nsense that the generative process of any values which depend on the\nsame random variable will be generated conditioned on the same samples.\nSimulating the world multiple times (i.e., fetching the value out of\nsession) results in new memoized values. To avoid persistent\nrandomness, simply define another random variable to work with.\n\nReferences\n----------\nhttps://probmods.org/chapters/02-generative-models.html#persistent-randomness-mem\n""""""\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Categorical\n\n\ndef eye_color(person):\n  random_variables = {x.name: x for x in ed.random_variables()}\n  if person + \'/\' in random_variables:\n    return random_variables[person + \'/\']\n  else:\n    return Categorical(probs=tf.ones(3) / 3, name=person)\n\n\ndef main(_):\n  # Only two categorical random variables are created.\n  eye_color(\'bob\')\n  eye_color(\'alice\')\n  eye_color(\'bob\')\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/pp_stochastic_control_flow.py,5,"b'""""""Stochastic control flow.\n\nWe sample from a geometric random variable by using samples from\nBernoulli random variables. It requires a while loop whose condition\nis stochastic.\n\nReferences\n----------\nhttps://probmods.org/chapters/02-generative-models.html#stochastic-recursion\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli\n\n\ndef geometric(p):\n  i = tf.constant(0)\n  sample = tf.while_loop(\n      cond=lambda i: tf.cast(1 - Bernoulli(probs=p), tf.bool),\n      body=lambda i: i + 1,\n      loop_vars=[i])\n  return sample\n\n\ndef main(_):\n  p = 0.1\n  geom = geometric(p)\n\n  sess = tf.Session()\n  samples = [sess.run(geom) for _ in range(1000)]\n  plt.hist(samples, bins=\'auto\')\n  plt.title(""Geometric({0})"".format(p))\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/pp_stochastic_recursion.py,7,"b'""""""Stochastic recursion.\n\nWe sample from a geometric random variable by using samples from\nBernoulli random variable. It uses a recursive function and requires\nlazy evaluation of the condition.\n\nRecursion is not available in TensorFlow and so neither is stochastic\nrecursion available in Edward\'s modeling language. There are several\nalternatives: (stochastic) while loops, wrapping around a Python\nimplementation (`tf.py_func`), and a CPS-style formulation.\n\nReferences\n----------\nhttps://probmods.org/chapters/02-generative-models.html#stochastic-recursion\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli\n\n\ndef geometric(p):\n    cond = tf.cast(Bernoulli(probs=p), tf.bool)\n\n    def fn1():\n      return tf.constant(0)\n\n    def fn2():\n      return geometric(p) + 1\n\n    # TensorFlow builds the op non-lazily, unrolling both functions\n    # before it checks the condition. This makes this function fail.\n    return tf.cond(cond, fn1, fn2)\n\n\ndef main(_):\n  p = tf.constant(0.9)\n  geom = geometric(p)\n\n  sess = tf.Session()\n  samples = [sess.run(geom) for _ in range(1000)]\n  plt.hist(samples, bins=\'auto\')\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/probabilistic_matrix_factorization.py,17,"b'""""""Probabilistic matrix factorization using variational inference.\n\nVisualizes the actual and the estimated rating matrices as heatmaps.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\ntf.flags.DEFINE_integer(""N"", default=50, help=""Number of users."")\ntf.flags.DEFINE_integer(""M"", default=60, help=""Number of movies."")\ntf.flags.DEFINE_integer(""D"", default=3, help=""Number of latent factors."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(U, V, N, M, noise_std=0.1):\n  R = np.dot(np.transpose(U), V) + np.random.normal(0, noise_std, size=(N, M))\n  return R\n\n\ndef get_indicators(N, M, prob_std=0.5):\n  ind = np.random.binomial(1, prob_std, (N, M))\n  return ind\n\n\ndef main(_):\n  # true latent factors\n  U_true = np.random.randn(FLAGS.D, FLAGS.N)\n  V_true = np.random.randn(FLAGS.D, FLAGS.M)\n\n  # DATA\n  R_true = build_toy_dataset(U_true, V_true, FLAGS.N, FLAGS.M)\n  I_train = get_indicators(FLAGS.N, FLAGS.M)\n  I_test = 1 - I_train\n\n  # MODEL\n  I = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.M])\n  U = Normal(loc=0.0, scale=1.0, sample_shape=[FLAGS.D, FLAGS.N])\n  V = Normal(loc=0.0, scale=1.0, sample_shape=[FLAGS.D, FLAGS.M])\n  R = Normal(loc=tf.matmul(tf.transpose(U), V) * I,\n             scale=tf.ones([FLAGS.N, FLAGS.M]))\n\n  # INFERENCE\n  qU = Normal(loc=tf.get_variable(""qU/loc"", [FLAGS.D, FLAGS.N]),\n              scale=tf.nn.softplus(\n                  tf.get_variable(""qU/scale"", [FLAGS.D, FLAGS.N])))\n  qV = Normal(loc=tf.get_variable(""qV/loc"", [FLAGS.D, FLAGS.M]),\n              scale=tf.nn.softplus(\n                  tf.get_variable(""qV/scale"", [FLAGS.D, FLAGS.M])))\n\n  inference = ed.KLqp({U: qU, V: qV}, data={R: R_true, I: I_train})\n  inference.run()\n\n  # CRITICISM\n  qR = Normal(loc=tf.matmul(tf.transpose(qU), qV),\n              scale=tf.ones([FLAGS.N, FLAGS.M]))\n\n  print(""Mean squared error on test data:"")\n  print(ed.evaluate(\'mean_squared_error\', data={qR: R_true, I: I_test}))\n\n  plt.imshow(R_true, cmap=\'hot\')\n  plt.show()\n\n  R_est = tf.matmul(tf.transpose(qU), qV).eval()\n  plt.imshow(R_est, cmap=\'hot\')\n  plt.show()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/probabilistic_pca_subsampling.py,18,"b'""""""Probabilistic principal components analysis (Tipping and Bishop, 1999).\n\nInference uses data subsampling.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/probabilistic-pca\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\ntf.flags.DEFINE_integer(""N"", default=5000, help=""Number of data points."")\ntf.flags.DEFINE_integer(""M"", default=100, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""D"", default=2, help=""Data dimensionality."")\ntf.flags.DEFINE_integer(""K"", default=1, help=""Latent dimensionality."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef build_toy_dataset(N, D, K, sigma=1):\n  x_train = np.zeros((D, N))\n  w = np.random.normal(0.0, 2.0, size=(D, K))\n  z = np.random.normal(0.0, 1.0, size=(K, N))\n  mean = np.dot(w, z)\n  for d in range(D):\n    for n in range(N):\n      x_train[d, n] = np.random.normal(mean[d, n], sigma)\n\n  print(""True principal axes:"")\n  print(w)\n  return x_train\n\n\ndef next_batch(x_train, M):\n  idx_batch = np.random.choice(FLAGS.N, M)\n  return x_train[:, idx_batch], idx_batch\n\n\ndef main(_):\n  ed.set_seed(142)\n\n  # DATA\n  x_train = build_toy_dataset(FLAGS.N, FLAGS.D, FLAGS.K)\n\n  # MODEL\n  w = Normal(loc=0.0, scale=10.0, sample_shape=[FLAGS.D, FLAGS.K])\n  z = Normal(loc=0.0, scale=1.0, sample_shape=[FLAGS.M, FLAGS.K])\n  x = Normal(loc=tf.matmul(w, z, transpose_b=True),\n             scale=tf.ones([FLAGS.D, FLAGS.M]))\n\n  # INFERENCE\n  qw_variables = [tf.get_variable(""qw/loc"", [FLAGS.D, FLAGS.K]),\n                  tf.get_variable(""qw/scale"", [FLAGS.D, FLAGS.K])]\n  qw = Normal(loc=qw_variables[0], scale=tf.nn.softplus(qw_variables[1]))\n\n  qz_variables = [tf.get_variable(""qz/loc"", [FLAGS.N, FLAGS.K]),\n                  tf.get_variable(""qz/scale"", [FLAGS.N, FLAGS.K])]\n  idx_ph = tf.placeholder(tf.int32, FLAGS.M)\n  qz = Normal(loc=tf.gather(qz_variables[0], idx_ph),\n              scale=tf.nn.softplus(tf.gather(qz_variables[1], idx_ph)))\n\n  x_ph = tf.placeholder(tf.float32, [FLAGS.D, FLAGS.M])\n  inference_w = ed.KLqp({w: qw}, data={x: x_ph, z: qz})\n  inference_z = ed.KLqp({z: qz}, data={x: x_ph, w: qw})\n\n  scale_factor = float(FLAGS.N) / FLAGS.M\n  inference_w.initialize(scale={x: scale_factor, z: scale_factor},\n                         var_list=qz_variables,\n                         n_samples=5)\n  inference_z.initialize(scale={x: scale_factor, z: scale_factor},\n                         var_list=qw_variables,\n                         n_samples=5)\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n  for _ in range(inference_w.n_iter):\n    x_batch, idx_batch = next_batch(x_train, FLAGS.M)\n    for _ in range(5):\n      inference_z.update(feed_dict={x_ph: x_batch, idx_ph: idx_batch})\n\n    info_dict = inference_w.update(feed_dict={x_ph: x_batch, idx_ph: idx_batch})\n    inference_w.print_progress(info_dict)\n\n    t = info_dict[\'t\']\n    if t % 100 == 0:\n      print(""\\nInferred principal axes:"")\n      print(sess.run(qw.mean()))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/rasch_model.py,13,"b'""""""Rasch model (Rasch, 1960).""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal, Empirical\nfrom scipy.special import expit\n\ntf.flags.DEFINE_integer(""nsubj"", default=200, help="""")\ntf.flags.DEFINE_integer(""nitem"", default=25, help="""")\ntf.flags.DEFINE_integer(""T"", default=5000, help=""Number of posterior samples."")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef main(_):\n  # DATA\n  trait_true = np.random.normal(size=[FLAGS.nsubj, 1])\n  thresh_true = np.random.normal(size=[1, FLAGS.nitem])\n  X_data = np.random.binomial(1, expit(trait_true - thresh_true))\n\n  # MODEL\n  trait = Normal(loc=0.0, scale=1.0, sample_shape=[FLAGS.nsubj, 1])\n  thresh = Normal(loc=0.0, scale=1.0, sample_shape=[1, FLAGS.nitem])\n  X = Bernoulli(logits=trait - thresh)\n\n  # INFERENCE\n  q_trait = Empirical(params=tf.get_variable(""q_trait/params"",\n                                             [FLAGS.T, FLAGS.nsubj, 1]))\n  q_thresh = Empirical(params=tf.get_variable(""q_thresh/params"",\n                                              [FLAGS.T, 1, FLAGS.nitem]))\n\n  inference = ed.HMC({trait: q_trait, thresh: q_thresh}, data={X: X_data})\n  inference.run(step_size=0.1)\n\n  # Alternatively, use variational inference.\n  # q_trait = Normal(\n  #     loc=tf.get_variable(""q_trait/loc"", [FLAGS.nsubj, 1]),\n  #     scale=tf.nn.softplus(\n  #         tf.get_variable(""q_trait/scale"", [FLAGS.nsubj, 1])))\n  # q_thresh = Normal(\n  #     loc=tf.get_variable(""q_thresh/loc"", [1, FLAGS.nitem]),\n  #     scale=tf.nn.softplus(\n  #         tf.get_variable(""q_thresh/scale"", [1, FLAGS.nitem])))\n\n  # inference = ed.KLqp({trait: q_trait, thresh: q_thresh}, data={X: X_data})\n  # inference.run(n_iter=2500, n_samples=10)\n\n  # CRITICISM\n  # Check that the inferred posterior mean captures the true traits.\n  plt.scatter(trait_true, q_trait.mean().eval())\n  plt.show()\n\n  print(""MSE between true traits and inferred posterior mean:"")\n  print(np.mean(np.square(trait_true - q_trait.mean().eval())))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/sigmoid_belief_network.py,21,"b'""""""Sigmoid belief network (Neal, 1990) trained on the Caltech 101\nSilhouettes data set.\n\nDefault settings take ~143s / epoch on a Titan X (Pascal). Results on\nepoch 100:\nTraining negative log-likelihood: 209.443\nTest negative log-likelihood: 161.244\n\nUsing n_train_samples=50 converges to test NLL of 157.824.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli\nfrom edward.util import Progbar\nfrom observations import caltech101_silhouettes\nfrom scipy.misc import imsave\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""out_dir"", default=""/tmp/out"", help="""")\ntf.flags.DEFINE_integer(""batch_size"", default=24,\n                        help=""Batch size during training."")\ntf.flags.DEFINE_list(""hidden_sizes"", default=[300, 100, 50, 10],\n                     help=""Hidden size per layer from bottom-up."")\ntf.flags.DEFINE_integer(""n_train_samples"", default=10,\n                        help=""Number of samples for training."")\ntf.flags.DEFINE_integer(""n_test_samples"", default=1000,\n                        help=""Number of samples to calculate test log-lik."")\ntf.flags.DEFINE_float(""step_size"", default=1e-3,\n                      help=""Learning rate step size."")\ntf.flags.DEFINE_integer(""n_epoch"", default=100, help="""")\ntf.flags.DEFINE_integer(""n_iter_per_epoch"", default=10000, help="""")\n\nFLAGS = tf.flags.FLAGS\nif not os.path.exists(FLAGS.out_dir):\n  os.makedirs(FLAGS.out_dir)\n\n\ndef generator(array, batch_size):\n  """"""Generate batch with respect to array\'s first axis.""""""\n  start = 0  # pointer to where we are in iteration\n  while True:\n    stop = start + batch_size\n    diff = stop - array.shape[0]\n    if diff <= 0:\n      batch = array[start:stop]\n      start += batch_size\n    else:\n      batch = np.concatenate((array[start:], array[:diff]))\n      start = diff\n    yield batch\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  (x_train, _), (x_test, _), (x_valid, _) = caltech101_silhouettes(\n      FLAGS.data_dir)\n  x_train_generator = generator(x_train, FLAGS.batch_size)\n  x_ph = tf.placeholder(tf.int32, [None, 28 * 28])\n\n  # MODEL\n  zs = [0] * len(FLAGS.hidden_sizes)\n  for l in reversed(range(len(FLAGS.hidden_sizes))):\n    if l == len(FLAGS.hidden_sizes) - 1:\n      logits = tf.zeros([tf.shape(x_ph)[0], FLAGS.hidden_sizes[l]])\n    else:\n      logits = tf.layers.dense(tf.cast(zs[l + 1], tf.float32),\n                               FLAGS.hidden_sizes[l], activation=None)\n    zs[l] = Bernoulli(logits=logits)\n\n  x = Bernoulli(logits=tf.layers.dense(tf.cast(zs[0], tf.float32),\n                                       28 * 28, activation=None))\n\n  # INFERENCE\n  # Define variational model with reverse ordering as probability model:\n  # if p is 15-100-300 from top-down, q is 300-100-15 from bottom-up.\n  qzs = [0] * len(FLAGS.hidden_sizes)\n  for l in range(len(FLAGS.hidden_sizes)):\n    if l == 0:\n      logits = tf.layers.dense(tf.cast(x_ph, tf.float32),\n                               FLAGS.hidden_sizes[l], activation=None)\n    else:\n      logits = tf.layers.dense(tf.cast(qzs[l - 1], tf.float32),\n                               FLAGS.hidden_sizes[l], activation=None)\n    qzs[l] = Bernoulli(logits=logits)\n\n  inference = ed.KLqp({z: qz for z, qz in zip(zs, qzs)}, data={x: x_ph})\n  optimizer = tf.train.AdamOptimizer(FLAGS.step_size)\n  inference.initialize(optimizer=optimizer, n_samples=FLAGS.n_train_samples)\n\n  # Build tensor for log-likelihood given one variational sample to run\n  # on test data.\n  x_post = ed.copy(x, {z: qz for z, qz in zip(zs, qzs)})\n  x_neg_log_prob = (-tf.reduce_sum(x_post.log_prob(x_ph)) /\n                    tf.cast(tf.shape(x_ph)[0], tf.float32))\n\n  sess = ed.get_session()\n  tf.global_variables_initializer().run()\n\n  for epoch in range(FLAGS.n_epoch):\n    print(""Epoch {}"".format(epoch))\n    train_loss = 0.0\n\n    pbar = Progbar(FLAGS.n_iter_per_epoch)\n    for t in range(1, FLAGS.n_iter_per_epoch + 1):\n      pbar.update(t)\n      x_batch = next(x_train_generator)\n      info_dict = inference.update(feed_dict={x_ph: x_batch})\n      train_loss += info_dict[\'loss\']\n\n    # Print per-data point loss, averaged over training epoch.\n    train_loss /= FLAGS.n_iter_per_epoch\n    train_loss /= FLAGS.batch_size\n    print(""Training negative log-likelihood: {:0.3f}"".format(train_loss))\n\n    test_loss = [sess.run(x_neg_log_prob, {x_ph: x_test})\n                 for _ in range(FLAGS.n_test_samples)]\n    test_loss = np.mean(test_loss)\n    print(""Test negative log-likelihood: {:0.3f}"".format(test_loss))\n\n    # Prior predictive check.\n    images = sess.run(x, {x_ph: x_batch})  # feed ph to determine sample size\n    for m in range(FLAGS.batch_size):\n      imsave(""{}/{}.png"".format(out_dir, m), images[m].reshape(28, 28))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/stochastic_block_model.py,8,"b'""""""Stochastic block model.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Multinomial, Beta, Dirichlet, PointMass\nfrom observations import karate\nfrom sklearn.metrics.cluster import adjusted_rand_score\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA\n  X_data, Z_true = karate(""~/data"")\n  N = X_data.shape[0]  # number of vertices\n  K = 2  # number of clusters\n\n  # MODEL\n  gamma = Dirichlet(concentration=tf.ones([K]))\n  Pi = Beta(concentration0=tf.ones([K, K]), concentration1=tf.ones([K, K]))\n  Z = Multinomial(total_count=1.0, probs=gamma, sample_shape=N)\n  X = Bernoulli(probs=tf.matmul(Z, tf.matmul(Pi, tf.transpose(Z))))\n\n  # INFERENCE (EM algorithm)\n  qgamma = PointMass(tf.nn.softmax(tf.get_variable(""qgamma/params"", [K])))\n  qPi = PointMass(tf.nn.sigmoid(tf.get_variable(""qPi/params"", [K, K])))\n  qZ = PointMass(tf.nn.softmax(tf.get_variable(""qZ/params"", [N, K])))\n\n  inference = ed.MAP({gamma: qgamma, Pi: qPi, Z: qZ}, data={X: X_data})\n  inference.initialize(n_iter=250)\n\n  tf.global_variables_initializer().run()\n\n  for _ in range(inference.n_iter):\n    info_dict = inference.update()\n    inference.print_progress(info_dict)\n\n  # CRITICISM\n  Z_pred = qZ.mean().eval().argmax(axis=1)\n  print(""Result (label flip can happen):"")\n  print(""Predicted"")\n  print(Z_pred)\n  print(""True"")\n  print(Z_true)\n  print(""Adjusted Rand Index ="", adjusted_rand_score(Z_pred, Z_true))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/vae.py,19,"b'""""""Variational auto-encoder for MNIST data.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/decoder\nhttp://edwardlib.org/tutorials/inference-networks\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import Progbar\nfrom observations import mnist\nfrom scipy.misc import imsave\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""out_dir"", default=""/tmp/out"", help="""")\ntf.flags.DEFINE_integer(""M"", default=100, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""d"", default=2, help=""Latent dimension."")\ntf.flags.DEFINE_integer(""n_epoch"", default=100, help="""")\n\nFLAGS = tf.flags.FLAGS\nif not os.path.exists(FLAGS.out_dir):\n  os.makedirs(FLAGS.out_dir)\n\n\ndef generator(array, batch_size):\n  """"""Generate batch with respect to array\'s first axis.""""""\n  start = 0  # pointer to where we are in iteration\n  while True:\n    stop = start + batch_size\n    diff = stop - array.shape[0]\n    if diff <= 0:\n      batch = array[start:stop]\n      start += batch_size\n    else:\n      batch = np.concatenate((array[start:], array[:diff]))\n      start = diff\n    batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n    batch = np.random.binomial(1, batch)  # binarize images\n    yield batch\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA. MNIST batches are fed at training time.\n  (x_train, _), (x_test, _) = mnist(FLAGS.data_dir)\n  x_train_generator = generator(x_train, FLAGS.M)\n\n  # MODEL\n  # Define a subgraph of the full model, corresponding to a minibatch of\n  # size M.\n  z = Normal(loc=tf.zeros([FLAGS.M, FLAGS.d]),\n             scale=tf.ones([FLAGS.M, FLAGS.d]))\n  hidden = tf.layers.dense(z, 256, activation=tf.nn.relu)\n  x = Bernoulli(logits=tf.layers.dense(hidden, 28 * 28))\n\n  # INFERENCE\n  # Define a subgraph of the variational model, corresponding to a\n  # minibatch of size M.\n  x_ph = tf.placeholder(tf.int32, [FLAGS.M, 28 * 28])\n  hidden = tf.layers.dense(tf.cast(x_ph, tf.float32), 256,\n                           activation=tf.nn.relu)\n  qz = Normal(loc=tf.layers.dense(hidden, FLAGS.d),\n              scale=tf.layers.dense(\n                  hidden, FLAGS.d, activation=tf.nn.softplus))\n\n  # Bind p(x, z) and q(z | x) to the same TensorFlow placeholder for x.\n  inference = ed.KLqp({z: qz}, data={x: x_ph})\n  optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n  inference.initialize(optimizer=optimizer)\n\n  tf.global_variables_initializer().run()\n\n  n_iter_per_epoch = x_train.shape[0] // FLAGS.M\n  for epoch in range(1, FLAGS.n_epoch + 1):\n    print(""Epoch: {0}"".format(epoch))\n    avg_loss = 0.0\n\n    pbar = Progbar(n_iter_per_epoch)\n    for t in range(1, n_iter_per_epoch + 1):\n      pbar.update(t)\n      x_batch = next(x_train_generator)\n      info_dict = inference.update(feed_dict={x_ph: x_batch})\n      avg_loss += info_dict[\'loss\']\n\n    # Print a lower bound to the average marginal likelihood for an\n    # image.\n    avg_loss /= n_iter_per_epoch\n    avg_loss /= FLAGS.M\n    print(""-log p(x) <= {:0.3f}"".format(avg_loss))\n\n    # Prior predictive check.\n    images = x.eval()\n    for m in range(FLAGS.M):\n      imsave(os.path.join(FLAGS.out_dir, \'%d.png\') % m,\n             images[m].reshape(28, 28))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/vae_convolutional.py,40,"b'""""""Convolutional variational auto-encoder for binarized MNIST.\n\nReferences\n----------\nhttp://edwardlib.org/tutorials/decoder\nhttp://edwardlib.org/tutorials/inference-networks\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import Progbar\nfrom observations import mnist\nfrom scipy.misc import imsave\n\ntf.flags.DEFINE_string(""data_dir"", default=""/tmp/data"", help="""")\ntf.flags.DEFINE_string(""out_dir"", default=""/tmp/out"", help="""")\ntf.flags.DEFINE_integer(""M"", default=128, help=""Batch size during training."")\ntf.flags.DEFINE_integer(""d"", default=10, help=""Latent dimension."")\ntf.flags.DEFINE_integer(""n_epoch"", default=100, help="""")\n\nFLAGS = tf.flags.FLAGS\nif not os.path.exists(FLAGS.out_dir):\n  os.makedirs(FLAGS.out_dir)\n\n\ndef generator(array, batch_size):\n  """"""Generate batch with respect to array\'s first axis.""""""\n  start = 0  # pointer to where we are in iteration\n  while True:\n    stop = start + batch_size\n    diff = stop - array.shape[0]\n    if diff <= 0:\n      batch = array[start:stop]\n      start += batch_size\n    else:\n      batch = np.concatenate((array[start:], array[:diff]))\n      start = diff\n    batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n    batch = np.random.binomial(1, batch)  # binarize images\n    yield batch\n\n\ndef generative_network(z):\n  """"""Generative network to parameterize generative model. It takes\n  latent variables as input and outputs the likelihood parameters.\n\n  logits = neural_network(z)\n  """"""\n  net = tf.reshape(z, [FLAGS.M, 1, 1, FLAGS.d])\n  net = tf.layers.conv2d_transpose(net, 128, 3, padding=\'VALID\')\n  net = tf.layers.batch_normalization(net)\n  net = tf.nn.elu(net)\n  net = tf.layers.conv2d_transpose(net, 64, 5, padding=\'VALID\')\n  net = tf.layers.batch_normalization(net)\n  net = tf.nn.elu(net)\n  net = tf.layers.conv2d_transpose(net, 32, 5, strides=2, padding=\'SAME\')\n  net = tf.layers.batch_normalization(net)\n  net = tf.nn.elu(net)\n  net = tf.layers.conv2d_transpose(net, 1, 5, strides=2, padding=\'SAME\')\n  net = tf.reshape(net, [FLAGS.M, -1])\n  return net\n\n\ndef inference_network(x):\n  """"""Inference network to parameterize variational model. It takes\n  data as input and outputs the variational parameters.\n\n  loc, scale = neural_network(x)\n  """"""\n  net = tf.reshape(x, [FLAGS.M, 28, 28, 1])\n  net = tf.layers.conv2d(net, 32, 5, strides=2, padding=\'SAME\')\n  net = tf.layers.batch_normalization(net)\n  net = tf.nn.elu(net)\n  net = tf.layers.conv2d(net, 64, 5, strides=2, padding=\'SAME\')\n  net = tf.layers.batch_normalization(net)\n  net = tf.nn.elu(net)\n  net = tf.layers.conv2d(net, 128, 5, padding=\'VALID\')\n  net = tf.layers.batch_normalization(net)\n  net = tf.nn.elu(net)\n  net = tf.layers.dropout(net, 0.1)\n  net = tf.reshape(net, [FLAGS.M, -1])\n  net = tf.layers.dense(net, FLAGS.d * 2, activation=None)\n  loc = net[:, :FLAGS.d]\n  scale = tf.nn.softplus(net[:, FLAGS.d:])\n  return loc, scale\n\n\ndef main(_):\n  ed.set_seed(42)\n\n  # DATA. MNIST batches are fed at training time.\n  (x_train, _), (x_test, _) = mnist(FLAGS.data_dir)\n  x_train_generator = generator(x_train, FLAGS.M)\n\n  # MODEL\n  z = Normal(loc=tf.zeros([FLAGS.M, FLAGS.d]),\n             scale=tf.ones([FLAGS.M, FLAGS.d]))\n  logits = generative_network(z)\n  x = Bernoulli(logits=logits)\n\n  # INFERENCE\n  x_ph = tf.placeholder(tf.int32, [FLAGS.M, 28 * 28])\n  loc, scale = inference_network(tf.cast(x_ph, tf.float32))\n  qz = Normal(loc=loc, scale=scale)\n\n  # Bind p(x, z) and q(z | x) to the same placeholder for x.\n  inference = ed.KLqp({z: qz}, data={x: x_ph})\n  optimizer = tf.train.AdamOptimizer(0.01, epsilon=1.0)\n  inference.initialize(optimizer=optimizer)\n\n  hidden_rep = tf.sigmoid(logits)\n\n  tf.global_variables_initializer().run()\n\n  n_iter_per_epoch = x_train.shape[0] // FLAGS.M\n  for epoch in range(1, FLAGS.n_epoch + 1):\n    print(""Epoch: {0}"".format(epoch))\n    avg_loss = 0.0\n\n    pbar = Progbar(n_iter_per_epoch)\n    for t in range(1, n_iter_per_epoch + 1):\n      pbar.update(t)\n      x_batch = next(x_train_generator)\n      info_dict = inference.update(feed_dict={x_ph: x_batch})\n      avg_loss += info_dict[\'loss\']\n\n    # Print a lower bound to the average marginal likelihood for an\n    # image.\n    avg_loss /= n_iter_per_epoch\n    avg_loss /= FLAGS.M\n    print(""-log p(x) <= {:0.3f}"".format(avg_loss))\n\n    # Visualize hidden representations.\n    images = hidden_rep.eval()\n    for m in range(FLAGS.M):\n      imsave(os.path.join(FLAGS.out_dir, \'%d.png\') % m,\n             images[m].reshape(28, 28))\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
docs/parser/doc_generator_visitor.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A `traverse` visitor for processing documentation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\n\nfrom tensorflow.python.util import tf_inspect\n\n\nclass DocGeneratorVisitor(object):\n  """"""A visitor that generates docs for a python object when __call__ed.""""""\n\n  def __init__(self, root_name=\'\'):\n    """"""Make a visitor.\n\n    As this visitor is starting its traversal at a module or class, it will not\n    be told the name of that object during traversal. `root_name` is the name it\n    should use for that object, effectively prefixing all names with\n    ""root_name."".\n\n    Args:\n      root_name: The name of the root module/class.\n    """"""\n    self.set_root_name(root_name)\n    self._index = {}\n    self._tree = {}\n    self._reverse_index = None\n    self._duplicates = None\n    self._duplicate_of = None\n\n  def set_root_name(self, root_name):\n    """"""Sets the root name for subsequent __call__s.""""""\n    self._root_name = root_name or \'\'\n    self._prefix = (root_name + \'.\') if root_name else \'\'\n\n  @property\n  def index(self):\n    """"""A map from fully qualified names to objects to be documented.\n\n    The index is filled when the visitor is passed to `traverse`.\n\n    Returns:\n      The index filled by traversal.\n    """"""\n    return self._index\n\n  @property\n  def tree(self):\n    """"""A map from fully qualified names to all its child names for traversal.\n\n    The full name to member names map is filled when the visitor is passed to\n    `traverse`.\n\n    Returns:\n      The full name to member name map filled by traversal.\n    """"""\n    return self._tree\n\n  @property\n  def reverse_index(self):\n    """"""A map from `id(object)` to the preferred fully qualified name.\n\n    This map only contains non-primitive objects (no numbers or strings) present\n    in `index` (for primitive objects, `id()` doesn\'t quite do the right thing).\n\n    It is computed when it, `duplicate_of`, or `duplicates` are first accessed.\n\n    Returns:\n      The `id(object)` to full name map.\n    """"""\n    self._maybe_find_duplicates()\n    return self._reverse_index\n\n  @property\n  def duplicate_of(self):\n    """"""A map from duplicate full names to a preferred fully qualified name.\n\n    This map only contains names that are not themself a preferred name.\n\n    It is computed when it, `reverse_index`, or `duplicates` are first accessed.\n\n    Returns:\n      The map from duplicate name to preferred name.\n    """"""\n    self._maybe_find_duplicates()\n    return self._duplicate_of\n\n  @property\n  def duplicates(self):\n    """"""A map from preferred full names to a list of all names for this symbol.\n\n    This function returns a map from preferred (master) name for a symbol to a\n    lexicographically sorted list of all aliases for that name (incl. the master\n    name). Symbols without duplicate names do not appear in this map.\n\n    It is computed when it, `reverse_index`, or `duplicate_of` are first\n    accessed.\n\n    Returns:\n      The map from master name to list of all duplicate names.\n    """"""\n    self._maybe_find_duplicates()\n    return self._duplicates\n\n  def _add_prefix(self, name):\n    """"""Adds the root name to a name.""""""\n    return self._prefix + name if name else self._root_name\n\n  def __call__(self, parent_name, parent, children):\n    """"""Visitor interface, see `tensorflow/tools/common:traverse` for details.\n\n    This method is called for each symbol found in a traversal using\n    `tensorflow/tools/common:traverse`. It should not be called directly in\n    user code.\n\n    Args:\n      parent_name: The fully qualified name of a symbol found during traversal.\n      parent: The Python object referenced by `parent_name`.\n      children: A list of `(name, py_object)` pairs enumerating, in alphabetical\n        order, the children (as determined by `tf_inspect.getmembers`) of\n          `parent`. `name` is the local name of `py_object` in `parent`.\n\n    Raises:\n      RuntimeError: If this visitor is called with a `parent` that is not a\n        class or module.\n    """"""\n    parent_name = self._add_prefix(parent_name)\n    self._index[parent_name] = parent\n    self._tree[parent_name] = []\n\n    if not (tf_inspect.ismodule(parent) or tf_inspect.isclass(parent)):\n      raise RuntimeError(\'Unexpected type in visitor -- %s: %r\' % (parent_name,\n                                                                   parent))\n\n    for i, (name, child) in enumerate(list(children)):\n      # Don\'t document __metaclass__\n      if name in [\'__metaclass__\']:\n        del children[i]\n        continue\n\n      full_name = \'.\'.join([parent_name, name]) if parent_name else name\n      self._index[full_name] = child\n      self._tree[parent_name].append(name)\n\n  def _maybe_find_duplicates(self):\n    """"""Compute data structures containing information about duplicates.\n\n    Find duplicates in `index` and decide on one to be the ""master"" name.\n\n    Computes a reverse_index mapping each object id to its master name.\n\n    Also computes a map `duplicate_of` from aliases to their master name (the\n    master name itself has no entry in this map), and a map `duplicates` from\n    master names to a lexicographically sorted list of all aliases for that name\n    (incl. the master name).\n\n    All these are computed and set as fields if they haven\'t already.\n    """"""\n    if self._reverse_index is not None:\n      return\n\n    # Maps the id of a symbol to its fully qualified name. For symbols that have\n    # several aliases, this map contains the first one found.\n    # We use id(py_object) to get a hashable value for py_object. Note all\n    # objects in _index are in memory at the same time so this is safe.\n    reverse_index = {}\n\n    # Make a preliminary duplicates map. For all sets of duplicate names, it\n    # maps the first name found to a list of all duplicate names.\n    raw_duplicates = {}\n    for full_name, py_object in six.iteritems(self._index):\n      # We cannot use the duplicate mechanism for some constants, since e.g.,\n      # id(c1) == id(c2) with c1=1, c2=1. This is unproblematic since constants\n      # have no usable docstring and won\'t be documented automatically.\n      if (py_object is not None and not\n          isinstance(py_object, six.integer_types + six.string_types +\n                     (six.binary_type, six.text_type, float, complex, bool)) and\n              py_object is not ()):\n        object_id = id(py_object)\n        if object_id in reverse_index:\n          master_name = reverse_index[object_id]\n          if master_name in raw_duplicates:\n            raw_duplicates[master_name].append(full_name)\n          else:\n            raw_duplicates[master_name] = [master_name, full_name]\n        else:\n          reverse_index[object_id] = full_name\n\n    # Decide on master names, rewire duplicates and make a duplicate_of map\n    # mapping all non-master duplicates to the master name. The master symbol\n    # does not have an entry in this map.\n    duplicate_of = {}\n    # Duplicates maps the main symbols to the set of all duplicates of that\n    # symbol (incl. itself).\n    duplicates = {}\n    for names in raw_duplicates.values():\n      names = sorted(names)\n\n      # Choose the lexicographically first name with the minimum number of\n      # submodules. This will prefer highest level namespace for any symbol.\n      master_name = min(names, key=lambda name: name.count(\'.\'))\n\n      duplicates[master_name] = names\n      for name in names:\n        if name != master_name:\n          duplicate_of[name] = master_name\n\n      # Set the reverse index to the canonical name.\n      reverse_index[id(self._index[master_name])] = master_name\n\n    self._duplicate_of = duplicate_of\n    self._duplicates = duplicates\n    self._reverse_index = reverse_index\n'"
docs/parser/generate.py,0,"b'""""""Generate docs for the Edward API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport edward as ed\nimport observations\n\nfrom tensorflow.python import debug as tf_debug\nfrom tensorflow.python.util import tf_inspect\nimport generate_lib\n\nif __name__ == \'__main__\':\n  doc_generator = generate_lib.DocGenerator()\n  doc_generator.add_output_dir_argument()\n  doc_generator.add_src_dir_argument()\n\n  # This doc generator works on the TensorFlow codebase. Since this script lives\n  # at docs/parser, and all code is defined somewhere inside\n  # edward/, we can compute the base directory (two levels up), which is\n  # valid unless we\'re trying to apply this to a different code base, or are\n  # moving the script around.\n  script_dir = os.path.dirname(tf_inspect.getfile(tf_inspect.currentframe()))\n  default_base_dir = os.path.join(script_dir, \'..\', \'..\', \'edward\')\n  doc_generator.add_base_dir_argument(default_base_dir)\n\n  flags = doc_generator.parse_known_args()\n\n  doc_generator.set_py_modules([(\'ed\', ed), (\'observations\', observations)])\n\n  sys.exit(doc_generator.build(flags))\n'"
docs/parser/generate_lib.py,8,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generate docs for the TensorFlow Python API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport six\n\nfrom tensorflow.python.util import tf_inspect\nimport public_api\nimport traverse\nimport doc_generator_visitor\nimport parser\nimport pretty_docs\nimport py_guide_parser\n\n\ndef _is_free_function(py_object, full_name, index):\n  """"""Check if input is a free function (and not a class- or static method).""""""\n  if not tf_inspect.isfunction(py_object):\n    return False\n\n  # Static methods are functions to tf_inspect (in 2.7), so check if the parent\n  # is a class. If there is no parent, it\'s not a function.\n  if \'.\' not in full_name:\n    return False\n\n  parent_name = full_name.rsplit(\'.\', 1)[0]\n  if tf_inspect.isclass(index[parent_name]):\n    return False\n\n  return True\n\n\ndef write_docs(output_dir, parser_config, yaml_toc):\n  """"""Write previously extracted docs to disk.\n\n  Write a docs page for each symbol included in the indices of parser_config to\n  a tree of docs at `output_dir`.\n\n  Symbols with multiple aliases will have only one page written about\n  them, which is referenced for all aliases.\n\n  Args:\n    output_dir: Directory to write documentation markdown files to. Will be\n      created if it doesn\'t exist.\n    parser_config: A `parser.ParserConfig` object, containing all the necessary\n      indices.\n    yaml_toc: Set to `True` to generate a ""_toc.yaml"" file.\n\n  Raises:\n    ValueError: if `output_dir` is not an absolute path\n  """"""\n  # Make output_dir.\n  if not os.path.isabs(output_dir):\n    raise ValueError(\n        ""\'output_dir\' must be an absolute path.\\n""\n        ""    output_dir=\'%s\'"" % output_dir)\n\n  try:\n    if not os.path.exists(output_dir):\n      os.makedirs(output_dir)\n  except OSError as e:\n    print(\'Creating output dir ""%s"" failed: %s\' % (output_dir, e))\n    raise\n\n  # These dictionaries are used for table-of-contents generation below\n  # They will contain, after the for-loop below::\n  #  - module name(string):classes and functions the module contains(list)\n  module_children = {}\n  #  - symbol name(string):pathname (string)\n  symbol_to_file = {}\n\n  # Parse and write Markdown pages, resolving cross-links (@{symbol}).\n  for full_name, py_object in six.iteritems(parser_config.index):\n\n    if full_name in parser_config.duplicate_of:\n      continue\n\n    # Methods and some routines are documented only as part of their class.\n    if not (tf_inspect.ismodule(py_object) or tf_inspect.isclass(py_object) or\n            _is_free_function(py_object, full_name, parser_config.index)):\n      continue\n\n    sitepath = os.path.join(\'api\',\n                            parser.documentation_path(full_name)[:-3])\n\n    # For TOC, we need to store a mapping from full_name to the file\n    # we\'re generating\n    symbol_to_file[full_name] = sitepath\n\n    # For a module, remember the module for the table-of-contents\n    if tf_inspect.ismodule(py_object):\n      if full_name in parser_config.tree:\n        module_children.setdefault(full_name, [])\n\n    # For something else that\'s documented,\n    # figure out what module it lives in\n    else:\n      subname = str(full_name)\n      while True:\n        subname = subname[:subname.rindex(\'.\')]\n        if tf_inspect.ismodule(parser_config.index[subname]):\n          module_children.setdefault(subname, []).append(full_name)\n          break\n\n    print(\'Writing docs for %s (%r).\' % (full_name, py_object))\n\n    # Generate docs for `py_object`, resolving references.\n    page_info = parser.docs_for_object(full_name, py_object, parser_config)\n\n    path = os.path.join(output_dir, parser.documentation_path(full_name))\n    directory = os.path.dirname(path)\n    try:\n      if not os.path.exists(directory):\n        os.makedirs(directory)\n      with open(path, \'w\') as f:\n        f.write(pretty_docs.build_md_page(page_info))\n    except OSError as e:\n      print(\'Cannot write documentation for %s to %s: %s\' % (full_name,\n                                                             directory, e))\n      raise\n\n  if yaml_toc:\n    # Generate table of contents\n\n    # Put modules in alphabetical order, case-insensitive\n    modules = sorted(module_children.keys(), key=lambda a: a.upper())\n\n    leftnav_path = os.path.join(output_dir, \'_toc.yaml\')\n    with open(leftnav_path, \'w\') as f:\n\n      # Generate header\n      f.write(\'# Automatically generated file; please do not edit\\ntoc:\\n\')\n      for module in modules:\n        f.write(\'  - title: \' + module + \'\\n\'\n                \'    section:\\n\' + \'    - title: Overview\\n\' +\n                \'      path: /\' + symbol_to_file[module] +\n                \'\\n\')\n\n        symbols_in_module = module_children.get(module, [])\n        # Sort case-insensitive, if equal sort case sensitive (upper first)\n        symbols_in_module.sort(key=lambda a: (a.upper(), a))\n\n        for full_name in symbols_in_module:\n          f.write(\'    - title: \' + full_name[len(module) + 1:] + \'\\n\'\n                  \'      path: /\' +\n                  symbol_to_file[full_name] + \'\\n\')\n\n  # Write a global index containing all full names with links.\n  # with open(os.path.join(output_dir, \'index.md\'), \'w\') as f:\n  #   f.write(\n  #       parser.generate_global_index(\'Edward\', parser_config.index,\n  #                                    parser_config.reference_resolver))\n\n\ndef add_dict_to_dict(add_from, add_to):\n  for key in add_from:\n    if key in add_to:\n      add_to[key].extend(add_from[key])\n    else:\n      add_to[key] = add_from[key]\n\n\n# Exclude some libaries in contrib from the documentation altogether.\ndef _get_default_private_map():\n  return {}\n\n\n# Exclude members of some libaries.\ndef _get_default_do_not_descend_map():\n  # TODO(wicke): Shrink this list once the modules get sealed.\n  return {\n      \'tf\': [\'cli\', \'lib\', \'wrappers\'],\n      \'tf.contrib\': [\n          \'compiler\',\n          \'factorization\',\n          \'grid_rnn\',\n          \'labeled_tensor\',\n          \'ndlstm\',\n          \'quantization\',\n          \'session_bundle\',\n          \'slim\',\n          \'solvers\',\n          \'specs\',\n          \'tensor_forest\',\n          \'tensorboard\',\n          \'testing\',\n          \'tfprof\',\n      ],\n      \'tf.contrib.bayesflow\': [\n          \'special_math\', \'stochastic_gradient_estimators\',\n          \'stochastic_variables\'\n      ],\n      \'tf.contrib.ffmpeg\': [\'ffmpeg_ops\'],\n      \'tf.contrib.graph_editor\': [\n          \'edit\', \'match\', \'reroute\', \'subgraph\', \'transform\', \'select\', \'util\'\n      ],\n      \'tf.contrib.keras\': [\'api\', \'python\'],\n      \'tf.contrib.layers\': [\'feature_column\', \'summaries\'],\n      \'tf.contrib.learn\': [\n          \'datasets\',\n          \'head\',\n          \'graph_actions\',\n          \'io\',\n          \'models\',\n          \'monitors\',\n          \'ops\',\n          \'preprocessing\',\n          \'utils\',\n      ],\n      \'tf.contrib.util\': [\'loader\'],\n  }\n\n\ndef extract(py_modules, private_map, do_not_descend_map):\n  """"""Extract docs from tf namespace and write them to disk.""""""\n  # Traverse the first module.\n  visitor = doc_generator_visitor.DocGeneratorVisitor(py_modules[0][0])\n  api_visitor = public_api.PublicAPIVisitor(visitor)\n  api_visitor.set_root_name(py_modules[0][0])\n  add_dict_to_dict(private_map, api_visitor.private_map)\n  add_dict_to_dict(do_not_descend_map, api_visitor.do_not_descend_map)\n\n  traverse.traverse(py_modules[0][1], api_visitor)\n\n  # Traverse all py_modules after the first:\n  for module_name, module in py_modules[1:]:\n    visitor.set_root_name(module_name)\n    api_visitor.set_root_name(module_name)\n    traverse.traverse(module, api_visitor)\n\n  return visitor\n\n\nclass _GetMarkdownTitle(py_guide_parser.PyGuideParser):\n  """"""Extract the title from a .md file.""""""\n\n  def __init__(self):\n    self.title = None\n    py_guide_parser.PyGuideParser.__init__(self)\n\n  def process_title(self, _, title):\n    if self.title is None:  # only use the first title\n      self.title = title\n\n\nclass _DocInfo(object):\n  """"""A simple struct for holding a doc\'s url and title.""""""\n\n  def __init__(self, url, title):\n    self.url = url\n    self.title = title\n\n\ndef build_doc_index(src_dir):\n  """"""Build an index from a keyword designating a doc to _DocInfo objects.""""""\n  doc_index = {}\n  if not os.path.isabs(src_dir):\n    raise ValueError(""\'src_dir\' must be an absolute path.\\n""\n                     ""    src_dir=\'%s\'"" % src_dir)\n\n  if not os.path.exists(src_dir):\n    raise ValueError(""\'src_dir\' path must exist.\\n""\n                     ""    src_dir=\'%s\'"" % src_dir)\n\n  for dirpath, _, filenames in os.walk(src_dir):\n    suffix = os.path.relpath(path=dirpath, start=src_dir)\n    for base_name in filenames:\n      if not base_name.endswith(\'.md\'):\n        continue\n      title_parser = _GetMarkdownTitle()\n      title_parser.process(os.path.join(dirpath, base_name))\n      key_parts = os.path.join(suffix, base_name[:-3]).split(\'/\')\n      if key_parts[-1] == \'index\':\n        key_parts = key_parts[:-1]\n      doc_info = _DocInfo(os.path.join(suffix, base_name), title_parser.title)\n      doc_index[key_parts[-1]] = doc_info\n      if len(key_parts) > 1:\n        doc_index[\'/\'.join(key_parts[-2:])] = doc_info\n\n  return doc_index\n\n\nclass _GuideRef(object):\n\n  def __init__(self, base_name, title, section_title, section_tag):\n    self.url = \'api_guides/python/\' + ((\'%s#%s\' % (base_name, section_tag))\n                                       if section_tag else base_name)\n    self.link_text = ((\'%s > %s\' % (title, section_title))\n                      if section_title else title)\n\n  def make_md_link(self, url_prefix):\n    return \'[%s](%s%s)\' % (self.link_text, url_prefix, self.url)\n\n\nclass _GenerateGuideIndex(py_guide_parser.PyGuideParser):\n  """"""Turn guide files into an index from symbol name to a list of _GuideRefs.""""""\n\n  def __init__(self):\n    self.index = {}\n    py_guide_parser.PyGuideParser.__init__(self)\n\n  def process(self, full_path, base_name):\n    """"""Index a file, reading from `full_path`, with `base_name` as the link.""""""\n    self.full_path = full_path\n    self.base_name = base_name\n    self.title = None\n    self.section_title = None\n    self.section_tag = None\n    py_guide_parser.PyGuideParser.process(self, full_path)\n\n  def process_title(self, _, title):\n    if self.title is None:  # only use the first title\n      self.title = title\n\n  def process_section(self, _, section_title, tag):\n    self.section_title = section_title\n    self.section_tag = tag\n\n  def process_line(self, _, line):\n    """"""Index @{symbol} references as in the current file & section.""""""\n    for match in parser.SYMBOL_REFERENCE_RE.finditer(line):\n      val = self.index.get(match.group(1), [])\n      val.append(\n          _GuideRef(self.base_name, self.title, self.section_title,\n                    self.section_tag))\n      self.index[match.group(1)] = val\n\n\ndef _build_guide_index(guide_src_dir):\n  """"""Return dict: symbol name -> _GuideRef from the files in `guide_src_dir`.""""""\n  index_generator = _GenerateGuideIndex()\n  if os.path.exists(guide_src_dir):\n    for full_path, base_name in py_guide_parser.md_files_in_dir(guide_src_dir):\n      index_generator.process(full_path, base_name)\n  return index_generator.index\n\n\nclass _UpdateTags(py_guide_parser.PyGuideParser):\n  """"""Rewrites a Python guide so that each section has an explicit tag.""""""\n\n  def process_section(self, line_number, section_title, tag):\n    self.replace_line(line_number, \'<h2 id=""%s"">%s</h2>\' % (tag, section_title))\n\n\nEXCLUDED = set([\'__init__.py\', \'OWNERS\', \'README.txt\'])\n\n\ndef _other_docs(src_dir, output_dir, reference_resolver):\n  """"""Convert all the files in `src_dir` and write results to `output_dir`.""""""\n  header = \'\'\n\n  # Iterate through all the source files and process them.\n  tag_updater = _UpdateTags()\n  for dirpath, _, filenames in os.walk(src_dir):\n    # How to get from `dirpath` to api/\n    relative_path_to_root = os.path.relpath(\n        path=os.path.join(src_dir, \'api\'), start=dirpath)\n\n    # Make the directory under output_dir.\n    new_dir = os.path.join(output_dir,\n                           os.path.relpath(path=dirpath, start=src_dir))\n    try:\n      if not os.path.exists(new_dir):\n        os.makedirs(new_dir)\n    except OSError as e:\n      print(\'Creating output dir ""%s"" failed: %s\' % (new_dir, e))\n      raise\n\n    for base_name in filenames:\n      if base_name in EXCLUDED:\n        print(\'Skipping excluded file %s...\' % base_name)\n        continue\n      full_in_path = os.path.join(dirpath, base_name)\n      suffix = os.path.relpath(path=full_in_path, start=src_dir)\n      full_out_path = os.path.join(output_dir, suffix)\n      if not base_name.endswith(\'.md\') and not base_name.endswith(\'.tex\'):\n        print(\'Copying non-md/tex file %s...\' % suffix)\n        open(full_out_path, \'w\').write(open(full_in_path).read())\n        continue\n      if dirpath.endswith(\'/api\'):\n        print(\'Processing Python guide %s...\' % base_name)\n        md_string = tag_updater.process(full_in_path)\n      else:\n        print(\'Processing doc %s...\' % suffix)\n        md_string = open(full_in_path).read()\n\n      if base_name.endswith(\'.tex\'):\n        output = reference_resolver.replace_references(md_string,\n                                                       relative_path_to_root,\n                                                       style=\'tex\')\n      else:\n        output = reference_resolver.replace_references(md_string,\n                                                       relative_path_to_root)\n      with open(full_out_path, \'w\') as f:\n        f.write(header + output)\n\n  print(\'Done.\')\n\n\nclass DocGenerator(object):\n  """"""Main entry point for generating docs.""""""\n\n  def __init__(self):\n    self.argument_parser = argparse.ArgumentParser()\n    self._py_modules = None\n    self._private_map = _get_default_private_map()\n    self._do_not_descend_map = _get_default_do_not_descend_map()\n    self.yaml_toc = True\n\n  def add_output_dir_argument(self):\n    self.argument_parser.add_argument(\n        \'--output_dir\',\n        type=str,\n        default=None,\n        required=True,\n        help=\'Directory to write docs to.\')\n\n  def add_src_dir_argument(self):\n    self.argument_parser.add_argument(\n        \'--src_dir\',\n        type=str,\n        default=None,\n        required=True,\n        help=\'Directory with the source docs.\')\n\n  def add_base_dir_argument(self, default_base_dir):\n    self.argument_parser.add_argument(\n        \'--base_dir\',\n        type=str,\n        default=default_base_dir,\n        help=\'Base directory to to strip from file names referenced in docs.\')\n\n  def parse_known_args(self):\n    flags, _ = self.argument_parser.parse_known_args()\n    return flags\n\n  def add_to_private_map(self, d):\n    add_dict_to_dict(d, self._private_map)\n\n  def add_to_do_not_descend_map(self, d):\n    add_dict_to_dict(d, self._do_not_descend_map)\n\n  def set_private_map(self, d):\n    self._private_map = d\n\n  def set_do_not_descend_map(self, d):\n    self._do_not_descend_map = d\n\n  def set_py_modules(self, py_modules):\n    self._py_modules = py_modules\n\n  def py_module_names(self):\n    if self._py_modules is None:\n      raise RuntimeError(\n          \'Must call set_py_modules() before running py_module_names().\')\n    return [name for (name, _) in self._py_modules]\n\n  def make_reference_resolver(self, visitor, doc_index):\n    return parser.ReferenceResolver.from_visitor(\n        visitor, doc_index, py_module_names=self.py_module_names())\n\n  def make_parser_config(self, visitor, reference_resolver, guide_index,\n                         base_dir):\n    return parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates=visitor.duplicates,\n        duplicate_of=visitor.duplicate_of,\n        tree=visitor.tree,\n        index=visitor.index,\n        reverse_index=visitor.reverse_index,\n        guide_index=guide_index,\n        base_dir=base_dir)\n\n  def run_extraction(self):\n    return extract(\n        self._py_modules, self._private_map, self._do_not_descend_map)\n\n  def build(self, flags):\n    """"""Actually build the docs.""""""\n    doc_index = build_doc_index(flags.src_dir)\n    visitor = self.run_extraction()\n    reference_resolver = self.make_reference_resolver(visitor, doc_index)\n\n    guide_index = _build_guide_index(\n        os.path.join(flags.src_dir, \'api\'))\n\n    parser_config = self.make_parser_config(visitor, reference_resolver,\n                                            guide_index, flags.base_dir)\n    output_dir = os.path.join(flags.output_dir, \'api\')\n\n    write_docs(output_dir, parser_config, yaml_toc=self.yaml_toc)\n    _other_docs(flags.src_dir, flags.output_dir, reference_resolver)\n\n    if parser.all_errors:\n      print(\'Errors during processing:\\n  \' + \'\\n  \'.join(parser.all_errors))\n      return 1\n    return 0\n'"
docs/parser/parser.py,10,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Turn Python docstrings into Markdown for TensorFlow documentation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport ast\nimport collections\nimport functools\nimport json\nimport os\nimport re\n\nimport codegen\nimport six\n\nfrom google.protobuf.message import Message as ProtoMessage\nfrom tensorflow.python.util import tf_inspect\n\n\n# A regular expression capturing a python indentifier.\nIDENTIFIER_RE = \'[a-zA-Z_][a-zA-Z0-9_]*\'\n\n# Log of all reported errors\nall_errors = []\n\n\ndef log_error(s):\n  all_errors.append(s)\n  print(\'ERROR:\', s)\n\n\ndef documentation_path(full_name):\n  """"""Returns the file path for the documentation for the given API symbol.\n\n  Given the fully qualified name of a library symbol, compute the path to which\n  to write the documentation for that symbol (relative to a base directory).\n  Documentation files are organized into directories that mirror the python\n  module/class structure.\n\n  Args:\n    full_name: Fully qualified name of a library symbol.\n\n  Returns:\n    The file path to which to write the documentation for `full_name`.\n  """"""\n  dirs = full_name.split(\'.\')\n  return os.path.join(*dirs) + \'.md\'\n\n\ndef _get_raw_docstring(py_object):\n  """"""Get the docs for a given python object.\n\n  Args:\n    py_object: A python object to retrieve the docs for (class, function/method,\n      or module).\n\n  Returns:\n    The docstring, or the empty string if no docstring was found.\n  """"""\n  # For object instances, tf_inspect.getdoc does give us the docstring of their\n  # type, which is not what we want. Only return the docstring if it is useful.\n  if (tf_inspect.isclass(py_object) or tf_inspect.ismethod(py_object) or\n          tf_inspect.isfunction(py_object) or tf_inspect.ismodule(py_object) or\n          isinstance(py_object, property)):\n    return tf_inspect.getdoc(py_object) or \'\'\n  else:\n    return \'\'\n\n\n# A regular expression for capturing a @{symbol} reference.\nSYMBOL_REFERENCE_RE = re.compile(r\'@\\{([^}]+)\\}\')\n\n\nclass ReferenceResolver(object):\n  """"""Class for replacing @{...} references with Markdown links.\n\n  Args:\n    duplicate_of: A map from duplicate names to preferred names of API\n      symbols.\n    doc_index: A `dict` mapping symbol name strings to objects with `url`\n      and `title` fields. Used to resolve @{$doc} references in docstrings.\n    index: A map from all full names to python objects.\n    py_module_names: A list of string names of Python modules.\n  """"""\n\n  def __init__(self, duplicate_of, doc_index, is_class, is_module,\n               py_module_names):\n    self._duplicate_of = duplicate_of\n    self._doc_index = doc_index\n    self._is_class = is_class\n    self._is_module = is_module\n    self._all_names = set(is_class.keys())\n    self._py_module_names = py_module_names\n\n  @classmethod\n  def from_visitor(cls, visitor, doc_index, **kwargs):\n    """"""A factory function for building a ReferenceResolver from a visitor.\n\n    Args:\n      visitor: an instance of `DocGeneratorVisitor`\n      doc_index: a dictionary mapping document names to references objects with\n        ""title"" and ""url"" fields\n      **kwargs: all remaining args are passed to the constructor\n    Returns:\n      an instance of `ReferenceResolver` ()\n    """"""\n    is_class = {\n        name: tf_inspect.isclass(visitor.index[name])\n        for name, obj in visitor.index.items()\n    }\n\n    is_module = {\n        name: tf_inspect.ismodule(visitor.index[name])\n        for name, obj in visitor.index.items()\n    }\n\n    return cls(\n        duplicate_of=visitor.duplicate_of,\n        doc_index=doc_index,\n        is_class=is_class,\n        is_module=is_module,\n        **kwargs)\n\n  @classmethod\n  def from_json_file(cls, filepath, doc_index):\n    with open(filepath) as f:\n      json_dict = json.load(f)\n\n    return cls(doc_index=doc_index, **json_dict)\n\n  def to_json_file(self, filepath):\n    """"""Converts the RefenceResolver to json and writes it to the specified file.\n\n    Args:\n      filepath: The file path to write the json to.\n    """"""\n    json_dict = {}\n    for key, value in self.__dict__.items():\n      # Drop these two fields. `_doc_index` is not serializable. `_all_names` is\n      # generated by the constructor.\n      if key in (\'_doc_index\', \'_all_names\'):\n        continue\n\n      # Strip off any leading underscores on field names as these are not\n      # recognized by the constructor.\n      json_dict[key.lstrip(\'_\')] = value\n\n    with open(filepath, \'w\') as f:\n      json.dump(json_dict, f)\n\n  def replace_references(self, string, relative_path_to_root, style=\'md\'):\n    """"""Replace ""@{symbol}"" references with links to symbol\'s documentation page.\n\n    This functions finds all occurrences of ""@{symbol}"" in `string`\n    and replaces them with markdown links to the documentation page\n    for ""symbol"".\n\n    `relative_path_to_root` is the relative path from the document\n    that contains the ""@{symbol}"" reference to the root of the API\n    documentation that is linked to. If the containing page is part of\n    the same API docset, `relative_path_to_root` can be set to\n    `os.path.dirname(documentation_path(name))`, where `name` is the\n    python name of the object whose documentation page the reference\n    lives on.\n\n    Args:\n      string: A string in which ""@{symbol}"" references should be replaced.\n      relative_path_to_root: The relative path from the containing document to\n        the root of the API documentation that is being linked to.\n\n    Returns:\n      `string`, with ""@{symbol}"" references replaced by Markdown links.\n    """"""\n    return re.sub(SYMBOL_REFERENCE_RE,\n                  lambda match: self._one_ref(match.group(1),\n                                              relative_path_to_root,\n                                              style),\n                  string)\n\n  def python_link(self, link_text, ref_full_name, relative_path_to_root,\n                  code_ref=True, style=\'md\'):\n    """"""Resolve a ""@{python symbol}"" reference to a Markdown link.\n\n    This will pick the canonical location for duplicate symbols.  The\n    input to this function should already be stripped of the \'@\' and\n    \'{}\'.  This function returns a Markdown link. If `code_ref` is\n    true, it is assumed that this is a code reference, so the link\n    text will be rendered as code (using backticks).\n    `link_text` should refer to a library symbol, starting with \'tf.\'.\n\n    Args:\n      link_text: The text of the Markdown link.\n      ref_full_name: The fully qualified name of the symbol to link to.\n      relative_path_to_root: The relative path from the location of the current\n        document to the root of the API documentation.\n      code_ref: If true (the default), put `link_text` in `...`.\n\n    Returns:\n      A markdown link to the documentation page of `ref_full_name`.\n    """"""\n    link = self.reference_to_url(ref_full_name, relative_path_to_root)\n    if code_ref:\n      if style == \'md\':\n        return \'[`%s`](%s)\' % (link_text, link)\n      else:\n        return \'\\href{%s}{\\\\texttt{%s}}\' % (link, link_text.replace(""_"", ""\\_""))\n    else:\n      if style == \'md\':\n        return \'[%s](%s)\' % (link_text, link)\n      else:\n        return \'\\href{%s}{%s}\' % (link, link_text)\n\n  def py_master_name(self, full_name):\n    """"""Return the master name for a Python symbol name.""""""\n    return self._duplicate_of.get(full_name, full_name)\n\n  def reference_to_url(self, ref_full_name, relative_path_to_root):\n    """"""Resolve a ""@{python symbol}"" reference to a relative path.\n\n    The input to this function should already be stripped of the \'@\'\n    and \'{}\', and its output is only the link, not the full Markdown.\n\n    If `ref_full_name` is the name of a class member, method, or property, the\n    link will point to the page of the containing class, and it will include the\n    method name as an anchor. For example, `tf.module.MyClass.my_method` will be\n    translated into a link to\n    `os.join.path(relative_path_to_root, \'tf/module/MyClass.md#my_method\')`.\n\n    Args:\n      ref_full_name: The fully qualified name of the symbol to link to.\n      relative_path_to_root: The relative path from the location of the current\n        document to the root of the API documentation.\n\n    Returns:\n      A relative path that links from the documentation page of `from_full_name`\n      to the documentation page of `ref_full_name`.\n\n    Raises:\n      RuntimeError: If `ref_full_name` is not documented.\n    """"""\n    master_name = self._duplicate_of.get(ref_full_name, ref_full_name)\n\n    # Check whether this link exists\n    if master_name not in self._all_names:\n      # TODO(josh11b): Make error reporting more uniform.\n      print(\'ERROR: Cannot make link to %s (original: %s): Not in index.\' %\n            (master_name, ref_full_name))\n      return \'BROKEN_LINK\'\n\n    # If this is a member of a class, link to the class page with an anchor.\n    ref_path = None\n    if not (self._is_class[master_name] or self._is_module[master_name]):\n      idents = master_name.split(\'.\')\n      if len(idents) > 1:\n        class_name = \'.\'.join(idents[:-1])\n        assert class_name in self._all_names\n        if self._is_class[class_name]:\n          ref_path = documentation_path(class_name) + \'#%s\' % idents[-1]\n\n    if not ref_path:\n      ref_path = documentation_path(master_name)\n\n    # TODO this breaks for files like ed.html and ed/ folder\n    ref_path = ref_path.replace(\'.md\', \'\')\n    return os.path.join(relative_path_to_root, ref_path)\n\n  def _one_ref(self, string, relative_path_to_root, style):\n    """"""Return a link for a single ""@{symbol}"" reference.""""""\n    # Look for link text after $.\n    dollar = string.rfind(\'$\')\n    if dollar > 0:  # Ignore $ in first character\n      link_text = string[dollar + 1:]\n      string = string[:dollar]\n      manual_link_text = True\n    else:\n      link_text = string\n      manual_link_text = False\n\n    # Handle different types of references.\n    if string.startswith(\'$\'):  # Doc reference\n      return self._doc_link(\n          string, link_text, manual_link_text, relative_path_to_root)\n\n    elif string.startswith(\'tensorflow::\'):\n      # C++ symbol\n      return self._cc_link(\n          string, link_text, manual_link_text, relative_path_to_root)\n\n    else:\n      is_python = False\n      for py_module_name in self._py_module_names:\n        if string == py_module_name or string.startswith(py_module_name + \'.\'):\n          is_python = True\n          break\n      if is_python:  # Python symbol\n        return self.python_link(link_text, string, relative_path_to_root,\n                                code_ref=True, style=style)\n\n    # Error!\n    log_error(\'Did not understand ""@{%s}""\' % string)\n    return \'ERROR:%s\' % string\n\n  def _doc_link(self, string, link_text, manual_link_text,\n                relative_path_to_root):\n    """"""Generate a link for a @{$...} reference.""""""\n    string = string[1:]  # remove leading $\n\n    # If string has a #, split that part into `hash_tag`\n    hash_pos = string.find(\'#\')\n    if hash_pos > -1:\n      hash_tag = string[hash_pos:]\n      string = string[:hash_pos]\n    else:\n      hash_tag = \'\'\n\n    if string in self._doc_index:\n      if not manual_link_text:\n        link_text = self._doc_index[string].title\n      url = os.path.normpath(os.path.join(\n          relative_path_to_root, \'../..\', self._doc_index[string].url))\n      return \'[%s](%s%s)\' % (link_text, url, hash_tag)\n    return self._doc_missing(string, hash_tag, link_text, manual_link_text,\n                             relative_path_to_root)\n\n  def _doc_missing(self, string, unused_hash_tag, link_text,\n                   unused_manual_link_text, unused_relative_path_to_root):\n    """"""Generate an error for unrecognized @{$...} references.""""""\n    log_error(\'Handle doc reference ""@{$%s}""\' % string)\n    return link_text\n\n  def _cc_link(self, string, link_text, unused_manual_link_text,\n               relative_path_to_root):\n    """"""Generate a link for a @{tensorflow::...} reference.""""""\n    # TODO(josh11b): Fix this hard-coding of paths.\n    if string == \'tensorflow::ClientSession\':\n      ret = \'class/tensorflow/client-session.md\'\n    elif string == \'tensorflow::Scope\':\n      ret = \'class/tensorflow/scope.md\'\n    elif string == \'tensorflow::Status\':\n      ret = \'class/tensorflow/status.md\'\n    elif string == \'tensorflow::Tensor\':\n      ret = \'class/tensorflow/tensor.md\'\n    elif string == \'tensorflow::ops::Const\':\n      ret = \'namespace/tensorflow/ops.md#const\'\n    else:\n      log_error(\'Handle C++ reference ""@{%s}""\' % string)\n      return \'TODO_C++:%s\' % string\n    # relative_path_to_root gets you to api_docs/python, we go from there\n    # to api_docs/cc, and then add ret.\n    cc_relative_path = os.path.normpath(os.path.join(\n        relative_path_to_root, \'../cc\', ret))\n    return \'[`%s`](%s)\' % (link_text, cc_relative_path)\n\n\n# TODO(aselle): Collect these into a big list for all modules and functions\n# and make a rosetta stone page.\ndef _handle_compatibility(doc):\n  """"""Parse and remove compatibility blocks from the main docstring.\n\n  Args:\n    doc: The docstring that contains compatibility notes""\n\n  Returns:\n    a tuple of the modified doc string and a hash that maps from compatibility\n    note type to the text of the note.\n  """"""\n  compatibility_notes = {}\n  match_compatibility = re.compile(r\'[ \\t]*@compatibility\\((\\w+)\\)\\s*\\n\'\n                                   r\'((?:[^@\\n]*\\n)+)\'\n                                   r\'\\s*@end_compatibility\')\n  for f in match_compatibility.finditer(doc):\n    compatibility_notes[f.group(1)] = f.group(2)\n  return match_compatibility.subn(r\'\', doc)[0], compatibility_notes\n\n\ndef _gen_pairs(items):\n  """"""Given an list of items [a,b,a,b...], generate pairs [(a,b),(a,b)...].\n\n  Args:\n    items: A list of items (length must be even)\n\n  Yields:\n    The original items, in pairs\n  """"""\n  assert len(items) % 2 == 0\n  items = iter(items)\n  while True:\n    yield next(items), next(items)\n\n\nclass _FunctionDetail(collections.namedtuple(\n        \'_FunctionDetail\', [\'keyword\', \'header\', \'items\'])):\n  """"""A simple class to contain function details.\n\n  Composed of a ""keyword"", a possibly empty ""header"" string, and a possibly\n  empty\n  list of key-value pair ""items"".\n  """"""\n  __slots__ = []\n\n  def __str__(self):\n    """"""Return the original string that represents the function detail.""""""\n    parts = [self.keyword + \':\\n\']\n    parts.append(self.header)\n    for key, value in self.items:\n      parts.append(\'  \' + key + \':\')\n      parts.append(value)\n\n    return \'\'.join(parts)\n\n\ndef _parse_function_details(docstring):\n  r""""""Given a docstring, split off the header and parse the function details.\n\n  For example the docstring of tf.nn.relu:\n\n  \'\'\'Computes rectified linear: `max(features, 0)`.\n\n  Args:\n    features: A `Tensor`. Must be one of the following types: `float32`,\n      `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`,\n      `half`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `features`.\n  \'\'\'\n\n  This is parsed, and returned as:\n\n  ```\n  (\'Computes rectified linear: `max(features, 0)`.\\n\\n\', [\n      _FunctionDetail(\n          keyword=\'Args\',\n          header=\'\',\n          items=[\n              (\'features\', \' A `Tensor`. Must be ...\'),\n              (\'name\', \' A name for the operation (optional).\\n\\n\')]),\n      _FunctionDetail(\n          keyword=\'Returns\',\n          header=\'  A `Tensor`. Has the same type as `features`.\',\n          items=[])\n  ])\n  ```\n\n  Args:\n    docstring: The docstring to parse\n\n  Returns:\n    A (header, function_details) pair, where header is a string and\n    function_details is a (possibly empty) list of `_FunctionDetail` objects.\n  """"""\n\n  detail_keywords = \'|\'.join([\n      \'Args\', \'Arguments\', \'Fields\', \'Returns\', \'Yields\', \'Raises\', \'Attributes\'\n  ])\n  tag_re = re.compile(\'(?<=\\n)(\' + detail_keywords + \'):\\n\', re.MULTILINE)\n  parts = tag_re.split(docstring)\n\n  # The first part is the main docstring\n  docstring = parts[0]\n\n  # Everything else alternates keyword-content\n  pairs = list(_gen_pairs(parts[1:]))\n\n  function_details = []\n  item_re = re.compile(r\'^  (\\w+):\', re.MULTILINE)\n\n  for keyword, content in pairs:\n    content = item_re.split(content)\n    header = content[0]\n    items = list(_gen_pairs(content[1:]))\n\n    function_details.append(_FunctionDetail(keyword, header, items))\n\n  return docstring, function_details\n\n\n_DocstringInfo = collections.namedtuple(\'_DocstringInfo\', [\n    \'brief\', \'docstring\', \'function_details\', \'compatibility\'\n])\n\n\ndef _parse_md_docstring(py_object, relative_path_to_root, reference_resolver):\n  """"""Parse the object\'s docstring and return a `_DocstringInfo`.\n\n  This function clears @@\'s from the docstring, and replaces @{} references\n  with markdown links.\n\n  For links within the same set of docs, the `relative_path_to_root` for a\n  docstring on the page for `full_name` can be set to:\n\n  ```python\n  relative_path_to_root = os.path.relpath(\n    path=\'.\', start=os.path.dirname(documentation_path(full_name)) or \'.\')\n  ```\n\n  Args:\n    py_object: A python object to retrieve the docs for (class, function/method,\n      or module).\n    relative_path_to_root: The relative path from the location of the current\n      document to the root of the Python API documentation. This is used to\n      compute links for ""@{symbol}"" references.\n    reference_resolver: An instance of ReferenceResolver.\n\n  Returns:\n    A _DocstringInfo object, all fields will be empty if no docstring was found.\n  """"""\n  # TODO(wicke): If this is a partial, use the .func docstring and add a note.\n  raw_docstring = _get_raw_docstring(py_object)\n\n  raw_docstring = reference_resolver.replace_references(\n      raw_docstring, relative_path_to_root)\n\n  atat_re = re.compile(r\' *@@[a-zA-Z_.0-9]+ *$\')\n  raw_docstring = \'\\n\'.join(\n      line for line in raw_docstring.split(\'\\n\') if not atat_re.match(line))\n\n  docstring, compatibility = _handle_compatibility(raw_docstring)\n  docstring, function_details = _parse_function_details(docstring)\n\n  return _DocstringInfo(\n      docstring.split(\'\\n\')[0], docstring, function_details, compatibility)\n\n\ndef _get_arg_spec(func):\n  """"""Extracts signature information from a function or functools.partial object.\n\n  For functions, uses `tf_inspect.getargspec`. For `functools.partial` objects,\n  corrects the signature of the underlying function to take into account the\n  removed arguments.\n\n  Args:\n    func: A function whose signature to extract.\n\n  Returns:\n    An `ArgSpec` namedtuple `(args, varargs, keywords, defaults)`, as returned\n    by `tf_inspect.getargspec`.\n  """"""\n  # getargspec does not work for functools.partial objects directly.\n  if isinstance(func, functools.partial):\n    argspec = tf_inspect.getargspec(func.func)\n    # Remove the args from the original function that have been used up.\n    first_default_arg = (\n        len(argspec.args or []) - len(argspec.defaults or []))\n    partial_args = len(func.args)\n    argspec_args = []\n\n    if argspec.args:\n      argspec_args = list(argspec.args[partial_args:])\n\n    argspec_defaults = list(argspec.defaults or ())\n    if argspec.defaults and partial_args > first_default_arg:\n      argspec_defaults = list(\n          argspec.defaults[partial_args - first_default_arg:])\n\n    first_default_arg = max(0, first_default_arg - partial_args)\n    for kwarg in (func.keywords or []):\n      if kwarg in (argspec.args or []):\n        i = argspec_args.index(kwarg)\n        argspec_args.pop(i)\n        if i >= first_default_arg:\n          argspec_defaults.pop(i - first_default_arg)\n        else:\n          first_default_arg -= 1\n    return tf_inspect.ArgSpec(args=argspec_args,\n                              varargs=argspec.varargs,\n                              keywords=argspec.keywords,\n                              defaults=tuple(argspec_defaults))\n  else:  # Regular function or method, getargspec will work fine.\n    return tf_inspect.getargspec(func)\n\n\ndef _remove_first_line_indent(string):\n  indent = len(re.match(r\'^\\s*\', string).group(0))\n  return \'\\n\'.join([line[indent:] for line in string.split(\'\\n\')])\n\n\ndef _generate_signature(func, reverse_index):\n  """"""Given a function, returns a list of strings representing its args.\n\n  This function produces a list of strings representing the arguments to a\n  python function. It uses tf_inspect.getargspec, which\n  does not generalize well to Python 3.x, which is more flexible in how *args\n  and **kwargs are handled. This is not a problem in TF, since we have to remain\n  compatible to Python 2.7 anyway.\n\n  This function uses `__name__` for callables if it is available. This can lead\n  to poor results for functools.partial and other callable objects.\n\n  The returned string is Python code, so if it is included in a Markdown\n  document, it should be typeset as code (using backticks), or escaped.\n\n  Args:\n    func: A function, method, or functools.partial to extract the signature for.\n    reverse_index: A map from object ids to canonical full names to use.\n\n  Returns:\n    A list of strings representing the argument signature of `func` as python\n    code.\n  """"""\n\n  args_list = []\n\n  argspec = _get_arg_spec(func)\n  first_arg_with_default = (\n      len(argspec.args or []) - len(argspec.defaults or []))\n\n  # Python documentation skips `self` when printing method signatures.\n  # Note we cannot test for ismethod here since unbound methods do not register\n  # as methods (in Python 3).\n  first_arg = 1 if \'self\' in argspec.args[:1] else 0\n\n  # Add all args without defaults.\n  for arg in argspec.args[first_arg:first_arg_with_default]:\n    args_list.append(arg)\n\n  # Add all args with defaults.\n  if argspec.defaults:\n    try:\n      source = _remove_first_line_indent(tf_inspect.getsource(func))\n      func_ast = ast.parse(source)\n      ast_defaults = func_ast.body[0].args.defaults\n    except IOError:  # If this is a builtin, getsource fails with IOError\n      # If we cannot get the source, assume the AST would be equal to the repr\n      # of the defaults.\n      ast_defaults = [None] * len(argspec.defaults)\n\n    for arg, default, ast_default in zip(argspec.args[first_arg_with_default:],\n                                         argspec.defaults, ast_defaults):\n      if id(default) in reverse_index:\n        default_text = reverse_index[id(default)]\n      elif ast_default is not None:\n        default_text = codegen.to_source(ast_default)\n        if default_text != repr(default):\n          # This may be an internal name. If so, handle the ones we know about.\n          # TODO(wicke): This should be replaced with a lookup in the index.\n          # TODO(wicke): (replace first ident with tf., check if in index)\n          internal_names = {\n              \'ops.GraphKeys\': \'tf.GraphKeys\',\n              \'_ops.GraphKeys\': \'tf.GraphKeys\',\n              \'init_ops.zeros_initializer\': \'tf.zeros_initializer\',\n              \'init_ops.ones_initializer\': \'tf.ones_initializer\',\n              \'saver_pb2.SaverDef\': \'tf.train.SaverDef\',\n          }\n          full_name_re = \'^%s(.%s)+\' % (IDENTIFIER_RE, IDENTIFIER_RE)\n          match = re.match(full_name_re, default_text)\n          if match:\n            lookup_text = default_text\n            for internal_name, public_name in six.iteritems(internal_names):\n              if match.group(0).startswith(internal_name):\n                lookup_text = public_name + default_text[len(internal_name):]\n                break\n            if default_text is lookup_text:\n              print(\'WARNING: Using default arg, failed lookup: %s, repr: %r\' %\n                    (default_text, default))\n            else:\n              default_text = lookup_text\n      else:\n        default_text = repr(default)\n\n      args_list.append(\'%s=%s\' % (arg, default_text))\n\n  # Add *args and *kwargs.\n  if argspec.varargs:\n    args_list.append(\'*\' + argspec.varargs)\n  if argspec.keywords:\n    args_list.append(\'**\' + argspec.keywords)\n\n  return args_list\n\n\ndef _get_guides_markdown(duplicate_names, guide_index, relative_path):\n  all_guides = []\n  for name in duplicate_names:\n    all_guides.extend(guide_index.get(name, []))\n  if not all_guides:\n    return \'\'\n  prefix = \'../\' * (relative_path.count(\'/\') + 3)\n  links = sorted(set([guide_ref.make_md_link(prefix)\n                      for guide_ref in all_guides]))\n  return \'See the guide%s: %s\\n\\n\' % (\n      \'s\' if len(links) > 1 else \'\', \', \'.join(links))\n\n\ndef _get_defining_class(py_class, name):\n  for cls in tf_inspect.getmro(py_class):\n    if name in cls.__dict__:\n      return cls\n  return None\n\n\nclass _LinkInfo(\n    collections.namedtuple(\n        \'_LinkInfo\', [\'short_name\', \'full_name\', \'obj\', \'doc\', \'url\'])):\n\n  __slots__ = []\n\n  def is_link(self):\n    return True\n\n\nclass _OtherMemberInfo(\n    collections.namedtuple(\'_OtherMemberInfo\',\n                           [\'short_name\', \'full_name\', \'obj\', \'doc\'])):\n\n  __slots__ = []\n\n  def is_link(self):\n    return False\n\n\n_PropertyInfo = collections.namedtuple(\n    \'_PropertyInfo\', [\'short_name\', \'full_name\', \'obj\', \'doc\'])\n\n_MethodInfo = collections.namedtuple(\n    \'_MethodInfo\', [\'short_name\', \'full_name\', \'obj\', \'doc\', \'signature\'])\n\n\nclass _FunctionPageInfo(object):\n  """"""Collects docs For a function Page.""""""\n\n  def __init__(self, full_name):\n    self._full_name = full_name\n    self._defined_in = None\n    self._aliases = None\n    self._doc = None\n    self._guides = None\n\n    self._signature = None\n\n  def for_function(self):\n    return True\n\n  def for_class(self):\n    return False\n\n  def for_module(self):\n    return False\n\n  @property\n  def full_name(self):\n    return self._full_name\n\n  @property\n  def short_name(self):\n    return self._full_name.split(\'.\')[-1]\n\n  @property\n  def defined_in(self):\n    return self._defined_in\n\n  def set_defined_in(self, defined_in):\n    assert self.defined_in is None\n    self._defined_in = defined_in\n\n  @property\n  def aliases(self):\n    return self._aliases\n\n  def set_aliases(self, aliases):\n    assert self.aliases is None\n    self._aliases = aliases\n\n  @property\n  def doc(self):\n    return self._doc\n\n  def set_doc(self, doc):\n    assert self.doc is None\n    self._doc = doc\n\n  @property\n  def guides(self):\n    return self._guides\n\n  def set_guides(self, guides):\n    assert self.guides is None\n    self._guides = guides\n\n  @property\n  def signature(self):\n    return self._signature\n\n  def set_signature(self, function, reverse_index):\n    """"""Attach the function\'s signature.\n\n    Args:\n      function: The python function being documented.\n      reverse_index: A map from object ids in the index to full names.\n    """"""\n\n    assert self.signature is None\n    self._signature = _generate_signature(function, reverse_index)\n\n\nclass _ClassPageInfo(object):\n  """"""Collects docs for a class page.\n\n  Attributes:\n    full_name: The fully qualified name of the object at the master\n      location. Aka `master_name`. For example: `tf.nn.sigmoid`.\n    short_name: The last component of the `full_name`. For example: `sigmoid`.\n    defined_in: The path to the file where this object is defined.\n    aliases: The list of all fully qualified names for the locations where the\n      object is visible in the public api. This includes the master location.\n    doc: A `_DocstringInfo` object representing the object\'s docstring (can be\n      created with `_parse_md_docstring`).\n    guides: A markdown string, of back links pointing to the api_guides that\n      reference this object.\n    bases: A list of `_LinkInfo` objects pointing to the docs for the parent\n      classes.\n    properties: A list of `_PropertyInfo` objects documenting the class\'\n      properties (attributes that use `@property`).\n    methods: A list of `_MethodInfo` objects documenting the class\' methods.\n    classes: A list of `_LinkInfo` objects pointing to docs for any nested\n      classes.\n    other_members: A list of `_OtherMemberInfo` objects documenting any other\n      object\'s defined inside the class object (mostly enum style fields).\n  """"""\n\n  def __init__(self, full_name):\n    self._full_name = full_name\n    self._defined_in = None\n    self._aliases = None\n    self._doc = None\n    self._guides = None\n\n    self._bases = None\n    self._properties = []\n    self._methods = []\n    self._classes = []\n    self._other_members = []\n\n  def for_function(self):\n    """"""Returns true if this object documents a function.""""""\n    return False\n\n  def for_class(self):\n    """"""Returns true if this object documents a class.""""""\n    return True\n\n  def for_module(self):\n    """"""Returns true if this object documents a module.""""""\n    return False\n\n  @property\n  def full_name(self):\n    """"""Returns the documented object\'s fully qualified name.""""""\n    return self._full_name\n\n  @property\n  def short_name(self):\n    """"""Returns the documented object\'s short name.""""""\n    return self._full_name.split(\'.\')[-1]\n\n  @property\n  def defined_in(self):\n    """"""Returns the path to the file where the documented object is defined.""""""\n    return self._defined_in\n\n  def set_defined_in(self, defined_in):\n    """"""Sets the `defined_in` path.""""""\n    assert self.defined_in is None\n    self._defined_in = defined_in\n\n  @property\n  def aliases(self):\n    """"""Returns a list of all full names for the documented object.""""""\n    return self._aliases\n\n  def set_aliases(self, aliases):\n    """"""Sets the `aliases` list.\n\n    Args:\n      aliases: A list of strings. Containing all the obejct\'s full names.\n    """"""\n    assert self.aliases is None\n    self._aliases = aliases\n\n  @property\n  def doc(self):\n    """"""Returns a `_DocstringInfo` created from the object\'s docstring.""""""\n    return self._doc\n\n  def set_doc(self, doc):\n    """"""Sets the `doc` field.\n\n    Args:\n      doc: An instance of `_DocstringInfo`.\n    """"""\n    assert self.doc is None\n    self._doc = doc\n\n  @property\n  def guides(self):\n    """"""Returns a markdown string containing backlinks to relevant api_guides.""""""\n    return self._guides\n\n  def set_guides(self, guides):\n    """"""Sets the `guides` field.\n\n    Args:\n      guides: A markdown string containing backlinks to all the api_guides that\n        link to the documented object.\n    """"""\n    assert self.guides is None\n    self._guides = guides\n\n  @property\n  def bases(self):\n    """"""Returns a list of `_LinkInfo` objects pointing to the class\' parents.""""""\n    return self._bases\n\n  def _set_bases(self, relative_path, parser_config):\n    """"""Builds the `bases` attribute, to document this class\' parent-classes.\n\n    This method sets the `bases` to a list of `_LinkInfo` objects point to the\n    doc pages for the class\' parents.\n\n    Args:\n      relative_path: The relative path from the doc this object describes to\n        the documentation root.\n      parser_config: An instance of `ParserConfig`.\n    """"""\n    bases = []\n    obj = parser_config.py_name_to_object(self.full_name)\n    for base in obj.__bases__:\n      base_full_name = parser_config.reverse_index.get(id(base), None)\n      if base_full_name is None:\n        continue\n      base_doc = _parse_md_docstring(base, relative_path,\n                                     parser_config.reference_resolver)\n      base_url = parser_config.reference_resolver.reference_to_url(\n          base_full_name, relative_path)\n\n      link_info = _LinkInfo(short_name=base_full_name.split(\'.\')[-1],\n                            full_name=base_full_name, obj=base,\n                            doc=base_doc, url=base_url)\n      bases.append(link_info)\n\n    self._bases = bases\n\n  @property\n  def properties(self):\n    """"""Returns a list of `_PropertyInfo` describing the class\' properties.""""""\n    return self._properties\n\n  def _add_property(self, short_name, full_name, obj, doc):\n    """"""Adds a `_PropertyInfo` entry to the `properties` list.\n\n    Args:\n      short_name: The property\'s short name.\n      full_name: The property\'s fully qualified name.\n      obj: The property object itself\n      doc: The property\'s parsed docstring, a `_DocstringInfo`.\n    """"""\n    property_info = _PropertyInfo(short_name, full_name, obj, doc)\n    self._properties.append(property_info)\n\n  @property\n  def methods(self):\n    """"""Returns a list of `_MethodInfo` describing the class\' methods.""""""\n    return self._methods\n\n  def _add_method(self, short_name, full_name, obj, doc, signature):\n    """"""Adds a `_MethodInfo` entry to the `methods` list.\n\n    Args:\n      short_name: The method\'s short name.\n      full_name: The method\'s fully qualified name.\n      obj: The method object itself\n      doc: The method\'s parsed docstring, a `_DocstringInfo`\n      signature: The method\'s parsed signature (see: `_generate_signature`)\n    """"""\n    method_info = _MethodInfo(short_name, full_name, obj, doc, signature)\n    self._methods.append(method_info)\n\n  @property\n  def classes(self):\n    """"""Returns a list of `_LinkInfo` pointing to any nested classes.""""""\n    return self._classes\n\n  def _add_class(self, short_name, full_name, obj, doc, url):\n    """"""Adds a `_LinkInfo` for a nested class to `classes` list.\n\n    Args:\n      short_name: The class\' short name.\n      full_name: The class\' fully qualified name.\n      obj: The class object itself\n      doc: The class\' parsed docstring, a `_DocstringInfo`\n      url: A url pointing to where the nested class is documented.\n    """"""\n    page_info = _LinkInfo(short_name, full_name, obj, doc, url)\n\n    self._classes.append(page_info)\n\n  @property\n  def other_members(self):\n    """"""Returns a list of `_OtherMemberInfo` describing any other contents.""""""\n    return self._other_members\n\n  def _add_other_member(self, short_name, full_name, obj, doc):\n    """"""Adds an `_OtherMemberInfo` entry to the `other_members` list.\n\n    Args:\n      short_name: The class\' short name.\n      full_name: The class\' fully qualified name.\n      obj: The class object itself\n      doc: The class\' parsed docstring, a `_DocstringInfo`\n    """"""\n    other_member_info = _OtherMemberInfo(short_name, full_name, obj, doc)\n    self._other_members.append(other_member_info)\n\n  def collect_docs_for_class(self, py_class, parser_config):\n    """"""Collects information necessary specifically for a class\'s doc page.\n\n    Mainly, this is details about the class\'s members.\n\n    Args:\n      py_class: The class object being documented\n      parser_config: An instance of ParserConfig.\n    """"""\n    doc_path = documentation_path(self.full_name)\n    relative_path = os.path.relpath(\n        path=\'.\', start=os.path.dirname(doc_path) or \'.\')\n\n    self._set_bases(relative_path, parser_config)\n\n    for short_name in parser_config.tree[self.full_name]:\n      # Remove builtin members that we never want to document.\n      if short_name in [\'__class__\', \'__base__\', \'__weakref__\', \'__doc__\',\n                        \'__module__\', \'__dict__\', \'__abstractmethods__\',\n                        \'__slots__\', \'__getnewargs__\']:\n        continue\n\n      child_name = \'.\'.join([self.full_name, short_name])\n      child = parser_config.py_name_to_object(child_name)\n\n      # Don\'t document anything that is defined in object or by protobuf.\n      defining_class = _get_defining_class(py_class, short_name)\n      if (defining_class is object or\n          defining_class is type or defining_class is tuple or\n          defining_class is BaseException or defining_class is Exception or\n          # The following condition excludes most protobuf-defined symbols.\n          defining_class and defining_class.__name__ in [\'CMessage\', \'Message\',\n                                                         \'MessageMeta\']):\n        continue\n      # TODO(markdaoust): Add a note in child docs showing the defining class.\n\n      child_doc = _parse_md_docstring(child, relative_path,\n                                      parser_config.reference_resolver)\n\n      if isinstance(child, property):\n        self._add_property(short_name, child_name, child, child_doc)\n\n      elif tf_inspect.isclass(child):\n        if defining_class is None:\n          continue\n        url = parser_config.reference_resolver.reference_to_url(\n            child_name, relative_path)\n        self._add_class(short_name, child_name, child, child_doc, url)\n\n      elif (tf_inspect.ismethod(child) or tf_inspect.isfunction(child) or\n            tf_inspect.isroutine(child)):\n        if defining_class is None:\n          continue\n\n        # Omit methods defined by namedtuple.\n        original_method = defining_class.__dict__[short_name]\n        if (hasattr(original_method, \'__module__\') and\n                (original_method.__module__ or \'\').startswith(\'namedtuple\')):\n          continue\n\n        # Some methods are often overridden without documentation. Because it\'s\n        # obvious what they do, don\'t include them in the docs if there\'s no\n        # docstring.\n        if not child_doc.brief.strip() and short_name in [\n                \'__str__\', \'__repr__\', \'__hash__\', \'__del__\', \'__copy__\']:\n          print(\'Skipping %s, defined in %s, no docstring.\' % (child_name,\n                                                               defining_class))\n          continue\n\n        try:\n          child_signature = _generate_signature(child,\n                                                parser_config.reverse_index)\n        except TypeError:\n          # If this is a (dynamically created) slot wrapper, tf_inspect will\n          # raise typeerror when trying to get to the code. Ignore such\n          # functions.\n          continue\n\n        self._add_method(short_name, child_name, child, child_doc,\n                         child_signature)\n      else:\n        # Exclude members defined by protobuf that are useless\n        if issubclass(py_class, ProtoMessage):\n          if (short_name.endswith(\'_FIELD_NUMBER\') or\n                  short_name in [\'__slots__\', \'DESCRIPTOR\']):\n            continue\n\n        # TODO(wicke): We may want to also remember the object itself.\n        self._add_other_member(short_name, child_name, child, child_doc)\n\n\nclass _ModulePageInfo(object):\n  """"""Collects docs for a module page.""""""\n\n  def __init__(self, full_name):\n    self._full_name = full_name\n    self._defined_in = None\n    self._aliases = None\n    self._doc = None\n    self._guides = None\n\n    self._modules = []\n    self._classes = []\n    self._functions = []\n    self._other_members = []\n\n  def for_function(self):\n    return False\n\n  def for_class(self):\n    return False\n\n  def for_module(self):\n    return True\n\n  @property\n  def full_name(self):\n    return self._full_name\n\n  @property\n  def short_name(self):\n    return self._full_name.split(\'.\')[-1]\n\n  @property\n  def defined_in(self):\n    return self._defined_in\n\n  def set_defined_in(self, defined_in):\n    assert self.defined_in is None\n    self._defined_in = defined_in\n\n  @property\n  def aliases(self):\n    return self._aliases\n\n  def set_aliases(self, aliases):\n    assert self.aliases is None\n    self._aliases = aliases\n\n  @property\n  def doc(self):\n    return self._doc\n\n  def set_doc(self, doc):\n    assert self.doc is None\n    self._doc = doc\n\n  @property\n  def guides(self):\n    return self._guides\n\n  def set_guides(self, guides):\n    assert self.guides is None\n    self._guides = guides\n\n  @property\n  def modules(self):\n    return self._modules\n\n  def _add_module(self, short_name, full_name, obj, doc, url):\n    self._modules.append(_LinkInfo(short_name, full_name, obj, doc, url))\n\n  @property\n  def classes(self):\n    return self._classes\n\n  def _add_class(self, short_name, full_name, obj, doc, url):\n    self._classes.append(_LinkInfo(short_name, full_name, obj, doc, url))\n\n  @property\n  def functions(self):\n    return self._functions\n\n  def _add_function(self, short_name, full_name, obj, doc, url):\n    self._functions.append(_LinkInfo(short_name, full_name, obj, doc, url))\n\n  @property\n  def other_members(self):\n    return self._other_members\n\n  def _add_other_member(self, short_name, full_name, obj, doc):\n    self._other_members.append(\n        _OtherMemberInfo(short_name, full_name, obj, doc))\n\n  def collect_docs_for_module(self, parser_config):\n    """"""Collect information necessary specifically for a module\'s doc page.\n\n    Mainly this is information about the members of the module.\n\n    Args:\n      parser_config: An instance of ParserConfig.\n    """"""\n    relative_path = os.path.relpath(\n        path=\'.\',\n        start=os.path.dirname(documentation_path(self.full_name)) or \'.\')\n\n    member_names = parser_config.tree.get(self.full_name, [])\n    for name in member_names:\n\n      if name in [\'__builtins__\', \'__doc__\', \'__file__\',\n                  \'__name__\', \'__path__\', \'__package__\']:\n        continue\n\n      member_full_name = self.full_name + \'.\' + name if self.full_name else name\n      member = parser_config.py_name_to_object(member_full_name)\n\n      member_doc = _parse_md_docstring(member, relative_path,\n                                       parser_config.reference_resolver)\n\n      url = parser_config.reference_resolver.reference_to_url(\n          member_full_name, relative_path)\n\n      if tf_inspect.ismodule(member):\n        self._add_module(name, member_full_name, member, member_doc, url)\n\n      elif tf_inspect.isclass(member):\n        self._add_class(name, member_full_name, member, member_doc, url)\n\n      elif tf_inspect.isfunction(member):\n        self._add_function(name, member_full_name, member, member_doc, url)\n\n      else:\n        self._add_other_member(name, member_full_name, member, member_doc)\n\n\nclass ParserConfig(object):\n  """"""Stores all indexes required to parse the docs.""""""\n\n  def __init__(self, reference_resolver, duplicates, duplicate_of, tree, index,\n               reverse_index, guide_index, base_dir):\n    """"""Object with the common config for docs_for_object() calls.\n\n    Args:\n      reference_resolver: An instance of ReferenceResolver.\n      duplicates: A `dict` mapping fully qualified names to a set of all\n        aliases of this name. This is used to automatically generate a list of\n        all aliases for each name.\n      duplicate_of: A map from duplicate names to preferred names of API\n        symbols.\n      tree: A `dict` mapping a fully qualified name to the names of all its\n        members. Used to populate the members section of a class or module page.\n      index: A `dict` mapping full names to objects.\n      reverse_index: A `dict` mapping object ids to full names.\n\n      guide_index: A `dict` mapping symbol name strings to objects with a\n        `make_md_link()` method.\n\n      base_dir: A base path that is stripped from file locations written to the\n        docs.\n    """"""\n    self.reference_resolver = reference_resolver\n    self.duplicates = duplicates\n    self.duplicate_of = duplicate_of\n    self.tree = tree\n    self.reverse_index = reverse_index\n    self.index = index\n    self.guide_index = guide_index\n    self.base_dir = base_dir\n    self.defined_in_prefix = \'edward/\'\n    self.code_url_prefix = (\n        \'https://github.com/blei-lab/edward/tree/master/edward/\')\n\n  def py_name_to_object(self, full_name):\n    """"""Return the Python object for a Python symbol name.""""""\n    return self.index[full_name]\n\n\ndef docs_for_object(full_name, py_object, parser_config):\n  """"""Return a PageInfo object describing a given object from the TF API.\n\n  This function uses _parse_md_docstring to parse the docs pertaining to\n  `object`.\n\n  This function resolves \'@{symbol}\' references in the docstrings into links to\n  the appropriate location. It also adds a list of alternative names for the\n  symbol automatically.\n\n  It assumes that the docs for each object live in a file given by\n  `documentation_path`, and that relative links to files within the\n  documentation are resolvable.\n\n  Args:\n    full_name: The fully qualified name of the symbol to be\n      documented.\n    py_object: The Python object to be documented. Its documentation is sourced\n      from `py_object`\'s docstring.\n    parser_config: A ParserConfig object.\n\n  Returns:\n    Either a `_FunctionPageInfo`, `_ClassPageInfo`, or a `_ModulePageInfo`\n    depending on the type of the python object being documented.\n\n  Raises:\n    RuntimeError: If an object is encountered for which we don\'t know how\n      to make docs.\n  """"""\n\n  # Which other aliases exist for the object referenced by full_name?\n  master_name = parser_config.reference_resolver.py_master_name(full_name)\n  duplicate_names = parser_config.duplicates.get(master_name, [full_name])\n\n  # TODO(wicke): Once other pieces are ready, enable this also for partials.\n  if (tf_inspect.ismethod(py_object) or tf_inspect.isfunction(py_object) or\n      # Some methods in classes from extensions come in as routines.\n          tf_inspect.isroutine(py_object)):\n    page_info = _FunctionPageInfo(master_name)\n    page_info.set_signature(py_object, parser_config.reverse_index)\n\n  elif tf_inspect.isclass(py_object):\n    page_info = _ClassPageInfo(master_name)\n    page_info.collect_docs_for_class(py_object, parser_config)\n\n  elif tf_inspect.ismodule(py_object):\n    page_info = _ModulePageInfo(master_name)\n    page_info.collect_docs_for_module(parser_config)\n\n  else:\n    raise RuntimeError(\'Cannot make docs for object %s: %r\' % (full_name,\n                                                               py_object))\n\n  relative_path = os.path.relpath(\n      path=\'.\', start=os.path.dirname(documentation_path(full_name)) or \'.\')\n\n  page_info.set_doc(_parse_md_docstring(\n      py_object, relative_path, parser_config.reference_resolver))\n\n  page_info.set_aliases(duplicate_names)\n\n  page_info.set_guides(_get_guides_markdown(\n      duplicate_names, parser_config.guide_index, relative_path))\n\n  page_info.set_defined_in(_get_defined_in(py_object, parser_config))\n\n  return page_info\n\n\nclass _PythonBuiltin(object):\n  """"""This class indicated that the object in question is a python builtin.\n\n  This can be used for the `defined_in` slot of the `PageInfo` objects.\n  """"""\n\n  def is_builtin(self):\n    return True\n\n  def is_python_file(self):\n    return False\n\n  def is_generated_file(self):\n    return False\n\n  def __str__(self):\n    return \'This is an alias for a Python built-in.\\n\\n\'\n\n\nclass _PythonFile(object):\n  """"""This class indicates that the object is defined in a regular python file.\n\n  This can be used for the `defined_in` slot of the `PageInfo` obejcts.\n  """"""\n\n  def __init__(self, path, parser_config):\n    self.path = path\n    self.path_prefix = parser_config.defined_in_prefix\n    self.code_url_prefix = parser_config.code_url_prefix\n\n  def is_builtin(self):\n    return False\n\n  def is_python_file(self):\n    return True\n\n  def is_generated_file(self):\n    return False\n\n  def __str__(self):\n    return \'Defined in [`{prefix}{path}`]({code_prefix}{path}).\\n\\n\'.format(\n        path=self.path, prefix=self.path_prefix,\n        code_prefix=self.code_url_prefix)\n\n\nclass _ProtoFile(object):\n  """"""This class indicates that the object is defined in a .proto file.\n\n  This can be used for the `defined_in` slot of the `PageInfo` objects.\n  """"""\n\n  def __init__(self, path, parser_config):\n    self.path = path\n    self.path_prefix = parser_config.defined_in_prefix\n    self.code_url_prefix = parser_config.code_url_prefix\n\n  def is_builtin(self):\n    return False\n\n  def is_python_file(self):\n    return False\n\n  def is_generated_file(self):\n    return False\n\n  def __str__(self):\n    return \'Defined in [`{prefix}{path}`]({code_prefix}{path}).\\n\\n\'.format(\n        path=self.path, prefix=self.path_prefix,\n        code_prefix=self.code_url_prefix)\n\n\nclass _GeneratedFile(object):\n  """"""This class indicates that the object is defined in a generated python file.\n\n  Generated files should not be linked to directly.\n\n  This can be used for the `defined_in` slot of the `PageInfo` objects.\n  """"""\n\n  def __init__(self, path, parser_config):\n    self.path = path\n    self.path_prefix = parser_config.defined_in_prefix\n\n  def is_builtin(self):\n    return False\n\n  def is_python_file(self):\n    return False\n\n  def is_generated_file(self):\n    return True\n\n  def __str__(self):\n    return \'Defined in `%s%s`.\\n\\n\' % (self.path_prefix, self.path)\n\n\ndef _get_defined_in(py_object, parser_config):\n  """"""Returns a description of where the passed in python object was defined.\n\n  Arguments:\n    py_object: The Python object.\n    parser_config: A ParserConfig object.\n\n  Returns:\n    Either a `_PythonBuiltin`, `_PythonFile`, or a `_GeneratedFile`\n  """"""\n  # Every page gets a note about where this object is defined\n  # TODO(wicke): If py_object is decorated, get the decorated object instead.\n  # TODO(wicke): Only use decorators that support this in TF.\n\n  try:\n    path = os.path.relpath(path=tf_inspect.getfile(py_object),\n                           start=parser_config.base_dir)\n  except TypeError:  # getfile throws TypeError if py_object is a builtin.\n    return _PythonBuiltin()\n\n  # TODO(wicke): If this is a generated file, link to the source instead.\n  # TODO(wicke): Move all generated files to a generated/ directory.\n  # TODO(wicke): And make their source file predictable from the file name.\n\n  # In case this is compiled, point to the original\n  if path.endswith(\'.pyc\'):\n    path = path[:-1]\n\n  # Never include links outside this code base.\n  if path.startswith(\'..\'):\n    return None\n\n  if re.match(r\'.*/gen_[^/]*\\.py$\', path):\n    return _GeneratedFile(path, parser_config)\n  elif re.match(r\'.*_pb2\\.py$\', path):\n    # The _pb2.py files all appear right next to their defining .proto file.\n    return _ProtoFile(path[:-7] + \'.proto\', parser_config)\n  else:\n    return _PythonFile(path, parser_config)\n\n\n# TODO(markdaoust): This should just parse, pretty_docs should generate the md.\ndef generate_global_index(library_name, index, reference_resolver):\n  """"""Given a dict of full names to python objects, generate an index page.\n\n  The index page generated contains a list of links for all symbols in `index`\n  that have their own documentation page.\n\n  Args:\n    library_name: The name for the documented library to use in the title.\n    index: A dict mapping full names to python objects.\n    reference_resolver: An instance of ReferenceResolver.\n\n  Returns:\n    A string containing an index page as Markdown.\n  """"""\n  symbol_links = []\n  for full_name, py_object in six.iteritems(index):\n    if (tf_inspect.ismodule(py_object) or tf_inspect.isfunction(py_object) or\n            tf_inspect.isclass(py_object)):\n      # In Python 3, unbound methods are functions, so eliminate those.\n      if tf_inspect.isfunction(py_object):\n        if full_name.count(\'.\') == 0:\n          parent_name = \'\'\n        else:\n          parent_name = full_name[:full_name.rfind(\'.\')]\n        if parent_name in index and tf_inspect.isclass(index[parent_name]):\n          # Skip methods (=functions with class parents).\n          continue\n      symbol_links.append((\n          full_name, reference_resolver.python_link(full_name, full_name, \'.\')))\n\n  lines = [\'# All symbols in %s\' % library_name, \'\']\n  for _, link in sorted(symbol_links, key=lambda x: x[0]):\n    lines.append(\'*  %s\' % link)\n\n  # TODO(markdaoust): use a _ModulePageInfo -> prety_docs.build_md_page()\n  return \'\\n\'.join(lines)\n'"
docs/parser/pretty_docs.py,2,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A module for converting parsed doc content into markdown pages.\n\nThe adjacent `parser` module creates `PageInfo` objects, containing all data\nnecessary to document an element of the TensorFlow API.\n\nThis module contains one public function, which handels the conversion of these\n`PageInfo` objects into a markdown string:\n\n    md_page = build_md_page(page_info)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\n\ndef build_md_page(page_info):\n  """"""Given a PageInfo object, return markdown for the page.\n\n  Args:\n    page_info: must be a `parser.FunctionPageInfo`, `parser.ClassPageInfo`, or\n        `parser.ModulePageInfo`\n\n  Returns:\n    Markdown for the page\n\n  Raises:\n    ValueError: if `page_info` is an instance of an unrecognized class\n  """"""\n  if page_info.for_function():\n    return _build_function_page(page_info)\n\n  if page_info.for_class():\n    return _build_class_page(page_info)\n\n  if page_info.for_module():\n    return _build_module_page(page_info)\n\n  raise ValueError(\'Unknown Page Info Type: %s\' % type(page_info))\n\n\ndef _build_function_page(page_info):\n  """"""Given a FunctionPageInfo object Return the page as an md string.""""""\n  parts = [_Metadata(page_info.full_name).build_html()]\n  parts.append(\'# %s\\n\\n\' % page_info.full_name)\n\n  if len(page_info.aliases) > 1:\n    parts.append(\'### Aliases:\\n\\n\')\n    parts.extend(\'* `%s`\\n\' % name for name in page_info.aliases)\n    parts.append(\'\\n\')\n\n  if page_info.signature is not None:\n    parts.append(_build_signature(page_info))\n\n  if page_info.defined_in:\n    parts.append(\'\\n\\n\')\n    parts.append(str(page_info.defined_in))\n\n  parts.append(page_info.guides)\n  parts.append(page_info.doc.docstring)\n  parts.append(_build_function_details(page_info.doc.function_details))\n  parts.append(_build_compatibility(page_info.doc.compatibility))\n\n  return \'\'.join(parts)\n\n\ndef _build_class_page(page_info):\n  """"""Given a ClassPageInfo object Return the page as an md string.""""""\n  meta_data = _Metadata(page_info.full_name)\n  for item in itertools.chain(\n          page_info.classes,\n          page_info.properties,\n          page_info.methods,\n          page_info.other_members):\n    meta_data.append(item)\n\n  parts = [meta_data.build_html()]\n\n  parts.append(\'# {page_info.full_name}\\n\\n\'.format(page_info=page_info))\n\n  parts.append(\'## Class `%s`\\n\\n\' % page_info.full_name.split(\'.\')[-1])\n  if page_info.bases:\n    parts.append(\'Inherits From: \')\n\n    link_template = \'[`{short_name}`]({url})\'\n    parts.append(\', \'.join(\n        link_template.format(**base.__dict__) for base in page_info.bases))\n\n  parts.append(\'\\n\\n\')\n\n  if len(page_info.aliases) > 1:\n    parts.append(\'### Aliases:\\n\\n\')\n    parts.extend(\'* Class `%s`\\n\' % name for name in page_info.aliases)\n    parts.append(\'\\n\')\n\n  if page_info.defined_in is not None:\n    parts.append(\'\\n\\n\')\n    parts.append(str(page_info.defined_in))\n\n  parts.append(page_info.guides)\n  parts.append(page_info.doc.docstring)\n  parts.append(_build_function_details(page_info.doc.function_details))\n  assert not page_info.doc.compatibility\n  parts.append(\'\\n\\n\')\n\n  if page_info.classes:\n    parts.append(\'## Child Classes\\n\')\n\n    link_template = (\'[`class {class_info.short_name}`]\'\n                     \'({class_info.url})\\n\\n\')\n    class_links = sorted(\n        link_template.format(class_info=class_info)\n        for class_info in page_info.classes)\n\n    parts.extend(class_links)\n\n  if page_info.properties:\n    parts.append(\'## Properties\\n\\n\')\n    for prop_info in sorted(page_info.properties):\n      h3 = \'<h3 id=""{short_name}""><code>{short_name}</code></h3>\\n\\n\'\n      parts.append(h3.format(short_name=prop_info.short_name))\n\n      parts.append(prop_info.doc.docstring)\n      parts.append(_build_function_details(prop_info.doc.function_details))\n      assert not prop_info.doc.compatibility\n      parts.append(\'\\n\\n\')\n\n    parts.append(\'\\n\\n\')\n\n  if page_info.methods:\n    parts.append(\'## Methods\\n\\n\')\n    # Sort the methods list, but make sure constructors come first.\n    constructors = [\'__init__\', \'__new__\']\n    inits = [method for method in page_info.methods\n             if method.short_name in constructors]\n    others = [method for method in page_info.methods\n              if method.short_name not in constructors]\n\n    for method_info in sorted(inits) + sorted(others):\n      h3 = (\'<h3 id=""{short_name}"">\'\n            \'<code>{short_name}</code>\'\n            \'</h3>\\n\\n\')\n      parts.append(h3.format(**method_info.__dict__))\n\n      if method_info.signature is not None:\n        parts.append(_build_signature(method_info))\n\n      parts.append(method_info.doc.docstring)\n      parts.append(_build_function_details(method_info.doc.function_details))\n      parts.append(_build_compatibility(method_info.doc.compatibility))\n      parts.append(\'\\n\\n\')\n    parts.append(\'\\n\\n\')\n\n  if page_info.other_members:\n    parts.append(\'## Class Members\\n\\n\')\n\n    # TODO(markdaoust): Document the value of the members,\n    #                   at least for basic types.\n\n    h3 = \'<h3 id=""{short_name}""><code>{short_name}</code></h3>\\n\\n\'\n    others_member_headings = (h3.format(short_name=info.short_name)\n                              for info in sorted(page_info.other_members))\n    parts.extend(others_member_headings)\n\n  return \'\'.join(parts)\n\n\ndef _build_module_page(page_info):\n  """"""Given a ClassPageInfo object Return the page as an md string.""""""\n  meta_data = _Metadata(page_info.full_name)\n\n  # Objects with their own pages are not added to the matadata list for the\n  # module, as the only thing on the module page is a link to the object\'s page.\n  for item in page_info.other_members:\n    meta_data.append(item)\n\n  parts = [meta_data.build_html()]\n\n  parts.append(\n      \'# Module: {full_name}\\n\\n\'.format(full_name=page_info.full_name))\n\n  if len(page_info.aliases) > 1:\n    parts.append(\'### Aliases:\\n\\n\')\n    parts.extend(\'* Module `%s`\\n\' % name for name in page_info.aliases)\n    parts.append(\'\\n\')\n\n  if page_info.defined_in is not None:\n    parts.append(\'\\n\\n\')\n    parts.append(str(page_info.defined_in))\n\n  parts.append(page_info.doc.docstring)\n  parts.append(\'\\n\\n\')\n\n  if page_info.modules:\n    parts.append(\'## Modules\\n\\n\')\n    template = \'[`{short_name}`]({url}) module\'\n\n    for item in page_info.modules:\n      parts.append(template.format(**item.__dict__))\n\n      if item.doc.brief:\n        parts.append(\': \' + item.doc.brief)\n\n      parts.append(\'\\n\\n\')\n\n  if page_info.classes:\n    parts.append(\'## Classes\\n\\n\')\n    template = \'[`class {short_name}`]({url})\'\n\n    for item in page_info.classes:\n      parts.append(template.format(**item.__dict__))\n\n      if item.doc.brief:\n        parts.append(\': \' + item.doc.brief)\n\n      parts.append(\'\\n\\n\')\n\n  if page_info.functions:\n    parts.append(\'## Functions\\n\\n\')\n    template = \'[`{short_name}(...)`]({url})\'\n\n    for item in page_info.functions:\n      parts.append(template.format(**item.__dict__))\n\n      if item.doc.brief:\n        parts.append(\': \' + item.doc.brief)\n\n      parts.append(\'\\n\\n\')\n\n  if page_info.other_members:\n    # TODO(markdaoust): Document the value of the members,\n    #                   at least for basic types.\n    parts.append(\'## Other Members\\n\\n\')\n\n    for item in page_info.other_members:\n      parts.append(\'`{short_name}`\\n\\n\'.format(**item.__dict__))\n\n  return \'\'.join(parts)\n\n\ndef _build_signature(obj_info):\n  """"""Returns a md code block showing the function signature.""""""\n  # Special case tf.range, since it has an optional first argument\n  if obj_info.full_name == \'tf.range\':\n    return (\n        \'``` python\\n\'\n        ""range(limit, delta=1, dtype=None, name=\'range\')\\n""\n        ""range(start, limit, delta=1, dtype=None, name=\'range\')\\n""\n        \'```\\n\\n\')\n\n  signature_template = \'\\n\'.join([\n      \'``` python\',\n      \'{name}({sig})\',\n      \'```\\n\\n\'])\n\n  if not obj_info.signature:\n    sig = \'\'\n  elif len(obj_info.signature) == 1:\n    sig = obj_info.signature[0]\n  else:\n    sig = \',\\n\'.join(\'    %s\' % sig_item for sig_item in obj_info.signature)\n    sig = \'\\n\' + sig + \'\\n\'\n\n  return signature_template.format(name=obj_info.short_name, sig=sig)\n\n\ndef _build_compatibility(compatibility):\n  """"""Return the compatibility section as an md string.""""""\n  parts = []\n  sorted_keys = sorted(compatibility.keys())\n  for key in sorted_keys:\n\n    value = compatibility[key]\n    parts.append(\'\\n\\n#### %s compatibility\\n%s\\n\' % (key, value))\n\n  return \'\'.join(parts)\n\n\ndef _build_function_details(function_details):\n  """"""Return the function details section as an md string.""""""\n  parts = []\n  for detail in function_details:\n    sub = []\n    sub.append(\'#### \' + detail.keyword + \':\\n\\n\')\n    sub.append(detail.header)\n    for key, value in detail.items:\n      sub.append(\'* <b>`%s`</b>:%s\' % (key, value))\n    parts.append(\'\'.join(sub))\n\n  return \'\\n\'.join(parts)\n\n\nclass _Metadata(object):\n  """"""A class for building a page\'s Metadata block.\n\n  Attributes:\n    name: The name of the page being described by the Metadata block.\n  """"""\n\n  def __init__(self, name):\n    """"""Creata a Metadata builder.\n\n    Args:\n      name: The name of the page being described by the Metadata block.\n    """"""\n    self.name = name\n    self._content = []\n\n  def append(self, item):\n    """"""Add an item from the page to the Metadata block.\n\n    Args:\n      item: The parsed page section to add.\n    """"""\n    self._content.append(item.short_name)\n\n  def build_html(self):\n    """"""Return the Metadata block as an Html string.""""""\n    parts = []\n    parts.append(\'---\' + \'\\n\')\n    parts.append(\'pagetitle: {}\'.format(self.name) + \'\\n\')\n    parts.append(\'---\' + \'\\n\')\n\n    schema = \'http://developers.google.com/ReferenceObject\'\n    parts.append(\'<div itemscope itemtype=""%s"">\' % schema)\n\n    parts.append(\'<meta itemprop=""name"" content=""%s"" />\' % self.name)\n    for item in self._content:\n      parts.append(\'<meta itemprop=""property"" content=""%s""/>\' % item)\n\n    parts.extend([\'</div>\' + \'\\n\'])\n\n    return \'\'.join(parts)\n'"
docs/parser/public_api.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Visitor restricting traversal to only the public tensorflow API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nfrom tensorflow.python.util import tf_inspect\n\n\nclass PublicAPIVisitor(object):\n  """"""Visitor to use with `traverse` to visit exactly the public TF API.""""""\n\n  def __init__(self, visitor):\n    """"""Constructor.\n\n    `visitor` should be a callable suitable as a visitor for `traverse`. It will\n    be called only for members of the public TensorFlow API.\n\n    Args:\n      visitor: A visitor to call for the public API.\n    """"""\n    self._visitor = visitor\n    self._root_name = \'tf\'\n\n    # Modules/classes we want to suppress entirely.\n    self._private_map = {\n        # Some implementations have this internal module that we shouldn\'t\n        # expose.\n        \'tf.flags\': [\'cpp_flags\'],\n    }\n\n    # Modules/classes we do not want to descend into if we hit them. Usually,\n    # system modules exposed through platforms for compatibility reasons.\n    # Each entry maps a module path to a name to ignore in traversal.\n    self._do_not_descend_map = {\n        \'tf\': [\n            \'core\',\n            \'examples\',\n            \'flags\',  # Don\'t add flags\n            # TODO(drpng): This can be removed once sealed off.\n            \'platform\',\n            # TODO(drpng): This can be removed once sealed.\n            \'pywrap_tensorflow\',\n            # TODO(drpng): This can be removed once sealed.\n            \'user_ops\',\n            \'python\',\n            \'tools\',\n            \'tensorboard\',\n        ],\n\n        # Everything below here is legitimate.\n        # It\'ll stay, but it\'s not officially part of the API.\n        \'tf.app\': [\'flags\'],\n        # Imported for compatibility between py2/3.\n        \'tf.test\': [\'mock\'],\n    }\n\n  @property\n  def private_map(self):\n    """"""A map from parents to symbols that should not be included at all.\n\n    This map can be edited, but it should not be edited once traversal has\n    begun.\n\n    Returns:\n      The map marking symbols to not include.\n    """"""\n    return self._private_map\n\n  @property\n  def do_not_descend_map(self):\n    """"""A map from parents to symbols that should not be descended into.\n\n    This map can be edited, but it should not be edited once traversal has\n    begun.\n\n    Returns:\n      The map marking symbols to not explore.\n    """"""\n    return self._do_not_descend_map\n\n  def set_root_name(self, root_name):\n    """"""Override the default root name of \'tf\'.""""""\n    self._root_name = root_name\n\n  def _is_private(self, path, name):\n    """"""Return whether a name is private.""""""\n    # TODO(wicke): Find out what names to exclude.\n    return ((path in self._private_map and\n             name in self._private_map[path]) or\n            (name.startswith(\'_\') and not re.match(\'__.*__$\', name) or\n             name in [\'__base__\', \'__class__\']))\n\n  def _do_not_descend(self, path, name):\n    """"""Safely queries if a specific fully qualified name should be excluded.""""""\n    return (path in self._do_not_descend_map and\n            name in self._do_not_descend_map[path])\n\n  def __call__(self, path, parent, children):\n    """"""Visitor interface, see `traverse` for details.""""""\n\n    # Avoid long waits in cases of pretty unambiguous failure.\n    if tf_inspect.ismodule(parent) and len(path.split(\'.\')) > 10:\n      raise RuntimeError(\'Modules nested too deep:\\n%s.%s\\n\\nThis is likely a \'\n                         \'problem with an accidental public import.\' %\n                         (self._root_name, path))\n\n    # Includes self._root_name\n    full_path = \'.\'.join([self._root_name, path]) if path else self._root_name\n\n    # Remove things that are not visible.\n    for name, child in list(children):\n      if self._is_private(full_path, name):\n        children.remove((name, child))\n\n    self._visitor(path, parent, children)\n\n    # Remove things that are visible, but which should not be descended into.\n    for name, child in list(children):\n      if self._do_not_descend(full_path, name):\n        children.remove((name, child))\n'"
docs/parser/py_guide_parser.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Library for operating on Python API Guide files.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\n\n\ndef md_files_in_dir(py_guide_src_dir):\n  """"""Returns a list of filename (full_path, base) pairs for guide files.""""""\n  all_in_dir = [(os.path.join(py_guide_src_dir, f), f)\n                for f in os.listdir(py_guide_src_dir)]\n  return [(full, f) for full, f in all_in_dir\n          if os.path.isfile(full) and f.endswith(\'.md\')]\n\n\nclass PyGuideParser(object):\n  """"""Simple parsing of a guide .md file.\n\n  Descendants can override the process_*() functions (called by process())\n  to either record information from the guide, or call replace_line()\n  to affect the return value of process().\n  """"""\n\n  def __init__(self):\n    self._lines = None\n\n  def process(self, full_path):\n    """"""Read and process the file at `full_path`.""""""\n    md_string = open(full_path).read()\n    self._lines = md_string.split(\'\\n\')\n    seen = set()\n\n    in_blockquote = False\n    for i, line in enumerate(self._lines):\n      if \'```\' in line:\n        in_blockquote = not in_blockquote\n\n      if not in_blockquote and line.startswith(\'# \'):\n        self.process_title(i, line[2:])\n      elif not in_blockquote and line.startswith(\'## \'):\n        section_title = line.strip()[3:]\n        existing_tag = re.search(\' {([^}]+)} *$\', line)\n        if existing_tag:\n          tag = existing_tag.group(1)\n        else:\n          tag = re.sub(\'[^a-zA-Z0-9]+\', \'_\', section_title)\n          if tag in seen:\n            suffix = 0\n            while True:\n              candidate = \'%s_%d\' % (tag, suffix)\n              if candidate not in seen:\n                tag = candidate\n                break\n        seen.add(tag)\n        self.process_section(i, section_title, tag)\n\n      elif in_blockquote:\n        self.process_in_blockquote(i, line)\n      else:\n        self.process_line(i, line)\n\n    ret = \'\\n\'.join(self._lines)\n    self._lines = None\n    return ret\n\n  def replace_line(self, line_number, line):\n    """"""Replace the contents of line numbered `line_number` with `line`.""""""\n    self._lines[line_number] = line\n\n  def process_title(self, line_number, title):\n    pass\n\n  def process_section(self, line_number, section_title, tag):\n    pass\n\n  def process_in_blockquote(self, line_number, line):\n    pass\n\n  def process_line(self, line_number, line):\n    pass\n'"
docs/parser/traverse.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Traversing Python modules and classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom tensorflow.python.util import tf_inspect\n\n__all__ = [\'traverse\']\n\n\ndef _traverse_internal(root, visit, stack, path):\n  """"""Internal helper for traverse.""""""\n\n  # Only traverse modules and classes\n  if not tf_inspect.isclass(root) and not tf_inspect.ismodule(root):\n    return\n\n  try:\n    children = tf_inspect.getmembers(root)\n  except ImportError:\n    # On some Python installations, some modules do not support enumerating\n    # members (six in particular), leading to import errors.\n    children = []\n\n  new_stack = stack + [root]\n  visit(path, root, children)\n  for name, child in children:\n    # Do not descend into built-in modules\n    if tf_inspect.ismodule(\n            child) and child.__name__ in sys.builtin_module_names:\n      continue\n\n    # Break cycles\n    if any(child is item for item in new_stack):  # `in`, but using `is`\n      continue\n\n    child_path = path + \'.\' + name if path else name\n    _traverse_internal(child, visit, new_stack, child_path)\n\n\ndef traverse(root, visit):\n  """"""Recursively enumerate all members of `root`.\n\n  Similar to the Python library function `os.path.walk`.\n\n  Traverses the tree of Python objects starting with `root`, depth first.\n  Parent-child relationships in the tree are defined by membership in modules or\n  classes. The function `visit` is called with arguments\n  `(path, parent, children)` for each module or class `parent` found in the tree\n  of python objects starting with `root`. `path` is a string containing the name\n  with which `parent` is reachable from the current context. For example, if\n  `root` is a local class called `X` which contains a class `Y`, `visit` will be\n  called with `(\'Y\', X.Y, children)`).\n\n  If `root` is not a module or class, `visit` is never called. `traverse`\n  never descends into built-in modules.\n\n  `children`, a list of `(name, object)` pairs are determined by\n  `tf_inspect.getmembers`. To avoid visiting parts of the tree, `children` can\n  be modified in place, using `del` or slice assignment.\n\n  Cycles (determined by reference equality, `is`) stop the traversal. A stack of\n  objects is kept to find cycles. Objects forming cycles may appear in\n  `children`, but `visit` will not be called with any object as `parent` which\n  is already in the stack.\n\n  Traversing system modules can take a long time, it is advisable to pass a\n  `visit` callable which blacklists such modules.\n\n  Args:\n    root: A python object with which to start the traversal.\n    visit: A function taking arguments `(path, parent, children)`. Will be\n      called for each object found in the traversal.\n  """"""\n  _traverse_internal(root, visit, [], \'\')\n'"
edward/criticisms/__init__.py,0,"b'""""""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom edward.criticisms.evaluate import *\nfrom edward.criticisms.ppc import *\nfrom edward.criticisms.ppc_plots import *\n\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n_allowed_symbols = [\n    \'evaluate\',\n    \'ppc\',\n    \'ppc_density_plot\',\n    \'ppc_stat_hist_plot\',\n]\n\nremove_undocumented(__name__, allowed_exception_list=_allowed_symbols)\n'"
edward/criticisms/evaluate.py,95,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.models import RandomVariable\nfrom edward.util import check_data, get_session, compute_multinomial_mode, \\\n    with_binary_averaging\n\ntry:\n  from edward.models import Bernoulli, Binomial, Categorical, \\\n      Multinomial, OneHotCategorical\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\ndef evaluate(metrics, data, n_samples=500, output_key=None, seed=None):\n  """"""Evaluate fitted model using a set of metrics.\n\n  A metric, or scoring rule [@winkler1994evaluating], is a function of\n  observed data under the posterior predictive distribution. For\n  example in supervised metrics such as classification accuracy, the\n  observed data (true output) is compared to the posterior\n  predictive\'s mean (predicted output). In unsupervised metrics such\n  as log-likelihood, the probability of observing the data is\n  calculated under the posterior predictive\'s log-density.\n\n  Args:\n    metrics: list of str and/or (str, params: dict) tuples, str,\n    or (str, params: dict) tuple.\n      List of metrics or a single metric:\n      `\'binary_accuracy\'`,\n      `\'categorical_accuracy\'`,\n      `\'sparse_categorical_accuracy\'`,\n      `\'log_loss\'` or `\'binary_crossentropy\'`,\n      `\'categorical_crossentropy\'`,\n      `\'sparse_categorical_crossentropy\'`,\n      `\'hinge\'`,\n      `\'squared_hinge\'`,\n      `\'mse\'` or `\'MSE\'` or `\'mean_squared_error\'`,\n      `\'mae\'` or `\'MAE\'` or `\'mean_absolute_error\'`,\n      `\'mape\'` or `\'MAPE\'` or `\'mean_absolute_percentage_error\'`,\n      `\'msle\'` or `\'MSLE\'` or `\'mean_squared_logarithmic_error\'`,\n      `\'poisson\'`,\n      `\'cosine\'` or `\'cosine_proximity\'`,\n      `\'log_lik\'` or `\'log_likelihood\'`.\n      In lieu of a metric string, this method also accepts (str, params: dict)\n      tuples; the first element of this tuple is the metric string, and\n      the second is a dict of associated params. At present, this dict only\n      expects one key, `\'average\'`, which stipulates the type of averaging to\n      perform on those metrics that permit binary averaging. Permissible\n      options include: `None`, `\'macro\'` and `\'micro\'`.\n    data: dict.\n      Data to evaluate model with. It binds observed variables (of type\n      `RandomVariable` or `tf.Tensor`) to their realizations (of\n      type `tf.Tensor`). It can also bind placeholders (of type\n      `tf.Tensor`) used in the model to their realizations.\n    n_samples: int.\n      Number of posterior samples for making predictions, using the\n      posterior predictive distribution.\n    output_key: RandomVariable or tf.Tensor.\n      It is the key in `data` which corresponds to the model\'s output.\n    seed: a Python integer. Used to create a random seed for the\n      distribution\n\n  Returns:\n    list of float or float.\n    A list of evaluations or a single evaluation.\n\n  Raises:\n    NotImplementedError.\n    If an input metric does not match an implemented metric in Edward.\n\n  #### Examples\n\n  ```python\n  # build posterior predictive after inference: it is\n  # parameterized by a posterior sample\n  x_post = ed.copy(x, {z: qz, beta: qbeta})\n\n  # log-likelihood performance\n  ed.evaluate(\'log_likelihood\', data={x_post: x_train})\n\n  # classification accuracy\n  # here, `x_ph` is any features the model is defined with respect to,\n  # and `y_post` is the posterior predictive distribution\n  ed.evaluate(\'binary_accuracy\', data={y_post: y_train, x_ph: x_train})\n\n  # mean squared error\n  ed.evaluate(\'mean_squared_error\', data={y: y_data, x: x_data})\n  ```\n\n  # mean squared logarithmic error with `\'micro\'` averaging\n  ed.evaluate((\'mean_squared_logarithmic_error\', {\'average\': \'micro\'}),\n              data={y: y_data, x: x_data})\n  """"""\n  sess = get_session()\n  if isinstance(metrics, str):\n    metrics = [metrics]\n  elif callable(metrics):\n    metrics = [metrics]\n  elif not isinstance(metrics, list):\n    raise TypeError(""metrics must have type str or list, or be callable."")\n\n  check_data(data)\n  if not isinstance(n_samples, int):\n    raise TypeError(""n_samples must have type int."")\n\n  if output_key is None:\n    # Default output_key to the only data key that isn\'t a placeholder.\n    keys = [key for key in six.iterkeys(data) if not\n            isinstance(key, tf.Tensor) or ""Placeholder"" not in key.op.type]\n    if len(keys) == 1:\n      output_key = keys[0]\n    else:\n      raise KeyError(""User must specify output_key."")\n  elif not isinstance(output_key, RandomVariable):\n    raise TypeError(""output_key must have type RandomVariable."")\n\n  # Create feed_dict for data placeholders that the model conditions\n  # on; it is necessary for all session runs.\n  feed_dict = {key: value for key, value in six.iteritems(data)\n               if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type}\n\n  # Form true data.\n  y_true = data[output_key]\n  # Make predictions (if there are any supervised metrics).\n  if metrics != [\'log_lik\'] and metrics != [\'log_likelihood\']:\n    binary_discrete = (Bernoulli, Binomial)\n    categorical_discrete = (Categorical, Multinomial, OneHotCategorical)\n    total_count = sess.run(getattr(output_key, \'total_count\', tf.constant(1.)))\n    if isinstance(output_key, binary_discrete + categorical_discrete):\n      # Average over realizations of their probabilities, then predict\n      # via argmax over probabilities.\n      probs = [sess.run(output_key.probs, feed_dict) for _ in range(n_samples)]\n      probs = np.sum(probs, axis=0) / n_samples\n      if isinstance(output_key, binary_discrete):\n        # make random prediction whenever probs is exactly 0.5\n        random = tf.random_uniform(shape=tf.shape(probs))\n        y_pred = tf.round(tf.where(tf.equal(0.5, probs), random, probs))\n      else:\n        if total_count > 1:\n          mode = compute_multinomial_mode(probs, total_count, seed)\n          if len(output_key.sample_shape):\n            y_pred = tf.reshape(tf.tile(mode, output_key.sample_shape),\n                                [-1, len(probs)])\n          else:\n            y_pred = mode\n        else:\n          y_pred = tf.argmax(probs, len(probs.shape) - 1)\n      probs = tf.constant(probs)\n    else:\n      # Monte Carlo estimate the mean of the posterior predictive.\n      y_pred = [sess.run(output_key, feed_dict) for _ in range(n_samples)]\n      y_pred = tf.cast(tf.add_n(y_pred), y_pred[0].dtype) / \\\n          tf.cast(n_samples, y_pred[0].dtype)\n    if len(y_true.shape) == 0:\n      y_true = tf.expand_dims(y_true, 0)\n      y_pred = tf.expand_dims(y_pred, 0)\n\n  # Evaluate y_true (according to y_pred if supervised) for all metrics.\n  evaluations = []\n  for metric in metrics:\n    if isinstance(metric, tuple):\n      metric, params = metric\n    else:\n      params = {}\n    if metric == \'accuracy\' or metric == \'crossentropy\':\n      # automate binary or sparse cat depending on its support\n      support = sess.run(tf.reduce_max(y_true), feed_dict)\n      if support <= 1:\n        metric = \'binary_\' + metric\n      else:\n        metric = \'sparse_categorical_\' + metric\n\n    if metric == \'binary_accuracy\':\n      evaluations += [binary_accuracy(y_true, y_pred, **params)]\n    elif metric == \'categorical_accuracy\':\n      evaluations += [categorical_accuracy(y_true, y_pred, **params)]\n    elif metric == \'sparse_categorical_accuracy\':\n      evaluations += [sparse_categorical_accuracy(y_true, y_pred, **params)]\n    elif metric == \'log_loss\' or metric == \'binary_crossentropy\':\n      evaluations += [binary_crossentropy(y_true, y_pred, **params)]\n    elif metric == \'categorical_crossentropy\':\n      evaluations += [categorical_crossentropy(y_true, y_pred, **params)]\n    elif metric == \'sparse_categorical_crossentropy\':\n      evaluations += [sparse_categorical_crossentropy(y_true, y_pred, **params)]\n    elif metric == \'multinomial_accuracy\':\n      evaluations += [multinomial_accuracy(y_true, y_pred, **params)]\n    elif metric == \'kl_divergence\':\n      y_true_ = y_true / total_count\n      y_pred_ = probs\n      evaluations += [kl_divergence(y_true_, y_pred_, **params)]\n    elif metric == \'hinge\':\n      evaluations += [hinge(y_true, y_pred, **params)]\n    elif metric == \'squared_hinge\':\n      evaluations += [squared_hinge(y_true, y_pred, **params)]\n    elif (metric == \'mse\' or metric == \'MSE\' or\n          metric == \'mean_squared_error\'):\n      evaluations += [mean_squared_error(y_true, y_pred, **params)]\n    elif (metric == \'mae\' or metric == \'MAE\' or\n          metric == \'mean_absolute_error\'):\n      evaluations += [mean_absolute_error(y_true, y_pred, **params)]\n    elif (metric == \'mape\' or metric == \'MAPE\' or\n          metric == \'mean_absolute_percentage_error\'):\n      evaluations += [mean_absolute_percentage_error(y_true, y_pred, **params)]\n    elif (metric == \'msle\' or metric == \'MSLE\' or\n          metric == \'mean_squared_logarithmic_error\'):\n      evaluations += [mean_squared_logarithmic_error(y_true, y_pred, **params)]\n    elif metric == \'poisson\':\n      evaluations += [poisson(y_true, y_pred, **params)]\n    elif metric == \'cosine\' or metric == \'cosine_proximity\':\n      evaluations += [cosine_proximity(y_true, y_pred, **params)]\n    elif metric == \'log_lik\' or metric == \'log_likelihood\':\n      # Monte Carlo estimate the log-density of the posterior predictive.\n      tensor = tf.reduce_mean(output_key.log_prob(y_true))\n      log_pred = [sess.run(tensor, feed_dict) for _ in range(n_samples)]\n      log_pred = tf.add_n(log_pred) / tf.cast(n_samples, tensor.dtype)\n      evaluations += [log_pred]\n    elif callable(metric):\n      evaluations += [metric(y_true, y_pred, **params)]\n    else:\n      raise NotImplementedError(""Metric is not implemented: {}"".format(metric))\n\n  if len(evaluations) == 1:\n    return sess.run(evaluations[0], feed_dict)\n  else:\n    return sess.run(evaluations, feed_dict)\n\n\n# Classification metrics\n\n\ndef binary_accuracy(y_true, y_pred):\n  """"""Binary prediction accuracy, also known as 0/1-loss.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of 0s and 1s (most generally, any real values a and b).\n    y_pred: tf.Tensor.\n      Tensor of predictions, with same shape as `y_true`.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n\n\ndef categorical_accuracy(y_true, y_pred):\n  """"""Multi-class prediction accuracy. One-hot representation for `y_true`.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of 0s and 1s, where the outermost dimension of size `K`\n      has only one 1 per row.\n    y_pred: tf.Tensor.\n      Tensor of predictions, with shape `y_true.shape[:-1]`. Each\n      entry is an integer {0, 1, ..., K-1}.\n  """"""\n  y_true = tf.cast(tf.argmax(y_true, len(y_true.shape) - 1), tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n\n\ndef sparse_categorical_accuracy(y_true, y_pred):\n  """"""Multi-class prediction accuracy. Label {0, 1, .., K-1}\n  representation for `y_true`.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of integers {0, 1, ..., K-1}.\n    y_pred: tf.Tensor.\n      Tensor of predictions, with same shape as `y_true`.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n\n\n# Classification metrics (with real-valued predictions)\n\n\ndef binary_crossentropy(y_true, y_pred):\n  """"""Binary cross-entropy.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of 0s and 1s.\n    y_pred: tf.Tensor.\n      Tensor of real values (logit probabilities), with same shape as\n      `y_true`.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(\n      tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n\n\ndef categorical_crossentropy(y_true, y_pred):\n  """"""Multi-class cross entropy. One-hot representation for `y_true`.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of 0s and 1s, where the outermost dimension of size K\n      has only one 1 per row.\n    y_pred: tf.Tensor.\n      Tensor of real values (logit probabilities), with same shape as\n      `y_true`. The outermost dimension is the number of classes.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n\n\ndef sparse_categorical_crossentropy(y_true, y_pred):\n  """"""Multi-class cross entropy. Label {0, 1, .., K-1} representation\n  for `y_true.`\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of integers {0, 1, ..., K-1}.\n    y_pred: tf.Tensor.\n      Tensor of real values (logit probabilities), with shape\n      `(y_true.shape, K)`. The outermost dimension is the number of classes.\n  """"""\n  y_true = tf.cast(y_true, tf.int64)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=y_pred, labels=y_true))\n\n\ndef multinomial_accuracy(y_true, y_pred):\n  """"""Multinomial prediction accuracy. `y_true` is a tensor\n  of integers, where the outermost dimension gives a draw\n  from a Multinomial distribution.\n\n  NB: In evaluating the accuracy between two Multinomials\n  results may vary across evaluations. This is because Edward\'s\n  algorithm for computing `y_pred`, i.e. the Multinomial\n  mode, yields variable results if `any(isinstance(p, float)\n  for p in total_count * probs)` (where `probs` is a vector\n  of the predicted Multinomial probabilities).\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n\n\ndef kl_divergence(y_true, y_pred):\n  """"""Kullback-Leibler divergence between two probability distributions. A\n  vector of probabilities for `y_true`.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of real values (probabilities) where the values in each row\n      of the outermost dimension sum to 1.\n    y_pred: tf.Tensor.\n      Same as `y_true`, and with the same shape.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  zeros = tf.zeros(shape=(tf.shape(y_true)))\n  summand = tf.where(tf.equal(y_true, 0.0), zeros,\n                     y_true * (tf.log(y_true) - tf.log(y_pred)))\n  return tf.reduce_sum(summand)\n\n\ndef hinge(y_true, y_pred):\n  """"""Hinge loss.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of 0s and 1s.\n    y_pred: tf.Tensor.\n      Tensor of real values, with same shape as `y_true`.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.maximum(1.0 - y_true * y_pred, 0.0))\n\n\ndef squared_hinge(y_true, y_pred):\n  """"""Squared hinge loss.\n\n  Args:\n    y_true: tf.Tensor.\n      Tensor of 0s and 1s.\n    y_pred: tf.Tensor.\n      Tensor of real values, with same shape as `y_true`.\n  """"""\n  y_true = tf.cast(y_true, tf.float32)\n  y_pred = tf.cast(y_pred, tf.float32)\n  return tf.reduce_mean(tf.square(tf.maximum(1.0 - y_true * y_pred, 0.0)))\n\n\n# Regression metrics\n\n\n@with_binary_averaging\ndef mean_squared_error(y_true, y_pred):\n  """"""Mean squared error loss.\n\n  Args:\n    y_true: tf.Tensor.\n    y_pred: tf.Tensor.\n      Tensors of same shape and type.\n  """"""\n  return tf.reduce_mean(tf.square(y_pred - y_true), axis=-2)\n\n\n@with_binary_averaging\ndef mean_absolute_error(y_true, y_pred):\n  """"""Mean absolute error loss.\n\n  Args:\n    y_true: tf.Tensor.\n    y_pred: tf.Tensor.\n      Tensors of same shape and type.\n  """"""\n  return tf.reduce_mean(tf.abs(y_pred - y_true), axis=-2)\n\n\n@with_binary_averaging\ndef mean_absolute_percentage_error(y_true, y_pred):\n  """"""Mean absolute percentage error loss.\n\n  Args:\n    y_true: tf.Tensor.\n    y_pred: tf.Tensor.\n      Tensors of same shape and type.\n  """"""\n  diff = tf.abs((y_true - y_pred) / tf.clip_by_value(tf.abs(y_true),\n                                                     1e-8, np.inf))\n  return 100.0 * tf.reduce_mean(diff, axis=-2)\n\n\n@with_binary_averaging\ndef mean_squared_logarithmic_error(y_true, y_pred):\n  """"""Mean squared logarithmic error loss.\n\n  Args:\n    y_true: tf.Tensor.\n    y_pred: tf.Tensor.\n      Tensors of same shape and type.\n  """"""\n  first_log = tf.log(tf.clip_by_value(y_pred, 1e-8, np.inf) + 1.0)\n  second_log = tf.log(tf.clip_by_value(y_true, 1e-8, np.inf) + 1.0)\n  return tf.reduce_mean(tf.square(first_log - second_log), axis=-2)\n\n\ndef poisson(y_true, y_pred):\n  """"""Negative Poisson log-likelihood of data `y_true` given predictions\n  `y_pred` (up to proportion).\n\n  Args:\n    y_true: tf.Tensor.\n    y_pred: tf.Tensor.\n      Tensors of same shape and type.\n  """"""\n  return tf.reduce_sum(y_pred - y_true * tf.log(y_pred + 1e-8))\n\n\ndef cosine_proximity(y_true, y_pred):\n  """"""Cosine similarity of two vectors.\n\n  Args:\n    y_true: tf.Tensor.\n    y_pred: tf.Tensor.\n      Tensors of same shape and type.\n  """"""\n  y_true = tf.nn.l2_normalize(y_true, len(y_true.shape) - 1)\n  y_pred = tf.nn.l2_normalize(y_pred, len(y_pred.shape) - 1)\n  return tf.reduce_sum(y_true * y_pred)\n'"
edward/criticisms/ppc.py,9,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.models import RandomVariable\nfrom edward.util import check_data, check_latent_vars, get_session\n\n\ndef ppc(T, data, latent_vars=None, n_samples=100):\n  """"""Posterior predictive check\n  [@rubin1984bayesianly; @meng1994posterior; @gelman1996posterior].\n\n  PPC\'s form an empirical distribution for the predictive discrepancy,\n\n  $p(T\\mid x) = \\int p(T(x^{\\\\text{rep}})\\mid z) p(z\\mid x) dz$\n\n  by drawing replicated data sets $x^{\\\\text{rep}}$ and\n  calculating $T(x^{\\\\text{rep}})$ for each data set. Then it\n  compares it to $T(x)$.\n\n  If `data` is inputted with the prior predictive distribution, then\n  it is a prior predictive check [@box1980sampling].\n\n  Args:\n    T: function.\n      Discrepancy function, which takes a dictionary of data and\n      dictionary of latent variables as input and outputs a `tf.Tensor`.\n    data: dict.\n      Data to compare to. It binds observed variables (of type\n      `RandomVariable` or `tf.Tensor`) to their realizations (of\n      type `tf.Tensor`). It can also bind placeholders (of type\n      `tf.Tensor`) used in the model to their realizations.\n    latent_vars: dict.\n      Collection of random variables (of type `RandomVariable` or\n      `tf.Tensor`) binded to their inferred posterior. This argument\n      is used when the discrepancy is a function of latent variables.\n    n_samples: int.\n      Number of replicated data sets.\n\n  Returns:\n    list of np.ndarray.\n    List containing the reference distribution, which is a NumPy array\n    with `n_samples` elements,\n\n    $(T(x^{{\\\\text{rep}},1}, z^{1}), ...,\n       T(x^{\\\\text{rep,nsamples}}, z^{\\\\text{nsamples}}))$\n\n    and the realized discrepancy, which is a NumPy array with\n    `n_samples` elements,\n\n    $(T(x, z^{1}), ..., T(x, z^{\\\\text{nsamples}})).$\n\n\n  #### Examples\n\n  ```python\n  # build posterior predictive after inference:\n  # it is parameterized by a posterior sample\n  x_post = ed.copy(x, {z: qz, beta: qbeta})\n\n  # posterior predictive check\n  # T is a user-defined function of data, T(data)\n  T = lambda xs, zs: tf.reduce_mean(xs[x_post])\n  ed.ppc(T, data={x_post: x_train})\n\n  # in general T is a discrepancy function of the data (both response and\n  # covariates) and latent variables, T(data, latent_vars)\n  T = lambda xs, zs: tf.reduce_mean(zs[z])\n  ed.ppc(T, data={y_post: y_train, x_ph: x_train},\n         latent_vars={z: qz, beta: qbeta})\n\n  # prior predictive check\n  # run ppc on original x\n  ed.ppc(T, data={x: x_train})\n  ```\n  """"""\n  sess = get_session()\n  if not callable(T):\n    raise TypeError(""T must be a callable function."")\n\n  check_data(data)\n  if latent_vars is None:\n    latent_vars = {}\n\n  check_latent_vars(latent_vars)\n  if not isinstance(n_samples, int):\n    raise TypeError(""n_samples must have type int."")\n\n  # Build replicated latent variables.\n  zrep = {key: tf.convert_to_tensor(value)\n          for key, value in six.iteritems(latent_vars)}\n\n  # Build replicated data.\n  xrep = {x: (x.value() if isinstance(x, RandomVariable) else obs)\n          for x, obs in six.iteritems(data)}\n\n  # Create feed_dict for data placeholders that the model conditions\n  # on; it is necessary for all session runs.\n  feed_dict = {key: value for key, value in six.iteritems(data)\n               if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type}\n\n  # Calculate discrepancy over many replicated data sets and latent\n  # variables.\n  Trep = T(xrep, zrep)\n  Tobs = T(data, zrep)\n  Treps = []\n  Ts = []\n  for _ in range(n_samples):\n    # Take a forward pass (session run) to get new samples for\n    # each calculation of the discrepancy.\n    # Alternatively, we could unroll the graph by registering this\n    # operation `n_samples` times, each for different parent nodes\n    # representing `xrep` and `zrep`. But it\'s expensive.\n    Treps += [sess.run(Trep, feed_dict)]\n    Ts += [sess.run(Tobs, feed_dict)]\n\n  return [np.stack(Treps), np.stack(Ts)]\n'"
edward/criticisms/ppc_plots.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef ppc_density_plot(y, y_rep):\n  """"""Create 1D kernel density plot comparing data to samples from posterior.\n\n  Args:\n    y: np.ndarray.\n      A 1-D NumPy array.\n    y_rep: np.ndarray.\n      A 2-D NumPy array where rows represent different samples from posterior.\n\n  Returns:\n    matplotlib axes\n\n  #### Examples\n\n  ```python\n  import matplotlib.pyplot as plt\n\n  y = np.random.randn(20)\n  y_rep = np.random.randn(20, 20)\n\n  ed.ppc_density_plot(y, y_rep)\n  plt.show()\n  ```\n  """"""\n  import matplotlib.pyplot as plt\n  import seaborn as sns\n  ax = sns.kdeplot(y, color=""maroon"")\n\n  n = y_rep.shape[0]\n\n  for i in range(n):\n    ax = sns.kdeplot(y_rep[i, :], color=""maroon"", alpha=0.2, linewidth=0.8)\n\n  y_line = plt.Line2D([], [], color=\'maroon\', label=\'y\')\n  y_rep_line = plt.Line2D([], [], color=\'maroon\', alpha=0.2, label=\'y_rep\')\n\n  handles = [y_line, y_rep_line]\n  labels = [\'y\', r\'$y_{rep}$\']\n\n  ax.legend(handles, labels)\n\n  return ax\n\n\ndef ppc_stat_hist_plot(y_stats, yrep_stats, stat_name=None, **kwargs):\n  """"""Create histogram plot comparing data to samples from posterior.\n\n  Args:\n    y_stats: float.\n      Float representing statistic value of observed data.\n    yrep_stats: np.ndarray.\n      A 1-D NumPy array.\n    stat_name: string.\n      Optional string value for including statistic name in legend.\n    **kwargs:\n      Keyword arguments used by seaborn.distplot can be given to customize plot.\n\n  Returns:\n    matplotlib axes.\n\n  #### Examples\n\n  ```python\n  import matplotlib.pyplot as plt\n\n  # DATA\n  x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n  # MODEL\n  p = Beta(1.0, 1.0)\n  x = Bernoulli(probs=p, sample_shape=10)\n\n  # INFERENCE\n  qp = Beta(tf.nn.softplus(tf.Variable(tf.random_normal([]))),\n            tf.nn.softplus(tf.Variable(tf.random_normal([]))))\n\n  inference = ed.KLqp({p: qp}, data={x: x_data})\n  inference.run(n_iter=500)\n\n  # CRITICISM\n  x_post = ed.copy(x, {p: qp})\n  y_rep, y = ed.ppc(\n      lambda xs, zs: tf.reduce_mean(tf.cast(xs[x_post], tf.float32)),\n      data={x_post: x_data})\n\n  ed.ppc_stat_hist_plot(\n      y[0], y_rep, stat_name=r\'$T \\equiv$mean\', bins=10)\n  plt.show()\n  ```\n  """"""\n  import matplotlib.pyplot as plt\n  import seaborn as sns\n  ax = sns.distplot(yrep_stats, kde=False, label=r\'$T(y_{rep})$\', **kwargs)\n\n  max_value = ax.get_ylim()[1]\n\n  plt.vlines(y_stats, ymin=0.0, ymax=max_value, label=\'T(y)\')\n\n  if stat_name is not None:\n    plt.legend(title=stat_name)\n  else:\n    plt.legend()\n\n  return ax\n'"
edward/inferences/__init__.py,0,"b'""""""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom edward.inferences.bigan_inference import *\nfrom edward.inferences.conjugacy import *\nfrom edward.inferences.gan_inference import *\nfrom edward.inferences.gibbs import *\nfrom edward.inferences.hmc import *\nfrom edward.inferences.implicit_klqp import *\nfrom edward.inferences.inference import *\nfrom edward.inferences.klpq import *\nfrom edward.inferences.klqp import *\nfrom edward.inferences.laplace import *\nfrom edward.inferences.map import *\nfrom edward.inferences.metropolis_hastings import *\nfrom edward.inferences.monte_carlo import *\nfrom edward.inferences.replica_exchange_mc import *\nfrom edward.inferences.sgld import *\nfrom edward.inferences.sghmc import *\nfrom edward.inferences.variational_inference import *\nfrom edward.inferences.wake_sleep import *\nfrom edward.inferences.wgan_inference import *\n\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n_allowed_symbols = [\n    \'BiGANInference\',\n    \'complete_conditional\',\n    \'GANInference\',\n    \'Gibbs\',\n    \'HMC\',\n    \'ImplicitKLqp\',\n    \'Inference\',\n    \'KLpq\',\n    \'KLqp\',\n    \'ReparameterizationKLqp\',\n    \'ReparameterizationKLKLqp\',\n    \'ReparameterizationEntropyKLqp\',\n    \'ReplicaExchangeMC\',\n    \'ScoreKLqp\',\n    \'ScoreKLKLqp\',\n    \'ScoreEntropyKLqp\',\n    \'ScoreRBKLqp\',\n    \'Laplace\',\n    \'MAP\',\n    \'MetropolisHastings\',\n    \'MonteCarlo\',\n    \'SGLD\',\n    \'SGHMC\',\n    \'VariationalInference\',\n    \'WakeSleep\',\n    \'WGANInference\',\n]\n\nremove_undocumented(__name__, allowed_exception_list=_allowed_symbols)\n'"
edward/inferences/bigan_inference.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.gan_inference import GANInference\nfrom edward.util import get_session\n\n\nclass BiGANInference(GANInference):\n  """"""Adversarially Learned Inference [@dumuolin2017adversarially] or\n  Bidirectional Generative Adversarial Networks [@donahue2017adversarial]\n  for joint learning of generator and inference networks.\n\n  Works for the class of implicit (and differentiable) probabilistic\n  models. These models do not require a tractable density and assume\n  only a program that generates samples.\n\n  #### Notes\n\n  `BiGANInference` matches a mapping from data to latent variables and a\n  mapping from latent variables to data through a joint\n  discriminator.\n\n  In building the computation graph for inference, the\n  discriminator\'s parameters can be accessed with the variable scope\n  ""Disc"".\n  In building the computation graph for inference, the\n  encoder and decoder parameters can be accessed with the variable scope\n  ""Gen"".\n\n  The objective function also adds to itself a summation over all tensors\n  in the `REGULARIZATION_LOSSES` collection.\n\n  #### Examples\n\n  ```python\n  with tf.variable_scope(""Gen""):\n    xf = gen_data(z_ph)\n    zf = gen_latent(x_ph)\n  inference = ed.BiGANInference({z_ph: zf}, {xf: x_ph}, discriminator)\n  ```\n  """"""\n  def __init__(self, latent_vars, data, discriminator):\n    if not callable(discriminator):\n      raise TypeError(""discriminator must be a callable function."")\n\n    self.discriminator = discriminator\n    # call grandparent\'s method; avoid parent (GANInference)\n    super(GANInference, self).__init__(latent_vars, data)\n\n  def build_loss_and_gradients(self, var_list):\n    x_true = list(six.itervalues(self.data))[0]\n    x_fake = list(six.iterkeys(self.data))[0]\n\n    z_true = list(six.iterkeys(self.latent_vars))[0]\n    z_fake = list(six.itervalues(self.latent_vars))[0]\n\n    with tf.variable_scope(""Disc""):\n        # xtzf := x_true, z_fake\n        d_xtzf = self.discriminator(x_true, z_fake)\n    with tf.variable_scope(""Disc"", reuse=True):\n        # xfzt := x_fake, z_true\n        d_xfzt = self.discriminator(x_fake, z_true)\n\n    loss_d = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.ones_like(d_xfzt), logits=d_xfzt) + \\\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.zeros_like(d_xtzf), logits=d_xtzf)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.zeros_like(d_xfzt), logits=d_xfzt) + \\\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.ones_like(d_xtzf), logits=d_xtzf)\n\n    reg_terms_d = tf.losses.get_regularization_losses(scope=""Disc"")\n    reg_terms = tf.losses.get_regularization_losses(scope=""Gen"")\n\n    loss_d = tf.reduce_mean(loss_d) + tf.reduce_sum(reg_terms_d)\n    loss = tf.reduce_mean(loss) + tf.reduce_sum(reg_terms)\n\n    var_list_d = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=""Disc"")\n    var_list = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=""Gen"")\n\n    grads_d = tf.gradients(loss_d, var_list_d)\n    grads = tf.gradients(loss, var_list)\n    grads_and_vars_d = list(zip(grads_d, var_list_d))\n    grads_and_vars = list(zip(grads, var_list))\n    return loss, grads_and_vars, loss_d, grads_and_vars_d\n'"
edward/inferences/gan_inference.py,47,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.variational_inference import VariationalInference\nfrom edward.util import get_session\n\n\nclass GANInference(VariationalInference):\n  """"""Parameter estimation with GAN-style training\n  [@goodfellow2014generative].\n\n  Works for the class of implicit (and differentiable) probabilistic\n  models. These models do not require a tractable density and assume\n  only a program that generates samples.\n\n  #### Notes\n\n  `GANInference` does not support latent variable inference. Note\n  that GAN-style training also samples from the prior: this does not\n  work well for latent variables that are shared across many data\n  points (global variables).\n\n  In building the computation graph for inference, the\n  discriminator\'s parameters can be accessed with the variable scope\n  ""Disc"".\n\n  GANs also only work for one observed random variable in `data`.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n\n  #### Examples\n\n  ```python\n  z = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))\n  x = generative_network(z)\n\n  inference = ed.GANInference({x: x_data}, discriminator)\n  ```\n  """"""\n  def __init__(self, data, discriminator):\n    """"""Create an inference algorithm.\n\n    Args:\n      data: dict.\n        Data dictionary which binds observed variables (of type\n        `RandomVariable` or `tf.Tensor`) to their realizations (of\n        type `tf.Tensor`).  It can also bind placeholders (of type\n        `tf.Tensor`) used in the model to their realizations.\n      discriminator: function.\n        Function (with parameters) to discriminate samples. It should\n        output logit probabilities (real-valued) and not probabilities\n        in $[0, 1]$.\n    """"""\n    if not callable(discriminator):\n      raise TypeError(""discriminator must be a callable function."")\n\n    self.discriminator = discriminator\n    super(GANInference, self).__init__(None, data)\n\n  def initialize(self, optimizer=None, optimizer_d=None,\n                 global_step=None, global_step_d=None, var_list=None,\n                 *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      optimizer: str or tf.train.Optimizer.\n        A TensorFlow optimizer, to use for optimizing the generator\n        objective. Alternatively, one can pass in the name of a\n        TensorFlow optimizer, and default parameters for the optimizer\n        will be used.\n      optimizer_d: str or tf.train.Optimizer.\n        A TensorFlow optimizer, to use for optimizing the discriminator\n        objective. Alternatively, one can pass in the name of a\n        TensorFlow optimizer, and default parameters for the optimizer\n        will be used.\n      global_step: tf.Variable.\n        Optional `Variable` to increment by one after the variables\n        for the generator have been updated. See\n        `tf.train.Optimizer.apply_gradients`.\n      global_step_d: tf.Variable.\n        Optional `Variable` to increment by one after the variables\n        for the discriminator have been updated. See\n        `tf.train.Optimizer.apply_gradients`.\n      var_list: list of tf.Variable.\n        List of TensorFlow variables to optimize over (in the generative\n        model). Default is all trainable variables that `latent_vars`\n        and `data` depend on.\n    """"""\n    # call grandparent\'s method; avoid parent (VariationalInference)\n    super(VariationalInference, self).initialize(*args, **kwargs)\n\n    self.loss, grads_and_vars, self.loss_d, grads_and_vars_d = \\\n        self.build_loss_and_gradients(var_list)\n\n    optimizer, global_step = _build_optimizer(optimizer, global_step)\n    optimizer_d, global_step_d = _build_optimizer(optimizer_d, global_step_d)\n\n    self.train = optimizer.apply_gradients(grads_and_vars,\n                                           global_step=global_step)\n    self.train_d = optimizer_d.apply_gradients(grads_and_vars_d,\n                                               global_step=global_step_d)\n\n    if self.logging:\n      tf.summary.scalar(""loss"", self.loss,\n                        collections=[self._summary_key])\n      tf.summary.scalar(""loss/discriminative"", self.loss_d,\n                        collections=[self._summary_key])\n      self.summarize = tf.summary.merge_all(key=self._summary_key)\n\n  def build_loss_and_gradients(self, var_list):\n    x_true = list(six.itervalues(self.data))[0]\n    x_fake = list(six.iterkeys(self.data))[0]\n    with tf.variable_scope(""Disc""):\n      d_true = self.discriminator(x_true)\n\n    with tf.variable_scope(""Disc"", reuse=True):\n      d_fake = self.discriminator(x_fake)\n\n    if self.logging:\n      tf.summary.histogram(""discriminator_outputs"",\n                           tf.concat([d_true, d_fake], axis=0),\n                           collections=[self._summary_key])\n\n    reg_terms_d = tf.losses.get_regularization_losses(scope=""Disc"")\n    reg_terms_all = tf.losses.get_regularization_losses()\n    reg_terms = [r for r in reg_terms_all if r not in reg_terms_d]\n\n    loss_d = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.ones_like(d_true), logits=d_true) + \\\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.zeros_like(d_fake), logits=d_fake)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.ones_like(d_fake), logits=d_fake)\n    loss_d = tf.reduce_mean(loss_d) + tf.reduce_sum(reg_terms_d)\n    loss = tf.reduce_mean(loss) + tf.reduce_sum(reg_terms)\n\n    var_list_d = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=""Disc"")\n    if var_list is None:\n      var_list = [v for v in tf.trainable_variables() if v not in var_list_d]\n\n    grads_d = tf.gradients(loss_d, var_list_d)\n    grads = tf.gradients(loss, var_list)\n    grads_and_vars_d = list(zip(grads_d, var_list_d))\n    grads_and_vars = list(zip(grads, var_list))\n    return loss, grads_and_vars, loss_d, grads_and_vars_d\n\n  def update(self, feed_dict=None, variables=None):\n    """"""Run one iteration of optimization.\n\n    Args:\n      feed_dict: dict.\n        Feed dictionary for a TensorFlow session run. It is used to feed\n        placeholders that are not fed during initialization.\n      variables: str.\n        Which set of variables to update. Either ""Disc"" or ""Gen"".\n        Default is both.\n\n    Returns:\n      dict.\n      Dictionary of algorithm-specific information. In this case, the\n      iteration number and generative and discriminative losses.\n\n    #### Notes\n\n    The outputted iteration number is the total number of calls to\n    `update`. Each update may include updating only a subset of\n    parameters.\n    """"""\n    if feed_dict is None:\n      feed_dict = {}\n\n    for key, value in six.iteritems(self.data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        feed_dict[key] = value\n\n    sess = get_session()\n    if variables is None:\n      _, _, t, loss, loss_d = sess.run(\n          [self.train, self.train_d, self.increment_t, self.loss, self.loss_d],\n          feed_dict)\n    elif variables == ""Gen"":\n      _, t, loss = sess.run(\n          [self.train, self.increment_t, self.loss], feed_dict)\n      loss_d = 0.0\n    elif variables == ""Disc"":\n      _, t, loss_d = sess.run(\n          [self.train_d, self.increment_t, self.loss_d], feed_dict)\n      loss = 0.0\n    else:\n      raise NotImplementedError(""variables must be None, \'Gen\', or \'Disc\'."")\n\n    if self.debug:\n      sess.run(self.op_check, feed_dict)\n\n    if self.logging and self.n_print != 0:\n      if t == 1 or t % self.n_print == 0:\n        summary = sess.run(self.summarize, feed_dict)\n        self.train_writer.add_summary(summary, t)\n\n    return {\'t\': t, \'loss\': loss, \'loss_d\': loss_d}\n\n  def print_progress(self, info_dict):\n    """"""Print progress to output.\n    """"""\n    if self.n_print != 0:\n      t = info_dict[\'t\']\n      if t == 1 or t % self.n_print == 0:\n        self.progbar.update(t, {\'Gen Loss\': info_dict[\'loss\'],\n                                \'Disc Loss\': info_dict[\'loss_d\']})\n\n\ndef _build_optimizer(optimizer, global_step):\n  if optimizer is None and global_step is None:\n    # Default optimizer always uses a global step variable.\n    global_step = tf.Variable(0, trainable=False, name=""global_step"")\n\n  if isinstance(global_step, tf.Variable):\n    starter_learning_rate = 0.1\n    learning_rate = tf.train.exponential_decay(starter_learning_rate,\n                                               global_step,\n                                               100, 0.9, staircase=True)\n  else:\n    learning_rate = 0.01\n\n  # Build optimizer.\n  if optimizer is None:\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n  elif isinstance(optimizer, str):\n    if optimizer == \'gradientdescent\':\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    elif optimizer == \'adadelta\':\n      optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n    elif optimizer == \'adagrad\':\n      optimizer = tf.train.AdagradOptimizer(learning_rate)\n    elif optimizer == \'momentum\':\n      optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n    elif optimizer == \'adam\':\n      optimizer = tf.train.AdamOptimizer(learning_rate)\n    elif optimizer == \'ftrl\':\n      optimizer = tf.train.FtrlOptimizer(learning_rate)\n    elif optimizer == \'rmsprop\':\n      optimizer = tf.train.RMSPropOptimizer(learning_rate)\n    else:\n      raise ValueError(\'Optimizer class not found:\', optimizer)\n  elif not isinstance(optimizer, tf.train.Optimizer):\n    raise TypeError(""Optimizer must be str, tf.train.Optimizer, or None."")\n\n  return optimizer, global_step\n'"
edward/inferences/gibbs.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nimport six\nimport tensorflow as tf\n\nfrom collections import OrderedDict\nfrom edward.inferences.conjugacy import complete_conditional\nfrom edward.inferences.monte_carlo import MonteCarlo\nfrom edward.models import RandomVariable\nfrom edward.util import check_latent_vars, get_session\n\n\nclass Gibbs(MonteCarlo):\n  """"""Gibbs sampling [@geman1984stochastic].\n\n  Note `Gibbs` assumes the proposal distribution has the same\n  support as the prior. The `auto_transform` attribute in\n  the method `initialize()` is not applicable.\n\n  #### Examples\n\n  ```python\n  x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n  p = Beta(1.0, 1.0)\n  x = Bernoulli(probs=p, sample_shape=10)\n\n  qp = Empirical(tf.Variable(tf.zeros(500)))\n  inference = ed.Gibbs({p: qp}, data={x: x_data})\n  ```\n  """"""\n  def __init__(self, latent_vars, proposal_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      proposal_vars: dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on; each is\n        binded to its complete conditionals which Gibbs cycles draws on.\n        If not specified, default is to use `ed.complete_conditional`.\n    """"""\n    super(Gibbs, self).__init__(latent_vars, data)\n\n    if proposal_vars is None:\n      proposal_vars = {z: complete_conditional(z)\n                       for z in six.iterkeys(self.latent_vars)}\n    else:\n      check_latent_vars(proposal_vars)\n\n    self.proposal_vars = proposal_vars\n\n  def initialize(self, scan_order=\'random\', *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      scan_order: list or str.\n        The scan order for each Gibbs update. If list, it is the\n        deterministic order of latent variables. An element in the list\n        can be a `RandomVariable` or itself a list of\n        `RandomVariable`s (this defines a blocked Gibbs sampler). If\n        \'random\', will use a random order at each update.\n    """"""\n    self.scan_order = scan_order\n    self.feed_dict = {}\n    kwargs[\'auto_transform\'] = False\n    return super(Gibbs, self).initialize(*args, **kwargs)\n\n  def update(self, feed_dict=None):\n    """"""Run one iteration of sampling.\n\n    Args:\n      feed_dict: dict.\n        Feed dictionary for a TensorFlow session run. It is used to feed\n        placeholders that are not fed during initialization.\n\n    Returns:\n      dict.\n      Dictionary of algorithm-specific information. In this case, the\n      acceptance rate of samples since (and including) this iteration.\n    """"""\n    sess = get_session()\n    if not self.feed_dict:\n      # Initialize feed for all conditionals to be the draws at step 0.\n      samples = OrderedDict(self.latent_vars)\n      inits = sess.run([qz.params[0] for qz in six.itervalues(samples)])\n      for z, init in zip(six.iterkeys(samples), inits):\n        self.feed_dict[z] = init\n\n      for key, value in six.iteritems(self.data):\n        if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n          self.feed_dict[key] = value\n        elif isinstance(key, RandomVariable) and \\\n                isinstance(value, (tf.Tensor, tf.Variable)):\n          self.feed_dict[key] = sess.run(value)\n\n    if feed_dict is None:\n      feed_dict = {}\n\n    self.feed_dict.update(feed_dict)\n\n    # Determine scan order.\n    if self.scan_order == \'random\':\n      scan_order = list(six.iterkeys(self.latent_vars))\n      random.shuffle(scan_order)\n    else:  # list\n      scan_order = self.scan_order\n\n    # Fetch samples by iterating over complete conditional draws.\n    for z in scan_order:\n      if isinstance(z, RandomVariable):\n        draw = sess.run(self.proposal_vars[z], self.feed_dict)\n        self.feed_dict[z] = draw\n      else:  # list\n        draws = sess.run([self.proposal_vars[zz] for zz in z], self.feed_dict)\n        for zz, draw in zip(z, draws):\n          self.feed_dict[zz] = draw\n\n    # Assign the samples to the Empirical random variables.\n    _, accept_rate = sess.run(\n        [self.train, self.n_accept_over_t], self.feed_dict)\n    t = sess.run(self.increment_t)\n\n    if self.debug:\n      sess.run(self.op_check, self.feed_dict)\n\n    if self.logging and self.n_print != 0:\n      if t == 1 or t % self.n_print == 0:\n        summary = sess.run(self.summarize, self.feed_dict)\n        self.train_writer.add_summary(summary, t)\n\n    return {\'t\': t, \'accept_rate\': accept_rate}\n\n  def build_update(self):\n    """"""\n    #### Notes\n\n    The updates assume each Empirical random variable is directly\n    parameterized by `tf.Variable`s.\n    """"""\n    # Update Empirical random variables according to the complete\n    # conditionals. We will feed the conditionals when calling `update()`.\n    assign_ops = []\n    for z, qz in six.iteritems(self.latent_vars):\n      variable = qz.get_variables()[0]\n      assign_ops.append(\n          tf.scatter_update(variable, self.t, self.proposal_vars[z]))\n\n    # Increment n_accept (if accepted).\n    assign_ops.append(self.n_accept.assign_add(1))\n    return tf.group(*assign_ops)\n'"
edward/inferences/hmc.py,22,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom collections import OrderedDict\nfrom edward.inferences.monte_carlo import MonteCarlo\nfrom edward.models import RandomVariable\nfrom edward.util import copy\n\n\nclass HMC(MonteCarlo):\n  """"""Hamiltonian Monte Carlo, also known as hybrid Monte Carlo\n  [@duane1987hybrid; @neal2011mcmc].\n\n  #### Notes\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$.\n  `HMC` substitutes the model\'s log marginal density\n\n  $\\log p(x, z) = \\log \\mathbb{E}_{q(\\\\beta)} [ p(x, z, \\\\beta) ]\n                \\\\approx \\log p(x, z, \\\\beta^*)$\n\n  leveraging a single Monte Carlo sample, where $\\\\beta^* \\sim\n  q(\\\\beta)$. This is unbiased (and therefore asymptotically exact as a\n  pseudo-marginal method) if $q(\\\\beta) = p(\\\\beta \\mid x)$.\n\n  #### Examples\n\n  ```python\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=10)\n\n  qmu = Empirical(tf.Variable(tf.zeros(500)))\n  inference = ed.HMC({mu: qmu}, {x: np.zeros(10, dtype=np.float32)})\n  ```\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(HMC, self).__init__(*args, **kwargs)\n\n  def initialize(self, step_size=0.25, n_steps=2, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      step_size: float.\n        Step size of numerical integrator.\n      n_steps: int.\n        Number of steps of numerical integrator.\n    """"""\n    self.step_size = step_size\n    self.n_steps = n_steps\n    # store global scope for log joint calculations\n    self._scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n    return super(HMC, self).initialize(*args, **kwargs)\n\n  def build_update(self):\n    """"""Simulate Hamiltonian dynamics using a numerical integrator.\n    Correct for the integrator\'s discretization error using an\n    acceptance ratio.\n\n    #### Notes\n\n    The updates assume each Empirical random variable is directly\n    parameterized by `tf.Variable`s.\n    """"""\n\n    # Gather the initial state, transformed to unconstrained space.\n    try:\n      self.latent_vars_unconstrained\n    except:\n      raise ValueError(""This implementation of HMC requires that all ""\n                       ""variables have unconstrained support. Please ""\n                       ""initialize with auto_transform=True to ensure ""\n                       ""this. (if your variables already have unconstrained ""\n                       ""support then doing this is a no-op)."")\n    old_sample = {z_unconstrained:\n                  tf.gather(qz_unconstrained.params, tf.maximum(self.t - 1, 0))\n                  for z_unconstrained, qz_unconstrained in\n                  six.iteritems(self.latent_vars_unconstrained)}\n    old_sample = OrderedDict(old_sample)\n\n    # Sample momentum.\n    old_r_sample = OrderedDict()\n    for z, qz in six.iteritems(self.latent_vars_unconstrained):\n      event_shape = qz.event_shape\n      old_r_sample[z] = tf.random_normal(event_shape, dtype=qz.dtype)\n\n    # Simulate Hamiltonian dynamics.\n    new_sample, new_r_sample = leapfrog(old_sample, old_r_sample,\n                                        self.step_size,\n                                        self._log_joint_unconstrained,\n                                        self.n_steps)\n\n    # Calculate acceptance ratio.\n    ratio = tf.reduce_sum([0.5 * tf.reduce_sum(tf.square(r))\n                           for r in six.itervalues(old_r_sample)])\n    ratio -= tf.reduce_sum([0.5 * tf.reduce_sum(tf.square(r))\n                            for r in six.itervalues(new_r_sample)])\n    ratio += self._log_joint_unconstrained(new_sample)\n    ratio -= self._log_joint_unconstrained(old_sample)\n\n    # Accept or reject sample.\n    u = tf.random_uniform([], dtype=ratio.dtype)\n    accept = tf.log(u) < ratio\n    sample_values = tf.cond(accept, lambda: list(six.itervalues(new_sample)),\n                            lambda: list(six.itervalues(old_sample)))\n    if not isinstance(sample_values, list):\n      # `tf.cond` returns tf.Tensor if output is a list of size 1.\n      sample_values = [sample_values]\n\n    sample = {z_unconstrained: sample_value for\n              z_unconstrained, sample_value in\n              zip(six.iterkeys(new_sample), sample_values)}\n\n    # Update Empirical random variables.\n    assign_ops = []\n    for z_unconstrained, qz_unconstrained in six.iteritems(\n            self.latent_vars_unconstrained):\n      variable = qz_unconstrained.get_variables()[0]\n      assign_ops.append(tf.scatter_update(\n          variable, self.t, sample[z_unconstrained]))\n\n    # Increment n_accept (if accepted).\n    assign_ops.append(self.n_accept.assign_add(tf.where(accept, 1, 0)))\n    return tf.group(*assign_ops)\n\n  def _log_joint_unconstrained(self, z_sample):\n    """"""\n    Given a sample in unconstrained latent space, transform it back into\n    the original space, and compute the log joint density with appropriate\n    Jacobian correction.\n    """"""\n\n    unconstrained_to_z = {v: k for (k, v) in self.transformations.items()}\n\n    # transform all samples back into the original (potentially\n    # constrained) space.\n    z_sample_transformed = {}\n    log_det_jacobian = 0.0\n    for z_unconstrained, qz_unconstrained in z_sample.items():\n      z = (unconstrained_to_z[z_unconstrained]\n           if z_unconstrained in unconstrained_to_z\n           else z_unconstrained)\n\n      try:\n        bij = self.transformations[z].bijector\n        z_sample_transformed[z] = bij.inverse(qz_unconstrained)\n        log_det_jacobian += tf.reduce_sum(\n            bij.inverse_log_det_jacobian(qz_unconstrained))\n      except:  # if z not in self.transformations,\n               # or is not a TransformedDist w/ bijector\n        z_sample_transformed[z] = qz_unconstrained\n\n    return self._log_joint(z_sample_transformed) + log_det_jacobian\n\n  def _log_joint(self, z_sample):\n    """"""Utility function to calculate model\'s log joint density,\n    log p(x, z), for inputs z (and fixed data x).\n\n    Args:\n      z_sample: dict.\n        Latent variable keys to samples.\n    """"""\n    scope = self._scope + tf.get_default_graph().unique_name(""sample"")\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    dict_swap = z_sample.copy()\n\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    log_joint = 0.0\n    for z in six.iterkeys(self.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      log_joint += tf.reduce_sum(z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(self.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        log_joint += tf.reduce_sum(x_copy.log_prob(dict_swap[x]))\n\n    return log_joint\n\n\ndef leapfrog(z_old, r_old, step_size, log_joint, n_steps):\n  z_new = z_old.copy()\n  r_new = r_old.copy()\n\n  grad_log_joint = tf.gradients(log_joint(z_new), list(six.itervalues(z_new)))\n  for _ in range(n_steps):\n    for i, key in enumerate(six.iterkeys(z_new)):\n      z, r = z_new[key], r_new[key]\n      r_new[key] = r + 0.5 * step_size * tf.convert_to_tensor(grad_log_joint[i])\n      z_new[key] = z + step_size * r_new[key]\n\n    grad_log_joint = tf.gradients(log_joint(z_new), list(six.itervalues(z_new)))\n    for i, key in enumerate(six.iterkeys(z_new)):\n      r_new[key] += 0.5 * step_size * tf.convert_to_tensor(grad_log_joint[i])\n\n  return z_new, r_new\n'"
edward/inferences/implicit_klqp.py,26,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.gan_inference import GANInference\nfrom edward.models import RandomVariable\nfrom edward.util import check_latent_vars, copy, get_session\n\n\nclass ImplicitKLqp(GANInference):\n  """"""Variational inference with implicit probabilistic models\n  [@tran2017deep].\n\n  It minimizes the KL divergence\n\n  $\\\\text{KL}( q(z, \\\\beta; \\lambda) \\| p(z, \\\\beta \\mid x) ),$\n\n  where $z$ are local variables associated to a data point and\n  $\\\\beta$ are global variables shared across data points.\n\n  Global latent variables require `log_prob()` and need to return a\n  random sample when fetched from the graph. Local latent variables\n  and observed variables require only a random sample when fetched\n  from the graph. (This is true for both $p$ and $q$.)\n\n  All variational factors must be reparameterizable: each of the\n  random variables (`rv`) satisfies `rv.is_reparameterized` and\n  `rv.is_continuous`.\n\n  #### Notes\n\n  Unlike `GANInference`, `discriminator` takes dict\'s as input,\n  and must subset to the appropriate values through lexical scoping\n  from the previously defined model and latent variables. This is\n  necessary as the discriminator can take an arbitrary set of data,\n  latent, and global variables.\n\n  Note the type for `discriminator`\'s output changes when one\n  passes in the `scale` argument to `initialize()`.\n\n  + If `scale` has at most one item, then `discriminator`\n  outputs a tensor whose multiplication with that element is\n  broadcastable. (For example, the output is a tensor and the single\n  scale factor is a scalar.)\n  + If `scale` has more than one item, then in order to scale\n  its corresponding output, `discriminator` must output a\n  dictionary of same size and keys as `scale`.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars, data=None, discriminator=None,\n               global_vars=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      discriminator: function.\n        Function (with parameters). Unlike `GANInference`, it is\n        interpreted as a ratio estimator rather than a discriminator.\n        It takes three arguments: a data dict, local latent variable\n        dict, and global latent variable dict. As with GAN\n        discriminators, it can take a batch of data points and local\n        variables, of size $M$, and output a vector of length\n        $M$.\n      global_vars: dict of RandomVariable to RandomVariable.\n        Identifying which variables in `latent_vars` are global\n        variables, shared across data points. These will not be\n        encompassed in the ratio estimation problem, and will be\n        estimated with tractable variational approximations.\n    """"""\n    if not callable(discriminator):\n      raise TypeError(""discriminator must be a callable function."")\n\n    self.discriminator = discriminator\n    if global_vars is None:\n      global_vars = {}\n\n    check_latent_vars(global_vars)\n    self.global_vars = global_vars\n    # call grandparent\'s method; avoid parent (GANInference)\n    super(GANInference, self).__init__(latent_vars, data)\n\n  def initialize(self, ratio_loss=\'log\', *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      ratio_loss: str or fn.\n        Loss function minimized to get the ratio estimator. \'log\' or \'hinge\'.\n        Alternatively, one can pass in a function of two inputs,\n        `psamples` and `qsamples`, and output a point-wise value\n        with shape matching the shapes of the two inputs.\n    """"""\n    if callable(ratio_loss):\n      self.ratio_loss = ratio_loss\n    elif ratio_loss == \'log\':\n      self.ratio_loss = log_loss\n    elif ratio_loss == \'hinge\':\n      self.ratio_loss = hinge_loss\n    else:\n      raise ValueError(\'Ratio loss not found:\', ratio_loss)\n\n    return super(ImplicitKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    """"""Build loss function\n\n    $-\\Big(\\mathbb{E}_{q(\\\\beta)} [\\log p(\\\\beta) - \\log q(\\\\beta) ] +\n        \\sum_{n=1}^N \\mathbb{E}_{q(\\\\beta)q(z_n\\mid\\\\beta)} [\n            r^*(x_n, z_n, \\\\beta) ] \\Big).$\n\n    We minimize it with respect to parameterized variational\n    families $q(z, \\\\beta; \\lambda)$.\n\n    $r^*(x_n, z_n, \\\\beta)$ is a function of a single data point\n    $x_n$, single local variable $z_n$, and all global\n    variables $\\\\beta$. It is equal to the log-ratio\n\n    $\\log p(x_n, z_n\\mid \\\\beta) - \\log q(x_n, z_n\\mid \\\\beta),$\n\n    where $q(x_n)$ is the empirical data distribution. Rather\n    than explicit calculation, $r^*(x, z, \\\\beta)$ is the\n    solution to a ratio estimation problem, minimizing the specified\n    `ratio_loss`.\n\n    Gradients are taken using the reparameterization trick\n    [@kingma2014auto].\n\n    #### Notes\n\n    This also includes model parameters $p(x, z, \\\\beta; \\\\theta)$\n    and variational distributions with inference networks\n    $q(z\\mid x)$.\n\n    There are a bunch of extensions we could easily do in this\n    implementation:\n\n    + further factorizations can be used to better leverage the\n      graph structure for more complicated models;\n    + score function gradients for global variables;\n    + use more samples; this would require the `copy()` utility\n      function for q\'s as well, and an additional loop. we opt not to\n      because it complicates the code;\n    + analytic KL/swapping out the penalty term for the globals.\n    """"""\n    # Collect tensors used in calculation of losses.\n    scope = tf.get_default_graph().unique_name(""inference"")\n    qbeta_sample = {}\n    pbeta_log_prob = 0.0\n    qbeta_log_prob = 0.0\n    for beta, qbeta in six.iteritems(self.global_vars):\n      # Draw a sample beta\' ~ q(beta) and calculate\n      # log p(beta\') and log q(beta\').\n      qbeta_sample[beta] = qbeta.value()\n      pbeta_log_prob += tf.reduce_sum(beta.log_prob(qbeta_sample[beta]))\n      qbeta_log_prob += tf.reduce_sum(qbeta.log_prob(qbeta_sample[beta]))\n\n    pz_sample = {}\n    qz_sample = {}\n    for z, qz in six.iteritems(self.latent_vars):\n      if z not in self.global_vars:\n        # Copy local variables p(z), q(z) to draw samples\n        # z\' ~ p(z | beta\'), z\' ~ q(z | beta\').\n        pz_copy = copy(z, dict_swap=qbeta_sample, scope=scope)\n        pz_sample[z] = pz_copy.value()\n        qz_sample[z] = qz.value()\n\n    # Collect x\' ~ p(x | z\', beta\') and x\' ~ q(x).\n    dict_swap = qbeta_sample.copy()\n    dict_swap.update(qz_sample)\n    x_psample = {}\n    x_qsample = {}\n    for x, x_data in six.iteritems(self.data):\n      if isinstance(x, tf.Tensor):\n        if ""Placeholder"" not in x.op.type:\n          # Copy p(x | z, beta) to get draw p(x | z\', beta\').\n          x_copy = copy(x, dict_swap=dict_swap, scope=scope)\n          x_psample[x] = x_copy\n          x_qsample[x] = x_data\n      elif isinstance(x, RandomVariable):\n        # Copy p(x | z, beta) to get draw p(x | z\', beta\').\n        x_copy = copy(x, dict_swap=dict_swap, scope=scope)\n        x_psample[x] = x_copy.value()\n        x_qsample[x] = x_data\n\n    with tf.variable_scope(""Disc""):\n      r_psample = self.discriminator(x_psample, pz_sample, qbeta_sample)\n\n    with tf.variable_scope(""Disc"", reuse=True):\n      r_qsample = self.discriminator(x_qsample, qz_sample, qbeta_sample)\n\n    # Form ratio loss and ratio estimator.\n    if len(self.scale) <= 1:\n      loss_d = tf.reduce_mean(self.ratio_loss(r_psample, r_qsample))\n      scale = list(six.itervalues(self.scale))\n      scale = scale[0] if scale else 1.0\n      scaled_ratio = tf.reduce_sum(scale * r_qsample)\n    else:\n      loss_d = [tf.reduce_mean(self.ratio_loss(r_psample[key], r_qsample[key]))\n                for key in six.iterkeys(self.scale)]\n      loss_d = tf.reduce_sum(loss_d)\n      scaled_ratio = [tf.reduce_sum(self.scale[key] * r_qsample[key])\n                      for key in six.iterkeys(self.scale)]\n      scaled_ratio = tf.reduce_sum(scaled_ratio)\n\n    reg_terms_d = tf.losses.get_regularization_losses(scope=""Disc"")\n    reg_terms_all = tf.losses.get_regularization_losses()\n    reg_terms = [r for r in reg_terms_all if r not in reg_terms_d]\n\n    # Form variational objective.\n    loss = -(pbeta_log_prob - qbeta_log_prob + scaled_ratio -\n             tf.reduce_sum(reg_terms))\n    loss_d = loss_d + tf.reduce_sum(reg_terms_d)\n\n    var_list_d = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=""Disc"")\n    if var_list is None:\n      var_list = [v for v in tf.trainable_variables() if v not in var_list_d]\n\n    grads = tf.gradients(loss, var_list)\n    grads_d = tf.gradients(loss_d, var_list_d)\n    grads_and_vars = list(zip(grads, var_list))\n    grads_and_vars_d = list(zip(grads_d, var_list_d))\n    return loss, grads_and_vars, loss_d, grads_and_vars_d\n\n\ndef log_loss(psample, qsample):\n  """"""Point-wise log loss.""""""\n  loss = tf.nn.sigmoid_cross_entropy_with_logits(\n      labels=tf.ones_like(psample), logits=psample) + \\\n      tf.nn.sigmoid_cross_entropy_with_logits(\n          labels=tf.zeros_like(qsample), logits=qsample)\n  return loss\n\n\ndef hinge_loss(psample, qsample):\n  """"""Point-wise hinge loss.""""""\n  loss = tf.nn.relu(1.0 - psample) + tf.nn.relu(1.0 + qsample)\n  return loss\n'"
edward/inferences/inference.py,31,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\nimport six\nimport tensorflow as tf\nimport os\n\nfrom datetime import datetime\nfrom edward.models import RandomVariable\nfrom edward.util import check_data, check_latent_vars, get_session, \\\n    get_variables, Progbar, transform\n\nfrom tensorflow.contrib.distributions import bijectors\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Inference(object):\n  """"""Abstract base class for inference. All inference algorithms in\n  Edward inherit from `Inference`, sharing common methods and\n  properties via a class hierarchy.\n\n  Specific algorithms typically inherit from other subclasses of\n  `Inference` rather than `Inference` directly. For example, one\n  might inherit from the abstract classes `MonteCarlo` or\n  `VariationalInference`.\n\n  To build an algorithm inheriting from `Inference`, one must at the\n  minimum implement `initialize` and `update`: the former builds\n  the computational graph for the algorithm; the latter runs the\n  computational graph for the algorithm.\n\n  To reset inference (e.g., internal variable counters incremented\n  over training), fetch inference\'s reset ops from session with\n  `sess.run(inference.reset)`.\n\n  #### Examples\n\n  ```python\n  # Set up probability model.\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n  # Set up posterior approximation.\n  qmu_loc = tf.Variable(tf.random_normal([]))\n  qmu_scale = tf.nn.softplus(tf.Variable(tf.random_normal([])))\n  qmu = Normal(loc=qmu_loc, scale=qmu_scale)\n\n  inference = ed.Inference({mu: qmu}, data={x: tf.zeros(50)})\n  ```\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: dict.\n        Collection of latent variables (of type `RandomVariable` or\n        `tf.Tensor`) to perform inference on. Each random variable is\n        binded to another random variable; the latter will infer the\n        former conditional on data.\n      data: dict.\n        Data dictionary which binds observed variables (of type\n        `RandomVariable` or `tf.Tensor`) to their realizations (of\n        type `tf.Tensor`). It can also bind placeholders (of type\n        `tf.Tensor`) used in the model to their realizations; and\n        prior latent variables (of type `RandomVariable`) to posterior\n        latent variables (of type `RandomVariable`).\n    """"""\n    sess = get_session()\n    if latent_vars is None:\n      latent_vars = {}\n    if data is None:\n      data = {}\n\n    check_latent_vars(latent_vars)\n    self.latent_vars = latent_vars\n\n    check_data(data)\n    self.data = {}\n    for key, value in six.iteritems(data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        self.data[key] = value\n      elif isinstance(key, (RandomVariable, tf.Tensor)):\n        if isinstance(value, (RandomVariable, tf.Tensor)):\n          self.data[key] = value\n        elif isinstance(value, (float, list, int, np.ndarray, np.number, str)):\n          # If value is a Python type, store it in the graph.\n          # Assign its placeholder with the key\'s data type.\n          with tf.variable_scope(None, default_name=""data""):\n            ph = tf.placeholder(key.dtype, np.shape(value))\n            var = tf.Variable(ph, trainable=False, collections=[])\n            sess.run(var.initializer, {ph: value})\n            self.data[key] = var\n\n  def run(self, variables=None, use_coordinator=True, *args, **kwargs):\n    """"""A simple wrapper to run inference.\n\n    1. Initialize algorithm via `initialize`.\n    2. (Optional) Build a TensorFlow summary writer for TensorBoard.\n    3. (Optional) Initialize TensorFlow variables.\n    4. (Optional) Start queue runners.\n    5. Run `update` for `self.n_iter` iterations.\n    6. While running, `print_progress`.\n    7. Finalize algorithm via `finalize`.\n    8. (Optional) Stop queue runners.\n\n    To customize the way inference is run, run these steps\n    individually.\n\n    Args:\n      variables: list.\n        A list of TensorFlow variables to initialize during inference.\n        Default is to initialize all variables (this includes\n        reinitializing variables that were already initialized). To\n        avoid initializing any variables, pass in an empty list.\n      use_coordinator: bool.\n        Whether to start and stop queue runners during inference using a\n        TensorFlow coordinator. For example, queue runners are necessary\n        for batch training with file readers.\n      *args, **kwargs:\n        Passed into `initialize`.\n    """"""\n    self.initialize(*args, **kwargs)\n\n    if variables is None:\n      init = tf.global_variables_initializer()\n    else:\n      init = tf.variables_initializer(variables)\n\n    # Feed placeholders in case initialization depends on them.\n    feed_dict = {}\n    for key, value in six.iteritems(self.data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        feed_dict[key] = value\n\n    init.run(feed_dict)\n\n    if use_coordinator:\n      # Start input enqueue threads.\n      self.coord = tf.train.Coordinator()\n      self.threads = tf.train.start_queue_runners(coord=self.coord)\n\n    for _ in range(self.n_iter):\n      info_dict = self.update()\n      self.print_progress(info_dict)\n\n    self.finalize()\n\n    if use_coordinator:\n      # Ask threads to stop.\n      self.coord.request_stop()\n      self.coord.join(self.threads)\n\n  @abc.abstractmethod\n  def initialize(self, n_iter=1000, n_print=None, scale=None,\n                 auto_transform=True, logdir=None, log_timestamp=True,\n                 log_vars=None, debug=False):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Any derived class of `Inference` **must** implement this method.\n    No methods which build ops should be called outside `initialize()`.\n\n    Args:\n      n_iter: int.\n        Number of iterations for algorithm when calling `run()`.\n        Alternatively if controlling inference manually, it is the\n        expected number of calls to `update()`; this number determines\n        tracking information during the print progress.\n      n_print: int.\n        Number of iterations for each print progress. To suppress print\n        progress, then specify 0. Default is `int(n_iter / 100)`.\n      scale: dict of RandomVariable to tf.Tensor.\n        A tensor to scale computation for any random variable that it is\n        binded to. Its shape must be broadcastable; it is multiplied\n        element-wise to the random variable. For example, this is useful\n        for mini-batch scaling when inferring global variables, or\n        applying masks on a random variable.\n      auto_transform: bool.\n        Whether to automatically transform continuous latent variables\n        of unequal support to be on the unconstrained space. It is\n        only applied if the argument is `True`, the latent variable\n        pair are `ed.RandomVariable`s with the `support` attribute,\n        the supports are both continuous and unequal.\n      logdir: str.\n        Directory where event file will be written. For details,\n        see `tf.summary.FileWriter`. Default is to log nothing.\n      log_timestamp: bool.\n        If True (and `logdir` is specified), create a subdirectory of\n        `logdir` to save the specific run results. The subdirectory\'s\n        name is the current UTC timestamp with format \'YYYYMMDD_HHMMSS\'.\n      log_vars: list.\n        Specifies the list of variables to log after each `n_print`\n        steps. If None, will log all variables. If `[]`, no variables\n        will be logged. `logdir` must be specified for variables to be\n        logged.\n      debug: bool.\n        If True, add checks for `NaN` and `Inf` to all computations\n        in the graph. May result in substantially slower execution\n        times.\n    """"""\n    self.n_iter = n_iter\n    if n_print is None:\n      self.n_print = int(n_iter / 100)\n    else:\n      self.n_print = n_print\n\n    self.progbar = Progbar(self.n_iter)\n    self.t = tf.Variable(0, trainable=False, name=""iteration"")\n\n    self.increment_t = self.t.assign_add(1)\n\n    if scale is None:\n      scale = {}\n    elif not isinstance(scale, dict):\n      raise TypeError(""scale must be a dict object."")\n\n    self.scale = scale\n\n    # map from original latent vars to unconstrained versions\n    self.transformations = {}\n    if auto_transform:\n      latent_vars = self.latent_vars.copy()\n      # latent_vars maps original latent vars to constrained Q\'s.\n      # latent_vars_unconstrained maps unconstrained vars to unconstrained Q\'s.\n      self.latent_vars = {}\n      self.latent_vars_unconstrained = {}\n      for z, qz in six.iteritems(latent_vars):\n        if hasattr(z, \'support\') and hasattr(qz, \'support\') and \\\n                z.support != qz.support and qz.support != \'point\':\n\n          # transform z to an unconstrained space\n          z_unconstrained = transform(z)\n          self.transformations[z] = z_unconstrained\n\n          # make sure we also have a qz that covers the unconstrained space\n          if qz.support == ""points"":\n            qz_unconstrained = qz\n          else:\n            qz_unconstrained = transform(qz)\n          self.latent_vars_unconstrained[z_unconstrained] = qz_unconstrained\n\n          # additionally construct the transformation of qz\n          # back into the original constrained space\n          if z_unconstrained != z:\n            qz_constrained = transform(\n                qz_unconstrained, bijectors.Invert(z_unconstrained.bijector))\n\n            try:  # attempt to pushforward the params of Empirical distributions\n              qz_constrained.params = z_unconstrained.bijector.inverse(\n                  qz_unconstrained.params)\n            except:  # qz_unconstrained is not an Empirical distribution\n              pass\n\n          else:\n            qz_constrained = qz_unconstrained\n\n          self.latent_vars[z] = qz_constrained\n        else:\n          self.latent_vars[z] = qz\n          self.latent_vars_unconstrained[z] = qz\n      del latent_vars\n\n    if logdir is not None:\n      self.logging = True\n      if log_timestamp:\n        logdir = os.path.expanduser(logdir)\n        logdir = os.path.join(\n            logdir, datetime.strftime(datetime.utcnow(), ""%Y%m%d_%H%M%S""))\n\n      self._summary_key = tf.get_default_graph().unique_name(""summaries"")\n      self._set_log_variables(log_vars)\n      self.train_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n    else:\n      self.logging = False\n\n    self.debug = debug\n    if self.debug:\n      self.op_check = tf.add_check_numerics_ops()\n\n    # Store reset ops which user can call. Subclasses should append\n    # any ops needed to reset internal variables in inference.\n    self.reset = [tf.variables_initializer([self.t])]\n\n  @abc.abstractmethod\n  def update(self, feed_dict=None):\n    """"""Run one iteration of inference.\n\n    Any derived class of `Inference` **must** implement this method.\n\n    Args:\n      feed_dict: dict.\n        Feed dictionary for a TensorFlow session run. It is used to feed\n        placeholders that are not fed during initialization.\n\n    Returns:\n      dict.\n        Dictionary of algorithm-specific information.\n    """"""\n    if feed_dict is None:\n      feed_dict = {}\n\n    for key, value in six.iteritems(self.data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        feed_dict[key] = value\n\n    sess = get_session()\n    t = sess.run(self.increment_t)\n\n    if self.debug:\n      sess.run(self.op_check, feed_dict)\n\n    if self.logging and self.n_print != 0:\n      if t == 1 or t % self.n_print == 0:\n        summary = sess.run(self.summarize, feed_dict)\n        self.train_writer.add_summary(summary, t)\n\n    return {\'t\': t}\n\n  def print_progress(self, info_dict):\n    """"""Print progress to output.\n\n    Args:\n      info_dict: dict.\n        Dictionary of algorithm-specific information.\n    """"""\n    if self.n_print != 0:\n      t = info_dict[\'t\']\n      if t == 1 or t % self.n_print == 0:\n        self.progbar.update(t)\n\n  def finalize(self):\n    """"""Function to call after convergence.\n    """"""\n    if self.logging:\n      self.train_writer.close()\n\n  def _set_log_variables(self, log_vars=None):\n    """"""Log variables to TensorBoard.\n\n    For each variable in `log_vars`, forms a `tf.summary.scalar` if\n    the variable has scalar shape; otherwise forms a `tf.summary.histogram`.\n\n    Args:\n      log_vars: list.\n        Specifies the list of variables to log after each `n_print`\n        steps. If None, will log all variables. If `[]`, no variables\n        will be logged.\n    """"""\n    if log_vars is None:\n      log_vars = []\n      for key in six.iterkeys(self.data):\n        log_vars += get_variables(key)\n\n      for key, value in six.iteritems(self.latent_vars):\n        log_vars += get_variables(key)\n        log_vars += get_variables(value)\n\n      log_vars = set(log_vars)\n\n    for var in log_vars:\n      # replace colons which are an invalid character\n      var_name = var.name.replace(\':\', \'/\')\n      # Log all scalars.\n      if len(var.shape) == 0:\n        tf.summary.scalar(""parameter/{}"".format(var_name),\n                          var, collections=[self._summary_key])\n      elif len(var.shape) == 1 and var.shape[0] == 1:\n        tf.summary.scalar(""parameter/{}"".format(var_name),\n                          var[0], collections=[self._summary_key])\n      else:\n        # If var is multi-dimensional, log a histogram of its values.\n        tf.summary.histogram(""parameter/{}"".format(var_name),\n                             var, collections=[self._summary_key])\n'"
edward/inferences/klpq.py,23,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.variational_inference import VariationalInference\nfrom edward.models import RandomVariable\nfrom edward.util import copy, get_descendants\n\ntry:\n  from edward.models import Normal\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass KLpq(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( p(z \\mid x) \\| q(z) ).$\n\n  To perform the optimization, this class uses a technique from\n  adaptive importance sampling [@oh1992adaptive].\n\n  #### Notes\n\n  `KLpq` also optimizes any model parameters $p(z\\mid x;\n  \\\\theta)$. It does this by variational EM, maximizing\n\n  $\\mathbb{E}_{p(z \\mid x; \\lambda)} [ \\log p(x, z; \\\\theta) ]$\n\n  with respect to $\\\\theta$.\n\n  In conditional inference, we infer $z` in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$. During gradient calculation, instead\n  of using the model\'s density\n\n  $\\log p(x, z^{(s)}), z^{(s)} \\sim q(z; \\lambda),$\n\n  for each sample $s=1,\\ldots,S$, `KLpq` uses\n\n  $\\log p(x, z^{(s)}, \\\\beta^{(s)}),$\n\n  where $z^{(s)} \\sim q(z; \\lambda)$ and$\\\\beta^{(s)}\n  \\sim q(\\\\beta)$.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(KLpq, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n    """"""\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    return super(KLpq, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    """"""Build loss function\n\n    $\\\\text{KL}( p(z \\mid x) \\| q(z) )\n      = \\mathbb{E}_{p(z \\mid x)} [ \\log p(z \\mid x) - \\log q(z; \\lambda) ]$\n\n    and stochastic gradients based on importance sampling.\n\n    The loss function can be estimated as\n\n    $\\sum_{s=1}^S [\n      w_{\\\\text{norm}}(z^s; \\lambda) (\\log p(x, z^s) - \\log q(z^s; \\lambda) ],$\n\n    where for $z^s \\sim q(z; \\lambda)$,\n\n    $w_{\\\\text{norm}}(z^s; \\lambda) =\n          w(z^s; \\lambda) / \\sum_{s=1}^S w(z^s; \\lambda)$\n\n    normalizes the importance weights, $w(z^s; \\lambda) = p(x,\n    z^s) / q(z^s; \\lambda)$.\n\n    This provides a gradient,\n\n    $- \\sum_{s=1}^S [\n      w_{\\\\text{norm}}(z^s; \\lambda) \\\\nabla_{\\lambda} \\log q(z^s; \\lambda) ].$\n    """"""\n    p_log_prob = [0.0] * self.n_samples\n    q_log_prob = [0.0] * self.n_samples\n    base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n    for s in range(self.n_samples):\n      # Form dictionary in order to replace conditioning on prior or\n      # observed variable with conditioning on a specific value.\n      scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n      dict_swap = {}\n      for x, qx in six.iteritems(self.data):\n        if isinstance(x, RandomVariable):\n          if isinstance(qx, RandomVariable):\n            qx_copy = copy(qx, scope=scope)\n            dict_swap[x] = qx_copy.value()\n          else:\n            dict_swap[x] = qx\n\n      for z, qz in six.iteritems(self.latent_vars):\n        # Copy q(z) to obtain new set of posterior samples.\n        qz_copy = copy(qz, scope=scope)\n        dict_swap[z] = qz_copy.value()\n        q_log_prob[s] += tf.reduce_sum(\n            qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n\n      for z in six.iterkeys(self.latent_vars):\n        z_copy = copy(z, dict_swap, scope=scope)\n        p_log_prob[s] += tf.reduce_sum(z_copy.log_prob(dict_swap[z]))\n\n      for x in six.iterkeys(self.data):\n        if isinstance(x, RandomVariable):\n          x_copy = copy(x, dict_swap, scope=scope)\n          p_log_prob[s] += tf.reduce_sum(x_copy.log_prob(dict_swap[x]))\n\n    p_log_prob = tf.stack(p_log_prob)\n    q_log_prob = tf.stack(q_log_prob)\n    reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n    if self.logging:\n      tf.summary.scalar(""loss/p_log_prob"", tf.reduce_mean(p_log_prob),\n                        collections=[self._summary_key])\n      tf.summary.scalar(""loss/q_log_prob"", tf.reduce_mean(q_log_prob),\n                        collections=[self._summary_key])\n      tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                        collections=[self._summary_key])\n\n    log_w = p_log_prob - q_log_prob\n    log_w_norm = log_w - tf.reduce_logsumexp(log_w)\n    w_norm = tf.exp(log_w_norm)\n    loss = tf.reduce_sum(w_norm * log_w) - reg_penalty\n\n    q_rvs = list(six.itervalues(self.latent_vars))\n    q_vars = [v for v in var_list\n              if len(get_descendants(tf.convert_to_tensor(v), q_rvs)) != 0]\n    q_grads = tf.gradients(\n        -(tf.reduce_sum(q_log_prob * tf.stop_gradient(w_norm)) - reg_penalty),\n        q_vars)\n    p_vars = [v for v in var_list if v not in q_vars]\n    p_grads = tf.gradients(-loss, p_vars)\n    grads_and_vars = list(zip(q_grads, q_vars)) + list(zip(p_grads, p_vars))\n    return loss, grads_and_vars\n'"
edward/inferences/klqp.py,145,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.variational_inference import VariationalInference\nfrom edward.models import RandomVariable\nfrom edward.util import copy, get_descendants\n\ntry:\n  from edward.models import Normal\n  from tensorflow.contrib.distributions import kl_divergence\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass KLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective by automatically selecting from a\n  variety of black box inference techniques.\n\n  #### Notes\n\n  `KLqp` also optimizes any model parameters $p(z \\mid x;\n  \\\\theta)$. It does this by variational EM, maximizing\n\n  $\\mathbb{E}_{q(z; \\lambda)} [ \\log p(x, z; \\\\theta) ]$\n\n  with respect to $\\\\theta$.\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$. During gradient calculation, instead\n  of using the model\'s density\n\n  $\\log p(x, z^{(s)}), z^{(s)} \\sim q(z; \\lambda),$\n\n  for each sample $s=1,\\ldots,S$, `KLqp` uses\n\n  $\\log p(x, z^{(s)}, \\\\beta^{(s)}),$\n\n  where $z^{(s)} \\sim q(z; \\lambda)$ and $\\\\beta^{(s)}\n  \\sim q(\\\\beta)$.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(KLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, kl_scaling=None, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n      kl_scaling: dict of RandomVariable to tf.Tensor.\n        Provides option to scale terms when using ELBO with KL divergence.\n        If the KL divergence terms are\n\n        $\\\\alpha_p \\mathbb{E}_{q(z\\mid x, \\lambda)} [\n              \\log q(z\\mid x, \\lambda) - \\log p(z)],$\n\n        then pass {$p(z)$: $\\\\alpha_p$} as `kl_scaling`,\n        where $\\\\alpha_p$ is a tensor. Its shape must be broadcastable;\n        it is multiplied element-wise to the batchwise KL terms.\n    """"""\n    if kl_scaling is None:\n      kl_scaling = {}\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n\n    self.n_samples = n_samples\n    self.kl_scaling = kl_scaling\n    return super(KLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    """"""Wrapper for the `KLqp` loss function.\n\n    $-\\\\text{ELBO} =\n        -\\mathbb{E}_{q(z; \\lambda)} [ \\log p(x, z) - \\log q(z; \\lambda) ]$\n\n    KLqp supports\n\n    1. score function gradients [@paisley2012variational]\n    2. reparameterization gradients [@kingma2014auto]\n\n    of the loss function.\n\n    If the KL divergence between the variational model and the prior\n    is tractable, then the loss function can be written as\n\n    $-\\mathbb{E}_{q(z; \\lambda)}[\\log p(x \\mid z)] +\n        \\\\text{KL}( q(z; \\lambda) \\| p(z) ),$\n\n    where the KL term is computed analytically [@kingma2014auto]. We\n    compute this automatically when $p(z)$ and $q(z; \\lambda)$ are\n    Normal.\n    """"""\n    is_reparameterizable = all([\n        rv.reparameterization_type ==\n        tf.contrib.distributions.FULLY_REPARAMETERIZED\n        for rv in six.itervalues(self.latent_vars)])\n    is_analytic_kl = all([isinstance(z, Normal) and isinstance(qz, Normal)\n                          for z, qz in six.iteritems(self.latent_vars)])\n    if not is_analytic_kl and self.kl_scaling:\n      raise TypeError(""kl_scaling must be None when using non-analytic KL term"")\n    if is_reparameterizable:\n      if is_analytic_kl:\n        return build_reparam_kl_loss_and_gradients(self, var_list)\n      # elif is_analytic_entropy:\n      #    return build_reparam_entropy_loss_and_gradients(self, var_list)\n      else:\n        return build_reparam_loss_and_gradients(self, var_list)\n    else:\n      # Prefer Rao-Blackwellization over analytic KL. Unknown what\n      # would happen stability-wise if the two are combined.\n      # if is_analytic_kl:\n      #   return build_score_kl_loss_and_gradients(self, var_list)\n      # Analytic entropies may lead to problems around\n      # convergence; for now it is deactivated.\n      # elif is_analytic_entropy:\n      #    return build_score_entropy_loss_and_gradients(self, var_list)\n      # else:\n      return build_score_rb_loss_and_gradients(self, var_list)\n\n\nclass ReparameterizationKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the reparameterization\n  gradient.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ReparameterizationKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n    """"""\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    return super(ReparameterizationKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_reparam_loss_and_gradients(self, var_list)\n\n\nclass ReparameterizationKLKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the reparameterization\n  gradient and an analytic KL term.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ReparameterizationKLKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, kl_scaling=None, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n      kl_scaling: dict of RandomVariable to tf.Tensor.\n        Provides option to scale terms when using ELBO with KL divergence.\n        If the KL divergence terms are\n\n        $\\\\alpha_p \\mathbb{E}_{q(z\\mid x, \\lambda)} [\n              \\log q(z\\mid x, \\lambda) - \\log p(z)],$\n\n        then pass {$p(z)$: $\\\\alpha_p$} as `kl_scaling`,\n        where $\\\\alpha_p$ is a tensor. Its shape must be broadcastable;\n        it is multiplied element-wise to the batchwise KL terms.\n    """"""\n    if kl_scaling is None:\n      kl_scaling = {}\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n\n    self.n_samples = n_samples\n    self.kl_scaling = kl_scaling\n    return super(ReparameterizationKLKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_reparam_kl_loss_and_gradients(self, var_list)\n\n\nclass ReparameterizationEntropyKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the reparameterization\n  gradient and an analytic entropy term.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ReparameterizationEntropyKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n    """"""\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    return super(ReparameterizationEntropyKLqp, self).initialize(\n        *args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_reparam_entropy_loss_and_gradients(self, var_list)\n\n\nclass ScoreKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the score function\n  gradient.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ScoreKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n    """"""\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    return super(ScoreKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_score_loss_and_gradients(self, var_list)\n\n\nclass ScoreKLKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the score function gradient\n  and an analytic KL term.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ScoreKLKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, kl_scaling=None, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n      kl_scaling: dict of RandomVariable to tf.Tensor.\n        Provides option to scale terms when using ELBO with KL divergence.\n        If the KL divergence terms are\n\n        $\\\\alpha_p \\mathbb{E}_{q(z\\mid x, \\lambda)} [\n              \\log q(z\\mid x, \\lambda) - \\log p(z)],$\n\n        then pass {$p(z)$: $\\\\alpha_p$} as `kl_scaling`,\n        where $\\\\alpha_p$ is a tensor. Its shape must be broadcastable;\n        it is multiplied element-wise to the batchwise KL terms.\n    """"""\n    if kl_scaling is None:\n      kl_scaling = {}\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    self.kl_scaling = kl_scaling\n    return super(ScoreKLKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_score_kl_loss_and_gradients(self, var_list)\n\n\nclass ScoreEntropyKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the score function gradient\n  and an analytic entropy term.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ScoreEntropyKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n    """"""\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    return super(ScoreEntropyKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_score_entropy_loss_and_gradients(self, var_list)\n\n\nclass ScoreRBKLqp(VariationalInference):\n  """"""Variational inference with the KL divergence\n\n  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n\n  This class minimizes the objective using the score function gradient\n  and Rao-Blackwellization.\n\n  #### Notes\n\n  Current Rao-Blackwellization is limited to Rao-Blackwellizing across\n  stochastic nodes in the computation graph. It does not\n  Rao-Blackwellize within a node such as when a node represents\n  multiple random variables via non-scalar batch shape.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `Normal` random variable that is defined internally with a\n        free parameter per location and scale and is initialized using\n        standard normal draws. The random variables to approximate\n        must be continuous.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        continuous = \\\n            (\'01\', \'nonnegative\', \'simplex\', \'real\', \'multivariate_real\')\n        for z in latent_vars:\n          if not hasattr(z, \'support\') or z.support not in continuous:\n            raise AttributeError(\n                ""Random variable {} is not continuous or a random ""\n                ""variable with supported continuous support."".format(z))\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          scale = tf.nn.softplus(\n              tf.Variable(tf.random_normal(batch_event_shape)))\n          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n\n    super(ScoreRBKLqp, self).__init__(latent_vars, data)\n\n  def initialize(self, n_samples=1, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples from variational model for calculating\n        stochastic gradients.\n    """"""\n    if n_samples <= 0:\n      raise ValueError(\n          ""n_samples should be greater than zero: {}"".format(n_samples))\n    self.n_samples = n_samples\n    return super(ScoreRBKLqp, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    return build_score_rb_loss_and_gradients(self, var_list)\n\n\ndef build_reparam_loss_and_gradients(inference, var_list):\n  """"""Build loss function. Its automatic differentiation\n  is a stochastic gradient of\n\n  $-\\\\text{ELBO} =\n      -\\mathbb{E}_{q(z; \\lambda)} [ \\log p(x, z) - \\log q(z; \\lambda) ]$\n\n  based on the reparameterization trick [@kingma2014auto].\n\n  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n  expectation using Monte Carlo sampling.\n  """"""\n  p_log_prob = [0.0] * inference.n_samples\n  q_log_prob = [0.0] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n      q_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) * qz_copy.log_prob(dict_swap[z]))\n\n    for z in six.iterkeys(inference.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      p_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_prob[s] += tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  p_log_prob = tf.reduce_mean(p_log_prob)\n  q_log_prob = tf.reduce_mean(q_log_prob)\n  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n  if inference.logging:\n    tf.summary.scalar(""loss/p_log_prob"", p_log_prob,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/q_log_prob"", q_log_prob,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                      collections=[inference._summary_key])\n\n  loss = -(p_log_prob - q_log_prob - reg_penalty)\n\n  grads = tf.gradients(loss, var_list)\n  grads_and_vars = list(zip(grads, var_list))\n  return loss, grads_and_vars\n\n\ndef build_reparam_kl_loss_and_gradients(inference, var_list):\n  """"""Build loss function. Its automatic differentiation\n  is a stochastic gradient of\n\n  .. math::\n\n    -\\\\text{ELBO} =  - ( \\mathbb{E}_{q(z; \\lambda)} [ \\log p(x \\mid z) ]\n          + \\\\text{KL}(q(z; \\lambda) \\| p(z)) )\n\n  based on the reparameterization trick [@kingma2014auto].\n\n  It assumes the KL is analytic.\n\n  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n  expectation using Monte Carlo sampling.\n  """"""\n  p_log_lik = [0.0] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_lik[s] += tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  p_log_lik = tf.reduce_mean(p_log_lik)\n\n  kl_penalty = tf.reduce_sum([\n      tf.reduce_sum(inference.kl_scaling.get(z, 1.0) * kl_divergence(qz, z))\n      for z, qz in six.iteritems(inference.latent_vars)])\n\n  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n  if inference.logging:\n    tf.summary.scalar(""loss/p_log_lik"", p_log_lik,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/kl_penalty"", kl_penalty,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                      collections=[inference._summary_key])\n\n  loss = -(p_log_lik - kl_penalty - reg_penalty)\n\n  grads = tf.gradients(loss, var_list)\n  grads_and_vars = list(zip(grads, var_list))\n  return loss, grads_and_vars\n\n\ndef build_reparam_entropy_loss_and_gradients(inference, var_list):\n  """"""Build loss function. Its automatic differentiation\n  is a stochastic gradient of\n\n  $-\\\\text{ELBO} =  -( \\mathbb{E}_{q(z; \\lambda)} [ \\log p(x , z) ]\n          + \\mathbb{H}(q(z; \\lambda)) )$\n\n  based on the reparameterization trick [@kingma2014auto].\n\n  It assumes the entropy is analytic.\n\n  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n  expectation using Monte Carlo sampling.\n  """"""\n  p_log_prob = [0.0] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n\n    for z in six.iterkeys(inference.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      p_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_prob[s] += tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  p_log_prob = tf.reduce_mean(p_log_prob)\n\n  q_entropy = tf.reduce_sum([\n      tf.reduce_sum(qz.entropy())\n      for z, qz in six.iteritems(inference.latent_vars)])\n\n  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n  if inference.logging:\n    tf.summary.scalar(""loss/p_log_prob"", p_log_prob,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/q_entropy"", q_entropy,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                      collections=[inference._summary_key])\n\n  loss = -(p_log_prob + q_entropy - reg_penalty)\n\n  grads = tf.gradients(loss, var_list)\n  grads_and_vars = list(zip(grads, var_list))\n  return loss, grads_and_vars\n\n\ndef build_score_loss_and_gradients(inference, var_list):\n  """"""Build loss function and gradients based on the score function\n  estimator [@paisley2012variational].\n\n  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n  expectation using Monte Carlo sampling.\n  """"""\n  p_log_prob = [0.0] * inference.n_samples\n  q_log_prob = [0.0] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n      q_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) *\n          qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n\n    for z in six.iterkeys(inference.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      p_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_prob[s] += tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  p_log_prob = tf.stack(p_log_prob)\n  q_log_prob = tf.stack(q_log_prob)\n  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n  if inference.logging:\n    tf.summary.scalar(""loss/p_log_prob"", tf.reduce_mean(p_log_prob),\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/q_log_prob"", tf.reduce_mean(q_log_prob),\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                      collections=[inference._summary_key])\n\n  losses = p_log_prob - q_log_prob\n  loss = -(tf.reduce_mean(losses) - reg_penalty)\n\n  q_rvs = list(six.itervalues(inference.latent_vars))\n  q_vars = [v for v in var_list\n            if len(get_descendants(tf.convert_to_tensor(v), q_rvs)) != 0]\n  q_grads = tf.gradients(\n      -(tf.reduce_mean(q_log_prob * tf.stop_gradient(losses)) - reg_penalty),\n      q_vars)\n  p_vars = [v for v in var_list if v not in q_vars]\n  p_grads = tf.gradients(loss, p_vars)\n  grads_and_vars = list(zip(q_grads, q_vars)) + list(zip(p_grads, p_vars))\n  return loss, grads_and_vars\n\n\ndef build_score_kl_loss_and_gradients(inference, var_list):\n  """"""Build loss function and gradients based on the score function\n  estimator [@paisley2012variational].\n\n  It assumes the KL is analytic.\n\n  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n  expectation using Monte Carlo sampling.\n  """"""\n  p_log_lik = [0.0] * inference.n_samples\n  q_log_prob = [0.0] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n      q_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) *\n          qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_lik[s] += tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  p_log_lik = tf.stack(p_log_lik)\n  q_log_prob = tf.stack(q_log_prob)\n\n  kl_penalty = tf.reduce_sum([\n      tf.reduce_sum(inference.kl_scaling.get(z, 1.0) * kl_divergence(qz, z))\n      for z, qz in six.iteritems(inference.latent_vars)])\n\n  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n  if inference.logging:\n    tf.summary.scalar(""loss/p_log_lik"", tf.reduce_mean(p_log_lik),\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/kl_penalty"", kl_penalty,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                      collections=[inference._summary_key])\n\n  loss = -(tf.reduce_mean(p_log_lik) - kl_penalty - reg_penalty)\n\n  q_rvs = list(six.itervalues(inference.latent_vars))\n  q_vars = [v for v in var_list\n            if len(get_descendants(tf.convert_to_tensor(v), q_rvs)) != 0]\n  q_grads = tf.gradients(\n      -(tf.reduce_mean(q_log_prob * tf.stop_gradient(p_log_lik)) - kl_penalty -\n          reg_penalty),\n      q_vars)\n  p_vars = [v for v in var_list if v not in q_vars]\n  p_grads = tf.gradients(loss, p_vars)\n  grads_and_vars = list(zip(q_grads, q_vars)) + list(zip(p_grads, p_vars))\n  return loss, grads_and_vars\n\n\ndef build_score_entropy_loss_and_gradients(inference, var_list):\n  """"""Build loss function and gradients based on the score function\n  estimator [@paisley2012variational].\n\n  It assumes the entropy is analytic.\n\n  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n  expectation using Monte Carlo sampling.\n  """"""\n  p_log_prob = [0.0] * inference.n_samples\n  q_log_prob = [0.0] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n      q_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) *\n          qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n\n    for z in six.iterkeys(inference.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      p_log_prob[s] += tf.reduce_sum(\n          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_prob[s] += tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  p_log_prob = tf.stack(p_log_prob)\n  q_log_prob = tf.stack(q_log_prob)\n\n  q_entropy = tf.reduce_sum([\n      tf.reduce_sum(qz.entropy())\n      for z, qz in six.iteritems(inference.latent_vars)])\n\n  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n  if inference.logging:\n    tf.summary.scalar(""loss/p_log_prob"", tf.reduce_mean(p_log_prob),\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/q_log_prob"", tf.reduce_mean(q_log_prob),\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/q_entropy"", q_entropy,\n                      collections=[inference._summary_key])\n    tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                      collections=[inference._summary_key])\n\n  loss = -(tf.reduce_mean(p_log_prob) + q_entropy - reg_penalty)\n\n  q_rvs = list(six.itervalues(inference.latent_vars))\n  q_vars = [v for v in var_list\n            if len(get_descendants(tf.convert_to_tensor(v), q_rvs)) != 0]\n  q_grads = tf.gradients(\n      -(tf.reduce_mean(q_log_prob * tf.stop_gradient(p_log_prob)) +\n          q_entropy - reg_penalty),\n      q_vars)\n  p_vars = [v for v in var_list if v not in q_vars]\n  p_grads = tf.gradients(loss, p_vars)\n  grads_and_vars = list(zip(q_grads, q_vars)) + list(zip(p_grads, p_vars))\n  return loss, grads_and_vars\n\n\ndef build_score_rb_loss_and_gradients(inference, var_list):\n  """"""Build loss function and gradients based on the score function\n  estimator [@paisley2012variational] and Rao-Blackwellization\n  [@ranganath2014black].\n\n  Computed by sampling from :math:`q(z;\\lambda)` and evaluating the\n  expectation using Monte Carlo sampling and Rao-Blackwellization.\n  """"""\n  # Build tensors for loss and gradient calculations. There is one set\n  # for each sample from the variational distribution.\n  p_log_probs = [{}] * inference.n_samples\n  q_log_probs = [{}] * inference.n_samples\n  base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n  for s in range(inference.n_samples):\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = base_scope + tf.get_default_graph().unique_name(""sample"")\n    dict_swap = {}\n    for x, qx in six.iteritems(inference.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for z, qz in six.iteritems(inference.latent_vars):\n      # Copy q(z) to obtain new set of posterior samples.\n      qz_copy = copy(qz, scope=scope)\n      dict_swap[z] = qz_copy.value()\n      q_log_probs[s][qz] = tf.reduce_sum(\n          inference.scale.get(z, 1.0) *\n          qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n\n    for z in six.iterkeys(inference.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      p_log_probs[s][z] = tf.reduce_sum(\n          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(inference.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        p_log_probs[s][x] = tf.reduce_sum(\n            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n  # Take gradients of Rao-Blackwellized loss for each variational parameter.\n  p_rvs = list(six.iterkeys(inference.latent_vars)) + \\\n      [x for x in six.iterkeys(inference.data) if isinstance(x, RandomVariable)]\n  q_rvs = list(six.itervalues(inference.latent_vars))\n  reverse_latent_vars = {v: k for k, v in six.iteritems(inference.latent_vars)}\n  grads = []\n  grads_vars = []\n  for var in var_list:\n    # Get all variational factors depending on the parameter.\n    descendants = get_descendants(tf.convert_to_tensor(var), q_rvs)\n    if len(descendants) == 0:\n      continue  # skip if not a variational parameter\n    # Get p and q\'s Markov blanket wrt these latent variables.\n    var_p_rvs = set()\n    for qz in descendants:\n      z = reverse_latent_vars[qz]\n      var_p_rvs.update(z.get_blanket(p_rvs) + [z])\n\n    var_q_rvs = set()\n    for qz in descendants:\n      var_q_rvs.update(qz.get_blanket(q_rvs) + [qz])\n\n    pi_log_prob = [0.0] * inference.n_samples\n    qi_log_prob = [0.0] * inference.n_samples\n    for s in range(inference.n_samples):\n      pi_log_prob[s] = tf.reduce_sum([p_log_probs[s][rv] for rv in var_p_rvs])\n      qi_log_prob[s] = tf.reduce_sum([q_log_probs[s][rv] for rv in var_q_rvs])\n\n    pi_log_prob = tf.stack(pi_log_prob)\n    qi_log_prob = tf.stack(qi_log_prob)\n    grad = tf.gradients(\n        -tf.reduce_mean(qi_log_prob *\n                        tf.stop_gradient(pi_log_prob - qi_log_prob)) +\n        tf.reduce_sum(tf.losses.get_regularization_losses()),\n        var)\n    grads.extend(grad)\n    grads_vars.append(var)\n\n  # Take gradients of total loss function for model parameters.\n  loss = -(tf.reduce_mean([tf.reduce_sum(list(six.itervalues(p_log_prob)))\n                           for p_log_prob in p_log_probs]) -\n           tf.reduce_mean([tf.reduce_sum(list(six.itervalues(q_log_prob)))\n                           for q_log_prob in q_log_probs]) -\n           tf.reduce_sum(tf.losses.get_regularization_losses()))\n  model_vars = [v for v in var_list if v not in grads_vars]\n  model_grads = tf.gradients(loss, model_vars)\n  grads.extend(model_grads)\n  grads_vars.extend(model_vars)\n  grads_and_vars = list(zip(grads, grads_vars))\n  return loss, grads_and_vars\n'"
edward/inferences/laplace.py,13,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.map import MAP\nfrom edward.models import PointMass, RandomVariable\nfrom edward.util import get_session, get_variables\nfrom edward.util import copy, transform\n\ntry:\n  from edward.models import \\\n      MultivariateNormalDiag, MultivariateNormalTriL, Normal\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass Laplace(MAP):\n  """"""Laplace approximation [@laplace1986memoir].\n\n  It approximates the posterior distribution using a multivariate\n  normal distribution centered at the mode of the posterior.\n\n  We implement this by running `MAP` to find the posterior mode.\n  This forms the mean of the normal approximation. We then compute the\n  inverse Hessian at the mode of the posterior. This forms the\n  covariance of the normal approximation.\n\n  #### Notes\n\n  If `MultivariateNormalDiag` or `Normal` random variables are\n  specified as approximations, then the Laplace approximation will\n  only produce the diagonal. This does not capture correlation among\n  the variables but it does not require a potentially expensive\n  matrix inversion.\n\n  Random variables with both scalar batch and event shape are not\n  supported as `tf.hessians` is currently not applicable to scalars.\n\n  Note that `Laplace` finds the location parameter of the normal\n  approximation using `MAP`, which is performed on the latent\n  variable\'s original (constrained) support. The scale parameter\n  is calculated by evaluating the Hessian of $-\\log p(x, z)$ in the\n  constrained space and under the mode. This implies the Laplace\n  approximation always has real support even if the target\n  distribution has constrained support.\n\n  #### Examples\n\n  ```python\n  X = tf.placeholder(tf.float32, [N, D])\n  w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n  y = Normal(loc=ed.dot(X, w), scale=tf.ones(N))\n\n  qw = MultivariateNormalTriL(\n      loc=tf.Variable(tf.random_normal([D])),\n      scale_tril=tf.Variable(tf.random_normal([D, D])))\n\n  inference = ed.Laplace({w: qw}, data={X: X_train, y: y_train})\n  ```\n  """"""\n  def __init__(self, latent_vars, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If list,\n        each random variable will be implictly optimized using a\n        `MultivariateNormalTriL` random variable that is defined\n        internally with unconstrained support and is initialized using\n        standard normal draws. If dictionary, each random\n        variable must be a `MultivariateNormalDiag`,\n        `MultivariateNormalTriL`, or `Normal` random variable.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        for z in latent_vars:\n          # Define location to have constrained support and\n          # unconstrained free parameters.\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          loc = tf.Variable(tf.random_normal(batch_event_shape))\n          if hasattr(z, \'support\'):\n            z_transform = transform(z)\n            if hasattr(z_transform, \'bijector\'):\n              loc = z_transform.bijector.inverse(loc)\n          scale_tril = tf.Variable(tf.random_normal(\n              batch_event_shape.concatenate(batch_event_shape[-1])))\n          qz = MultivariateNormalTriL(loc=loc, scale_tril=scale_tril)\n          latent_vars_dict[z] = qz\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n    elif isinstance(latent_vars, dict):\n      for qz in six.itervalues(latent_vars):\n        if not isinstance(\n                qz, (MultivariateNormalDiag, MultivariateNormalTriL, Normal)):\n          raise TypeError(""Posterior approximation must consist of only ""\n                          ""MultivariateNormalDiag, MultivariateTriL, or ""\n                          ""Normal random variables."")\n\n    # call grandparent\'s method; avoid parent (MAP)\n    super(MAP, self).__init__(latent_vars, data)\n\n  def initialize(self, *args, **kwargs):\n    # Store latent variables in a temporary object; MAP will\n    # optimize `PointMass` random variables, which subsequently\n    # optimizes location parameters of the normal approximations.\n    latent_vars_normal = self.latent_vars.copy()\n    self.latent_vars = {z: PointMass(params=qz.loc)\n                        for z, qz in six.iteritems(latent_vars_normal)}\n\n    super(Laplace, self).initialize(*args, **kwargs)\n\n    hessians = tf.hessians(self.loss, list(six.itervalues(self.latent_vars)))\n    self.finalize_ops = []\n    for z, hessian in zip(six.iterkeys(self.latent_vars), hessians):\n      qz = latent_vars_normal[z]\n      if isinstance(qz, (MultivariateNormalDiag, Normal)):\n        scale_var = get_variables(qz.variance())[0]\n        scale = 1.0 / tf.diag_part(hessian)\n      else:  # qz is MultivariateNormalTriL\n        scale_var = get_variables(qz.covariance())[0]\n        scale = tf.matrix_inverse(tf.cholesky(hessian))\n\n      self.finalize_ops.append(scale_var.assign(scale))\n\n    self.latent_vars = latent_vars_normal.copy()\n    del latent_vars_normal\n\n  def finalize(self, feed_dict=None):\n    """"""Function to call after convergence.\n\n    Computes the Hessian at the mode.\n\n    Args:\n      feed_dict: dict.\n        Feed dictionary for a TensorFlow session run during evaluation\n        of Hessian. It is used to feed placeholders that are not fed\n        during initialization.\n    """"""\n    if feed_dict is None:\n      feed_dict = {}\n\n    for key, value in six.iteritems(self.data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        feed_dict[key] = value\n\n    sess = get_session()\n    sess.run(self.finalize_ops, feed_dict)\n    super(Laplace, self).finalize()\n'"
edward/inferences/map.py,10,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.variational_inference import VariationalInference\nfrom edward.models import RandomVariable, PointMass\nfrom edward.util import copy, transform\n\ntry:\n  from tensorflow.contrib.distributions import bijectors\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass MAP(VariationalInference):\n  """"""Maximum a posteriori.\n\n  This class implements gradient-based optimization to solve the\n  optimization problem,\n\n  $\\min_{z} - p(z \\mid x).$\n\n  This is equivalent to using a `PointMass` variational distribution\n  and minimizing the unnormalized objective,\n\n  $- \\mathbb{E}_{q(z; \\lambda)} [ \\log p(x, z) ].$\n\n  #### Notes\n\n  This class is currently restricted to optimization over\n  differentiable latent variables. For example, it does not solve\n  discrete optimization.\n\n  This class also minimizes the loss with respect to any model\n  parameters $p(z \\mid x; \\\\theta)$.\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$. `MAP` optimizes\n  $\\mathbb{E}_{q(\\\\beta)} [ \\log p(x, z, \\\\beta) ]$, leveraging\n  a single Monte Carlo sample, $\\log p(x, z, \\\\beta^*)$, where\n  $\\\\beta^* \\sim q(\\\\beta)$. This is a lower bound to the\n  marginal density $\\log p(x, z)$, and it is exact if\n  $q(\\\\beta) = p(\\\\beta \\mid x)$ (up to stochasticity).\n\n  #### Examples\n\n  Most explicitly, `MAP` is specified via a dictionary:\n\n  ```python\n  qpi = PointMass(params=ed.to_simplex(tf.Variable(tf.zeros(K-1))))\n  qmu = PointMass(params=tf.Variable(tf.zeros(K*D)))\n  qsigma = PointMass(params=tf.nn.softplus(tf.Variable(tf.zeros(K*D))))\n  ed.MAP({pi: qpi, mu: qmu, sigma: qsigma}, data)\n  ```\n\n  We also automate the specification of `PointMass` distributions,\n  so one can pass in a list of latent variables instead:\n\n  ```python\n  ed.MAP([beta], data)\n  ed.MAP([pi, mu, sigma], data)\n  ```\n\n  Note that for `MAP` to optimize over latent variables with\n  constrained continuous support, the point mass must be constrained\n  to have the same support while its free parameters are\n  unconstrained; see, e.g., `qsigma` above. This is different than\n  performing MAP on the unconstrained space: in general, the MAP of\n  the transform is not the transform of the MAP.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list of RandomVariable or\n                   dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on. If\n        list, each random variable will be implictly optimized using a\n        `PointMass` random variable that is defined internally with\n        constrained support, has unconstrained free parameters, and is\n        initialized using standard normal draws. If dictionary, each\n        value in the dictionary must be a `PointMass` random variable\n        with the same support as the key.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars_dict = {}\n        for z in latent_vars:\n          # Define point masses to have constrained support and\n          # unconstrained free parameters.\n          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n          params = tf.Variable(tf.random_normal(batch_event_shape))\n          if hasattr(z, \'support\'):\n            z_transform = transform(z)\n            if hasattr(z_transform, \'bijector\'):\n              params = z_transform.bijector.inverse(params)\n          latent_vars_dict[z] = PointMass(params=params)\n        latent_vars = latent_vars_dict\n        del latent_vars_dict\n    elif isinstance(latent_vars, dict):\n      for qz in six.itervalues(latent_vars):\n        if not isinstance(qz, PointMass):\n          raise TypeError(""Posterior approximation must consist of only ""\n                          ""PointMass random variables."")\n\n    super(MAP, self).__init__(latent_vars, data)\n\n  def build_loss_and_gradients(self, var_list):\n    """"""Build loss function. Its automatic differentiation\n    is the gradient of\n\n    $- \\log p(x,z).$\n    """"""\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    scope = tf.get_default_graph().unique_name(""inference"")\n    dict_swap = {z: qz.value()\n                 for z, qz in six.iteritems(self.latent_vars)}\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          dict_swap[x] = qx.value()\n        else:\n          dict_swap[x] = qx\n\n    p_log_prob = 0.0\n    for z in six.iterkeys(self.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      p_log_prob += tf.reduce_sum(\n          self.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(self.data):\n      if isinstance(x, RandomVariable):\n        if dict_swap:\n          x_copy = copy(x, dict_swap, scope=scope)\n        else:\n          x_copy = x\n        p_log_prob += tf.reduce_sum(\n            self.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n    reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n    loss = -p_log_prob + reg_penalty\n\n    grads = tf.gradients(loss, var_list)\n    grads_and_vars = list(zip(grads, var_list))\n    return loss, grads_and_vars\n'"
edward/inferences/metropolis_hastings.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom collections import OrderedDict\nfrom edward.inferences.monte_carlo import MonteCarlo\nfrom edward.models import RandomVariable\nfrom edward.util import check_latent_vars, copy\n\n\nclass MetropolisHastings(MonteCarlo):\n  """"""Metropolis-Hastings [@metropolis1953equation; @hastings1970monte].\n\n  #### Notes\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$.\n  To calculate the acceptance ratio, `MetropolisHastings` uses an\n  estimate of the marginal density,\n\n  $p(x, z) = \\mathbb{E}_{q(\\\\beta)} [ p(x, z, \\\\beta) ]\n            \\\\approx p(x, z, \\\\beta^*)$\n\n  leveraging a single Monte Carlo sample, where $\\\\beta^* \\sim\n  q(\\\\beta)$. This is unbiased (and therefore asymptotically exact as a\n  pseudo-marginal method) if $q(\\\\beta) = p(\\\\beta \\mid x)$.\n\n  `MetropolisHastings` assumes the proposal distribution has the same\n  support as the prior. The `auto_transform` attribute in\n  the method `initialize()` is not applicable.\n\n  #### Examples\n\n  ```python\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=10)\n\n  qmu = Empirical(tf.Variable(tf.zeros(500)))\n  proposal_mu = Normal(loc=mu, scale=0.5)\n  inference = ed.MetropolisHastings({mu: qmu}, {mu: proposal_mu},\n                                    data={x: np.zeros(10, dtype=np.float32)})\n  ```\n  """"""\n  def __init__(self, latent_vars, proposal_vars, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      proposal_vars: dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on; each is\n        binded to a proposal distribution $g(z\' \\mid z)$.\n    """"""\n    check_latent_vars(proposal_vars)\n    self.proposal_vars = proposal_vars\n    super(MetropolisHastings, self).__init__(latent_vars, data)\n\n  def initialize(self, *args, **kwargs):\n    kwargs[\'auto_transform\'] = False\n    return super(MetropolisHastings, self).initialize(*args, **kwargs)\n\n  def build_update(self):\n    """"""Draw sample from proposal conditional on last sample. Then\n    accept or reject the sample based on the ratio,\n\n    $\\\\text{ratio} =\n          \\log p(x, z^{\\\\text{new}}) - \\log p(x, z^{\\\\text{old}}) -\n          \\log g(z^{\\\\text{new}} \\mid z^{\\\\text{old}}) +\n          \\log g(z^{\\\\text{old}} \\mid z^{\\\\text{new}})$\n\n    #### Notes\n\n    The updates assume each Empirical random variable is directly\n    parameterized by `tf.Variable`s.\n    """"""\n    old_sample = {z: tf.gather(qz.params, tf.maximum(self.t - 1, 0))\n                  for z, qz in six.iteritems(self.latent_vars)}\n    old_sample = OrderedDict(old_sample)\n\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    dict_swap = {}\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=\'conditional\')\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    dict_swap_old = dict_swap.copy()\n    dict_swap_old.update(old_sample)\n    base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n    scope_old = base_scope + \'old\'\n    scope_new = base_scope + \'new\'\n\n    # Draw proposed sample and calculate acceptance ratio.\n    new_sample = old_sample.copy()  # copy to ensure same order\n    ratio = 0.0\n    for z, proposal_z in six.iteritems(self.proposal_vars):\n      # Build proposal g(znew | zold).\n      proposal_znew = copy(proposal_z, dict_swap_old, scope=scope_old)\n      # Sample znew ~ g(znew | zold).\n      new_sample[z] = proposal_znew.value()\n      # Increment ratio.\n      ratio -= tf.reduce_sum(proposal_znew.log_prob(new_sample[z]))\n\n    dict_swap_new = dict_swap.copy()\n    dict_swap_new.update(new_sample)\n\n    for z, proposal_z in six.iteritems(self.proposal_vars):\n      # Build proposal g(zold | znew).\n      proposal_zold = copy(proposal_z, dict_swap_new, scope=scope_new)\n      # Increment ratio.\n      ratio += tf.reduce_sum(proposal_zold.log_prob(dict_swap_old[z]))\n\n    for z in six.iterkeys(self.latent_vars):\n      # Build priors p(znew) and p(zold).\n      znew = copy(z, dict_swap_new, scope=scope_new)\n      zold = copy(z, dict_swap_old, scope=scope_old)\n      # Increment ratio.\n      ratio += tf.reduce_sum(znew.log_prob(dict_swap_new[z]))\n      ratio -= tf.reduce_sum(zold.log_prob(dict_swap_old[z]))\n\n    for x in six.iterkeys(self.data):\n      if isinstance(x, RandomVariable):\n        # Build likelihoods p(x | znew) and p(x | zold).\n        x_znew = copy(x, dict_swap_new, scope=scope_new)\n        x_zold = copy(x, dict_swap_old, scope=scope_old)\n        # Increment ratio.\n        ratio += tf.reduce_sum(x_znew.log_prob(dict_swap[x]))\n        ratio -= tf.reduce_sum(x_zold.log_prob(dict_swap[x]))\n\n    # Accept or reject sample.\n    u = tf.random_uniform([], dtype=ratio.dtype)\n    accept = tf.log(u) < ratio\n    sample_values = tf.cond(accept, lambda: list(six.itervalues(new_sample)),\n                            lambda: list(six.itervalues(old_sample)))\n    if not isinstance(sample_values, list):\n      # `tf.cond` returns tf.Tensor if output is a list of size 1.\n      sample_values = [sample_values]\n\n    sample = {z: sample_value for z, sample_value in\n              zip(six.iterkeys(new_sample), sample_values)}\n\n    # Update Empirical random variables.\n    assign_ops = []\n    for z, qz in six.iteritems(self.latent_vars):\n      variable = qz.get_variables()[0]\n      assign_ops.append(tf.scatter_update(variable, self.t, sample[z]))\n\n    # Increment n_accept (if accepted).\n    assign_ops.append(self.n_accept.assign_add(tf.where(accept, 1, 0)))\n    return tf.group(*assign_ops)\n'"
edward/inferences/monte_carlo.py,14,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.inference import Inference\nfrom edward.models import Empirical, RandomVariable\nfrom edward.util import get_session\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass MonteCarlo(Inference):\n  """"""Abstract base class for Monte Carlo. Specific Monte Carlo methods\n  inherit from `MonteCarlo`, sharing methods in this class.\n\n  To build an algorithm inheriting from `MonteCarlo`, one must at the\n  minimum implement `build_update`: it determines how to assign\n  the samples in the `Empirical` approximations.\n\n  #### Notes\n\n  The number of Monte Carlo iterations is set according to the\n  minimum of all `Empirical` sizes.\n\n  Initialization is assumed from `params[0, :]`. This generalizes\n  initializing randomly and initializing from user input. Updates\n  are along this outer dimension, where iteration t updates\n  `params[t, :]` in each `Empirical` random variable.\n\n  No warm-up is implemented. Users must run MCMC for a long period\n  of time, then manually burn in the Empirical random variable.\n\n  #### Examples\n\n  Most explicitly, `MonteCarlo` is specified via a dictionary:\n\n  ```python\n  qpi = Empirical(params=tf.Variable(tf.zeros([T, K-1])))\n  qmu = Empirical(params=tf.Variable(tf.zeros([T, K*D])))\n  qsigma = Empirical(params=tf.Variable(tf.zeros([T, K*D])))\n  ed.MonteCarlo({pi: qpi, mu: qmu, sigma: qsigma}, data)\n  ```\n\n  The inferred posterior is comprised of `Empirical` random\n  variables with `T` samples. We also automate the specification\n  of `Empirical` random variables. One can pass in a list of\n  latent variables instead:\n\n  ```python\n  ed.MonteCarlo([beta], data)\n  ed.MonteCarlo([pi, mu, sigma], data)\n  ```\n\n  It defaults to `Empirical` random variables with 10,000 samples for\n  each dimension.\n  """"""\n  def __init__(self, latent_vars=None, data=None):\n    """"""Create an inference algorithm.\n\n    Args:\n      latent_vars: list or dict.\n        Collection of random variables (of type `RandomVariable` or\n        `tf.Tensor`) to perform inference on. If list, each random\n        variable will be approximated using a `Empirical` random\n        variable that is defined internally (with unconstrained\n        support). If dictionary, each value in the dictionary must be a\n        `Empirical` random variable.\n      data: dict.\n        Data dictionary which binds observed variables (of type\n        `RandomVariable` or `tf.Tensor`) to their realizations (of\n        type `tf.Tensor`). It can also bind placeholders (of type\n        `tf.Tensor`) used in the model to their realizations.\n    """"""\n    if isinstance(latent_vars, list):\n      with tf.variable_scope(None, default_name=""posterior""):\n        latent_vars = {z: Empirical(params=tf.Variable(tf.zeros(\n            [1e4] + z.batch_shape.concatenate(z.event_shape).as_list(),\n            dtype=z.dtype)))\n            for z in latent_vars}\n    elif isinstance(latent_vars, dict):\n      for qz in six.itervalues(latent_vars):\n        if not isinstance(qz, Empirical):\n          raise TypeError(""Posterior approximation must consist of only ""\n                          ""Empirical random variables."")\n        elif len(qz.sample_shape) != 0:\n          raise ValueError(""Empirical posterior approximations must have ""\n                           ""a scalar sample shape."")\n\n    super(MonteCarlo, self).__init__(latent_vars, data)\n\n  def initialize(self, *args, **kwargs):\n    kwargs[\'n_iter\'] = np.amin([qz.params.shape.as_list()[0] for\n                                qz in six.itervalues(self.latent_vars)])\n    super(MonteCarlo, self).initialize(*args, **kwargs)\n\n    self.n_accept = tf.Variable(0, trainable=False, name=""n_accept"")\n    self.n_accept_over_t = self.n_accept / self.t\n    self.train = self.build_update()\n\n    self.reset.append(tf.variables_initializer([self.n_accept]))\n\n    if self.logging:\n      tf.summary.scalar(""n_accept"", self.n_accept,\n                        collections=[self._summary_key])\n      self.summarize = tf.summary.merge_all(key=self._summary_key)\n\n  def update(self, feed_dict=None):\n    """"""Run one iteration of sampling.\n\n    Args:\n      feed_dict: dict.\n        Feed dictionary for a TensorFlow session run. It is used to feed\n        placeholders that are not fed during initialization.\n\n    Returns:\n      dict.\n      Dictionary of algorithm-specific information. In this case, the\n      acceptance rate of samples since (and including) this iteration.\n\n    #### Notes\n\n    We run the increment of `t` separately from other ops. Whether the\n    others op run with the `t` before incrementing or after incrementing\n    depends on which is run faster in the TensorFlow graph. Running it\n    separately forces a consistent behavior.\n    """"""\n    if feed_dict is None:\n      feed_dict = {}\n\n    for key, value in six.iteritems(self.data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        feed_dict[key] = value\n\n    sess = get_session()\n    _, accept_rate = sess.run([self.train, self.n_accept_over_t], feed_dict)\n    t = sess.run(self.increment_t)\n\n    if self.debug:\n      sess.run(self.op_check, feed_dict)\n\n    if self.logging and self.n_print != 0:\n      if t == 1 or t % self.n_print == 0:\n        summary = sess.run(self.summarize, feed_dict)\n        self.train_writer.add_summary(summary, t)\n\n    return {\'t\': t, \'accept_rate\': accept_rate}\n\n  def print_progress(self, info_dict):\n    """"""Print progress to output.\n    """"""\n    if self.n_print != 0:\n      t = info_dict[\'t\']\n      if t == 1 or t % self.n_print == 0:\n        self.progbar.update(t, {\'Acceptance Rate\': info_dict[\'accept_rate\']})\n\n  @abc.abstractmethod\n  def build_update(self):\n    """"""Build update rules, returning an assign op for parameters in\n    the `Empirical` random variables.\n\n    Any derived class of `MonteCarlo` **must** implement this method.\n\n    Raises:\n      NotImplementedError.\n    """"""\n    raise NotImplementedError()\n'"
edward/inferences/replica_exchange_mc.py,38,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\nimport numpy as np\n\nfrom collections import OrderedDict\nfrom edward.inferences.monte_carlo import MonteCarlo\nfrom edward.models import Empirical, RandomVariable\nfrom edward.util import check_latent_vars, copy\n\n\nclass _stateful_lambda:\n  """"""Class to use instead of lambda.\n  lambda is affected by the change of x,\n  so _stateful_lambda(x)() output x at the time of definition.\n  """"""\n\n  def __init__(self, x):\n    self.x = x\n\n  def __call__(self):\n    return self.x\n\n\nclass ReplicaExchangeMC(MonteCarlo):\n  """"""Replica Exchange MCMC [@swendsen1986replica; @hukushima1996exchange].\n\n  #### Examples\n  ```python\n  cat = Categorical(probs=[0.5,0.5])\n  x = Mixture(cat=cat, components=[\n      MultivariateNormalDiag([0.0,0.0], [1.0,1.0]),\n      MultivariateNormalDiag([10.0,10.0], [1.0,1.0])])\n  proposal_x = MultivariateNormalDiag(x, [1.0,1.0])\n  qx = Empirical(tf.Variable(tf.zeros([10000, 2])))\n  inference = ed.ReplicaExchangeMC(latent_vars={x: qx},\n                                   proposal_vars={x: proposal_x})\n  ```\n  """"""\n\n  def __init__(self, latent_vars, proposal_vars, data=None,\n               inverse_temperatures=np.logspace(0, -2, 5), exchange_freq=0.1):\n    """"""Create an inference algorithm.\n\n    Args:\n      proposal_vars: dict of RandomVariable to RandomVariable.\n        Collection of random variables to perform inference on; each is\n        binded to a proposal distribution $g(z\' \\mid z)$.\n      inverse_temperatures: list of inverse temperature.\n      exchange_freq: frequency of exchanging replica.\n    """"""\n    check_latent_vars(proposal_vars)\n    self.proposal_vars = proposal_vars\n    super(ReplicaExchangeMC, self).__init__(latent_vars, data)\n\n    self.n_replica = len(inverse_temperatures)\n    if inverse_temperatures[0] != 1:\n      raise ValueError(""inverse_temperatures[0] must be 1."")\n    self.inverse_temperatures = tf.cast(inverse_temperatures,\n                                        dtype=list(self.latent_vars)[0].dtype)\n\n    # Make replica.\n    self.replica_vars = []\n    for i in range(self.n_replica):\n      self.replica_vars.append({z: Empirical(params=tf.Variable(tf.zeros(\n          qz.params.shape, dtype=self.latent_vars[z].dtype))) for z, qz in\n          six.iteritems(self.latent_vars)})\n\n    self.exchange_freq = exchange_freq\n\n  def initialize(self, *args, **kwargs):\n    kwargs[\'auto_transform\'] = False\n    return super(ReplicaExchangeMC, self).initialize(*args, **kwargs)\n\n  def build_update(self):\n    """"""Perform sampling and exchange.\n    """"""\n    # Sample by Metropolis-Hastings for each replica.\n    replica_sample = []\n    replica_accept = []\n    for i in range(self.n_replica):\n      sample_, accept_ = self._mh_sample(self.replica_vars[i],\n                                         self.inverse_temperatures[i])\n      replica_sample.append(sample_)\n      replica_accept.append(accept_)\n    accept = replica_accept[0]\n\n    # Variable to store order of replicas after exchange\n    new_replica_idx = tf.Variable(tf.range(self.n_replica))\n    new_replica_idx = tf.assign(new_replica_idx, tf.range(self.n_replica))\n\n    # Variable to store ratio of current samples\n    replica_ratio = tf.Variable(tf.zeros(\n        self.n_replica, dtype=list(self.latent_vars)[0].dtype))\n    replica_ratio = self._replica_ratio(replica_ratio, replica_sample)\n\n    # Exchange adjacent replicas at frequency of exchange_freq\n    u = tf.random_uniform([])\n    exchange = u < self.exchange_freq\n    new_replica_idx = tf.cond(\n        exchange, lambda: self._replica_exchange(\n            new_replica_idx, replica_ratio), lambda: new_replica_idx)\n\n    # New replica sorted by new_replica_idx\n    new_replica_sample = []\n    for i in range(self.n_replica):\n      new_replica_sample.append(\n          {z: tf.case({tf.equal(tf.gather(new_replica_idx, i), j):\n                      _stateful_lambda(replica_sample[j][z])\n                      for j in range(self.n_replica)},\n           default=lambda: replica_sample[0][z], exclusive=True) for z, qz in\n           six.iteritems(self.latent_vars)})\n\n    assign_ops = []\n\n    # Update Empirical random variables.\n    for z, qz in six.iteritems(self.latent_vars):\n      variable = qz.get_variables()[0]\n      assign_ops.append(tf.scatter_update(variable, self.t,\n                                          new_replica_sample[0][z]))\n\n    for i in range(self.n_replica):\n      for z, qz in six.iteritems(self.replica_vars[i]):\n        variable = qz.get_variables()[0]\n        assign_ops.append(tf.scatter_update(variable, self.t,\n                                            new_replica_sample[i][z]))\n\n    # Increment n_accept (if accepted).\n    assign_ops.append(self.n_accept.assign_add(tf.where(accept, 1, 0)))\n\n    return tf.group(*assign_ops)\n\n  def _mh_sample(self, latent_vars, inverse_temperature):\n    """"""Draw sample by Metropolis-Hastings. Then\n    accept or reject the sample based on the ratio,\n    $\\\\text{ratio} = \\\\text{inverse_temperature}(\n          \\log p(x, z^{\\\\text{new}}) - \\log p(x, z^{\\\\text{old}}) -\n          \\log g(z^{\\\\text{new}} \\mid z^{\\\\text{old}}) +\n          \\log g(z^{\\\\text{old}} \\mid z^{\\\\text{new}}))$\n    """"""\n    old_sample = {z: tf.gather(qz.params, tf.maximum(self.t - 1, 0))\n                  for z, qz in six.iteritems(latent_vars)}\n    old_sample = OrderedDict(old_sample)\n\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    dict_swap = {}\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=\'conditional\')\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n    dict_swap_old = dict_swap.copy()\n    dict_swap_old.update(old_sample)\n    base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n    scope_old = base_scope + \'old\'\n    scope_new = base_scope + \'new\'\n\n    # Draw proposed sample and calculate acceptance ratio.\n    new_sample = old_sample.copy()  # copy to ensure same order\n    ratio = 0.0\n    for z, proposal_z in six.iteritems(self.proposal_vars):\n      # Build proposal g(znew | zold).\n      proposal_znew = copy(proposal_z, dict_swap_old, scope=scope_old)\n      # Sample znew ~ g(znew | zold).\n      new_sample[z] = proposal_znew.value()\n      # Increment ratio.\n      ratio -= tf.reduce_sum(proposal_znew.log_prob(new_sample[z]))\n\n    dict_swap_new = dict_swap.copy()\n    dict_swap_new.update(new_sample)\n\n    for z, proposal_z in six.iteritems(self.proposal_vars):\n      # Build proposal g(zold | znew).\n      proposal_zold = copy(proposal_z, dict_swap_new, scope=scope_new)\n      # Increment ratio.\n      ratio += tf.reduce_sum(proposal_zold.log_prob(dict_swap_old[z]))\n\n    for z in six.iterkeys(latent_vars):\n      # Build priors p(znew) and p(zold).\n      znew = copy(z, dict_swap_new, scope=scope_new)\n      zold = copy(z, dict_swap_old, scope=scope_old)\n      # Increment ratio.\n      ratio += tf.reduce_sum(znew.log_prob(dict_swap_new[z]))\n      ratio -= tf.reduce_sum(zold.log_prob(dict_swap_old[z]))\n\n    for x in six.iterkeys(self.data):\n      if isinstance(x, RandomVariable):\n        # Build likelihoods p(x | znew) and p(x | zold).\n        x_znew = copy(x, dict_swap_new, scope=scope_new)\n        x_zold = copy(x, dict_swap_old, scope=scope_old)\n        # Increment ratio.\n        ratio += tf.reduce_sum(x_znew.log_prob(dict_swap[x]))\n        ratio -= tf.reduce_sum(x_zold.log_prob(dict_swap[x]))\n\n    ratio *= inverse_temperature\n\n    # Accept or reject sample.\n    u = tf.random_uniform([], dtype=ratio.dtype)\n    accept = tf.log(u) < ratio\n    sample_values = tf.cond(accept, lambda: list(six.itervalues(new_sample)),\n                            lambda: list(six.itervalues(old_sample)))\n    if not isinstance(sample_values, list):\n      # `tf.cond` returns tf.Tensor if output is a list of size 1.\n      sample_values = [sample_values]\n\n    sample = {z: sample_value for z, sample_value in\n              zip(six.iterkeys(new_sample), sample_values)}\n    return sample, accept\n\n  def _replica_exchange(self, new_replica_idx, replica_ratio):\n    """"""Exchange replica according to the Metropolis-Hastings criterion.\n    $\\\\text{ratio} =\n          (\\log p(x, z_i) - \\log p(x, x_j))(\\\\text{inverse_temperature}_j -\n          \\\\text{inverse_temperature}_i)\n    """"""\n    i = tf.random_uniform((), maxval=2, dtype=tf.int32)\n\n    def cond(i, new_replica_idx):\n      return tf.less(i, self.n_replica - 1)\n\n    def body(i, new_replica_idx):\n      ratio = replica_ratio[i] - replica_ratio[i + 1]\n      ratio *= (self.inverse_temperatures[i + 1] - self.inverse_temperatures[i])\n      u = tf.random_uniform([], dtype=ratio.dtype)\n      exchange = tf.log(u) < ratio\n      new_replica_idx = tf.cond(\n          exchange,\n          lambda: tf.scatter_update(new_replica_idx, [i, i + 1], [i + 1, i]),\n          lambda: new_replica_idx)\n      return [i + 2, new_replica_idx]\n\n    return tf.while_loop(cond, body, loop_vars=[i, new_replica_idx])[1]\n\n  def _replica_ratio(self, replica_ratio, replica_sample):\n    replica_ratio = tf.assign(replica_ratio, tf.zeros(\n        self.n_replica, dtype=list(self.latent_vars)[0].dtype))\n\n    dict_swap = {}\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=\'conditional\')\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    for i in range(self.n_replica):\n      dict_swap_i = dict_swap.copy()\n      dict_swap_i.update(replica_sample[i])\n\n      base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n      scope_i = base_scope + \'_%d\' % i\n\n      for z in six.iterkeys(self.latent_vars):\n        # Build priors p(z_i) and p(z_j).\n        z_i = copy(z, dict_swap_i, scope=scope_i)\n        # Increment ratio.\n        replica_ratio = tf.scatter_update(\n            replica_ratio, i,\n            replica_ratio[i] + tf.reduce_sum(z_i.log_prob(dict_swap_i[z])))\n\n      for x in six.iterkeys(self.data):\n        if isinstance(x, RandomVariable):\n          # Build likelihoods p(x | z_i) and p(x | z_j).\n          x_z_i = copy(x, dict_swap_i, scope=scope_i)\n          # Increment ratio.\n          replica_ratio = tf.scatter_update(\n              replica_ratio, i,\n              replica_ratio[i] + tf.reduce_sum(x_z_i.log_prob(dict_swap[x])))\n    return replica_ratio\n'"
edward/inferences/sghmc.py,13,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.monte_carlo import MonteCarlo\nfrom edward.models import RandomVariable, Empirical\nfrom edward.util import copy\n\n\nclass SGHMC(MonteCarlo):\n  """"""Stochastic gradient Hamiltonian Monte Carlo [@chen2014stochastic].\n\n  #### Notes\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$.\n  `SGHMC` substitutes the model\'s log marginal density\n\n  $\\log p(x, z) = \\log \\mathbb{E}_{q(\\\\beta)} [ p(x, z, \\\\beta) ]\n                \\\\approx \\log p(x, z, \\\\beta^*)$\n\n  leveraging a single Monte Carlo sample, where $\\\\beta^* \\sim\n  q(\\\\beta)$. This is unbiased (and therefore asymptotically exact as a\n  pseudo-marginal method) if $q(\\\\beta) = p(\\\\beta \\mid x)$.\n\n  #### Examples\n\n  ```python\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=10)\n\n  qmu = Empirical(tf.Variable(tf.zeros(500)))\n  inference = ed.SGHMC({mu: qmu}, {x: np.zeros(10, dtype=np.float32)})\n  ```\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(SGHMC, self).__init__(*args, **kwargs)\n\n  def initialize(self, step_size=0.25, friction=0.1, *args, **kwargs):\n    """"""Initialize inference algorithm.\n\n    Args:\n      step_size: float.\n        Constant scale factor of learning rate.\n      friction: float.\n        Constant scale on the friction term in the Hamiltonian system.\n    """"""\n    self.step_size = step_size\n    self.friction = friction\n    self.v = {z: tf.Variable(tf.zeros(qz.params.shape[1:], dtype=qz.dtype))\n              for z, qz in six.iteritems(self.latent_vars)}\n    return super(SGHMC, self).initialize(*args, **kwargs)\n\n  def build_update(self):\n    """"""Simulate Hamiltonian dynamics with friction using a discretized\n    integrator. Its discretization error goes to zero as the learning\n    rate decreases.\n\n    Implements the update equations from (15) of @chen2014stochastic.\n    """"""\n    old_sample = {z: tf.gather(qz.params, tf.maximum(self.t - 1, 0))\n                  for z, qz in six.iteritems(self.latent_vars)}\n    old_v_sample = {z: v for z, v in six.iteritems(self.v)}\n\n    # Simulate Hamiltonian dynamics with friction.\n    learning_rate = self.step_size * 0.01\n    grad_log_joint = tf.gradients(self._log_joint(old_sample),\n                                  list(six.itervalues(old_sample)))\n\n    # v_sample is so named b/c it represents a velocity rather than momentum.\n    sample = {}\n    v_sample = {}\n    for z, grad_log_p in zip(six.iterkeys(old_sample), grad_log_joint):\n      qz = self.latent_vars[z]\n      event_shape = qz.event_shape\n      stddev = tf.sqrt(tf.cast(learning_rate * self.friction, qz.dtype))\n      normal = tf.random_normal(event_shape, dtype=qz.dtype)\n      sample[z] = old_sample[z] + old_v_sample[z]\n      v_sample[z] = ((1.0 - 0.5 * self.friction) * old_v_sample[z] +\n                     learning_rate * tf.convert_to_tensor(grad_log_p) +\n                     stddev * normal)\n\n    # Update Empirical random variables.\n    assign_ops = []\n    for z, qz in six.iteritems(self.latent_vars):\n      variable = qz.get_variables()[0]\n      assign_ops.append(tf.scatter_update(variable, self.t, sample[z]))\n      assign_ops.append(tf.assign(self.v[z], v_sample[z]).op)\n\n    # Increment n_accept.\n    assign_ops.append(self.n_accept.assign_add(1))\n    return tf.group(*assign_ops)\n\n  def _log_joint(self, z_sample):\n    """"""Utility function to calculate model\'s log joint density,\n    log p(x, z), for inputs z (and fixed data x).\n\n    Args:\n      z_sample: dict.\n        Latent variable keys to samples.\n    """"""\n    scope = tf.get_default_graph().unique_name(""inference"")\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    dict_swap = z_sample.copy()\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    log_joint = 0.0\n    for z in six.iterkeys(self.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      log_joint += tf.reduce_sum(\n          self.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(self.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        log_joint += tf.reduce_sum(\n            self.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n    return log_joint\n'"
edward/inferences/sgld.py,14,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.monte_carlo import MonteCarlo\nfrom edward.models import RandomVariable\nfrom edward.util import copy\n\n\nclass SGLD(MonteCarlo):\n  """"""Stochastic gradient Langevin dynamics [@welling2011bayesian].\n\n  #### Notes\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$.\n  `SGLD` substitutes the model\'s log marginal density\n\n  $\\log p(x, z) = \\log \\mathbb{E}_{q(\\\\beta)} [ p(x, z, \\\\beta) ]\n                \\\\approx \\log p(x, z, \\\\beta^*)$\n\n  leveraging a single Monte Carlo sample, where $\\\\beta^* \\sim\n  q(\\\\beta)$. This is unbiased (and therefore asymptotically exact as a\n  pseudo-marginal method) if $q(\\\\beta) = p(\\\\beta \\mid x)$.\n\n  #### Examples\n\n  ```python\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=10)\n\n  qmu = Empirical(tf.Variable(tf.zeros(500)))\n  inference = ed.SGLD({mu: qmu}, {x: np.zeros(10, dtype=np.float32)})\n  ```\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(SGLD, self).__init__(*args, **kwargs)\n\n  def initialize(self, step_size=0.25, *args, **kwargs):\n    """"""\n    Args:\n      step_size: float.\n        Constant scale factor of learning rate.\n    """"""\n    self.step_size = step_size\n    return super(SGLD, self).initialize(*args, **kwargs)\n\n  def build_update(self):\n    """"""Simulate Langevin dynamics using a discretized integrator. Its\n    discretization error goes to zero as the learning rate decreases.\n\n    #### Notes\n\n    The updates assume each Empirical random variable is directly\n    parameterized by `tf.Variable`s.\n    """"""\n    old_sample = {z: tf.gather(qz.params, tf.maximum(self.t - 1, 0))\n                  for z, qz in six.iteritems(self.latent_vars)}\n\n    # Simulate Langevin dynamics.\n    learning_rate = self.step_size / tf.pow(\n        tf.cast(self.t + 1, list(six.iterkeys(old_sample))[0].dtype), 0.55)\n    grad_log_joint = tf.gradients(self._log_joint(old_sample),\n                                  list(six.itervalues(old_sample)))\n    sample = {}\n    for z, grad_log_p in zip(six.iterkeys(old_sample), grad_log_joint):\n      qz = self.latent_vars[z]\n      event_shape = qz.event_shape\n      stddev = tf.sqrt(tf.cast(learning_rate, qz.dtype))\n      normal = tf.random_normal(event_shape, dtype=qz.dtype)\n      sample[z] = (old_sample[z] +\n                   0.5 * learning_rate * tf.convert_to_tensor(grad_log_p) +\n                   stddev * normal)\n\n    # Update Empirical random variables.\n    assign_ops = []\n    for z, qz in six.iteritems(self.latent_vars):\n      variable = qz.get_variables()[0]\n      assign_ops.append(tf.scatter_update(variable, self.t, sample[z]))\n\n    # Increment n_accept.\n    assign_ops.append(self.n_accept.assign_add(1))\n    return tf.group(*assign_ops)\n\n  def _log_joint(self, z_sample):\n    """"""Utility function to calculate model\'s log joint density,\n    log p(x, z), for inputs z (and fixed data x).\n\n    Args:\n      z_sample: dict.\n        Latent variable keys to samples.\n    """"""\n    scope = tf.get_default_graph().unique_name(""inference"")\n    # Form dictionary in order to replace conditioning on prior or\n    # observed variable with conditioning on a specific value.\n    dict_swap = z_sample.copy()\n    for x, qx in six.iteritems(self.data):\n      if isinstance(x, RandomVariable):\n        if isinstance(qx, RandomVariable):\n          qx_copy = copy(qx, scope=scope)\n          dict_swap[x] = qx_copy.value()\n        else:\n          dict_swap[x] = qx\n\n    log_joint = 0.0\n    for z in six.iterkeys(self.latent_vars):\n      z_copy = copy(z, dict_swap, scope=scope)\n      log_joint += tf.reduce_sum(\n          self.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n\n    for x in six.iterkeys(self.data):\n      if isinstance(x, RandomVariable):\n        x_copy = copy(x, dict_swap, scope=scope)\n        log_joint += tf.reduce_sum(\n            self.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n\n    return log_joint\n'"
edward/inferences/variational_inference.py,26,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.inference import Inference\nfrom edward.models import RandomVariable\nfrom edward.util import get_session, get_variables\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass VariationalInference(Inference):\n  """"""Abstract base class for variational inference. Specific\n  variational inference methods inherit from `VariationalInference`,\n  sharing methods such as a default optimizer.\n\n  To build an algorithm inheriting from `VariationalInference`, one\n  must at the minimum implement `build_loss_and_gradients`: it\n  determines the loss function and gradients to apply for a given\n  optimizer.\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(VariationalInference, self).__init__(*args, **kwargs)\n\n  def initialize(self, optimizer=None, var_list=None, use_prettytensor=False,\n                 global_step=None, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      optimizer: str or tf.train.Optimizer.\n        A TensorFlow optimizer, to use for optimizing the variational\n        objective. Alternatively, one can pass in the name of a\n        TensorFlow optimizer, and default parameters for the optimizer\n        will be used.\n      var_list: list of tf.Variable.\n        List of TensorFlow variables to optimize over. Default is all\n        trainable variables that `latent_vars` and `data` depend on,\n        excluding those that are only used in conditionals in `data`.\n      use_prettytensor: bool.\n        `True` if aim to use PrettyTensor optimizer (when using\n        PrettyTensor) or `False` if aim to use TensorFlow optimizer.\n        Defaults to TensorFlow.\n      global_step: tf.Variable.\n        A TensorFlow variable to hold the global step.\n    """"""\n    super(VariationalInference, self).initialize(*args, **kwargs)\n\n    if var_list is None:\n      # Traverse random variable graphs to get default list of variables.\n      var_list = set()\n      trainables = tf.trainable_variables()\n      for z, qz in six.iteritems(self.latent_vars):\n        var_list.update(get_variables(z, collection=trainables))\n        var_list.update(get_variables(qz, collection=trainables))\n\n      for x, qx in six.iteritems(self.data):\n        if isinstance(x, RandomVariable) and \\\n                not isinstance(qx, RandomVariable):\n          var_list.update(get_variables(x, collection=trainables))\n\n      var_list = list(var_list)\n\n    self.loss, grads_and_vars = self.build_loss_and_gradients(var_list)\n\n    if self.logging:\n      tf.summary.scalar(""loss"", self.loss, collections=[self._summary_key])\n      for grad, var in grads_and_vars:\n        # replace colons which are an invalid character\n        tf.summary.histogram(""gradient/"" +\n                             var.name.replace(\':\', \'/\'),\n                             grad, collections=[self._summary_key])\n        tf.summary.scalar(""gradient_norm/"" +\n                          var.name.replace(\':\', \'/\'),\n                          tf.norm(grad), collections=[self._summary_key])\n\n      self.summarize = tf.summary.merge_all(key=self._summary_key)\n\n    if optimizer is None and global_step is None:\n      # Default optimizer always uses a global step variable.\n      global_step = tf.Variable(0, trainable=False, name=""global_step"")\n\n    if isinstance(global_step, tf.Variable):\n      starter_learning_rate = 0.1\n      learning_rate = tf.train.exponential_decay(starter_learning_rate,\n                                                 global_step,\n                                                 100, 0.9, staircase=True)\n    else:\n      learning_rate = 0.01\n\n    # Build optimizer.\n    if optimizer is None:\n      optimizer = tf.train.AdamOptimizer(learning_rate)\n    elif isinstance(optimizer, str):\n      if optimizer == \'gradientdescent\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n      elif optimizer == \'adadelta\':\n        optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n      elif optimizer == \'adagrad\':\n        optimizer = tf.train.AdagradOptimizer(learning_rate)\n      elif optimizer == \'momentum\':\n        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n      elif optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n      elif optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(learning_rate)\n      elif optimizer == \'rmsprop\':\n        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n      else:\n        raise ValueError(\'Optimizer class not found:\', optimizer)\n    elif not isinstance(optimizer, tf.train.Optimizer):\n      raise TypeError(""Optimizer must be str, tf.train.Optimizer, or None."")\n\n    with tf.variable_scope(None, default_name=""optimizer"") as scope:\n      if not use_prettytensor:\n        self.train = optimizer.apply_gradients(grads_and_vars,\n                                               global_step=global_step)\n      else:\n        import prettytensor as pt\n        # Note PrettyTensor optimizer does not accept manual updates;\n        # it autodiffs the loss directly.\n        self.train = pt.apply_optimizer(optimizer, losses=[self.loss],\n                                        global_step=global_step,\n                                        var_list=var_list)\n\n    self.reset.append(tf.variables_initializer(\n        tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)))\n\n  def update(self, feed_dict=None):\n    """"""Run one iteration of optimization.\n\n    Args:\n      feed_dict: dict.\n        Feed dictionary for a TensorFlow session run. It is used to feed\n        placeholders that are not fed during initialization.\n\n    Returns:\n      dict.\n      Dictionary of algorithm-specific information. In this case, the\n      loss function value after one iteration.\n    """"""\n    if feed_dict is None:\n      feed_dict = {}\n\n    for key, value in six.iteritems(self.data):\n      if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n        feed_dict[key] = value\n\n    sess = get_session()\n    _, t, loss = sess.run([self.train, self.increment_t, self.loss], feed_dict)\n\n    if self.debug:\n      sess.run(self.op_check, feed_dict)\n\n    if self.logging and self.n_print != 0:\n      if t == 1 or t % self.n_print == 0:\n        summary = sess.run(self.summarize, feed_dict)\n        self.train_writer.add_summary(summary, t)\n\n    return {\'t\': t, \'loss\': loss}\n\n  def print_progress(self, info_dict):\n    """"""Print progress to output.\n    """"""\n    if self.n_print != 0:\n      t = info_dict[\'t\']\n      if t == 1 or t % self.n_print == 0:\n        self.progbar.update(t, {\'Loss\': info_dict[\'loss\']})\n\n  @abc.abstractmethod\n  def build_loss_and_gradients(self, var_list):\n    """"""Build loss function and its gradients. They will be leveraged\n    in an optimizer to update the model and variational parameters.\n\n    Any derived class of `VariationalInference` **must** implement\n    this method.\n\n    Raises:\n      NotImplementedError.\n    """"""\n    raise NotImplementedError()\n'"
edward/inferences/wake_sleep.py,18,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.variational_inference import VariationalInference\nfrom edward.models import RandomVariable\nfrom edward.util import copy, get_descendants\n\n\nclass WakeSleep(VariationalInference):\n  """"""Wake-Sleep algorithm [@hinton1995wake].\n\n  Given a probability model $p(x, z; \\\\theta)$ and variational\n  distribution $q(z\\mid x; \\\\lambda)$, wake-sleep alternates between\n  two phases:\n\n  + In the wake phase, $\\log p(x, z; \\\\theta)$ is maximized with\n  respect to model parameters $\\\\theta$ using bottom-up samples\n  $z\\sim q(z\\mid x; \\lambda)$.\n  + In the sleep phase, $\\log q(z\\mid x; \\lambda)$ is maximized with\n  respect to variational parameters $\\lambda$ using top-down\n  ""fantasy"" samples $z\\sim p(x, z; \\\\theta)$.\n\n  @hinton1995wake justify wake-sleep under the variational lower\n  bound of the description length,\n\n  $\\mathbb{E}_{q(z\\mid x; \\lambda)} [\n      \\log p(x, z; \\\\theta) - \\log q(z\\mid x; \\lambda)].$\n\n  Maximizing it with respect to $\\\\theta$ corresponds to the wake phase.\n  Instead of maximizing it with respect to $\\lambda$ (which\n  corresponds to minimizing $\\\\text{KL}(q\\|p)$), the sleep phase\n  corresponds to minimizing the reverse KL $\\\\text{KL}(p\\|q)$ in\n  expectation over the data distribution.\n\n  #### Notes\n\n  In conditional inference, we infer $z$ in $p(z, \\\\beta\n  \\mid x)$ while fixing inference over $\\\\beta$ using another\n  distribution $q(\\\\beta)$. During gradient calculation, instead\n  of using the model\'s density\n\n  $\\log p(x, z^{(s)}), z^{(s)} \\sim q(z; \\lambda),$\n\n  for each sample $s=1,\\ldots,S$, `WakeSleep` uses\n\n  $\\log p(x, z^{(s)}, \\\\beta^{(s)}),$\n\n  where $z^{(s)} \\sim q(z; \\lambda)$ and $\\\\beta^{(s)}\n  \\sim q(\\\\beta)$.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(WakeSleep, self).__init__(*args, **kwargs)\n\n  def initialize(self, n_samples=1, phase_q=\'sleep\', *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      n_samples: int.\n        Number of samples for calculating stochastic gradients during\n        wake and sleep phases.\n      phase_q: str.\n        Phase for updating parameters of q. If \'sleep\', update using\n        a sample from p. If \'wake\', update using a sample from q.\n        (Unlike reparameterization gradients, the sample is held\n        fixed.)\n    """"""\n    self.n_samples = n_samples\n    self.phase_q = phase_q\n    return super(WakeSleep, self).initialize(*args, **kwargs)\n\n  def build_loss_and_gradients(self, var_list):\n    p_log_prob = [0.0] * self.n_samples\n    q_log_prob = [0.0] * self.n_samples\n    base_scope = tf.get_default_graph().unique_name(""inference"") + \'/\'\n    for s in range(self.n_samples):\n      # Form dictionary in order to replace conditioning on prior or\n      # observed variable with conditioning on a specific value.\n      scope = base_scope + tf.get_default_graph().unique_name(""q_sample"")\n      dict_swap = {}\n      for x, qx in six.iteritems(self.data):\n        if isinstance(x, RandomVariable):\n          if isinstance(qx, RandomVariable):\n            qx_copy = copy(qx, scope=scope)\n            dict_swap[x] = qx_copy.value()\n          else:\n            dict_swap[x] = qx\n\n      # Sample z ~ q(z), then compute log p(x, z).\n      q_dict_swap = dict_swap.copy()\n      for z, qz in six.iteritems(self.latent_vars):\n        # Copy q(z) to obtain new set of posterior samples.\n        qz_copy = copy(qz, scope=scope)\n        q_dict_swap[z] = qz_copy.value()\n        if self.phase_q != \'sleep\':\n          # If not sleep phase, compute log q(z).\n          q_log_prob[s] += tf.reduce_sum(\n              self.scale.get(z, 1.0) *\n              qz_copy.log_prob(tf.stop_gradient(q_dict_swap[z])))\n\n      for z in six.iterkeys(self.latent_vars):\n        z_copy = copy(z, q_dict_swap, scope=scope)\n        p_log_prob[s] += tf.reduce_sum(\n            self.scale.get(z, 1.0) * z_copy.log_prob(q_dict_swap[z]))\n\n      for x in six.iterkeys(self.data):\n        if isinstance(x, RandomVariable):\n          x_copy = copy(x, q_dict_swap, scope=scope)\n          p_log_prob[s] += tf.reduce_sum(\n              self.scale.get(x, 1.0) * x_copy.log_prob(q_dict_swap[x]))\n\n      if self.phase_q == \'sleep\':\n        # Sample z ~ p(z), then compute log q(z).\n        scope = base_scope + tf.get_default_graph().unique_name(""p_sample"")\n        p_dict_swap = dict_swap.copy()\n        for z, qz in six.iteritems(self.latent_vars):\n          # Copy p(z) to obtain new set of prior samples.\n          z_copy = copy(z, scope=scope)\n          p_dict_swap[qz] = z_copy.value()\n        for qz in six.itervalues(self.latent_vars):\n          qz_copy = copy(qz, p_dict_swap, scope=scope)\n          q_log_prob[s] += tf.reduce_sum(\n              self.scale.get(z, 1.0) *\n              qz_copy.log_prob(tf.stop_gradient(p_dict_swap[qz])))\n\n    p_log_prob = tf.reduce_mean(p_log_prob)\n    q_log_prob = tf.reduce_mean(q_log_prob)\n    reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n\n    if self.logging:\n      tf.summary.scalar(""loss/p_log_prob"", p_log_prob,\n                        collections=[self._summary_key])\n      tf.summary.scalar(""loss/q_log_prob"", q_log_prob,\n                        collections=[self._summary_key])\n      tf.summary.scalar(""loss/reg_penalty"", reg_penalty,\n                        collections=[self._summary_key])\n\n    loss_p = -p_log_prob + reg_penalty\n    loss_q = -q_log_prob + reg_penalty\n\n    q_rvs = list(six.itervalues(self.latent_vars))\n    q_vars = [v for v in var_list\n              if len(get_descendants(tf.convert_to_tensor(v), q_rvs)) != 0]\n    q_grads = tf.gradients(loss_q, q_vars)\n    p_vars = [v for v in var_list if v not in q_vars]\n    p_grads = tf.gradients(loss_p, p_vars)\n    grads_and_vars = list(zip(q_grads, q_vars)) + list(zip(p_grads, p_vars))\n    return loss_p, grads_and_vars\n'"
edward/inferences/wgan_inference.py,22,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.inferences.gan_inference import GANInference\nfrom edward.util import get_session\n\n\nclass WGANInference(GANInference):\n  """"""Parameter estimation with GAN-style training\n  [@goodfellow2014generative], using the Wasserstein distance\n  [@arjovsky2017wasserstein].\n\n  Works for the class of implicit (and differentiable) probabilistic\n  models. These models do not require a tractable density and assume\n  only a program that generates samples.\n\n  #### Notes\n\n  Argument-wise, the only difference from `GANInference` is\n  conceptual: the `discriminator` is better described as a test\n  function or critic. `WGANInference` continues to use\n  `discriminator` only to share methods and attributes with\n  `GANInference`.\n\n  The objective function also adds to itself a summation over all\n  tensors in the `REGULARIZATION_LOSSES` collection.\n\n  #### Examples\n\n  ```python\n  z = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))\n  x = generative_network(z)\n\n  inference = ed.WGANInference({x: x_data}, discriminator)\n  ```\n  """"""\n  def __init__(self, *args, **kwargs):\n    super(WGANInference, self).__init__(*args, **kwargs)\n\n  def initialize(self, penalty=10.0, clip=None, *args, **kwargs):\n    """"""Initialize inference algorithm. It initializes hyperparameters\n    and builds ops for the algorithm\'s computation graph.\n\n    Args:\n      penalty: float.\n        Scalar value to enforce gradient penalty that ensures the\n        gradients have norm equal to 1 [@gulrajani2017improved]. Set to\n        None (or 0.0) if using no penalty.\n      clip: float.\n        Value to clip weights by. Default is no clipping.\n    """"""\n    self.penalty = penalty\n\n    super(WGANInference, self).initialize(*args, **kwargs)\n\n    self.clip_op = None\n    if clip is not None:\n      var_list = tf.get_collection(\n          tf.GraphKeys.TRAINABLE_VARIABLES, scope=""Disc"")\n      self.clip_op = [w.assign(tf.clip_by_value(w, -clip, clip))\n                      for w in var_list]\n\n  def build_loss_and_gradients(self, var_list):\n    x_true = list(six.itervalues(self.data))[0]\n    x_fake = list(six.iterkeys(self.data))[0]\n    with tf.variable_scope(""Disc""):\n      d_true = self.discriminator(x_true)\n\n    with tf.variable_scope(""Disc"", reuse=True):\n      d_fake = self.discriminator(x_fake)\n\n    if self.penalty is None or self.penalty == 0:\n      penalty = 0.0\n    else:\n      eps = tf.random_uniform(tf.shape(x_true))\n      x_interpolated = eps * x_true + (1.0 - eps) * x_fake\n      with tf.variable_scope(""Disc"", reuse=True):\n        d_interpolated = self.discriminator(x_interpolated)\n\n      gradients = tf.gradients(d_interpolated, [x_interpolated])[0]\n      slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients),\n                                     list(range(1, gradients.shape.ndims))))\n      penalty = self.penalty * tf.reduce_mean(tf.square(slopes - 1.0))\n\n    reg_terms_d = tf.losses.get_regularization_losses(scope=""Disc"")\n    reg_terms_all = tf.losses.get_regularization_losses()\n    reg_terms = [r for r in reg_terms_all if r not in reg_terms_d]\n\n    mean_true = tf.reduce_mean(d_true)\n    mean_fake = tf.reduce_mean(d_fake)\n    loss_d = mean_fake - mean_true + penalty + tf.reduce_sum(reg_terms_d)\n    loss = -mean_fake + tf.reduce_sum(reg_terms)\n\n    var_list_d = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=""Disc"")\n    if var_list is None:\n      var_list = [v for v in tf.trainable_variables() if v not in var_list_d]\n\n    grads_d = tf.gradients(loss_d, var_list_d)\n    grads = tf.gradients(loss, var_list)\n    grads_and_vars_d = list(zip(grads_d, var_list_d))\n    grads_and_vars = list(zip(grads, var_list))\n    return loss, grads_and_vars, loss_d, grads_and_vars_d\n\n  def update(self, feed_dict=None, variables=None):\n    info_dict = super(WGANInference, self).update(feed_dict, variables)\n\n    sess = get_session()\n    if self.clip_op is not None and variables in (None, ""Disc""):\n      sess.run(self.clip_op)\n\n    return info_dict\n'"
edward/models/__init__.py,0,"b'""""""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom edward.models.dirichlet_process import *\nfrom edward.models.empirical import *\nfrom edward.models.param_mixture import *\nfrom edward.models.point_mass import *\nfrom edward.models.random_variable import RandomVariable\nfrom edward.models.random_variables import *\n\nfrom tensorflow.python.util.all_util import remove_undocumented\nfrom edward.models import random_variables as _module\n\n_allowed_symbols = [\n    \'DirichletProcess\',\n    \'Empirical\',\n    \'ParamMixture\',\n    \'PointMass\',\n    \'RandomVariable\',\n]\nfor name in dir(_module):\n  obj = getattr(_module, name)\n  if (isinstance(obj, type) and\n          issubclass(obj, RandomVariable) and\n          obj != RandomVariable):\n    _allowed_symbols.append(name)\n\nremove_undocumented(__name__, allowed_exception_list=_allowed_symbols)\n'"
edward/models/dirichlet_process.py,35,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models.random_variable import RandomVariable\nfrom tensorflow.contrib.distributions import Distribution\n\ntry:\n  from edward.models.random_variables import Bernoulli, Beta\n  from tensorflow.contrib.distributions import NOT_REPARAMETERIZED\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass distributions_DirichletProcess(Distribution):\n  """"""Dirichlet process $\\mathcal{DP}(\\\\alpha, H)$.\n\n  It has two parameters: a positive real value $\\\\alpha$, known\n  as the concentration parameter (`concentration`), and a base\n  distribution $H$ (`base`).\n\n  #### Examples\n\n  ```python\n  # scalar concentration parameter, scalar base distribution\n  dp = DirichletProcess(0.1, Normal(loc=0.0, scale=1.0))\n  assert dp.shape == ()\n\n  # vector of concentration parameters, matrix of Exponentials\n  dp = DirichletProcess(tf.constant([0.1, 0.4]),\n                        Exponential(lam=tf.ones([5, 3])))\n  assert dp.shape == (2, 5, 3)\n  ```\n  """"""\n  def __init__(self,\n               concentration,\n               base,\n               validate_args=False,\n               allow_nan_stats=True,\n               name=""DirichletProcess""):\n    """"""Initialize a batch of Dirichlet processes.\n\n    Args:\n      concentration: tf.Tensor.\n        Concentration parameter. Must be positive real-valued. Its shape\n        determines the number of independent DPs (batch shape).\n      base: RandomVariable.\n        Base distribution. Its shape determines the shape of an\n        individual DP (event shape).\n    """"""\n    parameters = locals()\n    with tf.name_scope(name, values=[concentration]):\n      with tf.control_dependencies([\n          tf.assert_positive(concentration),\n      ] if validate_args else []):\n        if validate_args and isinstance(base, RandomVariable):\n          raise TypeError(""base must be a ed.RandomVariable object."")\n\n        self._concentration = tf.identity(concentration, name=""concentration"")\n        self._base = base\n\n        # Form empty tensor to store atom locations.\n        self._locs = tf.zeros(\n            [0] + self.batch_shape.as_list() + self.event_shape.as_list(),\n            dtype=self._base.dtype)\n\n        # Instantiate distribution to draw mixing proportions.\n        self._probs_dist = Beta(tf.ones_like(self._concentration),\n                                self._concentration,\n                                collections=[])\n        # Form empty tensor to store mixing proportions.\n        self._probs = tf.zeros(\n            [0] + self.batch_shape.as_list(),\n            dtype=self._probs_dist.dtype)\n\n    super(distributions_DirichletProcess, self).__init__(\n        dtype=tf.int32,\n        reparameterization_type=NOT_REPARAMETERIZED,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats,\n        parameters=parameters,\n        graph_parents=[self._concentration, self._locs, self._probs],\n        name=name)\n\n  @property\n  def base(self):\n    """"""Base distribution used for drawing the atom locations.""""""\n    return self._base\n\n  @property\n  def concentration(self):\n    """"""Concentration parameter.""""""\n    return self._concentration\n\n  @property\n  def locs(self):\n    """"""Atom locations. It has shape [None] + batch_shape +\n    event_shape, where the first dimension is the number of atoms,\n    instantiated only as needed.""""""\n    return self._locs\n\n  @property\n  def probs(self):\n    """"""Mixing proportions. It has shape [None] + batch_shape, where\n    the first dimension is the number of atoms, instantiated only as\n    needed.""""""\n    return self._probs\n\n  def _batch_shape_tensor(self):\n    return tf.shape(self.concentration)\n\n  def _batch_shape(self):\n    return self.concentration.shape\n\n  def _event_shape_tensor(self):\n    return tf.shape(self.base)\n\n  def _event_shape(self):\n    return self.base.shape\n\n  def _sample_n(self, n, seed=None):\n    """"""Sample `n` draws from the DP. Draws from the base\n    distribution are memoized across `n` and across calls to\n    `sample()`.\n\n    Draws from the base distribution are not memoized across the batch\n    shape, i.e., each independent DP in the batch shape has its own\n    memoized samples.\n\n    Returns:\n      tf.Tensor.\n      A `tf.Tensor` of shape `[n] + batch_shape + event_shape`,\n      where `n` is the number of samples for each DP,\n      `batch_shape` is the number of independent DPs, and\n      `event_shape` is the shape of the base distribution.\n\n    #### Notes\n\n    The implementation has one inefficiency, which is that it draws\n    (batch_shape,) samples from the base distribution when adding a\n    new persistent state. Ideally, we would only draw new samples for\n    those in the loop which require it.\n    """"""\n    if seed is not None:\n      raise NotImplementedError(""seed is not implemented."")\n\n    batch_shape = self.batch_shape.as_list()\n    event_shape = self.event_shape.as_list()\n    rank = 1 + len(batch_shape) + len(event_shape)\n    # Note this is for scoping within the while loop\'s body function.\n    self._temp_scope = [n, batch_shape, event_shape, rank]\n\n    # Start at the beginning of the stick, i.e. the k\'th index\n    k = tf.constant(0)\n\n    # Define boolean tensor. It is True for samples that require continuing\n    # the while loop and False for samples that can receive their base\n    # distribution (coin lands heads). Also note that we need one bool for\n    # each sample\n    bools = tf.ones([n] + batch_shape, dtype=tf.bool)\n\n    # Initialize all samples as zero, they will be overwritten in any case\n    draws = tf.zeros([n] + batch_shape + event_shape, dtype=self.base.dtype)\n\n    # Calculate shape invariance conditions for locs and probs as these\n    # can change shape between loop iterations.\n    locs_shape = tf.TensorShape([None])\n    probs_shape = tf.TensorShape([None])\n    if len(self.locs.shape) > 1:\n      locs_shape = locs_shape.concatenate(self.locs.shape[1:])\n      probs_shape = probs_shape.concatenate(self.probs.shape[1:])\n\n    # While we have not broken enough sticks, keep sampling.\n    _, _, self._locs, self._probs, samples = tf.while_loop(\n        self._sample_n_cond, self._sample_n_body,\n        loop_vars=[k, bools, self.locs, self.probs, draws],\n        shape_invariants=[\n            k.shape, bools.shape, locs_shape, probs_shape, draws.shape])\n\n    return samples\n\n  def _sample_n_cond(self, k, bools, locs, probs, draws):\n    # Proceed if at least one bool is True.\n    return tf.reduce_any(bools)\n\n  def _sample_n_body(self, k, bools, locs, probs, draws):\n    n, batch_shape, event_shape, rank = self._temp_scope\n\n    # If necessary, break a new piece of stick, i.e.\n    # add a new persistent atom location and weight.\n    locs, probs = tf.cond(\n        tf.shape(locs)[0] - 1 >= k,\n        lambda: (locs, probs),\n        lambda: (\n            tf.concat(\n                [locs, tf.expand_dims(self.base.sample(batch_shape), 0)], 0),\n            tf.concat(\n                [probs, tf.expand_dims(self._probs_dist.sample(), 0)], 0)))\n    locs_k = tf.gather(locs, k)\n    probs_k = tf.gather(probs, k)\n\n    # Assign True samples to the new locs_k.\n    if len(bools.shape) <= 1:\n      bools_tile = bools\n    else:\n      # `tf.where` only index subsets when `bools` is at most a\n      # vector. In general, `bools` has shape (n, batch_shape).\n      # Therefore we tile `bools` to be of shape\n      # (n, batch_shape, event_shape) in order to index per-element.\n      bools_tile = tf.tile(tf.reshape(\n          bools, [n] + batch_shape + [1] * len(event_shape)),\n          [1] + [1] * len(batch_shape) + event_shape)\n\n    locs_k_tile = tf.tile(tf.expand_dims(locs_k, 0), [n] + [1] * (rank - 1))\n    draws = tf.where(bools_tile, locs_k_tile, draws)\n\n    # Flip coins according to stick probabilities.\n    flips = Bernoulli(probs=probs_k).sample(n)\n    # If coin lands heads, assign sample\'s corresponding bool to False\n    # (this ends its ""while loop"").\n    bools = tf.where(tf.cast(flips, tf.bool), tf.zeros_like(bools), bools)\n    return k + 1, bools, locs, probs, draws\n\n\n# Generate random variable class similar to autogenerated ones from TensorFlow.\ndef __init__(self, *args, **kwargs):\n  RandomVariable.__init__(self, *args, **kwargs)\n\n\n_name = \'DirichletProcess\'\n_candidate = distributions_DirichletProcess\n__init__.__doc__ = _candidate.__init__.__doc__\n_globals = globals()\n_params = {\'__doc__\': _candidate.__doc__,\n           \'__init__\': __init__}\n_globals[_name] = type(_name, (RandomVariable, _candidate), _params)\n'"
edward/models/empirical.py,22,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models.random_variable import RandomVariable\nfrom tensorflow.contrib.distributions import Distribution\n\ntry:\n  from tensorflow.contrib.distributions import FULLY_REPARAMETERIZED\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass distributions_Empirical(Distribution):\n  """"""Empirical random variable.\n\n  #### Examples\n\n  ```python\n  # 100 samples of a scalar\n  x = Empirical(params=tf.zeros(100))\n  assert x.shape == ()\n\n  # 5 samples of a 2 x 3 matrix\n  x = Empirical(params=tf.zeros([5, 2, 3]))\n  assert x.shape == (2, 3)\n  ```\n  """"""\n  def __init__(self,\n               params,\n               validate_args=False,\n               allow_nan_stats=True,\n               name=""Empirical""):\n    """"""Initialize an `Empirical` random variable.\n\n    Args:\n      params: tf.Tensor.\n      Collection of samples. Its outer (left-most) dimension\n      determines the number of samples.\n    """"""\n    parameters = locals()\n    with tf.name_scope(name, values=[params]):\n      with tf.control_dependencies([]):\n        self._params = tf.identity(params, name=""params"")\n        try:\n          self._n = tf.shape(self._params)[0]\n        except ValueError:  # scalar params\n          self._n = tf.constant(1)\n\n    super(distributions_Empirical, self).__init__(\n        dtype=self._params.dtype,\n        reparameterization_type=FULLY_REPARAMETERIZED,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats,\n        parameters=parameters,\n        graph_parents=[self._params, self._n],\n        name=name)\n\n  @staticmethod\n  def _param_shapes(sample_shape):\n    return {""params"": tf.convert_to_tensor(sample_shape, dtype=tf.int32)}\n\n  @property\n  def params(self):\n    """"""Distribution parameter.""""""\n    return self._params\n\n  @property\n  def n(self):\n    """"""Number of samples.""""""\n    return self._n\n\n  def _batch_shape_tensor(self):\n    return tf.constant([], dtype=tf.int32)\n\n  def _batch_shape(self):\n    return tf.TensorShape([])\n\n  def _event_shape_tensor(self):\n    return tf.shape(self.params)[1:]\n\n  def _event_shape(self):\n    return self.params.shape[1:]\n\n  def _mean(self):\n    return tf.reduce_mean(self.params, 0)\n\n  def _stddev(self):\n    # broadcasting n x shape - shape = n x shape\n    r = self.params - self.mean()\n    return tf.sqrt(tf.reduce_mean(tf.square(r), 0))\n\n  def _variance(self):\n    return tf.square(self.stddev())\n\n  def _sample_n(self, n, seed=None):\n    input_tensor = self.params\n    if len(input_tensor.shape) == 0:\n      input_tensor = tf.expand_dims(input_tensor, 0)\n      multiples = tf.concat(\n          [tf.expand_dims(n, 0), [1] * len(self.event_shape)], 0)\n      return tf.tile(input_tensor, multiples)\n    else:\n      probs = tf.ones([self.n]) / tf.cast(self.n, dtype=tf.float32)\n      cat = tf.contrib.distributions.Categorical(probs)\n      indices = cat._sample_n(n, seed)\n      tensor = tf.gather(input_tensor, indices)\n      return tensor\n\n\n# Generate random variable class similar to autogenerated ones from TensorFlow.\ndef __init__(self, *args, **kwargs):\n  RandomVariable.__init__(self, *args, **kwargs)\n\n\n_name = \'Empirical\'\n_candidate = distributions_Empirical\n__init__.__doc__ = _candidate.__init__.__doc__\n_globals = globals()\n_params = {\'__doc__\': _candidate.__doc__,\n           \'__init__\': __init__,\n           \'support\': \'points\'}\n_globals[_name] = type(_name, (RandomVariable, _candidate), _params)\n'"
edward/models/param_mixture.py,28,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport tensorflow as tf\n\nfrom edward.models.random_variable import RandomVariable\nfrom tensorflow.contrib.distributions import Distribution\n\ntry:\n  from edward.models.random_variables import Categorical\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass distributions_ParamMixture(Distribution):\n  """"""A mixture distribution where all components are of the same family.\n\n  Note that this distribution actually represents the conditional\n  distribution of the observable variable given a latent categorical\n  variable `cat` saying which mixture component generated this\n  distribution.\n\n  #### Notes\n\n  Given `ParamMixture`\'s `sample_shape`, `batch_shape`, and\n  `event_shape`, its `components` has shape\n  `sample_shape + [num_components] + batch_shape + event_shape`,\n  and its `cat` has shape `sample_shape + batch_shape`.\n\n  #### Examples\n\n  ```python\n  probs = tf.ones(5) / 5.0\n  params = {\'mu\': tf.zeros(5), \'sigma\': tf.ones(5)}\n  x = ParamMixture(probs, params, Normal)\n  assert x.shape == ()\n\n  probs = tf.ones([2, 5]) / 5.0\n  params = {\'p\': tf.zeros([5, 2]) + 0.8}\n  x = ParamMixture(probs, params, Bernoulli)\n  assert x.shape == (2,)\n  ```\n  """"""\n  def __init__(self,\n               mixing_weights,\n               component_params,\n               component_dist,\n               validate_args=False,\n               allow_nan_stats=True,\n               name=""ParamMixture""):\n    """"""Initialize a batch of mixture random variables.\n\n    Args:\n      mixing_weights: tf.Tensor.\n        (Normalized) weights whose inner (right-most) dimension matches\n        the number of components.\n      component_params: dict.\n        Parameters of the per-component distributions.\n      component_dist: RandomVariable.\n        Distribution of each component. The outer (left-most) dimension\n        of its batch shape when instantiated determines the number of\n        components.\n    """"""\n    parameters = locals()\n    parameters.pop(""self"")\n    values = [mixing_weights] + list(six.itervalues(component_params))\n    with tf.name_scope(name, values=values):\n      if validate_args:\n        if not isinstance(component_params, dict):\n          raise TypeError(""component_params must be a dict."")\n        elif not issubclass(component_dist, RandomVariable):\n          raise TypeError(""component_dist must be a ed.RandomVariable object."")\n\n      # get sample_shape from inherited RandomVariable specifically\n      if hasattr(self, \'_kwargs\'):\n        sample_shape = self._kwargs.get(\'sample_shape\', ())\n      else:\n        sample_shape = ()\n\n      self._mixing_weights = tf.identity(mixing_weights, name=""mixing_weights"")\n      self._cat = Categorical(probs=self._mixing_weights,\n                              validate_args=validate_args,\n                              allow_nan_stats=allow_nan_stats,\n                              sample_shape=sample_shape)\n      self._component_params = component_params\n      self._components = component_dist(validate_args=validate_args,\n                                        allow_nan_stats=allow_nan_stats,\n                                        sample_shape=sample_shape,\n                                        collections=[],\n                                        **component_params)\n\n      if validate_args:\n        if not self._mixing_weights.shape[-1].is_compatible_with(\n                self._components.batch_shape[0]):\n          raise TypeError(""Last dimension of mixing_weights must match with ""\n                          ""the first dimension of components."")\n        elif not self._mixing_weights.shape[:-1].is_compatible_with(\n                self._components.batch_shape[1:]):\n          raise TypeError(""Dimensions of mixing_weights are not compatible ""\n                          ""with the dimensions of components."")\n\n      try:\n        self._num_components = self._cat.probs.shape.as_list()[-1]\n      except:  # if p has TensorShape None\n        raise NotImplementedError(""Number of components must be statically ""\n                                  ""determined."")\n\n      self._mean_val = None\n      self._variance_val = None\n      self._stddev_val = None\n      if self._cat.probs.shape.ndims <= 1:\n        with tf.name_scope(\'means\'):\n          try:\n            comp_means = self._components.mean()\n            comp_vars = self._components.variance()\n            comp_mean_sq = tf.square(comp_means) + comp_vars\n\n            # weights has shape batch_shape + [num_components]; change\n            # to broadcast with [num_components] + batch_shape + event_shape.\n            # The below reshaping only works for empty batch_shape.\n            weights = self._cat.probs\n            event_rank = self._components.event_shape.ndims\n            for _ in range(event_rank):\n              weights = tf.expand_dims(weights, -1)\n\n            self._mean_val = tf.reduce_sum(comp_means * weights, 0,\n                                           name=\'mean\')\n            mean_sq_val = tf.reduce_sum(comp_mean_sq * weights, 0,\n                                        name=\'mean_squared\')\n            self._variance_val = tf.subtract(mean_sq_val,\n                                             tf.square(self._mean_val),\n                                             name=\'variance\')\n            self._stddev_val = tf.sqrt(self._variance_val, name=\'stddev\')\n          except:\n            # This fails if _components.{mean,variance}() fails.\n            pass\n\n    super(distributions_ParamMixture, self).__init__(\n        dtype=self._components.dtype,\n        reparameterization_type=self._components.reparameterization_type,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats,\n        parameters=parameters,\n        graph_parents=[self._cat.value(), self._components.value()],\n        name=name)\n\n  @property\n  def cat(self):\n    return self._cat\n\n  @property\n  def components(self):\n    return self._components\n\n  @property\n  def num_components(self):\n    return self._num_components\n\n  def _batch_shape_tensor(self):\n    return self.cat.batch_shape_tensor()\n\n  def _batch_shape(self):\n    return self.cat.batch_shape\n\n  def _event_shape_tensor(self):\n    return self.components.event_shape_tensor()\n\n  def _event_shape(self):\n    return self.components.event_shape\n\n#   # This will work in TF 1.1\n#   @distribution_util.AppendDocstring(\n#     \'Note that this function returns the conditional log probability of the \'\n#     \'observed variable given the categorical variable `cat`. For the \'\n#     \'marginal log probability, use `marginal_log_prob()`.\')\n  def _log_prob(self, x, conjugate=False, **kwargs):\n    batch_event_rank = self.event_shape.ndims + self.batch_shape.ndims\n    # expand x to broadcast log probs over num_components dimension\n    expanded_x = tf.expand_dims(x, -1 - batch_event_rank)\n    if conjugate:\n      log_probs = self.components.conjugate_log_prob(expanded_x)\n    else:\n      log_probs = self.components.log_prob(expanded_x)\n\n    cat_axis = self.components.shape.ndims - 1 - batch_event_rank\n    selecter = tf.one_hot(self.cat, self.num_components,\n                          axis=cat_axis, dtype=log_probs.dtype)\n\n    # selecter has shape [n] + [num_components] + batch_shape; change\n    # to broadcast with [n] + [num_components] + batch_shape + event_shape.\n    while selecter.shape.ndims < log_probs.shape.ndims:\n      selecter = tf.expand_dims(selecter, -1)\n\n    # select the sampled component, sum out the component dimension\n    return tf.reduce_sum(log_probs * selecter, -1 - batch_event_rank)\n\n  def conjugate_log_prob(self):\n    return self._log_prob(self, conjugate=True)\n\n  def marginal_log_prob(self, x, **kwargs):\n    \'The marginal log probability of the observed variable. Sums out `cat`.\'\n    batch_event_rank = self.event_shape.ndims + self.batch_shape.ndims\n    # expand x to broadcast log probs over num_components dimension\n    expanded_x = tf.expand_dims(x, -1 - batch_event_rank)\n    log_probs = self.components.log_prob(expanded_x)\n\n    p_ndims = self.cat.probs.shape.ndims\n    perm = tf.concat([[p_ndims - 1], tf.range(p_ndims - 1)], 0)\n    transposed_p = tf.transpose(self.cat.probs, perm)\n\n    return tf.reduce_logsumexp(log_probs + tf.log(transposed_p),\n                               -1 - batch_event_rank)\n\n  def _sample_n(self, n, seed=None):\n    if getattr(self, \'_value\', None) is not None:\n      cat_sample = self.cat.sample(n)\n      comp_sample = self.components.sample(n)\n    else:\n      cat_sample = self.cat\n      comp_sample = self.components\n      # Add a leading dimension like Distribution.sample(1) would.\n      if n == 1:\n        comp_sample = tf.expand_dims(comp_sample, 0)\n        cat_sample = tf.expand_dims(cat_sample, 0)\n\n    # TODO avoid sampling n per component\n    batch_event_rank = self.event_shape.ndims + self.batch_shape.ndims\n    cat_axis = comp_sample.shape.ndims - 1 - batch_event_rank\n    selecter = tf.one_hot(cat_sample, self.num_components,\n                          axis=cat_axis, dtype=self.dtype)\n\n    # selecter has shape [n] + [num_components] + batch_shape; change\n    # to broadcast with [n] + [num_components] + batch_shape + event_shape.\n    while selecter.shape.ndims < comp_sample.shape.ndims:\n      selecter = tf.expand_dims(selecter, -1)\n\n    # select the sampled component, sum out the component dimension\n    result = tf.reduce_sum(comp_sample * selecter, cat_axis)\n    return result\n\n  def _mean(self):\n    if self._mean_val is None:\n      raise NotImplementedError()\n\n    return self._mean_val\n\n  def _stddev(self):\n    if self._stddev_val is None:\n      raise NotImplementedError()\n\n    return self._stddev_val\n\n  def _variance(self):\n    if self._variance_val is None:\n      raise NotImplementedError()\n\n    return self._variance_val\n\n\n# Generate random variable class similar to autogenerated ones from TensorFlow.\ndef __init__(self, *args, **kwargs):\n  RandomVariable.__init__(self, *args, **kwargs)\n\n\n_name = \'ParamMixture\'\n_candidate = distributions_ParamMixture\n__init__.__doc__ = _candidate.__init__.__doc__\n_globals = globals()\n_params = {\'__doc__\': _candidate.__doc__,\n           \'__init__\': __init__}\n_globals[_name] = type(_name, (RandomVariable, _candidate), _params)\n'"
edward/models/point_mass.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models.random_variable import RandomVariable\nfrom tensorflow.contrib.distributions import Distribution\n\ntry:\n  from tensorflow.contrib.distributions import FULLY_REPARAMETERIZED\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n\nclass distributions_PointMass(Distribution):\n  """"""PointMass random variable.\n\n  It is analogous to an Empirical random variable with one sample, but\n  its parameter argument does not have an outer dimension.\n\n  #### Examples\n\n  ```python\n  # scalar\n  x = PointMass(params=28.3)\n  assert x.shape == ()\n\n  # 5 x 2 x 3 tensor\n  x = PointMass(params=tf.zeros([5, 2, 3]))\n  assert x.shape == (5, 2, 3)\n  ```\n  """"""\n  def __init__(self,\n               params,\n               validate_args=False,\n               allow_nan_stats=True,\n               name=""PointMass""):\n    """"""Initialize a `PointMass` random variable.\n\n    Args:\n      params: tf.Tensor.\n        The location with all probability mass.\n    """"""\n    parameters = locals()\n    with tf.name_scope(name, values=[params]):\n      with tf.control_dependencies([]):\n        self._params = tf.identity(params, name=""params"")\n\n    super(distributions_PointMass, self).__init__(\n        dtype=self._params.dtype,\n        reparameterization_type=FULLY_REPARAMETERIZED,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats,\n        parameters=parameters,\n        graph_parents=[self._params],\n        name=name)\n\n  @staticmethod\n  def _param_shapes(sample_shape):\n    return {""params"": tf.expand_dims(\n        tf.convert_to_tensor(sample_shape, dtype=tf.int32), 0)}\n\n  @property\n  def params(self):\n    """"""Distribution parameter.""""""\n    return self._params\n\n  def _batch_shape_tensor(self):\n    return tf.constant([], dtype=tf.int32)\n\n  def _batch_shape(self):\n    return tf.TensorShape([])\n\n  def _event_shape_tensor(self):\n    return tf.shape(self.params)\n\n  def _event_shape(self):\n    return self.params.shape\n\n  def _mean(self):\n    return self.params\n\n  def _stddev(self):\n    return 0.0 * tf.ones_like(self.params)\n\n  def _variance(self):\n    return tf.square(self.stddev())\n\n  def _sample_n(self, n, seed=None):\n    input_tensor = self.params\n    input_tensor = tf.expand_dims(input_tensor, 0)\n    multiples = tf.concat(\n        [tf.expand_dims(n, 0), [1] * len(self.event_shape)], 0)\n    return tf.tile(input_tensor, multiples)\n\n\n# Generate random variable class similar to autogenerated ones from TensorFlow.\ndef __init__(self, *args, **kwargs):\n  RandomVariable.__init__(self, *args, **kwargs)\n\n\n_name = \'PointMass\'\n_candidate = distributions_PointMass\n__init__.__doc__ = _candidate.__init__.__doc__\n_globals = globals()\n_params = {\'__doc__\': _candidate.__doc__,\n           \'__init__\': __init__,\n           \'support\': \'point\'}\n_globals[_name] = type(_name, (RandomVariable, _candidate), _params)\n'"
edward/models/random_variable.py,27,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom collections import defaultdict\n\ntry:\n  from tensorflow.python.client.session import \\\n      register_session_run_conversion_functions\nexcept Exception as e:\n  raise ImportError(""{0}. Your TensorFlow version is not supported."".format(e))\n\n_RANDOM_VARIABLE_COLLECTION = defaultdict(list)\n\n\nclass RandomVariable(object):\n  """"""Base class for random variables.\n\n  A random variable is an object parameterized by tensors. It is\n  equipped with methods such as the log-density, mean, and sample.\n\n  It also wraps a tensor, where the tensor corresponds to a sample\n  from the random variable. This enables operations on the TensorFlow\n  graph, allowing random variables to be used in conjunction with\n  other TensorFlow ops.\n\n  The random variable\'s shape is given by\n\n  `sample_shape + batch_shape + event_shape`,\n\n  where `sample_shape` is an optional argument representing the\n  dimensions of samples drawn from the distribution (default is\n  a scalar); `batch_shape` is the number of independent random variables\n  (determined by the shape of its parameters); and `event_shape` is\n  the shape of one draw from the distribution (e.g., `Normal` has a\n  scalar `event_shape`; `Dirichlet` has a vector `event_shape`).\n\n  #### Notes\n\n  `RandomVariable` assumes use in a multiple inheritance setting. The\n  child class must first inherit `RandomVariable`, then second inherit a\n  class in `tf.contrib.distributions`. With Python\'s method resolution\n  order, this implies the following during initialization (using\n  `distributions.Bernoulli` as an example):\n\n  1. Start the `__init__()` of the child class, which passes all\n     `*args, **kwargs` to `RandomVariable`.\n  2. This in turn passes all `*args, **kwargs` to\n     `distributions.Bernoulli`, completing the `__init__()` of\n     `distributions.Bernoulli`.\n  3. Complete the `__init__()` of `RandomVariable`, which calls\n     `self.sample()`, relying on the method from\n     `distributions.Bernoulli`.\n  4. Complete the `__init__()` of the child class.\n\n  Methods from both `RandomVariable` and `distributions.Bernoulli`\n  populate the namespace of the child class. Methods from\n  `RandomVariable` will take higher priority if there are conflicts.\n\n  #### Examples\n\n  ```python\n  p = tf.constant(0.5)\n  x = Bernoulli(p)\n\n  z1 = tf.constant([[1.0, -0.8], [0.3, -1.0]])\n  z2 = tf.constant([[0.9, 0.2], [2.0, -0.1]])\n  x = Bernoulli(logits=tf.matmul(z1, z2))\n\n  mu = Normal(tf.constant(0.0), tf.constant(1.0))\n  x = Normal(mu, tf.constant(1.0))\n  ```\n  """"""\n  def __init__(self, *args, **kwargs):\n    """"""Create a new random variable.\n\n    Args:\n      sample_shape: tf.TensorShape.\n        Shape of samples to draw from the random variable.\n      value: tf.Tensor.\n        Fixed tensor to associate with random variable. Must have shape\n        `sample_shape + batch_shape + event_shape`.\n      collections: list.\n        Optional list of graph collections (lists). The random variable is\n        added to these collections. Defaults to `[ed.random_variables()]`.\n    """"""\n    # Force the Distribution class to always use the same name scope\n    # when scoping its parameter names and also when calling any\n    # methods such as sample.\n    name = kwargs.get(\'name\', type(self).__name__)\n    with tf.name_scope(name) as ns:\n      kwargs[\'name\'] = ns\n\n    # pop and store RandomVariable-specific parameters in _kwargs\n    sample_shape = kwargs.pop(\'sample_shape\', ())\n    value = kwargs.pop(\'value\', None)\n    collections = kwargs.pop(\'collections\', [""random_variables""])\n\n    # store args, kwargs for easy graph copying\n    self._args = args\n    self._kwargs = kwargs.copy()\n\n    if sample_shape != ():\n      self._kwargs[\'sample_shape\'] = sample_shape\n    if value is not None:\n      self._kwargs[\'value\'] = value\n    if collections != [""random_variables""]:\n      self._kwargs[\'collections\'] = collections\n\n    super(RandomVariable, self).__init__(*args, **kwargs)\n\n    self._sample_shape = tf.TensorShape(sample_shape)\n    if value is not None:\n      t_value = tf.convert_to_tensor(value, self.dtype)\n      value_shape = t_value.shape\n      expected_shape = self._sample_shape.concatenate(\n          self.batch_shape).concatenate(self.event_shape)\n      if not value_shape.is_compatible_with(expected_shape):\n        raise ValueError(\n            ""Incompatible shape for initialization argument \'value\'. ""\n            ""Expected %s, got %s."" % (expected_shape, value_shape))\n      else:\n        self._value = t_value\n    else:\n      try:\n        self._value = self.sample(self._sample_shape)\n      except NotImplementedError:\n        raise NotImplementedError(\n            ""sample is not implemented for {0}. You must either pass in the ""\n            ""value argument or implement sample for {0}.""\n            .format(self.__class__.__name__))\n\n    for collection in collections:\n      if collection == ""random_variables"":\n        collection = _RANDOM_VARIABLE_COLLECTION\n      collection[tf.get_default_graph()].append(self)\n\n  @property\n  def sample_shape(self):\n    """"""Sample shape of random variable.""""""\n    return self._sample_shape\n\n  @property\n  def shape(self):\n    """"""Shape of random variable.""""""\n    return self._value.shape\n\n  def __str__(self):\n    return ""RandomVariable(\\""%s\\""%s%s%s)"" % (\n        self.name,\n        ("", shape=%s"" % self.shape)\n        if self.shape.ndims is not None else """",\n        ("", dtype=%s"" % self.dtype.name) if self.dtype else """",\n        ("", device=%s"" % self.value().device) if self.value().device else """")\n\n  def __repr__(self):\n    return ""<ed.RandomVariable \'%s\' shape=%s dtype=%s>"" % (\n        self.name, self.shape, self.dtype.name)\n\n  def __hash__(self):\n    return id(self)\n\n  def __eq__(self, other):\n    return id(self) == id(other)\n\n  def __iter__(self):\n    raise TypeError(""\'RandomVariable\' object is not iterable."")\n\n  def __bool__(self):\n    raise TypeError(\n        ""Using a `ed.RandomVariable` as a Python `bool` is not allowed. ""\n        ""Use `if t is not None:` instead of `if t:` to test if a ""\n        ""random variable is defined, and use TensorFlow ops such as ""\n        ""tf.cond to execute subgraphs conditioned on a draw from ""\n        ""a random variable."")\n\n  def __nonzero__(self):\n    raise TypeError(\n        ""Using a `ed.RandomVariable` as a Python `bool` is not allowed. ""\n        ""Use `if t is not None:` instead of `if t:` to test if a ""\n        ""random variable is defined, and use TensorFlow ops such as ""\n        ""tf.cond to execute subgraphs conditioned on a draw from ""\n        ""a random variable."")\n\n  def eval(self, session=None, feed_dict=None):\n    """"""In a session, computes and returns the value of this random variable.\n\n    This is not a graph construction method, it does not add ops to the graph.\n\n    This convenience method requires a session where the graph\n    containing this variable has been launched. If no session is\n    passed, the default session is used.\n\n    Args:\n      session: tf.BaseSession.\n        The `tf.Session` to use to evaluate this random variable. If\n        none, the default session is used.\n      feed_dict: dict.\n        A dictionary that maps `tf.Tensor` objects to feed values. See\n        `tf.Session.run()` for a description of the valid feed values.\n\n    #### Examples\n\n    ```python\n    x = Normal(0.0, 1.0)\n    with tf.Session() as sess:\n      # Usage passing the session explicitly.\n      print(x.eval(sess))\n      # Usage with the default session.  The \'with\' block\n      # above makes \'sess\' the default session.\n      print(x.eval())\n    ```\n    """"""\n    return self.value().eval(session=session, feed_dict=feed_dict)\n\n  def value(self):\n    """"""Get tensor that the random variable corresponds to.""""""\n    return self._value\n\n  def get_ancestors(self, collection=None):\n    """"""Get ancestor random variables.""""""\n    from edward.util.random_variables import get_ancestors\n    return get_ancestors(self, collection)\n\n  def get_blanket(self, collection=None):\n    """"""Get the random variable\'s Markov blanket.""""""\n    from edward.util.random_variables import get_blanket\n    return get_blanket(self, collection)\n\n  def get_children(self, collection=None):\n    """"""Get child random variables.""""""\n    from edward.util.random_variables import get_children\n    return get_children(self, collection)\n\n  def get_descendants(self, collection=None):\n    """"""Get descendant random variables.""""""\n    from edward.util.random_variables import get_descendants\n    return get_descendants(self, collection)\n\n  def get_parents(self, collection=None):\n    """"""Get parent random variables.""""""\n    from edward.util.random_variables import get_parents\n    return get_parents(self, collection)\n\n  def get_siblings(self, collection=None):\n    """"""Get sibling random variables.""""""\n    from edward.util.random_variables import get_siblings\n    return get_siblings(self, collection)\n\n  def get_variables(self, collection=None):\n    """"""Get TensorFlow variables that the random variable depends on.""""""\n    from edward.util.random_variables import get_variables\n    return get_variables(self, collection)\n\n  def get_shape(self):\n    """"""Get shape of random variable.""""""\n    return self.shape\n\n  @staticmethod\n  def _overload_all_operators():\n    """"""Register overloads for all operators.""""""\n    for operator in tf.Tensor.OVERLOADABLE_OPERATORS:\n      RandomVariable._overload_operator(operator)\n\n  @staticmethod\n  def _overload_operator(operator):\n    """"""Defer an operator overload to `tf.Tensor`.\n\n    We pull the operator out of tf.Tensor dynamically to avoid ordering issues.\n\n    Args:\n      operator: string. The operator name.\n    """"""\n    def _run_op(a, *args):\n      return getattr(tf.Tensor, operator)(a.value(), *args)\n    # Propagate __doc__ to wrapper\n    try:\n      _run_op.__doc__ = getattr(tf.Tensor, operator).__doc__\n    except AttributeError:\n      pass\n\n    setattr(RandomVariable, operator, _run_op)\n\n  # ""This enables the Variable\'s overloaded ""right"" binary operators to\n  # run when the left operand is an ndarray, because it accords the\n  # Variable class higher priority than an ndarray, or a numpy matrix.""\n  # Taken from implementation of tf.Tensor.\n  __array_priority__ = 100\n\n  @staticmethod\n  def _session_run_conversion_fetch_function(tensor):\n    return ([tensor.value()], lambda val: val[0])\n\n  @staticmethod\n  def _session_run_conversion_feed_function(feed, feed_val):\n    return [(feed.value(), feed_val)]\n\n  @staticmethod\n  def _session_run_conversion_feed_function_for_partial_run(feed):\n    return [feed.value()]\n\n  @staticmethod\n  def _tensor_conversion_function(v, dtype=None, name=None, as_ref=False):\n    _ = name, as_ref\n    if dtype and not dtype.is_compatible_with(v.dtype):\n      raise ValueError(\n          ""Incompatible type conversion requested to type \'%s\' for variable ""\n          ""of type \'%s\'"" % (dtype.name, v.dtype.name))\n    return v.value()\n\n\nRandomVariable._overload_all_operators()\n\nregister_session_run_conversion_functions(\n    RandomVariable,\n    RandomVariable._session_run_conversion_fetch_function,\n    RandomVariable._session_run_conversion_feed_function,\n    RandomVariable._session_run_conversion_feed_function_for_partial_run)\n\ntf.register_tensor_conversion_function(\n    RandomVariable, RandomVariable._tensor_conversion_function)\n'"
edward/models/random_variables.py,1,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect as _inspect\n\nfrom edward.models.random_variable import RandomVariable as _RandomVariable\nfrom tensorflow.contrib import distributions as _distributions\n\n# Automatically generate random variable classes from classes in\n# tf.contrib.distributions.\n_globals = globals()\nfor _name in sorted(dir(_distributions)):\n  _candidate = getattr(_distributions, _name)\n  if (_inspect.isclass(_candidate) and\n          _candidate != _distributions.Distribution and\n          issubclass(_candidate, _distributions.Distribution)):\n\n    # to use _candidate's docstring, must write a new __init__ method\n    def __init__(self, *args, **kwargs):\n      _RandomVariable.__init__(self, *args, **kwargs)\n    __init__.__doc__ = _candidate.__init__.__doc__\n    _params = {'__doc__': _candidate.__doc__,\n               '__init__': __init__}\n    _globals[_name] = type(_name, (_RandomVariable, _candidate), _params)\n\n    del _candidate\n\n# Add supports; these are used, e.g., in conjugacy.\nBernoulli.support = 'binary'\nBeta.support = '01'\nBinomial.support = 'onehot'\nCategorical.support = 'categorical'\nChi2.support = 'nonnegative'\nDirichlet.support = 'simplex'\nExponential.support = 'nonnegative'\nGamma.support = 'nonnegative'\nInverseGamma.support = 'nonnegative'\nLaplace.support = 'real'\nMultinomial.support = 'onehot'\nMultivariateNormalDiag.support = 'multivariate_real'\nNormal.support = 'real'\nPoisson.support = 'countable'\n\ndel absolute_import\ndel division\ndel print_function\n"""
edward/util/__init__.py,0,"b'""""""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom edward.util.graphs import *\nfrom edward.util.metrics import *\nfrom edward.util.progbar import *\nfrom edward.util.random_variables import *\nfrom edward.util.tensorflow import *\n\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n_allowed_symbols = [\n    \'check_data\',\n    \'check_latent_vars\',\n    \'compute_multinomial_mode\',\n    \'copy\',\n    \'dot\',\n    \'get_ancestors\',\n    \'get_blanket\',\n    \'get_children\',\n    \'get_control_variate_coef\',\n    \'get_descendants\',\n    \'get_parents\',\n    \'get_session\',\n    \'get_siblings\',\n    \'get_variables\',\n    \'is_independent\',\n    \'Progbar\',\n    \'random_variables\',\n    \'rbf\',\n    \'set_seed\',\n    \'to_simplex\',\n    \'transform\',\n    \'with_binary_averaging\'\n]\n\nremove_undocumented(__name__, allowed_exception_list=_allowed_symbols)\n'"
edward/util/graphs.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport six\nimport sys\nimport tensorflow as tf\n\nfrom edward.models.random_variable import _RANDOM_VARIABLE_COLLECTION\n\n\ndef get_session():\n  """"""Get the globally defined TensorFlow session.\n\n  If the session is not already defined, then the function will create\n  a global session.\n\n  Returns:\n    _ED_SESSION: tf.InteractiveSession.\n  """"""\n  global _ED_SESSION\n  if tf.get_default_session() is None:\n    _ED_SESSION = tf.InteractiveSession()\n  else:\n    _ED_SESSION = tf.get_default_session()\n\n  save_stderr = sys.stderr\n  try:\n    import os\n    sys.stderr = open(os.devnull, \'w\')  # suppress keras import\n    from keras import backend as K\n    sys.stderr = save_stderr\n    have_keras = True\n  except ImportError:\n    sys.stderr = save_stderr\n    have_keras = False\n  if have_keras:\n    K.set_session(_ED_SESSION)\n\n  return _ED_SESSION\n\n\ndef random_variables(graph=None):\n  """"""Return all random variables in the TensorFlow graph.\n\n  Args:\n    graph: TensorFlow graph.\n\n  Returns:\n    list of RandomVariable.\n  """"""\n  if graph is None:\n    graph = tf.get_default_graph()\n\n  return _RANDOM_VARIABLE_COLLECTION[graph]\n\n\ndef set_seed(x):\n  """"""Set seed for both NumPy and TensorFlow.\n\n  Args:\n    x: int, float.\n      seed\n  """"""\n  node_names = list(six.iterkeys(tf.get_default_graph()._nodes_by_name))\n  if len(node_names) > 0 and node_names != [\'keras_learning_phase\']:\n    raise RuntimeError(""Seeding is not supported after initializing ""\n                       ""part of the graph. ""\n                       ""Please move set_seed to the beginning of your code."")\n\n  np.random.seed(x)\n  tf.set_random_seed(x)\n'"
edward/util/metrics.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom functools import wraps\n\nimport tensorflow as tf\n\n\ndef with_binary_averaging(metric):\n  """"""\n  Inspired by scikit-learn\'s _average_binary_score function:\n  https://github.com/scikit-learn/scikit-learn/blob/d9fdd8b0d1053cb47af8e3823b7a05279dd72054/sklearn/metrics/base.py#L23.\n\n  `None`: computes the specified metric along the second-to-last\n  dimension of `y_true` and `y_pred`. Returns a vector of ""class-wise""\n  metrics.\n  `\'macro\'`: same as `None`, except compute the (unweighted) global\n  average of the resulting vector.\n  `\'micro\'`: flatten `y_true` and `y_pred` into vectors, then compute\n  `\'macro\'`\n  """"""\n  AVERAGE_OPTIONS = (None, \'micro\', \'macro\')\n\n  @wraps(metric)\n  def with_binary_averaging(*args, **kwargs):\n    y_true, y_pred = args\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    if len(y_true.shape) < 2 and len(y_pred.shape) < 2:\n      y_true = tf.expand_dims(y_true, 0)\n      y_pred = tf.expand_dims(y_pred, 0)\n\n    average = kwargs.get(\'average\', \'macro\')\n    if average not in AVERAGE_OPTIONS:\n      raise ValueError(\'average has to be one of {0}\'\n                       \'\'.format(average_options))\n    if average is None:\n      return metric(y_true, y_pred)\n    if average == \'macro\':\n      return tf.reduce_mean(metric(y_true, y_pred))\n    if average == \'micro\':\n      y_true = tf.reshape(y_true, [1, -1])\n      y_pred = tf.reshape(y_pred, [1, -1])\n      return tf.reduce_mean(metric(y_true, y_pred))\n  return with_binary_averaging\n'"
edward/util/progbar.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport six\nimport sys\nimport time\n\n\nclass Progbar(object):\n  def __init__(self, target, width=30, interval=0.01, verbose=1):\n    """"""(Yet another) progress bar.\n\n    Args:\n      target: int.\n        Total number of steps expected.\n      width: int.\n        Width of progress bar.\n      interval: float.\n        Minimum time (in seconds) for progress bar to be displayed\n        during updates.\n      verbose: int.\n        Level of verbosity. 0 suppresses output; 1 is default.\n    """"""\n    self.target = target\n    self.width = width\n    self.interval = interval\n    self.verbose = verbose\n\n    self.stored_values = {}\n    self.start = time.time()\n    self.last_update = 0\n    self.total_width = 0\n    self.seen_so_far = 0\n\n  def update(self, current, values=None, force=False):\n    """"""Update progress bar, and print to standard output if `force`\n    is True, or the last update was completed longer than `interval`\n    amount of time ago, or `current` >= `target`.\n\n    The written output is the progress bar and all unique values.\n\n    Args:\n      current: int.\n        Index of current step.\n      values: dict of str to float.\n        Dict of name by value-for-last-step. The progress bar\n        will display averages for these values.\n      force: bool.\n        Whether to force visual progress update.\n    """"""\n    if values is None:\n      values = {}\n\n    for k, v in six.iteritems(values):\n      self.stored_values[k] = v\n\n    self.seen_so_far = current\n\n    now = time.time()\n    if (not force and\n            (now - self.last_update) < self.interval and\n            current < self.target):\n      return\n\n    self.last_update = now\n    if self.verbose == 0:\n      return\n\n    prev_total_width = self.total_width\n    sys.stdout.write(""\\b"" * prev_total_width)\n    sys.stdout.write(""\\r"")\n\n    # Write progress bar to stdout.\n    n_digits = len(str(self.target))\n    bar = \'%%%dd/%%%dd\' % (n_digits, n_digits) % (current, self.target)\n    bar += \' [{0}%]\'.format(str(int(current / self.target * 100)).rjust(3))\n    bar += \' \'\n    prog_width = int(self.width * float(current) / self.target)\n    if prog_width > 0:\n      try:\n        bar += (\'\xe2\x96\x88\' * prog_width)\n      except UnicodeEncodeError:\n        bar += (\'*\' * prog_width)\n\n    bar += (\' \' * (self.width - prog_width))\n    sys.stdout.write(bar)\n\n    # Write values to stdout.\n    if current:\n      time_per_unit = (now - self.start) / current\n    else:\n      time_per_unit = 0\n\n    eta = time_per_unit * (self.target - current)\n    info = \'\'\n    if current < self.target:\n      info += \' ETA: %ds\' % eta\n    else:\n      info += \' Elapsed: %ds\' % (now - self.start)\n\n    for k, v in six.iteritems(self.stored_values):\n      info += \' | {0:s}: {1:0.3f}\'.format(k, v)\n\n    self.total_width = len(bar) + len(info)\n    if prev_total_width > self.total_width:\n      info += ((prev_total_width - self.total_width) * "" "")\n\n    sys.stdout.write(info)\n    sys.stdout.flush()\n\n    if current >= self.target:\n      sys.stdout.write(""\\n"")\n'"
edward/util/random_variables.py,56,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom copy import deepcopy\nfrom edward.models.random_variable import RandomVariable\nfrom edward.models.random_variables import TransformedDistribution\nfrom edward.models import PointMass\nfrom edward.util.graphs import random_variables\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.python.framework.ops import set_shapes_for_outputs\nfrom tensorflow.python.util import compat\n\ntfb = tf.contrib.distributions.bijectors\n\n\ndef check_data(data):\n  """"""Check that the data dictionary passed during inference and\n  criticism is valid.\n  """"""\n  if not isinstance(data, dict):\n    raise TypeError(""data must have type dict."")\n\n  for key, value in six.iteritems(data):\n    if isinstance(key, tf.Tensor) and ""Placeholder"" in key.op.type:\n      if isinstance(value, RandomVariable):\n        raise TypeError(""The value of a feed cannot be a ed.RandomVariable ""\n                        ""object. ""\n                        ""Acceptable feed values include Python scalars, ""\n                        ""strings, lists, numpy ndarrays, or TensorHandles."")\n      elif isinstance(value, tf.Tensor):\n        raise TypeError(""The value of a feed cannot be a tf.Tensor object. ""\n                        ""Acceptable feed values include Python scalars, ""\n                        ""strings, lists, numpy ndarrays, or TensorHandles."")\n    elif isinstance(key, (RandomVariable, tf.Tensor)):\n      if isinstance(value, (RandomVariable, tf.Tensor)):\n        if not key.shape.is_compatible_with(value.shape):\n          raise TypeError(""Key-value pair in data does not have same ""\n                          ""shape: {}, {}"".format(key.shape, value.shape))\n        elif key.dtype != value.dtype:\n          raise TypeError(""Key-value pair in data does not have same ""\n                          ""dtype: {}, {}"".format(key.dtype, value.dtype))\n      elif isinstance(value, (float, list, int, np.ndarray, np.number, str)):\n        if not key.shape.is_compatible_with(np.shape(value)):\n          raise TypeError(""Key-value pair in data does not have same ""\n                          ""shape: {}, {}"".format(key.shape, np.shape(value)))\n        elif isinstance(value, (np.ndarray, np.number)) and \\\n                not np.issubdtype(value.dtype, np.float) and \\\n                not np.issubdtype(value.dtype, np.int) and \\\n                not np.issubdtype(value.dtype, np.str):\n          raise TypeError(""Data value has an invalid dtype: ""\n                          ""{}"".format(value.dtype))\n      else:\n        raise TypeError(""Data value has an invalid type: ""\n                        ""{}"".format(type(value)))\n    else:\n      raise TypeError(""Data key has an invalid type: {}"".format(type(key)))\n\n\ndef check_latent_vars(latent_vars):\n  """"""Check that the latent variable dictionary passed during inference and\n  criticism is valid.\n  """"""\n  if not isinstance(latent_vars, dict):\n    raise TypeError(""latent_vars must have type dict."")\n\n  for key, value in six.iteritems(latent_vars):\n    if not isinstance(key, (RandomVariable, tf.Tensor)):\n      raise TypeError(""Latent variable key has an invalid type: ""\n                      ""{}"".format(type(key)))\n    elif not isinstance(value, (RandomVariable, tf.Tensor)):\n      raise TypeError(""Latent variable value has an invalid type: ""\n                      ""{}"".format(type(value)))\n    elif not key.shape.is_compatible_with(value.shape):\n      raise TypeError(""Key-value pair in latent_vars does not have same ""\n                      ""shape: {}, {}"".format(key.shape, value.shape))\n    elif key.dtype != value.dtype:\n      raise TypeError(""Key-value pair in latent_vars does not have same ""\n                      ""dtype: {}, {}"".format(key.dtype, value.dtype))\n\n\ndef _get_context_copy(ctx, scope):\n    # contexts are stored in graph collections\n    # is there a more efficient way to do this?\n\n    graph = tf.get_default_graph()\n\n    for name, collection in six.iteritems(graph._collections):\n      if ctx in collection:\n        for item in collection:\n          if item.name == scope + ctx.name:\n            return item\n\n    return None\n\n\ndef _copy_context(ctx, context_matches, dict_swap, scope, copy_q):\n  if ctx is None:\n    return None\n\n  # We\'d normally check about returning early, but the context won\'t\n  # be copied until after all children are, so we check that first.\n\n  graph = tf.get_default_graph()\n\n  # copy all nodes within context\n  for tensorname in ctx._values:\n    tensor = graph.as_graph_element(tensorname)\n    copy(tensor, dict_swap, scope, True, copy_q)\n\n  # now make sure we haven\'t already copied the context we\'re currently\n  # trying to copy (in the course of copying another child)\n  ctx_copy = _get_context_copy(ctx, scope)\n  if ctx_copy:\n    return ctx_copy\n\n  ctx_copy = ctx.from_proto(ctx.to_proto(), scope[:-1])\n  outer_copy = _copy_context(ctx.outer_context, context_matches, dict_swap,\n                             scope, copy_q)\n  ctx_copy._outer_context = outer_copy\n\n  for name, collection in six.iteritems(graph._collections):\n      if ctx in collection:\n        graph.add_to_collection(name, ctx_copy)\n  return ctx_copy\n\n\ndef _copy_default(x, *args, **kwargs):\n  if isinstance(x, (RandomVariable, tf.Operation, tf.Tensor, tf.Variable)):\n    x = copy(x, *args, **kwargs)\n\n  return x\n\n\ndef copy(org_instance, dict_swap=None, scope=""copied"",\n         replace_itself=False, copy_q=False, copy_parent_rvs=True):\n  """"""Build a new node in the TensorFlow graph from `org_instance`,\n  where any of its ancestors existing in `dict_swap` are\n  replaced with `dict_swap`\'s corresponding value.\n\n  Copying is done recursively. Any `Operation` whose output is\n  required to copy `org_instance` is also copied (if it isn\'t already\n  copied within the new scope).\n\n  `tf.Variable`s, `tf.placeholder`s, and nodes of type `Queue` are\n  always reused and not copied. In addition, `tf.Operation`s with\n  operation-level seeds are copied with a new operation-level seed.\n\n  Args:\n    org_instance: RandomVariable, tf.Operation, tf.Tensor, or tf.Variable.\n      Node to add in graph with replaced ancestors.\n    dict_swap: dict.\n      Random variables, variables, tensors, or operations to swap with.\n      Its keys are what `org_instance` may depend on, and its values are\n      the corresponding object (not necessarily of the same class\n      instance, but must have the same type, e.g., float32) that is used\n      in exchange.\n    scope: str.\n      A scope for the new node(s). This is used to avoid name\n      conflicts with the original node(s).\n    replace_itself: bool.\n      Whether to replace `org_instance` itself if it exists in\n      `dict_swap`. (This is used for the recursion.)\n    copy_q: bool.\n      Whether to copy the replaced tensors too (if not already\n      copied within the new scope). Otherwise will reuse them.\n    copy_parent_rvs:\n      Whether to copy parent random variables `org_instance` depends\n      on. Otherwise will copy only the sample tensors and not the\n      random variable class itself.\n\n  Returns:\n    RandomVariable, tf.Variable, tf.Tensor, or tf.Operation.\n    The copied node.\n\n  Raises:\n    TypeError.\n    If `org_instance` is not one of the above types.\n\n  #### Examples\n\n  ```python\n  x = tf.constant(2.0)\n  y = tf.constant(3.0)\n  z = x * y\n\n  qx = tf.constant(4.0)\n  # The TensorFlow graph is currently\n  # `x` -> `z` <- y`, `qx`\n\n  # This adds a subgraph with newly copied nodes,\n  # `qx` -> `copied/z` <- `copied/y`\n  z_new = ed.copy(z, {x: qx})\n\n  sess = tf.Session()\n  sess.run(z)\n  6.0\n  sess.run(z_new)\n  12.0\n  ```\n  """"""\n  if not isinstance(org_instance,\n                    (RandomVariable, tf.Operation, tf.Tensor, tf.Variable)):\n    raise TypeError(""Could not copy instance: "" + str(org_instance))\n\n  if dict_swap is None:\n    dict_swap = {}\n  if scope[-1] != \'/\':\n    scope += \'/\'\n\n  # Swap instance if in dictionary.\n  if org_instance in dict_swap and replace_itself:\n    org_instance = dict_swap[org_instance]\n    if not copy_q:\n      return org_instance\n  elif isinstance(org_instance, tf.Tensor) and replace_itself:\n    # Deal with case when `org_instance` is the associated tensor\n    # from the RandomVariable, e.g., `z.value()`. If\n    # `dict_swap={z: qz}`, we aim to swap it with `qz.value()`.\n    for key, value in six.iteritems(dict_swap):\n      if isinstance(key, RandomVariable):\n        if org_instance == key.value():\n          if isinstance(value, RandomVariable):\n            org_instance = value.value()\n          else:\n            org_instance = value\n          if not copy_q:\n            return org_instance\n          break\n\n  # If instance is a tf.Variable, return it; do not copy any. Note we\n  # check variables via their name. If we get variables through an\n  # op\'s inputs, it has type tf.Tensor and not tf.Variable.\n  if isinstance(org_instance, (tf.Tensor, tf.Variable)):\n    for variable in tf.global_variables():\n      if org_instance.name == variable.name:\n        if variable in dict_swap and replace_itself:\n          # Deal with case when `org_instance` is the associated _ref\n          # tensor for a tf.Variable.\n          org_instance = dict_swap[variable]\n          if not copy_q or isinstance(org_instance, tf.Variable):\n            return org_instance\n          for variable in tf.global_variables():\n            if org_instance.name == variable.name:\n              return variable\n          break\n        else:\n          return variable\n\n  graph = tf.get_default_graph()\n  new_name = scope + org_instance.name\n\n  # If an instance of the same name exists, return it.\n  if isinstance(org_instance, RandomVariable):\n    for rv in random_variables():\n      if new_name == rv.name:\n        return rv\n  elif isinstance(org_instance, (tf.Tensor, tf.Operation)):\n    try:\n      return graph.as_graph_element(new_name,\n                                    allow_tensor=True,\n                                    allow_operation=True)\n    except:\n      pass\n\n  # Preserve ordering of random variables. Random variables are always\n  # copied first (from parent -> child) before any deterministic\n  # operations that depend on them.\n  if copy_parent_rvs and \\\n          isinstance(org_instance, (RandomVariable, tf.Tensor, tf.Variable)):\n    for v in get_parents(org_instance):\n      copy(v, dict_swap, scope, True, copy_q, True)\n\n  if isinstance(org_instance, RandomVariable):\n    rv = org_instance\n\n    # If it has copiable arguments, copy them.\n    args = [_copy_default(arg, dict_swap, scope, True, copy_q, False)\n            for arg in rv._args]\n\n    kwargs = {}\n    for key, value in six.iteritems(rv._kwargs):\n      if isinstance(value, list):\n        kwargs[key] = [_copy_default(v, dict_swap, scope, True, copy_q, False)\n                       for v in value]\n      else:\n        kwargs[key] = _copy_default(\n            value, dict_swap, scope, True, copy_q, False)\n\n    kwargs[\'name\'] = new_name\n    # Create new random variable with copied arguments.\n    try:\n      new_rv = type(rv)(*args, **kwargs)\n    except ValueError:\n      # Handle case where parameters are copied under absolute name\n      # scope. This can cause an error when creating a new random\n      # variable as tf.identity name ops are called on parameters (""op\n      # with name already exists""). To avoid remove absolute name scope.\n      kwargs[\'name\'] = new_name[:-1]\n      new_rv = type(rv)(*args, **kwargs)\n    return new_rv\n  elif isinstance(org_instance, tf.Tensor):\n    tensor = org_instance\n\n    # Do not copy tf.placeholders.\n    if \'Placeholder\' in tensor.op.type:\n      return tensor\n\n    # A tensor is one of the outputs of its underlying\n    # op. Therefore copy the op itself.\n    op = tensor.op\n    new_op = copy(op, dict_swap, scope, True, copy_q, False)\n\n    output_index = op.outputs.index(tensor)\n    new_tensor = new_op.outputs[output_index]\n\n    # Add copied tensor to collections that the original one is in.\n    for name, collection in six.iteritems(tensor.graph._collections):\n      if tensor in collection:\n        graph.add_to_collection(name, new_tensor)\n\n    return new_tensor\n  elif isinstance(org_instance, tf.Operation):\n    op = org_instance\n\n    # Do not copy queue operations.\n    if \'Queue\' in op.type:\n      return op\n\n    # Copy the node def.\n    # It is unique to every Operation instance. Replace the name and\n    # its operation-level seed if it has one.\n    node_def = deepcopy(op.node_def)\n    node_def.name = new_name\n\n    # when copying control flow contexts,\n    # we need to make sure frame definitions are copied\n    if \'frame_name\' in node_def.attr and node_def.attr[\'frame_name\'].s != b\'\':\n      node_def.attr[\'frame_name\'].s = (scope.encode(\'utf-8\') +\n                                       node_def.attr[\'frame_name\'].s)\n\n    if \'seed2\' in node_def.attr and tf.get_seed(None)[1] is not None:\n      node_def.attr[\'seed2\'].i = tf.get_seed(None)[1]\n\n    # Copy other arguments needed for initialization.\n    output_types = op._output_types[:]\n\n    # If it has an original op, copy it.\n    if op._original_op is not None:\n      original_op = copy(op._original_op, dict_swap, scope, True, copy_q, False)\n    else:\n      original_op = None\n\n    # Copy the op def.\n    # It is unique to every Operation type.\n    op_def = deepcopy(op.op_def)\n\n    new_op = tf.Operation(node_def,\n                          graph,\n                          [],  # inputs; will add them afterwards\n                          output_types,\n                          [],  # control inputs; will add them afterwards\n                          [],  # input types; will add them afterwards\n                          original_op,\n                          op_def)\n\n    # advertise op early to break recursions\n    graph._add_op(new_op)\n\n    # If it has control inputs, copy them.\n    control_inputs = []\n    for x in op.control_inputs:\n      elem = copy(x, dict_swap, scope, True, copy_q, False)\n      if not isinstance(elem, tf.Operation):\n        elem = tf.convert_to_tensor(elem)\n\n      control_inputs.append(elem)\n\n    new_op._add_control_inputs(control_inputs)\n\n    # If it has inputs, copy them.\n    for x in op.inputs:\n      elem = copy(x, dict_swap, scope, True, copy_q, False)\n      if not isinstance(elem, tf.Operation):\n        elem = tf.convert_to_tensor(elem)\n\n      new_op._add_input(elem)\n\n    # Copy the control flow context.\n    control_flow_context = _copy_context(op._get_control_flow_context(), {},\n                                         dict_swap, scope, copy_q)\n    new_op._set_control_flow_context(control_flow_context)\n\n    # Use Graph\'s private methods to add the op, following\n    # implementation of `tf.Graph().create_op()`.\n    compute_shapes = True\n    compute_device = True\n    op_type = new_name\n\n    if compute_shapes:\n      set_shapes_for_outputs(new_op)\n    graph._record_op_seen_by_control_dependencies(new_op)\n\n    if compute_device:\n      graph._apply_device_functions(new_op)\n\n    if graph._colocation_stack:\n      all_colocation_groups = []\n      for colocation_op in graph._colocation_stack:\n        all_colocation_groups.extend(colocation_op.colocation_groups())\n        if colocation_op.device:\n          # Make this device match the device of the colocated op, to\n          # provide consistency between the device and the colocation\n          # property.\n          if new_op.device and new_op.device != colocation_op.device:\n            logging.warning(""Tried to colocate %s with an op %s that had ""\n                            ""a different device: %s vs %s. ""\n                            ""Ignoring colocation property."",\n                            name, colocation_op.name, new_op.device,\n                            colocation_op.device)\n\n      all_colocation_groups = sorted(set(all_colocation_groups))\n      new_op.node_def.attr[""_class""].CopyFrom(attr_value_pb2.AttrValue(\n          list=attr_value_pb2.AttrValue.ListValue(s=all_colocation_groups)))\n\n    # Sets ""container"" attribute if\n    # (1) graph._container is not None\n    # (2) ""is_stateful"" is set in OpDef\n    # (3) ""container"" attribute is in OpDef\n    # (4) ""container"" attribute is None\n    if (graph._container and\n        op_type in graph._registered_ops and\n        graph._registered_ops[op_type].is_stateful and\n        ""container"" in new_op.node_def.attr and\n            not new_op.node_def.attr[""container""].s):\n      new_op.node_def.attr[""container""].CopyFrom(\n          attr_value_pb2.AttrValue(s=compat.as_bytes(graph._container)))\n\n    return new_op\n  else:\n    raise TypeError(""Could not copy instance: "" + str(org_instance))\n\n\ndef get_ancestors(x, collection=None):\n  """"""Get ancestor random variables of input.\n\n  Args:\n    x: RandomVariable or tf.Tensor.\n      Query node to find ancestors of.\n    collection: list of RandomVariable.\n      The collection of random variables to check with respect to;\n      defaults to all random variables in the graph.\n\n  Returns:\n    list of RandomVariable.\n    Ancestor random variables of x.\n\n  #### Examples\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(a, 1.0)\n  c = Normal(0.0, 1.0)\n  d = Normal(b * c, 1.0)\n  assert set(ed.get_ancestors(d)) == set([a, b, c])\n  ```\n  """"""\n  if collection is None:\n    collection = random_variables()\n\n  node_dict = {node.value(): node for node in collection}\n\n  # Traverse the graph. Add each node to the set if it\'s in the collection.\n  output = set()\n  visited = set()\n  nodes = {x}\n  while nodes:\n    node = nodes.pop()\n\n    if node in visited:\n      continue\n    visited.add(node)\n\n    if isinstance(node, RandomVariable):\n      node = node.value()\n\n    candidate_node = node_dict.get(node, None)\n    if candidate_node is not None and candidate_node != x:\n      output.add(candidate_node)\n\n    nodes.update(node.op.inputs)\n\n  return list(output)\n\n\ndef get_blanket(x, collection=None):\n  """"""Get Markov blanket of input, which consists of its parents, its\n  children, and the other parents of its children.\n\n  Args:\n    x: RandomVariable or tf.Tensor.\n      Query node to find Markov blanket of.\n    collection: list of RandomVariable.\n      The collection of random variables to check with respect to;\n      defaults to all random variables in the graph.\n\n  Returns:\n    list of RandomVariable.\n    Markov blanket of x.\n\n  #### Examples\n\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(0.0, 1.0)\n  c = Normal(a * b, 1.0)\n  d = Normal(0.0, 1.0)\n  e = Normal(c * d, 1.0)\n  assert set(ed.get_blanket(c)) == set([a, b, d, e])\n  ```\n  """"""\n  output = set()\n  output.update(get_parents(x, collection))\n  children = get_children(x, collection)\n  output.update(children)\n  for child in children:\n    output.update(get_parents(child, collection))\n\n  output.discard(x)\n  return list(output)\n\n\ndef get_children(x, collection=None):\n  """"""Get child random variables of input.\n\n  Args:\n    x: RandomVariable or tf.Tensor>\n      Query node to find children of.\n    collection: list of RandomVariable.\n      The collection of random variables to check with respect to;\n      defaults to all random variables in the graph.\n\n  Returns:\n    list of RandomVariable.\n    Child random variables of x.\n\n  #### Examples\n\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(a, 1.0)\n  c = Normal(a, 1.0)\n  d = Normal(c, 1.0)\n  assert set(ed.get_children(a)) == set([b, c])\n  ```\n  """"""\n  if collection is None:\n    collection = random_variables()\n\n  node_dict = {node.value(): node for node in collection}\n\n  # Traverse the graph. Add each node to the set if it\'s in the collection.\n  output = set()\n  visited = set()\n  nodes = {x}\n  while nodes:\n    node = nodes.pop()\n\n    if node in visited:\n      continue\n    visited.add(node)\n\n    if isinstance(node, RandomVariable):\n      node = node.value()\n\n    candidate_node = node_dict.get(node, None)\n    if candidate_node is not None and candidate_node != x:\n      output.add(candidate_node)\n    else:\n      for op in node.consumers():\n        nodes.update(op.outputs)\n\n  return list(output)\n\n\ndef get_descendants(x, collection=None):\n  """"""Get descendant random variables of input.\n\n  Args:\n    x: RandomVariable or tf.Tensor.\n      Query node to find descendants of.\n    collection: list of RandomVariable.\n      The collection of random variables to check with respect to;\n      defaults to all random variables in the graph.\n\n  Returns:\n    list of RandomVariable.\n    Descendant random variables of x.\n\n  #### Examples\n\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(a, 1.0)\n  c = Normal(a, 1.0)\n  d = Normal(c, 1.0)\n  assert set(ed.get_descendants(a)) == set([b, c, d])\n  ```\n  """"""\n  if collection is None:\n    collection = random_variables()\n\n  node_dict = {node.value(): node for node in collection}\n\n  # Traverse the graph. Add each node to the set if it\'s in the collection.\n  output = set()\n  visited = set()\n  nodes = {x}\n  while nodes:\n    node = nodes.pop()\n\n    if node in visited:\n      continue\n    visited.add(node)\n\n    if isinstance(node, RandomVariable):\n      node = node.value()\n\n    candidate_node = node_dict.get(node, None)\n    if candidate_node is not None and candidate_node != x:\n      output.add(candidate_node)\n\n    for op in node.consumers():\n      nodes.update(op.outputs)\n\n  return list(output)\n\n\ndef get_parents(x, collection=None):\n  """"""Get parent random variables of input.\n\n  Args:\n    x: RandomVariable or tf.Tensor.\n      Query node to find parents of.\n    collection: list of RandomVariable.\n      The collection of random variables to check with respect to;\n      defaults to all random variables in the graph.\n\n  Returns:\n    list of RandomVariable.\n    Parent random variables of x.\n\n  #### Examples\n\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(a, 1.0)\n  c = Normal(0.0, 1.0)\n  d = Normal(b * c, 1.0)\n  assert set(ed.get_parents(d)) == set([b, c])\n  ```\n  """"""\n  if collection is None:\n    collection = random_variables()\n\n  node_dict = {node.value(): node for node in collection}\n\n  # Traverse the graph. Add each node to the set if it\'s in the collection.\n  output = set()\n  visited = set()\n  nodes = {x}\n  while nodes:\n    node = nodes.pop()\n\n    if node in visited:\n      continue\n    visited.add(node)\n\n    if isinstance(node, RandomVariable):\n      node = node.value()\n\n    candidate_node = node_dict.get(node, None)\n    if candidate_node is not None and candidate_node != x:\n      output.add(candidate_node)\n    else:\n      nodes.update(node.op.inputs)\n\n  return list(output)\n\n\ndef get_siblings(x, collection=None):\n  """"""Get sibling random variables of input.\n\n  Args:\n    x: RandomVariable or tf.Tensor.\n      Query node to find siblings of.\n    collection: list of RandomVariable.\n      The collection of random variables to check with respect to;\n      defaults to all random variables in the graph.\n\n  Returns:\n    list of RandomVariable.\n    Sibling random variables of x.\n\n  #### Examples\n\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(a, 1.0)\n  c = Normal(a, 1.0)\n  assert ed.get_siblings(b) == [c]\n  ```\n  """"""\n  parents = get_parents(x, collection)\n  siblings = set()\n  for parent in parents:\n    siblings.update(get_children(parent, collection))\n\n  siblings.discard(x)\n  return list(siblings)\n\n\ndef get_variables(x, collection=None):\n  """"""Get parent TensorFlow variables of input.\n\n  Args:\n    x: RandomVariable or tf.Tensor.\n      Query node to find parents of.\n    collection: list of tf.Variable.\n      The collection of variables to check with respect to; defaults to\n      all variables in the graph.\n\n  Returns:\n    list of tf.Variable.\n    TensorFlow variables that x depends on.\n\n  #### Examples\n\n  ```python\n  a = tf.Variable(0.0)\n  b = tf.Variable(0.0)\n  c = Normal(a * b, 1.0)\n  assert set(ed.get_variables(c)) == set([a, b])\n  ```\n  """"""\n  if collection is None:\n    collection = tf.global_variables()\n\n  node_dict = {node.name: node for node in collection}\n\n  # Traverse the graph. Add each node to the set if it\'s in the collection.\n  output = set()\n  visited = set()\n  nodes = {x}\n  while nodes:\n    node = nodes.pop()\n\n    if node in visited:\n      continue\n    visited.add(node)\n\n    if isinstance(node, RandomVariable):\n      node = node.value()\n\n    candidate_node = node_dict.get(node.name, None)\n    if candidate_node is not None and candidate_node != x:\n      output.add(candidate_node)\n\n    nodes.update(node.op.inputs)\n\n  return list(output)\n\n\ndef is_independent(a, b, condition=None):\n  """"""Assess whether a is independent of b given the random variables in\n  condition.\n\n  Implemented using the Bayes-Ball algorithm [@schachter1998bayes].\n\n  Args:\n    a: RandomVariable or list of RandomVariable.\n       Query node(s).\n    b: RandomVariable or list of RandomVariable.\n       Query node(s).\n    condition: RandomVariable or list of RandomVariable.\n       Random variable(s) to condition on.\n\n  Returns:\n    bool.\n    True if a is independent of b given the random variables in condition.\n\n  #### Examples\n\n  ```python\n  a = Normal(0.0, 1.0)\n  b = Normal(a, 1.0)\n  c = Normal(a, 1.0)\n  assert ed.is_independent(b, c, condition=a)\n  ```\n  """"""\n  if condition is None:\n    condition = []\n  if not isinstance(a, list):\n    a = [a]\n  if not isinstance(b, list):\n    b = [b]\n  if not isinstance(condition, list):\n    condition = [condition]\n  A = set(a)\n  B = set(b)\n  condition = set(condition)\n\n  top_marked = set()\n  # The Bayes-Ball algorithm will traverse the belief network\n  # and add each node that is relevant to B given condition\n  # to the set bottom_marked. A and B are conditionally\n  # independent if no node in A is in bottom_marked.\n  bottom_marked = set()\n\n  schedule = [(node, ""child"") for node in B]\n  while schedule:\n    node, came_from = schedule.pop()\n\n    if node not in condition and came_from == ""child"":\n      if node not in top_marked:\n        top_marked.add(node)\n        for parent in get_parents(node):\n          schedule.append((parent, ""child""))\n\n      if not isinstance(node, PointMass) and node not in bottom_marked:\n        bottom_marked.add(node)\n        if node in A:\n          return False  # node in A is relevant to B\n        for child in get_children(node):\n          schedule.append((child, ""parent""))\n\n    elif came_from == ""parent"":\n      if node in condition and node not in top_marked:\n        top_marked.add(node)\n        for parent in get_parents(node):\n          schedule.append((parent, ""child""))\n\n      elif node not in condition and node not in bottom_marked:\n        bottom_marked.add(node)\n        if node in A:\n          return False  # node in A is relevant to B\n        for child in get_children(node):\n          schedule.append((child, ""parent""))\n\n  return True\n\n\ndef transform(x, *args, **kwargs):\n  """"""Transform a continuous random variable to the unconstrained space.\n\n  `transform` selects among a number of default transformations which\n  depend on the support of the provided random variable:\n\n  + $[0, 1]$ (e.g., Beta): Inverse of sigmoid.\n  + $[0, \\infty)$ (e.g., Gamma): Inverse of softplus.\n  + Simplex (e.g., Dirichlet): Inverse of softmax-centered.\n  + $(-\\infty, \\infty)$ (e.g., Normal, MultivariateNormalTriL): None.\n\n  Args:\n    x: RandomVariable.\n      Continuous random variable to transform.\n    *args, **kwargs:\n      Arguments to overwrite when forming the `TransformedDistribution`.\n      For example, manually specify the transformation by passing in\n      the `bijector` argument.\n\n  Returns:\n    RandomVariable.\n    A `TransformedDistribution` random variable, or the provided random\n    variable if no transformation was applied.\n\n  #### Examples\n\n  ```python\n  x = Gamma(1.0, 1.0)\n  y = ed.transform(x)\n  sess = tf.Session()\n  sess.run(y)\n  -2.2279539\n  ```\n  """"""\n  if len(args) != 0 or kwargs.get(\'bijector\', None) is not None:\n    return TransformedDistribution(x, *args, **kwargs)\n\n  try:\n    support = x.support\n  except AttributeError as e:\n    msg = """"""\'{}\' object has no \'support\'\n             so cannot be transformed."""""".format(type(x).__name__)\n    raise AttributeError(msg)\n\n  if support == \'01\':\n    bij = tfb.Invert(tfb.Sigmoid())\n    new_support = \'real\'\n  elif support == \'nonnegative\':\n    bij = tfb.Invert(tfb.Softplus())\n    new_support = \'real\'\n  elif support == \'simplex\':\n    bij = tfb.Invert(tfb.SoftmaxCentered(event_ndims=1))\n    new_support = \'multivariate_real\'\n  elif support in (\'real\', \'multivariate_real\'):\n    return x\n  else:\n    msg = ""\'transform\' does not handle supports of type \'{}\'"".format(support)\n    raise ValueError(msg)\n\n  new_x = TransformedDistribution(x, bij, *args, **kwargs)\n  new_x.support = new_support\n  return new_x\n\n\ndef compute_multinomial_mode(probs, total_count=1, seed=None):\n  """"""Compute the mode of a Multinomial random variable.\n\n  Args:\n    probs: 1-D Numpy array of Multinomial class probabilities\n    total_count: integer number of trials in single Multinomial draw\n    seed: a Python integer. Used to create a random seed for the\n      distribution\n\n  #### Examples\n\n  ```python\n  # returns either [2, 2, 1], [2, 1, 2] or [1, 2, 2]\n  probs = np.array(3 * [1/3])\n  total_count = 5\n  compute_multinomial_mode(probs, total_count)\n\n  # returns [3, 2, 0]\n  probs = np.array(3 * [1/3])\n  total_count = 5\n  compute_multinomial_mode(probs, total_count)\n  ```\n  """"""\n  def softmax(vec):\n    numerator = np.exp(vec)\n    return numerator / numerator.sum(axis=0)\n\n  random_state = np.random.RandomState(seed)\n  mode = np.zeros_like(probs, dtype=np.int32)\n  if total_count == 1:\n    mode[np.argmax(probs)] += 1\n    return list(mode)\n  remaining_count = total_count\n  uniform_prob = 1 / total_count\n\n  while remaining_count > 0:\n    if (probs < uniform_prob).all():\n      probs = softmax(probs)\n    mask = probs >= uniform_prob\n    overflow_count = int(mask.sum() - remaining_count)\n    if overflow_count > 0:\n      hot_indices = np.where(mask)[0]\n      cold_indices = random_state.choice(hot_indices, overflow_count,\n                                         replace=False)\n      mask[cold_indices] = False\n    mode[mask] += 1\n    probs[mask] -= uniform_prob\n    remaining_count -= np.sum(mask)\n  return mode\n'"
edward/util/tensorflow.py,47,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef dot(x, y):\n  """"""Compute dot product between a 2-D tensor and a 1-D tensor.\n\n  If x is a `[M x N]` matrix, then y is a `M`-vector.\n\n  If x is a `M`-vector, then y is a `[M x N]` matrix.\n\n  Args:\n    x: tf.Tensor.\n      A 1-D or 2-D tensor (see above).\n    y: tf.Tensor.\n      A 1-D or 2-D tensor (see above).\n\n  Returns:\n    tf.Tensor.\n    A 1-D tensor of length `N`.\n\n  Raises:\n    InvalidArgumentError.\n    If the inputs have Inf or NaN values.\n  """"""\n  x = tf.convert_to_tensor(x)\n  y = tf.convert_to_tensor(y)\n  dependencies = [tf.verify_tensor_all_finite(x, msg=\'\'),\n                  tf.verify_tensor_all_finite(y, msg=\'\')]\n  x = control_flow_ops.with_dependencies(dependencies, x)\n  y = control_flow_ops.with_dependencies(dependencies, y)\n\n  if len(x.shape) == 1:\n    vec = x\n    mat = y\n    return tf.reshape(tf.matmul(tf.expand_dims(vec, 0), mat), [-1])\n  else:\n    mat = x\n    vec = y\n    return tf.reshape(tf.matmul(mat, tf.expand_dims(vec, 1)), [-1])\n\n\ndef rbf(X, X2=None, lengthscale=1.0, variance=1.0):\n  """"""Radial basis function kernel, also known as the squared\n  exponential or exponentiated quadratic. It is defined as\n\n  $k(x, x\') = \\sigma^2 \\exp\\Big(\n      -\\\\frac{1}{2} \\sum_{d=1}^D \\\\frac{1}{\\ell_d^2} (x_d - x\'_d)^2 \\Big)$\n\n  for output variance $\\sigma^2$ and lengthscale $\\ell^2$.\n\n  The kernel is evaluated over all pairs of rows, `k(X[i, ], X2[j, ])`.\n  If `X2` is not specified, then it evaluates over all pairs\n  of rows in `X`, `k(X[i, ], X[j, ])`. The output is a matrix\n  where each entry (i, j) is the kernel over the ith and jth rows.\n\n  Args:\n    X: tf.Tensor.\n      N x D matrix of N data points each with D features.\n    X2: tf.Tensor.\n      N x D matrix of N data points each with D features.\n    lengthscale: tf.Tensor.\n      Lengthscale parameter, a positive scalar or D-dimensional vector.\n    variance: tf.Tensor.\n      Output variance parameter, a positive scalar.\n\n  #### Examples\n\n  ```python\n  X = tf.random_normal([100, 5])\n  K = ed.rbf(X)\n  assert K.shape == (100, 100)\n  ```\n  """"""\n  lengthscale = tf.convert_to_tensor(lengthscale)\n  variance = tf.convert_to_tensor(variance)\n  dependencies = [tf.assert_positive(lengthscale),\n                  tf.assert_positive(variance)]\n  lengthscale = control_flow_ops.with_dependencies(dependencies, lengthscale)\n  variance = control_flow_ops.with_dependencies(dependencies, variance)\n\n  X = tf.convert_to_tensor(X)\n  X = X / lengthscale\n  Xs = tf.reduce_sum(tf.square(X), 1)\n  if X2 is None:\n    X2 = X\n    X2s = Xs\n  else:\n    X2 = tf.convert_to_tensor(X2)\n    X2 = X2 / lengthscale\n    X2s = tf.reduce_sum(tf.square(X2), 1)\n\n  square = tf.reshape(Xs, [-1, 1]) + tf.reshape(X2s, [1, -1]) - \\\n      2 * tf.matmul(X, X2, transpose_b=True)\n  output = variance * tf.exp(-square / 2)\n  return output\n\n\ndef to_simplex(x):\n  """"""Transform real vector of length `(K-1)` to a simplex of dimension `K`\n  using a backward stick breaking construction.\n\n  Args:\n    x: tf.Tensor.\n      A 1-D or 2-D tensor.\n\n  Returns:\n    tf.Tensor.\n    A tensor of same shape as input but with last dimension of\n    size `K`.\n\n  Raises:\n    InvalidArgumentError.\n    If the input has Inf or NaN values.\n\n  #### Notes\n\n  x as a 3-D or higher tensor is not guaranteed to be supported.\n  """"""\n  x = tf.cast(x, dtype=tf.float32)\n  dependencies = [tf.verify_tensor_all_finite(x, msg=\'\')]\n  x = control_flow_ops.with_dependencies(dependencies, x)\n\n  if isinstance(x, (tf.Tensor, tf.Variable)):\n    shape = x.get_shape().as_list()\n  else:\n    shape = x.shape\n\n  if len(shape) == 1:\n    K_minus_one = shape[0]\n    eq = -tf.log(tf.cast(K_minus_one - tf.range(K_minus_one), dtype=tf.float32))\n    z = tf.sigmoid(eq + x)\n    pil = tf.concat([z, tf.constant([1.0])], 0)\n    piu = tf.concat([tf.constant([1.0]), 1.0 - z], 0)\n    S = tf.cumprod(piu)\n    return S * pil\n  else:\n    n_rows = shape[0]\n    K_minus_one = shape[1]\n    eq = -tf.log(tf.cast(K_minus_one - tf.range(K_minus_one), dtype=tf.float32))\n    z = tf.sigmoid(eq + x)\n    pil = tf.concat([z, tf.ones([n_rows, 1])], 1)\n    piu = tf.concat([tf.ones([n_rows, 1]), 1.0 - z], 1)\n    S = tf.cumprod(piu, axis=1)\n    return S * pil\n\n\ndef get_control_variate_coef(f, h):\n  """"""Returns scalar used by control variates method for variance reduction in\n  Monte Carlo methods.\n\n  If we have a statistic $m$ as an unbiased estimator of $\\mu$ and\n  and another statistic $t$ which is an unbiased estimator of\n  $\\\\tau$ then $m^* = m + c(t - \\\\tau)$ is also an unbiased\n  estimator of $\\mu$ for any coefficient $c$.\n\n  This function calculates the optimal coefficient\n\n  $c^* = \\\\frac{\\\\text{Cov}(m,t)}{\\\\text{Var}(t)}$\n\n  for minimizing the variance of $m^*$.\n\n  Args:\n    f: tf.Tensor.\n      A 1-D tensor.\n    h: tf.Tensor.\n      A 1-D tensor.\n\n  Returns:\n    tf.Tensor.\n    A 0 rank tensor\n  """"""\n  f_mu = tf.reduce_mean(f)\n  h_mu = tf.reduce_mean(h)\n\n  n = f.shape[0].value\n\n  cov_fh = tf.reduce_sum((f - f_mu) * (h - h_mu)) / (n - 1)\n  var_h = tf.reduce_sum(tf.square(h - h_mu)) / (n - 1)\n\n  a = cov_fh / var_h\n\n  return a\n'"
examples/eight_schools/eight_schools.py,16,"b'""""""Implement the stan 8 schools example using the recommended non-centred\nparameterization.\n\nThe Stan example is slightly modified to avoid improper priors and\navoid half-Cauchy priors.  Inference is with Edward using both HMC\nand KLQP.\n\nThis model has a hierachy and an inferred variance - yet the example is\nvery simple - only the Normal distribution is used.\n\n#### References\nhttps://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\nhttp://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\nimport numpy as np\nfrom edward.models import Normal, Empirical\n\n\ndef main(_):\n  # data\n  J = 8\n  data_y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n  data_sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n\n  # model definition\n  mu = Normal(0., 10.)\n  logtau = Normal(5., 1.)\n  theta_prime = Normal(tf.zeros(J), tf.ones(J))\n  sigma = tf.placeholder(tf.float32, J)\n  y = Normal(mu + tf.exp(logtau) * theta_prime, sigma * tf.ones([J]))\n\n  data = {y: data_y, sigma: data_sigma}\n\n  # ed.KLqp inference\n  with tf.variable_scope(\'q_logtau\'):\n    q_logtau = Normal(tf.get_variable(\'loc\', []),\n                      tf.nn.softplus(tf.get_variable(\'scale\', [])))\n\n  with tf.variable_scope(\'q_mu\'):\n    q_mu = Normal(tf.get_variable(\'loc\', []),\n                  tf.nn.softplus(tf.get_variable(\'scale\', [])))\n\n  with tf.variable_scope(\'q_theta_prime\'):\n    q_theta_prime = Normal(tf.get_variable(\'loc\', [J]),\n                           tf.nn.softplus(tf.get_variable(\'scale\', [J])))\n\n  inference = ed.KLqp({logtau: q_logtau, mu: q_mu,\n                      theta_prime: q_theta_prime}, data=data)\n  inference.run(n_samples=15, n_iter=60000)\n  print(""====  ed.KLqp inference ===="")\n  print(""E[mu] = %f"" % (q_mu.mean().eval()))\n  print(""E[logtau] = %f"" % (q_logtau.mean().eval()))\n  print(""E[theta_prime]="")\n  print((q_theta_prime.mean().eval()))\n  print(""====  end ed.KLqp inference ===="")\n  print("""")\n  print("""")\n\n  # HMC inference\n  S = 400000\n  burn = S // 2\n\n  hq_logtau = Empirical(tf.get_variable(\'hq_logtau\', [S]))\n  hq_mu = Empirical(tf.get_variable(\'hq_mu\', [S]))\n  hq_theta_prime = Empirical(tf.get_variable(\'hq_thetaprime\', [S, J]))\n\n  inference = ed.HMC({logtau: hq_logtau, mu: hq_mu,\n                     theta_prime: hq_theta_prime}, data=data)\n  inference.run()\n\n  print(""====  ed.HMC inference ===="")\n  print(""E[mu] = %f"" % (hq_mu.params.eval()[burn:].mean()))\n  print(""E[logtau] = %f"" % (hq_logtau.params.eval()[burn:].mean()))\n  print(""E[theta_prime]="")\n  print(hq_theta_prime.params.eval()[burn:, ].mean(0))\n  print(""====  end ed.HMC inference ===="")\n  print("""")\n  print("""")\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
examples/eight_schools/eight_schools_pystan.py,0,"b'""""""Implement the stan 8 schools example using the recommended non-centred\nparameterization.\n\nThe Stan example is slightly modified to avoid improper priors and avoid\nhalf-Cauchy priors.  Inference is with Stan using NUTS, pystan is required.\n\nThis model has a hierachy and an inferred variance - yet the example is\nvery simple - only the Normal distribution is used.\n\n#### References\nhttps://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\nhttp://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pystan\n\n\ndef main():\n  # data\n  J = 8\n  data_y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n  data_sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n\n  standata = dict(J=J, y=data_y, sigma=data_sigma)\n  fit = pystan.stan(\'eight_schools.stan\', data=standata, iter=100000)\n  print(fit)\n\nif __name__ == ""__main__"":\n  main()\n'"
tests/criticisms/evaluate_test.py,40,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Categorical, Multinomial, Normal\n\n\nclass test_evaluate_class(tf.test.TestCase):\n\n  RANDOM_SEED = 12345\n\n  def test_metrics(self):\n    with self.test_session():\n      x = Normal(loc=0.0, scale=1.0)\n      x_data = tf.constant(0.0)\n      ed.evaluate('mean_squared_error', {x: x_data}, n_samples=1)\n      ed.evaluate(['mean_squared_error'], {x: x_data}, n_samples=1)\n      ed.evaluate(['mean_squared_error', 'mean_absolute_error'],\n                  {x: x_data}, n_samples=1)\n      self.assertRaises(TypeError, ed.evaluate, x, {x: x_data}, n_samples=1)\n      self.assertRaises(NotImplementedError, ed.evaluate, 'hello world',\n                        {x: x_data}, n_samples=1)\n\n  def test_metrics_classification(self):\n    with self.test_session():\n      x = Bernoulli(probs=0.51)\n      x_data = tf.constant(1)\n      self.assertAllClose(\n          1.0,\n          ed.evaluate('binary_accuracy', {x: x_data}, n_samples=1))\n      x = Bernoulli(probs=0.51, sample_shape=5)\n      x_data = tf.constant([1, 1, 1, 0, 0])\n      self.assertAllClose(\n          0.6,\n          ed.evaluate('binary_accuracy', {x: x_data}, n_samples=1))\n      x = Bernoulli(probs=tf.constant([0.51, 0.49, 0.49]))\n      x_data = tf.constant([1, 0, 1])\n      self.assertAllClose(\n          2.0 / 3,\n          ed.evaluate('binary_accuracy', {x: x_data}, n_samples=1))\n\n      x = Categorical(probs=tf.constant([0.48, 0.51, 0.01]))\n      x_data = tf.constant(1)\n      self.assertAllClose(\n          1.0,\n          ed.evaluate('sparse_categorical_accuracy', {x: x_data}, n_samples=1))\n      x = Categorical(probs=tf.constant([0.48, 0.51, 0.01]), sample_shape=5)\n      x_data = tf.constant([1, 1, 1, 0, 2])\n      self.assertAllClose(\n          0.6,\n          ed.evaluate('sparse_categorical_accuracy', {x: x_data}, n_samples=1))\n      x = Categorical(\n          probs=tf.constant([[0.48, 0.51, 0.01], [0.51, 0.48, 0.01]]))\n      x_data = tf.constant([1, 2])\n      self.assertAllClose(\n          0.5,\n          ed.evaluate('sparse_categorical_accuracy', {x: x_data}, n_samples=1))\n\n      x = Multinomial(total_count=1.0, probs=tf.constant([0.48, 0.51, 0.01]))\n      x_data = tf.constant([0, 1, 0], dtype=x.dtype.as_numpy_dtype)\n      self.assertAllClose(\n          1.0,\n          ed.evaluate('categorical_accuracy', {x: x_data}, n_samples=1))\n      x = Multinomial(total_count=1.0, probs=tf.constant([0.48, 0.51, 0.01]),\n                      sample_shape=5)\n      x_data = tf.constant(\n          [[0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]],\n          dtype=x.dtype.as_numpy_dtype)\n      self.assertAllClose(\n          0.6,\n          ed.evaluate('categorical_accuracy', {x: x_data}, n_samples=1))\n\n      x = Multinomial(total_count=5.0, probs=tf.constant([0.4, 0.6, 0.0]))\n      x_data = tf.constant([2, 3, 0], dtype=x.dtype.as_numpy_dtype)\n      self.assertAllClose(\n          1.0,\n          ed.evaluate('multinomial_accuracy', {x: x_data}, n_samples=1))\n\n  def test_metrics_with_binary_averaging(self):\n    x = Multinomial(total_count=10.0, probs=tf.constant([0.2, 0.7, 0.1]))\n    x_data = tf.constant([5, 4, 1], dtype=x.dtype.as_numpy_dtype)\n    self.assertAllEqual(\n        np.array([9.0, 4.0, 1.0], dtype=np.float32),\n        ed.evaluate([('mean_squared_error', {'average': None})],\n                    {x: x_data}, n_samples=1, seed=self.RANDOM_SEED))\n    x = Multinomial(total_count=10.0, probs=tf.constant([0.2, 0.7, 0.1]))\n    x_data = tf.constant([5, 4, 1], dtype=x.dtype.as_numpy_dtype)\n    self.assertAllClose(\n        4.6666665,\n        ed.evaluate([('mean_squared_error', {'average': 'macro'})],\n                    {x: x_data}, n_samples=1, seed=self.RANDOM_SEED))\n    x = Multinomial(total_count=10.0, probs=tf.constant([0.2, 0.7, 0.1]))\n    x_data = tf.constant([5, 4, 1], dtype=x.dtype.as_numpy_dtype)\n    self.assertAllClose(\n        4.6666665,\n        ed.evaluate([('mean_squared_error', {'average': 'micro'})],\n                    {x: x_data}, n_samples=1, seed=self.RANDOM_SEED))\n\n    x = Multinomial(total_count=10.0, probs=tf.constant([0.2, 0.7, 0.1]),\n                    sample_shape=5)\n    x_data = tf.constant(\n        [[2, 7, 1], [3, 6, 1], [3, 5, 2], [4, 4, 2], [2, 7, 1]],\n        dtype=x.dtype.as_numpy_dtype)\n    self.assertAllEqual(\n        np.array([1.2, 1.4, 0.6], dtype=np.float32),\n        ed.evaluate([('mean_squared_error', {'average': None})],\n                    {x: x_data}, n_samples=1, seed=self.RANDOM_SEED))\n    x = Multinomial(total_count=10.0, probs=tf.constant([0.2, 0.7, 0.1]),\n                    sample_shape=5)\n    x_data = tf.constant(\n        [[2, 7, 1], [3, 6, 1], [3, 5, 2], [4, 4, 2], [2, 7, 1]],\n        dtype=x.dtype.as_numpy_dtype)\n    self.assertAllClose(\n        1.066666603088379,\n        ed.evaluate([('mean_squared_error', {'average': 'macro'})],\n                    {x: x_data}, n_samples=1, seed=self.RANDOM_SEED))\n    x = Multinomial(total_count=10.0, probs=tf.constant([0.2, 0.7, 0.1]),\n                    sample_shape=5)\n    x_data = tf.constant(\n        [[2, 7, 1], [3, 6, 1], [3, 5, 2], [4, 4, 2], [2, 7, 1]],\n        dtype=x.dtype.as_numpy_dtype)\n    self.assertAllClose(\n        1.0666667222976685,\n        ed.evaluate([('mean_squared_error', {'average': 'micro'})],\n                    {x: x_data}, n_samples=1, seed=self.RANDOM_SEED))\n\n  def test_data(self):\n    with self.test_session():\n      x_ph = tf.placeholder(tf.float32, [])\n      x = Normal(loc=x_ph, scale=1.0)\n      y = 2.0 * Normal(loc=0.0, scale=1.0)\n      x_data = tf.constant(0.0)\n      x_ph_data = np.array(0.0)\n      y_data = tf.constant(20.0)\n      ed.evaluate('mean_squared_error', {x: x_data, x_ph: x_ph_data},\n                  n_samples=1)\n      ed.evaluate('mean_squared_error', {y: y_data}, n_samples=1)\n      self.assertRaises(TypeError, ed.evaluate, 'mean_squared_error',\n                        {'y': y_data}, n_samples=1)\n\n  def test_n_samples(self):\n    with self.test_session():\n      x = Normal(loc=0.0, scale=1.0)\n      x_data = tf.constant(0.0)\n      ed.evaluate('mean_squared_error', {x: x_data}, n_samples=1)\n      ed.evaluate('mean_squared_error', {x: x_data}, n_samples=5)\n      self.assertRaises(TypeError, ed.evaluate, 'mean_squared_error',\n                        {x: x_data}, n_samples='1')\n\n  def test_output_key(self):\n    with self.test_session():\n      x_ph = tf.placeholder(tf.float32, [])\n      x = Normal(loc=x_ph, scale=1.0)\n      y = 2.0 * x\n      x_data = tf.constant(0.0)\n      x_ph_data = np.array(0.0)\n      y_data = tf.constant(20.0)\n      ed.evaluate('mean_squared_error', {x: x_data, x_ph: x_ph_data},\n                  n_samples=1)\n      ed.evaluate('mean_squared_error', {y: y_data, x_ph: x_ph_data},\n                  n_samples=1)\n      ed.evaluate('mean_squared_error', {x: x_data, y: y_data, x_ph: x_ph_data},\n                  n_samples=1, output_key=x)\n      self.assertRaises(KeyError, ed.evaluate, 'mean_squared_error',\n                        {x: x_data, y: y_data, x_ph: x_ph_data}, n_samples=1)\n      self.assertRaises(TypeError, ed.evaluate, 'mean_squared_error',\n                        {x: x_data, y: y_data, x_ph: x_ph_data}, n_samples=1,\n                        output_key='x')\n\n  def test_custom_metric(self):\n    def logcosh(y_true, y_pred):\n      diff = y_pred - y_true\n      return tf.reduce_mean(diff + tf.nn.softplus(-2.0 * diff) - tf.log(2.0),\n                            axis=-1)\n    with self.test_session():\n      x = Normal(loc=0.0, scale=1.0)\n      x_data = tf.constant(0.0)\n      ed.evaluate(logcosh, {x: x_data}, n_samples=1)\n      ed.evaluate(['mean_squared_error', logcosh], {x: x_data}, n_samples=1)\n      self.assertRaises(NotImplementedError, ed.evaluate, 'logcosh',\n                        {x: x_data}, n_samples=1)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/criticisms/metrics_test.py,21,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.criticisms.evaluate import *\n\nall_classification_metrics = [\n    binary_accuracy,\n    sparse_categorical_accuracy,\n]\n\nall_real_classification_metrics = [\n    binary_crossentropy,\n    categorical_crossentropy,\n    hinge,\n    squared_hinge,\n]\n\nall_regression_metrics = [\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_squared_logarithmic_error,\n    poisson,\n    cosine_proximity,\n]\n\nall_specialized_input_output_metrics = [\n    categorical_accuracy,\n    sparse_categorical_crossentropy,\n    kl_divergence\n]\n\nall_metrics_with_binary_averaging = [\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_squared_logarithmic_error\n]\n\n\nclass test_metrics_class(tf.test.TestCase):\n\n  def _check_averaging(self, metric, y_true, y_pred):\n    n_classes = tf.squeeze(tf.shape(y_true)[-1]).eval()\n    class_scores = [metric(y_true[i], y_pred[i]) for i in range(n_classes)]\n\n    # No averaging\n    no_average = metric(y_true, y_pred, average=None)\n    expected_no_average = tf.stack(class_scores)\n    self.assertAllEqual(no_average.eval(), expected_no_average.eval())\n\n    # Macro-averaging\n    macro_average = metric(y_true, y_pred, average='macro')\n    expected_macro_average = tf.reduce_mean(tf.stack(class_scores))\n    self.assertAllEqual(macro_average.eval(), expected_macro_average.eval())\n\n    # Micro-averaging\n    micro_average = metric(y_true, y_pred, average='micro')\n    expected_micro_average = metric(tf.reshape(y_true, [1, -1]),\n                                    tf.reshape(y_pred, [1, -1]))\n    self.assertAllEqual(micro_average.eval(), expected_micro_average.eval())\n\n  def test_classification_metrics(self):\n    with self.test_session():\n      y_true = tf.convert_to_tensor(np.random.randint(0, 1, (2, 3)))\n      y_pred = tf.convert_to_tensor(np.random.randint(0, 1, (2, 3)))\n      for metric in all_classification_metrics:\n        self.assertEqual(metric(y_true, y_pred).eval().shape, ())\n\n  def test_real_classification_metrics(self):\n    with self.test_session():\n      y_true = tf.convert_to_tensor(np.random.randint(0, 5, (6, 7)))\n      y_pred = tf.random_normal([6, 7])\n      for metric in all_real_classification_metrics:\n        self.assertEqual(metric(y_true, y_pred).eval().shape, ())\n\n  def test_regression_metrics(self):\n    with self.test_session():\n      y_true = tf.random_normal([6, 7])\n      y_pred = tf.random_normal([6, 7])\n      for metric in all_regression_metrics:\n        self.assertEqual(metric(y_true, y_pred).eval().shape, ())\n\n  def test_specialized_input_output_metrics(self):\n    with self.test_session():\n      for metric in all_specialized_input_output_metrics:\n        if metric == categorical_accuracy:\n          y_true = tf.convert_to_tensor(np.random.randint(0, 1, (6, 7)))\n          y_pred = tf.convert_to_tensor(np.random.randint(0, 7, (6,)))\n          self.assertEqual(metric(y_true, y_pred).eval().shape, ())\n        elif metric == sparse_categorical_crossentropy:\n          y_true = tf.convert_to_tensor(np.random.randint(0, 5, (6)))\n          y_pred = tf.random_normal([6, 7])\n          self.assertEqual(metric(y_true, y_pred).eval().shape, ())\n        elif metric == kl_divergence:\n          y_true = tf.nn.softmax(tf.random_normal([6]))\n          y_pred = tf.nn.softmax(tf.random_normal([6]))\n          self.assertEqual(metric(y_true, y_pred).eval().shape, ())\n        else:\n          raise NotImplementedError()\n\n  def test_metrics_with_binary_averaging(self):\n    with self.test_session():\n      y_true = tf.constant([[1.0, 2.0, 3.0], [2.0, 3.0, 4.0], [3.0, 4.0, 5.0]])\n      y_pred = tf.constant([[2.0, 4.0, 6.0], [4.0, 6.0, 8.0], [6.0, 8.0, 10.0]])\n      for metric in all_metrics_with_binary_averaging:\n        self._check_averaging(metric, y_true, y_pred)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/criticisms/ppc_plots_test.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\n\nclass test_ppc_plots_class(tf.test.TestCase):\n\n  def test_ppc_density_plot(self):\n    y = np.random.randn(20)\n    y_rep = np.random.randn(20, 20)\n\n    ed.ppc_density_plot(y, y_rep)\n\n  def test_ppc_stat_hist_plot(self):\n    y = np.random.randn(20)\n    t = 0.0\n\n    ed.ppc_stat_hist_plot(t, y, stat_name=""mean"", bins=10)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/criticisms/ppc_test.py,18,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_ppc_class(tf.test.TestCase):\n\n  def test_data(self):\n    with self.test_session():\n      x = Normal(loc=0.0, scale=1.0)\n      y = 2.0 * x\n      x_data = tf.constant(0.0)\n      y_data = tf.constant(0.0)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]), {x: x_data}, n_samples=1)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[y]), {y: y_data}, n_samples=1)\n      self.assertRaises(TypeError, ed.ppc, lambda xs, zs: tf.reduce_mean(xs[y]),\n                        {'y': y_data}, n_samples=1)\n\n  def test_latent_vars(self):\n    with self.test_session():\n      x = Normal(loc=0.0, scale=1.0)\n      y = 2.0 * x\n      z = Normal(loc=0.0, scale=1.0)\n      x_data = tf.constant(0.0)\n      y_data = tf.constant(0.0)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]) + tf.reduce_mean(zs[z]),\n             {x: x_data}, {z: z}, n_samples=1)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]) + tf.reduce_mean(zs[z]),\n             {x: x_data}, {z: y}, n_samples=1)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]) + tf.reduce_mean(zs[y]),\n             {x: x_data}, {y: y}, n_samples=1)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]) + tf.reduce_mean(zs[y]),\n             {x: x_data}, {y: z}, n_samples=1)\n      self.assertRaises(TypeError, ed.ppc, lambda xs, zs: tf.reduce_mean(xs[x]),\n                        {x: x_data}, {'y': z}, n_samples=1)\n\n  def test_n_samples(self):\n    with self.test_session():\n      x = Normal(loc=0.0, scale=1.0)\n      x_data = tf.constant(0.0)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]), {x: x_data}, n_samples=1)\n      ed.ppc(lambda xs, zs: tf.reduce_mean(xs[x]), {x: x_data}, n_samples=5)\n      self.assertRaises(TypeError, ed.ppc, lambda xs, zs: tf.reduce_mean(xs[x]),\n                        {x: x_data}, n_samples='1')\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/data/generate_test_saver.py,4,"b'""""""Generate `test_saver`.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, PointMass\n\n\ndef main(_):\n  x_data = np.array([0.0] * 50, dtype=np.float32)\n\n  mu = Normal(loc=0.0, scale=1.0)\n  x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n  with tf.variable_scope(""posterior""):\n    qmu = PointMass(params=tf.Variable(1.0))\n\n  inference = ed.MAP({mu: qmu}, data={x: x_data})\n  inference.run(n_iter=10)\n\n  sess = ed.get_session()\n  saver = tf.train.Saver()\n  saver.save(sess, ""test_saver"")\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
tests/data/generate_toy_data_tfrecords.py,4,"b'""""""Generate `toy_data.tfrecords`.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef main(_):\n  xs = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n  writer = tf.python_io.TFRecordWriter(""toy_data.tfrecords"")\n  for x in xs:\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\'outcome\':\n                 tf.train.Feature(float_list=tf.train.FloatList(value=[x]))}))\n    serialized = example.SerializeToString()\n    writer.write(serialized)\n\n  writer.close()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
tests/inferences/ar_process_test.py,9,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, PointMass\nfrom scipy.optimize import minimize\n\nfrom edward.models import RandomVariable\nfrom tensorflow.contrib.distributions import Distribution\nfrom tensorflow.contrib.distributions import FULLY_REPARAMETERIZED\n\n\nclass AutoRegressive(RandomVariable, Distribution):\n  # a 1-D AR(1) process\n  # a[t + 1] = a[t] + eps with eps ~ N(0, sig**2)\n  def __init__(self, T, a, sig, *args, **kwargs):\n    self.a = a\n    self.sig = sig\n    self.T = T\n    self.shocks = Normal(tf.zeros(T), scale=sig)\n    self.z = tf.scan(lambda acc, x: self.a * acc + x, self.shocks)\n\n    if 'dtype' not in kwargs:\n      kwargs['dtype'] = tf.float32\n    if 'allow_nan_stats' not in kwargs:\n      kwargs['allow_nan_stats'] = False\n    if 'reparameterization_type' not in kwargs:\n      kwargs['reparameterization_type'] = FULLY_REPARAMETERIZED\n    if 'validate_args' not in kwargs:\n      kwargs['validate_args'] = False\n    if 'name' not in kwargs:\n      kwargs['name'] = 'AutoRegressive'\n\n    super(AutoRegressive, self).__init__(*args, **kwargs)\n\n    self._args = (T, a, sig)\n\n  def _log_prob(self, value):\n    err = value - self.a * tf.pad(value[:-1], [[1, 0]], 'CONSTANT')\n    lpdf = self.shocks._log_prob(err)\n    return tf.reduce_sum(lpdf)\n\n  def _sample_n(self, n, seed=None):\n    return tf.scan(lambda acc, x: self.a * acc + x,\n                   self.shocks._sample_n(n, seed))\n\n\nclass test_ar_process(tf.test.TestCase):\n\n  def test_ar_mle(self):\n    # set up test data: a random walk\n    T = 100\n    z_true = np.zeros(T)\n    r = 0.95\n    sig = 0.01\n    eta = 0.01\n    for t in range(1, 100):\n      z_true[t] = r * z_true[t - 1] + sig * np.random.randn()\n\n    x_data = (z_true + eta * np.random.randn(T)).astype(np.float32)\n\n    # use scipy to find max likelihood\n    def cost(z):\n      initial = z[0]**2 / sig**2\n      ar = np.sum((z[1:] - r * z[:-1])**2) / sig**2\n      data = np.sum((x_data - z)**2) / eta**2\n      return initial + ar + data\n\n    mle = minimize(cost, np.zeros(T)).x\n\n    with self.test_session() as sess:\n      z = AutoRegressive(T, r, sig)\n      x = Normal(loc=z, scale=eta)\n\n      qz = PointMass(params=tf.Variable(tf.zeros(T)))\n      inference = ed.MAP({z: qz}, data={x: x_data})\n      inference.run(n_iter=500)\n\n      self.assertAllClose(qz.eval(), mle, rtol=1e-3, atol=1e-3)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/bayesian_nn_test.py,39,"b'""""""Check that inference classes run without error on a Bayesian neural net.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Empirical, Bernoulli, Normal\n\n\ndef neural_network(x, W_1, W_2, W_3, b_1, b_2):\n  h = tf.tanh(tf.matmul(x, W_1) + b_1)\n  h = tf.tanh(tf.matmul(h, W_2) + b_2)\n  h = tf.matmul(h, W_3)\n  return tf.reshape(h, [-1])\n\n\nclass test_inference_bayesian_nn_class(tf.test.TestCase):\n\n  def _test(self):\n    X_train = np.zeros([500, 100])\n    y_train = np.zeros(500)\n\n    N = X_train.shape[0]  # number of data points\n    D = X_train.shape[1]  # number of features\n\n    W_1 = Normal(loc=tf.zeros([D, 20]), scale=tf.ones([D, 20]) * 100)\n    W_2 = Normal(loc=tf.zeros([20, 15]), scale=tf.ones([20, 15]) * 100)\n    W_3 = Normal(loc=tf.zeros([15, 1]), scale=tf.ones([15, 1]) * 100)\n    b_1 = Normal(loc=tf.zeros(20), scale=tf.ones(20) * 100)\n    b_2 = Normal(loc=tf.zeros(15), scale=tf.ones(15) * 100)\n\n    X = tf.placeholder(tf.float32, [N, D])\n    y = Bernoulli(logits=neural_network(X, W_1, W_2, W_3, b_1, b_2))\n    return N, D, W_1, W_2, W_3, b_1, b_2, X, y, X_train, y_train\n\n  def test_gan_inference(self):\n    with self.test_session():\n      N, D, W_1, W_2, W_3, b_1, b_2, X, y, X_train, y_train = self._test()\n\n      with tf.variable_scope(""Gen""):\n        theta = tf.get_variable(""theta"", [1])\n        y = tf.cast(y, tf.float32) * theta\n\n      def discriminator(x):\n        w = tf.get_variable(""w"", [1])\n        return w * tf.cast(x, tf.float32)\n\n      inference = ed.GANInference(\n          data={y: tf.cast(y_train, tf.float32), X: X_train},\n          discriminator=discriminator)\n      inference.run(n_iter=1)\n\n  def test_wgan_inference(self):\n    with self.test_session():\n      N, D, W_1, W_2, W_3, b_1, b_2, X, y, X_train, y_train = self._test()\n\n      with tf.variable_scope(""Gen""):\n        theta = tf.get_variable(""theta"", [1])\n        y = tf.cast(y, tf.float32) * theta\n\n      def discriminator(x):\n        w = tf.get_variable(""w"", [1])\n        return w * tf.cast(x, tf.float32)\n\n      inference = ed.WGANInference(\n          data={y: tf.cast(y_train, tf.float32), X: X_train},\n          discriminator=discriminator)\n      inference.run(n_iter=1)\n\n  def test_hmc(self):\n    with self.test_session():\n      N, D, W_1, W_2, W_3, b_1, b_2, X, y, X_train, y_train = self._test()\n\n      T = 1  # number of MCMC samples\n      qW_1 = Empirical(params=tf.Variable(tf.random_normal([T, D, 20])))\n      qW_2 = Empirical(params=tf.Variable(tf.random_normal([T, 20, 15])))\n      qW_3 = Empirical(params=tf.Variable(tf.random_normal([T, 15, 1])))\n      qb_1 = Empirical(params=tf.Variable(tf.random_normal([T, 20])))\n      qb_2 = Empirical(params=tf.Variable(tf.random_normal([T, 15])))\n\n      inference = ed.HMC(\n          {W_1: qW_1, b_1: qb_1, W_2: qW_2, b_2: qb_2, W_3: qW_3},\n          data={y: y_train, X: X_train})\n      inference.run()\n\n  def test_metropolis_hastings(self):\n    with self.test_session():\n      N, D, W_1, W_2, W_3, b_1, b_2, X, y, X_train, y_train = self._test()\n\n      T = 1  # number of MCMC samples\n      qW_1 = Empirical(params=tf.Variable(tf.random_normal([T, D, 20])))\n      qW_2 = Empirical(params=tf.Variable(tf.random_normal([T, 20, 15])))\n      qW_3 = Empirical(params=tf.Variable(tf.random_normal([T, 15, 1])))\n      qb_1 = Empirical(params=tf.Variable(tf.random_normal([T, 20])))\n      qb_2 = Empirical(params=tf.Variable(tf.random_normal([T, 15])))\n\n      inference = ed.MetropolisHastings(\n          {W_1: qW_1, b_1: qb_1, W_2: qW_2, b_2: qb_2, W_3: qW_3},\n          {W_1: W_1, b_1: b_1, W_2: W_2, b_2: b_2, W_3: W_3},\n          data={y: y_train, X: X_train})\n      inference.run()\n\n  def test_sgld(self):\n    with self.test_session():\n      N, D, W_1, W_2, W_3, b_1, b_2, X, y, X_train, y_train = self._test()\n\n      T = 1  # number of MCMC samples\n      qW_1 = Empirical(params=tf.Variable(tf.random_normal([T, D, 20])))\n      qW_2 = Empirical(params=tf.Variable(tf.random_normal([T, 20, 15])))\n      qW_3 = Empirical(params=tf.Variable(tf.random_normal([T, 15, 1])))\n      qb_1 = Empirical(params=tf.Variable(tf.random_normal([T, 20])))\n      qb_2 = Empirical(params=tf.Variable(tf.random_normal([T, 15])))\n\n      inference = ed.SGLD(\n          {W_1: qW_1, b_1: qb_1, W_2: qW_2, b_2: qb_2, W_3: qW_3},\n          data={y: y_train, X: X_train})\n      inference.run()\n\nif __name__ == \'__main__\':\n  ed.set_seed(42)\n  tf.test.main()\n'"
tests/inferences/conjugacy_test.py,7,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward import models as rvs\n\n\nclass test_conjugacy_class(tf.test.TestCase):\n\n  def test_basic_bernoulli(self):\n    N = 10\n    z = rvs.Bernoulli(probs=0.75, sample_shape=N)\n    z_cond = ed.complete_conditional(z, [z])\n    self.assertIsInstance(z_cond, rvs.Bernoulli)\n\n    with self.test_session() as sess:\n      p_val = sess.run(z_cond.probs)\n\n    self.assertAllClose(p_val, 0.75 + np.zeros(N, np.float32))\n\n  def test_incomplete_blanket(self):\n    N = 10\n    z = rvs.Bernoulli(probs=0.75, sample_shape=N)\n    z_cond = ed.complete_conditional(z, [])\n    self.assertIsInstance(z_cond, rvs.Bernoulli)\n\n    with self.test_session() as sess:\n      p_val = sess.run(z_cond.probs)\n\n    self.assertAllClose(p_val, 0.75 + np.zeros(N, np.float32))\n\n  def test_missing_blanket(self):\n    N = 10\n    z = rvs.Bernoulli(probs=0.75, sample_shape=N)\n    z_cond = ed.complete_conditional(z)\n    self.assertIsInstance(z_cond, rvs.Bernoulli)\n\n    with self.test_session() as sess:\n      p_val = sess.run(z_cond.probs)\n\n    self.assertAllClose(p_val, 0.75 + np.zeros(N, np.float32))\n\n  def test_blanket_changes(self):\n    pi = rvs.Dirichlet(tf.ones(3))\n    mu = rvs.Normal(0.0, 1.0)\n    z = rvs.Categorical(probs=pi)\n\n    pi1_cond = ed.complete_conditional(pi, [z, pi])\n    pi2_cond = ed.complete_conditional(pi, [z, mu, pi])\n\n    self.assertIsInstance(pi1_cond, rvs.Dirichlet)\n    self.assertIsInstance(pi2_cond, rvs.Dirichlet)\n\n    with self.test_session() as sess:\n      conc1_val, conc2_val = sess.run([pi1_cond.concentration,\n                                       pi2_cond.concentration])\n\n    self.assertAllClose(conc1_val, conc2_val)\n\n  def test_beta_bernoulli(self):\n    x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n    a0 = 0.5\n    b0 = 1.5\n    pi = rvs.Beta(a0, b0)\n    x = rvs.Bernoulli(probs=pi, sample_shape=10)\n\n    pi_cond = ed.complete_conditional(pi, [pi, x])\n\n    self.assertIsInstance(pi_cond, rvs.Beta)\n\n    with self.test_session() as sess:\n      a_val, b_val = sess.run([pi_cond.concentration1,\n                               pi_cond.concentration0], {x: x_data})\n\n    self.assertAllClose(a_val, a0 + x_data.sum())\n    self.assertAllClose(b_val, b0 + (1 - x_data).sum())\n\n  def test_beta_binomial(self):\n    n_data = 10\n    x_data = 2\n\n    a0 = 0.5\n    b0 = 1.5\n    pi = rvs.Beta(a0, b0)\n    # use value since cannot sample\n    x = rvs.Binomial(total_count=n_data, probs=pi, value=0.0)\n\n    pi_cond = ed.complete_conditional(pi, [pi, x])\n\n    self.assertIsInstance(pi_cond, rvs.Beta)\n\n    with self.test_session() as sess:\n      a_val, b_val = sess.run([pi_cond.concentration1,\n                               pi_cond.concentration0], {x: x_data})\n\n    self.assertAllClose(a_val, a0 + x_data)\n    self.assertAllClose(b_val, b0 + n_data - x_data)\n\n  def test_gamma_exponential(self):\n    x_data = np.array([0.1, 0.5, 3.3, 2.7])\n\n    alpha0 = 0.5\n    beta0 = 1.75\n    lam = rvs.Gamma(alpha0, beta0)\n    x = rvs.Exponential(lam, sample_shape=4)\n\n    lam_cond = ed.complete_conditional(lam, [lam, x])\n\n    self.assertIsInstance(lam_cond, rvs.Gamma)\n\n    with self.test_session() as sess:\n      alpha_val, beta_val = sess.run(\n          [lam_cond.concentration, lam_cond.rate], {x: x_data})\n\n    self.assertAllClose(alpha_val, alpha0 + len(x_data))\n    self.assertAllClose(beta_val, beta0 + x_data.sum())\n\n  def test_gamma_poisson(self):\n    x_data = np.array([0, 1, 0, 7, 0, 0, 2, 0, 0, 1])\n\n    alpha0 = 0.5\n    beta0 = 1.75\n    lam = rvs.Gamma(alpha0, beta0)\n    # use value since cannot sample\n    x = rvs.Poisson(lam, value=tf.zeros(10), sample_shape=10)\n\n    lam_cond = ed.complete_conditional(lam, [lam, x])\n\n    self.assertIsInstance(lam_cond, rvs.Gamma)\n\n    with self.test_session() as sess:\n      alpha_val, beta_val = sess.run(\n          [lam_cond.concentration, lam_cond.rate], {x: x_data})\n\n    self.assertAllClose(alpha_val, alpha0 + x_data.sum())\n    self.assertAllClose(beta_val, beta0 + len(x_data))\n\n  def test_gamma_gamma(self):\n    x_data = np.array([0.1, 0.5, 3.3, 2.7])\n\n    alpha0 = 0.5\n    beta0 = 1.75\n    alpha_likelihood = 2.3\n    beta = rvs.Gamma(alpha0, beta0)\n    x = rvs.Gamma(alpha_likelihood, beta, sample_shape=4)\n\n    beta_cond = ed.complete_conditional(beta, [beta, x])\n\n    self.assertIsInstance(beta_cond, rvs.Gamma)\n\n    with self.test_session() as sess:\n      alpha_val, beta_val = sess.run(\n          [beta_cond.concentration, beta_cond.rate], {x: x_data})\n    self.assertAllClose(alpha_val, alpha0 + alpha_likelihood * len(x_data))\n    self.assertAllClose(beta_val, beta0 + x_data.sum())\n\n  def test_mul_rate_gamma(self):\n    x_data = np.array([0.1, 0.5, 3.3, 2.7])\n\n    alpha0 = 0.5\n    beta0 = 1.75\n    alpha_likelihood = 2.3\n    beta = rvs.Gamma(alpha0, beta0)\n    x = rvs.Gamma(alpha_likelihood, alpha_likelihood * beta, sample_shape=4)\n\n    beta_cond = ed.complete_conditional(beta, [beta, x])\n\n    self.assertIsInstance(beta_cond, rvs.Gamma)\n\n    with self.test_session() as sess:\n      alpha_val, beta_val = sess.run([beta_cond.concentration, beta_cond.rate],\n                                     {x: x_data})\n    self.assertAllClose(alpha_val, alpha0 + alpha_likelihood * len(x_data))\n    self.assertAllClose(beta_val, beta0 + alpha_likelihood * x_data.sum())\n\n  def test_normal_normal(self):\n    x_data = np.array([0.1, 0.5, 3.3, 2.7])\n\n    mu0 = 0.3\n    sigma0 = 2.1\n    sigma_likelihood = 1.2\n\n    mu = rvs.Normal(mu0, sigma0)\n    x = rvs.Normal(mu, sigma_likelihood, sample_shape=len(x_data))\n\n    mu_cond = ed.complete_conditional(mu, [mu, x])\n    self.assertIsInstance(mu_cond, rvs.Normal)\n\n    with self.test_session() as sess:\n      mu_val, sigma_val = sess.run([mu_cond.loc, mu_cond.scale], {x: x_data})\n\n    self.assertAllClose(sigma_val, (1.0 / sigma0**2 +\n                                    len(x_data) / sigma_likelihood**2) ** -0.5)\n    self.assertAllClose(mu_val,\n                        sigma_val**2 * (mu0 / sigma0**2 +\n                                        (1.0 / sigma_likelihood**2 *\n                                         x_data.sum())))\n\n  def test_inverse_gamma_normal(self):\n    x_data = np.array([0.1, 0.5, 3.3, 2.7])\n\n    sigmasq_conc = 1.3\n    sigmasq_rate = 2.1\n    x_loc = 0.3\n\n    sigmasq = rvs.InverseGamma(sigmasq_conc, sigmasq_rate)\n    x = rvs.Normal(x_loc, tf.sqrt(sigmasq), sample_shape=len(x_data))\n\n    sigmasq_cond = ed.complete_conditional(sigmasq, [sigmasq, x])\n    self.assertIsInstance(sigmasq_cond, rvs.InverseGamma)\n\n    with self.test_session() as sess:\n      conc_val, rate_val = sess.run(\n          [sigmasq_cond.concentration, sigmasq_cond.rate], {x: x_data})\n\n    self.assertAllClose(conc_val, sigmasq_conc + 0.5 * len(x_data))\n    self.assertAllClose(rate_val,\n                        sigmasq_rate + 0.5 * np.sum((x_data - x_loc)**2))\n\n  def test_normal_normal_scaled(self):\n    x_data = np.array([0.1, 0.5, 3.3, 2.7])\n\n    mu0 = 0.3\n    sigma0 = 2.1\n    sigma_likelihood = 1.2\n    c = 2.0\n\n    mu = rvs.Normal(mu0, sigma0)\n    x = rvs.Normal(c * mu, sigma_likelihood, sample_shape=len(x_data))\n\n    mu_cond = ed.complete_conditional(mu, [mu, x])\n    self.assertIsInstance(mu_cond, rvs.Normal)\n\n    with self.test_session() as sess:\n      mu_val, sigma_val = sess.run([mu_cond.loc, mu_cond.scale], {x: x_data})\n\n    self.assertAllClose(sigma_val,\n                        (1.0 / sigma0**2 +\n                         c**2 * len(x_data) / sigma_likelihood**2) ** -0.5)\n    self.assertAllClose(mu_val,\n                        sigma_val**2 * (mu0 / sigma0**2 +\n                                        (c / sigma_likelihood**2 *\n                                         x_data.sum())))\n\n  def test_dirichlet_categorical(self):\n    x_data = np.array([0, 0, 0, 0, 1, 1, 1, 2, 2, 3], np.int32)\n    N = x_data.shape[0]\n    D = x_data.max() + 1\n\n    alpha = np.zeros(D).astype(np.float32) + 2.0\n\n    theta = rvs.Dirichlet(alpha)\n    x = rvs.Categorical(probs=theta, sample_shape=N)\n\n    theta_cond = ed.complete_conditional(theta, [theta, x])\n\n    with self.test_session() as sess:\n      alpha_val = sess.run(theta_cond.concentration, {x: x_data})\n\n    self.assertAllClose(alpha_val, np.array([6.0, 5.0, 4.0, 3.0], np.float32))\n\n  def test_dirichlet_multinomial(self):\n    x_data = np.array([4, 3, 2, 1], np.int32)\n    N = x_data.sum()\n    D = x_data.shape[0]\n\n    alpha = np.zeros(D).astype(np.float32) + 2.0\n\n    theta = rvs.Dirichlet(alpha)\n    x = rvs.Multinomial(total_count=tf.cast(N, tf.float32), probs=theta)\n\n    theta_cond = ed.complete_conditional(theta, [theta, x])\n\n    with self.test_session() as sess:\n      alpha_val = sess.run(theta_cond.concentration, {x: x_data})\n\n    self.assertAllClose(alpha_val, np.array([6.0, 5.0, 4.0, 3.0], np.float32))\n\n  def test_mog(self):\n    x_val = np.array([1.1, 1.2, 2.1, 4.4, 5.5, 7.3, 6.8], np.float32)\n    z_val = np.array([0, 0, 0, 1, 1, 2, 2], np.int32)\n    pi_val = np.array([0.2, 0.3, 0.5], np.float32)\n    mu_val = np.array([1.0, 5.0, 7.0], np.float32)\n\n    N = x_val.shape[0]\n    K = z_val.max() + 1\n\n    pi_alpha = 1.3 + np.zeros(K, dtype=np.float32)\n    mu_sigma = 4.0\n    sigmasq = 2.0**2\n\n    pi = rvs.Dirichlet(pi_alpha)\n    mu = rvs.Normal(0.0, mu_sigma, sample_shape=[K])\n\n    x = rvs.ParamMixture(pi, {'loc': mu, 'scale': tf.sqrt(sigmasq)},\n                         rvs.Normal, sample_shape=N)\n    z = x.cat\n\n    mu_cond = ed.complete_conditional(mu)\n    pi_cond = ed.complete_conditional(pi)\n    z_cond = ed.complete_conditional(z)\n\n    with self.test_session() as sess:\n      pi_cond_alpha, mu_cond_mu, mu_cond_sigma, z_cond_p = (\n          sess.run([pi_cond.concentration, mu_cond.loc,\n                    mu_cond.scale, z_cond.probs],\n                   {z: z_val, x: x_val, pi: pi_val, mu: mu_val}))\n\n    true_pi = pi_alpha + np.unique(z_val, return_counts=True)[1]\n    self.assertAllClose(pi_cond_alpha, true_pi)\n    for k in range(K):\n      sigmasq_true = (1.0 / 4**2 + 1.0 / sigmasq * (z_val == k).sum())**-1\n      mu_true = sigmasq_true * (1.0 / sigmasq * x_val[z_val == k].sum())\n      self.assertAllClose(np.sqrt(sigmasq_true), mu_cond_sigma[k])\n      self.assertAllClose(mu_true, mu_cond_mu[k])\n    true_log_p_z = np.log(pi_val) - 0.5 / sigmasq * (x_val[:, np.newaxis] -\n                                                     mu_val)**2\n    true_log_p_z -= true_log_p_z.max(1, keepdims=True)\n    true_p_z = np.exp(true_log_p_z)\n    true_p_z /= true_p_z.sum(1, keepdims=True)\n    self.assertAllClose(z_cond_p, true_p_z)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/inferences/gan_inference_test.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\nfrom tensorflow.contrib import slim\n\n\ndef next_batch(M):\n  samples = np.random.normal(4.0, 0.1, M)\n  samples.sort()\n  return samples\n\n\ndef discriminative_network(x):\n  """"""Outputs probability in logits.""""""\n  h0 = slim.fully_connected(x, 10, activation_fn=tf.nn.relu)\n  return slim.fully_connected(h0, 1, activation_fn=None)\n\n\nclass test_gan_class(tf.test.TestCase):\n\n  def test_normal(self):\n    with self.test_session() as sess:\n      # DATA\n      M = 12  # batch size during training\n      x_ph = tf.placeholder(tf.float32, [M, 1])\n\n      # MODEL\n      with tf.variable_scope(""Gen""):\n        theta = tf.Variable(0.0)\n        x = Normal(theta, 0.1, sample_shape=[M, 1])\n\n      # INFERENCE\n      inference = ed.GANInference(\n          data={x: x_ph}, discriminator=discriminative_network)\n      inference.initialize(n_iter=1000)\n      tf.global_variables_initializer().run()\n\n      for _ in range(inference.n_iter):\n        x_data = next_batch(M).reshape([M, 1])\n        inference.update(feed_dict={x_ph: x_data})\n\n      # CRITICISM\n      self.assertAllClose(theta.eval(), 4.0, rtol=1.0, atol=1.0)\n\nif __name__ == \'__main__\':\n  ed.set_seed(54432132)\n  tf.test.main()\n'"
tests/inferences/gibbs_test.py,13,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Beta, Empirical, Normal\n\n\nclass test_gibbs_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, default, dtype):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=tf.constant(0.0, dtype=dtype),\n                  scale=tf.constant(1.0, dtype=dtype))\n      x = Normal(loc=mu, scale=tf.constant(1.0, dtype=dtype),\n                 sample_shape=50)\n\n      n_samples = 2000\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      if not default:\n        qmu = Empirical(params=tf.Variable(tf.ones(n_samples, dtype=dtype)))\n        inference = ed.Gibbs({mu: qmu}, data={x: x_data})\n      else:\n        inference = ed.Gibbs([mu], data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run()\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def test_beta_bernoulli(self):\n    with self.test_session() as sess:\n      x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n      p = Beta(1.0, 1.0)\n      x = Bernoulli(probs=p, sample_shape=10)\n\n      qp = Empirical(tf.Variable(tf.zeros(1000)))\n      inference = ed.Gibbs({p: qp}, data={x: x_data})\n      inference.run()\n\n      true_posterior = Beta(3.0, 9.0)\n\n      val_est, val_true = sess.run([qp.mean(), true_posterior.mean()])\n      self.assertAllClose(val_est, val_true, rtol=1e-2, atol=1e-2)\n\n      val_est, val_true = sess.run([qp.variance(), true_posterior.variance()])\n      self.assertAllClose(val_est, val_true, rtol=1e-2, atol=1e-2)\n\n  def test_normal_normal(self):\n    self._test_normal_normal(True, tf.float32)\n    self._test_normal_normal(False, tf.float32)\n    self._test_normal_normal(True, tf.float64)\n    self._test_normal_normal(False, tf.float64)\n\n  def test_data_tensor(self):\n    with self.test_session() as sess:\n      x_data = tf.zeros(50)\n\n      mu = Normal(0.0, 1.0)\n      x = Normal(mu, 1.0, sample_shape=50)\n\n      qmu = Empirical(tf.Variable(tf.ones(1000)))\n\n      # analytic solution: N(mu=0.0, sigma=\\sqrt{1/51}=0.140)\n      inference = ed.Gibbs({mu: qmu}, data={x: x_data})\n      inference.run()\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-2, atol=1e-2)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-2, atol=1e-2)\n\nif __name__ == '__main__':\n  ed.set_seed(127832)\n  tf.test.main()\n"""
tests/inferences/hmc_test.py,26,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Categorical, Empirical, Normal\n\n\nclass test_hmc_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, default, dtype):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=tf.constant(0.0, dtype=dtype),\n                  scale=tf.constant(1.0, dtype=dtype))\n      x = Normal(loc=mu, scale=tf.constant(1.0, dtype=dtype),\n                 sample_shape=50)\n\n      n_samples = 2000\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      if not default:\n        qmu = Empirical(params=tf.Variable(tf.ones(n_samples, dtype=dtype)))\n        inference = ed.HMC({mu: qmu}, data={x: x_data})\n      else:\n        inference = ed.HMC([mu], data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run()\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def _test_linear_regression(self, default, dtype):\n    def build_toy_dataset(N, w, noise_std=0.1):\n      D = len(w)\n      x = np.random.randn(N, D)\n      y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n      return x, y\n\n    with self.test_session() as sess:\n      N = 40  # number of data points\n      D = 10  # number of features\n\n      w_true = np.random.randn(D)\n      X_train, y_train = build_toy_dataset(N, w_true)\n      X_test, y_test = build_toy_dataset(N, w_true)\n\n      X = tf.placeholder(dtype, [N, D])\n      w = Normal(loc=tf.zeros(D, dtype=dtype), scale=tf.ones(D, dtype=dtype))\n      b = Normal(loc=tf.zeros(1, dtype=dtype), scale=tf.ones(1, dtype=dtype))\n      y = Normal(loc=ed.dot(X, w) + b, scale=0.1 * tf.ones(N, dtype=dtype))\n\n      n_samples = 2000\n      if not default:\n        qw = Empirical(tf.Variable(tf.zeros([n_samples, D], dtype=dtype)))\n        qb = Empirical(tf.Variable(tf.zeros([n_samples, 1], dtype=dtype)))\n        inference = ed.HMC({w: qw, b: qb}, data={X: X_train, y: y_train})\n      else:\n        inference = ed.HMC([w, b], data={X: X_train, y: y_train})\n        qw = inference.latent_vars[w]\n        qb = inference.latent_vars[b]\n      inference.run(step_size=0.01)\n\n      self.assertAllClose(qw.mean().eval(), w_true, rtol=5e-1, atol=5e-1)\n      self.assertAllClose(qb.mean().eval(), [0.0], rtol=5e-1, atol=5e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def test_normal_normal(self):\n    self._test_normal_normal(True, tf.float32)\n    self._test_normal_normal(False, tf.float32)\n    self._test_normal_normal(True, tf.float64)\n    self._test_normal_normal(False, tf.float64)\n\n  def test_linear_regression(self):\n    self._test_linear_regression(True, tf.float32)\n    self._test_linear_regression(False, tf.float32)\n    self._test_linear_regression(True, tf.float64)\n    self._test_linear_regression(False, tf.float64)\n\n  def test_indexedslices(self):\n    """"""Test that gradients accumulate when tf.gradients doesn\'t return\n    tf.Tensor (IndexedSlices).""""""\n    with self.test_session() as sess:\n      N = 10  # number of data points\n      K = 2  # number of clusters\n      T = 1  # number of MCMC samples\n\n      x_data = np.zeros(N, dtype=np.float32)\n\n      mu = Normal(0.0, 1.0, sample_shape=K)\n      c = Categorical(logits=tf.zeros(N))\n      x = Normal(tf.gather(mu, c), tf.ones(N))\n\n      qmu = Empirical(params=tf.Variable(tf.ones([T, K])))\n      qc = Empirical(params=tf.Variable(tf.ones([T, N])))\n\n      inference = ed.HMC({mu: qmu}, data={x: x_data})\n      inference.initialize()\n\nif __name__ == \'__main__\':\n  ed.set_seed(42)\n  tf.test.main()\n'"
tests/inferences/implicitklqp_test.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_implicit_klqp_class(tf.test.TestCase):\n\n  def test_normal_run(self):\n    def ratio_estimator(data, local_vars, global_vars):\n      """"""Use the optimal ratio estimator, r(z) = log p(z). We add a\n      TensorFlow variable as the algorithm assumes that the function\n      has parameters to optimize.""""""\n      w = tf.get_variable(""w"", [])\n      return z.log_prob(local_vars[z]) + w\n\n    with self.test_session() as sess:\n      z = Normal(loc=5.0, scale=1.0)\n\n      qz = Normal(loc=tf.Variable(tf.random_normal([])),\n                  scale=tf.nn.softplus(tf.Variable(tf.random_normal([]))))\n\n      inference = ed.ImplicitKLqp({z: qz}, discriminator=ratio_estimator)\n      inference.run(n_iter=200)\n\n      self.assertAllClose(qz.mean().eval(), 5.0, atol=1.0)\n\nif __name__ == \'__main__\':\n  ed.set_seed(47324)\n  tf.test.main()\n'"
tests/inferences/inference_auto_transform_test.py,33,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import (Empirical, Gamma, Normal, PointMass,\n                           TransformedDistribution, Beta, Bernoulli)\nfrom edward.util import transform\nfrom tensorflow.contrib.distributions import bijectors\n\n\nclass test_inference_auto_transform_class(tf.test.TestCase):\n\n  def test_auto_transform_true(self):\n    with self.test_session() as sess:\n      # Match normal || softplus-inverse-normal distribution with\n      # automated transformation on latter (assuming it is softplus).\n      x = TransformedDistribution(\n          distribution=Normal(0.0, 0.5),\n          bijector=tf.contrib.distributions.bijectors.Softplus())\n      x.support = \'nonnegative\'\n      qx = Normal(loc=tf.Variable(tf.random_normal([])),\n                  scale=tf.nn.softplus(tf.Variable(tf.random_normal([]))))\n\n      inference = ed.KLqp({x: qx})\n      inference.initialize(auto_transform=True, n_samples=5, n_iter=1000)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n\n      # Check approximation on constrained space has same moments as\n      # target distribution.\n      n_samples = 10000\n      x_mean, x_var = tf.nn.moments(x.sample(n_samples), 0)\n      x_unconstrained = inference.transformations[x]\n      qx_constrained = transform(qx, bijectors.Invert(x_unconstrained.bijector))\n      qx_mean, qx_var = tf.nn.moments(qx_constrained.sample(n_samples), 0)\n      stats = sess.run([x_mean, qx_mean, x_var, qx_var])\n      self.assertAllClose(info_dict[\'loss\'], 0.0, rtol=0.2, atol=0.2)\n      self.assertAllClose(stats[0], stats[1], rtol=1e-1, atol=1e-1)\n      self.assertAllClose(stats[2], stats[3], rtol=1e-1, atol=1e-1)\n\n  def test_auto_transform_false(self):\n    with self.test_session():\n      # Match normal || softplus-inverse-normal distribution without\n      # automated transformation; it should fail.\n      x = TransformedDistribution(\n          distribution=Normal(0.0, 0.5),\n          bijector=tf.contrib.distributions.bijectors.Softplus())\n      x.support = \'nonnegative\'\n      qx = Normal(loc=tf.Variable(tf.random_normal([])),\n                  scale=tf.nn.softplus(tf.Variable(tf.random_normal([]))))\n\n      inference = ed.KLqp({x: qx})\n      inference.initialize(auto_transform=False, n_samples=5, n_iter=150)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n\n      self.assertAllEqual(info_dict[\'loss\'], np.nan)\n\n  def test_map_custom(self):\n    with self.test_session() as sess:\n      x = Gamma(2.0, 0.5)\n      qx = PointMass(tf.nn.softplus(tf.Variable(0.5)))\n\n      inference = ed.MAP({x: qx})\n      inference.initialize(auto_transform=True, n_iter=500)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n\n      # Check approximation on constrained space has same mode as\n      # target distribution.\n      stats = sess.run([x.mode(), qx])\n      self.assertAllClose(stats[0], stats[1], rtol=1e-5, atol=1e-5)\n\n  def test_map_default(self):\n    with self.test_session() as sess:\n      x = Gamma(2.0, 0.5)\n\n      inference = ed.MAP([x])\n      inference.initialize(auto_transform=True, n_iter=500)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n\n      # Check approximation on constrained space has same mode as\n      # target distribution.\n      qx = inference.latent_vars[x]\n      stats = sess.run([x.mode(), qx])\n      self.assertAllClose(stats[0], stats[1], rtol=1e-5, atol=1e-5)\n\n  def test_laplace_default(self):\n    with self.test_session() as sess:\n      x = Gamma([2.0], [0.5])\n\n      inference = ed.Laplace([x])\n      optimizer = tf.train.AdamOptimizer(0.2)\n      inference.initialize(auto_transform=True, n_iter=500)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n\n      # Check approximation on constrained space has same mode as\n      # target distribution.\n      qx = inference.latent_vars[x]\n      stats = sess.run([x.mode(), qx.mean()])\n      self.assertAllClose(stats[0], stats[1], rtol=1e-5, atol=1e-5)\n\n  def test_hmc_custom(self):\n    with self.test_session() as sess:\n      x = TransformedDistribution(\n          distribution=Normal(1.0, 1.0),\n          bijector=tf.contrib.distributions.bijectors.Softplus())\n      x.support = \'nonnegative\'\n      qx = Empirical(tf.Variable(tf.random_normal([1000])))\n\n      inference = ed.HMC({x: qx})\n      inference.initialize(auto_transform=True, step_size=0.8)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n\n      # Check approximation on constrained space has same moments as\n      # target distribution.\n      n_samples = 10000\n      x_unconstrained = inference.transformations[x]\n      qx_constrained_params = x_unconstrained.bijector.inverse(qx.params)\n      x_mean, x_var = tf.nn.moments(x.sample(n_samples), 0)\n      qx_mean, qx_var = tf.nn.moments(qx_constrained_params[500:], 0)\n      stats = sess.run([x_mean, qx_mean, x_var, qx_var])\n      self.assertAllClose(stats[0], stats[1], rtol=1e-1, atol=1e-1)\n      self.assertAllClose(stats[2], stats[3], rtol=1e-1, atol=1e-1)\n\n  def test_hmc_default(self):\n    with self.test_session() as sess:\n      x = TransformedDistribution(\n          distribution=Normal(1.0, 1.0),\n          bijector=tf.contrib.distributions.bijectors.Softplus())\n      x.support = \'nonnegative\'\n\n      inference = ed.HMC([x])\n      inference.initialize(auto_transform=True, step_size=0.8)\n      tf.global_variables_initializer().run()\n      for _ in range(inference.n_iter):\n        info_dict = inference.update()\n        inference.print_progress(info_dict)\n\n      # Check approximation on constrained space has same moments as\n      # target distribution.\n      n_samples = 1000\n      qx_constrained = inference.latent_vars[x]\n      x_mean, x_var = tf.nn.moments(x.sample(n_samples), 0)\n      qx_mean, qx_var = tf.nn.moments(qx_constrained.params[500:], 0)\n      stats = sess.run([x_mean, qx_mean, x_var, qx_var])\n      self.assertAllClose(stats[0], stats[1], rtol=1e-1, atol=1e-1)\n      self.assertAllClose(stats[2], stats[3], rtol=1e-1, atol=1e-1)\n\n  def test_hmc_betabernoulli(self):\n    """"""Do we correctly handle dependencies of transformed variables?""""""\n\n    with self.test_session() as sess:\n      # model\n      z = Beta(1., 1., name=""z"")\n      xs = Bernoulli(probs=z, sample_shape=10)\n      x_obs = np.asarray([0, 0, 1, 1, 0, 0, 0, 0, 0, 1], dtype=np.int32)\n\n      # inference\n      qz_samples = tf.Variable(tf.random_uniform(shape=(1000,)))\n      qz = ed.models.Empirical(params=qz_samples, name=""z_posterior"")\n      inference_hmc = ed.inferences.HMC({z: qz}, data={xs: x_obs})\n      inference_hmc.run(step_size=1.0, n_steps=5, auto_transform=True)\n\n      # check that inferred posterior mean/variance is close to\n      # that of the exact Beta posterior\n      z_unconstrained = inference_hmc.transformations[z]\n      qz_constrained = z_unconstrained.bijector.inverse(qz_samples)\n      qz_mean, qz_var = sess.run(tf.nn.moments(qz_constrained, 0))\n\n      true_posterior = Beta(1. + np.sum(x_obs), 1. + np.sum(1 - x_obs))\n      pz_mean, pz_var = sess.run((true_posterior.mean(),\n                                  true_posterior.variance()))\n      self.assertAllClose(qz_mean, pz_mean, rtol=5e-2, atol=5e-2)\n      self.assertAllClose(qz_var, pz_var, rtol=1e-2, atol=1e-2)\n\n  def test_klqp_betabernoulli(self):\n    with self.test_session() as sess:\n      # model\n      z = Beta(1., 1., name=""z"")\n      xs = Bernoulli(probs=z, sample_shape=10)\n      x_obs = np.asarray([0, 0, 1, 1, 0, 0, 0, 0, 0, 1], dtype=np.int32)\n\n      # inference\n      qz_mean = tf.get_variable(""qz_mean"",\n                                initializer=tf.random_normal(()))\n      qz_std = tf.nn.softplus(tf.get_variable(name=""qz_prestd"",\n                                              initializer=tf.random_normal(())))\n      qz_unconstrained = ed.models.Normal(\n          loc=qz_mean, scale=qz_std, name=""z_posterior"")\n\n      inference_klqp = ed.inferences.KLqp(\n          {z: qz_unconstrained}, data={xs: x_obs})\n      inference_klqp.run(n_iter=500, auto_transform=True)\n\n      z_unconstrained = inference_klqp.transformations[z]\n      qz_constrained = z_unconstrained.bijector.inverse(\n          qz_unconstrained.sample(1000))\n      qz_mean, qz_var = sess.run(tf.nn.moments(qz_constrained, 0))\n\n      true_posterior = Beta(np.sum(x_obs) + 1., np.sum(1 - x_obs) + 1.)\n      pz_mean, pz_var = sess.run((true_posterior.mean(),\n                                  true_posterior.variance()))\n      self.assertAllClose(qz_mean, pz_mean, rtol=5e-2, atol=5e-2)\n      self.assertAllClose(qz_var, pz_var, rtol=1e-2, atol=1e-2)\n\nif __name__ == \'__main__\':\n  ed.set_seed(124125)\n  tf.test.main()\n'"
tests/inferences/inference_data_test.py,19,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_inference_data_class(tf.test.TestCase):\n\n  def test_preloaded_full(self):\n    with self.test_session() as sess:\n      x_data = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones(10) * mu, scale=tf.ones(1))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      inference = ed.KLqp({mu: qmu}, data={x: x_data})\n      inference.initialize()\n      tf.global_variables_initializer().run()\n\n      val = sess.run(inference.data[x])\n      self.assertAllEqual(val, x_data)\n\n  def test_feeding(self):\n    with self.test_session() as sess:\n      x_data = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n      x_ph = tf.placeholder(tf.float32, [10])\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones(10) * mu, scale=tf.ones(10))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      inference = ed.KLqp({mu: qmu}, data={x: x_ph})\n      inference.initialize()\n      tf.global_variables_initializer().run()\n\n      val = sess.run(  # avoid directly fetching placeholder\n          tf.identity(list(six.itervalues(inference.data))[0]),\n          feed_dict={inference.data[x]: x_data})\n      self.assertAllEqual(val, x_data)\n\n  def test_read_file(self):\n    with self.test_session() as sess:\n      # Construct a queue containing a list of filenames.\n      filename_queue = tf.train.string_input_producer(\n          [""tests/data/toy_data.tfrecords""])\n      # Read a single serialized example from a filename.\n      # `serialized_example` is a Tensor of type str.\n      reader = tf.TFRecordReader()\n      _, serialized_example = reader.read(filename_queue)\n      # Convert serialized example back to actual values,\n      # describing format of the objects to be returned.\n      features = tf.parse_single_example(\n          serialized_example,\n          features={\'outcome\': tf.FixedLenFeature([], tf.float32)})\n      x_batch = features[\'outcome\']\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones([]) * mu, scale=tf.ones([]))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      inference = ed.KLqp({mu: qmu}, data={x: x_batch})\n      inference.initialize(scale={x: 10.0})\n\n      tf.global_variables_initializer().run()\n\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(coord=coord)\n\n      # Check data varies by session run.\n      val = sess.run(inference.data[x])\n      val_1 = sess.run(inference.data[x])\n      self.assertNotEqual(val, val_1)\n\n      coord.request_stop()\n      coord.join(threads)\n\nif __name__ == \'__main__\':\n  ed.set_seed(1512351)\n  tf.test.main()\n'"
tests/inferences/inference_debug_test.py,9,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_inference_debug_class(tf.test.TestCase):\n\n  def test_placeholder(self):\n    with self.test_session():\n      N = 5\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones(N) * mu, scale=tf.ones(N))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      x_ph = tf.placeholder(tf.float32, [N])\n      inference = ed.KLqp({mu: qmu}, data={x: x_ph})\n      inference.initialize(debug=True)\n      tf.global_variables_initializer().run()\n      inference.update(feed_dict={x_ph: np.zeros(N, np.float32)})\n\n  def test_tensor(self):\n    with self.test_session():\n      N = 5\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones(N) * mu, scale=tf.ones(N))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      x_data = tf.zeros(N)\n      inference = ed.KLqp({mu: qmu}, data={x: x_data})\n      inference.run(n_iter=1, debug=True)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/inferences/inference_integer_test.py,16,"b'""""""Test that integer variables are handled properly during initialization.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, Categorical\n\n\ndef neural_network(x, W_0, W_1, b_0, b_1):\n  h = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n  h = tf.nn.relu(tf.matmul(h, W_1) + b_1)\n  return h\n\n\nclass test_integer_init(tf.test.TestCase):\n\n  def test(self):\n    with self.test_session():\n      X_train = np.zeros([100, 10], dtype=np.float32)\n      y_train = np.zeros(100, dtype=np.int32)\n\n      N, D = X_train.shape\n      H = 10  # number of hidden units\n      K = 10  # number of classes\n\n      W_0 = Normal(loc=tf.zeros([D, H]), scale=tf.ones([D, H]))\n      W_1 = Normal(loc=tf.zeros([H, K]), scale=tf.ones([H, K]))\n      b_0 = Normal(loc=tf.zeros(H), scale=tf.ones(H))\n      b_1 = Normal(loc=tf.zeros(K), scale=tf.ones(K))\n\n      y = Categorical(logits=neural_network(X_train, W_0, W_1, b_0, b_1))\n\n      qW_0 = Normal(\n          loc=tf.Variable(tf.random_normal([D, H])),\n          scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, H]))))\n      qW_1 = Normal(\n          loc=tf.Variable(tf.random_normal([H, K])),\n          scale=tf.nn.softplus(tf.Variable(tf.random_normal([H, K]))))\n      qb_0 = Normal(\n          loc=tf.Variable(tf.random_normal([H])),\n          scale=tf.nn.softplus(tf.Variable(tf.random_normal([H]))))\n      qb_1 = Normal(\n          loc=tf.Variable(tf.random_normal([K])),\n          scale=tf.nn.softplus(tf.Variable(tf.random_normal([K]))))\n\n      inference = ed.KLqp({W_0: qW_0, b_0: qb_0, W_1: qW_1, b_1: qb_1},\n                          data={y: y_train})\n      inference.run(n_iter=1)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/inferences/inference_reset_test.py,5,"b'""""""Test that reset op works.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_inference_reset_class(tf.test.TestCase):\n\n  def test(self):\n    with self.test_session() as sess:\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=5)\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      inference = ed.KLqp({mu: qmu}, data={x: tf.zeros(5)})\n      inference.initialize()\n      tf.global_variables_initializer().run()\n\n      first = sess.run(inference.t)\n      inference.update()\n      second = sess.run(inference.t)\n      self.assertEqual(first, second - 1)\n      sess.run(inference.reset)\n      third = sess.run(inference.t)\n      self.assertEqual(first, third)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/inferences/inference_scale_test.py,9,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_inference_scale_class(tf.test.TestCase):\n\n  def test_scale_0d(self):\n    with self.test_session():\n      N = 10\n      M = 5\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones(M) * mu, scale=tf.ones(M))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      x_ph = tf.placeholder(tf.float32, [M])\n      inference = ed.KLqp({mu: qmu}, data={x: x_ph})\n      inference.initialize(scale={x: float(N) / M})\n      self.assertAllEqual(inference.scale[x], float(N) / M)\n\n  def test_scale_1d(self):\n    with self.test_session():\n      N = 10\n      M = 5\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=tf.ones(M) * mu, scale=tf.ones(M))\n\n      qmu = Normal(loc=tf.Variable(0.0), scale=tf.constant(1.0))\n\n      x_ph = tf.placeholder(tf.float32, [M])\n      inference = ed.KLqp({mu: qmu}, data={x: x_ph})\n      inference.initialize(scale={x: tf.range(M, dtype=tf.float32)})\n      self.assertAllEqual(inference.scale[x].eval(), np.arange(M))\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/inferences/klpq_test.py,9,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\n\n\nclass test_klpq_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, Inference, default, *args, **kwargs):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      qmu_loc = tf.Variable(tf.random_normal([]))\n      qmu_scale = tf.nn.softplus(tf.Variable(tf.random_normal([])))\n      qmu = Normal(loc=qmu_loc, scale=qmu_scale)\n\n      if not default:\n        qmu_loc = tf.Variable(tf.random_normal([]))\n        qmu_scale = tf.nn.softplus(tf.Variable(tf.random_normal([])))\n        qmu = Normal(loc=qmu_loc, scale=qmu_scale)\n\n        # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n        inference = Inference({mu: qmu}, data={x: x_data})\n      else:\n        inference = Inference([mu], data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run(*args, **kwargs)\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES, scope=\'optimizer\')\n      old_t, old_variables = sess.run([inference.t, variables])\n      self.assertEqual(old_t, inference.n_iter)\n      sess.run(inference.reset)\n      new_t, new_variables = sess.run([inference.t, variables])\n      self.assertEqual(new_t, 0)\n      self.assertNotEqual(old_variables, new_variables)\n\n  def _test_model_parameter(self, Inference, *args, **kwargs):\n    with self.test_session() as sess:\n      x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n      p = tf.sigmoid(tf.Variable(0.5))\n      x = Bernoulli(probs=p, sample_shape=10)\n\n      inference = Inference({}, data={x: x_data})\n      inference.run(*args, **kwargs)\n\n      self.assertAllClose(p.eval(), 0.2, rtol=5e-2, atol=5e-2)\n\n  def test_klpq(self):\n    self._test_normal_normal(ed.KLpq, default=False, n_samples=25, n_iter=100)\n    self._test_normal_normal(ed.KLpq, default=True, n_samples=25, n_iter=100)\n    self._test_model_parameter(ed.KLpq, n_iter=50)\n\n  def test_klpq_nsamples_check(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 ""n_samples should be greater than zero: 0""):\n      self._test_normal_normal(ed.KLpq, default=True, n_samples=0, n_iter=10)\n\nif __name__ == \'__main__\':\n  ed.set_seed(42)\n  tf.test.main()\n'"
tests/inferences/klqp_test.py,7,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\n\n\nclass test_klqp_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, Inference, default, *args, **kwargs):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      if not default:\n        qmu_loc = tf.Variable(tf.random_normal([]))\n        qmu_scale = tf.nn.softplus(tf.Variable(tf.random_normal([])))\n        qmu = Normal(loc=qmu_loc, scale=qmu_scale)\n\n        # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n        inference = Inference({mu: qmu}, data={x: x_data})\n      else:\n        inference = Inference([mu], data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run(*args, **kwargs)\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=0.1, atol=0.6)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=0.15, atol=0.5)\n\n      variables = tf.get_collection(\n          tf.GraphKeys.GLOBAL_VARIABLES, scope=\'optimizer\')\n      old_t, old_variables = sess.run([inference.t, variables])\n      self.assertEqual(old_t, inference.n_iter)\n      sess.run(inference.reset)\n      new_t, new_variables = sess.run([inference.t, variables])\n      self.assertEqual(new_t, 0)\n      self.assertNotEqual(old_variables, new_variables)\n\n  def _test_model_parameter(self, Inference, *args, **kwargs):\n    with self.test_session() as sess:\n      x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n      p = tf.sigmoid(tf.Variable(0.5))\n      x = Bernoulli(probs=p, sample_shape=10)\n\n      inference = Inference({}, data={x: x_data})\n      inference.run(*args, **kwargs)\n\n      self.assertAllClose(p.eval(), 0.2, rtol=5e-2, atol=5e-2)\n\n  def test_klqp(self):\n    self._test_normal_normal(ed.KLqp, default=False, n_iter=5000)\n    self._test_normal_normal(ed.KLqp, default=True, n_iter=5000)\n    self._test_model_parameter(ed.KLqp, n_iter=50)\n\n  def test_klqp_nsamples_check(self):\n    with self.assertRaisesRegexp(ValueError,\n                                 ""n_samples should be greater than zero: 0""):\n      self._test_normal_normal(ed.KLqp, default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ReparameterizationEntropyKLqp,\n                               default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ReparameterizationKLqp,\n                               default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ReparameterizationKLKLqp,\n                               default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ScoreEntropyKLqp,\n                               default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ScoreKLqp,\n                               default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ScoreKLKLqp,\n                               default=True, n_samples=0, n_iter=10)\n      self._test_normal_normal(ed.ScoreRBKLqp,\n                               default=True, n_samples=0, n_iter=10)\n\n  def test_reparameterization_entropy_klqp(self):\n    self._test_normal_normal(\n        ed.ReparameterizationEntropyKLqp, default=False, n_iter=5000)\n    self._test_normal_normal(\n        ed.ReparameterizationEntropyKLqp, default=True, n_iter=5000)\n    self._test_model_parameter(ed.ReparameterizationEntropyKLqp, n_iter=50)\n\n  def test_reparameterization_klqp(self):\n    self._test_normal_normal(\n        ed.ReparameterizationKLqp, default=False, n_iter=5000)\n    self._test_normal_normal(\n        ed.ReparameterizationKLqp, default=True, n_iter=5000)\n    self._test_model_parameter(ed.ReparameterizationKLqp, n_iter=50)\n\n  def test_reparameterization_kl_klqp(self):\n    self._test_normal_normal(\n        ed.ReparameterizationKLKLqp, default=False, n_iter=5000)\n    self._test_normal_normal(\n        ed.ReparameterizationKLKLqp, default=True, n_iter=5000)\n    self._test_model_parameter(ed.ReparameterizationKLKLqp, n_iter=50)\n\n  def test_score_entropy_klqp(self):\n    self._test_normal_normal(\n        ed.ScoreEntropyKLqp, default=False, n_samples=5, n_iter=5000)\n    self._test_normal_normal(\n        ed.ScoreEntropyKLqp, default=True, n_samples=5, n_iter=5000)\n    self._test_model_parameter(ed.ScoreEntropyKLqp, n_iter=50)\n\n  def test_score_klqp(self):\n    self._test_normal_normal(\n        ed.ScoreKLqp, default=False, n_samples=5, n_iter=5000)\n    self._test_normal_normal(\n        ed.ScoreKLqp, default=True, n_samples=5, n_iter=5000)\n    self._test_model_parameter(ed.ScoreKLqp, n_iter=50)\n\n  def test_score_kl_klqp(self):\n    self._test_normal_normal(\n        ed.ScoreKLKLqp, default=False, n_samples=5, n_iter=5000)\n    self._test_normal_normal(\n        ed.ScoreKLKLqp, default=True, n_samples=5, n_iter=5000)\n    self._test_model_parameter(ed.ScoreKLKLqp, n_iter=50)\n\n  def test_score_rb_klqp(self):\n    self._test_normal_normal(\n        ed.ScoreRBKLqp, default=False, n_samples=5, n_iter=5000)\n    self._test_normal_normal(\n        ed.ScoreRBKLqp, default=True, n_samples=5, n_iter=5000)\n    self._test_model_parameter(ed.ScoreRBKLqp, n_iter=50)\n\nif __name__ == \'__main__\':\n  ed.set_seed(42)\n  tf.test.main()\n'"
tests/inferences/laplace_test.py,23,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import MultivariateNormalDiag, MultivariateNormalTriL, Normal\n\n\ndef build_toy_dataset(N, w, noise_std=0.1):\n  D = len(w)\n  x = np.random.randn(N, D).astype(np.float32)\n  y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n  return x, y\n\n\nclass test_laplace_class(tf.test.TestCase):\n\n  def _setup(self):\n    N = 250  # number of data points\n    D = 5  # number of features\n\n    # DATA\n    w_true = np.ones(D) * 5.0\n    X_train, y_train = build_toy_dataset(N, w_true)\n\n    # MODEL\n    X = tf.placeholder(tf.float32, [N, D])\n    w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n    b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n    y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N))\n\n    return N, D, w_true, X_train, y_train, X, w, b, y\n\n  def _test(self, sess, qw, qb, w_true):\n    qw_loc, qb_loc, qw_det_covariance, qb_det_covariance = \\\n        sess.run([qw.loc, qb.loc,\n                  tf.square(qw.scale.determinant()),\n                  tf.square(qb.scale.determinant())])\n\n    self.assertAllClose(qw_loc, w_true, atol=0.5)\n    self.assertAllClose(qb_loc, np.array([0.0]), atol=0.5)\n    self.assertAllClose(qw_det_covariance, 0.0, atol=0.1)\n    self.assertAllClose(qb_det_covariance, 0.0, atol=0.1)\n\n  def test_list(self):\n    with self.test_session() as sess:\n      N, D, w_true, X_train, y_train, X, w, b, y = self._setup()\n\n      # INFERENCE\n      inference = ed.Laplace([w, b], data={X: X_train, y: y_train})\n      inference.run(n_iter=100)\n\n      qw = inference.latent_vars[w]\n      qb = inference.latent_vars[b]\n      self._test(sess, qw, qb, w_true)\n\n  def test_multivariate_normal_tril(self):\n    with self.test_session() as sess:\n      N, D, w_true, X_train, y_train, X, w, b, y = self._setup()\n\n      # INFERENCE. Initialize scales at identity to verify if we\n      # learned an approximately zero determinant.\n      qw = MultivariateNormalTriL(\n          loc=tf.Variable(tf.random_normal([D])),\n          scale_tril=tf.Variable(tf.diag(tf.ones(D))))\n      qb = MultivariateNormalTriL(\n          loc=tf.Variable(tf.random_normal([1])),\n          scale_tril=tf.Variable(tf.diag(tf.ones(1))))\n\n      inference = ed.Laplace({w: qw, b: qb}, data={X: X_train, y: y_train})\n      inference.run(n_iter=100)\n\n      self._test(sess, qw, qb, w_true)\n\n  def test_multivariate_normal_diag(self):\n    with self.test_session() as sess:\n      N, D, w_true, X_train, y_train, X, w, b, y = self._setup()\n\n      # INFERENCE. Initialize scales at identity to verify if we\n      # learned an approximately zero determinant.\n      qw = MultivariateNormalDiag(\n          loc=tf.Variable(tf.random_normal([D])),\n          scale_diag=tf.Variable(tf.ones(D)))\n      qb = MultivariateNormalDiag(\n          loc=tf.Variable(tf.random_normal([1])),\n          scale_diag=tf.Variable(tf.ones(1)))\n\n      inference = ed.Laplace({w: qw, b: qb}, data={X: X_train, y: y_train})\n      inference.run(n_iter=100)\n\n      self._test(sess, qw, qb, w_true)\n      self.assertAllClose(qw.covariance().eval(),\n                          tf.diag(tf.diag_part(qw.covariance())).eval())\n      self.assertAllClose(qb.covariance().eval(),\n                          tf.diag(tf.diag_part(qb.covariance())).eval())\n\n  def test_normal(self):\n    with self.test_session() as sess:\n      N, D, w_true, X_train, y_train, X, w, b, y = self._setup()\n\n      # INFERENCE. Initialize scales at identity to verify if we\n      # learned an approximately zero determinant.\n      qw = Normal(\n          loc=tf.Variable(tf.random_normal([D])),\n          scale=tf.Variable(tf.ones(D)))\n      qb = Normal(\n          loc=tf.Variable(tf.random_normal([1])),\n          scale=tf.Variable(tf.ones(1)))\n\n      inference = ed.Laplace({w: qw, b: qb}, data={X: X_train, y: y_train})\n      inference.run(n_iter=100)\n\n      qw_loc, qb_loc, qw_scale_det, qb_scale_det = \\\n          sess.run([qw.loc, qb.loc,\n                    tf.reduce_prod(qw.scale), tf.reduce_prod(qb.scale)])\n      self.assertAllClose(qw_loc, w_true, atol=0.5)\n      self.assertAllClose(qb_loc, np.array([0.0]), atol=0.5)\n      self.assertAllClose(qw_scale_det, 0.0, atol=0.1)\n      self.assertAllClose(qb_scale_det, 0.0, atol=0.1)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/map_test.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, PointMass\n\n\nclass test_map_class(tf.test.TestCase):\n\n  def test_normalnormal_run(self):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      qmu = PointMass(params=tf.Variable(1.0))\n\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      inference = ed.MAP({mu: qmu}, data={x: x_data})\n      inference.run(n_iter=1000)\n\n      self.assertAllClose(qmu.mean().eval(), 0)\n\n  def test_normalnormal_regularization(self):\n    with self.test_session() as sess:\n      x_data = np.array([5.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      qmu = PointMass(params=tf.Variable(1.0))\n\n      inference = ed.MAP({mu: qmu}, data={x: x_data})\n      inference.run(n_iter=1000)\n      mu_val = qmu.mean().eval()\n\n      # regularized solution\n      regularizer = tf.contrib.layers.l2_regularizer(scale=1.0)\n      mu_reg = tf.get_variable(""mu_reg"", shape=[],\n                               regularizer=regularizer)\n      x_reg = Normal(loc=mu_reg, scale=1.0, sample_shape=50)\n\n      inference_reg = ed.MAP(None, data={x_reg: x_data})\n      inference_reg.run(n_iter=1000)\n\n      mu_reg_val = mu_reg.eval()\n      self.assertAllClose(mu_val, mu_reg_val)\n\nif __name__ == \'__main__\':\n  ed.set_seed(42)\n  tf.test.main()\n'"
tests/inferences/metropolishastings_test.py,22,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, Empirical\n\n\nclass test_metropolishastings_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, default, dtype):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=tf.constant(0.0, dtype=dtype),\n                  scale=tf.constant(1.0, dtype=dtype))\n      x = Normal(loc=mu, scale=tf.constant(1.0, dtype=dtype),\n                 sample_shape=50)\n\n      n_samples = 2000\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      if not default:\n        qmu = Empirical(params=tf.Variable(tf.ones(n_samples, dtype=dtype)))\n        inference = ed.MetropolisHastings({mu: qmu}, {mu: mu}, data={x: x_data})\n      else:\n        inference = ed.MetropolisHastings([mu], {mu: mu}, data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run()\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def _test_linear_regression(self, default, dtype):\n    def build_toy_dataset(N, w, noise_std=0.1):\n      D = len(w)\n      x = np.random.randn(N, D)\n      y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n      return x, y\n\n    with self.test_session() as sess:\n      N = 40  # number of data points\n      D = 10  # number of features\n\n      w_true = np.random.randn(D)\n      X_train, y_train = build_toy_dataset(N, w_true)\n      X_test, y_test = build_toy_dataset(N, w_true)\n\n      X = tf.placeholder(dtype, [N, D])\n      w = Normal(loc=tf.zeros(D, dtype=dtype), scale=tf.ones(D, dtype=dtype))\n      b = Normal(loc=tf.zeros(1, dtype=dtype), scale=tf.ones(1, dtype=dtype))\n      y = Normal(loc=ed.dot(X, w) + b, scale=0.1 * tf.ones(N, dtype=dtype))\n\n      proposal_w = Normal(loc=w, scale=0.5 * tf.ones(D, dtype=dtype))\n      proposal_b = Normal(loc=b, scale=0.5 * tf.ones(1, dtype=dtype))\n\n      n_samples = 2000\n      if not default:\n        qw = Empirical(tf.Variable(tf.zeros([n_samples, D], dtype=dtype)))\n        qb = Empirical(tf.Variable(tf.zeros([n_samples, 1], dtype=dtype)))\n        inference = ed.MetropolisHastings(\n            {w: qw, b: qb}, {w: proposal_w, b: proposal_b},\n            data={X: X_train, y: y_train})\n      else:\n        inference = ed.MetropolisHastings(\n            [w, b], {w: proposal_w, b: proposal_b},\n            data={X: X_train, y: y_train})\n        qw = inference.latent_vars[w]\n        qb = inference.latent_vars[b]\n      inference.run()\n\n      self.assertAllClose(qw.mean().eval(), w_true, rtol=5e-1, atol=5e-1)\n      self.assertAllClose(qb.mean().eval(), [0.0], rtol=5e-1, atol=5e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def test_normal_normal(self):\n    self._test_normal_normal(True, tf.float32)\n    self._test_normal_normal(False, tf.float32)\n    self._test_normal_normal(True, tf.float64)\n    self._test_normal_normal(False, tf.float64)\n\n  def test_linear_regression(self):\n    self._test_linear_regression(True, tf.float32)\n    self._test_linear_regression(False, tf.float32)\n    self._test_linear_regression(True, tf.float64)\n    self._test_linear_regression(False, tf.float64)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/replicaexchangemc_test.py,22,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, Empirical\n\n\nclass test_replicaexchangemc_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, default, dtype):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=tf.constant(0.0, dtype=dtype),\n                  scale=tf.constant(1.0, dtype=dtype))\n      x = Normal(loc=mu, scale=tf.constant(1.0, dtype=dtype),\n                 sample_shape=50)\n\n      n_samples = 2000\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      if not default:\n        qmu = Empirical(params=tf.Variable(tf.ones(n_samples, dtype=dtype)))\n        inference = ed.ReplicaExchangeMC({mu: qmu}, {mu: mu}, data={x: x_data})\n      else:\n        inference = ed.ReplicaExchangeMC([mu], {mu: mu}, data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run()\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def _test_linear_regression(self, default, dtype):\n    def build_toy_dataset(N, w, noise_std=0.1):\n      D = len(w)\n      x = np.random.randn(N, D)\n      y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n      return x, y\n\n    with self.test_session() as sess:\n      N = 40  # number of data points\n      D = 10  # number of features\n\n      w_true = np.random.randn(D)\n      X_train, y_train = build_toy_dataset(N, w_true)\n      X_test, y_test = build_toy_dataset(N, w_true)\n\n      X = tf.placeholder(dtype, [N, D])\n      w = Normal(loc=tf.zeros(D, dtype=dtype), scale=tf.ones(D, dtype=dtype))\n      b = Normal(loc=tf.zeros(1, dtype=dtype), scale=tf.ones(1, dtype=dtype))\n      y = Normal(loc=ed.dot(X, w) + b, scale=0.1 * tf.ones(N, dtype=dtype))\n\n      proposal_w = Normal(loc=w, scale=0.5 * tf.ones(D, dtype=dtype))\n      proposal_b = Normal(loc=b, scale=0.5 * tf.ones(1, dtype=dtype))\n\n      n_samples = 2000\n      if not default:\n        qw = Empirical(tf.Variable(tf.zeros([n_samples, D], dtype=dtype)))\n        qb = Empirical(tf.Variable(tf.zeros([n_samples, 1], dtype=dtype)))\n        inference = ed.ReplicaExchangeMC(\n            {w: qw, b: qb}, {w: proposal_w, b: proposal_b},\n            data={X: X_train, y: y_train})\n      else:\n        inference = ed.ReplicaExchangeMC(\n            [w, b], {w: proposal_w, b: proposal_b},\n            data={X: X_train, y: y_train})\n        qw = inference.latent_vars[w]\n        qb = inference.latent_vars[b]\n      inference.run()\n\n      self.assertAllClose(qw.mean().eval(), w_true, rtol=5e-1, atol=5e-1)\n      self.assertAllClose(qb.mean().eval(), [0.0], rtol=5e-1, atol=5e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def test_normal_normal(self):\n    self._test_normal_normal(True, tf.float32)\n    self._test_normal_normal(False, tf.float32)\n    self._test_normal_normal(True, tf.float64)\n    self._test_normal_normal(False, tf.float64)\n\n  def test_linear_regression(self):\n    self._test_linear_regression(True, tf.float32)\n    self._test_linear_regression(False, tf.float32)\n    self._test_linear_regression(True, tf.float64)\n    self._test_linear_regression(False, tf.float64)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/saver_test.py,15,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, PointMass\n\n\nclass test_saver_class(tf.test.TestCase):\n\n  def test_export_meta_graph(self):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      qmu = PointMass(params=tf.Variable(1.0))\n\n      inference = ed.MAP({mu: qmu}, data={x: x_data})\n      inference.run(n_iter=10)\n\n      saver = tf.train.Saver()\n      saver.export_meta_graph(""/tmp/test_saver.meta"")\n\n  def test_import_meta_graph(self):\n    with self.test_session() as sess:\n      new_saver = tf.train.import_meta_graph(""tests/data/test_saver.meta"")\n      new_saver.restore(sess, ""tests/data/test_saver"")\n      qmu_variable = tf.get_collection(\n          tf.GraphKeys.TRAINABLE_VARIABLES, scope=""posterior"")[0]\n      self.assertNotEqual(qmu_variable.eval(), 1.0)\n\n  def test_restore(self):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      with tf.variable_scope(""posterior""):\n        qmu = PointMass(params=tf.Variable(1.0))\n\n      inference = ed.MAP({mu: qmu}, data={x: x_data})\n\n      saver = tf.train.Saver()\n      saver.restore(sess, ""tests/data/test_saver"")\n      qmu_variable = tf.get_collection(\n          tf.GraphKeys.TRAINABLE_VARIABLES, scope=""posterior"")[0]\n      self.assertNotEqual(qmu_variable.eval(), 1.0)\n\n  def test_save(self):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=0.0, scale=1.0)\n      x = Normal(loc=mu, scale=1.0, sample_shape=50)\n\n      with tf.variable_scope(""posterior""):\n        qmu = PointMass(params=tf.Variable(1.0))\n\n      inference = ed.MAP({mu: qmu}, data={x: x_data})\n      inference.run(n_iter=10)\n\n      saver = tf.train.Saver()\n      saver.save(sess, ""/tmp/test_saver"")\n\nif __name__ == \'__main__\':\n  ed.set_seed(23451)\n  tf.test.main()\n'"
tests/inferences/sghmc_test.py,20,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, Empirical\n\n\nclass test_sghmc_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, default, dtype):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=tf.constant(0.0, dtype=dtype),\n                  scale=tf.constant(1.0, dtype=dtype))\n      x = Normal(loc=mu, scale=tf.constant(1.0, dtype=dtype),\n                 sample_shape=50)\n\n      n_samples = 2000\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      if not default:\n        qmu = Empirical(params=tf.Variable(tf.ones(n_samples, dtype=dtype)))\n        inference = ed.SGHMC({mu: qmu}, data={x: x_data})\n      else:\n        inference = ed.SGHMC([mu], data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run(step_size=0.025)\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def _test_linear_regression(self, default, dtype):\n    def build_toy_dataset(N, w, noise_std=0.1):\n      D = len(w)\n      x = np.random.randn(N, D)\n      y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n      return x, y\n\n    with self.test_session() as sess:\n      N = 40  # number of data points\n      D = 10  # number of features\n\n      w_true = np.random.randn(D)\n      X_train, y_train = build_toy_dataset(N, w_true)\n      X_test, y_test = build_toy_dataset(N, w_true)\n\n      X = tf.placeholder(dtype, [N, D])\n      w = Normal(loc=tf.zeros(D, dtype=dtype), scale=tf.ones(D, dtype=dtype))\n      b = Normal(loc=tf.zeros(1, dtype=dtype), scale=tf.ones(1, dtype=dtype))\n      y = Normal(loc=ed.dot(X, w) + b, scale=0.1 * tf.ones(N, dtype=dtype))\n\n      n_samples = 2000\n      if not default:\n        qw = Empirical(tf.Variable(tf.zeros([n_samples, D], dtype=dtype)))\n        qb = Empirical(tf.Variable(tf.zeros([n_samples, 1], dtype=dtype)))\n        inference = ed.SGHMC({w: qw, b: qb}, data={X: X_train, y: y_train})\n      else:\n        inference = ed.SGHMC([w, b], data={X: X_train, y: y_train})\n        qw = inference.latent_vars[w]\n        qb = inference.latent_vars[b]\n      inference.run(step_size=0.0001)\n\n      self.assertAllClose(qw.mean().eval(), w_true, rtol=5e-1, atol=5e-1)\n      self.assertAllClose(qb.mean().eval(), [0.0], rtol=5e-1, atol=5e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def test_normal_normal(self):\n    self._test_normal_normal(True, tf.float32)\n    self._test_normal_normal(False, tf.float32)\n    self._test_normal_normal(True, tf.float64)\n    self._test_normal_normal(False, tf.float64)\n\n  def test_linear_regression(self):\n    self._test_linear_regression(True, tf.float32)\n    self._test_linear_regression(False, tf.float32)\n    self._test_linear_regression(True, tf.float64)\n    self._test_linear_regression(False, tf.float64)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/sgld_test.py,20,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal, Empirical\n\n\nclass test_sgld_class(tf.test.TestCase):\n\n  def _test_normal_normal(self, default, dtype):\n    with self.test_session() as sess:\n      x_data = np.array([0.0] * 50, dtype=np.float32)\n\n      mu = Normal(loc=tf.constant(0.0, dtype=dtype),\n                  scale=tf.constant(1.0, dtype=dtype))\n      x = Normal(loc=mu, scale=tf.constant(1.0, dtype=dtype),\n                 sample_shape=50)\n\n      n_samples = 2000\n      # analytic solution: N(loc=0.0, scale=\\sqrt{1/51}=0.140)\n      if not default:\n        qmu = Empirical(params=tf.Variable(tf.ones(n_samples, dtype=dtype)))\n        inference = ed.SGLD({mu: qmu}, data={x: x_data})\n      else:\n        inference = ed.SGLD([mu], data={x: x_data})\n        qmu = inference.latent_vars[mu]\n      inference.run(step_size=0.10)\n\n      self.assertAllClose(qmu.mean().eval(), 0, rtol=1e-1, atol=1e-1)\n      self.assertAllClose(qmu.stddev().eval(), np.sqrt(1 / 51),\n                          rtol=1e-1, atol=1e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def _test_linear_regression(self, default, dtype):\n    def build_toy_dataset(N, w, noise_std=0.1):\n      D = len(w)\n      x = np.random.randn(N, D)\n      y = np.dot(x, w) + np.random.normal(0, noise_std, size=N)\n      return x, y\n\n    with self.test_session() as sess:\n      N = 40  # number of data points\n      D = 10  # number of features\n\n      w_true = np.random.randn(D)\n      X_train, y_train = build_toy_dataset(N, w_true)\n      X_test, y_test = build_toy_dataset(N, w_true)\n\n      X = tf.placeholder(dtype, [N, D])\n      w = Normal(loc=tf.zeros(D, dtype=dtype), scale=tf.ones(D, dtype=dtype))\n      b = Normal(loc=tf.zeros(1, dtype=dtype), scale=tf.ones(1, dtype=dtype))\n      y = Normal(loc=ed.dot(X, w) + b, scale=0.1 * tf.ones(N, dtype=dtype))\n\n      n_samples = 2000\n      if not default:\n        qw = Empirical(tf.Variable(tf.zeros([n_samples, D], dtype=dtype)))\n        qb = Empirical(tf.Variable(tf.zeros([n_samples, 1], dtype=dtype)))\n        inference = ed.SGLD({w: qw, b: qb}, data={X: X_train, y: y_train})\n      else:\n        inference = ed.SGLD([w, b], data={X: X_train, y: y_train})\n        qw = inference.latent_vars[w]\n        qb = inference.latent_vars[b]\n      inference.run(step_size=0.001)\n\n      self.assertAllClose(qw.mean().eval(), w_true, rtol=5e-1, atol=5e-1)\n      self.assertAllClose(qb.mean().eval(), [0.0], rtol=5e-1, atol=5e-1)\n\n      old_t, old_n_accept = sess.run([inference.t, inference.n_accept])\n      if not default:\n        self.assertEqual(old_t, n_samples)\n      else:\n        self.assertEqual(old_t, 1e4)\n      self.assertGreater(old_n_accept, 0.1)\n      sess.run(inference.reset)\n      new_t, new_n_accept = sess.run([inference.t, inference.n_accept])\n      self.assertEqual(new_t, 0)\n      self.assertEqual(new_n_accept, 0)\n\n  def test_normal_normal(self):\n    self._test_normal_normal(True, tf.float32)\n    self._test_normal_normal(False, tf.float32)\n    self._test_normal_normal(True, tf.float64)\n    self._test_normal_normal(False, tf.float64)\n\n  def test_linear_regression(self):\n    self._test_linear_regression(True, tf.float32)\n    self._test_linear_regression(False, tf.float32)\n    self._test_linear_regression(True, tf.float64)\n    self._test_linear_regression(False, tf.float64)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/simplify_test.py,5,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.inferences.conjugacy.simplify import *\nfrom edward.inferences.conjugacy.simplify import _mul_n\n\n\nclass test_simplify_class(tf.test.TestCase):\n\n  def test_mul_n(self):\n    a = tf.constant(1.0)\n    b = tf.constant(2.0)\n    c = tf.constant(3.0)\n    ab = _mul_n([a, b])\n    abc = _mul_n([a, b, c])\n\n    with self.test_session() as sess:\n      self.assertEqual(sess.run(ab), 2.0)\n      self.assertEqual(sess.run(abc), 6.0)\n\n  def test_is_number(self):\n    self.assertTrue(is_number(1))\n    self.assertFalse(is_number('one'))\n\n  def test_identity_op_simplify(self):\n    expr = ('#Identity', ('#Mul', ('#Identity', ('#x',)),\n                          ('#Identity', (3.7,))))\n    did_something, new_expr = identity_op_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', ('#x',), (3.7,)))\n    did_something, new_expr = power_op_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_pow_simplify_and_power_op_simplify(self):\n    expr = ('#Square', ('#Reciprocal', ('#Sqrt', ('#x',))))\n    did_something, new_expr = power_op_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr,\n                     ('#CPow2.0000e+00',\n                      ('#CPow-1.0000e+00', ('#CPow5.0000e-01', ('#x',)))))\n    did_something, new_expr = power_op_simplify(new_expr)\n    self.assertFalse(did_something)\n\n    did_something, new_expr = pow_simplify(new_expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#CPow-1.0000e+00', ('#x',)))\n    did_something, new_expr = pow_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_log_cpow_simplify(self):\n    expr = ('#Log', ('#CPow2.3000e+01', ('#x',)))\n    did_something, new_expr = log_pow_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', (2.3000e+01,), ('#Log', ('#x',))))\n    did_something, new_expr = log_pow_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_log_pow_simplify(self):\n    expr = ('#Mul', (3.3,), ('#Log', ('#Pow', (2.3,), (1.3,))))\n    did_something, new_expr = log_pow_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', (3.3,), ('#Mul', (1.3,),\n                                                 ('#Log', (2.3,)))))\n    did_something, new_expr = log_pow_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_log_mul_simplify(self):\n    expr = ('#Log', ('#Mul', (3,), (4.2,), (1.2e+01,), ('#x',)))\n    did_something, new_expr = log_mul_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Add', ('#Log', (3,)),\n                                ('#Log', (4.2,)), ('#Log', (1.2e+01,)),\n                                ('#Log', ('#x',))))\n    did_something, new_expr = log_mul_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_cpow_mul_simplify(self):\n    expr = ('#CPow2.1', ('#Mul', (3,), (4.0,), (1.2e+01,)))\n    did_something, new_expr = pow_mul_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', ('#CPow2.1', (3,)),\n                                ('#CPow2.1', (4.0,)),\n                                ('#CPow2.1', (1.2e+01,))))\n    did_something, new_expr = pow_mul_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_pow_mul_simplify(self):\n    expr = ('#Pow', ('#Mul', (3,), (4.0,), (1.2e+01,)), (2.1,))\n    did_something, new_expr = pow_mul_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', ('#Pow', (3,), (2.1,)),\n                                ('#Pow', (4.0,), (2.1,)),\n                                ('#Pow', (1.2e+01,), (2.1,))))\n    did_something, new_expr = pow_mul_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_mul_add_simplify(self):\n    expr = ('#Mul', ('#Add', (3.0,), (2.0,)),\n            ('#Add', (4.0,), (5.0,)))\n    did_something, new_expr = mul_add_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Add', ('#Add', ('#Mul', (3.0,), (4.0,)),\n                                         ('#Mul', (3.0,), (5.0,))),\n                                ('#Add', ('#Mul', (2.0,), (4.0,)),\n                                 ('#Mul', (2.0,), (5.0,)))))\n    did_something, new_expr = pow_mul_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_add_add_simplify(self):\n    expr = ('#Add', (3.0,), ('#Add', (4.0,), (5.0,), ('#Add', (6.0,))))\n    did_something, new_expr = add_add_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Add', (3.0,), (4.0,), (5.0,), (6.0,)))\n    did_something, new_expr = add_add_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_mul_mul_simplify(self):\n    expr = ('#Mul', (3.0,), ('#Mul', (4.0,), (5.0,), ('#Mul', (6.0,))))\n    did_something, new_expr = mul_mul_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', (3.0,), (4.0,), (5.0,), (6.0,)))\n    did_something, new_expr = mul_mul_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_mul_one_simplify(self):\n    expr = ('#Mul', (3.0,), (1.0,), (4.0,), (5.0,), (6.0,), (1.0,))\n    did_something, new_expr = mul_one_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Mul', (3.0,), (4.0,), (5.0,), (6.0,)))\n    did_something, new_expr = mul_one_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_add_zero_simplify(self):\n    expr = ('#Add', (3.0,), (0.0,), (4.0,), (5.0,), (6.0,), (0.0,))\n    did_something, new_expr = add_zero_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Add', (3.0,), (4.0,), (5.0,), (6.0,)))\n    did_something, new_expr = add_zero_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_mul_zero_simplify(self):\n    expr = ('#Mul', (3.0,), (0.0,), (5.0,), (6.0,), (1.0,))\n    did_something, new_expr = mul_zero_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, (0,))\n    did_something, new_expr = mul_zero_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_square_add_simplify(self):\n    expr = ('#CPow2.0000e+00', ('#Add', (1.0,), ('#Neg', (2.0,))))\n    did_something, new_expr = square_add_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Add', ('#CPow2.0000e+00', (1.0,)),\n                                ('#Mul', (2.0,), (1.0,), ('#Neg', (2.0,))),\n                                ('#CPow2.0000e+00', ('#Neg', (2.0,)))))\n    did_something, new_expr = square_add_simplify(new_expr)\n    self.assertFalse(did_something)\n\n  def test_expr_contains(self):\n    expr = ('#Add', ('#Mul', (1.5, ('#CPow1.2000e+00', ('#x',)))),\n            ('#Mul', (1.2,), (7,)))\n    self.assertTrue(expr_contains(expr, '#x'))\n    self.assertFalse(expr_contains(expr, '#CPow'))\n    self.assertTrue(expr_contains(expr, '#CPow1.2000e+00'))\n    self.assertTrue(expr_contains(expr, '#Mul'))\n    self.assertTrue(expr_contains(expr, '#Add'))\n    self.assertTrue(expr_contains(expr[1], '#x'))\n    self.assertFalse(expr_contains(expr[2], '#x'))\n\n  def test_add_const_simplify(self):\n    expr = ('#Add', ('#Mul', (1.5, ('#CPow1.2000e+00', ('#x',)))),\n            ('#Mul', (1.2,), (7,)))\n    did_something, new_expr = add_const_simplify(expr)\n    self.assertTrue(did_something)\n    self.assertEqual(new_expr, ('#Add', ('#Mul',\n                                         (1.5,\n                                          ('#CPow1.2000e+00', ('#x',))))))\n    did_something, new_expr = add_const_simplify(new_expr)\n    self.assertFalse(did_something)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/inferences/wakesleep_test.py,3,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\n\n\nclass test_wakesleep_class(tf.test.TestCase):\n\n  def _test_model_parameter(self, Inference, *args, **kwargs):\n    with self.test_session() as sess:\n      x_data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n      p = tf.sigmoid(tf.Variable(0.5))\n      x = Bernoulli(probs=p, sample_shape=10)\n\n      inference = Inference({}, data={x: x_data})\n      inference.run(*args, **kwargs)\n\n      self.assertAllClose(p.eval(), 0.2, rtol=5e-2, atol=5e-2)\n\n  def test_wakesleep(self):\n    self._test_model_parameter(ed.WakeSleep, n_iter=50)\n\nif __name__ == '__main__':\n  ed.set_seed(42)\n  tf.test.main()\n"""
tests/inferences/wgan_inference_test.py,11,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\nfrom tensorflow.contrib import slim\n\n\ndef next_batch(M):\n  samples = np.random.normal(4.0, 0.1, M)\n  samples.sort()\n  return samples\n\n\ndef discriminative_network(x):\n  """"""Outputs probability in logits.""""""\n  h0 = slim.fully_connected(x, 10, activation_fn=tf.nn.relu)\n  return slim.fully_connected(h0, 1, activation_fn=None)\n\n\nclass test_wgan_class(tf.test.TestCase):\n\n  def test_normal_clip(self):\n    with self.test_session() as sess:\n      # DATA\n      M = 12  # batch size during training\n      x_ph = tf.placeholder(tf.float32, [M, 1])\n\n      # MODEL\n      with tf.variable_scope(""Gen""):\n        theta = tf.Variable(0.0)\n        x = Normal(theta, 0.1, sample_shape=[M, 1])\n\n      # INFERENCE\n      inference = ed.WGANInference(\n          data={x: x_ph}, discriminator=discriminative_network)\n      inference.initialize(penalty=None, clip=0.01, n_iter=500)\n      tf.global_variables_initializer().run()\n\n      for _ in range(inference.n_iter):\n        x_data = next_batch(M).reshape([M, 1])\n        for _ in range(5):\n          info_dict_d = inference.update(feed_dict={x_ph: x_data},\n                                         variables=""Disc"")\n\n        inference.update(feed_dict={x_ph: x_data}, variables=""Gen"")\n\n      self.assertAllClose(theta.eval(), 4.0, rtol=1.0, atol=1.0)\n\n  def test_normal_penalty(self):\n    with self.test_session() as sess:\n      # DATA\n      M = 12  # batch size during training\n      x_ph = tf.placeholder(tf.float32, [M, 1])\n\n      # MODEL\n      with tf.variable_scope(""Gen""):\n        theta = tf.Variable(0.0)\n        x = Normal(theta, 0.1, sample_shape=[M, 1])\n\n      # INFERENCE\n      inference = ed.WGANInference(\n          data={x: x_ph}, discriminator=discriminative_network)\n      inference.initialize(penalty=0.1, n_iter=500)\n      tf.global_variables_initializer().run()\n\n      for _ in range(inference.n_iter):\n        x_data = next_batch(M).reshape([M, 1])\n        for _ in range(5):\n          info_dict_d = inference.update(feed_dict={x_ph: x_data},\n                                         variables=""Disc"")\n\n        inference.update(feed_dict={x_ph: x_data}, variables=""Gen"")\n\n      # CRITICISM\n      self.assertAllClose(theta.eval(), 4.0, rtol=1.0, atol=1.0)\n\nif __name__ == \'__main__\':\n  ed.set_seed(12451)\n  tf.test.main()\n'"
tests/models/bernoulli_doc_test.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli\nfrom tensorflow.contrib import distributions as ds\n\n\nclass test_bernoulli_doc_class(tf.test.TestCase):\n\n  def test(self):\n    self.assertGreater(len(Bernoulli.__doc__), 0)\n    self.assertEqual(Bernoulli.__doc__, ds.Bernoulli.__doc__)\n    self.assertEqual(Bernoulli.__name__, ""Bernoulli"")\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/models/bernoulli_log_prob_test.py,6,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli\nfrom tensorflow.contrib import distributions as ds\n\n\nclass test_bernoulli_log_prob_class(tf.test.TestCase):\n\n  def _test(self, probs, n):\n    rv = Bernoulli(probs)\n    dist = ds.Bernoulli(probs)\n    x = rv.sample(n).eval()\n    self.assertAllEqual(rv.log_prob(x).eval(), dist.log_prob(x).eval())\n\n  def test_1d(self):\n    with self.test_session():\n      self._test(tf.zeros([1]) + 0.5, [1])\n      self._test(tf.zeros([1]) + 0.5, [5])\n      self._test(tf.zeros([5]) + 0.5, [1])\n      self._test(tf.zeros([5]) + 0.5, [5])\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/bernoulli_sample_test.py,7,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli\nfrom tensorflow.contrib import distributions as ds\n\n\nclass test_bernoulli_sample_class(tf.test.TestCase):\n\n  def _test(self, probs, n):\n    rv = Bernoulli(probs)\n    dist = ds.Bernoulli(probs)\n    self.assertEqual(rv.sample(n).shape, dist.sample(n).shape)\n\n  def test_0d(self):\n    with self.test_session():\n      self._test(0.5, [1])\n      self._test(np.array(0.5), [1])\n      self._test(tf.constant(0.5), [1])\n\n  def test_1d(self):\n    with self.test_session():\n      self._test(np.array([0.5]), [1])\n      self._test(np.array([0.5]), [5])\n      self._test(np.array([0.2, 0.8]), [1])\n      self._test(np.array([0.2, 0.8]), [10])\n      self._test(tf.constant([0.5]), [1])\n      self._test(tf.constant([0.5]), [5])\n      self._test(tf.constant([0.2, 0.8]), [1])\n      self._test(tf.constant([0.2, 0.8]), [10])\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/dirichlet_process_sample_test.py,18,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import DirichletProcess, Normal\n\n\nclass test_dirichletprocess_sample_class(tf.test.TestCase):\n\n  def _test(self, n, concentration, base):\n    x = DirichletProcess(concentration=concentration, base=base)\n    val_est = x.sample(n).shape.as_list()\n    val_true = n + tf.convert_to_tensor(concentration).shape.as_list() + \\\n        tf.convert_to_tensor(base).shape.as_list()\n    self.assertEqual(val_est, val_true)\n\n  def test_concentration_0d_base_0d(self):\n    with self.test_session():\n      self._test([1], 0.5, Normal(loc=0.0, scale=0.5))\n      self._test([5], tf.constant(0.5), Normal(loc=0.0, scale=0.5))\n\n  def test_concentration_1d_base_0d(self):\n    with self.test_session():\n      self._test([1], np.array([0.5]), Normal(loc=0.0, scale=0.5))\n      self._test([5], tf.constant([0.5]), Normal(loc=0.0, scale=0.5))\n      self._test([1], tf.constant([0.2, 1.5]), Normal(loc=0.0, scale=0.5))\n      self._test([5], tf.constant([0.2, 1.5]), Normal(loc=0.0, scale=0.5))\n\n  def test_concentration_0d_base_1d(self):\n    with self.test_session():\n      self._test([1], 0.5, Normal(loc=tf.zeros(3), scale=tf.ones(3)))\n      self._test([5], tf.constant(0.5),\n                 Normal(loc=tf.zeros(3), scale=tf.ones(3)))\n\n  def test_concentration_1d_base_2d(self):\n    with self.test_session():\n      self._test([1], np.array([0.5]),\n                 Normal(loc=tf.zeros([3, 4]), scale=tf.ones([3, 4])))\n      self._test([5], tf.constant([0.5]),\n                 Normal(loc=tf.zeros([3, 4]), scale=tf.ones([3, 4])))\n      self._test([1], tf.constant([0.2, 1.5]),\n                 Normal(loc=tf.zeros([3, 4]), scale=tf.ones([3, 4])))\n      self._test([5], tf.constant([0.2, 1.5]),\n                 Normal(loc=tf.zeros([3, 4]), scale=tf.ones([3, 4])))\n\n  def test_persistent_state(self):\n    with self.test_session() as sess:\n      dp = DirichletProcess(0.1, Normal(loc=0.0, scale=1.0))\n      x = dp.sample(5)\n      y = dp.sample(5)\n      x_data, y_data, locs = sess.run([x, y, dp.locs])\n      for sample in x_data:\n        self.assertTrue(sample in locs)\n      for sample in y_data:\n        self.assertTrue(sample in locs)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/empirical_sample_test.py,8,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Empirical\n\n\nclass test_empirical_sample_class(tf.test.TestCase):\n\n  def _test(self, params, n):\n    x = Empirical(params=params)\n    val_est = x.sample(n).shape.as_list()\n    val_true = n + tf.convert_to_tensor(params).shape.as_list()[1:]\n    self.assertEqual(val_est, val_true)\n\n  def test_0d(self):\n    with self.test_session():\n      self._test(0.5, [1])\n      self._test(np.array(0.5), [1])\n      self._test(tf.constant(0.5), [1])\n      self._test(np.array([0.5]), [1])\n      self._test(np.array([0.5]), [5])\n      self._test(np.array([0.2, 0.8]), [1])\n      self._test(np.array([0.2, 0.8]), [10])\n      self._test(tf.constant([0.5]), [1])\n      self._test(tf.constant([0.5]), [5])\n      self._test(tf.constant([0.2, 0.8]), [1])\n      self._test(tf.constant([0.2, 0.8]), [10])\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/keras_core_layers_test.py,12,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport keras.layers as layers\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_keras_core_layers_class(tf.test.TestCase):\n\n  def test_dense(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Dense(32)(x.value())\n\n  def test_activation(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Activation('tanh')(x.value())\n\n  def test_dropout(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Dropout(0.5)(x.value())\n\n  def test_flatten(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Flatten()(x.value())\n    with self.test_session():\n      self.assertEqual(y.eval().shape, (100, 50))\n\n  def test_reshape(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Reshape((5, 10))(x.value())\n    with self.test_session():\n      self.assertEqual(y.eval().shape, (100, 5, 10))\n\n  def test_permute(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Permute((2, 1))(x.value())\n    with self.test_session():\n      self.assertEqual(y.eval().shape, (100, 5, 10))\n\n  def test_repeat_vector(self):\n    x = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))\n    y = layers.RepeatVector(2)(x.value())\n    with self.test_session():\n      self.assertEqual(y.eval().shape, (100, 2, 10))\n\n  def test_lambda(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Lambda(lambda x: x ** 2)(x.value())\n\n  def test_activity_regularization(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.ActivityRegularization(l1=0.1)(x.value())\n\n  def test_masking(self):\n    x = Normal(loc=tf.zeros([100, 10, 5]), scale=tf.ones([100, 10, 5]))\n    y = layers.Masking()(x.value())\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/param_mixture_sample_test.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Beta, Dirichlet, Normal, ParamMixture\n\n\nclass test_param_mixture_sample_class(tf.test.TestCase):\n\n  def _test(self, n, *args, **kwargs):\n    rv = ParamMixture(*args, **kwargs)\n    val_est = rv.sample(n).shape\n    val_true = tf.TensorShape(n).concatenate(\n        rv.cat.batch_shape).concatenate(rv.components.event_shape)\n    self.assertEqual(val_est, val_true)\n\n    self.assertEqual(rv.sample_shape, rv.cat.sample_shape)\n    self.assertEqual(rv.sample_shape, rv.components.sample_shape)\n    self.assertEqual(rv.batch_shape, rv.cat.batch_shape)\n    self.assertEqual(rv.event_shape, rv.components.event_shape)\n\n  def test_batch_0d_event_0d(self):\n    """"""Mixture of 3 normal distributions.""""""\n    with self.test_session():\n      probs = np.array([0.2, 0.3, 0.5], np.float32)\n      loc = np.array([1.0, 5.0, 7.0], np.float32)\n      scale = np.array([1.5, 1.5, 1.5], np.float32)\n\n      self._test([], probs, {\'loc\': loc, \'scale\': scale}, Normal)\n      self._test([5], probs, {\'loc\': loc, \'scale\': scale}, Normal)\n\n  def test_batch_0d_event_1d(self):\n    """"""Mixture of 2 Dirichlet distributions.""""""\n    with self.test_session():\n      probs = np.array([0.4, 0.6], np.float32)\n      concentration = np.ones([2, 3], np.float32)\n\n      self._test([], probs, {\'concentration\': concentration}, Dirichlet)\n      self._test([5], probs, {\'concentration\': concentration}, Dirichlet)\n\n  def test_batch_1d_event_0d(self):\n    """"""Two mixtures each of 3 beta distributions.""""""\n    with self.test_session():\n      probs = np.array([[0.2, 0.3, 0.5], [0.2, 0.3, 0.5]], np.float32)\n      conc1 = np.array([[2.0, 0.5], [1.0, 1.0], [0.5, 2.0]], np.float32)\n      conc0 = conc1 + 2.0\n\n      self._test([], probs, {\'concentration1\': conc1, \'concentration0\': conc0},\n                 Beta)\n      self._test([5], probs, {\'concentration1\': conc1, \'concentration0\': conc0},\n                 Beta)\n\n      probs = np.array([0.2, 0.3, 0.5], np.float32)\n      self.assertRaises(ValueError, self._test, [], probs,\n                        {\'concentration1\': conc1, \'concentration0\': conc0},\n                        Beta)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/models/param_mixture_stats_test.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Beta, Normal, ParamMixture\n\n\ndef _make_histograms(values, hists, hist_centers, x_axis, n_bins):\n  if len(values.shape) > 1:\n    for i in range(values.shape[1]):\n      _make_histograms(values[:, i], hists[:, i], hist_centers[:, i],\n                       x_axis[:, i], n_bins)\n  else:\n    hist, hist_bins = np.histogram(values, bins=n_bins)\n    bin_width = hist_bins[1] - hist_bins[0]\n    hists[:] = hist / float(hist.sum())\n    hist_centers[:] = 0.5 * (hist_bins[1:] + hist_bins[:-1])\n    x_axis[:n_bins] = hist_centers\n\n\nclass test_param_mixture_class(tf.test.TestCase):\n\n  def _test(self, probs, params, dist):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(10003)\n\n      N = 50000\n\n      x = ParamMixture(probs, params, dist, sample_shape=N)\n      cat = x.cat\n      components = x.components\n\n      marginal_logp = x.marginal_log_prob(x)\n      cond_logp = x.log_prob(x)\n\n      comp_means = components.mean()\n      comp_stddevs = components.stddev()\n      marginal_mean = x.mean()\n      marginal_stddev = x.stddev()\n      marginal_var = x.variance()\n\n    sess = self.test_session(graph=g)\n    with self.test_session(graph=g) as sess:\n      to_eval = [x, cat, components, comp_means, comp_stddevs, marginal_mean,\n                 marginal_stddev, marginal_var, marginal_logp, cond_logp]\n      vals = sess.run(to_eval)\n      vals = {k: v for k, v in zip(to_eval, vals)}\n\n      # Test that marginal statistics are reasonable\n      self.assertAllClose(vals[x].mean(0), vals[marginal_mean],\n                          rtol=0.01, atol=0.01)\n      self.assertAllClose(vals[x].std(0), vals[marginal_stddev],\n                          rtol=0.01, atol=0.01)\n      self.assertAllClose(vals[x].var(0), vals[marginal_var],\n                          rtol=0.01, atol=0.01)\n\n      # Test that per-component statistics are reasonable\n      for k in range(x.num_components):\n        selector = (vals[cat] == k)\n        self.assertAllClose(selector.mean(), probs[k], rtol=0.01, atol=0.01)\n        x_k = vals[x][selector]\n        self.assertAllClose(x_k.mean(0), vals[comp_means][k],\n                            rtol=0.05, atol=0.05)\n        self.assertAllClose(x_k.std(0), vals[comp_stddevs][k],\n                            rtol=0.05, atol=0.05)\n\n      n_bins = 100\n      x_hists = np.zeros((n_bins,) + vals[x].shape[1:])\n      hist_centers = np.zeros_like(x_hists)\n      x_axis = np.zeros((N,) + vals[x].shape[1:])\n      _make_histograms(vals[x], x_hists, hist_centers, x_axis, n_bins)\n\n      x_marginal_val = sess.run(marginal_logp, {x: x_axis,\n                                                components: vals[components]})\n      # Test that histograms match marginal log prob\n      x_pseudo_hist = np.exp(x_marginal_val[:n_bins])\n      self.assertAllClose(x_pseudo_hist.sum(0) * (x_axis[1] - x_axis[0]), 1.,\n                          rtol=0.1, atol=0.1)\n      x_pseudo_hist /= x_pseudo_hist.sum(0, keepdims=True)\n      self.assertLess(abs(x_pseudo_hist - x_hists).sum(0).mean(), 0.1)\n\n      # Test that histograms match conditional log prob\n      for k in range(probs.shape[-1]):\n        k_cat = k + np.zeros(x_axis.shape, np.int32)\n        x_vals_k = sess.run(x, {cat: k_cat, components: vals[components]})\n        _make_histograms(x_vals_k, x_hists, hist_centers, x_axis, n_bins)\n        x_cond_logp_val_k = sess.run(cond_logp, {x: x_axis, cat: k_cat,\n                                                 components: vals[components]})\n        x_pseudo_hist = np.exp(x_cond_logp_val_k[:n_bins])\n        self.assertAllClose(x_pseudo_hist.sum(0) * (x_axis[1] - x_axis[0]), 1.,\n                            rtol=0.1, atol=0.1)\n        x_pseudo_hist /= x_pseudo_hist.sum(0, keepdims=True)\n        self.assertLess(abs(x_pseudo_hist - x_hists).sum(0).mean(), 0.1)\n\n  def test_normal(self):\n    """"""Mixture of 3 normal distributions.""""""\n    probs = np.array([0.2, 0.3, 0.5], np.float32)\n    loc = np.array([1.0, 5.0, 7.0], np.float32)\n    scale = np.array([1.5, 1.5, 1.5], np.float32)\n\n    self._test(probs, {\'loc\': loc, \'scale\': scale}, Normal)\n\n  def test_beta(self):\n    """"""Mixture of 3 beta distributions.""""""\n    probs = np.array([0.2, 0.3, 0.5], np.float32)\n    conc1 = np.array([2.0, 1.0, 0.5], np.float32)\n    conc0 = conc1 + 2.0\n\n    self._test(probs, {\'concentration1\': conc1, \'concentration0\': conc0},\n               Beta)\n\n  def test_batch_beta(self):\n    """"""Two mixtures of 3 beta distributions.""""""\n    probs = np.array([[0.2, 0.3, 0.5], [0.2, 0.3, 0.5]], np.float32)\n    conc1 = np.array([[2.0, 0.5], [1.0, 1.0], [0.5, 2.0]], np.float32)\n    conc0 = conc1 + 2.0\n\n    # self._test(probs, {\'concentration1\': conc1, \'concentration0\': conc0},\n    #            Beta)\n    self.assertRaises(NotImplementedError,\n                      self._test, probs,\n                      {\'concentration1\': conc1, \'concentration0\': conc0},\n                      Beta)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/models/point_mass_sample_test.py,8,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import PointMass\n\n\nclass test_pointmass_sample_class(tf.test.TestCase):\n\n  def _test(self, params, n):\n    x = PointMass(params=params)\n    val_est = x.sample(n).shape.as_list()\n    val_true = n + tf.convert_to_tensor(params).shape.as_list()\n    self.assertEqual(val_est, val_true)\n\n  def test_0d(self):\n    with self.test_session():\n      self._test(0.5, [1])\n      self._test(np.array(0.5), [1])\n      self._test(tf.constant(0.5), [1])\n\n  def test_1d(self):\n    with self.test_session():\n      self._test(np.array([0.5]), [1])\n      self._test(np.array([0.5]), [5])\n      self._test(np.array([0.2, 0.8]), [1])\n      self._test(np.array([0.2, 0.8]), [10])\n      self._test(tf.constant([0.5]), [1])\n      self._test(tf.constant([0.5]), [5])\n      self._test(tf.constant([0.2, 0.8]), [1])\n      self._test(tf.constant([0.2, 0.8]), [10])\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/random_variable_gradients_test.py,5,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\n\n\nclass test_random_variable_gradients_class(tf.test.TestCase):\n\n  def test_first_order(self):\n    with self.test_session() as sess:\n      x = Bernoulli(0.5)\n      y = 2 * x\n      z = tf.gradients(y, x)[0]\n      self.assertEqual(z.eval(), 2)\n\n  def test_second_order(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 2 * (x ** 2)\n      z = tf.gradients(y, x)[0]\n      z = tf.gradients(z, x)[0]\n      self.assertEqual(z.eval(), 4.0)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/random_variable_operators_test.py,4,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_random_variable_operators_class(tf.test.TestCase):\n\n  def test_add(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x + y\n      z_value = x.value() + y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_radd(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y + x\n      z_value = y + x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_sub(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x - y\n      z_value = x.value() - y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_rsub(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y - x\n      z_value = y - x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_mul(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x * y\n      z_value = x.value() * y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_rmul(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y * x\n      z_value = y * x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_div(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x / y\n      z_value = x.value() / y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_rdiv(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y / x\n      z_value = y / x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_floordiv(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x // y\n      z_value = x.value() // y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_rfloordiv(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y // x\n      z_value = y // x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_mod(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x % y\n      z_value = x.value() % y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_rmod(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y % x\n      z_value = y % x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_lt(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x < y\n      z_value = x.value() < y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_le(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x <= y\n      z_value = x.value() <= y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_gt(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x > y\n      z_value = x.value() > y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_ge(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x >= y\n      z_value = x.value() >= y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  # need to test with a random variable of boolean\n  # def test_and(self):\n  #   with self.test_session() as sess:\n      # x = tf.cast(Bernoulli(0.5), tf.bool)\n      # y = True\n      # z = x & y\n      # z_value = x.value() & y\n      # z_eval, z_value_eval = sess.run([z, z_value])\n      # self.assertAllEqual(z_eval, z_value_eval)\n\n  # def test_rand(self):\n  # def test_or(self):\n  # def test_ror(self):\n  # def test_xor(self):\n  # def test_rxor(self):\n\n  def test_getitem(self):\n    with self.test_session() as sess:\n      x = Normal(tf.zeros([3, 4]), tf.ones([3, 4]))\n      z = x[0:2, 2:3]\n      z_value = x.value()[0:2, 2:3]\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_pow(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = x ** y\n      z_value = x.value() ** y\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_rpow(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = 5.0\n      z = y ** x\n      z_value = y ** x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  # def test_invert(self):\n\n  def test_neg(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      z = -x\n      z_value = -x.value()\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_abs(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      z = abs(x)\n      z_value = abs(x.value())\n      z_eval, z_value_eval = sess.run([z, z_value])\n      self.assertAllEqual(z_eval, z_value_eval)\n\n  def test_hash(self):\n    x = Normal(0.0, 1.0)\n    y = 5.0\n    self.assertNotEqual(hash(x), hash(y))\n    self.assertEqual(hash(x), id(x))\n\n  def test_eq(self):\n    x = Normal(0.0, 1.0)\n    y = 5.0\n    self.assertNotEqual(x, y)\n    self.assertEqual(x, x)\n\n  def test_bool_nonzero(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      self.assertRaises(TypeError, lambda: not x)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/random_variable_session_test.py,7,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport tensorflow as tf\n\nfrom edward.models import Normal\n\n\nclass test_random_variable_session_class(tf.test.TestCase):\n\n  def test_eval(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 0.1)\n      x_ph = tf.placeholder(tf.float32, [])\n      y = Normal(x_ph, 0.1)\n      self.assertLess(x.eval(), 5.0)\n      self.assertLess(x.eval(sess), 5.0)\n      self.assertLess(x.eval(feed_dict={x_ph: 100.0}), 5.0)\n      self.assertGreater(y.eval(feed_dict={x_ph: 100.0}), 5.0)\n      self.assertGreater(y.eval(sess, feed_dict={x_ph: 100.0}), 5.0)\n      self.assertRaises(tf.errors.InvalidArgumentError, y.eval)\n      self.assertRaises(tf.errors.InvalidArgumentError, y.eval, sess)\n\n  def test_run(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 0.1)\n      x_ph = tf.placeholder(tf.float32, [])\n      y = Normal(x_ph, 0.1)\n      self.assertLess(sess.run(x), 5.0)\n      self.assertLess(sess.run(x, feed_dict={x_ph: 100.0}), 5.0)\n      self.assertGreater(sess.run(y, feed_dict={x_ph: 100.0}), 5.0)\n      self.assertRaises(tf.errors.InvalidArgumentError, sess.run, y)\n\nif __name__ == '__main__':\n  ed.set_seed(82341)\n  tf.test.main()\n"""
tests/models/random_variable_shape_test.py,7,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Dirichlet\n\n\nclass test_random_variable_shape_class(tf.test.TestCase):\n\n  def _test(self, rv, sample_shape, batch_shape, event_shape):\n    self.assertEqual(rv.shape, sample_shape + batch_shape + event_shape)\n    self.assertEqual(rv.sample_shape, sample_shape)\n    self.assertEqual(rv.batch_shape, batch_shape)\n    self.assertEqual(rv.event_shape, event_shape)\n\n  def test_bernoulli(self):\n    with self.test_session():\n      self._test(Bernoulli(0.5), [], [], [])\n      self._test(Bernoulli(tf.zeros([2, 3])), [], [2, 3], [])\n      self._test(Bernoulli(0.5, sample_shape=2), [2], [], [])\n      self._test(Bernoulli(0.5, sample_shape=[2, 1]), [2, 1], [], [])\n\n  def test_dirichlet(self):\n    with self.test_session():\n      self._test(Dirichlet(tf.zeros(3)), [], [], [3])\n      self._test(Dirichlet(tf.zeros([2, 3])), [], [2], [3])\n      self._test(Dirichlet(tf.zeros(3), sample_shape=1), [1], [], [3])\n      self._test(Dirichlet(tf.zeros(3), sample_shape=[2, 1]), [2, 1], [], [3])\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/models/random_variable_value_test.py,3,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal, Poisson, RandomVariable\nfrom edward.util import copy\n\n\nclass test_random_variable_value_class(tf.test.TestCase):\n\n  def _test_sample(self, RV, value, *args, **kwargs):\n    rv = RV(*args, value=value, **kwargs)\n    value_shape = rv.value().shape\n    expected_shape = rv.sample_shape.concatenate(\n        rv.batch_shape).concatenate(rv.event_shape)\n    self.assertEqual(value_shape, expected_shape)\n    self.assertEqual(rv.dtype, rv.value().dtype)\n\n  def _test_copy(self, RV, value, *args, **kwargs):\n    rv1 = RV(*args, value=value, **kwargs)\n    rv2 = copy(rv1)\n    value_shape1 = rv1.value().shape\n    value_shape2 = rv2.value().shape\n    self.assertEqual(value_shape1, value_shape2)\n\n  def test_shape_and_dtype(self):\n    with self.test_session():\n      self._test_sample(Normal, 2, loc=0.5, scale=1.0)\n      self._test_sample(Normal, [2], loc=[0.5], scale=[1.0])\n      self._test_sample(Poisson, 2, rate=0.5)\n\n  def test_unknown_shape(self):\n    with self.test_session():\n      x = Bernoulli(0.5, value=tf.placeholder(tf.int32))\n\n  def test_mismatch_raises(self):\n    with self.test_session():\n      self.assertRaises(ValueError, self._test_sample, Normal, 2,\n                        loc=[0.5, 0.5], scale=1.0)\n      self.assertRaises(ValueError, self._test_sample, Normal, 2,\n                        loc=[0.5], scale=[1.0])\n      self.assertRaises(ValueError, self._test_sample, Normal,\n                        np.zeros([10, 3]), loc=[0.5, 0.5], scale=[1.0, 1.0])\n\n  def test_copy(self):\n    with self.test_session():\n      self._test_copy(Normal, 2, loc=0.5, scale=1.0)\n      self._test_copy(Normal, [2], loc=[0.5], scale=[1.0])\n      self._test_copy(Poisson, 2, rate=0.5)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/notebooks/notebooks_test.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport nbformat\nimport os\nimport sys\nimport time\nimport traceback\nimport tensorflow as tf\n\nfrom nbconvert.preprocessors import ExecutePreprocessor\nfrom nbconvert.preprocessors.execute import CellExecutionError\n\n\nclass test_notebooks(tf.test.TestCase):\n\n  def _exec_notebook(self, ep, filename):\n    with open(filename) as f:\n      nb = nbformat.read(f, as_version=nbformat.current_nbformat)\n      try:\n        out = ep.preprocess(nb, {})\n      except CellExecutionError:\n        print(\'-\' * 60)\n        traceback.print_exc(file=sys.stdout)\n        print(\'-\' * 60)\n        self.assertTrue(False,\n                        \'Error executing the notebook %s. See above for error.\'\n                        % filename)\n\n  def test_all_notebooks(self):\n    """""" Test all notebooks except blacklist. """"""\n    blacklist = [\'gan.ipynb\', \'iclr2017.ipynb\']\n    nbpath = \'notebooks/\'\n    # see http://nbconvert.readthedocs.io/en/stable/execute_api.html\n    ep = ExecutePreprocessor(timeout=120,\n                             kernel_name=\'python\' + str(sys.version_info[0]),\n                             interrupt_on_timeout=True)\n    os.chdir(nbpath)\n    files = glob.glob(""*.ipynb"")\n    for filename in files:\n      if filename not in blacklist:\n        start = time.time()\n        self._exec_notebook(ep, filename)\n        end = time.time()\n        print(filename, \'took %g seconds.\' % (end - start))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/check_data_test.py,10,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Normal\nfrom edward.util import check_data\n\n\nclass test_check_data_class(tf.test.TestCase):\n\n  def test(self):\n    with self.test_session():\n      x = Normal(0.0, 1.0)\n      qx = Normal(0.0, 1.0)\n      x_ph = tf.placeholder(tf.float32, [])\n\n      check_data({x: tf.constant(0.0)})\n      check_data({x: np.float64(0.0)})\n      check_data({x: np.int64(0)})\n      check_data({x: 0.0})\n      check_data({x: 0})\n      check_data({x: False})\n      check_data({x: '0'})\n      check_data({x: x_ph})\n      check_data({x: qx})\n      check_data({2.0 * x: tf.constant(0.0)})\n      self.assertRaises(TypeError, check_data, {0.0: x})\n      self.assertRaises(TypeError, check_data, {x: tf.zeros(5)})\n      self.assertRaises(TypeError, check_data, {x_ph: x})\n      self.assertRaises(TypeError, check_data, {x_ph: x})\n      self.assertRaises(TypeError, check_data,\n                        {x: tf.constant(0, tf.float64)})\n      self.assertRaises(TypeError, check_data,\n                        {x_ph: tf.constant(0.0)})\n\n      x_vec = Normal(tf.constant([0.0]), tf.constant([1.0]))\n      qx_vec = Normal(tf.constant([0.0]), tf.constant([1.0]))\n\n      check_data({x_vec: qx_vec})\n      check_data({x_vec: [0.0]})\n      check_data({x_vec: [0]})\n      check_data({x_vec: ['0']})\n      self.assertRaises(TypeError, check_data, {x: qx_vec})\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/util/check_latent_vars_test.py,6,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Normal\nfrom edward.util import check_latent_vars\n\n\nclass test_check_latent_vars_class(tf.test.TestCase):\n\n  def test(self):\n    with self.test_session():\n      mu = Normal(0.0, 1.0)\n      qmu = Normal(tf.Variable(0.0), tf.constant(1.0))\n      qmu_vec = Normal(tf.constant([0.0]), tf.constant([1.0]))\n\n      check_latent_vars({mu: qmu})\n      check_latent_vars({mu: tf.constant(0.0)})\n      check_latent_vars({tf.constant(0.0): qmu})\n      self.assertRaises(TypeError, check_latent_vars, {mu: '5'})\n      self.assertRaises(TypeError, check_latent_vars, {mu: qmu_vec})\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/util/copy_test.py,59,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Categorical, Mixture, Normal\n\n\nclass test_copy_class(tf.test.TestCase):\n\n  def test_scope(self):\n    with self.test_session():\n      x = tf.constant(2.0)\n      x_new = ed.copy(x, scope=\'new_scope\')\n      self.assertTrue(x_new.name.startswith(\'new_scope\'))\n\n  def test_replace_itself(self):\n    with self.test_session():\n      x = tf.constant(2.0)\n      y = tf.constant(3.0)\n      x_new = ed.copy(x, {x: y}, replace_itself=False)\n      self.assertEqual(x_new.eval(), 2.0)\n      x_new = ed.copy(x, {x: y}, replace_itself=True)\n      self.assertEqual(x_new.eval(), 3.0)\n\n  def test_copy_q(self):\n    with self.test_session() as sess:\n      x = tf.constant(2.0)\n      y = tf.random_normal([])\n      x_new = ed.copy(x, {x: y}, replace_itself=True, copy_q=False)\n      x_new_val, y_val = sess.run([x_new, y])\n      self.assertEqual(x_new_val, y_val)\n      x_new = ed.copy(x, {x: y}, replace_itself=True, copy_q=True)\n      x_new_val, x_val, y_val = sess.run([x_new, x, y])\n      self.assertNotEqual(x_new_val, x_val)\n      self.assertNotEqual(x_new_val, y_val)\n\n  def test_copy_parent_rvs(self):\n    with self.test_session() as sess:\n      x = Normal(0.0, 1.0)\n      y = tf.constant(3.0)\n      z = x * y\n      z_new = ed.copy(z, scope=\'no_copy_parent_rvs\', copy_parent_rvs=False)\n      self.assertEqual(len(ed.random_variables()), 1)\n      z_new = ed.copy(z, scope=\'copy_parent_rvs\', copy_parent_rvs=True)\n      self.assertEqual(len(ed.random_variables()), 2)\n\n  def test_placeholder(self):\n    with self.test_session() as sess:\n      x = tf.placeholder(tf.float32, name=""CustomName"")\n      y = tf.constant(3.0)\n      z = x * y\n      z_new = ed.copy(z)\n      self.assertEqual(sess.run(z_new, feed_dict={x: 4.0}), 12.0)\n\n  def test_variable(self):\n    with self.test_session() as sess:\n      x = tf.Variable(2.0, name=""CustomName"")\n      y = tf.constant(3.0)\n      z = x * y\n      z_new = ed.copy(z)\n      tf.variables_initializer([x]).run()\n      self.assertEqual(z_new.eval(), 6.0)\n\n  def test_queue(self):\n    with self.test_session() as sess:\n      tensor = tf.constant([0.0, 1.0, 2.0, 3.0])\n      x = tf.train.batch([tensor], batch_size=2, enqueue_many=True,\n                         name=\'CustomName\')\n      y = tf.constant(3.0)\n      z = x * y\n      z_new = ed.copy(z)\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(coord=coord)\n      self.assertAllEqual(sess.run(z_new), np.array([0.0, 3.0]))\n      self.assertAllEqual(sess.run(z_new), np.array([6.0, 9.0]))\n      coord.request_stop()\n      coord.join(threads)\n\n  def test_list(self):\n    with self.test_session() as sess:\n      x = Normal(tf.constant(0.0), tf.constant(0.1))\n      y = Normal(tf.constant(10.0), tf.constant(0.1))\n      cat = Categorical(logits=tf.zeros(5))\n      components = [Normal(x, tf.constant(0.1))\n                    for _ in range(5)]\n      z = Mixture(cat=cat, components=components)\n      z_new = ed.copy(z, {x: y.value()})\n      self.assertGreater(z_new.value().eval(), 5.0)\n\n  def test_random(self):\n    with self.test_session() as sess:\n      ed.set_seed(3742)\n      x = tf.random_normal([])\n      x_copy = ed.copy(x)\n\n      result_copy, result = sess.run([x_copy, x])\n      self.assertNotAlmostEquals(result_copy, result)\n\n  def test_scan(self):\n    with self.test_session() as sess:\n      ed.set_seed(42)\n      op = tf.scan(lambda a, x: a + x, tf.constant([2.0, 3.0, 1.0]))\n      copy_op = ed.copy(op)\n\n      result_copy, result = sess.run([copy_op, op])\n      self.assertAllClose(result_copy, [2.0, 5.0, 6.0])\n      self.assertAllClose(result, [2.0, 5.0, 6.0])\n\n  def test_scan_gradients(self):\n    with self.test_session() as sess:\n      a = tf.Variable([1.0, 2.0, 3.0])\n      op = tf.scan(lambda a, x: a + x, a)\n      copy_op = ed.copy(op)\n      gradient = tf.gradients(op, [a])[0]\n      copy_gradient = tf.gradients(copy_op, [a])[0]\n\n      tf.variables_initializer([a]).run()\n      result_copy, result = sess.run([copy_gradient, gradient])\n      self.assertAllClose(result, [3.0, 2.0, 1.0])\n      self.assertAllClose(result_copy, [3.0, 2.0, 1.0])\n\n  def test_nested_scan_gradients(self):\n    with self.test_session() as sess:\n      a = tf.Variable([1.0, 2.0, 3.0])\n      i = tf.constant(0.0)\n      tot = tf.constant([0.0, 0.0, 0.0])\n      op = tf.while_loop(lambda i, tot: i < 5,\n                         lambda i, tot: (i + 1,\n                                         tot + tf.scan(lambda x0, x:\n                                                       x0 + i * x, a, 0.0)),\n                         [i, tot])[1]\n      copy_op = ed.copy(op)\n      gradient = tf.gradients(op, [a])[0]\n      copy_gradient = tf.gradients(copy_op, [a])[0]\n\n      tf.variables_initializer([a]).run()\n      result_copy, result = sess.run([copy_gradient, gradient])\n      self.assertAllClose(result, [30.0, 20.0, 10.0])\n      self.assertAllClose(result_copy, [30.0, 20.0, 10.0])\n\n  def test_swap_tensor_tensor(self):\n    with self.test_session():\n      x = tf.constant(2.0)\n      y = tf.constant(3.0)\n      z = x * y\n      qx = tf.constant(4.0)\n      z_new = ed.copy(z, {x: qx})\n      self.assertEqual(z_new.eval(), 12.0)\n\n  def test_swap_placeholder_tensor(self):\n    with self.test_session():\n      x = tf.placeholder(tf.float32, name=""CustomName"")\n      y = tf.constant(3.0)\n      z = x * y\n      qx = tf.constant(4.0)\n      z_new = ed.copy(z, {x: qx})\n      self.assertEqual(z_new.eval(), 12.0)\n\n  def test_swap_tensor_placeholder(self):\n    with self.test_session() as sess:\n      x = tf.constant(2.0)\n      y = tf.constant(3.0)\n      z = x * y\n      qx = tf.placeholder(tf.float32, name=""CustomName"")\n      z_new = ed.copy(z, {x: qx})\n      self.assertEqual(sess.run(z_new, feed_dict={qx: 4.0}), 12.0)\n\n  def test_swap_variable_tensor(self):\n    with self.test_session():\n      x = tf.Variable(2.0, name=""CustomName"")\n      y = tf.constant(3.0)\n      z = x * y\n      qx = tf.constant(4.0)\n      z_new = ed.copy(z, {x: qx})\n      tf.variables_initializer([x]).run()\n      self.assertEqual(z_new.eval(), 12.0)\n\n  def test_swap_tensor_variable(self):\n    with self.test_session() as sess:\n      x = tf.constant(2.0)\n      y = tf.constant(3.0)\n      z = x * y\n      qx = tf.Variable(4.0, name=""CustomName"")\n      z_new = ed.copy(z, {x: qx})\n      tf.variables_initializer([qx]).run()\n      self.assertEqual(z_new.eval(), 12.0)\n\n  def test_swap_rv_rv(self):\n    with self.test_session():\n      ed.set_seed(325135)\n      x = Normal(0.0, 0.1)\n      y = tf.constant(1.0)\n      z = x * y\n      qx = Normal(10.0, 0.1)\n      z_new = ed.copy(z, {x: qx})\n      self.assertGreater(z_new.eval(), 5.0)\n\n  def test_swap_rv_tensor(self):\n    with self.test_session():\n      ed.set_seed(289362)\n      x = Normal(0.0, 0.1)\n      y = tf.constant(1.0)\n      z = x * y\n      qx = Normal(10.0, 0.1)\n      z_new = ed.copy(z, {x: qx.value()})\n      self.assertGreater(z_new.eval(), 5.0)\n\n  def test_swap_tensor_rv(self):\n    with self.test_session():\n      ed.set_seed(95258)\n      x = Normal(0.0, 0.1)\n      y = tf.constant(1.0)\n      z = x * y\n      qx = Normal(10.0, 0.1)\n      z_new = ed.copy(z, {x.value(): qx})\n      self.assertGreater(z_new.eval(), 5.0)\n\n  def test_ordering_rv_tensor(self):\n    # Check that random variables are copied correctly in dependency\n    # structure.\n    with self.test_session() as sess:\n      ed.set_seed(12432)\n      x = Bernoulli(logits=0.0)\n      y = tf.cast(x, tf.float32)\n      y_new = ed.copy(y)\n      x_new = ed.copy(x)\n      x_new_val, y_new_val = sess.run([x_new, y_new])\n      self.assertEqual(x_new_val, y_new_val)\n\n  def test_ordering_rv_rv(self):\n    # Check that random variables are copied correctly in dependency\n    # structure.\n    with self.test_session() as sess:\n      ed.set_seed(21782)\n      x = Normal(loc=0.0, scale=10.0)\n      x_abs = tf.abs(x)\n      y = Normal(loc=x_abs, scale=1e-8)\n      y_new = ed.copy(y)\n      x_new = ed.copy(x)\n      x_new_val, y_new_val = sess.run([x_new, y_new])\n      self.assertAllClose(abs(x_new_val), y_new_val)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/dot_test.py,8,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.util import dot\n\n\nclass test_dot_class(tf.test.TestCase):\n\n  def test_dot(self):\n    with self.test_session():\n      a = tf.constant(np.arange(5, dtype=np.float32))\n      b = tf.diag(tf.ones([5]))\n      self.assertAllEqual(dot(a, b).eval(),\n                          np.dot(a.eval(), b.eval()))\n      self.assertAllEqual(dot(b, a).eval(),\n                          np.dot(b.eval(), a.eval()))\n\n  def test_all_finite_raises(self):\n    with self.test_session():\n      a = np.inf * tf.ones([5])\n      b = tf.diag(tf.ones([5]))\n      with self.assertRaisesOpError('Inf'):\n        dot(a, b).eval()\n      a = tf.ones([5]) * np.arange(5)\n      b = np.inf * tf.diag(tf.ones([5]))\n      with self.assertRaisesOpError('Inf'):\n        dot(a, b).eval()\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/util/get_ancestors_test.py,11,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_ancestors\n\n\nclass test_get_ancestors_class(tf.test.TestCase):\n\n  def test_v_structure(self):\n    """"""a -> b -> e <- d <- c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(0.0, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(b * d, 1.0)\n      self.assertEqual(get_ancestors(a), [])\n      self.assertEqual(get_ancestors(b), [a])\n      self.assertEqual(get_ancestors(c), [])\n      self.assertEqual(get_ancestors(d), [c])\n      self.assertEqual(set(get_ancestors(e)), set([a, b, c, d]))\n\n  def test_a_structure(self):\n    """"""e <- d <- a -> b -> c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(a, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_ancestors(a), [])\n      self.assertEqual(get_ancestors(b), [a])\n      self.assertEqual(set(get_ancestors(c)), set([a, b]))\n      self.assertEqual(get_ancestors(d), [a])\n      self.assertEqual(set(get_ancestors(e)), set([a, d]))\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_ancestors(a), [])\n      self.assertEqual(get_ancestors(b), [a])\n      self.assertEqual(set(get_ancestors(c)), set([a, b]))\n      self.assertEqual(set(get_ancestors(d)), set([a, b, c]))\n      self.assertEqual(set(get_ancestors(e)), set([a, b, c, d]))\n\n  def test_tensor(self):\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = tf.constant(2.0)\n      c = a + b\n      d = Normal(c, 1.0)\n      self.assertEqual(get_ancestors(a), [])\n      self.assertEqual(get_ancestors(b), [])\n      self.assertEqual(get_ancestors(c), [a])\n      self.assertEqual(get_ancestors(d), [a])\n\n  def test_control_flow(self):\n    with self.test_session():\n      a = Bernoulli(0.5)\n      b = Normal(0.0, 1.0)\n      c = tf.constant(0.0)\n      d = tf.cond(tf.cast(a, tf.bool), lambda: b, lambda: c)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_ancestors(a), [])\n      self.assertEqual(get_ancestors(b), [])\n      self.assertEqual(get_ancestors(c), [])\n      self.assertEqual(set(get_ancestors(d)), set([a, b]))\n      self.assertEqual(set(get_ancestors(e)), set([a, b]))\n\n  def test_scan(self):\n    """"""copied from test_chain_structure""""""\n    def cumsum(x):\n      return tf.scan(lambda a, x: a + x, x)\n\n    with self.test_session():\n      a = Normal(tf.ones([3]), tf.ones([3]))\n      b = Normal(cumsum(a), tf.ones([3]))\n      c = Normal(cumsum(b), tf.ones([3]))\n      d = Normal(cumsum(c), tf.ones([3]))\n      e = Normal(cumsum(d), tf.ones([3]))\n      self.assertEqual(get_ancestors(a), [])\n      self.assertEqual(get_ancestors(b), [a])\n      self.assertEqual(set(get_ancestors(c)), set([a, b]))\n      self.assertEqual(set(get_ancestors(d)), set([a, b, c]))\n      self.assertEqual(set(get_ancestors(e)), set([a, b, c, d]))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/get_blanket_test.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_blanket\n\n\nclass test_get_blanket_class(tf.test.TestCase):\n\n  def test_blanket_structure(self):\n    """"""a -> c <- b\n            |\n            v\n       d -> f <- e\n    """"""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(0.0, 1.0)\n      c = Normal(a * b, 1.0)\n      d = Normal(0.0, 1.0)\n      e = Normal(0.0, 1.0)\n      f = Normal(c * d * e, 1.0)\n      self.assertEqual(set(get_blanket(a)), set([b, c]))\n      self.assertEqual(set(get_blanket(b)), set([a, c]))\n      self.assertEqual(set(get_blanket(c)), set([a, b, d, e, f]))\n      self.assertEqual(set(get_blanket(d)), set([c, e, f]))\n      self.assertEqual(set(get_blanket(e)), set([c, d, f]))\n      self.assertEqual(set(get_blanket(f)), set([c, d, e]))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/get_children_test.py,11,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_children\n\n\nclass test_get_children_class(tf.test.TestCase):\n\n  def test_v_structure(self):\n    """"""a -> b -> e <- d <- c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(0.0, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(b * d, 1.0)\n      self.assertEqual(get_children(a), [b])\n      self.assertEqual(get_children(b), [e])\n      self.assertEqual(get_children(c), [d])\n      self.assertEqual(get_children(d), [e])\n      self.assertEqual(get_children(e), [])\n\n  def test_a_structure(self):\n    """"""e <- d <- a -> b -> c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(a, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(set(get_children(a)), set([b, d]))\n      self.assertEqual(get_children(b), [c])\n      self.assertEqual(get_children(c), [])\n      self.assertEqual(get_children(d), [e])\n      self.assertEqual(get_children(e), [])\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_children(a), [b])\n      self.assertEqual(get_children(b), [c])\n      self.assertEqual(get_children(c), [d])\n      self.assertEqual(get_children(d), [e])\n      self.assertEqual(get_children(e), [])\n\n  def test_tensor(self):\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = tf.constant(2.0)\n      c = a + b\n      d = Normal(c, 1.0)\n      self.assertEqual(get_children(a), [d])\n      self.assertEqual(get_children(b), [d])\n      self.assertEqual(get_children(c), [d])\n      self.assertEqual(get_children(d), [])\n\n  def test_control_flow(self):\n    with self.test_session():\n      a = Bernoulli(0.5)\n      b = Normal(0.0, 1.0)\n      c = tf.constant(0.0)\n      d = tf.cond(tf.cast(a, tf.bool), lambda: b, lambda: c)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_children(a), [e])\n      self.assertEqual(get_children(b), [e])\n      self.assertEqual(get_children(c), [e])\n      self.assertEqual(get_children(d), [e])\n      self.assertEqual(get_children(e), [])\n\n  def test_scan(self):\n    """"""copied from test_chain_structure""""""\n    def cumsum(x):\n      return tf.scan(lambda a, x: a + x, x)\n\n    with self.test_session():\n      a = Normal(tf.ones([3]), tf.ones([3]))\n      b = Normal(cumsum(a), tf.ones([3]))\n      c = Normal(cumsum(b), tf.ones([3]))\n      d = Normal(cumsum(c), tf.ones([3]))\n      e = Normal(cumsum(d), tf.ones([3]))\n      self.assertEqual(get_children(a), [b])\n      self.assertEqual(get_children(b), [c])\n      self.assertEqual(get_children(c), [d])\n      self.assertEqual(get_children(d), [e])\n      self.assertEqual(get_children(e), [])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/get_control_variate_coef_test.py,4,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.util.tensorflow import get_control_variate_coef\n\n\nclass test_get_control_variate_coef(tf.test.TestCase):\n\n  def test_calculate_correct_coefficient(self):\n    with self.test_session():\n      f = tf.constant([1.0, 2.0, 3.0, 4.0])\n      h = tf.constant([2.0, 3.0, 8.0, 1.0])\n      self.assertAllClose(get_control_variate_coef(f, h).eval(),\n                          0.03448276)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/util/get_descendants_test.py,11,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_descendants\n\n\nclass test_get_descendants_class(tf.test.TestCase):\n\n  def test_v_structure(self):\n    """"""a -> b -> e <- d <- c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(0.0, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(b * d, 1.0)\n      self.assertEqual(set(get_descendants(a)), set([b, e]))\n      self.assertEqual(get_descendants(b), [e])\n      self.assertEqual(set(get_descendants(c)), set([d, e]))\n      self.assertEqual(get_descendants(d), [e])\n      self.assertEqual(get_descendants(e), [])\n\n  def test_a_structure(self):\n    """"""e <- d <- a -> b -> c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(a, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(set(get_descendants(a)), set([b, c, d, e]))\n      self.assertEqual(get_descendants(b), [c])\n      self.assertEqual(get_descendants(c), [])\n      self.assertEqual(get_descendants(d), [e])\n      self.assertEqual(get_descendants(e), [])\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(set(get_descendants(a)), set([b, c, d, e]))\n      self.assertEqual(set(get_descendants(b)), set([c, d, e]))\n      self.assertEqual(set(get_descendants(c)), set([d, e]))\n      self.assertEqual(get_descendants(d), [e])\n      self.assertEqual(get_descendants(e), [])\n\n  def test_tensor(self):\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = tf.constant(2.0)\n      c = a + b\n      d = Normal(c, 1.0)\n      self.assertEqual(get_descendants(a), [d])\n      self.assertEqual(get_descendants(b), [d])\n      self.assertEqual(get_descendants(c), [d])\n      self.assertEqual(get_descendants(d), [])\n\n  def test_control_flow(self):\n    with self.test_session():\n      a = Bernoulli(0.5)\n      b = Normal(0.0, 1.0)\n      c = tf.constant(0.0)\n      d = tf.cond(tf.cast(a, tf.bool), lambda: b, lambda: c)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_descendants(a), [e])\n      self.assertEqual(get_descendants(b), [e])\n      self.assertEqual(get_descendants(c), [e])\n      self.assertEqual(get_descendants(d), [e])\n      self.assertEqual(get_descendants(e), [])\n\n  def test_scan(self):\n    """"""copied from test_chain_structure""""""\n    def cumsum(x):\n      return tf.scan(lambda a, x: a + x, x)\n\n    with self.test_session():\n      a = Normal(tf.ones([3]), tf.ones([3]))\n      b = Normal(cumsum(a), tf.ones([3]))\n      c = Normal(cumsum(b), tf.ones([3]))\n      d = Normal(cumsum(c), tf.ones([3]))\n      e = Normal(cumsum(d), tf.ones([3]))\n      self.assertEqual(set(get_descendants(a)), set([b, c, d, e]))\n      self.assertEqual(set(get_descendants(b)), set([c, d, e]))\n      self.assertEqual(set(get_descendants(c)), set([d, e]))\n      self.assertEqual(get_descendants(d), [e])\n      self.assertEqual(get_descendants(e), [])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/get_parents_test.py,11,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_parents\n\n\nclass test_get_parents_class(tf.test.TestCase):\n\n  def test_v_structure(self):\n    """"""a -> b -> e <- d <- c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(0.0, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(b * d, 1.0)\n      self.assertEqual(get_parents(a), [])\n      self.assertEqual(get_parents(b), [a])\n      self.assertEqual(get_parents(c), [])\n      self.assertEqual(get_parents(d), [c])\n      self.assertEqual(set(get_parents(e)), set([b, d]))\n\n  def test_a_structure(self):\n    """"""e <- d <- a -> b -> c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(a, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_parents(a), [])\n      self.assertEqual(get_parents(b), [a])\n      self.assertEqual(get_parents(c), [b])\n      self.assertEqual(get_parents(d), [a])\n      self.assertEqual(get_parents(e), [d])\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_parents(a), [])\n      self.assertEqual(get_parents(b), [a])\n      self.assertEqual(get_parents(c), [b])\n      self.assertEqual(get_parents(d), [c])\n      self.assertEqual(get_parents(e), [d])\n\n  def test_tensor(self):\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = tf.constant(2.0)\n      c = a + b\n      d = Normal(c, 1.0)\n      self.assertEqual(get_parents(a), [])\n      self.assertEqual(get_parents(b), [])\n      self.assertEqual(get_parents(c), [a])\n      self.assertEqual(get_parents(d), [a])\n\n  def test_control_flow(self):\n    with self.test_session():\n      a = Bernoulli(0.5)\n      b = Normal(0.0, 1.0)\n      c = tf.constant(0.0)\n      d = tf.cond(tf.cast(a, tf.bool), lambda: b, lambda: c)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_parents(a), [])\n      self.assertEqual(get_parents(b), [])\n      self.assertEqual(get_parents(c), [])\n      self.assertEqual(set(get_parents(d)), set([a, b]))\n      self.assertEqual(set(get_parents(e)), set([a, b]))\n\n  def test_scan(self):\n    """"""copied from test_chain_structure""""""\n    def cumsum(x):\n      return tf.scan(lambda a, x: a + x, x)\n\n    with self.test_session():\n      a = Normal(tf.ones([3]), tf.ones([3]))\n      b = Normal(cumsum(a), tf.ones([3]))\n      c = Normal(cumsum(b), tf.ones([3]))\n      d = Normal(cumsum(c), tf.ones([3]))\n      e = Normal(cumsum(d), tf.ones([3]))\n      self.assertEqual(get_parents(a), [])\n      self.assertEqual(get_parents(b), [a])\n      self.assertEqual(get_parents(c), [b])\n      self.assertEqual(get_parents(d), [c])\n      self.assertEqual(get_parents(e), [d])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/get_siblings_test.py,11,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_siblings\n\n\nclass test_get_siblings_class(tf.test.TestCase):\n\n  def test_v_structure(self):\n    """"""a -> b -> e <- d <- c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(0.0, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(b * d, 1.0)\n      self.assertEqual(get_siblings(a), [])\n      self.assertEqual(get_siblings(b), [])\n      self.assertEqual(get_siblings(c), [])\n      self.assertEqual(get_siblings(d), [])\n      self.assertEqual(get_siblings(e), [])\n\n  def test_a_structure(self):\n    """"""e <- d <- a -> b -> c""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(a, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_siblings(a), [])\n      self.assertEqual(get_siblings(b), [d])\n      self.assertEqual(get_siblings(c), [])\n      self.assertEqual(get_siblings(d), [b])\n      self.assertEqual(get_siblings(e), [])\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(c, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_siblings(a), [])\n      self.assertEqual(get_siblings(b), [])\n      self.assertEqual(get_siblings(c), [])\n      self.assertEqual(get_siblings(d), [])\n      self.assertEqual(get_siblings(e), [])\n\n  def test_tensor(self):\n    with self.test_session():\n      a = Normal(0.0, 1.0)\n      b = tf.constant(2.0)\n      c = a + b\n      d = Normal(c, 1.0)\n      self.assertEqual(get_siblings(a), [])\n      self.assertEqual(get_siblings(b), [])\n      self.assertEqual(get_siblings(c), [d])\n      self.assertEqual(get_siblings(d), [])\n\n  def test_control_flow(self):\n    with self.test_session():\n      a = Bernoulli(0.5)\n      b = Normal(0.0, 1.0)\n      c = tf.constant(0.0)\n      d = tf.cond(tf.cast(a, tf.bool), lambda: b, lambda: c)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_siblings(a), [])\n      self.assertEqual(get_siblings(b), [])\n      self.assertEqual(get_siblings(c), [])\n      self.assertEqual(get_siblings(d), [e])\n      self.assertEqual(get_siblings(e), [])\n\n  def test_scan(self):\n    """"""copied from test_a_structure""""""\n    def cumsum(x):\n      return tf.scan(lambda a, x: a + x, x)\n\n    with self.test_session():\n      a = Normal(tf.ones([3]), tf.ones([3]))\n      b = Normal(cumsum(a), tf.ones([3]))\n      c = Normal(cumsum(b), tf.ones([3]))\n      d = Normal(cumsum(a), tf.ones([3]))\n      e = Normal(cumsum(d), tf.ones([3]))\n      self.assertEqual(get_siblings(a), [])\n      self.assertEqual(get_siblings(b), [d])\n      self.assertEqual(get_siblings(c), [])\n      self.assertEqual(get_siblings(d), [b])\n      self.assertEqual(get_siblings(e), [])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/get_variables_test.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Bernoulli, Normal\nfrom edward.util import get_variables\n\n\nclass test_get_variables_class(tf.test.TestCase):\n\n  def test_v_structure(self):\n    """"""a -> b -> e <- d <- c""""""\n    with self.test_session():\n      a = tf.Variable(0.0)\n      b = Normal(a, 1.0)\n      c = tf.Variable(0.0)\n      d = Normal(c, 1.0)\n      e = Normal(b * d, 1.0)\n      self.assertEqual(get_variables(a), [])\n      self.assertEqual(get_variables(b), [a])\n      self.assertEqual(get_variables(c), [])\n      self.assertEqual(get_variables(d), [c])\n      self.assertEqual(set(get_variables(e)), set([a, c]))\n\n  def test_a_structure(self):\n    """"""e <- d <- a -> b -> c""""""\n    with self.test_session():\n      a = tf.Variable(0.0)\n      b = Normal(a, 1.0)\n      c = Normal(b, 1.0)\n      d = Normal(a, 1.0)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_variables(a), [])\n      self.assertEqual(get_variables(b), [a])\n      self.assertEqual(get_variables(c), [a])\n      self.assertEqual(get_variables(d), [a])\n      self.assertEqual(get_variables(e), [a])\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    with self.test_session():\n      a = tf.Variable(0.0)\n      b = tf.Variable(a)\n      c = Normal(b, 1.0)\n      self.assertEqual(get_variables(a), [])\n      self.assertEqual(get_variables(b), [])\n      self.assertEqual(get_variables(c), [b])\n\n  def test_tensor(self):\n    with self.test_session():\n      a = tf.Variable(0.0)\n      b = tf.constant(2.0)\n      c = a + b\n      d = tf.Variable(a)\n      self.assertEqual(get_variables(a), [])\n      self.assertEqual(get_variables(b), [])\n      self.assertEqual(get_variables(c), [a])\n      self.assertEqual(get_variables(d), [])\n\n  def test_control_flow(self):\n    with self.test_session():\n      a = Bernoulli(0.5)\n      b = tf.Variable(0.0)\n      c = tf.constant(0.0)\n      d = tf.cond(tf.cast(a, tf.bool), lambda: b, lambda: c)\n      e = Normal(d, 1.0)\n      self.assertEqual(get_variables(d), [b])\n      self.assertEqual(get_variables(e), [b])\n\n  def test_scan(self):\n    with self.test_session():\n      b = tf.Variable(0.0)\n      op = tf.scan(lambda a, x: a + b + x, tf.constant([2.0, 3.0, 1.0]))\n\n      self.assertEqual(get_variables(op), [b])\n\n  def test_scan_with_a_structure(self):\n    """"""copied from test_a_structure""""""\n    def cumsum(x):\n      return tf.scan(lambda a, x: a + x, x)\n\n    with self.test_session():\n      a = tf.Variable([1.0, 1.0, 1.0])\n      b = Normal(cumsum(a), tf.ones([3]))\n      c = Normal(cumsum(b), tf.ones([3]))\n      d = Normal(cumsum(a), tf.ones([3]))\n      e = Normal(cumsum(d), tf.ones([3]))\n      self.assertEqual(get_variables(a), [])\n      self.assertEqual(get_variables(b), [a])\n      self.assertEqual(get_variables(c), [a])\n      self.assertEqual(get_variables(d), [a])\n      self.assertEqual(get_variables(e), [a])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/is_independent_test.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom edward.models import Normal\nfrom edward.util import is_independent\n\n\nclass test_is_independent_class(tf.test.TestCase):\n\n  def test_chain_structure(self):\n    """"""a -> b -> c -> d -> e""""""\n    a = Normal(0.0, 1.0)\n    b = Normal(a, 1.0)\n    c = Normal(b, 1.0)\n    d = Normal(c, 1.0)\n    e = Normal(d, 1.0)\n    self.assertTrue(is_independent(c, e, d))\n    self.assertTrue(is_independent([a, b, c], e, d))\n    self.assertTrue(is_independent([a, b], [d, e], c))\n    self.assertFalse(is_independent([a, b, e], d, c))\n\n  def test_binary_structure(self):\n    """"""f <- c <- a -> b -> d\n            |         |\n            v         v\n            g         e\n    """"""\n    a = Normal(0.0, 1.0)\n    b = Normal(a, 1.0)\n    c = Normal(a, 1.0)\n    d = Normal(b, 1.0)\n    e = Normal(b, 1.0)\n    f = Normal(c, 1.0)\n    g = Normal(c, 1.0)\n    self.assertFalse(is_independent(b, c))\n    self.assertTrue(is_independent(b, c, a))\n    self.assertTrue(is_independent(d, [a, c, e, f, g], b))\n    self.assertFalse(is_independent(b, [e, d], a))\n    self.assertFalse(is_independent(a, [b, c, d, e, f, g]))\n\n  def test_grid_structure(self):\n    """"""a -> b -> c\n       |    |    |\n       v    v    v\n       d -> e -> f\n    """"""\n    a = Normal(0.0, 1.0)\n    b = Normal(a, 1.0)\n    c = Normal(b, 1.0)\n    d = Normal(a, 1.0)\n    e = Normal(b + d, 1.0)\n    f = Normal(e + c, 1.0)\n    self.assertFalse(is_independent(f, [a, b, d]))\n    self.assertTrue(is_independent(f, [a, b, d], [e, c]))\n    self.assertTrue(is_independent(e, [a, c], [b, d]))\n    self.assertFalse(is_independent(e, f, [b, d]))\n    self.assertFalse(is_independent(e, f, [a, b, c, d]))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/random_variables_test.py,2,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.util.random_variables import compute_multinomial_mode\n\n\nclass test_compute_multinomial_mode(tf.test.TestCase):\n\n  RANDOM_SEED = 12345\n\n  def test_correct_mode_computed_with_uniform_probabilities(self):\n    with self.test_session():\n      probs = np.array(3 * [1 / 3.0])\n      total_count = 5\n      self.assertAllEqual(\n          compute_multinomial_mode(probs, total_count, seed=self.RANDOM_SEED),\n          np.array([1, 2, 2]))\n      probs = np.array([0.6, 0.4, 0.0])\n      total_count = 5\n      self.assertAllEqual(\n          compute_multinomial_mode(probs, total_count, seed=self.RANDOM_SEED),\n          np.array([2, 2, 1]))\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/util/rbf_test.py,24,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.util import rbf\n\n\nclass test_rbf_class(tf.test.TestCase):\n\n  def test_x(self):\n    with self.test_session():\n      X = tf.constant([[0.0], [0.0]])\n      X2 = tf.constant([[0.0], [0.0]])\n      self.assertAllClose(rbf(X).eval(),\n                          [[1.0, 1.0], [1.0, 1.0]])\n      self.assertAllClose(rbf(X, X2).eval(),\n                          [[1.0, 1.0], [1.0, 1.0]])\n\n  def test_x2(self):\n    with self.test_session():\n      X = tf.constant([[10.0], [2.0]])\n      X2 = tf.constant([[2.0], [10.0]])\n      self.assertAllClose(rbf(X, X2).eval(),\n                          [[1.266417e-14, 1.0], [1.0, 1.266417e-14]])\n      self.assertAllClose(rbf(X2, X).eval(),\n                          [[1.266417e-14, 1.0], [1.0, 1.266417e-14]])\n\n      X = tf.constant([[2.0, 2.5], [4.1, 5.0]])\n      X2 = tf.constant([[1.5, 2.0], [3.1, 4.2]])\n      self.assertAllClose(rbf(X, X2).eval(),\n                          [[0.778800, 0.128734],\n                           [0.000378, 0.440431]], atol=1e-5, rtol=1e-5)\n\n  def test_lengthscale(self):\n    """"""checked calculations by hand, e.g.,\n    np.exp(-((2.0 - 1.5)**2 / (2.0**2) + (2.5 - 2.0)**2 / (1.5**2)) / 2)\n    np.exp(-((2.0 - 3.1)**2 / (2.0**2) + (2.5 - 4.2)**2 / (1.5**2)) / 2)\n    np.exp(-((4.1 - 1.5)**2 / (2.0**2) + (5.0 - 2.0)**2 / (1.5**2)) / 2)\n    np.exp(-((4.1 - 3.1)**2 / (2.0**2) + (5.0 - 4.2)**2 / (1.5**2)) / 2)\n    """"""\n    with self.test_session():\n      X = tf.constant([[2.0, 2.5], [4.1, 5.0]])\n      X2 = tf.constant([[1.5, 2.0], [3.1, 4.2]])\n      lengthscale1 = tf.constant(2.0)\n      lengthscale2 = tf.constant([2.0, 2.0])\n      lengthscale3 = tf.constant([2.0, 1.5])\n      self.assertAllClose(rbf(X, X2, lengthscale1).eval(),\n                          [[0.939413, 0.598996],\n                           [0.139456, 0.814647]], atol=1e-5, rtol=1e-5)\n      self.assertAllClose(rbf(X, X2, lengthscale2).eval(),\n                          [[0.939413, 0.598996],\n                           [0.139456, 0.814647]], atol=1e-5, rtol=1e-5)\n      self.assertAllClose(rbf(X, X2, lengthscale3).eval(),\n                          [[0.916855, 0.452271],\n                           [0.058134, 0.765502]], atol=1e-5, rtol=1e-5)\n\n  def test_variance(self):\n    with self.test_session():\n      X = tf.constant([[2.0, 2.5], [4.1, 5.0]])\n      X2 = tf.constant([[1.5, 2.0], [3.1, 4.2]])\n      variance = tf.constant(1.4)\n      self.assertAllClose(rbf(X, X2, variance=variance).eval(),\n                          [[1.090321, 0.180228],\n                           [0.000529, 0.616604]], atol=1e-5, rtol=1e-5)\n\n  def test_all(self):\n    with self.test_session():\n      X = tf.constant([[2.0, 2.5], [4.1, 5.0]])\n      X2 = tf.constant([[1.5, 2.0], [3.1, 4.2]])\n      lengthscale = tf.constant([2.0, 1.5])\n      variance = tf.constant(1.4)\n      self.assertAllClose(rbf(X, X2, lengthscale, variance).eval(),\n                          [[1.283597, 0.633180],\n                           [0.081387, 1.071704]], atol=1e-5, rtol=1e-5)\n\n  def test_raises(self):\n    with self.test_session():\n      X1 = tf.constant([[0.0]])\n      X2 = tf.constant([[0.0]])\n      lengthscale = tf.constant(-5.0)\n      variance = tf.constant(-1.0)\n      with self.assertRaisesOpError(\'Condition\'):\n        rbf(X1, X2, variance=variance).eval()\n        rbf(X1, X2, lengthscale).eval()\n        rbf(X1, X2, lengthscale, variance).eval()\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tests/util/to_simplex_test.py,8,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.util import to_simplex\n\n\nclass test_to_simplex_class(tf.test.TestCase):\n\n  def test_to_simplex_1d(self):\n    with self.test_session():\n      x = tf.constant([0.0])\n      self.assertAllClose(to_simplex(x).eval(),\n                          [0.5, 0.5])\n      x = tf.constant([0.0, 10.0])\n      self.assertAllClose(to_simplex(x).eval(),\n                          [3.333333e-01, 6.666363e-01, 3.027916e-05])\n\n  def test_to_simplex_2d(self):\n    with self.test_session():\n      x = tf.constant([[0.0], [0.0]])\n      self.assertAllClose(to_simplex(x).eval(),\n                          [[0.5, 0.5], [0.5, 0.5]])\n      x = tf.constant([[0.0, 10.0], [0.0, 10.0]])\n      self.assertAllClose(to_simplex(x).eval(),\n                          [[3.333333e-01, 6.666363e-01, 3.027916e-05],\n                           [3.333333e-01, 6.666363e-01, 3.027916e-05]])\n\n  def test_all_finite_raises(self):\n    with self.test_session():\n      x = tf.constant([12.5, np.inf])\n      with self.assertRaisesOpError('Inf'):\n        to_simplex(x).eval()\n      x = tf.constant([12.5, np.nan])\n      with self.assertRaisesOpError('NaN'):\n        to_simplex(x).eval()\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
tests/util/transform_test.py,3,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward as ed\nimport numpy as np\nimport tensorflow as tf\n\nfrom collections import namedtuple\nfrom edward.models import (\n    Beta, Dirichlet, DirichletProcess, Gamma, MultivariateNormalDiag,\n    Normal, Poisson, TransformedDistribution)\nfrom tensorflow.contrib.distributions import bijectors\n\n\nclass test_transform_class(tf.test.TestCase):\n\n  def assertSamplePosNeg(self, sample):\n    num_pos = np.sum((sample > 0.0), axis=0, keepdims=True)\n    num_neg = np.sum((sample < 0.0), axis=0, keepdims=True)\n    self.assertTrue((num_pos > 0).all())\n    self.assertTrue((num_neg > 0).all())\n\n  def test_args(self):\n    with self.test_session():\n      x = Normal(-100.0, 1.0)\n      y = ed.transform(x, bijectors.Softplus())\n      sample = y.sample(10).eval()\n      self.assertTrue((sample >= 0.0).all())\n\n  def test_kwargs(self):\n    with self.test_session():\n      x = Normal(-100.0, 1.0)\n      y = ed.transform(x, bijector=bijectors.Softplus())\n      sample = y.sample(10).eval()\n      self.assertTrue((sample >= 0.0).all())\n\n  def test_01(self):\n    with self.test_session():\n      x = Beta(1.0, 1.0)\n      y = ed.transform(x)\n      self.assertIsInstance(y, TransformedDistribution)\n      sample = y.sample(10, seed=1).eval()\n      self.assertSamplePosNeg(sample)\n\n  def test_nonnegative(self):\n    with self.test_session():\n      x = Gamma(1.0, 1.0)\n      y = ed.transform(x)\n      self.assertIsInstance(y, TransformedDistribution)\n      sample = y.sample(10, seed=1).eval()\n      self.assertSamplePosNeg(sample)\n\n  def test_simplex(self):\n    with self.test_session():\n      x = Dirichlet([1.1, 1.2, 1.3, 1.4])\n      y = ed.transform(x)\n      self.assertIsInstance(y, TransformedDistribution)\n      sample = y.sample(10, seed=1).eval()\n      self.assertSamplePosNeg(sample)\n\n  def test_real(self):\n    with self.test_session():\n      x = Normal(0.0, 1.0)\n      y = ed.transform(x)\n      self.assertIsInstance(y, Normal)\n      sample = y.sample(10, seed=1).eval()\n      self.assertSamplePosNeg(sample)\n\n  def test_multivariate_real(self):\n    with self.test_session():\n      x = MultivariateNormalDiag(tf.zeros(2), tf.ones(2))\n      y = ed.transform(x)\n      sample = y.sample(10, seed=1).eval()\n      self.assertSamplePosNeg(sample)\n\n  def test_no_support(self):\n    with self.test_session():\n      x = DirichletProcess(1.0, Normal(0.0, 1.0))\n      with self.assertRaises(AttributeError):\n        y = ed.transform(x)\n\n  def test_unhandled_support(self):\n    with self.test_session():\n      FakeRV = namedtuple('FakeRV', ['support'])\n      x = FakeRV(support='rational')\n      with self.assertRaises(ValueError):\n        y = ed.transform(x)\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
edward/inferences/conjugacy/__init__.py,0,"b'""""""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom edward.inferences.conjugacy.conjugacy import *\n\nfrom tensorflow.python.util.all_util import remove_undocumented\n\n_allowed_symbols = [\n    \'complete_conditional\',\n]\n\nremove_undocumented(__name__, allowed_exception_list=_allowed_symbols)\n'"
edward/inferences/conjugacy/conjugacy.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport edward.inferences.conjugacy.conjugate_log_probs\nimport numpy as np\nimport six\nimport tensorflow as tf\nimport time\n\nfrom collections import defaultdict\nfrom edward.inferences.conjugacy.simplify \\\n    import symbolic_suff_stat, full_simplify, expr_contains, reconstruct_expr\nfrom edward.models.random_variables import *\nfrom edward.util import copy, get_blanket\n\n\ndef mvn_diag_from_natural_params(p1, p2):\n  sigmasq = 0.5 * tf.reciprocal(-p1)\n  loc = sigmasq * p2\n  return {\'loc\': loc, \'scale_diag\': tf.sqrt(sigmasq)}\n\n\ndef normal_from_natural_params(p1, p2):\n  sigmasq = 0.5 * tf.reciprocal(-p1)\n  loc = sigmasq * p2\n  return {\'loc\': loc, \'scale\': tf.sqrt(sigmasq)}\n\n\n_suff_stat_to_dist = defaultdict(dict)\n_suff_stat_to_dist[\'binary\'][((\'#x\',),)] = (\n    Bernoulli, lambda p1: {\'logits\': p1})\n_suff_stat_to_dist[\'01\'][((\'#Log\', (\'#One_minus\', (\'#x\',))),\n                          (\'#Log\', (\'#x\',)))] = (\n    Beta, lambda p1, p2: {\'concentration1\': p2 + 1,\n                          \'concentration0\': p1 + 1})\n_suff_stat_to_dist[\'categorical\'][((\'#OneHot\', (\'#x\',),),)] = (\n    Categorical, lambda p1: {\'logits\': p1})\n_suff_stat_to_dist[\'nonnegative\'][((\'#Log\', (\'#x\',)),)] = (\n    Chi2, lambda p1: {\'df\': 2.0 * (p1 + 1)})\n_suff_stat_to_dist[\'simplex\'][((\'#Log\', (\'#x\',)),)] = (\n    Dirichlet, lambda p1: {\'concentration\': p1 + 1})\n_suff_stat_to_dist[\'nonnegative\'][((\'#x\',),)] = (\n    Exponential, lambda p1: {\'rate\': -p1})\n_suff_stat_to_dist[\'nonnegative\'][((\'#Log\', (\'#x\',)),\n                                   (\'#x\',))] = (\n    Gamma, lambda p1, p2: {\'concentration\': p1 + 1, \'rate\': -p2})\n_suff_stat_to_dist[\'nonnegative\'][((\'#CPow-1.0000e+00\', (\'#x\',)),\n                                   (\'#Log\', (\'#x\',)))] = (\n    InverseGamma, lambda p1, p2: {\'concentration\': -p2 - 1, \'rate\': -p1})\n_suff_stat_to_dist[\'multivariate_real\'][((\'#CPow2.0000e+00\', (\'#x\',)),\n                                        (\'#x\',))] = (\n    MultivariateNormalDiag, mvn_diag_from_natural_params)\n_suff_stat_to_dist[\'real\'][((\'#CPow2.0000e+00\', (\'#x\',)),\n                            (\'#x\',))] = (\n    Normal, normal_from_natural_params)\n_suff_stat_to_dist[\'countable\'][((\'#x\',),)] = (\n    Poisson, lambda p1: {\'rate\': tf.exp(p1)})\n\n\ndef complete_conditional(rv, cond_set=None):\n  """"""Returns the conditional distribution `RandomVariable`\n  $p(\\\\text{rv}\\mid \\cdot)$.\n\n  This function tries to infer the conditional distribution of `rv`\n  given `cond_set`, a set of other `RandomVariable`s in the graph. It\n  will only be able to do this if\n\n  1. $p(\\\\text{rv}\\mid \\\\text{cond\\_set})$ is in a tractable\n     exponential family; and\n  2. the truth of assumption 1 is not obscured in the TensorFlow graph.\n\n  In other words, this function will do its best to recognize conjugate\n  relationships when they exist. But it may not always be able to do the\n  necessary algebra.\n\n  Args:\n    rv: RandomVariable.\n      The random variable whose conditional distribution we are interested in.\n    cond_set: iterable of RandomVariable.\n      The set of random variables we want to condition on. Default is all\n      random variables in the graph. (It makes no difference if `cond_set`\n      does or does not include `rv`.)\n\n  #### Notes\n\n  When calling `complete_conditional()` multiple times, one should\n  usually pass an explicit `cond_set`. Otherwise\n  `complete_conditional()` will try to condition on the\n  `RandomVariable`s returned by previous calls to itself. This may\n  result in unpredictable behavior.\n  """"""\n  if cond_set is None:\n    # Default to Markov blanket, excluding conditionals. This is useful if\n    # calling complete_conditional many times without passing in cond_set.\n    cond_set = get_blanket(rv)\n    cond_set = [i for i in cond_set if not\n                (\'complete_conditional\' in i.name and \'cond_dist\' in i.name)]\n\n  cond_set = set([rv] + list(cond_set))\n  with tf.name_scope(\'complete_conditional_%s\' % rv.name) as scope:\n    # log_joint holds all the information we need to get a conditional.\n    log_joint = get_log_joint(cond_set)\n\n    # Pull out the nodes that are nonlinear functions of rv into s_stats.\n    stop_nodes = set([i.value() for i in cond_set])\n    subgraph = extract_subgraph(log_joint, stop_nodes)\n    s_stats = suff_stat_nodes(subgraph, rv.value(), cond_set)\n    s_stats = list(set(s_stats))\n\n    # Simplify those nodes, and put any new linear terms into multipliers_i.\n    s_stat_exprs = defaultdict(list)\n    for s_stat in s_stats:\n      expr = symbolic_suff_stat(s_stat, rv.value(), stop_nodes)\n      expr = full_simplify(expr)\n      multipliers_i, s_stats_i = extract_s_stat_multipliers(expr)\n      s_stat_exprs[s_stats_i].append(\n          (s_stat, reconstruct_multiplier(multipliers_i)))\n\n    # Sort out the sufficient statistics to identify this conditional\'s family.\n    s_stat_keys = list(six.iterkeys(s_stat_exprs))\n    order = np.argsort([str(i) for i in s_stat_keys])\n    dist_key = tuple((s_stat_keys[i] for i in order))\n    dist_constructor, constructor_params = (\n        _suff_stat_to_dist[rv.support].get(dist_key, (None, None)))\n    if dist_constructor is None:\n      raise NotImplementedError(\'Conditional distribution has sufficient \'\n                                \'statistics %s, but no available \'\n                                \'exponential-family distribution has those \'\n                                \'sufficient statistics.\' % str(dist_key))\n\n    # Swap sufficient statistics for placeholders, then take gradients\n    # w.r.t. those placeholders to get natural parameters. The original\n    # nodes involving the sufficient statistic nodes are swapped for new\n    # nodes that depend linearly on the sufficient statistic placeholders.\n    s_stat_placeholders = []\n    swap_dict = {}\n    swap_back = {}\n    for s_stat_expr in six.itervalues(s_stat_exprs):\n      if rv.dtype == tf.float64:\n        s_stat_placeholder = tf.placeholder(rv.dtype,\n                                            s_stat_expr[0][0].get_shape())\n        swap_back[s_stat_placeholder] = rv.value()\n      else:\n        s_stat_placeholder = tf.placeholder(tf.float32,\n                                            s_stat_expr[0][0].get_shape())\n        swap_back[s_stat_placeholder] = tf.cast(rv.value(), tf.float32)\n\n      s_stat_placeholders.append(s_stat_placeholder)\n      for s_stat_node, multiplier in s_stat_expr:\n        fake_node = s_stat_placeholder * multiplier\n        swap_dict[s_stat_node] = fake_node\n        swap_back[fake_node] = s_stat_node\n\n    for i in cond_set:\n      if i != rv:\n        val = i.value()\n        val_placeholder = tf.placeholder(val.dtype)\n        swap_dict[val] = val_placeholder\n        swap_back[val_placeholder] = val\n        swap_back[val] = val  # prevent random variable nodes from being copied\n\n    scope_name = scope + str(time.time())  # ensure unique scope when copying\n    log_joint_copy = copy(log_joint, swap_dict, scope=scope_name + \'swap\')\n    nat_params = tf.gradients(log_joint_copy, s_stat_placeholders)\n\n    # Remove any dependencies on those old placeholders.\n    nat_params = [copy(nat_param, swap_back, scope=scope_name + \'swapback\')\n                  for nat_param in nat_params]\n    nat_params = [nat_params[i] for i in order]\n\n    return dist_constructor(name=\'cond_dist\', **constructor_params(*nat_params))\n\n\ndef get_log_joint(cond_set):\n  g = tf.get_default_graph()\n  cond_set_names = [i.name[:-1] for i in cond_set]\n  cond_set_names.sort()\n  cond_set_name = \'log_joint_of_\' + \'_\'.join(cond_set_names)\n  with tf.name_scope(""conjugate_log_joint/"") as scope:\n    try:\n      # Use log joint tensor if already built in graph.\n      return g.get_tensor_by_name(scope + cond_set_name + \':0\')\n    except:\n      pass\n\n    terms = []\n    for b in cond_set:\n      name = b.name.replace(\':\', \'_\') + \'_conjugate_log_prob\'\n      try:\n        # Use log prob tensor if already built in graph.\n        conjugate_log_prob = g.get_tensor_by_name(scope + name + \':0\')\n      except:\n        if getattr(b, ""conjugate_log_prob"", None) is None:\n          raise NotImplementedError(""conjugate_log_prob not implemented for""\n                                    "" {}"".format(type(b)))\n        conjugate_log_prob = tf.reduce_sum(b.conjugate_log_prob(), name=name)\n\n      terms.append(conjugate_log_prob)\n\n    result = tf.add_n(terms, name=cond_set_name)\n    return result\n\n\ndef extract_s_stat_multipliers(expr):\n  if expr[0] != \'#Mul\':\n    return (), expr\n  s_stats = []\n  multipliers = []\n  for i in expr[1:]:\n    if expr_contains(i, \'#x\'):\n      multiplier, s_stat = extract_s_stat_multipliers(i)\n      multipliers += multiplier\n      s_stats += s_stat\n    else:\n      multipliers.append(i)\n  return tuple(multipliers), tuple(s_stats)\n\n\ndef reconstruct_multiplier(multipliers):\n  result = 1.\n  for m in multipliers:\n    result *= reconstruct_expr(m)\n  return result\n\n\ndef extract_subgraph(root, stop_nodes=set()):\n  """"""Copies the TF graph structure into something more pythonic.\n  """"""\n  result = [root]\n  for input in root.op.inputs:\n    if input in stop_nodes:\n      result.append((input,))\n    else:\n      result.append(extract_subgraph(input, stop_nodes))\n  return tuple(result)\n\n\ndef subgraph_leaves(subgraph):\n  """"""Returns a list of leaf nodes from extract_subgraph().\n  """"""\n  if len(subgraph) == 1:\n    return subgraph\n  result = []\n  for input in subgraph[1:]:\n    result += subgraph_leaves(input)\n  return tuple(result)\n\n\ndef is_child(subgraph, node, stop_nodes):\n  if len(subgraph) == 1:\n    return subgraph[0] == node\n  for input in subgraph[1:]:\n    if input not in stop_nodes and is_child(input, node, stop_nodes):\n      return True\n  return False\n\n\n_linear_types = [\'Add\', \'AddN\', \'Sub\', \'Mul\', \'Neg\', \'Identity\', \'Sum\',\n                 \'Assert\', \'Reshape\', \'Slice\', \'StridedSlice\', \'Gather\',\n                 \'GatherNd\', \'Squeeze\', \'Concat\', \'ExpandDims\']\n_n_important_args = {\'Sum\': 1}\n\n\ndef suff_stat_nodes(subgraph, node, stop_nodes):\n  """"""Finds nonlinear nodes depending on `node`.\n  """"""\n  if subgraph[0] == node:\n    return (node,)\n  elif len(subgraph) == 1:\n    return ()\n  op_type = str(subgraph[0].op.type)\n  if op_type in _linear_types:\n    result = []\n    stop_index = _n_important_args.get(op_type, None)\n    stop_index = stop_index + 1 if stop_index is not None else None\n    for input in subgraph[1:stop_index]:\n      result += suff_stat_nodes(input, node, stop_nodes)\n    return tuple(result)\n  else:\n    if is_child(subgraph, node, stop_nodes):\n      return (subgraph[0],)\n    else:\n      return ()\n'"
edward/inferences/conjugacy/conjugate_log_probs.py,39,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom edward.models.random_variables import *\n\n\ndef _val_wrapper(f):\n  def wrapped(self, val=None):\n    if val is None:\n      return f(self, self)\n    else:\n      return f(self, val)\n  return wrapped\n\n\n@_val_wrapper\ndef bernoulli_log_prob(self, val):\n  probs = self.parameters['probs']\n  f_val = tf.cast(val, tf.float32)\n  return f_val * tf.log(probs) + (1.0 - f_val) * tf.log(1.0 - probs)\n\n\n@_val_wrapper\ndef beta_log_prob(self, val):\n  conc0 = self.parameters['concentration0']\n  conc1 = self.parameters['concentration1']\n  result = (conc1 - 1.0) * tf.log(val)\n  result += (conc0 - 1.0) * tf.log(1.0 - val)\n  result += -tf.lgamma(conc1) - tf.lgamma(conc0) + tf.lgamma(conc1 + conc0)\n  return result\n\n\n@_val_wrapper\ndef binomial_log_prob(self, val):\n  n = self.parameters['total_count']\n  probs = self.parameters['probs']\n  f_n = tf.cast(n, tf.float32)\n  f_val = tf.cast(val, tf.float32)\n  result = f_val * tf.log(probs) + (f_n - f_val) * tf.log(1.0 - probs)\n  result += tf.lgamma(f_n + 1) - tf.lgamma(f_val + 1) - \\\n      tf.lgamma(f_n - f_val + 1)\n  return result\n\n\n@_val_wrapper\ndef categorical_log_prob(self, val):\n  probs = self.parameters['probs']\n  one_hot = tf.one_hot(val, probs.shape[-1], dtype=tf.float32)\n  return tf.reduce_sum(tf.log(probs) * one_hot, -1)\n\n\n@_val_wrapper\ndef chi2_log_prob(self, val):\n  df = self.parameters['df']\n  eta = 0.5 * df - 1\n  result = tf.reduce_sum(eta * tf.log(val), -1)\n  result += tf.exp(-0.5 * val)\n  result -= tf.lgamma(eta + 1) + (eta + 1) * tf.log(2.0)\n  return result\n\n\n@_val_wrapper\ndef dirichlet_log_prob(self, val):\n  conc = self.parameters['concentration']\n  result = tf.reduce_sum((conc - 1.0) * tf.log(val), -1)\n  result += tf.reduce_sum(-tf.lgamma(conc), -1)\n  result += tf.lgamma(tf.reduce_sum(conc, -1))\n  return result\n\n\n@_val_wrapper\ndef exponential_log_prob(self, val):\n  rate = self.parameters['rate']\n  result = tf.log(rate) - rate * val\n  return result\n\n\n@_val_wrapper\ndef gamma_log_prob(self, val):\n  conc = self.parameters['concentration']\n  rate = self.parameters['rate']\n  result = (conc - 1.0) * tf.log(val)\n  result -= rate * val\n  result += -tf.lgamma(conc) + conc * tf.log(rate)\n  return result\n\n\n@_val_wrapper\ndef inverse_gamma_log_prob(self, val):\n  conc = self.parameters['concentration']\n  rate = self.parameters['rate']\n  result = -(conc + 1) * tf.log(val)\n  result -= rate * tf.reciprocal(val)\n  result += -tf.lgamma(conc) + conc * tf.log(rate)\n  return result\n\n\n@_val_wrapper\ndef laplace_log_prob(self, val):\n  loc = self.parameters['loc']\n  scale = self.parameters['scale']\n  f_val = tf.cast(val, tf.float32)\n  result = -tf.log(2.0 * scale) - tf.abs(f_val - loc) / scale\n  return result\n\n\n@_val_wrapper\ndef multinomial_log_prob(self, val):\n  n = self.parameters['total_count']\n  probs = self.parameters['probs']\n  f_n = tf.cast(n, tf.float32)\n  f_val = tf.cast(val, tf.float32)\n  result = tf.reduce_sum(tf.log(probs) * f_val, -1)\n  result += tf.lgamma(f_n + 1) - tf.reduce_sum(tf.lgamma(f_val + 1), -1)\n  return result\n\n\n@_val_wrapper\ndef mvn_diag_log_prob(self, val):\n  loc = self.parameters['loc']\n  scale_diag = self.parameters['scale_diag']\n  prec = tf.reciprocal(tf.square(scale_diag))\n  result = prec * (-0.5 * tf.square(val) - 0.5 * tf.square(loc) +\n                   val * loc)\n  result -= tf.log(scale_diag) + 0.5 * tf.log(2 * np.pi)\n  return result\n\n\n@_val_wrapper\ndef normal_log_prob(self, val):\n  loc = self.parameters['loc']\n  scale = self.parameters['scale']\n  prec = tf.reciprocal(tf.square(scale))\n  result = prec * (-0.5 * tf.square(val) - 0.5 * tf.square(loc) +\n                   val * loc)\n  result -= tf.log(scale) + 0.5 * tf.cast(tf.log(2 * np.pi), dtype=result.dtype)\n  return result\n\n\n@_val_wrapper\ndef poisson_log_prob(self, val):\n  rate = self.parameters['rate']\n  f_val = tf.cast(val, tf.float32)\n  result = f_val * tf.log(rate)\n  result += -rate - tf.lgamma(f_val + 1)\n  return result\n\n\nBernoulli.conjugate_log_prob = bernoulli_log_prob\nBeta.conjugate_log_prob = beta_log_prob\nBinomial.conjugate_log_prob = binomial_log_prob\nCategorical.conjugate_log_prob = categorical_log_prob\nChi2.conjugate_log_prob = chi2_log_prob\nDirichlet.conjugate_log_prob = dirichlet_log_prob\nExponential.conjugate_log_prob = exponential_log_prob\nGamma.conjugate_log_prob = gamma_log_prob\nInverseGamma.conjugate_log_prob = inverse_gamma_log_prob\nLaplace.conjugate_log_prob = laplace_log_prob\nMultinomial.conjugate_log_prob = multinomial_log_prob\nMultivariateNormalDiag.conjugate_log_prob = mvn_diag_log_prob\nNormal.conjugate_log_prob = normal_log_prob\nPoisson.conjugate_log_prob = poisson_log_prob\n"""
edward/inferences/conjugacy/simplify.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _mul_n(x):\n  if len(x) == 2:\n    return tf.multiply(x[0], x[1])\n  else:\n    return tf.multiply(x[0], _mul_n(x[1:]))\n\n\n_extractable_nodes = {\n    \'Add\': tf.add_n,\n    \'Sub\': tf.subtract,\n    \'Mul\': _mul_n,\n    \'Log\': tf.log,\n    \'Exp\': tf.exp,\n    \'Pow\': tf.pow,\n    \'Square\': tf.square,\n    \'Reciprocal\': tf.reciprocal,\n    \'Sqrt\': tf.sqrt,\n    \'Identity\': lambda x: x,\n    \'One_minus\': lambda x: 1 - x,\n    # Makes some assumptions.\n    \'OneHot\': lambda x: tf.one_hot(x, tf.reduce_max(x) + 1, dtype=tf.float32)\n}\n\n\ndef symbolic_suff_stat(node, base_node, stop_nodes):\n  """"""Extracts a symbolic representation of the graph rooted at `node`.\n  """"""\n  if node == base_node:\n    return (\'#x\',)\n  elif node in stop_nodes:\n    return (node,)\n\n  if node.op.type in _extractable_nodes:\n    result = [\'#%s\' % str(node.op.type)]\n  else:\n    result = [node]\n\n  result += [symbolic_suff_stat(i, base_node, stop_nodes)\n             for i in node.op.inputs]\n  return tuple(result)\n\n\ndef is_number(x):\n  if isinstance(x, tf.Tensor):\n    return True\n  try:\n    float(x)\n    return True\n  except:\n    return False\n\n\ndef reconstruct_expr(expr):\n  if is_number(expr[0]):\n    return expr[0]\n  if expr[0] == \'#x\':\n    raise ValueError(\'#x cannot appear in expr to be reconstructed.\')\n  args = [reconstruct_expr(i) for i in expr[1:]]\n  if str(expr[0])[:5] == \'#CPow\':\n    return tf.pow(args[0], np.float32(expr[0][5:]))\n  if expr[0][0] == \'#\':\n    tf_fn = _extractable_nodes.get(expr[0][1:], None)\n    assert(tf_fn is not None)\n    return tf_fn(*args)\n  assert(False)\n\n\n_simplify_fns = []\n\n\ndef full_simplify(expr, simplify_fns=_simplify_fns):\n  while True:\n    did_something = False\n    for fn in simplify_fns:\n      did_something_i, expr = fn(expr)\n      did_something = did_something or did_something_i\n    if not did_something:\n      break\n  return expr\n\n\ndef _register_simplify_fn(fn):\n  """"""Wraps and registers simplification functions.\n\n  A simplification function takes as input an expression and possible\n  some other args/kwargs, and returns either None (if it did not find\n  anything to do at this node) or a simplified version of the graph\n  below this node.\n\n  The wrapped function will repeatedly apply this simplification\n  function to all nodes of the graph until it stops doing anything.\n  """"""\n  def wrapped(expr, *args, **kwargs):\n    result = fn(expr, *args, **kwargs)\n    if result is None:\n      did_something = False\n      new_args = []\n      for i in expr[1:]:\n        did_something_i, new_arg = wrapped(i)\n        did_something = did_something or did_something_i\n        new_args.append(new_arg)\n      return did_something, (expr[0],) + tuple(new_args)\n    else:\n      return True, result\n\n  def repeat_wrapped(expr, *args, **kwargs):\n    did_something = False\n    did_something_i = True\n    while did_something_i:\n      did_something_i, expr = wrapped(expr, *args, **kwargs)\n      did_something = did_something or did_something_i\n    return did_something, expr\n\n  _simplify_fns.append(repeat_wrapped)\n  return repeat_wrapped\n\n\n@_register_simplify_fn\ndef identity_op_simplify(expr):\n  if expr[0] == \'#Identity\':\n    return expr[1]\n\n\n_power_ops = {\n    \'#Reciprocal\': -1.,\n    \'#Square\': 2.,\n    \'#Sqrt\': 0.5,\n}\n\n\n@_register_simplify_fn\ndef power_op_simplify(expr):\n  op_power = _power_ops.get(expr[0], None)\n  if op_power:\n    return (\'#CPow%.4e\' % op_power,) + expr[1:]\n\n\n@_register_simplify_fn\ndef pow_simplify(expr):\n  if str(expr[0])[:5] != \'#CPow\':\n    return None\n  if str(expr[1][0])[:5] != \'#CPow\':\n    return None\n\n  op_power = float(expr[0][5:])\n  sub_power = float(expr[1][0][5:])\n  new_power = sub_power * op_power\n  if new_power == 1.:\n    return expr[1][1]\n  else:\n    return (\'#CPow%.4e\' % new_power, expr[1][1])\n\n\n@_register_simplify_fn\ndef log_pow_simplify(expr):\n  if expr[0] == \'#Log\' and str(expr[1][0])[:5] == \'#CPow\':\n    return (\'#Mul\', (float(expr[1][0][5:]),), (\'#Log\', expr[1][1]))\n  if expr[0] == \'#Log\' and expr[1][0] == \'#Pow\':\n    return (\'#Mul\', expr[1][2], (\'#Log\', expr[1][1]))\n\n\n@_register_simplify_fn\ndef log_mul_simplify(expr):\n  if expr[0] == \'#Log\' and expr[1][0] == \'#Mul\':\n    return (\'#Add\',) + tuple((\'#Log\', i) for i in expr[1][1:])\n\n\n@_register_simplify_fn\ndef pow_mul_simplify(expr):\n  if str(expr[0])[:5] == \'#CPow\' and expr[1][0] == \'#Mul\':\n    return (\'#Mul\',) + tuple(((expr[0], i) for i in expr[1][1:]))\n  if expr[0] == \'#Pow\' and expr[1][0] == \'#Mul\':\n    return (\'#Mul\',) + tuple(((\'#Pow\', i, expr[2]) for i in expr[1][1:]))\n\n\n@_register_simplify_fn\ndef mul_add_simplify(expr):\n  """"""Turns Mul(Add(.), .) into Add(Mul(.), Mul(.),...)""""""\n  if expr[0] != \'#Mul\':\n    return None\n  for i in range(1, len(expr)):\n    if expr[i][0] == \'#Add\':\n      other_args = expr[1:i] + expr[i + 1:]\n      return (\'#Add\',) + tuple(((\'#Mul\',) + other_args + (j,)\n                                for j in expr[i][1:]))\n\n\ndef commutative_simplify(expr, op_name):\n  if expr[0] != op_name:\n    return None\n  new_args = []\n  did_something = False\n  for i in expr[1:]:\n    if i[0] == op_name:\n      new_args += i[1:]\n      did_something = True\n    else:\n      new_args.append(i)\n  if did_something:\n    return (op_name,) + tuple(new_args)\n\n\n@_register_simplify_fn\ndef add_add_simplify(expr):\n  return commutative_simplify(expr, \'#Add\')\n\n\n@_register_simplify_fn\ndef mul_mul_simplify(expr):\n  return commutative_simplify(expr, \'#Mul\')\n\n\ndef identity_simplify(expr, op_name, identity_val):\n  if expr[0] != op_name:\n    return None\n  if len(expr) == 2:\n    return expr[1]\n  new_args = []\n  did_something = False\n  for i in expr[1:]:\n    if i[0] != identity_val:\n      new_args.append(i)\n    else:\n      did_something = True\n  if did_something:\n    if len(new_args) > 1:\n      return (op_name,) + tuple(new_args)\n    else:\n      return new_args[0]\n\n\n@_register_simplify_fn\ndef mul_one_simplify(expr):\n  return identity_simplify(expr, \'#Mul\', 1)\n\n\n@_register_simplify_fn\ndef add_zero_simplify(expr):\n  return identity_simplify(expr, \'#Add\', 0)\n\n\n@_register_simplify_fn\ndef mul_zero_simplify(expr):\n  if expr[0] != \'#Mul\':\n    return None\n  for i in expr[1:]:\n    if i[0] == 0:\n      return (0,)\n\n\n@_register_simplify_fn\ndef square_add_simplify(expr):\n  if not (expr[0] == \'#CPow2.0000e+00\' and expr[1][0] == \'#Add\'):\n    return None\n  terms = []\n  for i in range(1, len(expr[1])):\n    terms.append((\'#CPow2.0000e+00\', expr[1][i]))\n    for j in range(i + 1, len(expr[1])):\n      terms.append((\'#Mul\', (2.0,), expr[1][i], expr[1][j]))\n  return (\'#Add\',) + tuple(terms)\n\n\ndef expr_contains(expr, node_type):\n  if expr[0] == node_type:\n    return True\n  for i in expr[1:]:\n    if expr_contains(i, node_type):\n      return True\n  return False\n\n\n@_register_simplify_fn\ndef add_const_simplify(expr):\n  """"""Prunes branches not containing any #x nodes.""""""\n  if expr[0] != \'#Add\':\n    return None\n  did_something = False\n  new_args = []\n  for i in range(1, len(expr)):\n    if expr_contains(expr[i], \'#x\'):\n      new_args.append(expr[i])\n    else:\n      did_something = True\n  if did_something:\n    return (\'#Add\',) + tuple(new_args)\n\n\n@_register_simplify_fn\ndef one_m_simplify(expr):\n  """"""Replaces (""#Sub"", (<wrapped constant 1>,), (.)) with (""#One_minus"", .).""""""\n  if expr[0] != \'#Sub\' or not isinstance(expr[1][0], tf.Tensor):\n    return None\n  value = tf.contrib.util.constant_value(expr[1][0].op.outputs[0])\n  if value == 1.0:\n    return (\'#One_minus\', expr[2])\n\n\n@_register_simplify_fn\ndef cast_simplify(expr):\n  """"""Replaces (<wrapped cast>, (.)) with (.).""""""\n  if isinstance(expr[0], tf.Tensor) and expr[0].op.type == \'Cast\':\n    return expr[1]\n\n\n@_register_simplify_fn\ndef onehot_simplify(expr):\n  """"""Gets rid of extraneous args to OneHot.""""""\n  if expr[0] == \'#OneHot\' and len(expr) > 2:\n    return expr[:2]\n'"
