file_path,api_count,code
assignment1/q1_softmax.py,0,"b'import numpy as np\nimport random\n\ndef softmax(x):\n    """"""\n    Compute the softmax function for each row of the input x.\n\n    It is crucial that this function is optimized for speed because\n    it will be used frequently in later code.\n    You might find numpy functions np.exp, np.sum, np.reshape,\n    np.max, and numpy broadcasting useful for this task. (numpy\n    broadcasting documentation:\n    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n    You should also make sure that your code works for one\n    dimensional inputs (treat the vector as a row), you might find\n    it helpful for your later problems.\n\n    You must implement the optimization in problem 1(a) of the \n    written assignment!\n    """"""\n    assert len(x.shape) <= 2\n    y = np.exp(x - np.max(x, axis=len(x.shape) - 1, keepdims=True))\n    normalization = np.sum(y, axis=len(x.shape) - 1, keepdims=True)\n    return np.divide(y, normalization)\n\ndef test_softmax_basic():\n    """"""\n    Some simple tests to get you started. \n    Warning: these are not exhaustive.\n    """"""\n    print ""Running basic tests...""\n    test1 = softmax(np.array([1,2]))\n    print test1\n    assert np.amax(np.fabs(test1 - np.array(\n        [0.26894142,  0.73105858]))) <= 1e-6\n\n    test2 = softmax(np.array([[1001,1002],[3,4]]))\n    print test2\n    assert np.amax(np.fabs(test2 - np.array(\n        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n\n    test3 = softmax(np.array([[-1001,-1002]]))\n    print test3\n    assert np.amax(np.fabs(test3 - np.array(\n        [0.73105858, 0.26894142]))) <= 1e-6\n\n    print ""You should verify these results!\\n""\n\ndef test_softmax():\n    """""" \n    Use this space to test your softmax implementation by running:\n        python q1_softmax.py \n    This function will not be called by the autograder, nor will\n    your tests be graded.\n    """"""\n    print ""Running your tests...""\n\nif __name__ == ""__main__"":\n    test_softmax_basic()\n    test_softmax()\n'"
assignment1/q2_gradcheck.py,0,"b'import numpy as np\nimport random\n\nfrom q2_sigmoid import sigmoid, sigmoid_grad\n\n# First implement a gradient checker by filling in the following functions\ndef gradcheck_naive(f_and_grad, x):\n    """""" \n    Gradient check for a function f\n    - f_and_grad should be a function that takes a single argument and outputs the cost and its gradients\n    - x is the point (numpy array) to check the gradient at\n    """""" \n\n    rndstate = random.getstate()\n    random.setstate(rndstate)  \n    fx, grad = f_and_grad(x) # Evaluate function value at original point\n\n    y = np.copy(x)\n    # Iterate over all indexes in x\n    it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n    while not it.finished:\n        ix = it.multi_index\n\n        ### try modifying x[ix] with h defined above to compute numerical gradients\n        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n        ### possible to test cost functions with built in randomness later\n        reldiff = 1.0\n        for negative_log_h in xrange(2, 22):\n          h = 0.5 ** negative_log_h\n          y[ix] = x[ix] + h\n          random.setstate(rndstate)\n          fy, _ = f_and_grad(y)\n          y[ix] = x[ix]\n          numgrad = (fy - fx) / h\n          if fx != fy:\n            reldiff = min(reldiff, abs(numgrad - grad[ix]) / max((1.0, abs(numgrad), abs(grad[ix]))))\n\n        # Compare gradients\n        print \'reldiff\', reldiff\n        if reldiff > 1e-5:\n            print ""Gradient check failed.""\n            print ""First gradient error found at index %s"" % str(ix)\n            print ""Your gradient: %f \\t Numerical gradient: %f"" % (grad[ix], numgrad)\n            return\n    \n        it.iternext() # Step to next dimension\n\n    print ""Gradient check passed!""\n\ndef sanity_check():\n    """"""\n    Some basic sanity checks.\n    """"""\n    quad_and_grad = lambda x: (np.sum(x ** 2), x * 2)\n\n    print ""Running sanity checks...""\n    gradcheck_naive(quad_and_grad, np.array(123.456))      # scalar test\n    gradcheck_naive(quad_and_grad, np.random.randn(3,))    # 1-D test\n    gradcheck_naive(quad_and_grad, np.random.randn(4,5))   # 2-D test\n    print """"\n\ndef your_sanity_checks(): \n    """"""\n    Use this space add any additional sanity checks by running:\n        python q2_gradcheck.py\n    This function will not be called by the autograder, nor will\n    your additional tests be graded.\n    """"""\n    print ""Running your sanity checks...""\n    sigmoid_and_grad = lambda x: (np.sum(sigmoid(x)), sigmoid_grad(sigmoid(x)))\n    gradcheck_naive(sigmoid_and_grad, np.array(1.23456))      # scalar test\n    gradcheck_naive(sigmoid_and_grad, np.random.randn(3,))    # 1-D test\n    gradcheck_naive(sigmoid_and_grad, np.random.randn(4,5))   # 2-D test\n    gradcheck_naive(sigmoid_and_grad, np.arange(-5.0, 5.0, 0.1))   # range test\n    sincos_and_grad = lambda x: (np.sin(x) + np.cos(x), np.cos(x) - np.sin(x))\n\n    gradcheck_naive(sincos_and_grad, np.array(1.0))\n\n    print\n\nif __name__ == ""__main__"":\n    sanity_check()\n    your_sanity_checks()\n'"
assignment1/q2_neural.py,0,"b'import numpy as np\nimport random\n\nfrom q1_softmax import softmax\nfrom q2_sigmoid import sigmoid, sigmoid_grad\nfrom q2_gradcheck import gradcheck_naive\n\ndef forward_backward_prop(data, labels, params, dimensions):\n    """""" \n    Forward and backward propagation for a two-layer sigmoidal network \n    \n    Compute the forward propagation and for the cross entropy cost,\n    and backward propagation for the gradients for all parameters.\n    """"""\n\n    ### Unpack network parameters (do not modify)\n    ofs = 0\n    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n\n    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n    ofs += Dx * H\n    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n    ofs += H\n    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n    ofs += H * Dy\n    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n\n    ### YOUR CODE HERE: forward propagation\n    h_per_item = sigmoid(np.dot(data, W1) + b1)\n    yhat_per_item = softmax(np.dot(h_per_item, W2) + b2)\n    cost = -np.sum(labels * np.log(yhat_per_item))\n    ### END YOUR CODE\n\n    ### YOUR CODE HERE: backward propagation\n    grad_softmax_per_item = yhat_per_item - labels\n    grad_b2 = np.sum(grad_softmax_per_item, axis=0, keepdims=True)\n    grad_W2 = np.dot(h_per_item.T, grad_softmax_per_item)\n    grad_sigmoid_per_item = sigmoid_grad(h_per_item)\n    grad_b1_per_item = np.dot(grad_softmax_per_item, W2.T) * grad_sigmoid_per_item\n    grad_b1 = np.sum(grad_b1_per_item, axis=0, keepdims=True)\n    grad_W1 = np.dot(data.T, grad_b1_per_item)\n    ### END YOUR CODE\n    assert grad_b2.shape == b2.shape\n    assert grad_W2.shape == W2.shape\n    assert grad_b1.shape == b1.shape\n    assert grad_W1.shape == W1.shape\n    \n    ### Stack gradients (do not modify)\n    grad = np.concatenate((grad_W1.flatten(), grad_b1.flatten(), \n        grad_W2.flatten(), grad_b2.flatten()))\n    \n    return cost, grad\n\ndef sanity_check():\n    """"""\n    Set up fake data and parameters for the neural network, and test using \n    gradcheck.\n    """"""\n    print ""Running sanity check...""\n\n    N = 20\n    dimensions = [10, 5, 10]\n    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n    labels = np.zeros((N, dimensions[2]))\n    for i in xrange(N):\n        labels[i,random.randint(0,dimensions[2]-1)] = 1\n    \n    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n        dimensions[1] + 1) * dimensions[2], )\n\n    gradcheck_naive(lambda params: forward_backward_prop(data, labels, params,\n        dimensions), params)\n\ndef your_sanity_checks(): \n    """"""\n    Use this space add any additional sanity checks by running:\n        python q2_neural.py \n    This function will not be called by the autograder, nor will\n    your additional tests be graded.\n    """"""\n    print ""Running your sanity checks...""\n    ### YOUR CODE HERE\n    raise NotImplementedError\n    ### END YOUR CODE\n\nif __name__ == ""__main__"":\n    sanity_check()\n    your_sanity_checks()\n'"
assignment1/q2_sigmoid.py,0,"b'import numpy as np\nfrom scipy.special import expit\n\ndef sigmoid(x):\n    """"""\n    Compute the sigmoid function for the input here.\n    """"""\n    return expit(x)\n\ndef sigmoid_grad(f):\n    """"""\n    Compute the gradient for the sigmoid function here. Note that\n    for this implementation, the input f should be the sigmoid\n    function value of your original input x. \n    """"""\n    return f - f * f\n\ndef test_sigmoid_basic():\n    """"""\n    Some simple tests to get you started. \n    Warning: these are not exhaustive.\n    """"""\n    print ""Running basic tests...""\n    x = np.array([[1, 2], [-1, -2]])\n    f = sigmoid(x)\n    g = sigmoid_grad(f)\n    print f\n    assert np.amax(f - np.array([[0.73105858, 0.88079708], \n        [0.26894142, 0.11920292]])) <= 1e-6\n    print g\n    assert np.amax(g - np.array([[0.19661193, 0.10499359],\n        [0.19661193, 0.10499359]])) <= 1e-6\n    print ""You should verify these results!\\n""\n\ndef test_sigmoid(): \n    """"""\n    Use this space to test your sigmoid implementation by running:\n        python q2_sigmoid.py \n    This function will not be called by the autograder, nor will\n    your tests be graded.\n    """"""\n    print ""Running your tests...""\n    ### YOUR CODE HERE\n    raise NotImplementedError\n    ### END YOUR CODE\n\nif __name__ == ""__main__"":\n    test_sigmoid_basic();\n    test_sigmoid()\n'"
assignment1/q3_run.py,0,"b'import random\nimport numpy as np\nfrom cs224d.data_utils import *\nimport matplotlib.pyplot as plt\n\nfrom q3_word2vec import *\nfrom q3_sgd import *\n\n# Reset the random seed to make sure that everyone gets the same results\nrandom.seed(314)\ndataset = StanfordSentiment()\ntokens = dataset.tokens()\nnWords = len(tokens)\n\n# We are going to train 10-dimensional vectors for this assignment\ndimVectors = 10\n\n# Context size\nC = 5\n\ndef train_word2vec_embeddings():\n  # Reset the random seed to make sure that everyone gets the same results\n  random.seed(31415)\n  np.random.seed(9265)\n  wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / \\\n          dimVectors, np.zeros((nWords, dimVectors))), axis=0)\n  wordVectors0 = sgd(\n      lambda vec: word2vec_sgd_wrapper(cbow, tokens, vec, dataset, C, \n          negSamplingCostAndGradient), \n      wordVectors, 0.3, 70000, None, True, PRINT_EVERY=10)\n  print ""sanity check: cost at convergence should be around or below 10""\n\n  # sum the input and output word vectors\n  wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n\ndef visualize_word2vec_embeddings():\n  # Visualize the word vectors you trained\n  _, wordVectors0, _ = load_saved_params()\n  wordVectors = wordVectors0[:nWords,:] + wordVectors0[nWords:,:]\n  visualizeWords = [""the"", ""a"", ""an"", "","", ""."", ""?"", ""!"", ""``"", ""\'\'"", ""--"", \n          ""good"", ""great"", ""cool"", ""brilliant"", ""wonderful"", ""well"", ""amazing"",\n          ""worth"", ""sweet"", ""enjoyable"", ""boring"", ""bad"", ""waste"", ""dumb"", \n          ""annoying""]\n  visualizeIdx = [tokens[word] for word in visualizeWords]\n  visualizeVecs = wordVectors[visualizeIdx, :]\n  temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n  covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n  U,S,V = np.linalg.svd(covariance)\n  coord = temp.dot(U[:,0:2]) \n\n  for i in xrange(len(visualizeWords)):\n      plt.text(coord[i,0], coord[i,1], visualizeWords[i], \n          bbox=dict(facecolor=\'green\', alpha=0.1))\n      \n  plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n  plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n\n  plt.savefig(\'q3_cbow.png\')\n  plt.show()\n\nif __name__ == \'__main__\':\n  #train_word2vec_embeddings()\n  visualize_word2vec_embeddings()\n'"
assignment1/q3_sgd.py,0,"b'# Save parameters every a few SGD iterations as fail-safe\nSAVE_PARAMS_EVERY = 1000\n\nimport glob\nimport random\nimport numpy as np\nimport os.path as op\nimport cPickle as pickle\n\ndef load_saved_params():\n    """""" A helper function that loads previously saved parameters and resets iteration start """"""\n    st = 0\n    for f in glob.glob(""saved_params_*.npy""):\n        iter = int(op.splitext(op.basename(f))[0].split(""_"")[2])\n        if (iter > st):\n            st = iter\n    if st > 0:\n        print ""Loading saved params %d"" % st\n        with open(""saved_params_%d.npy"" % st, ""r"") as f:\n            params = pickle.load(f)\n            state = pickle.load(f)\n        return st, params, state\n    else:\n        return st, None, None\n    \ndef save_params(iter, params):\n    with open(""saved_params_%d.npy"" % iter, ""w"") as f:\n        pickle.dump(params, f)\n        pickle.dump(random.getstate(), f)\n\ndef sgd(f_and_grad, x0, step, iterations, postprocess = None, useSaved = False, PRINT_EVERY=10):\n    """""" Stochastic Gradient Descent """"""\n    # Implement the stochastic gradient descent method in this        \n    # function.                                                       \n    \n    # Inputs:                                                         \n    # - f_and_grad: the function to optimize, it should take a single   \n    #     argument and yield two outputs, a cost and the gradient  \n    #     with respect to the arguments                            \n    # - x0: the initial point to start SGD from                     \n    # - step: the step size for SGD                                 \n    # - iterations: total iterations to run SGD for                 \n    # - postprocess: postprocessing function for the parameters  \n    #     if necessary. In the case of word2vec we will need to    \n    #     normalize the word vectors to have unit length.          \n    # - PRINT_EVERY: specifies every how many iterations to output  \n\n    # Output:                                                         \n    # - x: the parameter value after SGD finishes  \n    \n    # Anneal learning rate every several iterations\n    ANNEAL_EVERY = 20000\n    \n    if useSaved:\n        start_iter, oldx, state = load_saved_params()\n        if start_iter > 0:\n            x0 = oldx;\n            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n            \n        if state:\n            random.setstate(state)\n    else:\n        start_iter = 0\n    \n    x = x0\n    \n    if not postprocess:\n        postprocess = lambda x: x\n\n    expcost = None\n    \n    for iter in xrange(start_iter + 1, iterations + 1):\n        ### Don\'t forget to apply the postprocessing after every iteration!\n        ### You might want to print the progress every few iterations.\n      \n        cost, grad = f_and_grad(x)\n        x = postprocess(x - step * grad)\n\n        if iter % PRINT_EVERY == 0:\n            if not expcost:\n                expcost = cost\n            else:\n                expcost = .95 * expcost + .05 * cost\n            print ""iter %d: %f"" % (iter, expcost)\n        \n        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n            save_params(iter, x)\n        \n        if iter % ANNEAL_EVERY == 0:\n            step *= 0.5\n    \n    return x\n\ndef sanity_check():\n    quad = lambda x: (np.sum(x ** 2), x * 2)\n\n    print ""Running sanity checks...""\n    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n    print ""test 1 result:"", t1\n    assert abs(t1) <= 1e-6\n\n    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n    print ""test 2 result:"", t2\n    assert abs(t2) <= 1e-6\n\n    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n    print ""test 3 result:"", t3\n    assert abs(t3) <= 1e-6\n    \n    print """"\n\ndef your_sanity_checks(): \n    """"""\n    Use this space add any additional sanity checks by running:\n        python q3_sgd.py \n    This function will not be called by the autograder, nor will\n    your additional tests be graded.\n    """"""\n    print ""Running your sanity checks...""\n\nif __name__ == ""__main__"":\n    sanity_check();\n    your_sanity_checks();\n'"
assignment1/q3_word2vec.py,0,"b'import numpy as np\nimport random\n\nfrom q1_softmax import softmax\nfrom q2_gradcheck import gradcheck_naive\nfrom q2_sigmoid import sigmoid, sigmoid_grad\n\ndef normalizeRows(x):\n    """""" Row normalization function """"""\n    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n\ndef test_normalize_rows():\n    print ""Testing normalizeRows...""\n    x = normalizeRows(np.array([[3.0,4.0],[1, 2]])) \n    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n    print x\n    assert (np.amax(np.fabs(x - np.array([[0.6,0.8],[0.4472136,0.89442719]]))) <= 1e-6)\n\ndef softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n    """""" Softmax cost function for word2vec models """"""\n    \n    # Implement the cost and gradients for one predicted word vector  \n    # and one target word vector as a building block for word2vec     \n    # models, assuming the softmax prediction function and cross      \n    # entropy loss.                                                   \n    \n    # Inputs:                                                         \n    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in \n    #   the written component or \\hat{r} in an earlier version)\n    # - target: integer, the index of the target word               \n    # - outputVectors: ""output"" vectors (as rows) for all tokens     \n    # - dataset: needed for negative sampling, unused here.         \n    \n    # Outputs:                                                        \n    # - cost: cross entropy cost for the softmax word prediction    \n    # - gradPred: the gradient with respect to the predicted word   \n    #        vector                                                \n    # - grad: the gradient with respect to all the other word        \n    #        vectors                                               \n    \n    # We will not provide starter code for this function, but feel    \n    # free to reference the code you previously wrote for this        \n    # assignment!                                                  \n\n    assert predicted.shape[-1] == outputVectors.shape[-1]\n    scalar_products = np.sum(outputVectors * predicted, axis=1)\n    yhat = softmax(scalar_products)\n    cost = -np.log(yhat[target])\n    gradPred = np.sum(outputVectors * yhat[:, np.newaxis], axis=0) - outputVectors[target]\n    grad = yhat[:, np.newaxis] * predicted[np.newaxis, :]\n    grad[target] = grad[target] - predicted\n    return cost, gradPred, grad\n\ndef negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n    K=10):\n    """""" Negative sampling cost function for word2vec models """"""\n\n    # Implement the cost and gradients for one predicted word vector  \n    # and one target word vector as a building block for word2vec     \n    # models, using the negative sampling technique. K is the sample  \n    # size. You might want to use dataset.sampleTokenIdx() to sample  \n    # a random word index. \n    # \n    # Note: See test_word2vec below for dataset\'s initialization.\n    #                                       \n    # Input/Output Specifications: same as softmaxCostAndGradient     \n    # We will not provide starter code for this function, but feel    \n    # free to reference the code you previously wrote for this        \n    # assignment!\n\n    assert predicted.shape[-1] == outputVectors.shape[-1]\n    negative_samples = [dataset.sampleTokenIdx() for k in xrange(K)]\n    cost = -np.log(sigmoid(np.dot(outputVectors[target], predicted))) \\\n           -sum(np.log(sigmoid(-np.dot(outputVectors[k], predicted)))\n                for k in negative_samples)\n    gradPred = (sigmoid(np.dot(outputVectors[target], predicted)) - 1.0) * outputVectors[target] + \\\n                sum((1.0 - sigmoid(-np.dot(outputVectors[k], predicted))) * outputVectors[k]\n                    for k in negative_samples)\n    grad = np.zeros_like(outputVectors)\n    grad[target] += (sigmoid(np.dot(outputVectors[target], predicted)) - 1.0) * predicted\n    for k in negative_samples:\n      grad[k] += (1.0 - sigmoid(-np.dot(outputVectors[k], predicted))) * predicted\n    return cost, gradPred, grad\n\ndef skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n    """""" Skip-gram model in word2vec """"""\n\n    # Implement the skip-gram model in this function.\n\n    # Inputs:                                                         \n    # - currrentWord: a string of the current center word           \n    # - C: integer, context size                                    \n    # - contextWords: list of no more than 2*C strings, the context words                                               \n    # - tokens: a dictionary that maps words to their indices in    \n    #      the word vector list                                \n    # - inputVectors: ""input"" word vectors (as rows) for all tokens           \n    # - outputVectors: ""output"" word vectors (as rows) for all tokens         \n    # - word2vecCostAndGradient: the cost and gradient function for \n    #      a prediction vector given the target word vectors,  \n    #      could be one of the two cost functions you          \n    #      implemented above\n\n    # Outputs:                                                        \n    # - cost: the cost function value for the skip-gram model       \n    # - grad: the gradient with respect to the word vectors         \n    # We will not provide starter code for this function, but feel    \n    # free to reference the code you previously wrote for this        \n    # assignment!\n\n    cost = 0.0\n    gradIn = np.zeros_like(inputVectors)\n    gradOut = np.zeros_like(outputVectors)\n\n    center = tokens[currentWord]\n    for context_word in contextWords:\n      target = tokens[context_word]\n      cur_cost, cur_grad_predicted, cur_grad_out = \\\n          word2vecCostAndGradient(inputVectors[center], target, outputVectors, dataset)\n      cost += cur_cost\n      gradIn[center] += cur_grad_predicted\n      gradOut += cur_grad_out\n\n    return cost, gradIn, gradOut\n\ndef cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n    """""" CBOW model in word2vec """"""\n\n    # Implement the continuous bag-of-words model in this function.            \n    # Input/Output specifications: same as the skip-gram model        \n    # We will not provide starter code for this function, but feel    \n    # free to reference the code you previously wrote for this        \n    # assignment!\n\n    #################################################################\n    # IMPLEMENTING CBOW IS EXTRA CREDIT, DERIVATIONS IN THE WRIITEN #\n    # ASSIGNMENT ARE NOT!                                           #  \n    #################################################################\n  \n    cost = 0.0\n    gradIn = np.zeros(inputVectors.shape)\n    gradOut = np.zeros(outputVectors.shape)\n\n    center = tokens[currentWord]\n    vhat = sum(inputVectors[tokens[j]] for j in contextWords)\n    cost, grad_predicted, gradOut = word2vecCostAndGradient(vhat, center, outputVectors, dataset)\n    for j in contextWords:\n      gradIn[tokens[j]] += grad_predicted\n\n    return cost, gradIn, gradOut\n\n#############################################\n# Testing functions below. DO NOT MODIFY!   #\n#############################################\n\ndef word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n    batchsize = 50\n    cost = 0.0\n    grad = np.zeros(wordVectors.shape)\n    N = wordVectors.shape[0]\n    inputVectors = wordVectors[:N/2,:]\n    outputVectors = wordVectors[N/2:,:]\n    for i in xrange(batchsize):\n        C1 = random.randint(1,C)\n        centerword, context = dataset.getRandomContext(C1)\n        \n        if word2vecModel == skipgram:\n            denom = 1\n        else:\n            denom = 1\n        \n        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n        cost += c / batchsize / denom\n        grad[:N/2, :] += gin / batchsize / denom\n        grad[N/2:, :] += gout / batchsize / denom\n        \n    return cost, grad\n\ndef test_word2vec():\n    # Interface to the dataset for negative sampling\n    dataset = type(\'dummy\', (), {})()\n    def dummySampleTokenIdx():\n        return random.randint(0, 4)\n\n    def getRandomContext(C):\n        tokens = [""a"", ""b"", ""c"", ""d"", ""e""]\n        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n           for i in xrange(2*C)]\n    dataset.sampleTokenIdx = dummySampleTokenIdx\n    dataset.getRandomContext = getRandomContext\n\n    random.seed(31415)\n    np.random.seed(9265)\n    dummy_vectors = normalizeRows(np.random.randn(10,3))\n    dummy_tokens = dict([(""a"",0), (""b"",1), (""c"",2),(""d"",3),(""e"",4)])\n    print ""==== Gradient check for skip-gram ====""\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n    print ""\\n==== Gradient check for CBOW      ====""\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n\n    print ""\\n=== Results ===""\n    print skipgram(""c"", 3, [""a"", ""b"", ""e"", ""d"", ""b"", ""c""], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n    print skipgram(""c"", 1, [""a"", ""b""], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n    print cbow(""a"", 2, [""a"", ""b"", ""c"", ""a""], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n    print cbow(""a"", 2, [""a"", ""b"", ""a"", ""c""], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n\nif __name__ == ""__main__"":\n    test_normalize_rows()\n    test_word2vec()\n'"
assignment1/q4_sentiment.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom cs224d.data_utils import *\n\nfrom q3_sgd import load_saved_params, sgd\nfrom q4_softmaxreg import softmaxRegression, getSentenceFeature, accuracy, predictionLoss, regularizationLoss\n\n# Try different regularizations and pick the best!\n# NOTE: fill in one more ""your code here"" below before running!\nREGULARIZATION = [0.5 ** power for power in xrange(10, 16)]   # Assign a list of floats in the block below\n\n# Load the dataset\ndataset = StanfordSentiment()\ntokens = dataset.tokens()\nnWords = len(tokens)\n\n# Load the word vectors we trained earlier \n_, wordVectors0, _ = load_saved_params()\nwordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\ndimVectors = wordVectors.shape[1]\n\n# Load the train set\ntrainset = dataset.getTrainSentences()\nnTrain = len(trainset)\ntrainFeatures = np.zeros((nTrain, dimVectors))\ntrainLabels = np.zeros((nTrain,), dtype=np.int32)\nfor i in xrange(nTrain):\n    words, trainLabels[i] = trainset[i]\n    trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n\n# Prepare dev set features\ndevset = dataset.getDevSentences()\nnDev = len(devset)\ndevFeatures = np.zeros((nDev, dimVectors))\ndevLabels = np.zeros((nDev,), dtype=np.int32)\nfor i in xrange(nDev):\n    words, devLabels[i] = devset[i]\n    devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n\n# Try our regularization parameters\nresults = []\nfor regularization in REGULARIZATION:\n    random.seed(3141)\n    np.random.seed(59265)\n    weights = np.random.randn(dimVectors, 5)\n    print ""Training for reg=%f"" % regularization \n\n    # We will do batch optimization\n    weights = sgd(lambda weights: softmaxRegression(trainFeatures, trainLabels, \n        weights, regularization, nopredictions=True), weights, 0.3, 200000, PRINT_EVERY=20000)\n\n    # Test on train set\n    _, _, pred = softmaxRegression(trainFeatures, trainLabels, weights)\n    trainAccuracy = accuracy(trainLabels, pred)\n    print ""Train accuracy (%%): %f"" % trainAccuracy\n\n    # Test on dev set\n    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n    devAccuracy = accuracy(devLabels, pred)\n    print ""Dev accuracy (%%): %f"" % devAccuracy\n    print ""Prediction VS ||Weights||^2: %f - %f"" % \\\n        (predictionLoss(trainFeatures, trainLabels, weights),\n         regularizationLoss(weights, 1.0))\n\n    # Save the results and weights\n    results.append({\n        ""reg"" : regularization, \n        ""weights"" : weights, \n        ""train"" : trainAccuracy, \n        ""dev"" : devAccuracy})\n\n# Print the accuracies\nprint """"\nprint ""=== Recap ===""\nprint ""Reg\\t\\tTrain\\t\\tDev""\nfor result in results:\n    print ""%E\\t%f\\t%f"" % (\n        result[""reg""], \n        result[""train""], \n        result[""dev""])\nprint """"\n\n# Pick the best regularization parameters\nbest_setup_dict = max(results, key=lambda setup_dict: setup_dict[""dev""])\nBEST_REGULARIZATION = best_setup_dict[""reg""]\nBEST_WEIGHTS = best_setup_dict[""weights""]\n\n# Test your findings on the test set\ntestset = dataset.getTestSentences()\nnTest = len(testset)\ntestFeatures = np.zeros((nTest, dimVectors))\ntestLabels = np.zeros((nTest,), dtype=np.int32)\nfor i in xrange(nTest):\n    words, testLabels[i] = testset[i]\n    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n\n_, _, pred = softmaxRegression(testFeatures, testLabels, BEST_WEIGHTS)\nprint ""Best regularization value: %E"" % BEST_REGULARIZATION\nprint ""Test accuracy (%%): %f"" % accuracy(testLabels, pred)\n\n# Make a plot of regularization vs accuracy\nplt.plot(REGULARIZATION, [x[""train""] for x in results])\nplt.plot(REGULARIZATION, [x[""dev""] for x in results])\nplt.xscale(\'log\')\nplt.xlabel(""regularization"")\nplt.ylabel(""accuracy"")\nplt.legend([\'train\', \'dev\'], loc=\'upper left\')\nplt.savefig(""q4_reg_v_acc.png"")\nplt.show()\n\n'"
assignment1/q4_softmaxreg.py,0,"b'import numpy as np\nimport random\n\nfrom cs224d.data_utils import *\n\nfrom q1_softmax import softmax\nfrom q2_gradcheck import gradcheck_naive\nfrom q3_sgd import load_saved_params\n\ndef getSentenceFeature(tokens, wordVectors, sentence):\n    """""" Obtain the sentence feature for sentiment analysis by averaging its word vectors """"""\n    # Implement computation for the sentence features given a sentence.                                                       \n    \n    # Inputs:                                                         \n    # - tokens: a dictionary that maps words to their indices in    \n    #          the word vector list                                \n    # - wordVectors: word vectors (each row) for all tokens                \n    # - sentence: a list of words in the sentence of interest \n\n    # Output:                                                         \n    # - sentVector: feature vector for the sentence    \n\n    sentence_vectors = [wordVectors[tokens[word]] for word in sentence]\n\n    return sum(sentence_vectors) * 1.0 / len(sentence_vectors)\n\ndef predictionLoss(features, labels, weights):\n    prob = softmax(features.dot(weights))\n    if len(features.shape) == 1:\n        features = features[np.newaxis, :]\n    N = features.shape[0]\n    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i))\n    return np.sum(-np.log(prob[range(N), labels])) / N\n\ndef regularizationLoss(weights, regularization):\n    return 0.5 * regularization * np.sum(weights ** 2)\n\ndef softmaxRegression(features, labels, weights, regularization=0.0, nopredictions=False):\n    """""" Softmax Regression """"""\n    # Implement softmax regression with weight regularization.        \n    \n    # Inputs:                                                         \n    # - features: feature vectors, each row is a feature vector     \n    # - labels: labels corresponding to the feature vectors         \n    # - weights: weights of the regressor                           \n    # - regularization: L2 regularization constant                  \n    \n    # Output:                                                         \n    # - cost: cost of the regressor                                 \n    # - grad: gradient of the regressor cost with respect to its    \n    #        weights                                               \n    # - pred: label predictions of the regressor (you might find    \n    #        np.argmax helpful)  \n    prob = softmax(features.dot(weights))\n    if len(features.shape) == 1:\n        features = features[np.newaxis, :]\n    N = features.shape[0]\n    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n    cost = predictionLoss(features, labels, weights) + \\\n           regularizationLoss(weights, regularization)\n\n    labels_onehot = np.zeros_like(prob)\n    labels_onehot[range(N), labels] = 1.0\n    grad = 1.0 / N * np.dot(features.T, prob - labels_onehot) + regularization * weights\n\n    pred = np.argmax(prob, axis=len(prob.shape) - 1)\n\n    if nopredictions:\n        return cost, grad\n    else:\n        return cost, grad, pred\n\ndef accuracy(y, yhat):\n    """""" Precision for classifier """"""\n    assert(y.shape == yhat.shape)\n    return np.sum(y == yhat) * 100.0 / y.size\n\ndef sanity_check():\n    """"""\n    Run python q4_softmaxreg.py.\n    """"""\n    random.seed(314159)\n    np.random.seed(265)\n\n    dataset = StanfordSentiment()\n    tokens = dataset.tokens()\n    nWords = len(tokens)\n\n    _, wordVectors0, _ = load_saved_params()\n    wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n    dimVectors = wordVectors.shape[1]\n\n    N = 17\n    dummy_weights = 0.1 * np.random.randn(dimVectors, 5)\n    dummy_features = np.zeros((N, dimVectors))\n    dummy_labels = np.zeros((N,), dtype=np.int32)    \n    for i in xrange(N):\n        words, dummy_labels[i] = dataset.getRandomTrainSentence()\n        dummy_features[i, :] = getSentenceFeature(tokens, wordVectors, words)\n    print ""==== Gradient check for softmax regression ====""\n    gradcheck_naive(lambda weights: softmaxRegression(dummy_features,\n        dummy_labels, weights, 1.0, nopredictions = True), dummy_weights)\n\n    print ""\\n=== Results ===""\n    print softmaxRegression(dummy_features, dummy_labels, dummy_weights, 1.0)\n\nif __name__ == ""__main__"":\n    sanity_check()\n'"
assignment2/model.py,3,"b'class Model(object):\n  """"""Abstracts a Tensorflow graph for a learning task.\n\n  We use various Model classes as usual abstractions to encapsulate tensorflow\n  computational graphs. Each algorithm you will construct in this homework will\n  inherit from a Model object.\n  """"""\n\n  def load_data(self):\n    """"""Loads data from disk and stores it in memory.\n\n    Feel free to add instance variables to Model object that store loaded data.    \n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def add_placeholders(self):\n    """"""Adds placeholder variables to tensorflow computational graph.\n\n    Tensorflow uses placeholder variables to represent locations in a\n    computational graph where data is inserted.  These placeholders are used as\n    inputs by the rest of the model building code and will be fed data during\n    training.\n\n    See for more information:\n\n    https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def create_feed_dict(self, input_batch, label_batch):\n    """"""Creates the feed_dict for training the given step.\n\n    A feed_dict takes the form of:\n\n    feed_dict = {\n        <placeholder>: <tensor of values to be passed for placeholder>,\n        ....\n    }\n  \n    If label_batch is None, then no labels are added to feed_dict.\n\n    Hint: The keys for the feed_dict should be a subset of the placeholder\n          tensors created in add_placeholders.\n    \n    Args:\n      input_batch: A batch of input data.\n      label_batch: A batch of label data.\n    Returns:\n      feed_dict: The feed dictionary mapping from placeholders to values.\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def add_model(self, input_data):\n    """"""Implements core of model that transforms input_data into predictions.\n\n    The core transformation for this model which transforms a batch of input\n    data into a batch of predictions.\n\n    Args:\n      input_data: A tensor of shape (batch_size, n_features).\n    Returns:\n      out: A tensor of shape (batch_size, n_classes)\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def add_loss_op(self, pred):\n    """"""Adds ops for loss to the computational graph.\n\n    Args:\n      pred: A tensor of shape (batch_size, n_classes)\n    Returns:\n      loss: A 0-d tensor (scalar) output\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def run_epoch(self, sess, input_data, input_labels):\n    """"""Runs an epoch of training.\n\n    Trains the model for one-epoch.\n  \n    Args:\n      sess: tf.Session() object\n      input_data: np.ndarray of shape (n_samples, n_features)\n      input_labels: np.ndarray of shape (n_samples, n_classes)\n    Returns:\n      average_loss: scalar. Average minibatch loss of model on epoch.\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def fit(self, sess, input_data, input_labels):\n    """"""Fit model on provided data.\n\n    Args:\n      sess: tf.Session()\n      input_data: np.ndarray of shape (n_samples, n_features)\n      input_labels: np.ndarray of shape (n_samples, n_classes)\n    Returns:\n      losses: list of loss per epoch\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\n  def predict(self, sess, input_data, input_labels=None):\n    """"""Make predictions from the provided model.\n    Args:\n      sess: tf.Session()\n      input_data: np.ndarray of shape (n_samples, n_features)\n      input_labels: np.ndarray of shape (n_samples, n_classes)\n    Returns:\n      average_loss: Average loss of model.\n      predictions: Predictions of model on input_data\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n\nclass LanguageModel(Model):\n  """"""Abstracts a Tensorflow graph for learning language models.\n\n  Adds ability to do embedding.\n  """"""\n\n  def add_embedding(self):\n    """"""Add embedding layer. that maps from vocabulary to vectors.\n    """"""\n    raise NotImplementedError(""Each Model must re-implement this method."")\n'"
assignment2/q1_classifier.py,17,"b'import time\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom q1_softmax import softmax, cross_entropy_loss\nfrom model import Model\nfrom utils import data_iterator\n\nclass Config(object):\n  """"""Holds model hyperparams and data information.\n\n  The config class is used to store various hyperparameters and dataset\n  information parameters. Model objects are passed a Config() object at\n  instantiation.\n  """"""\n  batch_size = 64\n  n_samples = 1024\n  n_features = 100\n  n_classes = 5\n  # You may adjust the max_epochs to ensure convergence.\n  max_epochs = 50\n  # You may adjust this learning rate to ensure convergence.\n  lr = 1e-3 \n\nclass SoftmaxModel(Model):\n  """"""Implements a Softmax classifier with cross-entropy loss.""""""\n\n  def load_data(self):\n    """"""Creates a synthetic dataset and stores it in memory.""""""\n    np.random.seed(1234)\n    self.input_data = np.random.rand(\n        self.config.n_samples, self.config.n_features)\n    self.input_labels = np.ones((self.config.n_samples,), dtype=np.int32)\n\n  def add_placeholders(self):\n    """"""Generate placeholder variables to represent the input tensors.\n\n    These placeholders are used as inputs by the rest of the model building\n    code and will be fed data during training.\n\n    Adds following nodes to the computational graph\n\n    input_placeholder: Input placeholder tensor of shape\n                       (batch_size, n_features), type tf.float32\n    labels_placeholder: Labels placeholder tensor of shape\n                       (batch_size, n_classes), type tf.int32\n\n    Add these placeholders to self as the instance variables\n  \n      self.input_placeholder\n      self.labels_placeholder\n\n    (Don\'t change the variable names)\n    """"""\n    self.input_placeholder = tf.placeholder(tf.float32, [self.config.batch_size, self.config.n_features])\n    self.labels_placeholder = tf.placeholder(tf.int32, [self.config.batch_size, self.config.n_classes])\n\n  def create_feed_dict(self, input_batch, label_batch):\n    """"""Creates the feed_dict for softmax classifier.\n\n    A feed_dict takes the form of:\n\n    feed_dict = {\n        <placeholder>: <tensor of values to be passed for placeholder>,\n        ....\n    }\n\n    If label_batch is None, then no labels are added to feed_dict.\n\n    Hint: The keys for the feed_dict should match the placeholder tensors\n          created in add_placeholders.\n    \n    Args:\n      input_batch: A batch of input data.\n      label_batch: A batch of label data.\n    Returns:\n      feed_dict: The feed dictionary mapping from placeholders to values.\n    """"""\n    if label_batch is not None:\n      return {self.input_placeholder: input_batch, self.labels_placeholder: label_batch}\n    else:\n      return {self.input_placeholder: input_batch}\n\n  def add_training_op(self, loss):\n    """"""Sets up the training Ops.\n\n    Creates an optimizer and applies the gradients to all trainable variables.\n    The Op returned by this function is what must be passed to the\n    `sess.run()` call to cause the model to train. See \n\n    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n    for more information.\n\n    Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n          Calling optimizer.minimize() will return a train_op object.\n\n    Args:\n      loss: Loss tensor, from cross_entropy_loss.\n    Returns:\n      train_op: The Op for training.\n    """"""\n    return tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n\n  def add_model(self, input_data):\n    """"""Adds a linear-layer plus a softmax transformation\n\n    The core transformation for this model which transforms a batch of input\n    data into a batch of predictions. In this case, the mathematical\n    transformation effected is\n\n    y = softmax(xW + b)\n\n    Hint: Make sure to create tf.Variables as needed. Also, make sure to use\n          tf.name_scope to ensure that your name spaces are clean.\n    Hint: For this simple use-case, it\'s sufficient to initialize both weights W\n          and biases b with zeros.\n\n    Args:\n      input_data: A tensor of shape (batch_size, n_features).\n    Returns:\n      out: A tensor of shape (batch_size, n_classes)\n    """"""\n    with tf.variable_scope(""softmax_model""):\n      self.W = tf.Variable(tf.zeros([self.config.n_features, self.config.n_classes]), name=""weights"")\n      self.b = tf.Variable(tf.zeros([self.config.n_classes]), name=""biases"")\n    return softmax(tf.matmul(input_data, self.W) + self.b)\n\n  def add_loss_op(self, pred):\n    """"""Adds cross_entropy_loss ops to the computational graph.\n\n    Hint: Use the cross_entropy_loss function we defined. This should be a very\n          short function.\n    Args:\n      pred: A tensor of shape (batch_size, n_classes)\n    Returns:\n      loss: A 0-d tensor (scalar)\n    """"""\n    return cross_entropy_loss(self.labels_placeholder, pred)\n\n  def run_epoch(self, sess, input_data, input_labels):\n    """"""Runs an epoch of training.\n\n    Trains the model for one-epoch.\n  \n    Args:\n      sess: tf.Session() object\n      input_data: np.ndarray of shape (n_samples, n_features)\n      input_labels: np.ndarray of shape (n_samples, n_classes)\n    Returns:\n      average_loss: scalar. Average minibatch loss of model on epoch.\n    """"""\n    # And then after everything is built, start the training loop.\n    average_loss = 0\n    for step, (input_batch, label_batch) in enumerate(\n        data_iterator(input_data, input_labels,\n                      batch_size=self.config.batch_size,\n                      label_size=self.config.n_classes)):\n\n      # Fill a feed dictionary with the actual set of images and labels\n      # for this particular training step.\n      feed_dict = self.create_feed_dict(input_batch, label_batch)\n\n      # Run one step of the model.  The return values are the activations\n      # from the `self.train_op` (which is discarded) and the `loss` Op.  To\n      # inspect the values of your Ops or variables, you may include them\n      # in the list passed to sess.run() and the value tensors will be\n      # returned in the tuple from the call.\n      _, loss_value = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n      average_loss += loss_value\n\n    average_loss = average_loss / step\n    return average_loss \n\n  def fit(self, sess, input_data, input_labels):\n    """"""Fit model on provided data.\n\n    Args:\n      sess: tf.Session()\n      input_data: np.ndarray of shape (n_samples, n_features)\n      input_labels: np.ndarray of shape (n_samples, n_classes)\n    Returns:\n      losses: list of loss per epoch\n    """"""\n    losses = []\n    for epoch in range(self.config.max_epochs):\n      start_time = time.time()\n      average_loss = self.run_epoch(sess, input_data, input_labels)\n      duration = time.time() - start_time\n      # Print status to stdout.\n      print(\'Epoch %d: loss = %.2f (%.3f sec)\'\n             % (epoch, average_loss, duration))\n      losses.append(average_loss)\n    return losses\n\n  def __init__(self, config):\n    """"""Initializes the model.\n\n    Args:\n      config: A model configuration object of type Config\n    """"""\n    self.config = config\n    # Generate placeholders for the images and labels.\n    self.load_data()\n    self.add_placeholders()\n    self.pred = self.add_model(self.input_placeholder)\n    self.loss = self.add_loss_op(self.pred)\n    self.train_op = self.add_training_op(self.loss)\n  \ndef test_SoftmaxModel():\n  """"""Train softmax model for a number of steps.""""""\n  config = Config()\n  with tf.Graph().as_default():\n    model = SoftmaxModel(config)\n  \n    # Create a session for running Ops on the Graph.\n    sess = tf.Session()\n  \n    # Run the Op to initialize the variables.\n    init = tf.initialize_all_variables()\n    sess.run(init)\n  \n    losses = model.fit(sess, model.input_data, model.input_labels)\n\n  # If ops are implemented correctly, the average loss should fall close to zero\n  # rapidly.\n  assert losses[-1] < .5\n  print ""Basic (non-exhaustive) classifier tests pass\\n""\n\nif __name__ == ""__main__"":\n    test_SoftmaxModel()\n'"
assignment2/q1_softmax.py,24,"b'import numpy as np\nimport tensorflow as tf\n\ndef softmax(x):\n  """"""\n  Compute the softmax function in tensorflow.\n\n  You might find the tensorflow functions tf.exp, tf.reduce_max,\n  tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n  not need to use all of these functions). Recall also that many common\n  tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n  if x and y are both tensors). Make sure to implement the numerical stability\n  fixes as in the previous homework!\n\n  Args:\n    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n         represented by row-vectors. (For simplicity, no need to handle 1-d\n         input as in the previous homework)\n  Returns:\n    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n         tensor in this problem.\n  """"""\n  per_item_min = tf.reduce_min(x, 1, keep_dims=True)\n  y = tf.exp(x - per_item_min)\n  normalization = tf.reduce_sum(y, 1, keep_dims=True)\n  return tf.div(y, normalization)\n\ndef cross_entropy_loss(y, yhat):\n  """"""\n  Compute the cross entropy loss in tensorflow.\n\n  y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n  of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n  be of dtype tf.float32.\n\n  The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n  solutions are possible, so you may not need to use all of these functions).\n\n  Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n        functions.\n\n  Args:\n    y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n    yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n          probability distribution and should sum to 1.\n  Returns:\n    out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n          tensor in the problem.\n  """"""\n  return tf.reduce_sum(-tf.to_float(y) * tf.log(yhat))\n\n\ndef test_softmax_basic():\n  """"""\n  Some simple tests to get you started. \n  Warning: these are not exhaustive.\n  """"""\n  print ""Running basic tests...""\n  test1 = softmax(tf.convert_to_tensor(\n      np.array([[1001,1002],[3,4]]), dtype=tf.float32))\n  with tf.Session():\n      test1 = test1.eval()\n  assert np.amax(np.fabs(test1 - np.array(\n      [0.26894142,  0.73105858]))) <= 1e-6\n\n  test2 = softmax(tf.convert_to_tensor(\n      np.array([[-1001,-1002]]), dtype=tf.float32))\n  with tf.Session():\n      test2 = test2.eval()\n  assert np.amax(np.fabs(test2 - np.array(\n      [0.73105858, 0.26894142]))) <= 1e-6\n\n  print ""Basic (non-exhaustive) softmax tests pass\\n""\n\ndef test_cross_entropy_loss_basic():\n  """"""\n  Some simple tests to get you started.\n  Warning: these are not exhaustive.\n  """"""\n  y = np.array([[0, 1], [1, 0], [1, 0]])\n  yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n\n  test1 = cross_entropy_loss(\n      tf.convert_to_tensor(y, dtype=tf.int32),\n      tf.convert_to_tensor(yhat, dtype=tf.float32))\n  with tf.Session():\n    test1 = test1.eval()\n  result = -3 * np.log(.5)\n  assert np.amax(np.fabs(test1 - result)) <= 1e-6\n  print ""Basic (non-exhaustive) cross-entropy tests pass\\n""\n\nif __name__ == ""__main__"":\n  test_softmax_basic()\n  test_cross_entropy_loss_basic()\n'"
assignment2/q2_NER.py,40,"b'import os\nimport getpass\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom q2_initialization import xavier_weight_init\nimport data_utils.utils as du\nimport data_utils.ner as ner\nfrom utils import data_iterator\nfrom model import LanguageModel\n\nclass Config(object):\n  """"""Holds model hyperparams and data information.\n\n  The config class is used to store various hyperparameters and dataset\n  information parameters. Model objects are passed a Config() object at\n  instantiation.\n  """"""\n  embed_size = 50\n  batch_size = 64\n  label_size = 5\n  hidden_size = 100\n  max_epochs = 24\n  early_stopping = 2\n  dropout = 0.9\n  lr = 0.001\n  l2 = 0.0001\n  window_size = 3\n\nclass NERModel(LanguageModel):\n  """"""Implements a NER (Named Entity Recognition) model.\n\n  This class implements a deep network for named entity recognition. It\n  inherits from LanguageModel, which has an add_embedding method in addition to\n  the standard Model method.\n  """"""\n\n  def load_data(self, debug=False):\n    """"""Loads starter word-vectors and train/dev/test data.""""""\n    # Load the starter word vectors\n    self.wv, word_to_num, num_to_word = ner.load_wv(\n      \'data/ner/vocab.txt\', \'data/ner/wordVectors.txt\')\n    tagnames = [\'O\', \'LOC\', \'MISC\', \'ORG\', \'PER\']\n    self.num_to_tag = dict(enumerate(tagnames))\n    tag_to_num = {v:k for k,v in self.num_to_tag.iteritems()}\n\n    # Load the training set\n    docs = du.load_dataset(\'data/ner/train\')\n    self.X_train, self.y_train = du.docs_to_windows(\n        docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n    if debug:\n      self.X_train = self.X_train[:1024]\n      self.y_train = self.y_train[:1024]\n\n    # Load the dev set (for tuning hyperparameters)\n    docs = du.load_dataset(\'data/ner/dev\')\n    self.X_dev, self.y_dev = du.docs_to_windows(\n        docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n    if debug:\n      self.X_dev = self.X_dev[:1024]\n      self.y_dev = self.y_dev[:1024]\n\n    # Load the test set (dummy labels only)\n    docs = du.load_dataset(\'data/ner/test.masked\')\n    self.X_test, self.y_test = du.docs_to_windows(\n        docs, word_to_num, tag_to_num, wsize=self.config.window_size)\n\n  def add_placeholders(self):\n    """"""Generate placeholder variables to represent the input tensors\n\n    These placeholders are used as inputs by the rest of the model building\n    code and will be fed data during training.  Note that when ""None"" is in a\n    placeholder\'s shape, it\'s flexible\n\n    Adds following nodes to the computational graph\n\n    input_placeholder: Input placeholder tensor of shape\n                       (None, window_size), type tf.int32\n    labels_placeholder: Labels placeholder tensor of shape\n                        (None, label_size), type tf.float32\n    dropout_placeholder: Dropout value placeholder (scalar),\n                         type tf.float32\n\n    Add these placeholders to self as the instance variables\n  \n      self.input_placeholder\n      self.labels_placeholder\n      self.dropout_placeholder\n\n    (Don\'t change the variable names)\n    """"""\n    self.input_placeholder = tf.placeholder(tf.int32, (None, self.config.window_size))\n    self.labels_placeholder = tf.placeholder(tf.float32, (None, self.config.label_size))\n    self.dropout_placeholder = tf.placeholder(tf.float32)\n\n  def create_feed_dict(self, input_batch, dropout, label_batch=None):\n    """"""Creates the feed_dict for softmax classifier.\n\n    A feed_dict takes the form of:\n\n    feed_dict = {\n        <placeholder>: <tensor of values to be passed for placeholder>,\n        ....\n    }\n\n\n    Hint: The keys for the feed_dict should be a subset of the placeholder\n          tensors created in add_placeholders.\n    Hint: When label_batch is None, don\'t add a labels entry to the feed_dict.\n    \n    Args:\n      input_batch: A batch of input data.\n      label_batch: A batch of label data.\n    Returns:\n      feed_dict: The feed dictionary mapping from placeholders to values.\n    """"""\n    feed_dict = {self.input_placeholder: input_batch, self.dropout_placeholder: dropout}\n    if label_batch is not None:\n      feed_dict[self.labels_placeholder] = label_batch\n    return feed_dict\n\n  def add_embedding(self):\n    """"""Add embedding layer that maps from vocabulary to vectors.\n\n    Creates an embedding tensor (of shape (len(self.wv), embed_size). Use the\n    input_placeholder to retrieve the embeddings for words in the current batch.\n\n    (Words are discrete entities. They need to be transformed into vectors for use\n    in deep-learning. Although we won\'t do so in this problem, in practice it\'s\n    useful to initialize the embedding with pre-trained word-vectors. For this\n    problem, using the default initializer is sufficient.)\n\n    Hint: This layer should use the input_placeholder to index into the\n          embedding.\n    Hint: You might find tf.nn.embedding_lookup useful.\n    Hint: See following link to understand what -1 in a shape means.\n      https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#reshape\n    Hint: Check the last slide from the TensorFlow lecture.\n    Hint: Here are the dimensions of the variables you will need to create:\n\n      L: (len(self.wv), embed_size)\n\n    Returns:\n      window: tf.Tensor of shape (-1, window_size*embed_size)\n    """"""\n    self.embeddings = tf.Variable(tf.constant(self.wv, dtype=tf.float32))\n    # The embedding lookup is currently only implemented for the CPU\n    with tf.device(\'/cpu:0\'):\n      return tf.reshape(tf.nn.embedding_lookup(self.embeddings, self.input_placeholder),\n                        (-1, self.config.window_size * self.config.embed_size))\n\n  def add_model(self, window):\n    """"""Adds the 1-hidden-layer NN.\n\n    Hint: Use a variable_scope (e.g. ""Layer"") for the first hidden layer, and\n          another variable_scope (e.g. ""Softmax"") for the linear transformation\n          preceding the softmax. Make sure to use the xavier_weight_init you\n          defined in the previous part to initialize weights.\n    Hint: Make sure to add in regularization and dropout to this network.\n          Regularization should be an addition to the cost function, while\n          dropout should be added after both variable scopes.\n    Hint: You might consider using a tensorflow Graph Collection (e.g\n          ""total_loss"") to collect the regularization and loss terms (which you\n          will add in add_loss_op below).\n    Hint: Here are the dimensions of the various variables you will need to\n          create\n\n          W:  (window_size*embed_size, hidden_size)\n          b1: (hidden_size,)\n          U:  (hidden_size, label_size)\n          b2: (label_size)\n\n    https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#graph-collections\n    Args:\n      window: tf.Tensor of shape (-1, window_size*embed_size)\n    Returns:\n      output: tf.Tensor of shape (batch_size, label_size)\n    """"""\n    with tf.variable_scope(""HiddenLayer""):\n      self.W = tf.Variable(xavier_weight_init()((self.config.window_size * self.config.embed_size, self.config.hidden_size)))\n      self.b1 = tf.Variable(xavier_weight_init()((self.config.hidden_size,)))\n    with tf.variable_scope(""SoftmaxLayer""):\n      self.U = tf.Variable(xavier_weight_init()((self.config.hidden_size, self.config.label_size)))\n      self.b2 = tf.Variable(xavier_weight_init()((self.config.label_size,)))\n    tf.add_to_collection(""regularization"", tf.reduce_sum(tf.square(self.W)))\n    tf.add_to_collection(""regularization"", tf.reduce_sum(tf.square(self.b1)))\n    tf.add_to_collection(""regularization"", tf.reduce_sum(tf.square(self.U)))\n    tf.add_to_collection(""regularization"", tf.reduce_sum(tf.square(self.b2)))\n    hidden_outputs = tf.nn.sigmoid(tf.matmul(tf.to_float(window), self.W) + self.b1)\n    output = tf.matmul(tf.nn.dropout(hidden_outputs, self.dropout_placeholder), self.U) + self.b2\n    return output\n\n  def add_loss_op(self, pred):\n    """"""Adds cross_entropy_loss ops to the computational graph.\n\n    Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n          implementation. You might find tf.reduce_mean useful.\n    Args:\n      pred: A tensor of shape (batch_size, n_classes)\n    Returns:\n      loss: A 0-d tensor (scalar)\n    """"""\n    loss = self.config.l2 * sum(tf.get_collection(""regularization""))\n    loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, self.labels_placeholder))\n    return loss\n\n  def add_training_op(self, loss):\n    """"""Sets up the training Ops.\n\n    Creates an optimizer and applies the gradients to all trainable variables.\n    The Op returned by this function is what must be passed to the\n    `sess.run()` call to cause the model to train. See \n\n    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n    for more information.\n\n    Hint: Use tf.train.AdamOptimizer for this model.\n          Calling optimizer.minimize() will return a train_op object.\n\n    Args:\n      loss: Loss tensor, from cross_entropy_loss.\n    Returns:\n      train_op: The Op for training.\n    """"""\n    return tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n\n  def __init__(self, config):\n    """"""Constructs the network using the helper functions defined above.""""""\n    self.config = config\n    self.load_data(debug=False)\n    self.add_placeholders()\n    window = self.add_embedding()\n    y = self.add_model(window)\n\n    self.loss = self.add_loss_op(y)\n    self.predictions = tf.nn.softmax(y)\n    one_hot_prediction = tf.argmax(self.predictions, 1)\n    correct_prediction = tf.equal(\n        tf.argmax(self.labels_placeholder, 1), one_hot_prediction)\n    self.correct_predictions = tf.reduce_sum(tf.cast(correct_prediction, \'int32\'))\n    self.train_op = self.add_training_op(self.loss)\n\n  def run_epoch(self, session, input_data, input_labels,\n                shuffle=True, verbose=True):\n    orig_X, orig_y = input_data, input_labels\n    dp = self.config.dropout\n    # We\'re interested in keeping track of the loss and accuracy during training\n    total_loss = []\n    total_correct_examples = 0\n    total_processed_examples = 0\n    total_steps = len(orig_X) / self.config.batch_size\n    for step, (x, y) in enumerate(\n      data_iterator(orig_X, orig_y, batch_size=self.config.batch_size,\n                   label_size=self.config.label_size, shuffle=shuffle)):\n      feed = self.create_feed_dict(input_batch=x, dropout=dp, label_batch=y)\n      loss, total_correct, _ = session.run(\n          [self.loss, self.correct_predictions, self.train_op],\n          feed_dict=feed)\n      total_processed_examples += len(x)\n      total_correct_examples += total_correct\n      total_loss.append(loss)\n      ##\n      if verbose and step % verbose == 0:\n        sys.stdout.write(\'\\r{} / {} : loss = {}\'.format(\n            step, total_steps, np.mean(total_loss)))\n        sys.stdout.flush()\n    if verbose:\n        sys.stdout.write(\'\\r\')\n        sys.stdout.flush()\n    return np.mean(total_loss), total_correct_examples / float(total_processed_examples)\n\n  def predict(self, session, X, y=None):\n    """"""Make predictions from the provided model.""""""\n    # If y is given, the loss is also calculated\n    # We deactivate dropout by setting it to 1\n    dp = 1\n    losses = []\n    results = []\n    if np.any(y):\n        data = data_iterator(X, y, batch_size=self.config.batch_size,\n                             label_size=self.config.label_size, shuffle=False)\n    else:\n        data = data_iterator(X, batch_size=self.config.batch_size,\n                             label_size=self.config.label_size, shuffle=False)\n    for step, (x, y) in enumerate(data):\n      feed = self.create_feed_dict(input_batch=x, dropout=dp)\n      if np.any(y):\n        feed[self.labels_placeholder] = y\n        loss, preds = session.run(\n            [self.loss, self.predictions], feed_dict=feed)\n        losses.append(loss)\n      else:\n        preds = session.run(self.predictions, feed_dict=feed)\n      predicted_indices = preds.argmax(axis=1)\n      results.extend(predicted_indices)\n    return np.mean(losses), results\n\ndef print_confusion(confusion, num_to_tag):\n    """"""Helper method that prints confusion matrix.""""""\n    # Summing top to bottom gets the total number of tags guessed as T\n    total_guessed_tags = confusion.sum(axis=0)\n    # Summing left to right gets the total number of true tags\n    total_true_tags = confusion.sum(axis=1)\n    print\n    print confusion\n    for i, tag in sorted(num_to_tag.items()):\n        prec = confusion[i, i] / float(total_guessed_tags[i])\n        recall = confusion[i, i] / float(total_true_tags[i])\n        print \'Tag: {} - P {:2.4f} / R {:2.4f}\'.format(tag, prec, recall)\n\ndef calculate_confusion(config, predicted_indices, y_indices):\n    """"""Helper method that calculates confusion matrix.""""""\n    confusion = np.zeros((config.label_size, config.label_size), dtype=np.int32)\n    for i in xrange(len(y_indices)):\n        correct_label = y_indices[i]\n        guessed_label = predicted_indices[i]\n        confusion[correct_label, guessed_label] += 1\n    return confusion\n\ndef save_predictions(predictions, filename):\n  """"""Saves predictions to provided file.""""""\n  with open(filename, ""wb"") as f:\n    for prediction in predictions:\n      f.write(str(prediction) + ""\\n"")\n\ndef test_NER():\n  """"""Test NER model implementation.\n\n  You can use this function to test your implementation of the Named Entity\n  Recognition network. When debugging, set max_epochs in the Config object to 1\n  so you can rapidly iterate.\n  """"""\n  config = Config()\n  with tf.Graph().as_default():\n    model = NERModel(config)\n\n    init = tf.initialize_all_variables()\n    saver = tf.train.Saver()\n\n    with tf.Session() as session:\n      best_val_loss = float(\'inf\')\n      best_val_epoch = 0\n\n      session.run(init)\n      for epoch in xrange(config.max_epochs):\n        print \'Epoch {}\'.format(epoch)\n        start = time.time()\n        ###\n        train_loss, train_acc = model.run_epoch(session, model.X_train,\n                                                model.y_train)\n        val_loss, predictions = model.predict(session, model.X_dev, model.y_dev)\n        print \'Training loss: {}\'.format(train_loss)\n        print \'Training acc: {}\'.format(train_acc)\n        print \'Validation loss: {}\'.format(val_loss)\n        if val_loss < best_val_loss:\n          best_val_loss = val_loss\n          best_val_epoch = epoch\n          if not os.path.exists(""./weights""):\n            os.makedirs(""./weights"")\n          saver.save(session, \'./weights/ner.weights\')\n        if epoch - best_val_epoch > config.early_stopping:\n          break\n        ###\n        confusion = calculate_confusion(config, predictions, model.y_dev)\n        print_confusion(confusion, model.num_to_tag)\n        print \'Total time: {}\'.format(time.time() - start)\n\n      saver.restore(session, \'./weights/ner.weights\')\n      print \'Test\'\n      print \'=-=-=\'\n      print \'Writing predictions to q2_test.predicted\'\n      _, predictions = model.predict(session, model.X_test, model.y_test)\n      save_predictions(predictions, ""q2_test.predicted"")\n\nif __name__ == ""__main__"":\n  test_NER()\n'"
assignment2/q2_initialization.py,4,"b'import numpy as np\nimport tensorflow as tf\n\ndef xavier_weight_init():\n  """"""\n  Returns function that creates random tensor. \n\n  The specified function will take in a shape (tuple or 1-d array) and must\n  return a random tensor of the specified shape and must be drawn from the\n  Xavier initialization distribution.\n\n  Hint: You might find tf.random_uniform useful.\n  """"""\n  def _xavier_initializer(shape, **kwargs):\n    """"""Defines an initializer for the Xavier distribution.\n\n    This function will be used as a variable scope initializer.\n\n    https://www.tensorflow.org/versions/r0.7/how_tos/variable_scope/index.html#initializers-in-variable-scope\n\n    Args:\n      shape: Tuple or 1-d array that species dimensions of requested tensor.\n    Returns:\n      out: tf.Tensor of specified shape sampled from Xavier distribution.\n    """"""\n    epsilon = np.sqrt(6.0 / sum(shape))\n    return tf.random_uniform(shape, -epsilon, epsilon)\n  # Returns defined initializer function.\n  return _xavier_initializer\n\ndef test_initialization_basic():\n  """"""\n  Some simple tests for the initialization.\n  """"""\n  print ""Running basic tests...""\n  xavier_initializer = xavier_weight_init()\n  shape = (1,)\n  xavier_mat = xavier_initializer(shape)\n  assert xavier_mat.get_shape() == shape\n\n  shape = (1, 2, 3)\n  xavier_mat = xavier_initializer(shape)\n  assert xavier_mat.get_shape() == shape\n  print ""Basic (non-exhaustive) Xavier initialization tests pass\\n""\n\ndef test_initialization():\n  """""" \n  Use this space to test your Xavier initialization code by running:\n      python q1_initialization.py \n  This function will not be called by the autograder, nor will\n  your tests be graded.\n  """"""\n  print ""Running your tests...""\n  shape = (12, 17)\n  xavier_mat = xavier_weight_init()(shape)\n  assert xavier_mat.get_shape() == shape\n  sess = tf.Session()\n  print np.max(np.abs(sess.run(xavier_mat))), np.sqrt(6.0 / sum(shape))\n\nif __name__ == ""__main__"":\n    test_initialization_basic()\n    test_initialization()\n'"
assignment2/q3_RNNLM.py,33,"b'import getpass\nimport math\nimport sys\nimport time\n\nimport numpy as np\nfrom copy import deepcopy\n\nfrom utils import calculate_perplexity, get_ptb_dataset, Vocab\nfrom utils import ptb_iterator, sample\n\nimport tensorflow as tf\nfrom tensorflow.python.ops.seq2seq import sequence_loss\nfrom model import LanguageModel\n\n# Let\'s set the parameters of our model\n# http://arxiv.org/pdf/1409.2329v4.pdf shows parameters that would achieve near\n# SotA numbers\n\nclass Config(object):\n  """"""Holds model hyperparams and data information.\n\n  The config class is used to store various hyperparameters and dataset\n  information parameters. Model objects are passed a Config() object at\n  instantiation.\n  """"""\n  batch_size = 64\n  embed_size = 50\n  hidden_size = 100\n  num_steps = 10\n  max_epochs = 16\n  early_stopping = 2\n  dropout = 0.9\n  lr = 0.001\n\nclass RNNLM_Model(LanguageModel):\n\n  def load_data(self, debug=False):\n    """"""Loads starter word-vectors and train/dev/test data.""""""\n    self.vocab = Vocab()\n    self.vocab.construct(get_ptb_dataset(\'train\'))\n    self.encoded_train = np.array(\n        [self.vocab.encode(word) for word in get_ptb_dataset(\'train\')],\n        dtype=np.int32)\n    self.encoded_valid = np.array(\n        [self.vocab.encode(word) for word in get_ptb_dataset(\'valid\')],\n        dtype=np.int32)\n    self.encoded_test = np.array(\n        [self.vocab.encode(word) for word in get_ptb_dataset(\'test\')],\n        dtype=np.int32)\n    if debug:\n      num_debug = 1024\n      self.encoded_train = self.encoded_train[:num_debug]\n      self.encoded_valid = self.encoded_valid[:num_debug]\n      self.encoded_test = self.encoded_test[:num_debug]\n\n  def add_placeholders(self):\n    """"""Generate placeholder variables to represent the input tensors\n\n    These placeholders are used as inputs by the rest of the model building\n    code and will be fed data during training.  Note that when ""None"" is in a\n    placeholder\'s shape, it\'s flexible\n\n    Adds following nodes to the computational graph.\n    (When None is in a placeholder\'s shape, it\'s flexible)\n\n    input_placeholder: Input placeholder tensor of shape\n                       (None, num_steps), type tf.int32\n    labels_placeholder: Labels placeholder tensor of shape\n                        (None, num_steps), type tf.float32\n    dropout_placeholder: Dropout value placeholder (scalar),\n                         type tf.float32\n\n    Add these placeholders to self as the instance variables\n  \n      self.input_placeholder\n      self.labels_placeholder\n      self.dropout_placeholder\n\n    (Don\'t change the variable names)\n    """"""\n    self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.num_steps], ""input_placeholder"")\n    self.labels_placeholder = tf.placeholder(tf.int64, [None, self.config.num_steps], ""labels_placeholder"")\n    self.dropout_placeholder = tf.placeholder(tf.float32, name=""dropout_placeholder"")\n\n  def add_embedding(self):\n    """"""Add embedding layer.\n\n    Hint: This layer should use the input_placeholder to index into the\n          embedding.\n    Hint: You might find tf.nn.embedding_lookup useful.\n    Hint: You might find tf.split, tf.squeeze useful in constructing tensor inputs\n    Hint: Check the last slide from the TensorFlow lecture.\n    Hint: Here are the dimensions of the variables you will need to create:\n\n      L: (len(self.vocab), embed_size)\n\n    Returns:\n      inputs: List of length num_steps, each of whose elements should be\n              a tensor of shape (batch_size, embed_size).\n    """"""\n    self.embeddings = tf.get_variable(""embeddings"",\n                                      [len(self.vocab), self.config.embed_size])\n\n    # The embedding lookup is currently only implemented for the CPU\n    with tf.device(\'/cpu:0\'):\n      all_embeddings = tf.nn.embedding_lookup(self.embeddings, self.input_placeholder) # batch x step x embed\n      embeddings_list = tf.split(1, self.config.num_steps, all_embeddings)\n      return [tf.squeeze(item, squeeze_dims=(1,)) for item in embeddings_list]\n\n  def add_projection(self, rnn_outputs):\n    """"""Adds a projection layer.\n\n    The projection layer transforms the hidden representation to a distribution\n    over the vocabulary.\n\n    Hint: Here are the dimensions of the variables you will need to\n          create\n\n          U:   (hidden_size, len(vocab))\n          b_2: (len(vocab),)\n\n    Args:\n      rnn_outputs: List of length num_steps, each of whose elements should be\n                   a tensor of shape (batch_size, embed_size).\n    Returns:\n      outputs: List of length num_steps, each a tensor of shape\n               (batch_size, len(vocab)\n    """"""\n    self.U = tf.get_variable(""U"", [self.config.hidden_size, len(self.vocab)])\n    self.b_2 = tf.get_variable(""b_2"", [len(self.vocab)])\n    return [tf.matmul(tf.nn.dropout(state, self.dropout_placeholder), self.U) + self.b_2\n              for state in rnn_outputs]\n\n  def add_loss_op(self, output):\n    """"""Adds loss ops to the computational graph.\n\n    Hint: Use tensorflow.python.ops.seq2seq.sequence_loss to implement sequence loss. \n\n    Args:\n      output: A tensor of shape (None, self.vocab)\n    Returns:\n      loss: A 0-d tensor (scalar)\n    """"""\n    return sequence_loss([output],\n                         [tf.reshape(\n                             self.labels_placeholder,\n                             [self.config.batch_size * self.config.num_steps, -1])],\n                         [tf.constant(1.0)])\n\n  def add_training_op(self, loss):\n    """"""Sets up the training Ops.\n\n    Creates an optimizer and applies the gradients to all trainable variables.\n    The Op returned by this function is what must be passed to the\n    `sess.run()` call to cause the model to train. See \n\n    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n    for more information.\n\n    Hint: Use tf.train.AdamOptimizer for this model.\n          Calling optimizer.minimize() will return a train_op object.\n\n    Args:\n      loss: Loss tensor, from cross_entropy_loss.\n    Returns:\n      train_op: The Op for training.\n    """"""\n    return tf.train.AdamOptimizer(self.config.lr).minimize(loss)\n\n  def __init__(self, config):\n    self.config = config\n    self.load_data(debug=False)\n    self.add_placeholders()\n    self.inputs = self.add_embedding()\n    self.rnn_outputs = self.add_model(self.inputs)\n    self.outputs = self.add_projection(self.rnn_outputs)\n    # We want to check how well we correctly predict the next word\n    # We cast o to float64 as there are numerical issues at hand\n    # (i.e. sum(output of softmax) = 1.00000298179 and not 1)\n    self.predictions = [tf.nn.softmax(tf.cast(o, \'float64\')) for o in self.outputs]\n    # Reshape the output into len(vocab) sized chunks - the -1 says as many as\n    # needed to evenly divide\n    output = tf.reshape(tf.concat(1, self.outputs), [-1, len(self.vocab)])\n    self.calculate_loss = self.add_loss_op(output)\n    self.train_step = self.add_training_op(self.calculate_loss)\n\n\n  def add_model(self, inputs):\n    """"""Creates the RNN LM model.\n\n    In the space provided below, you need to implement the equations for the\n    RNN LM model. Note that you may NOT use built in rnn_cell functions from\n    tensorflow.\n\n    Hint: Use a zeros tensor of shape (batch_size, hidden_size) as\n          initial state for the RNN. Add this to self as instance variable\n\n          self.initial_state\n  \n          (Don\'t change variable name)\n    Hint: Add the last RNN output to self as instance variable\n\n          self.final_state\n\n          (Don\'t change variable name)\n    Hint: Make sure to apply dropout to the inputs and the outputs.\n    Hint: Use a variable scope (e.g. ""RNN"") to define RNN variables.\n    Hint: Perform an explicit for-loop over inputs. You can use\n          scope.reuse_variables() to ensure that the weights used at each\n          iteration (each time-step) are the same. (Make sure you don\'t call\n          this for iteration 0 though or nothing will be initialized!)\n    Hint: Here are the dimensions of the various variables you will need to\n          create:\n\n          H: (hidden_size, hidden_size)\n          I: (embed_size, hidden_size)\n          b_1: (hidden_size,)\n\n    Args:\n      inputs: List of length num_steps, each of whose elements should be\n              a tensor of shape (batch_size, embed_size).\n    Returns:\n      outputs: List of length num_steps, each of whose elements should be\n               a tensor of shape (batch_size, hidden_size)\n    """"""\n    self.H = tf.get_variable(""H"", [self.config.hidden_size, self.config.hidden_size])\n    self.I = tf.get_variable(""I"", [self.config.embed_size, self.config.hidden_size])\n    self.b_1 = tf.get_variable(""b_1"", [self.config.hidden_size])\n    self.initial_state = tf.zeros((self.config.batch_size, self.config.hidden_size))\n    state = self.initial_state\n    rnn_outputs = []\n    for time_step in xrange(self.config.num_steps):\n      state = tf.nn.sigmoid(tf.matmul(state, self.H) + tf.matmul(inputs[time_step], self.I) + self.b_1)\n      rnn_outputs.append(state)\n    self.final_state = state\n    return rnn_outputs\n\n  def run_epoch(self, session, data, train_op=None, verbose=10):\n    config = self.config\n    dp = config.dropout\n    if not train_op:\n      train_op = tf.no_op()\n      dp = 1\n    total_steps = sum(1 for x in ptb_iterator(data, config.batch_size, config.num_steps))\n    total_loss = []\n    state = self.initial_state.eval()\n    for step, (x, y) in enumerate(\n      ptb_iterator(data, config.batch_size, config.num_steps)):\n      # We need to pass in the initial state and retrieve the final state to give\n      # the RNN proper history\n      feed = {self.input_placeholder: x,\n              self.labels_placeholder: y,\n              self.initial_state: state,\n              self.dropout_placeholder: dp}\n      loss, state, _ = session.run(\n          [self.calculate_loss, self.final_state, train_op], feed_dict=feed)\n      total_loss.append(loss)\n      if verbose and step % verbose == 0:\n          sys.stdout.write(\'\\r{} / {} : pp = {}\'.format(\n              step, total_steps, np.exp(np.mean(total_loss))))\n          sys.stdout.flush()\n    if verbose:\n      sys.stdout.write(\'\\r\')\n    return np.exp(np.mean(total_loss))\n\ndef feed_token(session, model, state, token):\n  state, prediction = session.run(\n      [model.final_state, model.predictions[-1]],\n      feed_dict={model.initial_state: state,\n                 model.input_placeholder: [[token]],\n                 model.dropout_placeholder: 1.0})\n  return state, prediction\n\ndef generate_text(session, model, config, starting_text=\'<eos>\',\n                  stop_length=100, stop_tokens=None, temp=1.0):\n  """"""Generate text from the model.\n\n  Hint: Create a feed-dictionary and use sess.run() to execute the model. Note\n        that you will need to use model.initial_state as a key to feed_dict\n  Hint: Fetch model.final_state and model.predictions[-1]. (You set\n        model.final_state in add_model() and model.predictions is set in\n        __init__)\n  Hint: Store the outputs of running the model in local variables state and\n        y_pred (used in the pre-implemented parts of this function.)\n\n  Args:\n    session: tf.Session() object\n    model: Object of type RNNLM_Model\n    config: A Config() object\n    starting_text: Initial text passed to model.\n  Returns:\n    output: List of word idxs\n  """"""\n  state = model.initial_state.eval()\n  # Imagine tokens as a batch size of one, length of len(tokens[0])\n  tokens = [model.vocab.encode(word) for word in starting_text.split()]\n  for i in xrange(len(tokens) - 1):\n    state, _ = feed_token(session, model, state, tokens[i])\n  for i in xrange(stop_length):\n    state, prediction = feed_token(session, model, state, tokens[-1])\n    next_word_idx = sample(prediction.flatten(), temperature=temp)\n    tokens.append(next_word_idx)\n    if stop_tokens and model.vocab.decode(tokens[-1]) in stop_tokens:\n      break\n  output = [model.vocab.decode(word_idx) for word_idx in tokens]\n  return output\n\ndef generate_sentence(session, model, config, *args, **kwargs):\n  """"""Convenice to generate a sentence from the model.""""""\n  return generate_text(session, model, config, *args, stop_tokens=[\'<eos>\'], **kwargs)\n\ndef test_RNNLM():\n  config = Config()\n  gen_config = deepcopy(config)\n  gen_config.batch_size = gen_config.num_steps = 1\n\n  # We create the training model and generative model\n  with tf.variable_scope(\'RNNLM\') as scope:\n    model = RNNLM_Model(config)\n    # This instructs gen_model to reuse the same variables as the model above\n    scope.reuse_variables()\n    gen_model = RNNLM_Model(gen_config)\n\n  init = tf.initialize_all_variables()\n  saver = tf.train.Saver()\n\n  with tf.Session() as session:\n    best_val_pp = float(\'inf\')\n    best_val_epoch = 0\n  \n    session.run(init)\n    for epoch in xrange(config.max_epochs):\n      print \'Epoch {}\'.format(epoch)\n      start = time.time()\n      ###\n      train_pp = model.run_epoch(\n          session, model.encoded_train,\n          train_op=model.train_step)\n      valid_pp = model.run_epoch(session, model.encoded_valid)\n      print \'Training perplexity: {}\'.format(train_pp)\n      print \'Validation perplexity: {}\'.format(valid_pp)\n      if valid_pp < best_val_pp:\n        best_val_pp = valid_pp\n        best_val_epoch = epoch\n        saver.save(session, \'./ptb_rnnlm.weights\')\n      if epoch - best_val_epoch > config.early_stopping:\n        break\n      print \'Total time: {}\'.format(time.time() - start)\n \n    saver.restore(session, \'ptb_rnnlm.weights\')\n    test_pp = model.run_epoch(session, model.encoded_test)\n    print \'=-=\' * 5\n    print \'Test perplexity: {}\'.format(test_pp)\n    print \'=-=\' * 5\n    starting_text = \'in palo alto\'\n    while starting_text:\n      print \' \'.join(generate_sentence(\n          session, gen_model, gen_config, starting_text=starting_text, temp=1.0))\n      starting_text = raw_input(\'> \')\n\nif __name__ == ""__main__"":\n    test_RNNLM()\n'"
assignment2/utils.py,0,"b'from collections import defaultdict\n\nimport numpy as np\n\nclass Vocab(object):\n  def __init__(self):\n    self.word_to_index = {}\n    self.index_to_word = {}\n    self.word_freq = defaultdict(int)\n    self.total_words = 0\n    self.unknown = \'<unk>\'\n    self.add_word(self.unknown, count=0)\n\n  def add_word(self, word, count=1):\n    if word not in self.word_to_index:\n      index = len(self.word_to_index)\n      self.word_to_index[word] = index\n      self.index_to_word[index] = word\n    self.word_freq[word] += count\n\n  def construct(self, words):\n    for word in words:\n      self.add_word(word)\n    self.total_words = float(sum(self.word_freq.values()))\n    print \'{} total words with {} uniques\'.format(self.total_words, len(self.word_freq))\n\n  def encode(self, word):\n    if word not in self.word_to_index:\n      word = self.unknown\n    return self.word_to_index[word]\n\n  def decode(self, index):\n    return self.index_to_word[index]\n\n  def __len__(self):\n    return len(self.word_freq)\n\ndef calculate_perplexity(log_probs):\n  # https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf\n  perp = 0\n  for p in log_probs:\n    perp += -p\n  return np.exp(perp / len(log_probs))\n\ndef get_ptb_dataset(dataset=\'train\'):\n  fn = \'data/ptb/ptb.{}.txt\'\n  for line in open(fn.format(dataset)):\n    for word in line.split():\n      yield word\n    # Add token to the end of the line\n    # Equivalent to <eos> in:\n    # https://github.com/wojzaremba/lstm/blob/master/data.lua#L32\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py#L31\n    yield \'<eos>\'\n\ndef ptb_iterator(raw_data, batch_size, num_steps):\n  # Pulled from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py#L82\n  raw_data = np.array(raw_data, dtype=np.int32)\n  data_len = len(raw_data)\n  batch_len = data_len // batch_size\n  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n  for i in range(batch_size):\n    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n  epoch_size = (batch_len - 1) // num_steps\n  if epoch_size == 0:\n    raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n  for i in range(epoch_size):\n    x = data[:, i * num_steps:(i + 1) * num_steps]\n    y = data[:, i * num_steps + 1:(i + 1) * num_steps + 1]\n    yield (x, y)\n\ndef sample(a, temperature=1.0):\n    # helper function to sample an index from a probability array\n    # from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n    a = np.log(a) / temperature\n    a = np.exp(a) / np.sum(np.exp(a))\n    return np.argmax(np.random.multinomial(1, a, 1))\n\ndef data_iterator(orig_X, orig_y=None, batch_size=32, label_size=2, shuffle=False):\n  # Optionally shuffle the data before training\n  if shuffle:\n    indices = np.random.permutation(len(orig_X))\n    data_X = orig_X[indices]\n    data_y = orig_y[indices] if np.any(orig_y) else None\n  else:\n    data_X = orig_X\n    data_y = orig_y\n  ###\n  total_processed_examples = 0\n  total_steps = int(np.ceil(len(data_X) / float(batch_size)))\n  for step in xrange(total_steps):\n    # Create the batch by selecting up to batch_size elements\n    batch_start = step * batch_size\n    x = data_X[batch_start:batch_start + batch_size]\n    # Convert our target from the class index to a one hot vector\n    y = None\n    if np.any(data_y):\n      y_indices = data_y[batch_start:batch_start + batch_size]\n      y = np.zeros((len(x), label_size), dtype=np.int32)\n      y[np.arange(len(y_indices)), y_indices] = 1\n    ###\n    yield x, y\n    total_processed_examples += len(x)\n  # Sanity check to make sure we iterated over all the dataset as intended\n  assert total_processed_examples == len(data_X), \'Expected {} and processed {}\'.format(len(data_X), total_processed_examples)\n'"
assignment3/rnn_dynamic_graph.py,41,"b'import os, sys, shutil, time, itertools\nimport math, random\nfrom collections import OrderedDict\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport tree as tr\nfrom utils import Vocab\n\nRESET_AFTER = 50\nMODEL_STR = \'rnn_embed=%d_l2=%f_lr=%f.weights\'\nSAVE_DIR = \'./weights/\'\n\n\nclass Config(object):\n  """"""Holds model hyperparams and data information.\n     Model objects are passed a Config() object at instantiation.\n  """"""\n  embed_size = 35\n  label_size = 2\n  early_stopping = 2\n  anneal_threshold = 0.99\n  anneal_by = 1.5\n  max_epochs = 30\n  lr = 0.01\n  l2 = 0.02\n\n  model_name = MODEL_STR % (embed_size, l2, lr)\n\n\nclass RNN_Model():\n\n  def load_data(self):\n    """"""Loads train/dev/test data and builds vocabulary.""""""\n    self.train_data, self.dev_data, self.test_data = tr.simplified_data(\n        700, 100, 200)\n\n    # build vocab from training data\n    self.vocab = Vocab()\n    train_sents = [t.get_words() for t in self.train_data]\n    self.vocab.construct(list(itertools.chain.from_iterable(train_sents)))\n\n  def inference(self, tree, predict_only_root=False):\n    """"""For a given tree build the RNN models computation graph up to where it\n        may be used for inference.\n    Args:\n        tree: a Tree object on which to build the computation graph for the\n          RNN\n    Returns:\n        softmax_linear: Output tensor with the computed logits.\n    """"""\n    node_tensors = self.add_model(tree.root)\n    if predict_only_root:\n      node_tensors = node_tensors[tree.root]\n    else:\n      node_tensors = [tensor for node, tensor in node_tensors.iteritems()\n                      if node.label != 2]\n      node_tensors = tf.concat(node_tensors,0)\n    return self.add_projections(node_tensors)\n\n  def add_model_vars(self):\n    \'\'\'\n    You model contains the following parameters:\n        embedding:  tensor(vocab_size, embed_size)\n        W1:         tensor(2* embed_size, embed_size)\n        b1:         tensor(1, embed_size)\n        U:          tensor(embed_size, output_size)\n        bs:         tensor(1, output_size)\n    Hint: Add the tensorflow variables to the graph here and *reuse* them\n    while building\n            the compution graphs for composition and projection for each\n            tree\n    Hint: Use a variable_scope ""Composition"" for the composition layer, and\n          ""Projection"") for the linear transformations preceding the\n          softmax.\n    \'\'\'\n    with tf.variable_scope(\'Embeddings\'):\n      tf.get_variable(\'embeddings\', [len(self.vocab), self.config.embed_size])\n    with tf.variable_scope(\'Composition\'):\n      tf.get_variable(\'W1\',\n                      [2 * self.config.embed_size, self.config.embed_size])\n      tf.get_variable(\'b1\', [1, self.config.embed_size])\n    with tf.variable_scope(\'Projection\'):\n      tf.get_variable(\'U\', [self.config.embed_size, self.config.label_size])\n      tf.get_variable(\'bs\', [1, self.config.label_size])\n\n  def embed_word(self, word):\n    with tf.variable_scope(\'Embeddings\', reuse=True):\n      embeddings = tf.get_variable(\'embeddings\')\n    with tf.device(\'/cpu:0\'):\n      return tf.expand_dims(\n          tf.nn.embedding_lookup(embeddings, self.vocab.encode(word)), 0)\n\n  def add_model(self, node):\n    """"""Recursively build the model to compute the phrase embeddings in the tree\n\n    Hint: Refer to tree.py and vocab.py before you start. Refer to\n          the model\'s vocab with self.vocab\n    Hint: Reuse the ""Composition"" variable_scope here\n    Hint: Store a node\'s vector representation in node.tensor so it can be\n          used by it\'s parent\n    Hint: If node is a leaf node, it\'s vector representation is just that of\n    the\n          word vector (see tf.gather()).\n    Args:\n        node: a Node object\n    Returns:\n        node_tensors: Dict: key = Node, value = tensor(1, embed_size)\n    """"""\n    with tf.variable_scope(\'Composition\', reuse=True):\n      W1 = tf.get_variable(\'W1\')\n      b1 = tf.get_variable(\'b1\')\n\n    node_tensors = OrderedDict()\n    curr_node_tensor = None\n    if node.isLeaf:\n      curr_node_tensor = self.embed_word(node.word)\n    else:\n      node_tensors.update(self.add_model(node.left))\n      node_tensors.update(self.add_model(node.right))\n      node_input = tf.concat(\n          [node_tensors[node.left], node_tensors[node.right]],1)\n      curr_node_tensor = tf.nn.relu(tf.matmul(node_input, W1) + b1)\n\n    node_tensors[node] = curr_node_tensor\n    return node_tensors\n\n  def add_projections(self, node_tensors):\n    """"""Add projections to the composition vectors to compute the raw sentiment scores\n\n    Hint: Reuse the ""Projection"" variable_scope here\n    Args:\n        node_tensors: tensor(?, embed_size)\n    Returns:\n        output: tensor(?, label_size)\n    """"""\n    with tf.variable_scope(\'Projection\', reuse=True):\n      U = tf.get_variable(\'U\')\n      bs = tf.get_variable(\'bs\')\n    logits = tf.matmul(node_tensors, U) + bs\n    return logits\n\n  def loss(self, logits, labels):\n    """"""Adds loss ops to the computational graph.\n\n    Hint: Use sparse_softmax_cross_entropy_with_logits\n    Hint: Remember to add l2_loss (see tf.nn.l2_loss)\n    Args:\n        logits: tensor(num_nodes, output_size)\n        labels: python list, len = num_nodes\n    Returns:\n        loss: tensor 0-D\n    """"""\n    softmax_loss = tf.reduce_sum(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(logits =logits, labels=tf.constant(\n            labels)))\n    with tf.variable_scope(\'Composition\', reuse=True):\n      W1 = tf.get_variable(\'W1\')\n    with tf.variable_scope(\'Projection\', reuse=True):\n      U = tf.get_variable(\'U\')\n    return softmax_loss + self.config.l2 * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(U)\n                                           )\n\n  def training(self, loss_tensor):\n    """"""Sets up the training Ops.\n\n    Creates an optimizer and applies the gradients to all trainable\n    variables.\n    The Op returned by this function is what must be passed to the\n    `sess.run()` call to cause the model to train. See\n\n    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n\n    for more information.\n\n    Hint: Use tf.train.GradientDescentOptimizer for this model.\n            Calling optimizer.minimize() will return a train_op object.\n\n    Args:\n        loss: tensor 0-D\n    Returns:\n        train_op: tensorflow op for training.\n    """"""\n    return tf.train.GradientDescentOptimizer(self.config.lr).minimize(\n        loss_tensor)\n\n  def predictions(self, y):\n    """"""Returns predictions from sparse scores\n\n    Args:\n        y: tensor(?, label_size)\n    Returns:\n        predictions: tensor(?)\n    """"""\n    return tf.argmax(y, 1)\n\n  def __init__(self, config):\n    self.config = config\n    self.load_data()\n\n  def predict(self, trees, weights_path, get_loss=False):\n    """"""Make predictions from the provided model.""""""\n    results = []\n    losses = []\n    for i in xrange(int(math.ceil(len(trees) / float(RESET_AFTER)))):\n      with tf.Graph().as_default(), tf.Session() as sess:\n        self.add_model_vars()\n        saver = tf.train.Saver()\n        saver.restore(sess, weights_path)\n        for tree in trees[i * RESET_AFTER:(i + 1) * RESET_AFTER]:\n          logits = self.inference(tree, True)\n          predictions = self.predictions(logits)\n          root_prediction = sess.run(predictions)[0]\n          if get_loss:\n            root_label = tree.root.label\n            loss = sess.run(self.loss(logits, [root_label]))\n            losses.append(loss)\n          results.append(root_prediction)\n    return results, losses\n\n  def run_epoch(self, new_model=False, verbose=True):\n    step = 0\n    loss_history = []\n    random.shuffle(self.train_data)\n    while step < len(self.train_data):\n      with tf.Graph().as_default(), tf.Session() as sess:\n        self.add_model_vars()\n        if new_model:\n          init = tf.initialize_all_variables()\n          sess.run(init)\n          new_model = False\n        else:\n          saver = tf.train.Saver()\n          saver.restore(sess, SAVE_DIR + \'%s.temp\' % self.config.model_name)\n        for _ in xrange(RESET_AFTER):\n          if step >= len(self.train_data):\n            break\n          tree = self.train_data[step]\n          logits = self.inference(tree)\n          labels = [l for l in tree.labels if l != 2]\n          loss_tensor = self.loss(logits, labels)\n          train_op = self.training(loss_tensor)\n          loss_value, _ = sess.run([loss_tensor, train_op])\n          loss_history.append(loss_value)\n          if verbose:\n            sys.stdout.write(\'\\r{} / {} :    loss = {}\'.format(step, len(\n                self.train_data), np.mean(loss_history)))\n            sys.stdout.flush()\n          step += 1\n        saver = tf.train.Saver()\n        if not os.path.exists(SAVE_DIR):\n          os.makedirs(SAVE_DIR)\n        saver.save(sess, SAVE_DIR + \'%s.temp\' % self.config.model_name)\n    train_preds, _ = self.predict(self.train_data,\n                                  SAVE_DIR + \'%s.temp\' % self.config.model_name)\n    val_preds, val_losses = self.predict(\n        self.dev_data,\n        SAVE_DIR + \'%s.temp\' % self.config.model_name,\n        get_loss=True)\n    train_labels = [t.root.label for t in self.train_data]\n    val_labels = [t.root.label for t in self.dev_data]\n    train_acc = np.equal(train_preds, train_labels).mean()\n    val_acc = np.equal(val_preds, val_labels).mean()\n\n    print\n    print \'Training acc (only root node): {}\'.format(train_acc)\n    print \'Valiation acc (only root node): {}\'.format(val_acc)\n    print self.make_conf(train_labels, train_preds)\n    print self.make_conf(val_labels, val_preds)\n    return train_acc, val_acc, loss_history, np.mean(val_losses)\n\n  def train(self, verbose=True):\n    complete_loss_history = []\n    train_acc_history = []\n    val_acc_history = []\n    prev_epoch_loss = float(\'inf\')\n    best_val_loss = float(\'inf\')\n    best_val_epoch = 0\n    stopped = -1\n    for epoch in xrange(self.config.max_epochs):\n      print \'epoch %d\' % epoch\n      if epoch == 0:\n        train_acc, val_acc, loss_history, val_loss = self.run_epoch(\n            new_model=True)\n      else:\n        train_acc, val_acc, loss_history, val_loss = self.run_epoch()\n      complete_loss_history.extend(loss_history)\n      train_acc_history.append(train_acc)\n      val_acc_history.append(val_acc)\n\n      #lr annealing\n      epoch_loss = np.mean(loss_history)\n      if epoch_loss > prev_epoch_loss * self.config.anneal_threshold:\n        self.config.lr /= self.config.anneal_by\n        print \'annealed lr to %f\' % self.config.lr\n      prev_epoch_loss = epoch_loss\n\n      #save if model has improved on val\n      if val_loss < best_val_loss:\n        shutil.copyfile(SAVE_DIR + \'%s.temp\' % self.config.model_name,\n                        SAVE_DIR + \'%s\' % self.config.model_name)\n        best_val_loss = val_loss\n        best_val_epoch = epoch\n\n      # if model has not imprvoved for a while stop\n      if epoch - best_val_epoch > self.config.early_stopping:\n        stopped = epoch\n        #break\n    if verbose:\n      sys.stdout.write(\'\\r\')\n      sys.stdout.flush()\n\n    print \'\\n\\nstopped at %d\\n\' % stopped\n    return {\n        \'loss_history\': complete_loss_history,\n        \'train_acc_history\': train_acc_history,\n        \'val_acc_history\': val_acc_history,\n    }\n\n  def make_conf(self, labels, predictions):\n    confmat = np.zeros([2, 2])\n    for l, p in itertools.izip(labels, predictions):\n      confmat[l, p] += 1\n    return confmat\n\n\ndef plot_loss_history(stats):\n  plt.plot(stats[\'loss_history\'])\n  plt.title(\'Loss history\')\n  plt.xlabel(\'Iteration\')\n  plt.ylabel(\'Loss\')\n  plt.savefig(\'loss_history.png\')\n  plt.show()\n\n\ndef test_RNN():\n  """"""Test RNN model implementation.\n\n  You can use this function to test your implementation of the Named Entity\n  Recognition network. When debugging, set max_epochs in the Config object to\n  1\n  so you can rapidly iterate.\n  """"""\n  config = Config()\n  model = RNN_Model(config)\n  start_time = time.time()\n  stats = model.train(verbose=True)\n  print \'Training time: {}\'.format(time.time() - start_time)\n\n  plot_loss_history(stats)\n\n  start_time = time.time()\n  val_preds, val_losses = model.predict(\n      model.dev_data,\n      SAVE_DIR + \'%s.temp\' % model.config.model_name,\n      get_loss=True)\n  val_labels = [t.root.label for t in model.dev_data]\n  val_acc = np.equal(val_preds, val_labels).mean()\n  print val_acc\n\n  print \'Test\'\n  print \'=-=-=\'\n  predictions, _ = model.predict(model.test_data,\n                                 SAVE_DIR + \'%s.temp\' % model.config.model_name)\n  labels = [t.root.label for t in model.test_data]\n  print model.make_conf(labels, predictions)\n  test_acc = np.equal(predictions, labels).mean()\n  print \'Test acc: {}\'.format(test_acc)\n  print \'Time to run inference on dev+test: {}\'.format(time.time() - start_time)\n\n\nif __name__ == \'__main__\':\n  test_RNN()\n'"
assignment3/rnn_static_graph.py,49,"b'import os, sys, shutil, time, itertools\nimport math, random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport utils\nimport tree\n\nMODEL_STR = \'rnn_embed=%d_l2=%f_lr=%f.weights\'\nSAVE_DIR = \'./weights/\'\n\n\nclass Config(object):\n  """"""Holds model hyperparams and data information.\n  Model objects are passed a Config() object at instantiation.\n  """"""\n  embed_size = 35\n  label_size = 2\n  early_stopping = 2\n  anneal_threshold = 0.99\n  anneal_by = 1.5\n  max_epochs = 30\n  lr = 0.01\n  l2 = 0.02\n\n  model_name = MODEL_STR % (embed_size, l2, lr)\n\n\nclass RecursiveNetStaticGraph():\n\n  def __init__(self, config):\n    self.config = config\n\n    # Load train data and build vocabulary\n    self.train_data, self.dev_data, self.test_data = tree.simplified_data(700,\n                                                                          100,\n                                                                          200)\n    # print(""data "",self.train_data)\n    self.vocab = utils.Vocab()\n    train_sents = [t.get_words() for t in self.train_data]\n    self.vocab.construct(list(itertools.chain.from_iterable(train_sents)))\n\n    # add input placeholders\n    self.is_leaf_placeholder = tf.placeholder(\n        tf.bool, (None), name=\'is_leaf_placeholder\')\n    self.left_children_placeholder = tf.placeholder(\n        tf.int32, (None), name=\'left_children_placeholder\')\n    self.right_children_placeholder = tf.placeholder(\n        tf.int32, (None), name=\'right_children_placeholder\')\n    self.node_word_indices_placeholder = tf.placeholder(\n        tf.int32, (None), name=\'node_word_indices_placeholder\')\n    self.labels_placeholder = tf.placeholder(\n        tf.int32, (None), name=\'labels_placeholder\')\n\n    # add model variables\n    with tf.variable_scope(\'Embeddings\'):\n      embeddings = tf.get_variable(\'embeddings\',\n                                   [len(self.vocab), self.config.embed_size])\n    with tf.variable_scope(\'Composition\'):\n      W1 = tf.get_variable(\'W1\',\n                           [2 * self.config.embed_size, self.config.embed_size])\n      b1 = tf.get_variable(\'b1\', [1, self.config.embed_size])\n    with tf.variable_scope(\'Projection\'):\n      U = tf.get_variable(\'U\', [self.config.embed_size, self.config.label_size])\n      bs = tf.get_variable(\'bs\', [1, self.config.label_size])\n\n    # build recursive graph\n\n    tensor_array = tf.TensorArray(\n        tf.float32,\n        size=0,\n        dynamic_size=True,\n        clear_after_read=False,\n        infer_shape=False)\n\n    def embed_word(word_index):\n      with tf.device(\'/cpu:0\'):\n        return tf.expand_dims(tf.gather(embeddings, word_index), 0)\n\n    def combine_children(left_tensor, right_tensor):\n      return tf.nn.relu(tf.matmul(tf.concat([left_tensor, right_tensor],1), W1) + b1)\n\n    def loop_body(tensor_array, i):\n      node_is_leaf = tf.gather(self.is_leaf_placeholder, i)\n      node_word_index = tf.gather(self.node_word_indices_placeholder, i)\n      left_child = tf.gather(self.left_children_placeholder, i)\n      right_child = tf.gather(self.right_children_placeholder, i)\n      print(left_child,""left_child"")\n      node_tensor = tf.cond(\n          node_is_leaf,\n          lambda: embed_word(node_word_index),\n          lambda: combine_children(tensor_array.read(left_child),\n                                   tensor_array.read(right_child)))\n      tensor_array = tensor_array.write(i, node_tensor)\n      i = tf.add(i, 1)\n      return tensor_array, i\n\n    loop_cond = lambda tensor_array, i: \\\n        tf.less(i, tf.squeeze(tf.shape(self.is_leaf_placeholder)))\n    self.tensor_array, _ = tf.while_loop(loop_cond, loop_body, [tensor_array, 0], parallel_iterations=1)\n\n    # add projection layer\n    self.logits = tf.matmul(self.tensor_array.concat(), U) + bs\n    self.root_logits = tf.matmul(\n        self.tensor_array.read(self.tensor_array.size() - 1), U) + bs\n    self.root_prediction = tf.squeeze(tf.argmax(self.root_logits, 1))\n\n    # add loss layer\n    regularization_loss = self.config.l2 * (\n        tf.nn.l2_loss(W1) + tf.nn.l2_loss(U))\n    included_indices = tf.where(tf.less(self.labels_placeholder, 2))\n    self.full_loss = regularization_loss + tf.reduce_sum(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=tf.gather(self.logits, included_indices),labels=tf.gather(\n                self.labels_placeholder, included_indices)))\n    self.root_loss = tf.reduce_sum(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=self.root_logits,labels=self.labels_placeholder[-1:]))\n\n    # add training op\n    self.train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(\n        self.full_loss)\n\n  def build_feed_dict(self, node):\n    nodes_list = []\n    tree.leftTraverse(node, lambda node, args: args.append(node), nodes_list)\n    node_to_index = OrderedDict()\n    for i in xrange(len(nodes_list)):\n      node_to_index[nodes_list[i]] = i\n    feed_dict = {\n        self.is_leaf_placeholder: [node.isLeaf for node in nodes_list],\n        self.left_children_placeholder: [node_to_index[node.left] if\n                                         not node.isLeaf else -1\n                                         for node in nodes_list],\n        self.right_children_placeholder: [node_to_index[node.right] if\n                                          not node.isLeaf else -1\n                                          for node in nodes_list],\n        self.node_word_indices_placeholder: [self.vocab.encode(node.word) if\n                                             node.word else -1\n                                             for node in nodes_list],\n        self.labels_placeholder: [node.label for node in nodes_list]\n    }\n    return feed_dict\n\n  def predict(self, trees, weights_path, get_loss=False):\n    """"""Make predictions from the provided model.""""""\n    results = []\n    losses = []\n    with tf.Session() as sess:\n      saver = tf.train.Saver()\n      saver.restore(sess, weights_path)\n      for tree in trees:\n        feed_dict = self.build_feed_dict(tree.root)\n        if get_loss:\n          root_prediction, loss = sess.run(\n              [self.root_prediction, self.root_loss], feed_dict=feed_dict)\n          losses.append(loss)\n        else:\n          root_prediction = sess.run(self.root_prediction, feed_dict=feed_dict)\n        results.append(root_prediction)\n    return results, losses\n\n  def run_epoch(self, new_model=False, verbose=True):\n    loss_history = []\n    # training\n    random.shuffle(self.train_data)\n    with tf.Session() as sess:\n      if new_model:\n        sess.run(tf.initialize_all_variables())\n      else:\n        saver = tf.train.Saver()\n        saver.restore(sess, SAVE_DIR + \'%s.temp\' % self.config.model_name)\n      for step, tree in enumerate(self.train_data):\n        print(tree,""tree"")\n        feed_dict = self.build_feed_dict(tree.root)\n        loss_value, _ = sess.run([self.full_loss, self.train_op],\n                                 feed_dict=feed_dict)\n        loss_history.append(loss_value)\n        if verbose:\n          sys.stdout.write(\'\\r{} / {} :    loss = {}\'.format(step, len(\n              self.train_data), np.mean(loss_history)))\n          sys.stdout.flush()\n      saver = tf.train.Saver()\n      if not os.path.exists(SAVE_DIR):\n        os.makedirs(SAVE_DIR)\n      saver.save(sess, SAVE_DIR + \'%s.temp\' % self.config.model_name)\n    # statistics\n    train_preds, _ = self.predict(self.train_data,\n                                  SAVE_DIR + \'%s.temp\' % self.config.model_name)\n    val_preds, val_losses = self.predict(\n        self.dev_data,\n        SAVE_DIR + \'%s.temp\' % self.config.model_name,\n        get_loss=True)\n    train_labels = [t.root.label for t in self.train_data]\n    val_labels = [t.root.label for t in self.dev_data]\n    train_acc = np.equal(train_preds, train_labels).mean()\n    val_acc = np.equal(val_preds, val_labels).mean()\n\n    print\n    print \'Training acc (only root node): {}\'.format(train_acc)\n    print \'Valiation acc (only root node): {}\'.format(val_acc)\n    print self.make_conf(train_labels, train_preds)\n    print self.make_conf(val_labels, val_preds)\n    return train_acc, val_acc, loss_history, np.mean(val_losses)\n\n  def train(self, verbose=True):\n    complete_loss_history = []\n    train_acc_history = []\n    val_acc_history = []\n    prev_epoch_loss = float(\'inf\')\n    best_val_loss = float(\'inf\')\n    best_val_epoch = 0\n    stopped = -1\n    for epoch in xrange(self.config.max_epochs):\n      print \'epoch %d\' % epoch\n      if epoch == 0:\n        train_acc, val_acc, loss_history, val_loss = self.run_epoch(\n            new_model=True)\n      else:\n        train_acc, val_acc, loss_history, val_loss = self.run_epoch()\n      complete_loss_history.extend(loss_history)\n      train_acc_history.append(train_acc)\n      val_acc_history.append(val_acc)\n\n      #lr annealing\n      epoch_loss = np.mean(loss_history)\n      if epoch_loss > prev_epoch_loss * self.config.anneal_threshold:\n        self.config.lr /= self.config.anneal_by\n        print \'annealed lr to %f\' % self.config.lr\n      prev_epoch_loss = epoch_loss\n\n      #save if model has improved on val\n      if val_loss < best_val_loss:\n        shutil.copyfile(SAVE_DIR + \'%s.temp\' % self.config.model_name,\n                        SAVE_DIR + \'%s\' % self.config.model_name)\n        best_val_loss = val_loss\n        best_val_epoch = epoch\n\n      # if model has not imprvoved for a while stop\n      if epoch - best_val_epoch > self.config.early_stopping:\n        stopped = epoch\n        #break\n    if verbose:\n      sys.stdout.write(\'\\r\')\n      sys.stdout.flush()\n\n    print \'\\n\\nstopped at %d\\n\' % stopped\n    return {\n        \'loss_history\': complete_loss_history,\n        \'train_acc_history\': train_acc_history,\n        \'val_acc_history\': val_acc_history,\n    }\n\n  def make_conf(self, labels, predictions):\n    confmat = np.zeros([2, 2])\n    for l, p in itertools.izip(labels, predictions):\n      confmat[l, p] += 1\n    return confmat\n\n\ndef plot_loss_history(stats):\n  plt.plot(stats[\'loss_history\'])\n  plt.title(\'Loss history\')\n  plt.xlabel(\'Iteration\')\n  plt.ylabel(\'Loss\')\n  plt.savefig(\'loss_history.png\')\n  plt.show()\n\n\ndef test_RNN():\n  """"""Test RNN model implementation.\n  """"""\n  config = Config()\n  model = RecursiveNetStaticGraph(config)\n  #graph_def = tf.get_default_graph().as_graph_def()\n  #with open(\'static_graph.pb\', \'wb\') as f:\n  #  f.write(graph_def.SerializeToString())\n\n  start_time = time.time()\n  stats = model.train(verbose=True)\n  print \'Training time: {}\'.format(time.time() - start_time)\n\n  plot_loss_history(stats)\n\n  start_time = time.time()\n  val_preds, val_losses = model.predict(\n      model.dev_data,\n      SAVE_DIR + \'%s.temp\' % model.config.model_name,\n      get_loss=True)\n  val_labels = [t.root.label for t in model.dev_data]\n  val_acc = np.equal(val_preds, val_labels).mean()\n  print val_acc\n\n  print \'-\' * 20\n  print \'Test\'\n  predictions, _ = model.predict(model.test_data,\n                                 SAVE_DIR + \'%s.temp\' % model.config.model_name)\n  labels = [t.root.label for t in model.test_data]\n  print model.make_conf(labels, predictions)\n  test_acc = np.equal(predictions, labels).mean()\n  print \'Test acc: {}\'.format(test_acc)\n  print \'Inference time, dev+test: {}\'.format(time.time() - start_time)\n  print \'-\' * 20\n\n\nif __name__ == \'__main__\':\n  test_RNN()\n  # train()\n'"
assignment3/tree.py,0,"b'import random\n# This file contains the dataset in a useful way. We populate a list of\n# Trees to train/test our Neural Nets such that each Tree contains any\n# number of Node objects.\n\n# The best way to get a feel for how these objects are used in the program is to drop pdb.set_trace() in a few places throughout the codebase\n# to see how the trees are used.. look where loadtrees() is called etc..\n\n\nclass Node:  # a node in the tree\n    def __init__(self, label, word=None):\n        self.label = label\n        self.word = word\n        self.parent = None  # reference to parent\n        self.left = None  # reference to left child\n        self.right = None  # reference to right child\n        # true if I am a leaf (could have probably derived this from if I have\n        # a word)\n        self.isLeaf = False\n        # true if we have finished performing fowardprop on this node (note,\n        # there are many ways to implement the recursion.. some might not\n        # require this flag)\n\n    def __str__(self):\n        if self.isLeaf:\n            return \'[{0}:{1}]\'.format(self.word, self.label)\n        return \'({0} <- [{1}:{2}] -> {3})\'.format(self.left, self.word, self.label, self.right)\n\n\nclass Tree:\n\n    def __init__(self, treeString, openChar=\'(\', closeChar=\')\'):\n        tokens = []\n        self.open = \'(\'\n        self.close = \')\'\n        for toks in treeString.strip().split():\n            tokens += list(toks)\n        self.root = self.parse(tokens)\n        # get list of labels as obtained through a post-order traversal\n        self.labels = get_labels(self.root)\n        self.num_words = len(self.labels)\n\n    def parse(self, tokens, parent=None):\n        assert tokens[0] == self.open, ""Malformed tree""\n        assert tokens[-1] == self.close, ""Malformed tree""\n\n        split = 2  # position after open and label\n        countOpen = countClose = 0\n\n        if tokens[split] == self.open:\n            countOpen += 1\n            split += 1\n        # Find where left child and right child split\n        while countOpen != countClose:\n            if tokens[split] == self.open:\n                countOpen += 1\n            if tokens[split] == self.close:\n                countClose += 1\n            split += 1\n\n        # New node\n        node = Node(int(tokens[1]))  # zero index labels\n\n        node.parent = parent\n\n        # leaf Node\n        if countOpen == 0:\n            node.word = \'\'.join(tokens[2:-1]).lower()  # lower case?\n            node.isLeaf = True\n            return node\n\n        node.left = self.parse(tokens[2:split], parent=node)\n        node.right = self.parse(tokens[split:-1], parent=node)\n\n        return node\n\n    def get_words(self):\n        leaves = getLeaves(self.root)\n        words = [node.word for node in leaves]\n        return words\n\n\ndef leftTraverse(node, nodeFn=None, args=None):\n    """"""\n    Recursive function traverses tree\n    from left to right. \n    Calls nodeFn at each node\n    """"""\n    if node is None:\n        return\n    leftTraverse(node.left, nodeFn, args)\n    leftTraverse(node.right, nodeFn, args)\n    nodeFn(node, args)\n\n\ndef getLeaves(node):\n    if node is None:\n        return []\n    if node.isLeaf:\n        return [node]\n    else:\n        return getLeaves(node.left) + getLeaves(node.right)\n\n\ndef get_labels(node):\n    if node is None:\n        return []\n    return get_labels(node.left) + get_labels(node.right) + [node.label]\n\n\ndef clearFprop(node, words):\n    node.fprop = False\n\n\ndef loadTrees(dataSet=\'train\'):\n    """"""\n    Loads training trees. Maps leaf node words to word ids.\n    """"""\n    file = \'trees/%s.txt\' % dataSet\n    print ""Loading %s trees.."" % dataSet\n    with open(file, \'r\') as fid:\n        trees = [Tree(l) for l in fid.readlines()]\n\n    return trees\n\ndef simplified_data(num_train, num_dev, num_test):\n    rndstate = random.getstate()\n    random.seed(0)\n    trees = loadTrees(\'train\') + loadTrees(\'dev\') + loadTrees(\'test\')\n    \n    #filter extreme trees\n    pos_trees = [t for t in trees if t.root.label==4]\n    neg_trees = [t for t in trees if t.root.label==0]\n\n    #binarize labels\n    binarize_labels(pos_trees)\n    binarize_labels(neg_trees)\n    \n    #split into train, dev, test\n    print len(pos_trees), len(neg_trees)\n    pos_trees = sorted(pos_trees, key=lambda t: len(t.get_words()))\n    neg_trees = sorted(neg_trees, key=lambda t: len(t.get_words()))\n    num_train/=2\n    num_dev/=2\n    num_test/=2\n    train = pos_trees[:num_train] + neg_trees[:num_train]\n    dev = pos_trees[num_train : num_train+num_dev] + neg_trees[num_train : num_train+num_dev]\n    test = pos_trees[num_train+num_dev : num_train+num_dev+num_test] + neg_trees[num_train+num_dev : num_train+num_dev+num_test]\n    random.shuffle(train)\n    random.shuffle(dev)\n    random.shuffle(test)\n    random.setstate(rndstate)\n\n\n    return train, dev, test\n\n\ndef binarize_labels(trees):\n    def binarize_node(node, _):\n        if node.label<2:\n            node.label = 0\n        elif node.label>2:\n            node.label = 1\n    for tree in trees:\n        leftTraverse(tree.root, binarize_node, None)\n        tree.labels = get_labels(tree.root)\n'"
assignment3/utils.py,0,"b""from collections import defaultdict\n\n\nclass Vocab(object):\n    def __init__(self):\n        self.word_to_index = {}\n        self.index_to_word = {}\n        self.word_freq = defaultdict(int)\n        self.total_words = 0\n        self.unknown = '<unk>'\n        self.add_word(self.unknown, count=0)\n\n    def add_word(self, word, count=1):\n        if word not in self.word_to_index:\n            index = len(self.word_to_index)\n            self.word_to_index[word] = index\n            self.index_to_word[index] = word\n        self.word_freq[word] += count\n\n    def construct(self, words):\n        for word in words:\n            self.add_word(word)\n        self.total_words = float(sum(self.word_freq.values()))\n        print '{} total words with {} uniques'.format(self.total_words, len(self.word_freq))\n\n    def encode(self, word):\n        assert type(word) is str\n        if word not in self.word_to_index:\n            word = self.unknown\n        return self.word_to_index[word]\n\n    def decode(self, index):\n        return self.index_to_word[index]\n\n    def __len__(self):\n        return len(self.word_freq)\n"""
assignment1/cs224d/__init__.py,0,b''
assignment1/cs224d/data_utils.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport cPickle as pickle\nimport numpy as np\nimport os\nimport random\n\nclass StanfordSentiment:\n    def __init__(self, path=None, tablesize = 1000000):\n        if not path:\n            path = ""cs224d/datasets/stanfordSentimentTreebank""\n\n        self.path = path\n        self.tablesize = tablesize\n\n    def tokens(self):\n        if hasattr(self, ""_tokens"") and self._tokens:\n            return self._tokens\n\n        tokens = dict()\n        tokenfreq = dict()\n        wordcount = 0\n        revtokens = []\n        idx = 0\n\n        for sentence in self.sentences():\n            for w in sentence:\n                wordcount += 1\n                if not w in tokens:\n                    tokens[w] = idx\n                    revtokens += [w]\n                    tokenfreq[w] = 1\n                    idx += 1\n                else:\n                    tokenfreq[w] += 1\n\n        tokens[""UNK""] = idx\n        revtokens += [""UNK""]\n        tokenfreq[""UNK""] = 1\n        wordcount += 1\n\n        self._tokens = tokens\n        self._tokenfreq = tokenfreq\n        self._wordcount = wordcount\n        self._revtokens = revtokens\n        return self._tokens\n    \n    def sentences(self):\n        if hasattr(self, ""_sentences"") and self._sentences:\n            return self._sentences\n\n        sentences = []\n        with open(self.path + ""/datasetSentences.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                splitted = line.strip().split()[1:]\n                # Deal with some peculiar encoding issues with this file\n                sentences += [[w.lower().decode(""utf-8"").encode(\'latin1\') for w in splitted]]\n                \n        self._sentences = sentences\n        self._sentlengths = np.array([len(s) for s in sentences])\n        self._cumsentlen = np.cumsum(self._sentlengths)\n\n        return self._sentences\n\n    def numSentences(self):\n        if hasattr(self, ""_numSentences"") and self._numSentences:\n            return self._numSentences\n        else:\n            self._numSentences = len(self.sentences())\n            return self._numSentences\n\n    def allSentences(self):\n        if hasattr(self, ""_allsentences"") and self._allsentences:\n            return self._allsentences\n\n        sentences = self.sentences()\n        rejectProb = self.rejectProb()\n        tokens = self.tokens()\n        allsentences = [[w for w in s \n            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n            for s in sentences * 30]\n\n        allsentences = [s for s in allsentences if len(s) > 1]\n        \n        self._allsentences = allsentences\n        \n        return self._allsentences\n\n    def getRandomContext(self, C=5):\n        allsent = self.allSentences()\n        sentID = random.randint(0, len(allsent) - 1)\n        sent = allsent[sentID]\n        wordID = random.randint(0, len(sent) - 1)\n\n        context = sent[max(0, wordID - C):wordID] \n        if wordID+1 < len(sent):\n            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n\n        centerword = sent[wordID]\n        context = [w for w in context if w != centerword]\n\n        if len(context) > 0:\n            return centerword, context\n        else:\n            return self.getRandomContext(C)\n\n    def sent_labels(self):\n        if hasattr(self, ""_sent_labels"") and self._sent_labels:\n            return self._sent_labels\n\n        dictionary = dict()\n        phrases = 0\n        with open(self.path + ""/dictionary.txt"", ""r"") as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                splitted = line.split(""|"")\n                dictionary[splitted[0].lower()] = int(splitted[1])\n                phrases += 1\n\n        labels = [0.0] * phrases\n        with open(self.path + ""/sentiment_labels.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                line = line.strip()\n                if not line: continue\n                splitted = line.split(""|"")\n                labels[int(splitted[0])] = float(splitted[1])\n\n        sent_labels = [0.0] * self.numSentences()\n        sentences = self.sentences()\n        for i in xrange(self.numSentences()):\n            sentence = sentences[i]\n            full_sent = "" "".join(sentence).replace(\'-lrb-\', \'(\').replace(\'-rrb-\', \')\')\n            sent_labels[i] = labels[dictionary[full_sent]]\n            \n        self._sent_labels = sent_labels\n        return self._sent_labels\n\n    def dataset_split(self):\n        if hasattr(self, ""_split"") and self._split:\n            return self._split\n\n        split = [[] for i in xrange(3)]\n        with open(self.path + ""/datasetSplit.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                splitted = line.strip().split("","")\n                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n\n        self._split = split\n        return self._split\n\n    def getRandomTrainSentence(self):\n        split = self.dataset_split()\n        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n\n    def categorify(self, label):\n        if label <= 0.2:\n            return 0\n        elif label <= 0.4:\n            return 1\n        elif label <= 0.6:\n            return 2\n        elif label <= 0.8:\n            return 3\n        else:\n            return 4\n\n    def getDevSentences(self):\n        return self.getSplitSentences(2)\n\n    def getTestSentences(self):\n        return self.getSplitSentences(1)\n\n    def getTrainSentences(self):\n        return self.getSplitSentences(0)\n\n    def getSplitSentences(self, split=0):\n        ds_split = self.dataset_split()\n        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n\n    def sampleTable(self):\n        if hasattr(self, \'_sampleTable\') and self._sampleTable is not None:\n            return self._sampleTable\n\n        nTokens = len(self.tokens())\n        samplingFreq = np.zeros((nTokens,))\n        self.allSentences()\n        i = 0\n        for w in xrange(nTokens):\n            w = self._revtokens[i]\n            if w in self._tokenfreq:\n                freq = 1.0 * self._tokenfreq[w]\n                # Reweigh\n                freq = freq ** 0.75\n            else:\n                freq = 0.0\n            samplingFreq[i] = freq\n            i += 1\n\n        samplingFreq /= np.sum(samplingFreq)\n        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n\n        self._sampleTable = [0] * self.tablesize\n\n        j = 0\n        for i in xrange(self.tablesize):\n            while i > samplingFreq[j]:\n                j += 1\n            self._sampleTable[i] = j\n\n        return self._sampleTable\n\n    def rejectProb(self):\n        if hasattr(self, \'_rejectProb\') and self._rejectProb is not None:\n            return self._rejectProb\n\n        threshold = 1e-5 * self._wordcount\n\n        nTokens = len(self.tokens())\n        rejectProb = np.zeros((nTokens,))\n        for i in xrange(nTokens):\n            w = self._revtokens[i]\n            freq = 1.0 * self._tokenfreq[w]\n            # Reweigh\n            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n\n        self._rejectProb = rejectProb\n        return self._rejectProb\n\n    def sampleTokenIdx(self):\n        return self.sampleTable()[random.randint(0, self.tablesize - 1)]'"
assignment2/data_utils/__init__.py,0,b''
assignment2/data_utils/ner.py,0,"b'##\n# Utility functions for NER assignment\n# Assigment 2, part 1 for CS224D\n##\n\nfrom utils import invert_dict\nfrom numpy import *\n\ndef load_wv(vocabfile, wvfile):\n    wv = loadtxt(wvfile, dtype=float)\n    with open(vocabfile) as fd:\n        words = [line.strip() for line in fd]\n    num_to_word = dict(enumerate(words))\n    word_to_num = invert_dict(num_to_word)\n    return wv, word_to_num, num_to_word\n\n\ndef save_predictions(y, filename):\n    """"""Save predictions, one per line.""""""\n    with open(filename, \'w\') as fd:\n        fd.write(""\\n"".join(map(str, y)))\n        fd.write(""\\n"")'"
assignment2/data_utils/utils.py,0,"b'import sys, os, re, json\nimport itertools\nfrom collections import Counter\nimport time\nfrom numpy import *\n\nimport pandas as pd\n\n\ndef invert_dict(d):\n    return {v:k for k,v in d.iteritems()}\n\ndef flatten1(lst):\n    return list(itertools.chain.from_iterable(lst))\n\ndef load_wv_pandas(fname):\n    return pd.read_hdf(fname, \'data\')\n\ndef extract_wv(df):\n    num_to_word = dict(enumerate(df.index))\n    word_to_num = invert_dict(num_to_word)\n    wv = df.as_matrix()\n    return wv, word_to_num, num_to_word\n\ndef canonicalize_digits(word):\n    if any([c.isalpha() for c in word]): return word\n    word = re.sub(""\\d"", ""DG"", word)\n    if word.startswith(""DG""):\n        word = word.replace("","", """") # remove thousands separator\n    return word\n\ndef canonicalize_word(word, wordset=None, digits=True):\n    word = word.lower()\n    if digits:\n        if (wordset != None) and (word in wordset): return word\n        word = canonicalize_digits(word) # try to canonicalize numbers\n    if (wordset == None) or (word in wordset): return word\n    else: return ""UUUNKKK"" # unknown token\n\n\n##\n# Utility functions used to create dataset\n##\ndef augment_wv(df, extra=[""UUUNKKK""]):\n    for e in extra:\n        df.loc[e] = zeros(len(df.columns))\n\ndef prune_wv(df, vocab, extra=[""UUUNKKK""]):\n    """"""Prune word vectors to vocabulary.""""""\n    items = set(vocab).union(set(extra))\n    return df.filter(items=items, axis=\'index\')\n\ndef load_wv_raw(fname):\n    return pd.read_table(fname, sep=""\\s+"",\n                         header=None,\n                         index_col=0,\n                         quoting=3)\n\ndef load_dataset(fname):\n    docs = []\n    with open(fname) as fd:\n        cur = []\n        for line in fd:\n            # new sentence on -DOCSTART- or blank line\n            if re.match(r""-DOCSTART-.+"", line) or (len(line.strip()) == 0):\n                if len(cur) > 0:\n                    docs.append(cur)\n                cur = []\n            else: # read in tokens\n                cur.append(line.strip().split(""\\t"",1))\n        # flush running buffer\n        docs.append(cur)\n    return docs\n\ndef extract_tag_set(docs):\n    tags = set(flatten1([[t[1].split(""|"")[0] for t in d] for d in docs]))\n    return tags\n\ndef extract_word_set(docs):\n    words = set(flatten1([[t[0] for t in d] for d in docs]))\n    return words\n\ndef pad_sequence(seq, left=1, right=1):\n    return left*[(""<s>"", """")] + seq + right*[(""</s>"", """")]\n\n##\n# For window models\ndef seq_to_windows(words, tags, word_to_num, tag_to_num, left=1, right=1):\n    ns = len(words)\n    X = []\n    y = []\n    for i in range(ns):\n        if words[i] == ""<s>"" or words[i] == ""</s>"":\n            continue # skip sentence delimiters\n        tagn = tag_to_num[tags[i]]\n        idxs = [word_to_num[words[ii]]\n                for ii in range(i - left, i + right + 1)]\n        X.append(idxs)\n        y.append(tagn)\n    return array(X), array(y)\n\ndef docs_to_windows(docs, word_to_num, tag_to_num, wsize=3):\n    pad = (wsize - 1)/2\n    docs = flatten1([pad_sequence(seq, left=pad, right=pad) for seq in docs])\n\n    words, tags = zip(*docs)\n    words = [canonicalize_word(w, word_to_num) for w in words]\n    tags = [t.split(""|"")[0] for t in tags]\n    return seq_to_windows(words, tags, word_to_num, tag_to_num, pad, pad)\n\ndef window_to_vec(window, L):\n    """"""Concatenate word vectors for a given window.""""""\n    return concatenate([L[i] for i in window])\n\n##\n# For fixed-window LM:\n# each row of X is a list of word indices\n# each entry of y is the word index to predict\ndef seq_to_lm_windows(words, word_to_num, ngram=2):\n    ns = len(words)\n    X = []\n    y = []\n    for i in range(ns):\n        if words[i] == ""<s>"":\n            continue # skip sentence begin, but do predict end\n        idxs = [word_to_num[words[ii]]\n                for ii in range(i - ngram + 1, i + 1)]\n        X.append(idxs[:-1])\n        y.append(idxs[-1])\n    return array(X), array(y)\n\ndef docs_to_lm_windows(docs, word_to_num, ngram=2):\n    docs = flatten1([pad_sequence(seq, left=(ngram-1), right=1)\n                     for seq in docs])\n    words = [canonicalize_word(wt[0], word_to_num) for wt in docs]\n    return seq_to_lm_windows(words, word_to_num, ngram)\n\n\n##\n# For RNN LM\n# just convert each sentence to a list of indices\n# after padding each with <s> ... </s> tokens\ndef seq_to_indices(words, word_to_num):\n    return array([word_to_num[w] for w in words])\n\ndef docs_to_indices(docs, word_to_num):\n    docs = [pad_sequence(seq, left=1, right=1) for seq in docs]\n    ret = []\n    for seq in docs:\n        words = [canonicalize_word(wt[0], word_to_num) for wt in seq]\n        ret.append(seq_to_indices(words, word_to_num))\n\n    # return as numpy array for fancier slicing\n    return array(ret, dtype=object)\n\ndef offset_seq(seq):\n    return seq[:-1], seq[1:]\n\ndef seqs_to_lmXY(seqs):\n    X, Y = zip(*[offset_seq(s) for s in seqs])\n    return array(X, dtype=object), array(Y, dtype=object)\n\n##\n# For RNN tagger\n# return X, Y as lists\n# where X[i] is indices, Y[i] is tags for a sequence\n# NOTE: this does not use padding tokens!\n#    (RNN should natively handle begin/end)\ndef docs_to_tag_sequence(docs, word_to_num, tag_to_num):\n    # docs = [pad_sequence(seq, left=1, right=1) for seq in docs]\n    X = []\n    Y = []\n    for seq in docs:\n        if len(seq) < 1: continue\n        words, tags = zip(*seq)\n\n        words = [canonicalize_word(w, word_to_num) for w in words]\n        x = seq_to_indices(words, word_to_num)\n        X.append(x)\n\n        tags = [t.split(""|"")[0] for t in tags]\n        y = seq_to_indices(tags, tag_to_num)\n        Y.append(y)\n\n    # return as numpy array for fancier slicing\n    return array(X, dtype=object), array(Y, dtype=object)\n\ndef idxs_to_matrix(idxs, L):\n    """"""Return a matrix X with each row\n    as a word vector for the corresponding\n    index in idxs.""""""\n    return vstack([L[i] for i in idxs])'"
