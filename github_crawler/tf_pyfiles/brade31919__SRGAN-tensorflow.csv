file_path,api_count,code
main.py,66,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nfrom lib.model import data_loader, generator, SRGAN, test_data_loader, inference_data_loader, save_images, SRResnet\nfrom lib.ops import *\nimport math\nimport time\nimport numpy as np\n\nFlags = tf.app.flags\n\n# The system parameter\nFlags.DEFINE_string(\'output_dir\', None, \'The output directory of the checkpoint\')\nFlags.DEFINE_string(\'summary_dir\', None, \'The dirctory to output the summary\')\nFlags.DEFINE_string(\'mode\', \'train\', \'The mode of the model train, test.\')\nFlags.DEFINE_string(\'checkpoint\', None, \'If provided, the weight will be restored from the provided checkpoint\')\nFlags.DEFINE_boolean(\'pre_trained_model\', False, \'If set True, the weight will be loaded but the global_step will still \'\n                                                 \'be 0. If set False, you are going to continue the training. That is, \'\n                                                 \'the global_step will be initiallized from the checkpoint, too\')\nFlags.DEFINE_string(\'pre_trained_model_type\', \'SRResnet\', \'The type of pretrained model (SRGAN or SRResnet)\')\nFlags.DEFINE_boolean(\'is_training\', True, \'Training => True, Testing => False\')\nFlags.DEFINE_string(\'vgg_ckpt\', \'./vgg19/vgg_19.ckpt\', \'path to checkpoint file for the vgg19\')\nFlags.DEFINE_string(\'task\', None, \'The task: SRGAN, SRResnet\')\n# The data preparing operation\nFlags.DEFINE_integer(\'batch_size\', 16, \'Batch size of the input batch\')\nFlags.DEFINE_string(\'input_dir_LR\', None, \'The directory of the input resolution input data\')\nFlags.DEFINE_string(\'input_dir_HR\', None, \'The directory of the high resolution input data\')\nFlags.DEFINE_boolean(\'flip\', True, \'Whether random flip data augmentation is applied\')\nFlags.DEFINE_boolean(\'random_crop\', True, \'Whether perform the random crop\')\nFlags.DEFINE_integer(\'crop_size\', 24, \'The crop size of the training image\')\nFlags.DEFINE_integer(\'name_queue_capacity\', 2048, \'The capacity of the filename queue (suggest large to ensure\'\n                                                  \'enough random shuffle.\')\nFlags.DEFINE_integer(\'image_queue_capacity\', 2048, \'The capacity of the image queue (suggest large to ensure\'\n                                                   \'enough random shuffle\')\nFlags.DEFINE_integer(\'queue_thread\', 10, \'The threads of the queue (More threads can speedup the training process.\')\n# Generator configuration\nFlags.DEFINE_integer(\'num_resblock\', 16, \'How many residual blocks are there in the generator\')\n# The content loss parameter\nFlags.DEFINE_string(\'perceptual_mode\', \'VGG54\', \'The type of feature used in perceptual loss\')\nFlags.DEFINE_float(\'EPS\', 1e-12, \'The eps added to prevent nan\')\nFlags.DEFINE_float(\'ratio\', 0.001, \'The ratio between content loss and adversarial loss\')\nFlags.DEFINE_float(\'vgg_scaling\', 0.0061, \'The scaling factor for the perceptual loss if using vgg perceptual loss\')\n# The training parameters\nFlags.DEFINE_float(\'learning_rate\', 0.0001, \'The learning rate for the network\')\nFlags.DEFINE_integer(\'decay_step\', 500000, \'The steps needed to decay the learning rate\')\nFlags.DEFINE_float(\'decay_rate\', 0.1, \'The decay rate of each decay step\')\nFlags.DEFINE_boolean(\'stair\', False, \'Whether perform staircase decay. True => decay in discrete interval.\')\nFlags.DEFINE_float(\'beta\', 0.9, \'The beta1 parameter for the Adam optimizer\')\nFlags.DEFINE_integer(\'max_epoch\', None, \'The max epoch for the training\')\nFlags.DEFINE_integer(\'max_iter\', 1000000, \'The max iteration of the training\')\nFlags.DEFINE_integer(\'display_freq\', 20, \'The diplay frequency of the training process\')\nFlags.DEFINE_integer(\'summary_freq\', 100, \'The frequency of writing summary\')\nFlags.DEFINE_integer(\'save_freq\', 10000, \'The frequency of saving images\')\n\n\nFLAGS = Flags.FLAGS\n\n# Print the configuration of the model\nprint_configuration_op(FLAGS)\n\n# Check the output_dir is given\nif FLAGS.output_dir is None:\n    raise ValueError(\'The output directory is needed\')\n\n# Check the output directory to save the checkpoint\nif not os.path.exists(FLAGS.output_dir):\n    os.mkdir(FLAGS.output_dir)\n\n# Check the summary directory to save the event\nif not os.path.exists(FLAGS.summary_dir):\n    os.mkdir(FLAGS.summary_dir)\n\n# The testing mode\nif FLAGS.mode == \'test\':\n    # Check the checkpoint\n    if FLAGS.checkpoint is None:\n        raise ValueError(\'The checkpoint file is needed to performing the test.\')\n\n    # In the testing time, no flip and crop is needed\n    if FLAGS.flip == True:\n        FLAGS.flip = False\n\n    if FLAGS.crop_size is not None:\n        FLAGS.crop_size = None\n\n    # Declare the test data reader\n    test_data = test_data_loader(FLAGS)\n\n    inputs_raw = tf.placeholder(tf.float32, shape=[1, None, None, 3], name=\'inputs_raw\')\n    targets_raw = tf.placeholder(tf.float32, shape=[1, None, None, 3], name=\'targets_raw\')\n    path_LR = tf.placeholder(tf.string, shape=[], name=\'path_LR\')\n    path_HR = tf.placeholder(tf.string, shape=[], name=\'path_HR\')\n\n    with tf.variable_scope(\'generator\'):\n        if FLAGS.task == \'SRGAN\' or FLAGS.task == \'SRResnet\':\n            gen_output = generator(inputs_raw, 3, reuse=False, FLAGS=FLAGS)\n        else:\n            raise NotImplementedError(\'Unknown task!!\')\n\n    print(\'Finish building the network\')\n\n    with tf.name_scope(\'convert_image\'):\n        # Deprocess the images outputed from the model\n        inputs = deprocessLR(inputs_raw)\n        targets = deprocess(targets_raw)\n        outputs = deprocess(gen_output)\n\n        # Convert back to uint8\n        converted_inputs = tf.image.convert_image_dtype(inputs, dtype=tf.uint8, saturate=True)\n        converted_targets = tf.image.convert_image_dtype(targets, dtype=tf.uint8, saturate=True)\n        converted_outputs = tf.image.convert_image_dtype(outputs, dtype=tf.uint8, saturate=True)\n\n    with tf.name_scope(\'encode_image\'):\n        save_fetch = {\n            ""path_LR"": path_LR,\n            ""path_HR"": path_HR,\n            ""inputs"": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\'input_pngs\'),\n            ""outputs"": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\'output_pngs\'),\n            ""targets"": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=\'target_pngs\')\n        }\n\n    # Define the weight initiallizer (In inference time, we only need to restore the weight of the generator)\n    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'generator\')\n    weight_initiallizer = tf.train.Saver(var_list)\n\n    # Define the initialization operation\n    init_op = tf.global_variables_initializer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        # Load the pretrained model\n        print(\'Loading weights from the pre-trained model\')\n        weight_initiallizer.restore(sess, FLAGS.checkpoint)\n\n        max_iter = len(test_data.inputs)\n        print(\'Evaluation starts!!\')\n        for i in range(max_iter):\n            input_im = np.array([test_data.inputs[i]]).astype(np.float32)\n            target_im = np.array([test_data.targets[i]]).astype(np.float32)\n            path_lr = test_data.paths_LR[i]\n            path_hr = test_data.paths_HR[i]\n            results = sess.run(save_fetch, feed_dict={inputs_raw: input_im, targets_raw: target_im,\n                                                      path_LR: path_lr, path_HR: path_hr})\n            filesets = save_images(results, FLAGS)\n            for i, f in enumerate(filesets):\n                print(\'evaluate image\', f[\'name\'])\n\n\n# the inference mode (just perform super resolution on the input image)\nelif FLAGS.mode == \'inference\':\n    # Check the checkpoint\n    if FLAGS.checkpoint is None:\n        raise ValueError(\'The checkpoint file is needed to performing the test.\')\n\n    # In the testing time, no flip and crop is needed\n    if FLAGS.flip == True:\n        FLAGS.flip = False\n\n    if FLAGS.crop_size is not None:\n        FLAGS.crop_size = None\n\n    # Declare the test data reader\n    inference_data = inference_data_loader(FLAGS)\n\n    inputs_raw = tf.placeholder(tf.float32, shape=[1, None, None, 3], name=\'inputs_raw\')\n    path_LR = tf.placeholder(tf.string, shape=[], name=\'path_LR\')\n\n    with tf.variable_scope(\'generator\'):\n        if FLAGS.task == \'SRGAN\' or FLAGS.task == \'SRResnet\':\n            gen_output = generator(inputs_raw, 3, reuse=False, FLAGS=FLAGS)\n        else:\n            raise NotImplementedError(\'Unknown task!!\')\n\n    print(\'Finish building the network\')\n\n    with tf.name_scope(\'convert_image\'):\n        # Deprocess the images outputed from the model\n        inputs = deprocessLR(inputs_raw)\n        outputs = deprocess(gen_output)\n\n        # Convert back to uint8\n        converted_inputs = tf.image.convert_image_dtype(inputs, dtype=tf.uint8, saturate=True)\n        converted_outputs = tf.image.convert_image_dtype(outputs, dtype=tf.uint8, saturate=True)\n\n    with tf.name_scope(\'encode_image\'):\n        save_fetch = {\n            ""path_LR"": path_LR,\n            ""inputs"": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\'input_pngs\'),\n            ""outputs"": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\'output_pngs\')\n        }\n\n    # Define the weight initiallizer (In inference time, we only need to restore the weight of the generator)\n    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'generator\')\n    weight_initiallizer = tf.train.Saver(var_list)\n\n    # Define the initialization operation\n    init_op = tf.global_variables_initializer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        # Load the pretrained model\n        print(\'Loading weights from the pre-trained model\')\n        weight_initiallizer.restore(sess, FLAGS.checkpoint)\n\n        max_iter = len(inference_data.inputs)\n        print(\'Evaluation starts!!\')\n        for i in range(max_iter):\n            input_im = np.array([inference_data.inputs[i]]).astype(np.float32)\n            path_lr = inference_data.paths_LR[i]\n            results = sess.run(save_fetch, feed_dict={inputs_raw: input_im, path_LR: path_lr})\n            filesets = save_images(results, FLAGS)\n            for i, f in enumerate(filesets):\n                print(\'evaluate image\', f[\'name\'])\n\n\n# The training mode\nelif FLAGS.mode == \'train\':\n    # Load data for training and testing\n    # ToDo Add online downscaling\n    data = data_loader(FLAGS)\n    print(\'Data count = %d\' % (data.image_count))\n\n    # Connect to the network\n    if FLAGS.task == \'SRGAN\':\n        Net = SRGAN(data.inputs, data.targets, FLAGS)\n    elif FLAGS.task ==\'SRResnet\':\n        Net = SRResnet(data.inputs, data.targets, FLAGS)\n    else:\n        raise NotImplementedError(\'Unknown task type\')\n\n    print(\'Finish building the network!!!\')\n\n    # Convert the images output from the network\n    with tf.name_scope(\'convert_image\'):\n        # Deprocess the images outputed from the model\n        inputs = deprocessLR(data.inputs)\n        targets = deprocess(data.targets)\n        outputs = deprocess(Net.gen_output)\n\n        # Convert back to uint8\n        converted_inputs = tf.image.convert_image_dtype(inputs, dtype=tf.uint8, saturate=True)\n        converted_targets = tf.image.convert_image_dtype(targets, dtype=tf.uint8, saturate=True)\n        converted_outputs = tf.image.convert_image_dtype(outputs, dtype=tf.uint8, saturate=True)\n\n    # Compute PSNR\n    with tf.name_scope(""compute_psnr""):\n        psnr = compute_psnr(converted_targets, converted_outputs)\n\n    # Add image summaries\n    with tf.name_scope(\'inputs_summary\'):\n        tf.summary.image(\'input_summary\', converted_inputs)\n\n    with tf.name_scope(\'targets_summary\'):\n        tf.summary.image(\'target_summary\', converted_targets)\n\n    with tf.name_scope(\'outputs_summary\'):\n        tf.summary.image(\'outputs_summary\', converted_outputs)\n\n    # Add scalar summary\n    if FLAGS.task == \'SRGAN\':\n        tf.summary.scalar(\'discriminator_loss\', Net.discrim_loss)\n        tf.summary.scalar(\'adversarial_loss\', Net.adversarial_loss)\n        tf.summary.scalar(\'content_loss\', Net.content_loss)\n        tf.summary.scalar(\'generator_loss\', Net.content_loss + FLAGS.ratio*Net.adversarial_loss)\n        tf.summary.scalar(\'PSNR\', psnr)\n        tf.summary.scalar(\'learning_rate\', Net.learning_rate)\n    elif FLAGS.task == \'SRResnet\':\n        tf.summary.scalar(\'content_loss\', Net.content_loss)\n        tf.summary.scalar(\'generator_loss\', Net.content_loss)\n        tf.summary.scalar(\'PSNR\', psnr)\n        tf.summary.scalar(\'learning_rate\', Net.learning_rate)\n\n\n    # Define the saver and weight initiallizer\n    saver = tf.train.Saver(max_to_keep=10)\n\n    # The variable list\n    var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    # Here if we restore the weight from the SRResnet the var_list2 do not need to contain the discriminator weights\n    # On contrary, if you initial your weight from other SRGAN checkpoint, var_list2 need to contain discriminator\n    # weights.\n    if FLAGS.task == \'SRGAN\':\n        if FLAGS.pre_trained_model_type == \'SRGAN\':\n            var_list2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\') + \\\n                      tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'discriminator\')\n        elif FLAGS.pre_trained_model_type == \'SRResnet\':\n            var_list2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\n        else:\n            raise ValueError(\'Unknown pre_trained model type!!\')\n    elif FLAGS.task == \'SRResnet\':\n        var_list2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\n\n    weight_initiallizer = tf.train.Saver(var_list2)\n\n    # When using MSE loss, no need to restore the vgg net\n    if not FLAGS.perceptual_mode == \'MSE\':\n        vgg_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'vgg_19\')\n        vgg_restore = tf.train.Saver(vgg_var_list)\n\n    # Start the session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    # Use superviser to coordinate all queue and summary writer\n    sv = tf.train.Supervisor(logdir=FLAGS.summary_dir, save_summaries_secs=0, saver=None)\n    with sv.managed_session(config=config) as sess:\n        if (FLAGS.checkpoint is not None) and (FLAGS.pre_trained_model is False):\n            print(\'Loading model from the checkpoint...\')\n            checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoint)\n            saver.restore(sess, checkpoint)\n\n        elif (FLAGS.checkpoint is not None) and (FLAGS.pre_trained_model is True):\n            print(\'Loading weights from the pre-trained model\')\n            weight_initiallizer.restore(sess, FLAGS.checkpoint)\n\n        if not FLAGS.perceptual_mode == \'MSE\':\n            vgg_restore.restore(sess, FLAGS.vgg_ckpt)\n            print(\'VGG19 restored successfully!!\')\n\n        # Performing the training\n        if FLAGS.max_epoch is None:\n            if FLAGS.max_iter is None:\n                raise ValueError(\'one of max_epoch or max_iter should be provided\')\n            else:\n                max_iter = FLAGS.max_iter\n        else:\n            max_iter = FLAGS.max_epoch * data.steps_per_epoch\n\n        print(\'Optimization starts!!!\')\n        start = time.time()\n        for step in range(max_iter):\n            fetches = {\n                ""train"": Net.train,\n                ""global_step"": sv.global_step,\n            }\n\n            if ((step+1) % FLAGS.display_freq) == 0:\n                if FLAGS.task == \'SRGAN\':\n                    fetches[""discrim_loss""] = Net.discrim_loss\n                    fetches[""adversarial_loss""] = Net.adversarial_loss\n                    fetches[""content_loss""] = Net.content_loss\n                    fetches[""PSNR""] = psnr\n                    fetches[""learning_rate""] = Net.learning_rate\n                    fetches[""global_step""] = Net.global_step\n                elif FLAGS.task == \'SRResnet\':\n                    fetches[""content_loss""] = Net.content_loss\n                    fetches[""PSNR""] = psnr\n                    fetches[""learning_rate""] = Net.learning_rate\n                    fetches[""global_step""] = Net.global_step\n\n            if ((step+1) % FLAGS.summary_freq) == 0:\n                fetches[""summary""] = sv.summary_op\n\n            results = sess.run(fetches)\n\n            if ((step + 1) % FLAGS.summary_freq) == 0:\n                print(\'Recording summary!!\')\n                sv.summary_writer.add_summary(results[\'summary\'], results[\'global_step\'])\n\n            if ((step + 1) % FLAGS.display_freq) == 0:\n                train_epoch = math.ceil(results[""global_step""] / data.steps_per_epoch)\n                train_step = (results[""global_step""] - 1) % data.steps_per_epoch + 1\n                rate = (step + 1) * FLAGS.batch_size / (time.time() - start)\n                remaining = (max_iter - step) * FLAGS.batch_size / rate\n                print(""progress  epoch %d  step %d  image/sec %0.1f  remaining %dm"" % (train_epoch, train_step, rate, remaining / 60))\n                if FLAGS.task == \'SRGAN\':\n                    print(""global_step"", results[""global_step""])\n                    print(""PSNR"", results[""PSNR""])\n                    print(""discrim_loss"", results[""discrim_loss""])\n                    print(""adversarial_loss"", results[""adversarial_loss""])\n                    print(""content_loss"", results[""content_loss""])\n                    print(""learning_rate"", results[\'learning_rate\'])\n                elif FLAGS.task == \'SRResnet\':\n                    print(""global_step"", results[""global_step""])\n                    print(""PSNR"", results[""PSNR""])\n                    print(""content_loss"", results[""content_loss""])\n                    print(""learning_rate"", results[\'learning_rate\'])\n\n            if ((step +1) % FLAGS.save_freq) == 0:\n                print(\'Save the checkpoint\')\n                saver.save(sess, os.path.join(FLAGS.output_dir, \'model\'), global_step=sv.global_step)\n\n        print(\'Optimization done!!!!!!!!!!!!\')\n\n\n\n\n\n\n\n\n'"
lib/__init__.py,0,b''
lib/model.py,104,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom lib.ops import *\nimport collections\nimport os\nimport math\nimport scipy.misc as sic\nimport numpy as np\n\n\n# Define the dataloader\ndef data_loader(FLAGS):\n    with tf.device(\'/cpu:0\'):\n        # Define the returned data batches\n        Data = collections.namedtuple(\'Data\', \'paths_LR, paths_HR, inputs, targets, image_count, steps_per_epoch\')\n\n        #Check the input directory\n        if (FLAGS.input_dir_LR == \'None\') or (FLAGS.input_dir_HR == \'None\'):\n            raise ValueError(\'Input directory is not provided\')\n\n        if (not os.path.exists(FLAGS.input_dir_LR)) or (not os.path.exists(FLAGS.input_dir_HR)):\n            raise ValueError(\'Input directory not found\')\n\n        image_list_LR = os.listdir(FLAGS.input_dir_LR)\n        image_list_LR = [_ for _ in image_list_LR if _.endswith(\'.png\')]\n        if len(image_list_LR)==0:\n            raise Exception(\'No png files in the input directory\')\n\n        image_list_LR_temp = sorted(image_list_LR)\n        image_list_LR = [os.path.join(FLAGS.input_dir_LR, _) for _ in image_list_LR_temp]\n        image_list_HR = [os.path.join(FLAGS.input_dir_HR, _) for _ in image_list_LR_temp]\n\n        image_list_LR_tensor = tf.convert_to_tensor(image_list_LR, dtype=tf.string)\n        image_list_HR_tensor = tf.convert_to_tensor(image_list_HR, dtype=tf.string)\n\n        with tf.variable_scope(\'load_image\'):\n            # define the image list queue\n            # image_list_LR_queue = tf.train.string_input_producer(image_list_LR, shuffle=False, capacity=FLAGS.name_queue_capacity)\n            # image_list_HR_queue = tf.train.string_input_producer(image_list_HR, shuffle=False, capacity=FLAGS.name_queue_capacity)\n            #print(\'[Queue] image list queue use shuffle: %s\'%(FLAGS.mode == \'Train\'))\n            output = tf.train.slice_input_producer([image_list_LR_tensor, image_list_HR_tensor],\n                                                   shuffle=False, capacity=FLAGS.name_queue_capacity)\n\n            # Reading and decode the images\n            reader = tf.WholeFileReader(name=\'image_reader\')\n            image_LR = tf.read_file(output[0])\n            image_HR = tf.read_file(output[1])\n            input_image_LR = tf.image.decode_png(image_LR, channels=3)\n            input_image_HR = tf.image.decode_png(image_HR, channels=3)\n            input_image_LR = tf.image.convert_image_dtype(input_image_LR, dtype=tf.float32)\n            input_image_HR = tf.image.convert_image_dtype(input_image_HR, dtype=tf.float32)\n\n            assertion = tf.assert_equal(tf.shape(input_image_LR)[2], 3, message=""image does not have 3 channels"")\n            with tf.control_dependencies([assertion]):\n                input_image_LR = tf.identity(input_image_LR)\n                input_image_HR = tf.identity(input_image_HR)\n\n            # Normalize the low resolution image to [0, 1], high resolution to [-1, 1]\n            a_image = preprocessLR(input_image_LR)\n            b_image = preprocess(input_image_HR)\n\n            inputs, targets = [a_image, b_image]\n\n        # The data augmentation part\n        with tf.name_scope(\'data_preprocessing\'):\n            with tf.name_scope(\'random_crop\'):\n                # Check whether perform crop\n                if (FLAGS.random_crop is True) and FLAGS.mode == \'train\':\n                    print(\'[Config] Use random crop\')\n                    # Set the shape of the input image. the target will have 4X size\n                    input_size = tf.shape(inputs)\n                    target_size = tf.shape(targets)\n                    offset_w = tf.cast(tf.floor(tf.random_uniform([], 0, tf.cast(input_size[1], tf.float32) - FLAGS.crop_size)),\n                                       dtype=tf.int32)\n                    offset_h = tf.cast(tf.floor(tf.random_uniform([], 0, tf.cast(input_size[0], tf.float32) - FLAGS.crop_size)),\n                                       dtype=tf.int32)\n\n                    if FLAGS.task == \'SRGAN\' or FLAGS.task == \'SRResnet\':\n                        inputs = tf.image.crop_to_bounding_box(inputs, offset_h, offset_w, FLAGS.crop_size,\n                                                               FLAGS.crop_size)\n                        targets = tf.image.crop_to_bounding_box(targets, offset_h*4, offset_w*4, FLAGS.crop_size*4,\n                                                                FLAGS.crop_size*4)\n                    elif FLAGS.task == \'denoise\':\n                        inputs = tf.image.crop_to_bounding_box(inputs, offset_h, offset_w, FLAGS.crop_size,\n                                                               FLAGS.crop_size)\n                        targets = tf.image.crop_to_bounding_box(targets, offset_h, offset_w,\n                                                                FLAGS.crop_size, FLAGS.crop_size)\n                # Do not perform crop\n                else:\n                    inputs = tf.identity(inputs)\n                    targets = tf.identity(targets)\n\n            with tf.variable_scope(\'random_flip\'):\n                # Check for random flip:\n                if (FLAGS.flip is True) and (FLAGS.mode == \'train\'):\n                    print(\'[Config] Use random flip\')\n                    # Produce the decision of random flip\n                    decision = tf.random_uniform([], 0, 1, dtype=tf.float32)\n\n                    input_images = random_flip(inputs, decision)\n                    target_images = random_flip(targets, decision)\n                else:\n                    input_images = tf.identity(inputs)\n                    target_images = tf.identity(targets)\n\n            if FLAGS.task == \'SRGAN\' or FLAGS.task == \'SRResnet\':\n                input_images.set_shape([FLAGS.crop_size, FLAGS.crop_size, 3])\n                target_images.set_shape([FLAGS.crop_size*4, FLAGS.crop_size*4, 3])\n            elif FLAGS.task == \'denoise\':\n                input_images.set_shape([FLAGS.crop_size, FLAGS.crop_size, 3])\n                target_images.set_shape([FLAGS.crop_size, FLAGS.crop_size, 3])\n\n        if FLAGS.mode == \'train\':\n            paths_LR_batch, paths_HR_batch, inputs_batch, targets_batch = tf.train.shuffle_batch([output[0], output[1], input_images, target_images],\n                                            batch_size=FLAGS.batch_size, capacity=FLAGS.image_queue_capacity+4*FLAGS.batch_size,\n                                            min_after_dequeue=FLAGS.image_queue_capacity, num_threads=FLAGS.queue_thread)\n        else:\n            paths_LR_batch, paths_HR_batch, inputs_batch, targets_batch = tf.train.batch([output[0], output[1], input_images, target_images],\n                                            batch_size=FLAGS.batch_size, num_threads=FLAGS.queue_thread, allow_smaller_final_batch=True)\n\n        steps_per_epoch = int(math.ceil(len(image_list_LR) / FLAGS.batch_size))\n        if FLAGS.task == \'SRGAN\' or FLAGS.task == \'SRResnet\':\n            inputs_batch.set_shape([FLAGS.batch_size, FLAGS.crop_size, FLAGS.crop_size, 3])\n            targets_batch.set_shape([FLAGS.batch_size, FLAGS.crop_size*4, FLAGS.crop_size*4, 3])\n        elif FLAGS.task == \'denoise\':\n            inputs_batch.set_shape([FLAGS.batch_size, FLAGS.crop_size, FLAGS.crop_size, 3])\n            targets_batch.set_shape([FLAGS.batch_size, FLAGS.crop_size, FLAGS.crop_size, 3])\n    return Data(\n        paths_LR=paths_LR_batch,\n        paths_HR=paths_HR_batch,\n        inputs=inputs_batch,\n        targets=targets_batch,\n        image_count=len(image_list_LR),\n        steps_per_epoch=steps_per_epoch\n    )\n\n\n# The test data loader. Allow input image with different size\ndef test_data_loader(FLAGS):\n    # Get the image name list\n    if (FLAGS.input_dir_LR == \'None\') or (FLAGS.input_dir_HR == \'None\'):\n        raise ValueError(\'Input directory is not provided\')\n\n    if (not os.path.exists(FLAGS.input_dir_LR)) or (not os.path.exists(FLAGS.input_dir_HR)):\n        raise ValueError(\'Input directory not found\')\n\n    image_list_LR_temp = os.listdir(FLAGS.input_dir_LR)\n    image_list_LR = [os.path.join(FLAGS.input_dir_LR, _) for _ in image_list_LR_temp if _.split(\'.\')[-1] == \'png\']\n    image_list_HR = [os.path.join(FLAGS.input_dir_HR, _) for _ in image_list_LR_temp if _.split(\'.\')[-1] == \'png\']\n\n    # Read in and preprocess the images\n    def preprocess_test(name, mode):\n        im = sic.imread(name, mode=""RGB"").astype(np.float32)\n        # check grayscale image\n        if im.shape[-1] != 3:\n            h, w = im.shape\n            temp = np.empty((h, w, 3), dtype=np.uint8)\n            temp[:, :, :] = im[:, :, np.newaxis]\n            im = temp.copy()\n        if mode == \'LR\':\n            im = im / np.max(im)\n        elif mode == \'HR\':\n            im = im / np.max(im)\n            im = im * 2 - 1\n\n        return im\n\n    image_LR = [preprocess_test(_, \'LR\') for _ in image_list_LR]\n    image_HR = [preprocess_test(_, \'HR\') for _ in image_list_HR]\n\n    # Push path and image into a list\n    Data = collections.namedtuple(\'Data\', \'paths_LR, paths_HR, inputs, targets\')\n\n    return Data(\n        paths_LR = image_list_LR,\n        paths_HR = image_list_HR,\n        inputs = image_LR,\n        targets = image_HR\n    )\n\n\n# The inference data loader. Allow input image with different size\ndef inference_data_loader(FLAGS):\n    # Get the image name list\n    if (FLAGS.input_dir_LR == \'None\'):\n        raise ValueError(\'Input directory is not provided\')\n\n    if not os.path.exists(FLAGS.input_dir_LR):\n        raise ValueError(\'Input directory not found\')\n\n    image_list_LR_temp = os.listdir(FLAGS.input_dir_LR)\n    image_list_LR = [os.path.join(FLAGS.input_dir_LR, _) for _ in image_list_LR_temp if _.split(\'.\')[-1] == \'png\']\n\n    # Read in and preprocess the images\n    def preprocess_test(name):\n        im = sic.imread(name, mode=""RGB"").astype(np.float32)\n        # check grayscale image\n        if im.shape[-1] != 3:\n            h, w = im.shape\n            temp = np.empty((h, w, 3), dtype=np.uint8)\n            temp[:, :, :] = im[:, :, np.newaxis]\n            im = temp.copy()\n        im = im / np.max(im)\n\n        return im\n\n    image_LR = [preprocess_test(_) for _ in image_list_LR]\n\n    # Push path and image into a list\n    Data = collections.namedtuple(\'Data\', \'paths_LR, inputs\')\n\n    return Data(\n        paths_LR=image_list_LR,\n        inputs=image_LR\n    )\n\n\n# Definition of the generator\ndef generator(gen_inputs, gen_output_channels, reuse=False, FLAGS=None):\n    # Check the flag\n    if FLAGS is None:\n        raise  ValueError(\'No FLAGS is provided for generator\')\n\n    # The Bx residual blocks\n    def residual_block(inputs, output_channel, stride, scope):\n        with tf.variable_scope(scope):\n            net = conv2(inputs, 3, output_channel, stride, use_bias=False, scope=\'conv_1\')\n            net = batchnorm(net, FLAGS.is_training)\n            net = prelu_tf(net)\n            net = conv2(net, 3, output_channel, stride, use_bias=False, scope=\'conv_2\')\n            net = batchnorm(net, FLAGS.is_training)\n            net = net + inputs\n\n        return net\n\n\n    with tf.variable_scope(\'generator_unit\', reuse=reuse):\n        # The input layer\n        with tf.variable_scope(\'input_stage\'):\n            net = conv2(gen_inputs, 9, 64, 1, scope=\'conv\')\n            net = prelu_tf(net)\n\n        stage1_output = net\n\n        # The residual block parts\n        for i in range(1, FLAGS.num_resblock+1 , 1):\n            name_scope = \'resblock_%d\'%(i)\n            net = residual_block(net, 64, 1, name_scope)\n\n        with tf.variable_scope(\'resblock_output\'):\n            net = conv2(net, 3, 64, 1, use_bias=False, scope=\'conv\')\n            net = batchnorm(net, FLAGS.is_training)\n\n        net = net + stage1_output\n\n        with tf.variable_scope(\'subpixelconv_stage1\'):\n            net = conv2(net, 3, 256, 1, scope=\'conv\')\n            net = pixelShuffler(net, scale=2)\n            net = prelu_tf(net)\n\n        with tf.variable_scope(\'subpixelconv_stage2\'):\n            net = conv2(net, 3, 256, 1, scope=\'conv\')\n            net = pixelShuffler(net, scale=2)\n            net = prelu_tf(net)\n\n        with tf.variable_scope(\'output_stage\'):\n            net = conv2(net, 9, gen_output_channels, 1, scope=\'conv\')\n\n    return net\n\n\n# Definition of the discriminator\ndef discriminator(dis_inputs, FLAGS=None):\n    if FLAGS is None:\n        raise ValueError(\'No FLAGS is provided for generator\')\n\n    # Define the discriminator block\n    def discriminator_block(inputs, output_channel, kernel_size, stride, scope):\n        with tf.variable_scope(scope):\n            net = conv2(inputs, kernel_size, output_channel, stride, use_bias=False, scope=\'conv1\')\n            net = batchnorm(net, FLAGS.is_training)\n            net = lrelu(net, 0.2)\n\n        return net\n\n    with tf.device(\'/gpu:0\'):\n        with tf.variable_scope(\'discriminator_unit\'):\n            # The input layer\n            with tf.variable_scope(\'input_stage\'):\n                net = conv2(dis_inputs, 3, 64, 1, scope=\'conv\')\n                net = lrelu(net, 0.2)\n\n            # The discriminator block part\n            # block 1\n            net = discriminator_block(net, 64, 3, 2, \'disblock_1\')\n\n            # block 2\n            net = discriminator_block(net, 128, 3, 1, \'disblock_2\')\n\n            # block 3\n            net = discriminator_block(net, 128, 3, 2, \'disblock_3\')\n\n            # block 4\n            net = discriminator_block(net, 256, 3, 1, \'disblock_4\')\n\n            # block 5\n            net = discriminator_block(net, 256, 3, 2, \'disblock_5\')\n\n            # block 6\n            net = discriminator_block(net, 512, 3, 1, \'disblock_6\')\n\n            # block_7\n            net = discriminator_block(net, 512, 3, 2, \'disblock_7\')\n\n            # The dense layer 1\n            with tf.variable_scope(\'dense_layer_1\'):\n                net = slim.flatten(net)\n                net = denselayer(net, 1024)\n                net = lrelu(net, 0.2)\n\n            # The dense layer 2\n            with tf.variable_scope(\'dense_layer_2\'):\n                net = denselayer(net, 1)\n                net = tf.nn.sigmoid(net)\n\n    return net\n\n\ndef VGG19_slim(input, type, reuse, scope):\n    # Define the feature to extract according to the type of perceptual\n    if type == \'VGG54\':\n        target_layer = scope + \'vgg_19/conv5/conv5_4\'\n    elif type == \'VGG22\':\n        target_layer = scope + \'vgg_19/conv2/conv2_2\'\n    else:\n        raise NotImplementedError(\'Unknown perceptual type\')\n    _, output = vgg_19(input, is_training=False, reuse=reuse)\n    output = output[target_layer]\n\n    return output\n\n\n# Define the whole network architecture\ndef SRGAN(inputs, targets, FLAGS):\n    # Define the container of the parameter\n    Network = collections.namedtuple(\'Network\', \'discrim_real_output, discrim_fake_output, discrim_loss, \\\n        discrim_grads_and_vars, adversarial_loss, content_loss, gen_grads_and_vars, gen_output, train, global_step, \\\n        learning_rate\')\n\n    # Build the generator part\n    with tf.variable_scope(\'generator\'):\n        output_channel = targets.get_shape().as_list()[-1]\n        gen_output = generator(inputs, output_channel, reuse=False, FLAGS=FLAGS)\n        gen_output.set_shape([FLAGS.batch_size, FLAGS.crop_size*4, FLAGS.crop_size*4, 3])\n\n    # Build the fake discriminator\n    with tf.name_scope(\'fake_discriminator\'):\n        with tf.variable_scope(\'discriminator\', reuse=False):\n            discrim_fake_output = discriminator(gen_output, FLAGS=FLAGS)\n\n    # Build the real discriminator\n    with tf.name_scope(\'real_discriminator\'):\n        with tf.variable_scope(\'discriminator\', reuse=True):\n            discrim_real_output = discriminator(targets, FLAGS=FLAGS)\n\n    # Use the VGG54 feature\n    if FLAGS.perceptual_mode == \'VGG54\':\n        with tf.name_scope(\'vgg19_1\') as scope:\n            extracted_feature_gen = VGG19_slim(gen_output, FLAGS.perceptual_mode, reuse=False, scope=scope)\n        with tf.name_scope(\'vgg19_2\') as scope:\n            extracted_feature_target = VGG19_slim(targets, FLAGS.perceptual_mode, reuse=True, scope=scope)\n\n    # Use the VGG22 feature\n    elif FLAGS.perceptual_mode == \'VGG22\':\n        with tf.name_scope(\'vgg19_1\') as scope:\n            extracted_feature_gen = VGG19_slim(gen_output, FLAGS.perceptual_mode, reuse=False, scope=scope)\n        with tf.name_scope(\'vgg19_2\') as scope:\n            extracted_feature_target = VGG19_slim(targets, FLAGS.perceptual_mode, reuse=True, scope=scope)\n\n    # Use MSE loss directly\n    elif FLAGS.perceptual_mode == \'MSE\':\n        extracted_feature_gen = gen_output\n        extracted_feature_target = targets\n\n    else:\n        raise NotImplementedError(\'Unknown perceptual type!!\')\n\n    # Calculating the generator loss\n    with tf.variable_scope(\'generator_loss\'):\n        # Content loss\n        with tf.variable_scope(\'content_loss\'):\n            # Compute the euclidean distance between the two features\n            diff = extracted_feature_gen - extracted_feature_target\n            if FLAGS.perceptual_mode == \'MSE\':\n                content_loss = tf.reduce_mean(tf.reduce_sum(tf.square(diff), axis=[3]))\n            else:\n                content_loss = FLAGS.vgg_scaling*tf.reduce_mean(tf.reduce_sum(tf.square(diff), axis=[3]))\n\n        with tf.variable_scope(\'adversarial_loss\'):\n            adversarial_loss = tf.reduce_mean(-tf.log(discrim_fake_output + FLAGS.EPS))\n\n        gen_loss = content_loss + (FLAGS.ratio)*adversarial_loss\n        print(adversarial_loss.get_shape())\n        print(content_loss.get_shape())\n\n    # Calculating the discriminator loss\n    with tf.variable_scope(\'discriminator_loss\'):\n        discrim_fake_loss = tf.log(1 - discrim_fake_output + FLAGS.EPS)\n        discrim_real_loss = tf.log(discrim_real_output + FLAGS.EPS)\n\n        discrim_loss = tf.reduce_mean(-(discrim_fake_loss + discrim_real_loss))\n\n    # Define the learning rate and global step\n    with tf.variable_scope(\'get_learning_rate_and_global_step\'):\n        global_step = tf.contrib.framework.get_or_create_global_step()\n        learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, FLAGS.decay_step, FLAGS.decay_rate, staircase=FLAGS.stair)\n        incr_global_step = tf.assign(global_step, global_step + 1)\n\n    with tf.variable_scope(\'dicriminator_train\'):\n        discrim_tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'discriminator\')\n        discrim_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=FLAGS.beta)\n        discrim_grads_and_vars = discrim_optimizer.compute_gradients(discrim_loss, discrim_tvars)\n        discrim_train = discrim_optimizer.apply_gradients(discrim_grads_and_vars)\n\n    with tf.variable_scope(\'generator_train\'):\n        # Need to wait discriminator to perform train step\n        with tf.control_dependencies([discrim_train]+ tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            gen_tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\n            gen_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=FLAGS.beta)\n            gen_grads_and_vars = gen_optimizer.compute_gradients(gen_loss, gen_tvars)\n            gen_train = gen_optimizer.apply_gradients(gen_grads_and_vars)\n\n    #[ToDo] If we do not use moving average on loss??\n    exp_averager = tf.train.ExponentialMovingAverage(decay=0.99)\n    update_loss = exp_averager.apply([discrim_loss, adversarial_loss, content_loss])\n\n    return Network(\n        discrim_real_output = discrim_real_output,\n        discrim_fake_output = discrim_fake_output,\n        discrim_loss = exp_averager.average(discrim_loss),\n        discrim_grads_and_vars = discrim_grads_and_vars,\n        adversarial_loss = exp_averager.average(adversarial_loss),\n        content_loss = exp_averager.average(content_loss),\n        gen_grads_and_vars = gen_grads_and_vars,\n        gen_output = gen_output,\n        train = tf.group(update_loss, incr_global_step, gen_train),\n        global_step = global_step,\n        learning_rate = learning_rate\n    )\n\n\ndef SRResnet(inputs, targets, FLAGS):\n    # Define the container of the parameter\n    Network = collections.namedtuple(\'Network\', \'content_loss, gen_grads_and_vars, gen_output, train, global_step, \\\n            learning_rate\')\n\n    # Build the generator part\n    with tf.variable_scope(\'generator\'):\n        output_channel = targets.get_shape().as_list()[-1]\n        gen_output = generator(inputs, output_channel, reuse=False, FLAGS=FLAGS)\n        gen_output.set_shape([FLAGS.batch_size, FLAGS.crop_size * 4, FLAGS.crop_size * 4, 3])\n\n    # Use the VGG54 feature\n    if FLAGS.perceptual_mode == \'VGG54\':\n        with tf.name_scope(\'vgg19_1\') as scope:\n            extracted_feature_gen = VGG19_slim(gen_output, FLAGS.perceptual_mode, reuse=False, scope=scope)\n        with tf.name_scope(\'vgg19_2\') as scope:\n            extracted_feature_target = VGG19_slim(targets, FLAGS.perceptual_mode, reuse=True, scope=scope)\n\n    elif FLAGS.perceptual_mode == \'VGG22\':\n        with tf.name_scope(\'vgg19_1\') as scope:\n            extracted_feature_gen = VGG19_slim(gen_output, FLAGS.perceptual_mode, reuse=False, scope=scope)\n        with tf.name_scope(\'vgg19_2\') as scope:\n            extracted_feature_target = VGG19_slim(targets, FLAGS.perceptual_mode, reuse=True, scope=scope)\n\n    elif FLAGS.perceptual_mode == \'MSE\':\n        extracted_feature_gen = gen_output\n        extracted_feature_target = targets\n\n    else:\n        raise NotImplementedError(\'Unknown perceptual type\')\n\n    # Calculating the generator loss\n    with tf.variable_scope(\'generator_loss\'):\n        # Content loss\n        with tf.variable_scope(\'content_loss\'):\n            # Compute the euclidean distance between the two features\n            # check=tf.equal(extracted_feature_gen, extracted_feature_target)\n            diff = extracted_feature_gen - extracted_feature_target\n            if FLAGS.perceptual_mode == \'MSE\':\n                content_loss = tf.reduce_mean(tf.reduce_sum(tf.square(diff), axis=[3]))\n            else:\n                content_loss = FLAGS.vgg_scaling * tf.reduce_mean(tf.reduce_sum(tf.square(diff), axis=[3]))\n\n        gen_loss = content_loss\n\n    # Define the learning rate and global step\n    with tf.variable_scope(\'get_learning_rate_and_global_step\'):\n        global_step = tf.contrib.framework.get_or_create_global_step()\n        learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, FLAGS.decay_step, FLAGS.decay_rate,\n                                                   staircase=FLAGS.stair)\n        incr_global_step = tf.assign(global_step, global_step + 1)\n\n    with tf.variable_scope(\'generator_train\'):\n        # Need to wait discriminator to perform train step\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            gen_tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'generator\')\n            gen_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=FLAGS.beta)\n            gen_grads_and_vars = gen_optimizer.compute_gradients(gen_loss, gen_tvars)\n            gen_train = gen_optimizer.apply_gradients(gen_grads_and_vars)\n\n    # [ToDo] If we do not use moving average on loss??\n    exp_averager = tf.train.ExponentialMovingAverage(decay=0.99)\n    update_loss = exp_averager.apply([content_loss])\n\n    return Network(\n        content_loss=exp_averager.average(content_loss),\n        gen_grads_and_vars=gen_grads_and_vars,\n        gen_output=gen_output,\n        train=tf.group(update_loss, incr_global_step, gen_train),\n        global_step=global_step,\n        learning_rate=learning_rate\n    )\n\n\ndef save_images(fetches, FLAGS, step=None):\n    image_dir = os.path.join(FLAGS.output_dir, ""images"")\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n\n    filesets = []\n    in_path = fetches[\'path_LR\']\n    name, _ = os.path.splitext(os.path.basename(str(in_path)))\n    fileset = {""name"": name, ""step"": step}\n\n    if FLAGS.mode == \'inference\':\n        kind = ""outputs""\n        filename = name + "".png""\n        if step is not None:\n            filename = ""%08d-%s"" % (step, filename)\n        fileset[kind] = filename\n        out_path = os.path.join(image_dir, filename)\n        contents = fetches[kind][0]\n        with open(out_path, ""wb"") as f:\n            f.write(contents)\n        filesets.append(fileset)\n    else:\n        for kind in [""inputs"", ""outputs"", ""targets""]:\n            filename = name + ""-"" + kind + "".png""\n            if step is not None:\n                filename = ""%08d-%s"" % (step, filename)\n            fileset[kind] = filename\n            out_path = os.path.join(image_dir, filename)\n            contents = fetches[kind][0]\n            with open(out_path, ""wb"") as f:\n                f.write(contents)\n        filesets.append(fileset)\n    return filesets\n\n\n\n\n\n\n\n\n\n\n\n'"
lib/model_dense.py,10,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom lib.ops import *\nimport collections\nimport os\nimport math\nimport scipy.misc as sic\nimport numpy as np\n\n\n# The dense layer\ndef denseConvlayer(layer_inputs, bottleneck_scale, growth_rate, is_training):\n    # Build the bottleneck operation\n    net = layer_inputs\n    net_temp = tf.identity(net)\n    net = batchnorm(net, is_training)\n    net = prelu_tf(net, name='Prelu_1')\n    net = conv2(net, kernel=1, output_channel=bottleneck_scale*growth_rate, stride=1, use_bias=False, scope='conv1x1')\n    net = batchnorm(net, is_training)\n    net = prelu_tf(net, name='Prelu_2')\n    net = conv2(net, kernel=3, output_channel=growth_rate, stride=1, use_bias=False, scope='conv3x3')\n\n    # Concatenate the processed feature to the feature\n    net = tf.concat([net_temp, net], axis=3)\n\n    return net\n\n\n# The transition layer\ndef transitionLayer(layer_inputs, output_channel, is_training):\n    net = layer_inputs\n    net = batchnorm(net, is_training)\n    net = prelu_tf(net)\n    net = conv2(net, 1, output_channel, stride=1, use_bias=False, scope='conv1x1')\n\n    return net\n\n\n# The dense block\ndef denseBlock(block_inputs, num_layers, bottleneck_scale, growth_rate, FLAGS):\n    # Build each layer consecutively\n    net = block_inputs\n    for i in range(num_layers):\n        with tf.variable_scope('dense_conv_layer%d'%(i+1)):\n            net = denseConvlayer(net, bottleneck_scale, growth_rate, FLAGS.is_training)\n\n    return net\n\n\n# Here we define the dense block version generator\ndef generatorDense(gen_inputs, gen_output_channels, reuse=False, FLAGS=None):\n    # Check the flag\n    if FLAGS is None:\n        raise ValueError('No FLAGS is provided for generator')\n\n    # The main netowrk\n    with tf.variable_scope('generator_unit', reuse=reuse):\n        # The input stage\n        with tf.variable_scope('input_stage'):\n            net = conv2(gen_inputs, 9, 64, 1, scope='conv')\n            net = prelu_tf(net)\n\n        # The dense block part\n        # Define the denseblock configuration\n        layer_per_block = 16\n        bottleneck_scale = 4\n        growth_rate = 12\n        transition_output_channel = 128\n        with tf.variable_scope('denseBlock_1'):\n            net = denseBlock(net, layer_per_block, bottleneck_scale, growth_rate, FLAGS)\n\n        with tf.variable_scope('transition_layer_1'):\n            net = transitionLayer(net, transition_output_channel, FLAGS.is_training)\n\n        with tf.variable_scope('subpixelconv_stage1'):\n            net = conv2(net, 3, 256, 1, scope='conv')\n            net = pixelShuffler(net, scale=2)\n            net = prelu_tf(net)\n\n        with tf.variable_scope('subpixelconv_stage2'):\n            net = conv2(net, 3, 256, 1, scope='conv')\n            net = pixelShuffler(net, scale=2)\n            net = prelu_tf(net)\n\n        with tf.variable_scope('output_stage'):\n            net = conv2(net, 9, gen_output_channels, 1, scope='conv')\n\n        return net\n\n\n\n\n"""
lib/ops.py,36,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport pdb\n\n\ndef preprocess(image):\n    with tf.name_scope(""preprocess""):\n        # [0, 1] => [-1, 1]\n        return image * 2 - 1\n\n\ndef deprocess(image):\n    with tf.name_scope(""deprocess""):\n        # [-1, 1] => [0, 1]\n        return (image + 1) / 2\n\n\ndef preprocessLR(image):\n    with tf.name_scope(""preprocessLR""):\n        return tf.identity(image)\n\n\ndef deprocessLR(image):\n    with tf.name_scope(""deprocessLR""):\n        return tf.identity(image)\n\n\n# Define the convolution building block\ndef conv2(batch_input, kernel=3, output_channel=64, stride=1, use_bias=True, scope=\'conv\'):\n    # kernel: An integer specifying the width and height of the 2D convolution window\n    with tf.variable_scope(scope):\n        if use_bias:\n            return slim.conv2d(batch_input, output_channel, [kernel, kernel], stride, \'SAME\', data_format=\'NHWC\',\n                            activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer())\n        else:\n            return slim.conv2d(batch_input, output_channel, [kernel, kernel], stride, \'SAME\', data_format=\'NHWC\',\n                            activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer(),\n                            biases_initializer=None)\n\n\ndef conv2_NCHW(batch_input, kernel=3, output_channel=64, stride=1, use_bias=True, scope=\'conv_NCHW\'):\n    # Use NCWH to speed up the inference\n    # kernel: list of 2 integer specifying the width and height of the 2D convolution window\n    with tf.variable_scope(scope):\n        if use_bias:\n            return slim.conv2d(batch_input, output_channel, [kernel, kernel], stride, \'SAME\', data_format=\'NCWH\',\n                               activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer())\n        else:\n            return slim.conv2d(batch_input, output_channel, [kernel, kernel], stride, \'SAME\', data_format=\'NCWH\',\n                               activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer(),\n                               biases_initializer=None)\n\n\n# Define our tensorflow version PRelu\ndef prelu_tf(inputs, name=\'Prelu\'):\n    with tf.variable_scope(name):\n        alphas = tf.get_variable(\'alpha\', inputs.get_shape()[-1], initializer=tf.zeros_initializer(), dtype=tf.float32)\n    pos = tf.nn.relu(inputs)\n    neg = alphas * (inputs - abs(inputs)) * 0.5\n\n    return pos + neg\n\n\n# Define our Lrelu\ndef lrelu(inputs, alpha):\n    return keras.layers.LeakyReLU(alpha=alpha).call(inputs)\n\n\ndef batchnorm(inputs, is_training):\n    return slim.batch_norm(inputs, decay=0.9, epsilon=0.001, updates_collections=tf.GraphKeys.UPDATE_OPS,\n                        scale=False, fused=True, is_training=is_training)\n\n\n# Our dense layer\ndef denselayer(inputs, output_size):\n    output = tf.layers.dense(inputs, output_size, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer())\n    return output\n\n\n# The implementation of PixelShuffler\ndef pixelShuffler(inputs, scale=2):\n    size = tf.shape(inputs)\n    batch_size = size[0]\n    h = size[1]\n    w = size[2]\n    c = inputs.get_shape().as_list()[-1]\n\n    # Get the target channel size\n    channel_target = c // (scale * scale)\n    channel_factor = c // channel_target\n\n    shape_1 = [batch_size, h, w, channel_factor // scale, channel_factor // scale]\n    shape_2 = [batch_size, h * scale, w * scale, 1]\n\n    # Reshape and transpose for periodic shuffling for each channel\n    input_split = tf.split(inputs, channel_target, axis=3)\n    output = tf.concat([phaseShift(x, scale, shape_1, shape_2) for x in input_split], axis=3)\n\n    return output\n\n\ndef phaseShift(inputs, scale, shape_1, shape_2):\n    # Tackle the condition when the batch is None\n    X = tf.reshape(inputs, shape_1)\n    X = tf.transpose(X, [0, 1, 3, 2, 4])\n\n    return tf.reshape(X, shape_2)\n\n\n# The random flip operation used for loading examples\ndef random_flip(input, decision):\n    f1 = tf.identity(input)\n    f2 = tf.image.flip_left_right(input)\n    output = tf.cond(tf.less(decision, 0.5), lambda: f2, lambda: f1)\n\n    return output\n\n\n# The operation used to print out the configuration\ndef print_configuration_op(FLAGS):\n    print(\'[Configurations]:\')\n    a = FLAGS.mode\n    #pdb.set_trace()\n    for name, value in FLAGS.__flags.items():\n        if type(value) == float:\n            print(\'\\t%s: %f\'%(name, value))\n        elif type(value) == int:\n            print(\'\\t%s: %d\'%(name, value))\n        elif type(value) == str:\n            print(\'\\t%s: %s\'%(name, value))\n        elif type(value) == bool:\n            print(\'\\t%s: %s\'%(name, value))\n        else:\n            print(\'\\t%s: %s\' % (name, value))\n\n    print(\'End of configuration\')\n\n\ndef compute_psnr(ref, target):\n    ref = tf.cast(ref, tf.float32)\n    target = tf.cast(target, tf.float32)\n    diff = target - ref\n    sqr = tf.multiply(diff, diff)\n    err = tf.reduce_sum(sqr)\n    v = tf.shape(diff)[0] * tf.shape(diff)[1] * tf.shape(diff)[2] * tf.shape(diff)[3]\n    mse = err / tf.cast(v, tf.float32)\n    psnr = 10. * (tf.log(255. * 255. / mse) / tf.log(10.))\n\n    return psnr\n\n\n# VGG19 component\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n  Args:\n    weight_decay: The l2 regularization coefficient.\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\n# VGG19 net\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=False,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           reuse = False,\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, 3, scope=\'conv1\', reuse=reuse)\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, 3, scope=\'conv2\',reuse=reuse)\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, 3, scope=\'conv3\', reuse=reuse)\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, 3, scope=\'conv4\',reuse=reuse)\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, 3, scope=\'conv5\',reuse=reuse)\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n      return net, end_points\nvgg_19.default_image_size = 224\n'"
tool/__init__.py,0,b''
tool/convertPNG.py,0,b''
tool/resizeImage.py,0,"b""from PIL import Image\nimport os\nfrom multiprocessing import Pool\n\n# Define the input and output image\ninput_dir = '../data/RAISE_HR/'\noutput_dir = '../data/RAISE_LR/'\nscale = 4.\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nimage_list = os.listdir(input_dir)\nimage_list = [os.path.join(input_dir, _) for _ in image_list]\n\n# Define the pool function\ndef downscale(name):\n    print(name)\n    with Image.open(name) as im:\n        w, h = im.size\n        w_new = int(w / scale)\n        h_new = int(h / scale)\n        im_new = im.resize((w_new, h_new), Image.ANTIALIAS)\n\n        save_name = os.path.join(output_dir, name.split('/')[-1])\n        im_new.save(save_name)\n\np = Pool(5)\np.map(downscale, image_list)\n# for name in image_list:\n#     print name\n#     with Image.open(name) as im:\n#         w, h = im.size\n#         w_new = int(w / scale)\n#         h_new = int(w / scale)\n#         im.resize((w_new, h_new), Image.ANTIALIAS)\n#\n#         save_name = os.path.join(output_dir, name.split('/')[-1].split('-0')[0]+'.png')\n#         im.save(save_name)"""
