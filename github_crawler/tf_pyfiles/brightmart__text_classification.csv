file_path,api_count,code
a08_predict_ensemble.py,59,"b'# -*- coding: utf-8 -*-\r\n#prediction using multi-models. take out: create multiple graphs. each graph associate with a session. add logits of models.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nfrom a3_entity_network import EntityNetwork\r\nsys.path.append("".."")\r\nfrom a08_DynamicMemoryNetwork.data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import pad_sequences #to_categorical\r\nimport codecs\r\nfrom a08_DynamicMemoryNetwork.a8_dynamic_memory_network import DynamicMemoryNetwork\r\nfrom p7_TextCNN_model import TextCNN\r\nfrom p71_TextRCNN_mode2 import TextRCNN\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 80, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir_dmn"",""../checkpoint_dynamic_memory_network/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",60,""max sentence length"")\r\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",1,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""../train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\r\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\r\ntf.app.flags.DEFINE_string(""predict_target_file"",""zhihu_result_ensemble_2_0814.csv"",""target file path for final prediction"")\r\ntf.app.flags.DEFINE_string(""predict_source_file"",\'../test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\r\ntf.app.flags.DEFINE_integer(""story_length"",1,""story length"")\r\ntf.app.flags.DEFINE_boolean(""use_gated_gru"",True,""whether to use gated gru as  memory update mechanism. if false,use weighted sum of candidate sentences according to gate"")\r\ntf.app.flags.DEFINE_integer(""num_pass"",3,""number of pass to run"") #e.g. num_pass=1,2,3,4.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\ntf.app.flags.DEFINE_boolean(""decode_with_sequences"",False,""if your task is sequence generating, you need to set this true.default is false, for predict a label"")\r\n###################################above from dynamic memory. below from entityNet#######################################################################################\r\ntf.app.flags.DEFINE_string(""ckpt_dir_entity"",""../checkpoint_entity_network5-b40-60-l2B/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""block_size"",40,""block size"")\r\ntf.app.flags.DEFINE_boolean(""use_bi_lstm"",True,""whether to use bi-directional lstm for encode of story and query"")\r\n###################################above from dynamic memory. below from entityNet#######################################################################################\r\ntf.app.flags.DEFINE_string(""ckpt_dir_cnn"",""../checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512/bak_important/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\r\ntf.app.flags.DEFINE_integer(""num_filters"", 512, ""number of filters"") #128\r\nfilter_sizes=[3,4,5,7,10,15,20,25]\r\n###################################above is TextRCNN######################################################################################################################\r\ntf.app.flags.DEFINE_string(""ckpt_dir_rcnn"",""../checkpoint_rcnn/text_rcnn_title_desc_checkpoint2/"",""checkpoint location for the model"")\r\n#tf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\r\n###################################above is RCNN############################################################################################################################\r\n###################################above is TextCNN_256embedding############################################################################################################\r\ntf.app.flags.DEFINE_string(""ckpt_dir_cnn_256_embedding"",""../checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_0814/"",""checkpoint location for the model"")\r\nfilter_sizes_256_embedding=[3,4,5,6,7,8,9,10,15,20,25] #[1,2,3,4,5,6,7,8,9,10]#[1,2,3,4,5,6,7,8,9]#[5,6,7,8,9] #[2,3,5,6,7,8]#[3,4,5,7,10,15,20,25] #[1,2,3,4,5,6,7][3,5,7]#[7,8,9,10,15,20,25] #[3,4,5,7,10,15,20,25]-->[6,7,8,10,15,20,25,30,35]BAD EPOCH2:13.2  #\r\ntf.app.flags.DEFINE_integer(""num_filters_256_embedding"", 128, ""number of filters"") #256--->512--->600\r\ntf.app.flags.DEFINE_integer(""embed_size_256_embedding"", 256, ""embedding and hidden size"") #256--->512--->600\r\n###################################above is TextCNN_256embedding############################################################################################################\r\n###################################above is HAN############################################################################################################\r\ntf.app.flags.DEFINE_string(""ckpt_dir_cnn_256_embedding"",""../checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_0814/"",""checkpoint location for the model"")\r\n\r\n###################################above is THAN############################################################################################################\r\n\r\ndef main(_):\r\n    # 1.load data with vocabulary of words and labels\r\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""dynamic_memory_network"")\r\n    vocab_size = len(vocabulary_word2index)\r\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""dynamic_memory_network"")\r\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\r\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\r\n    testX=[]\r\n    question_id_list=[]\r\n    for tuple in test:\r\n        question_id,question_string_list=tuple\r\n        question_id_list.append(question_id)\r\n        testX.append(question_string_list)\r\n    # 2.Data preprocessing: Sequence padding\r\n    print(""start padding...."")\r\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n    testX2_cnn = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length, for CNN\r\n    print(""end padding..."")\r\n   # 3.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    graph1 = tf.Graph().as_default()\r\n    graph2 = tf.Graph().as_default()\r\n    graph3 = tf.Graph().as_default()\r\n    graph4 = tf.Graph().as_default()\r\n    graph5 = tf.Graph().as_default()\r\n    global sess_dmn\r\n    global sess_entity\r\n    global sess_cnn\r\n    global sess_rcnn\r\n    with graph1:#DynamicMemoryNetwork\r\n        sess_dmn = tf.Session(config=config)\r\n        model_dmn = DynamicMemoryNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                                     FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,num_pass=FLAGS.num_pass,\r\n                                     use_gated_gru=FLAGS.use_gated_gru,decode_with_sequences=FLAGS.decode_with_sequences,multi_label_flag=FLAGS.multi_label_flag,l2_lambda=FLAGS.l2_lambda)\r\n        saver_dmn = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_dmn + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint of DMN."")\r\n            saver_dmn.restore(sess_dmn, tf.train.latest_checkpoint(FLAGS.ckpt_dir_dmn))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop.DMN"")\r\n            return\r\n    with graph2:#EntityNet\r\n        sess_entity = tf.Session(config=config)\r\n        model_entity = EntityNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                              FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,\r\n                              multi_label_flag=True, block_size=FLAGS.block_size,use_bi_lstm=FLAGS.use_bi_lstm)\r\n        saver_entity = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_entity + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint of EntityNet."")\r\n            saver_entity.restore(sess_entity, tf.train.latest_checkpoint(FLAGS.ckpt_dir_entity))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop.EntityNet."")\r\n            return\r\n    with graph3:#TextCNN\r\n        sess_cnn=tf.Session(config=config)\r\n        model_cnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\r\n                          FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training)\r\n        saver_cnn = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_cnn + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint.TextCNN."")\r\n            saver_cnn.restore(sess_cnn, tf.train.latest_checkpoint(FLAGS.ckpt_dir_cnn))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop.TextCNN."")\r\n            return\r\n    with graph5:  #TextCNN_256embedding\r\n        sess_cnn_256_embedding = tf.Session(config=config)\r\n        model_cnn_256_embedding = TextCNN(filter_sizes_256_embedding, FLAGS.num_filters_256_embedding, FLAGS.num_classes, FLAGS.learning_rate,\r\n                                FLAGS.batch_size,FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len, vocab_size,\r\n                                FLAGS.embed_size_256_embedding, FLAGS.is_training)\r\n        saver_cnn_256_embedding = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_cnn_256_embedding + ""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint.TextCNN_256_embedding"")\r\n            saver_cnn_256_embedding.restore(sess_cnn_256_embedding, tf.train.latest_checkpoint(FLAGS.ckpt_dir_cnn_256_embedding))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop.TextCNN_256_embedding."")\r\n            return\r\n    #with graph4:#RCNN\r\n    #    sess_rcnn=tf.Session(config=config)\r\n    #    model_rcnn=TextRCNN(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sentence_len,\r\n    #            vocab_size,FLAGS.embed_size,FLAGS.is_training,FLAGS.batch_size,multi_label_flag=FLAGS.multi_label_flag)\r\n    #    saver_rcnn = tf.train.Saver()\r\n    #    if os.path.exists(FLAGS.ckpt_dir_rcnn + ""checkpoint""):\r\n    #        print(""Restoring Variables from Checkpoint.TextRCNN."")\r\n    #        saver_rcnn.restore(sess_rcnn, tf.train.latest_checkpoint(FLAGS.ckpt_dir_rcnn))\r\n    #    else:\r\n    #        print(""Can\'t find the checkpoint.going to stop.TextRCNN."")\r\n    #        return\r\n\r\n        # 5.feed data, to get logits\r\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\r\n        index=0\r\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\r\n        global sess_dmn\r\n        global sess_entity\r\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\r\n            #1.DMN\r\n            logits_dmn=sess_dmn.run(model_dmn.logits,feed_dict={model_dmn.query:testX2[start:end],model_dmn.story: np.expand_dims(testX2[start:end],axis=1),\r\n                                                        model_dmn.dropout_keep_prob:1.0})\r\n            #2.EntityNet\r\n            logits_entity=sess_entity.run(model_entity.logits,feed_dict={model_entity.query:testX2[start:end],model_entity.story: np.expand_dims(testX2[start:end],axis=1),\r\n                                                        model_entity.dropout_keep_prob:1.0})\r\n            #3.CNN\r\n            logits_cnn = sess_cnn.run(model_cnn.logits,feed_dict={model_cnn.input_x: testX2_cnn[start:end], model_cnn.dropout_keep_prob: 1})\r\n            #4.RCNN\r\n            #logits_rcnn = sess_rcnn.run(model_rcnn.logits, feed_dict={model_rcnn.input_x: testX2_cnn[start:end],model_rcnn.dropout_keep_prob: 1})  # \'shape of logits:\', ( 1, 1999)\r\n            #5.CN_256_original_embeddding\r\n            logits_cnn_256_embedding =sess_cnn_256_embedding.run(model_cnn_256_embedding.logits,feed_dict={model_cnn_256_embedding.input_x: testX2_cnn[start:end],\r\n                                                                 model_cnn_256_embedding.dropout_keep_prob: 1})\r\n            #how to combine to logits: average\r\n            logits=logits_cnn*0.3+logits_cnn_256_embedding*0.3+logits_entity*0.2+logits_dmn*0.2#+logits_rcnn*0.15\r\n            question_id_sublist=question_id_list[start:end]\r\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\r\n            index=index+1\r\n        predict_target_file_f.close()\r\n\r\n# get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n    return label_list\r\n\r\n# get label using logits\r\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    value_list=[]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        value_list.append(logits[index])\r\n    return label_list,value_list\r\n\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\n# get label using logits\r\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\r\n    #print(""get_label_using_logits.shape:"", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\r\n    for i,logits in enumerate(logits_batch):\r\n        index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n        index_list=index_list[::-1]\r\n        label_list=[]\r\n        for index in index_list:\r\n            label=vocabulary_index2word_label[index]\r\n            label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        #print(""get_label_using_logits.label_list"",label_list)\r\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\r\n    f.flush()\r\n    #return label_list\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()\r\n'"
a00_Bert/__init__.py,0,b''
a00_Bert/bert_modeling.py,79,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=True,\n               scope=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n        it is must faster if this is True, on the CPU or GPU, it is faster if\n        this is False.\n      scope: (optional) variable scope. Defaults to ""bert"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    # change 11.24 is_training is changed from bool to placeholder.\n\n    config = copy.deepcopy(config)\n    # if not is_training:\n    #     config.hidden_dropout_prob = 0.0\n    #     config.attention_probs_dropout_prob = 0.0\n    def not_apply_dropout():\n        config.hidden_dropout_prob = 0.0\n        config.attention_probs_dropout_prob = 0.0\n        return 1\n\n    def apply_dropout():\n        return 1\n\n    tf.cond(is_training,apply_dropout,not_apply_dropout)\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=""bert""):\n      with tf.variable_scope(""embeddings""):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=""word_embeddings"",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef gelu(input_tensor):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n\n  Args:\n    input_tensor: float Tensor to perform activation.\n\n  Returns:\n    `input_tensor` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n  return input_tensor * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return tf.contrib.layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n      for TPUs.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  if use_one_hot_embeddings:\n    flat_input_ids = tf.reshape(input_ids, [-1])\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  if seq_length > max_position_embeddings:\n    raise ValueError(""The seq length (%d) cannot be greater than ""\n                     ""`max_position_embeddings` (%d)"" %\n                     (seq_length, max_position_embeddings))\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    full_position_embeddings = tf.get_variable(\n        name=position_embedding_name,\n        shape=[max_position_embeddings, width],\n        initializer=create_initializer(initializer_range))\n    # Since the position embedding table is a learned variable, we create it\n    # using a (long) sequence length `max_position_embeddings`. The actual\n    # sequence length might be shorter than this, for faster training of\n    # tasks that do not have long sequences.\n    #\n    # So `full_position_embeddings` is effectively an embedding table\n    # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n    # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n    # perform a slice.\n    if seq_length < max_position_embeddings:\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n    else:\n      position_embeddings = full_position_embeddings\n\n    num_dims = len(output.shape.as_list())\n\n    # Only the last two dimensions are relevant (`seq_length` and `width`), so\n    # we broadcast among the first dimensions, which is typically just\n    # the batch size.\n    position_broadcast_shape = []\n    for _ in range(num_dims - 2):\n      position_broadcast_shape.append(1)\n    position_broadcast_shape.extend([seq_length, width])\n    position_embeddings = tf.reshape(position_embeddings,\n                                     position_broadcast_shape)\n    output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*V]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*V]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
a00_Bert/optimization.py,25,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Functions and classes related to optimization (weight updates).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport tensorflow as tf\n\n\ndef create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n  """"""Creates an optimizer training op.""""""\n  global_step = tf.train.get_or_create_global_step()\n\n  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n\n  # Implements linear decay of the learning rate.\n  learning_rate = tf.train.polynomial_decay(\n      learning_rate,\n      global_step,\n      num_train_steps,\n      end_learning_rate=0.0,\n      power=1.0,\n      cycle=False)\n\n  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n  if num_warmup_steps:\n    global_steps_int = tf.cast(global_step, tf.int32)\n    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n    global_steps_float = tf.cast(global_steps_int, tf.float32)\n    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n    warmup_percent_done = global_steps_float / warmup_steps_float\n    warmup_learning_rate = init_lr * warmup_percent_done\n\n    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n    learning_rate = (\n        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n\n  # It is recommended that you use this optimizer for fine tuning, since this\n  # is how the model was trained (note that the Adam m/v variables are NOT\n  # loaded from init_checkpoint.)\n  optimizer = AdamWeightDecayOptimizer(\n      learning_rate=learning_rate,\n      weight_decay_rate=0.01,\n      beta_1=0.9,\n      beta_2=0.999,\n      epsilon=1e-6,\n      exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])\n\n  if use_tpu:\n    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n  tvars = tf.trainable_variables()\n  grads = tf.gradients(loss, tvars)\n\n  # This is how the model was pre-trained.\n  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n\n  train_op = optimizer.apply_gradients(\n      zip(grads, tvars), global_step=global_step)\n\n  new_global_step = global_step + 1\n  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n  return train_op\n\n\nclass AdamWeightDecayOptimizer(tf.train.Optimizer):\n  """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""\n\n  def __init__(self,\n               learning_rate,\n               weight_decay_rate=0.0,\n               beta_1=0.9,\n               beta_2=0.999,\n               epsilon=1e-6,\n               exclude_from_weight_decay=None,\n               name=""AdamWeightDecayOptimizer""):\n    """"""Constructs a AdamWeightDecayOptimizer.""""""\n    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n\n    self.learning_rate = learning_rate\n    self.weight_decay_rate = weight_decay_rate\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    self.epsilon = epsilon\n    self.exclude_from_weight_decay = exclude_from_weight_decay\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    """"""See base class.""""""\n    assignments = []\n    for (grad, param) in grads_and_vars:\n      if grad is None or param is None:\n        continue\n\n      param_name = self._get_variable_name(param.name)\n\n      m = tf.get_variable(\n          name=param_name + ""/adam_m"",\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n      v = tf.get_variable(\n          name=param_name + ""/adam_v"",\n          shape=param.shape.as_list(),\n          dtype=tf.float32,\n          trainable=False,\n          initializer=tf.zeros_initializer())\n\n      # Standard Adam update.\n      next_m = (\n          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n      next_v = (\n          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n                                                    tf.square(grad)))\n\n      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n\n      # Just adding the square of the weights to the loss function is *not*\n      # the correct way of using L2 regularization/weight decay with Adam,\n      # since that will interact with the m and v parameters in strange ways.\n      #\n      # Instead we want ot decay the weights in a manner that doesn\'t interact\n      # with the m/v parameters. This is equivalent to adding the square\n      # of the weights to the loss with plain (non-momentum) SGD.\n      if self._do_use_weight_decay(param_name):\n        update += self.weight_decay_rate * param\n\n      update_with_lr = self.learning_rate * update\n\n      next_param = param - update_with_lr\n\n      assignments.extend(\n          [param.assign(next_param),\n           m.assign(next_m),\n           v.assign(next_v)])\n    return tf.group(*assignments, name=name)\n\n  def _do_use_weight_decay(self, param_name):\n    """"""Whether to use L2 weight decay for `param_name`.""""""\n    if not self.weight_decay_rate:\n      return False\n    if self.exclude_from_weight_decay:\n      for r in self.exclude_from_weight_decay:\n        if re.search(r, param_name) is not None:\n          return False\n    return True\n\n  def _get_variable_name(self, param_name):\n    """"""Get the variable name from the tensor name.""""""\n    m = re.match(""^(.*):\\\\d+$"", param_name)\n    if m is not None:\n      param_name = m.group(1)\n    return param_name\n'"
a00_Bert/run_classifier_predict_online.py,34,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BERT finetuning runner of classification for online prediction. input is a list. output is a label.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\nimport modeling\nimport tokenization\nimport tensorflow as tf\nimport numpy as np\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nBERT_BASE_DIR=""./checkpoint_finetuing_law512/""\nflags.DEFINE_string(""bert_config_file"", BERT_BASE_DIR+""bert_config.json"",\n    ""The config json file corresponding to the pre-trained BERT model. ""\n    ""This specifies the model architecture."")\n\nflags.DEFINE_string(""task_name"", ""sentence_pair"", ""The name of the task to train."")\n\nflags.DEFINE_string(""vocab_file"", BERT_BASE_DIR+""vocab.txt"",\n                    ""The vocabulary file that the BERT model was trained on."")\n\nflags.DEFINE_string(""init_checkpoint"", BERT_BASE_DIR, # model.ckpt-66870--> /model.ckpt-66870\n    ""Initial checkpoint (usually from a pre-trained BERT model)."")\n\nflags.DEFINE_integer(""max_seq_length"", 512,\n    ""The maximum total input sequence length after WordPiece tokenization. ""\n    ""Sequences longer than this will be truncated, and sequences shorter ""\n    ""than this will be padded."")\n\nflags.DEFINE_bool(\n    ""do_lower_case"", True,\n    ""Whether to lower case the input text. Should be True for uncased ""\n    ""models and False for cased models."")\n\nclass InputExample(object):\n  """"""A single training/test example for simple sequence classification.""""""\n\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    """"""Constructs a InputExample.\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    """"""\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass InputFeatures(object):\n  """"""A single set of features of data.""""""\n\n  def __init__(self, input_ids, input_mask, segment_ids, label_id):\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.segment_ids = segment_ids\n    self.label_id = label_id\n\n\nclass DataProcessor(object):\n  """"""Base class for data converters for sequence classification data sets.""""""\n\n  def get_train_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for the train set.""""""\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for the dev set.""""""\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for prediction.""""""\n    raise NotImplementedError()\n\n  def get_labels(self):\n    """"""Gets the list of labels for this data set.""""""\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    """"""Reads a tab separated value file.""""""\n    with tf.gfile.Open(input_file, ""r"") as f:\n      reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\nclass SentencePairClassificationProcessor(DataProcessor):\n  """"""Processor for the internal data set. sentence pair classification""""""\n  def __init__(self):\n    self.language = ""zh""\n\n  #def get_train_examples(self, data_dir):\n  #  """"""See base class.""""""\n  #  return self._create_examples(\n  #      self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  #def get_dev_examples(self, data_dir):\n  #  """"""See base class.""""""\n  #  return self._create_examples(\n  #      self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  #def get_test_examples(self, data_dir):\n  #  """"""See base class.""""""\n  #  return self._create_examples(\n  #      self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""0"", ""1""]\n\n  #def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n  #  examples = []\n  #  for (i, line) in enumerate(lines):\n  #    if i == 0:\n  #      continue\n  #    guid = ""%s-%s"" % (set_type, i)\n  #    label = tokenization.convert_to_unicode(line[0])\n  #    text_a = tokenization.convert_to_unicode(line[1])\n  #    text_b = tokenization.convert_to_unicode(line[2])\n  #    examples.append(\n  #        InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n  #  return examples\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length,tokenizer):\n  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""\n  label_map = {}\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with ""- 3""\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with ""- 2""\n    if len(tokens_a) > max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where ""type_ids"" are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as as the ""sentence vector"". Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(""[CLS]"")\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(""[SEP]"")\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(""[SEP]"")\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) < max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n\n  label_id = label_map[example.label]\n  if ex_index < 5:\n    tf.logging.info(""*** Example ***"")\n    tf.logging.info(""guid: %s"" % (example.guid))\n    tf.logging.info(""tokens: %s"" % "" "".join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n    tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id)\n  return feature\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\ndef create_int_feature(values):\n  f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n  return f\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n                 labels, num_labels, use_one_hot_embeddings):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  # In the demo, we are doing a simple classification task on the entire\n  # segment.\n  #\n  # If you want to use the token-level output, use model.get_sequence_output()\n  # instead.\n  output_layer = model.get_pooled_output()\n\n  hidden_size = output_layer.shape[-1].value\n\n  output_weights = tf.get_variable(\n      ""output_weights"", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable(\n      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities,model)\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\nprocessors = {\n  ""sentence_pair"":SentencePairClassificationProcessor,\n}\nbert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\ntask_name = FLAGS.task_name.lower()\nprint(""task_name:"",task_name)\nprocessor = processors[task_name]()\nlabel_list = processor.get_labels()\n#lines_dev=processor.get_dev_examples(""./TEXT_DIR"")\nindex2label={i:label_list[i] for i in range(len(label_list))}\ntokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n\ndef main(_):\n    pass\n\n# init mode and session\n# move something codes outside of function, so that this code will run only once during online prediction when predict_online is invoked.\nis_training=False\nuse_one_hot_embeddings=False\nbatch_size=1\nnum_labels=len(label_list)\ngpu_config = tf.ConfigProto()\ngpu_config.gpu_options.allow_growth = True\nsess=tf.Session(config=gpu_config)\nmodel=None\nglobal graph\ninput_ids_p,input_mask_p,label_ids_p,segment_ids_p=None,None,None,None\nif not os.path.exists(FLAGS.init_checkpoint + ""checkpoint""):\n    raise Exception(""failed to get checkpoint. going to return "")\n\ngraph = tf.get_default_graph()\nwith graph.as_default():\n    print(""going to restore checkpoint"")\n    #sess.run(tf.global_variables_initializer())\n    input_ids_p = tf.placeholder(tf.int32, [batch_size, FLAGS.max_seq_length], name=""input_ids"")\n    input_mask_p = tf.placeholder(tf.int32, [batch_size, FLAGS.max_seq_length], name=""input_mask"")\n    label_ids_p = tf.placeholder(tf.int32, [batch_size], name=""label_ids"")\n    segment_ids_p = tf.placeholder(tf.int32, [FLAGS.max_seq_length], name=""segment_ids"")\n    total_loss, per_example_loss, logits, probabilities,model = create_model(\n        bert_config, is_training, input_ids_p, input_mask_p, segment_ids_p,\n        label_ids_p, num_labels, use_one_hot_embeddings)\n    saver = tf.train.Saver()\n    saver.restore(sess, tf.train.latest_checkpoint(FLAGS.init_checkpoint))\n\ndef predict_online(line):\n    """"""\n    do online prediction. each time make prediction for one instance.\n    you can change to a batch if you want.\n    :param line: a list. element is: [dummy_label,text_a,text_b]\n    :return:\n    """"""\n    label = line[0] #tokenization.convert_to_unicode(line[0]) # this should compatible with format you defined in processor.\n    text_a = line[1] #tokenization.convert_to_unicode(line[1])\n    text_b = line[2] #tokenization.convert_to_unicode(line[2])\n    example= InputExample(guid=0, text_a=text_a, text_b=text_b, label=label)\n    feature = convert_single_example(0, example, label_list,FLAGS.max_seq_length, tokenizer)\n    input_ids = np.reshape([feature.input_ids],(1,FLAGS.max_seq_length))\n    input_mask = np.reshape([feature.input_mask],(1,FLAGS.max_seq_length))\n    segment_ids =  np.reshape([feature.segment_ids],(FLAGS.max_seq_length))\n    label_ids =[feature.label_id]\n\n    global graph\n    with graph.as_default():\n        feed_dict = {input_ids_p: input_ids, input_mask_p: input_mask,segment_ids_p:segment_ids,label_ids_p:label_ids}\n        possibility = sess.run([probabilities], feed_dict)\n        possibility=possibility[0][0] # get first label\n        label_index=np.argmax(possibility)\n        label_predict=index2label[label_index]\n        #print(""label_predict:"",label_predict,"";possibility:"",possibility)\n    return label_predict,possibility\n\nif __name__ == ""__main__"":\n  example=[\'0\',\'\\u5165\\u804c\\u4e00\\u5e74\\u534a\\u672a\\u7b7e\\u52b3\\u52a8\\u5408\\u540c\\u5c0f\\u83f2\\u6bd5\\u4e1a\\u4e8e\\u67d0\\u62a4\\u6821\\uff0c\\u548c\\u5176\\u4ed6\\u7684\\u9ad8\\u6821\\u6bd5\\u4e1a\\u751f\\u4e00\\u6837\\uff0c\\u5979\\u4e5f\\u5f00\\u59cb\\u7740\\u624b\\u627e\\u5de5\\u4f5c\\u3002\\u5f88\\u5feb\\uff0c\\u4e00\\u5bb6\\u6c11\\u529e\\u533b\\u9662\\u901a\\u8fc7\\u67d0\\u62db\\u8058\\u7f51\\u7ad9\\u627e\\u5230\\u5c0f\\u83f2\\uff0c\\u901a\\u8fc7\\u9762\\u8bd5\\u540e\\uff0c\\u5c0f\\u83f2\\u4fbf\\u5f00\\u59cb\\u4e86\\u81ea\\u5df1\\u7684\\u804c\\u573a\\u751f\\u6daf\\u3002\\u8f6c\\u773c\\u6bd5\\u4e1a\\u5de5\\u4f5c\\u8fd1\\u4e00\\u5e74\\uff0c\\u533b\\u9662\\u4ecd\\u8fdf\\u8fdf\\u4e0d\\u4e0e\\u5176\\u7b7e\\u8ba2\\u52b3\\u52a8\\u5408\\u540c\\uff0c\\u5c0f\\u83f2\\u4e0e\\u5355\\u4f4d\\u591a\\u6b21\\u6c9f\\u901a\\u534f\\u5546\\u672a\\u679c\\uff0c\\u65e0\\u5948\\u5c06\\u533b\\u9662\\u8bc9\\u81f3\\u6cd5\\u9662\\u000d\\u000a\',\'\\u652f\\u4ed8\\u5de5\\u8d44\']\n  result=predict_online(example)\n  print(""result:"",result)\n\n'"
a00_Bert/tokenization.py,2,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef convert_to_unicode(text):\n  """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(""utf-8"", ""ignore"")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it\'s a Unicode string and in the other it\'s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(""utf-8"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.gfile.GFile(vocab_file, ""r"") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a peice of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn\'t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don\'t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    """"""Adds whitespace around any CJK character.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append("" "")\n        output.append(char)\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n  def _is_chinese_char(self, cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n  """"""Runs WordPiece tokenziation.""""""\n\n  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = ""unaffable""\n      output = [""un"", ""##aff"", ""##able""]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    """"""\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = """".join(chars[start:end])\n          if start > 0:\n            substr = ""##"" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat.startswith(""C""):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n'"
a00_Bert/train_bert_multi-label.py,39,"b'# coding=utf-8\n""""""\ntrain bert model\n\n1.get training data and vocabulary & labels dict\n2. create model\n3. train the model and report f1 score\n""""""\nimport bert_modeling as modeling\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom utils import load_data,init_label_dict,get_target_label_short,compute_confuse_matrix,compute_micro_macro,compute_confuse_matrix_batch,get_label_using_logits_batch,get_target_label_short_batch\n\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(""cache_file_h5py"",""../data/ieee_zhihu_cup/data.h5"",""path of training/validation/test data."") #../data/sample_multiple_label.txt\ntf.app.flags.DEFINE_string(""cache_file_pickle"",""../data/ieee_zhihu_cup/vocab_label.pik"",""path of vocabulary and label files"") #../data/sample_multiple_label.txt\n\ntf.app.flags.DEFINE_float(""learning_rate"",0.0001,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 256, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is training.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs to run."")\n\n# below hyper-parameter is for bert model\n# to train a big model,                     use hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072\n# to train a middel size model, train fast. use hidden_size=128, num_hidden_layers=4, num_attention_heads=8, intermediate_size=1024\ntf.app.flags.DEFINE_integer(""hidden_size"",128,""hidden size"") # 768\ntf.app.flags.DEFINE_integer(""num_hidden_layers"",2,""number of hidden layers"") # 12--->4\ntf.app.flags.DEFINE_integer(""num_attention_heads"",4,""number of attention headers"") # 12\ntf.app.flags.DEFINE_integer(""intermediate_size"",256,""intermediate size of hidden layer"") # 3072-->512\ntf.app.flags.DEFINE_integer(""max_seq_length"",200,""max sequence length"")\n\ndef main(_):\n    # 1. get training data and vocabulary & labels dict\n    word2index, label2index, trainX, trainY, vaildX, vaildY, testX, testY = load_data(FLAGS.cache_file_h5py,FLAGS.cache_file_pickle)\n    vocab_size = len(word2index); print(""bert model.vocab_size:"", vocab_size);\n    num_labels = len(label2index); print(""num_labels:"", num_labels); cls_id=word2index[\'CLS\'];print(""id of \'CLS\':"",word2index[\'CLS\'])\n    num_examples, FLAGS.max_seq_length = trainX.shape;print(""num_examples of training:"", num_examples, "";max_seq_length:"", FLAGS.max_seq_length)\n\n    # 2. create model, define train operation\n    bert_config = modeling.BertConfig(vocab_size=len(word2index), hidden_size=FLAGS.hidden_size, num_hidden_layers=FLAGS.num_hidden_layers,\n                                      num_attention_heads=FLAGS.num_attention_heads,intermediate_size=FLAGS.intermediate_size)\n    input_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=""input_ids"") # FLAGS.batch_size\n    input_mask = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=""input_mask"")\n    segment_ids = tf.placeholder(tf.int32, [None,FLAGS.max_seq_length],name=""segment_ids"")\n    label_ids = tf.placeholder(tf.float32, [None,num_labels], name=""label_ids"")\n    is_training = tf.placeholder(tf.bool, name=""is_training"") # FLAGS.is_training\n\n    use_one_hot_embeddings = False\n    loss, per_example_loss, logits, probabilities, model = create_model(bert_config, is_training, input_ids, input_mask,\n                                                            segment_ids, label_ids, num_labels,use_one_hot_embeddings)\n    # define train operation\n    #num_train_steps = int(float(num_examples) / float(FLAGS.batch_size * FLAGS.num_epochs)); use_tpu=False; num_warmup_steps = int(num_train_steps * 0.1)\n    #train_op = optimization.create_optimizer(loss, FLAGS.learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n    global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n    train_op = tf.contrib.layers.optimize_loss(loss, global_step=global_step, learning_rate=FLAGS.learning_rate,optimizer=""Adam"", clip_gradients=3.0)\n\n    # 3. train the model by calling create model, get loss\n    gpu_config = tf.ConfigProto()\n    gpu_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=gpu_config)\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    if os.path.exists(FLAGS.ckpt_dir + ""checkpoint""):\n        print(""Checkpoint Exists. Restoring Variables from Checkpoint."")\n        saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n    number_of_training_data = len(trainX)\n    iteration = 0\n    curr_epoch = 0 #sess.run(textCNN.epoch_step)\n    batch_size = FLAGS.batch_size\n    for epoch in range(curr_epoch, FLAGS.num_epochs):\n        loss_total, counter = 0.0, 0\n        for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n            iteration = iteration + 1 ###\n            input_mask_, segment_ids_, input_ids_=get_input_mask_segment_ids(trainX[start:end],cls_id) # input_ids_,input_mask_,segment_ids_\n            feed_dict = {input_ids: input_ids_, input_mask: input_mask_, segment_ids:segment_ids_,\n                         label_ids:trainY[start:end],is_training:True}\n            curr_loss,_ = sess.run([loss,train_op], feed_dict)\n            loss_total, counter = loss_total + curr_loss, counter + 1\n            if counter % 30 == 0:\n                print(epoch,""\\t"",iteration,""\\tloss:"",loss_total/float(counter),""\\tcurrent_loss:"",curr_loss)\n            if counter % 300==0:\n                print(""input_ids["",start,""]:"",input_ids_[0]);#print(""trainY[start:end]:"",trainY[start:end])\n                try:\n                    target_labels = get_target_label_short_batch(trainY[start:end]);#print(""target_labels:"",target_labels)\n                    print(""trainY["",start,""]:"",target_labels[0])\n                except:\n                    pass\n            # evaulation\n            if start!=0 and start % (1000 * FLAGS.batch_size) == 0:\n                eval_loss, f1_score, f1_micro, f1_macro = do_eval(sess,input_ids,input_mask,segment_ids,label_ids,is_training,loss,\n                                                                  probabilities,vaildX, vaildY, num_labels,batch_size,cls_id)\n                print(""Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tF1_micro:%.3f\\tF1_macro:%.3f"" % (\n                    epoch, eval_loss, f1_score, f1_micro, f1_macro))\n                # save model to checkpoint\n                #if start % (4000 * FLAGS.batch_size)==0:\n                save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                print(""Going to save model.."")\n                saver.save(sess, save_path, global_step=epoch)\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,labels, num_labels, use_one_hot_embeddings,reuse_flag=False):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  output_layer = model.get_pooled_output()\n  hidden_size = output_layer.shape[-1].value\n  with tf.variable_scope(""weights"",reuse=reuse_flag):\n      output_weights = tf.get_variable(""output_weights"", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n      output_bias = tf.get_variable(""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    #if is_training:\n    #    print(""###create_model.is_training:"",is_training)\n    #    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n    def apply_dropout_last_layer(output_layer):\n        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n        return output_layer\n\n    def not_apply_dropout(output_layer):\n        return output_layer\n\n    output_layer=tf.cond(is_training, lambda: apply_dropout_last_layer(output_layer), lambda:not_apply_dropout(output_layer))\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    print(""output_layer:"",output_layer.shape,"";output_weights:"",output_weights.shape,"";logits:"",logits.shape) # shape=(?, 1999)\n\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.sigmoid(logits) #tf.nn.softmax(logits, axis=-1)\n    per_example_loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits) # shape=(?, 1999)\n    loss_batch = tf.reduce_sum(per_example_loss,axis=1)  #  (?,)\n    loss=tf.reduce_mean(loss_batch) #  (?,)\n\n    return loss, per_example_loss, logits, probabilities,model\n\n\ndef do_eval(sess,input_ids,input_mask,segment_ids,label_ids,is_training,loss,probabilities,vaildX, vaildY, num_labels,batch_size,cls_id):\n    """"""\n    evalution on model using validation data\n    """"""\n    num_eval=1000\n    vaildX = vaildX[0:num_eval]\n    vaildY = vaildY[0:num_eval]\n    number_examples = len(vaildX)\n    eval_loss, eval_counter, eval_f1_score, eval_p, eval_r = 0.0, 0, 0.0, 0.0, 0.0\n    label_dict = init_label_dict(num_labels)\n    print(""do_eval.number_examples:"",number_examples)\n    f1_score_micro_sklearn_total=0.0\n    # batch_size=1 # TODO\n    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n        input_mask_, segment_ids_, input_ids_ = get_input_mask_segment_ids(vaildX[start:end],cls_id)\n        feed_dict = {input_ids: input_ids_,input_mask:input_mask_,segment_ids:segment_ids_,\n                     label_ids:vaildY[start:end],is_training:False}\n        curr_eval_loss, prob = sess.run([loss, probabilities],feed_dict)\n        target_labels=get_target_label_short_batch(vaildY[start:end])\n        predict_labels=get_label_using_logits_batch(prob)\n        if start%100==0:\n            print(""prob.shape:"",prob.shape,"";prob:"",prob)\n            print(""predict_labels:"",predict_labels)\n\n        #print(""predict_labels:"",predict_labels)\n        label_dict=compute_confuse_matrix_batch(target_labels,predict_labels,label_dict,name=\'bert\')\n        eval_loss, eval_counter = eval_loss + curr_eval_loss, eval_counter + 1\n\n    f1_micro, f1_macro = compute_micro_macro(label_dict)  # label_dictis a dict, key is: accusation,value is: (TP,FP,FN). where TP is number of True Positive\n    f1_score_result = (f1_micro + f1_macro) / 2.0\n    return eval_loss / float(eval_counter+0.00001), f1_score_result, f1_micro, f1_macro\n\ndef get_input_mask_segment_ids(train_x_batch,cls_id):\n    """"""\n    get input mask and segment ids given a batch of input x.\n    if sequence length of input x is max_sequence_length, then shape of both input_mask and segment_ids should be\n    [batch_size, max_sequence_length]. for those padding tokens, input_mask will be zero, value for all other place is one.\n    :param train_x_batch:\n    :return: input_mask_,segment_ids\n    """"""\n    batch_size,max_sequence_length=train_x_batch.shape\n    input_mask=np.ones((batch_size,max_sequence_length),dtype=np.int32)\n    # set 0 for token in padding postion\n    for i in range(batch_size):\n        input_x_=train_x_batch[i] # a list, length is max_sequence_length\n        input_x=list(input_x_)\n        for j in range(len(input_x)):\n            if input_x[j]==0:\n                input_mask[i][j:]=0\n                break\n    # insert CLS token for classification\n    input_ids=np.zeros((batch_size,max_sequence_length),dtype=np.int32)\n    #print(""input_ids.shape1:"",input_ids.shape)\n    for k in range(batch_size):\n        input_id_list=list(train_x_batch[k])\n        input_id_list.insert(0,cls_id)\n        del input_id_list[-1]\n        input_ids[k]=input_id_list\n    #print(""input_ids.shape2:"",input_ids.shape)\n\n    segment_ids=np.ones((batch_size,max_sequence_length),dtype=np.int32)\n    return input_mask, segment_ids,input_ids\n\n#train_x_batch=np.ones((3,5))\n#train_x_batch[0,4]=0\n#train_x_batch[1,3]=0\n#train_x_batch[1,4]=0\n#cls_id=2\n#print(""train_x_batch:"",train_x_batch)\n#input_mask, segment_ids,input_ids=get_input_mask_segment_ids(train_x_batch,cls_id)\n#print(""input_mask:"",input_mask, ""segment_ids:"",segment_ids,""input_ids:"",input_ids)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a00_Bert/train_bert_toy_task.py,16,"b'# coding=utf-8\n""""""\ntrain bert model\n""""""\nimport modeling\nimport tensorflow as tf\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(description=\'Describe your program\')\nparser.add_argument(\'-batch_size\', \'--batch_size\', type=int,default=128)\nargs = parser.parse_args()\nbatch_size=args.batch_size\nprint(""batch_size:"",batch_size)\ndef bert_train_fn():\n    is_training=True\n    hidden_size = 768\n    num_labels = 10\n    #batch_size=128\n    max_seq_length=512\n    use_one_hot_embeddings = False\n    bert_config = modeling.BertConfig(vocab_size=21128, hidden_size=hidden_size, num_hidden_layers=12,\n                                      num_attention_heads=12,intermediate_size=3072)\n\n    input_ids = tf.placeholder(tf.int32, [batch_size, max_seq_length], name=""input_ids"")\n    input_mask = tf.placeholder(tf.int32, [batch_size, max_seq_length], name=""input_mask"")\n    segment_ids = tf.placeholder(tf.int32, [batch_size,max_seq_length],name=""segment_ids"")\n    label_ids = tf.placeholder(tf.float32, [batch_size,num_labels], name=""label_ids"")\n    loss, per_example_loss, logits, probabilities, model = create_model(bert_config, is_training, input_ids, input_mask,\n                                                                        segment_ids, label_ids, num_labels,\n                                                                        use_one_hot_embeddings)\n    # 1. generate or load training/validation/test data. e.g. train:(X,y). X is input_ids,y is labels.\n\n    # 2. train the model by calling create model, get loss\n    gpu_config = tf.ConfigProto()\n    gpu_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=gpu_config)\n    sess.run(tf.global_variables_initializer())\n    for i in range(1000):\n        input_ids_=np.ones((batch_size,max_seq_length),dtype=np.int32)\n        input_mask_=np.ones((batch_size,max_seq_length),dtype=np.int32)\n        segment_ids_=np.ones((batch_size,max_seq_length),dtype=np.int32)\n        label_ids_=np.ones((batch_size,num_labels),dtype=np.float32)\n        feed_dict = {input_ids: input_ids_, input_mask: input_mask_,segment_ids:segment_ids_,label_ids:label_ids_}\n        loss_ = sess.run([loss], feed_dict)\n        print(""loss:"",loss_)\n    # 3. eval the model from time to time\n\ndef bert_predict_fn():\n    # 1. predict based on\n    pass\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,labels, num_labels, use_one_hot_embeddings):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  output_layer = model.get_pooled_output()\n  hidden_size = output_layer.shape[-1].value\n  output_weights = tf.get_variable(""output_weights"", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n  output_bias = tf.get_variable(""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    if is_training:  # if training, add dropout\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    print(""output_layer:"",output_layer.shape,"";output_weights:"",output_weights.shape,"";logits:"",logits.shape)\n\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    per_example_loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return loss, per_example_loss, logits, probabilities,model\n\nbert_train_fn()\n'"
a00_Bert/utils.py,0,"b'# -*- coding: utf-8 -*-\n\nimport pickle\nimport h5py\nimport os\nimport numpy as np\nimport random\n\nrandom_number=300\n\ndef load_data(cache_file_h5py,cache_file_pickle):\n    """"""\n    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n    :param cache_file_h5py:\n    :param cache_file_pickle:\n    :return:\n    """"""\n    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n        raise RuntimeError(""############################ERROR##############################\\n. ""\n                           ""please download cache file, it include training data and vocabulary & labels. ""\n                           ""link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.""\n                           ""cache_file_h5py and FLAGS.cache_file_pickle suggested location."")\n    print(""INFO. cache file exists. going to load cache file"")\n    f_data = h5py.File(cache_file_h5py, \'r\')\n    print(""f_data.keys:"",list(f_data.keys()))\n    train_X=f_data[\'train_X\'] # np.array(\n    print(""train_X.shape:"",train_X.shape)\n    train_Y=f_data[\'train_Y\'] # np.array(\n    print(""train_Y.shape:"",train_Y.shape,"";"")\n    vaild_X=f_data[\'vaild_X\'] # np.array(\n    valid_Y=f_data[\'valid_Y\'] # np.array(\n    test_X=f_data[\'test_X\'] # np.array(\n    test_Y=f_data[\'test_Y\'] # np.array(\n    #f_data.close()\n\n    word2index, label2index=None,None\n    with open(cache_file_pickle, \'rb\') as data_f_pickle:\n        word2index, label2index=pickle.load(data_f_pickle)\n    print(""INFO. cache file load successful..."")\n    return word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y\n\n#######################################\ndef compute_f1_score(predict_y,eval_y):\n    """"""\n    compoute f1_score.\n    :param logits: [batch_size,label_size]\n    :param evalY: [batch_size,label_size]\n    :return:\n    """"""\n    f1_score=0.0\n    p_5=0.0\n    r_5=0.0\n    return f1_score,p_5,r_5\n\ndef compute_f1_score_removed(label_list_top5,eval_y):\n    """"""\n    compoute f1_score.\n    :param logits: [batch_size,label_size]\n    :param evalY: [batch_size,label_size]\n    :return:\n    """"""\n    num_correct_label=0\n    eval_y_short=get_target_label_short(eval_y)\n    for label_predict in label_list_top5:\n        if label_predict in eval_y_short:\n            num_correct_label=num_correct_label+1\n    #P@5=Precision@5\n    num_labels_predicted=len(label_list_top5)\n    all_real_labels=len(eval_y_short)\n    p_5=num_correct_label/num_labels_predicted\n    #R@5=Recall@5\n    r_5=num_correct_label/all_real_labels\n    f1_score=2.0*p_5*r_5/(p_5+r_5+0.000001)\n    return f1_score,p_5,r_5\n\ndef compute_confuse_matrix(target_y,predict_y,label_dict,name=\'default\'):\n    """"""\n    compute true postive(TP), false postive(FP), false negative(FN) given target lable and predict label\n    :param target_y:\n    :param predict_y:\n    :param label_dict {label:(TP,FP,FN)}\n    :return: macro_f1(a scalar),micro_f1(a scalar)\n    """"""\n    #1.get target label and predict label\n    if random.choice([x for x in range(random_number)]) ==1:\n        print(name+"".target_y:"",target_y,"";predict_y:"",predict_y) #debug purpose\n\n    #2.count number of TP,FP,FN for each class\n    y_labels_unique=[]\n    y_labels_unique.extend(target_y)\n    y_labels_unique.extend(predict_y)\n    y_labels_unique=list(set(y_labels_unique))\n    for i,label in enumerate(y_labels_unique): #e.g. label=2\n        TP, FP, FN = label_dict[label]\n        if label in predict_y and label in target_y:#predict=1,truth=1 (TP)\n            TP=TP+1\n        elif label in predict_y and label not in target_y:#predict=1,truth=0(FP)\n            FP=FP+1\n        elif label not in predict_y and label in target_y:#predict=0,truth=1(FN)\n            FN=FN+1\n        label_dict[label] = (TP, FP, FN)\n    return label_dict\n\ndef compute_micro_macro(label_dict):\n    """"""\n    compute f1 of micro and macro\n    :param label_dict:\n    :return: f1_micro,f1_macro: scalar, scalar\n    """"""\n    f1_micro = compute_f1_micro_use_TFFPFN(label_dict)\n    f1_macro= compute_f1_macro_use_TFFPFN(label_dict)\n    return f1_micro,f1_macro\n\ndef compute_TF_FP_FN_micro(label_dict):\n    """"""\n    compute micro FP,FP,FN\n    :param label_dict_accusation: a dict. {label:(TP, FP, FN)}\n    :return:TP_micro,FP_micro,FN_micro\n    """"""\n    TP_micro,FP_micro,FN_micro=0.0,0.0,0.0\n    for label,tuplee in label_dict.items():\n        TP,FP,FN=tuplee\n        TP_micro=TP_micro+TP\n        FP_micro=FP_micro+FP\n        FN_micro=FN_micro+FN\n    return TP_micro,FP_micro,FN_micro\ndef compute_f1_micro_use_TFFPFN(label_dict):\n    """"""\n    compute f1_micro\n    :param label_dict: {label:(TP,FP,FN)}\n    :return: f1_micro: a scalar\n    """"""\n    TF_micro_accusation, FP_micro_accusation, FN_micro_accusation =compute_TF_FP_FN_micro(label_dict)\n    f1_micro_accusation = compute_f1(TF_micro_accusation, FP_micro_accusation, FN_micro_accusation,\'micro\')\n    return f1_micro_accusation\n\ndef compute_f1_macro_use_TFFPFN(label_dict):\n    """"""\n    compute f1_macro\n    :param label_dict: {label:(TP,FP,FN)}\n    :return: f1_macro\n    """"""\n    f1_dict= {}\n    num_classes=len(label_dict)\n    for label, tuplee in label_dict.items():\n        TP,FP,FN=tuplee\n        f1_score_onelabel=compute_f1(TP,FP,FN,\'macro\')\n        f1_dict[label]=f1_score_onelabel\n    f1_score_sum=0.0\n    for label,f1_score in f1_dict.items():\n        f1_score_sum=f1_score_sum+f1_score\n    f1_score=f1_score_sum/float(num_classes)\n    return f1_score\n\nsmall_value=0.00001\ndef compute_f1(TP,FP,FN,compute_type):\n    """"""\n    compute f1\n    :param TP_micro: number.e.g. 200\n    :param FP_micro: number.e.g. 200\n    :param FN_micro: number.e.g. 200\n    :return: f1_score: a scalar\n    """"""\n    precison=TP/(TP+FP+small_value)\n    recall=TP/(TP+FN+small_value)\n    f1_score=(2*precison*recall)/(precison+recall+small_value)\n\n    if random.choice([x for x in range(500)]) == 1:print(compute_type,""precison:"",str(precison),"";recall:"",str(recall),"";f1_score:"",f1_score)\n\n    return f1_score\ndef init_label_dict(num_classes):\n    """"""\n    init label dict. this dict will be used to save TP,FP,FN\n    :param num_classes:\n    :return: label_dict: a dict. {label_index:(0,0,0)}\n    """"""\n    label_dict={}\n    for i in range(num_classes):\n        label_dict[i]=(0,0,0)\n    return label_dict\n\ndef get_target_label_short(eval_y):\n    eval_y_short=[] #will be like:[22,642,1391]\n    for index,label in enumerate(eval_y):\n        if label>0:\n            eval_y_short.append(index)\n    return eval_y_short\n\ndef get_target_label_short_batch(eval_y_big): # tested.\n    eval_y_short_big=[] #will be like:[22,642,1391]\n    for ind, eval_y in enumerate(eval_y_big):\n        eval_y_short=[]\n        for index,label in enumerate(eval_y):\n            if label>0:\n                eval_y_short.append(index)\n        eval_y_short_big.append(eval_y_short)\n    return eval_y_short_big\n\n#eval_y_big=np.zeros((3,6))\n#eval_y_big[0,5]=1\n#eval_y_big[0,0]=1\n#eval_y_big[1,0]=1\n#eval_y_big[1,1]=1\n#print(""eval_y_big:"",eval_y_big)\n#result=get_target_label_short_batch(eval_y_big)\n#print(""result:"",result)\n\n#get top5 predicted labels\ndef get_label_using_prob(prob,top_number=5):\n    y_predict_labels = [i for i in range(len(prob)) if prob[i] >= 0.50]  # TODO 0.5PW e.g.[2,12,13,10]\n    if len(y_predict_labels) < 1:\n        y_predict_labels = [np.argmax(prob)]\n    return y_predict_labels\n\ndef get_label_using_logits_batch(prob,top_number=5): # tested.\n    result_labels=[]\n    for i in range(len(prob)):\n        single_prob=prob[i]\n        labels=get_label_using_prob(single_prob)\n        result_labels.append(labels)\n    return result_labels\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\ndef compute_confuse_matrix_batch(y_targetlabel_list,y_logits_array,label_dict,name=\'default\'):\n    """"""\n    compute confuse matrix for a batch\n    :param y_targetlabel_list: a list; each element is a mulit-hot,e.g. [1,0,0,1,...]\n    :param y_logits_array: a 2-d array. [batch_size,num_class]\n    :param label_dict:{label:(TP, FP, FN)}\n    :param name: a string for debug purpose\n    :return:label_dict:{label:(TP, FP, FN)}\n    """"""\n    for i,y_targetlabel_list_single in enumerate(y_targetlabel_list):\n        label_dict=compute_confuse_matrix(y_targetlabel_list_single,y_logits_array[i],label_dict,name=name)\n    return label_dict'"
a00_boosting/a08_boosting.py,1,"b'# -*- coding: utf-8 -*-\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n#main process for boosting:\r\n#1.compute label weight after each epoch using validation data.\r\n#2.get weights for each batch during traininig process\r\n#3.compute loss using cross entropy with weights\r\n\r\n#1.compute label weight after each epoch using validation data.\r\ndef compute_labels_weights(weights_label,logits,labels):\r\n    """"""\r\n    compute weights for labels in current batch, and update weights_label(a dict)\r\n    :param weights_label:a dict\r\n    :param logit: [None,Vocabulary_size]\r\n    :param label: [None,]\r\n    :return:\r\n    """"""\r\n    labels_predict=np.argmax(logits,axis=1) # logits:(256,108,754)\r\n    for i in range(len(labels)):\r\n        label=labels[i]\r\n        label_predict=labels_predict[i]\r\n        weight=weights_label.get(label,None)\r\n        if weight==None:\r\n            if label_predict == label:\r\n                weights_label[label]=(1,1)\r\n            else:\r\n                weights_label[label]=(1,0)\r\n        else:\r\n            number=weight[0]\r\n            correct=weight[1]\r\n            number=number+1\r\n            if label_predict==label:\r\n                correct=correct+1\r\n            weights_label[label]=(number,correct)\r\n    return weights_label\r\n\r\n#2.get weights for each batch during traininig process\r\ndef get_weights_for_current_batch(answer_list,weights_dict):\r\n    """"""\r\n    get weights for current batch\r\n    :param  answer_list: a numpy array contain labels for a batch\r\n    :param  weights_dict: a dict that contain weights for all labels\r\n    :return: a list. length is label size.\r\n    """"""\r\n    weights_list_batch=list(np.ones((len(answer_list))))\r\n    answer_list=list(answer_list)\r\n    for i,label in enumerate(answer_list):\r\n        acc=weights_dict[label]\r\n        weights_list_batch[i]=min(1.5,1.0/(acc+0.001))\r\n    #if np.random.choice(200)==0: #print something from time to time\r\n    #    print(""weights_list_batch:"",weights_list_batch)\r\n    return weights_list_batch\r\n\r\n#3.compute loss using cross entropy with weights\r\ndef loss(logits,labels,weights):\r\n    loss= tf.losses.sparse_softmax_cross_entropy(labels, logits,weights=weights)\r\n    return loss\r\n\r\n#######################################################################\r\n#util function\r\ndef get_weights_label_as_standard_dict(weights_label):\r\n    weights_dict = {}\r\n    for k,v in weights_label.items():\r\n        count,correct=v\r\n        weights_dict[k]=float(correct)/float(count)\r\n    return weights_dict\r\n'"
a01_FastText/p5_fastTextB_predict_multilabel.py,20,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p5_fastTextB_model import fastTextB as fastText\nfrom p4_zhihu_load_data import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport codecs\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""label_size"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_integer(""num_sampled"",100,""number of noise sampling"")\ntf.app.flags.DEFINE_string(""ckpt_dir"",""fast_text_checkpoint_multi/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",300,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 3, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""fast_text_checkpoint_multi/zhihu_result_ftB_multilabel.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-v4only-title.txt\',""target file path for final prediction"")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary()\n    vocab_size = len(vocabulary_word2index)\n    print(""vocab_size:"",vocab_size)\n    #iii=0\n    #iii/0\n    vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label()\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file) #TODO\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists) #TODO\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n\n    # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        fast_text=fastText(FLAGS.label_size, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.num_sampled,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        batch_size=1\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data+1, batch_size)):\n            logits=sess.run(fast_text.logits,feed_dict={fast_text.sentence:testX2[start:end]}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a01_FastText/p6_fastTextB_model_multilabel.py,26,"b'# autor:xul\n# fast text. using: very simple model;n-gram to captrue location information;h-softmax to speed up training/inference\n# for the n-gram you can use data_util to generate. see method process_one_sentence_to_get_ui_bi_tri_gram under aa1_data_util/data_util_zhihu.py\nimport tensorflow as tf\n\nclass fastTextB:\n    def __init__(self, label_size, learning_rate, batch_size, decay_steps, decay_rate,num_sampled,sentence_len,vocab_size,embed_size,is_training,max_label_per_example=5):\n        """"""init all hyperparameter here""""""\n        # 1.set hyper-paramter\n        self.label_size = label_size #e.g.1999\n        self.batch_size = batch_size\n        self.num_sampled = num_sampled\n        self.sentence_len=sentence_len\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.max_label_per_example=max_label_per_example\n        self.initializer=tf.random_normal_initializer(stddev=0.1)\n\n        # 2.add placeholder (X,label)\n        self.sentence = tf.placeholder(tf.int32, [None, self.sentence_len], name=""sentence"")     #X\n        self.labels = tf.placeholder(tf.int64, [None,self.max_label_per_example], name=""Labels"") #y [1,2,3,3,3]\n        self.labels_l1999=tf.placeholder(tf.float32,[None,self.label_size]) # int64\n        #3.set some variables\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0, trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\n\n        #4.init weights\n        self.instantiate_weights()\n        #5.main graph: inference\n        self.logits = self.inference() #[None, self.label_size]\n        #6.calculate loss\n        self.loss_val = self.loss()\n        #7.start training by update parameters using according loss\n        self.train_op = self.train()\n\n        #8.calcuate accuracy\n        # correct_prediction = tf.equal(tf.argmax(self.logits, 1), self.labels) #2.TODO tf.argmax(self.logits, 1)-->[batch_size]\n        # self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") #TODO\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        # embedding matrix\n        self.Embedding = tf.get_variable(""Embedding"", [self.vocab_size, self.embed_size],initializer=self.initializer)\n        self.W = tf.get_variable(""W"", [self.embed_size, self.label_size],initializer=self.initializer)\n        self.b = tf.get_variable(""b"", [self.label_size])\n\n    def inference(self):\n        """"""main computation graph here: 1.embedding-->2.average-->3.linear classifier""""""\n        # 1.get emebedding of words in the sentence\n        sentence_embeddings = tf.nn.embedding_lookup(self.Embedding,self.sentence)  # [None,self.sentence_len,self.embed_size]\n\n        # 2.average vectors, to get representation of the sentence\n        self.sentence_embeddings = tf.reduce_mean(sentence_embeddings, axis=1)  # [None,self.embed_size]\n\n        # 3.linear classifier layer\n        logits = tf.matmul(self.sentence_embeddings, self.W) + self.b #[None, self.label_size]==tf.matmul([None,self.embed_size],[self.embed_size,self.label_size])\n        return logits\n\n\n    def loss(self,l2_lambda=0.0001):\n        """"""calculate loss using (NCE)cross entropy here""""""\n        # Compute the average NCE loss for the batch.\n        # tf.nce_loss automatically draws a new sample of the negative labels each\n        # time we evaluate the loss.\n        #if self.is_training:#training\n            #labels=tf.reshape(self.labels,[-1])               #3.[batch_size,max_label_per_example]------>[batch_size*max_label_per_example,]\n            #labels=tf.expand_dims(labels,1)                   #[batch_size*max_label_per_example,]----->[batch_size*max_label_per_example,1]\n            #nce_loss: notice-->for now, if you have a variable number of target classes, you can pad them out to a constant number by either repeating them or by padding with an otherwise unused class.\n         #   loss = tf.reduce_mean(#inputs\'s SHAPE should be: [batch_size, dim]\n         #       tf.nn.nce_loss(weights=tf.transpose(self.W),  #[embed_size, label_size]--->[label_size,embed_size]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n         #                      biases=self.b,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n         #                      labels=self.labels,                 #4.[batch_size,max_label_per_example]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n         #                      inputs=self.sentence_embeddings,#TODO [None,self.embed_size] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n         #                      num_sampled=self.num_sampled,  #  scalar. 100\n         #                      num_true=self.max_label_per_example,\n         #                      num_classes=self.label_size,partition_strategy=""div""))  #scalar. 1999\n        #else:#eval(/inference)\n        labels_multi_hot = self.labels_l1999 #[batch_size,label_size]\n        #sigmoid_cross_entropy_with_logits:Computes sigmoid cross entropy given `logits`.Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive.  For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_multi_hot,logits=self.logits) #labels:[batch_size,label_size];logits:[batch, label_size]\n        loss = tf.reduce_mean(tf.reduce_sum(loss, axis=1)) # reduce_sum\n        print(""loss:"",loss)\n\n        # add regularization result in not converge\n        self.l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n        print(""l2_losses:"",self.l2_losses)\n        loss=loss+self.l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n'"
a01_FastText/p6_fastTextB_train_multilabel.py,25,"b'# -*- coding: utf-8 -*-\n""""""\ntraining the model.\nprocess--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nfast text. using: very simple model;n-gram to captrue location information;h-softmax to speed up training/inference\nfor the n-gram,you can use data_util to generate. see method process_one_sentence_to_get_ui_bi_tri_gram under aa1_data_util/data_util_zhihu.py\n\n""""""\nimport tensorflow as tf\nimport numpy as np\nfrom p6_fastTextB_model_multilabel import fastTextB as fastText\n#from p4_zhihu_load_data import load_data_with_multilabels,create_voabulary,create_voabulary_label\n#from tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\nimport h5py\n#configuration\nFLAGS=tf.app.flags.FLAGS\n#tf.app.flags.DEFINE_integer(""label_size"",1999,""number of label"")\ntf.app.flags.DEFINE_string(""cache_file_h5py"",""../data/ieee_zhihu_cup/data.h5"",""path of training/validation/test data."") #../data/sample_multiple_label.txt\ntf.app.flags.DEFINE_string(""cache_file_pickle"",""../data/ieee_zhihu_cup/vocab_label.pik"",""path of vocabulary and label files"") #../data/sample_multiple_label.txt\n\ntf.app.flags.DEFINE_float(""learning_rate"",0.001,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Batch size for training/evaluating."") #512\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 20000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_integer(""num_sampled"",10,""number of noise sampling"") #100\ntf.app.flags.DEFINE_string(""ckpt_dir"",""fast_text_checkpoint_multi/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",200,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",128,""embedding size"") #100\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",25,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\n#tf.app.flags.DEFINE_string(""training_path"", \'/home/xul/xul/9_fastTextB/training-data/test-zhihu6-only-title-multilabel-trigram.txt\', ""location of traning data."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",False,""whether to use embedding or not."")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    trainX, trainY, testX, testY = None, None, None, None\n    #vocabulary_word2index, vocabulary_index2word = create_voabulary()\n    #vocab_size = len(vocabulary_word2index)\n    #vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label()\n    #train,test = load_data_with_multilabels(vocabulary_word2index, vocabulary_word2index_label,FLAGS.training_path) #[1,11,3,1998,1998]\n    #trainX, trainY= train #TODO trainY1999\n    #testX, testY = test #TODO testY1999\n    #print(""testX.shape:"", np.array(testX).shape);print(""testY.shape:"", np.array(testY).shape)  # 2500\xe4\xb8\xaalabel\n    # 2.Data preprocessing\n    # Sequence padding\n    #print(""start padding & transform to one hot..."")\n    #trainX = pad_sequences(trainX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    #testX = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    #print(""end padding & transform to one hot..."")\n    word2index, label2index, trainX, trainY, vaildX, vaildY, testX, testY=load_data(FLAGS.cache_file_h5py, FLAGS.cache_file_pickle)\n    index2label={v:k for k,v in label2index.items()}\n    vocab_size = len(word2index);print(""cnn_model.vocab_size:"",vocab_size);num_classes=len(label2index);print(""num_classes:"",num_classes)\n    num_examples,FLAGS.sentence_len=trainX.shape\n    print(""num_examples of training:"",num_examples,"";sentence_len:"",FLAGS.sentence_len)\n\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        fast_text=fastText(num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.num_sampled,FLAGS.sentence_len,\n                           vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                vocabulary_index2word={v:k for k,v in word2index.items()}\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, fast_text)\n\n        curr_epoch=sess.run(fast_text.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):#range(start,stop,step_size)\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                #train_Y_batch=process_labels(trainY[start:end],number=start)\n                curr_loss,current_l2_loss,_=sess.run([fast_text.loss_val,fast_text.l2_losses,fast_text.train_op],\n                                                     feed_dict={fast_text.sentence:trainX[start:end],fast_text.labels_l1999:trainY[start:end]}) #fast_text.labels_l1999:trainY1999[start:end]\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end]) #2d-array. each element slength is a 100.\n                    print(""train_Y_batch:"",trainY[start:end]) #a list,each element is a list.element:may be has 1,2,3,4,5 labels.\n                    #print(""trainY1999[start:end]:"",trainY1999[start:end])\n                loss,counter=loss+curr_loss,counter+1 #acc+curr_acc,\n                if counter %50==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tL2 Loss:%.3f"" %(epoch,counter,loss/float(counter),current_l2_loss)) #\\tTrain Accuracy:%.3f--->,acc/float(counter)\n\n                if start%(1000*FLAGS.batch_size)==0:\n                    eval_loss, eval_accuracy = do_eval(sess, fast_text, vaildX, vaildY, batch_size,index2label)  # testY1999,eval_acc\n                    print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_accuracy))  # ,\\tValidation Accuracy: %.3f--->eval_acc\n                    # save model to checkpoint\n                    if start%(6000*FLAGS.batch_size)==0:\n                        print(""Going to save checkpoint."")\n                        save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                        saver.save(sess, save_path, global_step=epoch)  # fast_text.epoch_step\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(fast_text.epoch_increment)\n\n            # 4.validation\n            print(""epoch:"",epoch,""validate_every:"",FLAGS.validate_every,""validate or not:"",(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss,eval_accuracy=do_eval(sess,fast_text,vaildX,vaildY,batch_size,index2label) #testY1999,eval_acc\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_accuracy)) #,\\tValidation Accuracy: %.3f--->eval_acc\n                #save model to checkpoint\n                print(""Going to save checkpoint."")\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch) #fast_text.epoch_step\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, fast_text, testX, testY,batch_size,index2label) #testY1999\n    pass\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,fast_text,evalX,evalY,batch_size,vocabulary_index2word_label): #evalY1999\n    evalX=evalX[0:3000]\n    evalY=evalY[0:3000]\n    number_examples,labels=evalX.shape\n    print(""number_examples for validation:"",number_examples)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    batch_size=1\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        evalY_batch=process_labels(evalY[start:end])\n        curr_eval_loss,logit = sess.run([fast_text.loss_val,fast_text.logits], #curr_eval_acc-->fast_text.accuracy\n                                          feed_dict={fast_text.sentence: evalX[start:end],fast_text.labels_l1999: evalY[start:end]}) #,fast_text.labels_l1999:evalY1999[start:end]\n        #print(""do_eval.logits_"",logits_.shape)\n        label_list_top5 = get_label_using_logits(logit[0], vocabulary_index2word_label)\n        curr_eval_acc=calculate_accuracy(list(label_list_top5),evalY_batch[0] ,eval_counter) # evalY[start:end][0]\n        eval_loss,eval_counter,eval_acc=eval_loss+curr_eval_loss,eval_counter+1,eval_acc+curr_eval_acc\n\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,fast_text):\n    print(""using pre-trained word emebedding.started..."")\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(\'zhihu-word2vec-multilabel.bin-100\', kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(fast_text.Embedding,\n                                   word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    if eval_counter<10:\n        print(""labels_predicted:"",labels_predicted,"" ;labels:"",labels)\n    count = 0\n    label_dict = {x: x for x in labels}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\ndef load_data(cache_file_h5py,cache_file_pickle):\n    """"""\n    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n    :param cache_file_h5py:\n    :param cache_file_pickle:\n    :return:\n    """"""\n    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n        raise RuntimeError(""############################ERROR##############################\\n. ""\n                           ""please download cache file, it include training data and vocabulary & labels. ""\n                           ""link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.""\n                           ""cache_file_h5py and FLAGS.cache_file_pickle suggested location."")\n    print(""INFO. cache file exists. going to load cache file"")\n    f_data = h5py.File(cache_file_h5py, \'r\')\n    print(""f_data.keys:"",list(f_data.keys()))\n    train_X=f_data[\'train_X\'] # np.array(\n    print(""train_X.shape:"",train_X.shape)\n    train_Y=f_data[\'train_Y\'] # np.array(\n    print(""train_Y.shape:"",train_Y.shape,"";"")\n    vaild_X=f_data[\'vaild_X\'] # np.array(\n    valid_Y=f_data[\'valid_Y\'] # np.array(\n    test_X=f_data[\'test_X\'] # np.array(\n    test_Y=f_data[\'test_Y\'] # np.array(\n\n\n    word2index, label2index=None,None\n    with open(cache_file_pickle, \'rb\') as data_f_pickle:\n        word2index, label2index=pickle.load(data_f_pickle)\n    print(""INFO. cache file load successful..."")\n    return word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y\n\ndef process_labels(trainY_batch,require_size=5,number=None):\n    """"""\n    process labels to get fixed size labels given a spense label\n    :param trainY_batch:\n    :return:\n    """"""\n    #print(""###trainY_batch:"",trainY_batch)\n    num_examples,_=trainY_batch.shape\n    trainY_batch_result=np.zeros((num_examples,require_size),dtype=int)\n\n    for index in range(num_examples):\n        y_list_sparse=trainY_batch[index]\n        y_list_dense = [i for i, label in enumerate(y_list_sparse) if int(label) == 1]\n        y_list=proces_label_to_algin(y_list_dense,require_size=require_size)\n        trainY_batch_result[index]=y_list\n        if number is not None and number%30==0:\n            pass\n            #print(""####0.y_list_sparse:"",y_list_sparse)\n            #print(""####1.y_list_dense:"",y_list_dense)\n            #print(""####2.y_list:"",y_list) # 1.label_index: [315] ;2.y_list: [315, 315, 315, 315, 315] ;3.y_list: [0. 0. 0. ... 0. 0. 0.]\n    if number is not None and number % 30 == 0:\n        #print(""###3trainY_batch_result:"",trainY_batch_result)\n        pass\n    return trainY_batch_result\n\ndef proces_label_to_algin(ys_list,require_size=5):\n    """"""\n    given a list of labels, process it to fixed size(\'require_size\')\n    :param ys_list: a list\n    :return: a list\n    """"""\n    ys_list_result=[0 for x in range(require_size)]\n    if len(ys_list)>=require_size: #\xe8\xb6\x85\xe9\x95\xbf\n        ys_list_result=ys_list[0:require_size]\n    else:#\xe5\xa4\xaa\xe7\x9f\xad\n       if len(ys_list)==1:\n           ys_list_result =[ys_list[0] for x in range(require_size)]\n       elif len(ys_list)==2:\n           ys_list_result = [ys_list[0],ys_list[0],ys_list[0],ys_list[1],ys_list[1]]\n       elif len(ys_list) == 3:\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[1], ys_list[2]]\n       elif len(ys_list) == 4:\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[2], ys_list[3]]\n    return ys_list_result\nif __name__ == ""__main__"":\n    tf.app.run()'"
a02_TextCNN/__init__.py,0,b''
a02_TextCNN/data_util.py,0,"b'# -*- coding: utf-8 -*-\nimport codecs\nimport random\nimport numpy as np\nfrom tflearn.data_utils import pad_sequences\nfrom collections import Counter\nimport os\nimport pickle\n\nPAD_ID = 0\nUNK_ID=1\n_PAD=""_PAD""\n_UNK=""UNK""\n\n\ndef load_data_multilabel(traning_data_path,vocab_word2index, vocab_label2index,sentence_len,training_portion=0.95):\n    """"""\n    convert data as indexes using word2index dicts.\n    :param traning_data_path:\n    :param vocab_word2index:\n    :param vocab_label2index:\n    :return:\n    """"""\n    file_object = codecs.open(traning_data_path, mode=\'r\', encoding=\'utf-8\')\n    lines = file_object.readlines()\n    random.shuffle(lines)\n    label_size=len(vocab_label2index)\n    X = []\n    Y = []\n    for i,line in enumerate(lines):\n        raw_list = line.strip().split(""__label__"")\n        input_list = raw_list[0].strip().split("" "")\n        input_list = [x.strip().replace("" "", """") for x in input_list if x != \'\']\n        x=[vocab_word2index.get(x,UNK_ID) for x in input_list]\n        label_list = raw_list[1:]\n        label_list=[l.strip().replace("" "", """") for l in label_list if l != \'\']\n        label_list=[vocab_label2index[label] for label in label_list]\n        y=transform_multilabel_as_multihot(label_list,label_size)\n        X.append(x)\n        Y.append(y)\n        if i<10:print(i,""line:"",line)\n\n    X = pad_sequences(X, maxlen=sentence_len, value=0.)  # padding to max length\n    number_examples = len(lines)\n    training_number=int(training_portion* number_examples)\n    train = (X[0:training_number], Y[0:training_number])\n    valid_number=min(1000,number_examples-training_number)\n    test = (X[training_number+ 1:training_number+valid_number+1], Y[training_number + 1:training_number+valid_number+1])\n    return train,test\n\n\ndef transform_multilabel_as_multihot(label_list,label_size):\n    """"""\n    convert to multi-hot style\n    :param label_list: e.g.[0,1,4], here 4 means in the 4th position it is true value(as indicate by\'1\')\n    :param label_size: e.g.199\n    :return:e.g.[1,1,0,1,0,0,........]\n    """"""\n    result=np.zeros(label_size)\n    #set those location as 1, all else place as 0.\n    result[label_list] = 1\n    return result\n\n#use pretrained word embedding to get word vocabulary and labels, and its relationship with index\ndef create_vocabulary(training_data_path,vocab_size,name_scope=\'cnn\'):\n    """"""\n    create vocabulary\n    :param training_data_path:\n    :param vocab_size:\n    :param name_scope:\n    :return:\n    """"""\n\n    cache_vocabulary_label_pik=\'cache\'+""_""+name_scope # path to save cache\n    if not os.path.isdir(cache_vocabulary_label_pik): # create folder if not exists.\n        os.makedirs(cache_vocabulary_label_pik)\n\n    # if cache exists. load it; otherwise create it.\n    cache_path =cache_vocabulary_label_pik+""/""+\'vocab_label.pik\'\n    print(""cache_path:"",cache_path,""file_exists:"",os.path.exists(cache_path))\n    if os.path.exists(cache_path):\n        with open(cache_path, \'rb\') as data_f:\n            return pickle.load(data_f)\n    else:\n        vocabulary_word2index={}\n        vocabulary_index2word={}\n        vocabulary_word2index[_PAD]=PAD_ID\n        vocabulary_index2word[PAD_ID]=_PAD\n        vocabulary_word2index[_UNK]=UNK_ID\n        vocabulary_index2word[UNK_ID]=_UNK\n\n        vocabulary_label2index={}\n        vocabulary_index2label={}\n\n        #1.load raw data\n        file_object = codecs.open(training_data_path, mode=\'r\', encoding=\'utf-8\')\n        lines=file_object.readlines()\n        #2.loop each line,put to counter\n        c_inputs=Counter()\n        c_labels=Counter()\n        for line in lines:\n            raw_list=line.strip().split(""__label__"")\n\n            input_list = raw_list[0].strip().split("" "")\n            input_list = [x.strip().replace("" "", """") for x in input_list if x != \'\']\n            label_list=[l.strip().replace("" "","""") for l in raw_list[1:] if l!=\'\']\n            c_inputs.update(input_list)\n            c_labels.update(label_list)\n        #return most frequency words\n        vocab_list=c_inputs.most_common(vocab_size)\n        label_list=c_labels.most_common()\n        #put those words to dict\n        for i,tuplee in enumerate(vocab_list):\n            word,_=tuplee\n            vocabulary_word2index[word]=i+2\n            vocabulary_index2word[i+2]=word\n\n        for i,tuplee in enumerate(label_list):\n            label,_=tuplee;label=str(label)\n            vocabulary_label2index[label]=i\n            vocabulary_index2label[i]=label\n\n        #save to file system if vocabulary of words not exists.\n        if not os.path.exists(cache_path):\n            with open(cache_path, \'ab\') as data_f:\n                pickle.dump((vocabulary_word2index,vocabulary_index2word,vocabulary_label2index,vocabulary_index2label), data_f)\n    return vocabulary_word2index,vocabulary_index2word,vocabulary_label2index,vocabulary_index2label\n\n\ndef load_data(cache_file_h5py,cache_file_pickle):\n    """"""\n    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n    :param cache_file_h5py:\n    :param cache_file_pickle:\n    :return:\n    """"""\n    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n        raise RuntimeError(""############################ERROR##############################\\n. ""\n                           ""please download cache file, it include training data and vocabulary & labels. ""\n                           ""link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.""\n                           ""cache_file_h5py and FLAGS.cache_file_pickle suggested location."")\n    print(""INFO. cache file exists. going to load cache file"")\n    f_data = h5py.File(cache_file_h5py, \'r\')\n    print(""f_data.keys:"",list(f_data.keys()))\n    train_X=f_data[\'train_X\'] # np.array(\n    print(""train_X.shape:"",train_X.shape)\n    train_Y=f_data[\'train_Y\'] # np.array(\n    print(""train_Y.shape:"",train_Y.shape,"";"")\n    vaild_X=f_data[\'vaild_X\'] # np.array(\n    valid_Y=f_data[\'valid_Y\'] # np.array(\n    test_X=f_data[\'test_X\'] # np.array(\n    test_Y=f_data[\'test_Y\'] # np.array(\n    #print(train_X)\n    #f_data.close()\n\n    word2index, label2index=None,None\n    with open(cache_file_pickle, \'rb\') as data_f_pickle:\n        word2index, label2index=pickle.load(data_f_pickle)\n    print(""INFO. cache file load successful..."")\n    return word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y\n\n#training_data_path=\'../data/sample_multiple_label3.txt\'\n#vocab_size=100\n#create_voabulary(training_data_path,vocab_size)\n'"
a02_TextCNN/p7_TextCNN_model.py,83,"b'# -*- coding: utf-8 -*-\n#TextCNN: 1. embeddding layers, 2.convolutional layer, 3.max-pooling, 4.softmax layer.\n# print(""started..."")\nimport tensorflow as tf\nimport numpy as np\n\nclass TextCNN:\n    def __init__(self, filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size\n                 ,initializer=tf.random_normal_initializer(stddev=0.1),multi_label_flag=False,clip_gradients=5.0,decay_rate_big=0.50):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")#ADD learning_rate\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * decay_rate_big)\n        self.filter_sizes=filter_sizes # it is a list of int. e.g. [3,4,5]\n        self.num_filters=num_filters\n        self.initializer=initializer\n        self.num_filters_total=self.num_filters * len(filter_sizes) #how many filters totally.\n        self.multi_label_flag=multi_label_flag\n        self.clip_gradients = clip_gradients\n        self.is_training_flag = tf.placeholder(tf.bool, name=""is_training_flag"")\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        #self.input_y = tf.placeholder(tf.int32, [None,],name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32,[None,self.num_classes], name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n        self.iter = tf.placeholder(tf.int32) #training iteration\n        self.tst=tf.placeholder(tf.bool)\n        self.use_mulitple_layer_cnn=False\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.b1 = tf.Variable(tf.ones([self.num_filters]) / 10)\n        self.b2 = tf.Variable(tf.ones([self.num_filters]) / 10)\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        self.possibility=tf.nn.sigmoid(self.logits)\n        if multi_label_flag:\n            print(""going to use multi label loss."");\n            self.loss_val = self.loss_multilabel()\n        else:print(""going to use single label loss."");self.loss_val = self.loss()\n        self.train_op = self.train()\n        if not self.multi_label_flag:\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\n            print(""self.predictions:"", self.predictions)\n            correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy =tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.num_filters_total, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size] #ADD 2017.06.09\n\n    def inference(self):\n        """"""main computation graph here: 1.embedding-->2.CONV-BN-RELU-MAX_POOLING-->3.linear classifier""""""\n        # 1.=====>get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x)#[None,sentence_length,embed_size]\n        self.sentence_embeddings_expanded=tf.expand_dims(self.embedded_words,-1) #[None,sentence_length,embed_size,1). expand dimension so meet input requirement of 2d-conv\n\n        # 2.=====>loop each filter size. for each filter, do:convolution-pooling layer(a.create filters,b.conv,c.apply nolinearity,d.max-pooling)--->\n        # you can use:tf.nn.conv2d;tf.nn.relu;tf.nn.max_pool; feature shape is 4-d. feature is a new variable\n        #if self.use_mulitple_layer_cnn: # this may take 50G memory.\n        #    print(""use multiple layer CNN"")\n        #    h=self.cnn_multiple_layers()\n        #else: # this take small memory, less than 2G memory.\n        print(""use single layer CNN"")\n        h=self.cnn_single_layer()\n        #5. logits(use linear layer)and predictions(argmax)\n        with tf.name_scope(""output""):\n            logits = tf.matmul(h,self.W_projection) + self.b_projection  #shape:[None, self.num_classes]==tf.matmul([None,self.embed_size],[self.embed_size,self.num_classes])\n        return logits\n\n    def cnn_single_layer(self):\n        pooled_outputs = []\n        for i, filter_size in enumerate(self.filter_sizes):\n            # with tf.name_scope(""convolution-pooling-%s"" %filter_size):\n            with tf.variable_scope(""convolution-pooling-%s"" % filter_size):\n                # ====>a.create filter\n                filter = tf.get_variable(""filter-%s"" % filter_size, [filter_size, self.embed_size, 1, self.num_filters],initializer=self.initializer)\n                # ====>b.conv operation: conv2d===>computes a 2-D convolution given 4-D `input` and `filter` tensors.\n                # Conv.Input: given an input tensor of shape `[batch, in_height, in_width, in_channels]` and a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`\n                # Conv.Returns: A `Tensor`. Has the same type as `input`.\n                #         A 4-D tensor. The dimension order is determined by the value of `data_format`, see below for details.\n                # 1)each filter with conv2d\'s output a shape:[1,sequence_length-filter_size+1,1,1];2)*num_filters--->[1,sequence_length-filter_size+1,1,num_filters];3)*batch_size--->[batch_size,sequence_length-filter_size+1,1,num_filters]\n                # input data format:NHWC:[batch, height, width, channels];output:4-D\n                conv = tf.nn.conv2d(self.sentence_embeddings_expanded, filter, strides=[1, 1, 1, 1], padding=""VALID"",name=""conv"")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]\n                conv = tf.contrib.layers.batch_norm(conv, is_training=self.is_training_flag, scope=\'cnn_bn_\')\n\n                # ====>c. apply nolinearity\n                b = tf.get_variable(""b-%s"" % filter_size, [self.num_filters])  # ADD 2017-06-09\n                h = tf.nn.relu(tf.nn.bias_add(conv, b),""relu"")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n                # ====>. max-pooling.  value: A 4-D `Tensor` with shape `[batch, height, width, channels]\n                #                  ksize: A list of ints that has length >= 4.  The size of the window for each dimension of the input tensor.\n                #                  strides: A list of ints that has length >= 4.  The stride of the sliding window for each dimension of the input tensor.\n                pooled = tf.nn.max_pool(h, ksize=[1, self.sequence_length - filter_size + 1, 1, 1],strides=[1, 1, 1, 1], padding=\'VALID\',name=""pool"")  # shape:[batch_size, 1, 1, num_filters].max_pool:performs the max pooling on the input.\n                pooled_outputs.append(pooled)\n        # 3.=====>combine all pooled features, and flatten the feature.output\' shape is a [1,None]\n        # e.g. >>> x1=tf.ones([3,3]);x2=tf.ones([3,3]);x=[x1,x2]\n        #         x12_0=tf.concat(x,0)---->x12_0\' shape:[6,3]\n        #         x12_1=tf.concat(x,1)---->x12_1\' shape;[3,6]\n        self.h_pool = tf.concat(pooled_outputs,3)  # shape:[batch_size, 1, 1, num_filters_total]. tf.concat=>concatenates tensors along one dimension.where num_filters_total=num_filters_1+num_filters_2+num_filters_3\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1,self.num_filters_total])  # shape should be:[None,num_filters_total]. here this operation has some result as tf.sequeeze().e.g. x\'s shape:[3,3];tf.reshape(-1,x) & (3, 3)---->(1,9)\n\n        # 4.=====>add dropout: use tf.nn.dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, keep_prob=self.dropout_keep_prob)  # [None,num_filters_total]\n        h = tf.layers.dense(self.h_drop, self.num_filters_total, activation=tf.nn.tanh, use_bias=True)\n        return h\n\n    def cnn_multiple_layers(self):\n        # 2.=====>loop each filter size. for each filter, do:convolution-pooling layer(a.create filters,b.conv,c.apply nolinearity,d.max-pooling)--->\n        # you can use:tf.nn.conv2d;tf.nn.relu;tf.nn.max_pool; feature shape is 4-d. feature is a new variable\n        pooled_outputs = []\n        print(""sentence_embeddings_expanded:"",self.sentence_embeddings_expanded)\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.variable_scope(\'cnn_multiple_layers\' + ""convolution-pooling-%s"" % filter_size):\n                # 1) CNN->BN->relu\n                filter = tf.get_variable(""filter-%s"" % filter_size,[filter_size, self.embed_size, 1, self.num_filters],initializer=self.initializer)\n                conv = tf.nn.conv2d(self.sentence_embeddings_expanded, filter, strides=[1, 1, 1, 1],padding=""SAME"",name=""conv"")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]\n                conv = tf.contrib.layers.batch_norm(conv, is_training=self.is_training_flag, scope=\'cnn1\')\n                print(i, ""conv1:"", conv)\n                b = tf.get_variable(""b-%s"" % filter_size, [self.num_filters])  # ADD 2017-06-09\n                h = tf.nn.relu(tf.nn.bias_add(conv, b),""relu"")  # shape:[batch_size,sequence_length,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n\n                # 2) CNN->BN->relu\n                h = tf.reshape(h, [-1, self.sequence_length, self.num_filters,1])  # shape:[batch_size,sequence_length,num_filters,1]\n                # Layer2:CONV-RELU\n                filter2 = tf.get_variable(""filter2-%s"" % filter_size,[filter_size, self.num_filters, 1, self.num_filters],initializer=self.initializer)\n                conv2 = tf.nn.conv2d(h, filter2, strides=[1, 1, 1, 1], padding=""SAME"",name=""conv2"")  # shape:[batch_size,sequence_length-filter_size*2+2,1,num_filters]\n                conv2 = tf.contrib.layers.batch_norm(conv2, is_training=self.is_training_flag, scope=\'cnn2\')\n                print(i, ""conv2:"", conv2)\n                b2 = tf.get_variable(""b2-%s"" % filter_size, [self.num_filters])  # ADD 2017-06-09\n                h = tf.nn.relu(tf.nn.bias_add(conv2, b2),""relu2"")  # shape:[batch_size,sequence_length,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n\n                # 3. Max-pooling\n                pooling_max = tf.squeeze(tf.nn.max_pool(h, ksize=[1,self.sequence_length, 1, 1],strides=[1, 1, 1, 1], padding=\'VALID\', name=""pool""))\n                # pooling_avg=tf.squeeze(tf.reduce_mean(h,axis=1)) #[batch_size,num_filters]\n                print(i, ""pooling:"", pooling_max)\n                # pooling=tf.concat([pooling_max,pooling_avg],axis=1) #[batch_size,num_filters*2]\n                pooled_outputs.append(pooling_max)  # h:[batch_size,sequence_length,1,num_filters]\n        # concat\n        h = tf.concat(pooled_outputs, axis=1)  # [batch_size,num_filters*len(self.filter_sizes)]\n        print(""h.concat:"", h)\n\n        with tf.name_scope(""dropout""):\n            h = tf.nn.dropout(h,keep_prob=self.dropout_keep_prob)  # [batch_size,sequence_length - filter_size + 1,num_filters]\n        return h  # [batch_size,sequence_length - filter_size + 1,num_filters]\n\n    def loss_multilabel(self,l2_lambda=0.0001): #0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            #input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            #losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"",losses) #shape=(?, 1999).\n            losses=tf.reduce_sum(losses,axis=1) #shape=(?,). loss for all data in the batch\n            loss=tf.reduce_mean(losses)         #shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss(self,l2_lambda=0.0001):#0.001\n        with tf.name_scope(""loss""):\n            #input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def train_old(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\n        return train_op\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay_rate, staircase=True)\n        self.learning_rate_=learning_rate\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        gradients, variables = zip(*optimizer.compute_gradients(self.loss_val))\n        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) #ADD 2018.06.01\n        with tf.control_dependencies(update_ops):  #ADD 2018.06.01\n            train_op = optimizer.apply_gradients(zip(gradients, variables))\n        return train_op\n\n#test started. toy task: given a sequence of data. compute it\'s label: sum of its previous element,itself and next element greater than a threshold, it\'s label is 1,otherwise 0.\n#e.g. given inputs:[1,0,1,1,0]; outputs:[0,1,1,1,0].\n#invoke test() below to test the model in this toy task.\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to transform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=5\n    learning_rate=0.001\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.95\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1.0 #0.5\n    filter_sizes=[2,3,4]\n    num_filters=128\n    multi_label_flag=True\n    textRNN=TextCNN(filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training,multi_label_flag=multi_label_flag)\n    with tf.Session() as sess:\n       sess.run(tf.global_variables_initializer())\n       for i in range(500):\n           input_x=np.random.randn(batch_size,sequence_length) #[None, self.sequence_length]\n           input_x[input_x>=0]=1\n           input_x[input_x <0] = 0\n           input_y_multilabel=get_label_y(input_x)\n           loss,possibility,W_projection_value,_=sess.run([textRNN.loss_val,textRNN.possibility,textRNN.W_projection,textRNN.train_op],\n                                                    feed_dict={textRNN.input_x:input_x,textRNN.input_y_multilabel:input_y_multilabel,\n                                                               textRNN.dropout_keep_prob:dropout_keep_prob,textRNN.tst:False})\n           print(i,""loss:"",loss,""-------------------------------------------------------"")\n           print(""label:"",input_y_multilabel);#print(""possibility:"",possibility)\n\ndef get_label_y(input_x):\n    length=input_x.shape[0]\n    input_y=np.zeros((input_x.shape))\n    for i in range(length):\n        element=input_x[i,:] #[5,]\n        result=compute_single_label(element)\n        input_y[i,:]=result\n    return input_y\n\ndef compute_single_label(listt):\n    result=[]\n    length=len(listt)\n    for i,e in enumerate(listt):\n        previous=listt[i-1] if i>0 else 0\n        current=listt[i]\n        next=listt[i+1] if i<length-1 else 0\n        summ=previous+current+next\n        if summ>=2:\n            summ=1\n        else:\n            summ=0\n        result.append(summ)\n    return result\n\n\n#test()\n'"
a02_TextCNN/p7_TextCNN_predict.py,25,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\n# currently this file is not well test, so you can just ignore this file util it is tested or write a function, input is\n# a strings,output is predicted labels.\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\n#from a02_TextCNN.other_experiement.data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\nimport pickle\nimport h5py\nfrom data_util import load_data\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_cnn_title_desc_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",200,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""text_cnn_title_desc_checkpoint/zhihu_result_cnn_multilabel_v6_e14.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\ntf.app.flags.DEFINE_integer(""num_filters"", 128, ""number of filters"") #128\n\n##############################################################################################################################################\nfilter_sizes=[6,7,8]#[1,2,3,4,5,6,7]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n# 1.load data with vocabulary of words and labels\n#vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',\n#                                                                word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\nword2index, label2index, _, _, _, _, _, _ = load_data(FLAGS.cache_file_h5py,FLAGS.cache_file_pickle)\n\nvocab_size = len(word2index)\nvocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\nquestionid_question_lists = load_final_test_data(FLAGS.predict_source_file)\ntest = load_data_predict(word2index, vocabulary_word2index_label, questionid_question_lists)\ntestX = []\nquestion_id_list = []\nfor tuple in test:\n    question_id, question_string_list = tuple\n    question_id_list.append(question_id)\n    testX.append(question_string_list)\n# 2.Data preprocessing: Sequence padding\nprint(""start padding...."")\ntestX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\nprint(""end padding..."")\n# 3.create session.\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\ngraph=tf.Graph().as_default()\nglobal sess\nglobal textCNN\nwith graph:\n    sess=tf.Session(config=config)\n# 4.Instantiate Model\n    textCNN = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\n                  FLAGS.decay_steps, FLAGS.decay_rate,\n                  FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training)\n    saver = tf.train.Saver()\n    if os.path.exists(FLAGS.ckpt_dir + ""checkpoint""):\n        print(""Restoring Variables from Checkpoint"")\n        saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n    else:\n        print(""Can\'t find the checkpoint.going to stop"")\n    #return\n# 5.feed data, to get logits\nnumber_of_training_data = len(testX2);\nprint(""number_of_training_data:"", number_of_training_data)\n#index = 0\n#predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n#############################################################################################################################################\ndef get_logits_with_value_by_input(start,end):\n    x=testX2[start:end]\n    global sess\n    global textCNN\n    logits = sess.run(textCNN.logits, feed_dict={textCNN.input_x: x, textCNN.dropout_keep_prob: 1})\n    predicted_labels,value_labels = get_label_using_logits_with_value(logits[0], vocabulary_index2word_label)\n    value_labels_exp= np.exp(value_labels)\n    p_labels=value_labels_exp/np.sum(value_labels_exp)\n    return predicted_labels,p_labels\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,FLAGS.decay_rate,\n                        FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textCNN.logits,feed_dict={textCNN.input_x:testX2[start:end],textCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\ndef load_data(cache_file_h5py,cache_file_pickle):\n    """"""\n    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n    :param cache_file_h5py:\n    :param cache_file_pickle:\n    :return:\n    """"""\n    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n        raise RuntimeError(""############################ERROR##############################\\n. ""\n                           ""please download cache file, it include training data and vocabulary & labels. ""\n                           ""link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.""\n                           ""cache_file_h5py and FLAGS.cache_file_pickle suggested location."")\n    print(""INFO. cache file exists. going to load cache file"")\n    f_data = h5py.File(cache_file_h5py, \'r\')\n    print(""f_data.keys:"",list(f_data.keys()))\n    train_X=f_data[\'train_X\'] # np.array(\n    print(""train_X.shape:"",train_X.shape)\n    train_Y=f_data[\'train_Y\'] # np.array(\n    print(""train_Y.shape:"",train_Y.shape,"";"")\n    vaild_X=f_data[\'vaild_X\'] # np.array(\n    valid_Y=f_data[\'valid_Y\'] # np.array(\n    test_X=f_data[\'test_X\'] # np.array(\n    test_Y=f_data[\'test_Y\'] # np.array(\n    #f_data.close()\n\n    word2index, label2index=None,None\n    with open(cache_file_pickle, \'rb\') as data_f_pickle:\n        word2index, label2index=pickle.load(data_f_pickle)\n    print(""INFO. cache file load successful..."")\n    return word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y\n\nif __name__ == ""__main__"":\n    #tf.app.run()\n    labels,list_value=get_logits_with_value_by_input(0, 1)\n    print(""labels:"",labels)\n    print(""list_value:"", list_value)'"
a02_TextCNN/p7_TextCNN_train.py,28,"b'# -*- coding: utf-8 -*-\n#import sys\n#reload(sys)\n#sys.setdefaultencoding(\'utf-8\') #gb2312\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n#import sys\n#reload(sys)\n#sys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p7_TextCNN_model import TextCNN\n#from data_util import create_vocabulary,load_data_multilabel\nimport pickle\nimport h5py\nimport os\nimport random\nfrom numba import jit\n#configuration\nFLAGS=tf.app.flags.FLAGS\n\n#tf.app.flags.DEFINE_string(""traning_data_path"",""../data/sample_multiple_label.txt"",""path of traning data."") #../data/sample_multiple_label.txt\n#tf.app.flags.DEFINE_integer(""vocab_size"",100000,""maximum vocab size."")\n\ntf.app.flags.DEFINE_string(""cache_file_h5py"",""../data/ieee_zhihu_cup/data.h5"",""path of training/validation/test data."") #../data/sample_multiple_label.txt\ntf.app.flags.DEFINE_string(""cache_file_pickle"",""../data/ieee_zhihu_cup/vocab_label.pik"",""path of vocabulary and label files"") #../data/sample_multiple_label.txt\n\ntf.app.flags.DEFINE_float(""learning_rate"",0.0003,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 64, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 1000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_cnn_title_desc_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",200,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",128,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training_flag"",True,""is training.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",False,""whether to use embedding or not."")\ntf.app.flags.DEFINE_integer(""num_filters"", 128, ""number of filters"") #256--->512\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""word2vec-title-desc.bin"",""word2vec\'s vocabulary and vectors"")\ntf.app.flags.DEFINE_string(""name_scope"",""cnn"",""name scope value."")\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\nfilter_sizes=[6,7,8]\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #trainX, trainY, testX, testY = None, None, None, None\n    #vocabulary_word2index, vocabulary_index2word, vocabulary_label2index, _= create_vocabulary(FLAGS.traning_data_path,FLAGS.vocab_size,name_scope=FLAGS.name_scope)\n    word2index, label2index, trainX, trainY, vaildX, vaildY, testX, testY=load_data(FLAGS.cache_file_h5py, FLAGS.cache_file_pickle)\n    vocab_size = len(word2index);print(""cnn_model.vocab_size:"",vocab_size);num_classes=len(label2index);print(""num_classes:"",num_classes)\n    num_examples,FLAGS.sentence_len=trainX.shape\n    print(""num_examples of training:"",num_examples,"";sentence_len:"",FLAGS.sentence_len)\n    #train, test= load_data_multilabel(FLAGS.traning_data_path,vocabulary_word2index, vocabulary_label2index,FLAGS.sentence_len)\n    #trainX, trainY = train;testX, testY = test\n    #print some message for debug purpose\n    print(""trainX[0:10]:"", trainX[0:10])\n    print(""trainY[0]:"", trainY[0:10])\n    print(""train_y_short:"", trainY[0])\n\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                        FLAGS.decay_rate,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint."")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n            #for i in range(3): #decay learning rate if necessary.\n            #    print(i,""Going to decay learning rate by half."")\n            #    sess.run(textCNN.learning_rate_decay_half_op)\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                index2word={v:k for k,v in word2index.items()}\n                assign_pretrained_word_embedding(sess, index2word, vocab_size, textCNN,FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textCNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        iteration=0\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, counter =  0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                iteration=iteration+1\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])\n                feed_dict = {textCNN.input_x: trainX[start:end],textCNN.dropout_keep_prob: 0.8,textCNN.is_training_flag:FLAGS.is_training_flag}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[textCNN.input_y] = trainY[start:end]\n                else:\n                    feed_dict[textCNN.input_y_multilabel]=trainY[start:end]\n                curr_loss,lr,_=sess.run([textCNN.loss_val,textCNN.learning_rate,textCNN.train_op],feed_dict)\n                loss,counter=loss+curr_loss,counter+1\n                if counter %50==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tLearning rate:%.5f"" %(epoch,counter,loss/float(counter),lr))\n\n                ########################################################################################################\n                if start%(3000*FLAGS.batch_size)==0: # eval every 3000 steps.\n                    eval_loss, f1_score,f1_micro,f1_macro = do_eval(sess, textCNN, vaildX, vaildY,num_classes)\n                    print(""Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tF1_micro:%.3f\\tF1_macro:%.3f"" % (epoch, eval_loss, f1_score,f1_micro,f1_macro))\n                    # save model to checkpoint\n                    save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                    print(""Going to save model.."")\n                    saver.save(sess, save_path, global_step=epoch)\n                ########################################################################################################\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textCNN.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss,f1_score,f1_micro,f1_macro=do_eval(sess,textCNN,testX,testY,num_classes)\n                print(""Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tF1_micro:%.3f\\tF1_macro:%.3f"" % (epoch,eval_loss,f1_score,f1_micro,f1_macro))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss,f1_score,f1_micro,f1_macro = do_eval(sess, textCNN, testX, testY,num_classes)\n        print(""Test Loss:%.3f\\tF1 Score:%.3f\\tF1_micro:%.3f\\tF1_macro:%.3f"" % ( test_loss,f1_score,f1_micro,f1_macro))\n    pass\n\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess, textCNN, evalX, evalY, num_classes):\n    evalX = evalX[0:3000]\n    evalY = evalY[0:3000]\n    number_examples = len(evalX)\n    eval_loss, eval_counter, eval_f1_score, eval_p, eval_r = 0.0, 0, 0.0, 0.0, 0.0\n    batch_size = 1\n    predict = []\n\n    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples + batch_size, batch_size)):\n        \'\'\' evaluation in one batch \'\'\'\n        feed_dict = {textCNN.input_x: evalX[start:end],\n                     textCNN.input_y_multilabel: evalY[start:end],\n                     textCNN.dropout_keep_prob: 1.0,\n                     textCNN.is_training_flag: False}\n        current_eval_loss, logits = sess.run(\n            [textCNN.loss_val, textCNN.logits], feed_dict)\n        predict = [*predict, np.argmax(np.array(logits[0]))]\n        eval_loss += current_eval_loss\n        eval_counter += 1\n    evalY = [np.argmax(ii) for ii in evalY]\n\n    if not FLAGS.multi_label_flag:\n        predict = [int(ii > 0.5) for ii in predict]\n    _, _, f1_macro, f1_micro, _ = fastF1(predict, evalY, num_classes)\n    f1_score = (f1_micro + f1_macro) / 2.0\n    return eval_loss / float(eval_counter), f1_score, f1_micro, f1_macro\n\n@jit\ndef fastF1(result: list, predict: list, num_classes: int):\n    \'\'\' f1 score \'\'\'\n    true_total, r_total, p_total, p, r = 0, 0, 0, 0, 0\n    total_list = []\n    for trueValue in range(num_classes):\n        trueNum, recallNum, precisionNum = 0, 0, 0\n        for index, values in enumerate(result):\n            if values == trueValue:\n                recallNum += 1\n                if values == predict[index]:\n                    trueNum += 1\n            if predict[index] == trueValue:\n                precisionNum += 1\n        R = trueNum / recallNum if recallNum else 0\n        P = trueNum / precisionNum if precisionNum else 0\n        true_total += trueNum\n        r_total += recallNum\n        p_total += precisionNum\n        p += P\n        r += R\n        f1 = (2 * P * R) / (P + R) if (P + R) else 0\n        total_list.append([P, R, f1])\n    p, r = np.array([p, r]) / num_classes\n    micro_r, micro_p = true_total / np.array([r_total, p_total])\n    macro_f1 = (2 * p * r) / (p + r) if (p + r) else 0\n    micro_f1 = (2 * micro_p * micro_r) / (micro_p + micro_r) if (micro_p + micro_r) else 0\n    accuracy = true_total / len(result)\n    print(\'P: {:.2f}%, R: {:.2f}%, Micro_f1: {:.2f}%, Macro_f1: {:.2f}%, Accuracy: {:.2f}\'.format(\n        p * 100, r * 100, micro_f1 * 100, macro_f1 * 100, accuracy * 100))\n    return p, r, macro_f1, micro_f1, total_list\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textCNN,word2vec_model_path):\n    import word2vec # we put import here so that many people who do not use word2vec do not need to install this package. you can move import to the beginning of this file.\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(2, vocab_size):  # loop each word. notice that the first two words are pad and unknown token\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\ndef load_data(cache_file_h5py,cache_file_pickle):\n    """"""\n    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n    :param cache_file_h5py:\n    :param cache_file_pickle:\n    :return:\n    """"""\n    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n        raise RuntimeError(""############################ERROR##############################\\n. ""\n                           ""please download cache file, it include training data and vocabulary & labels. ""\n                           ""link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.""\n                           ""cache_file_h5py and FLAGS.cache_file_pickle suggested location."")\n    print(""INFO. cache file exists. going to load cache file"")\n    f_data = h5py.File(cache_file_h5py, \'r\')\n    print(""f_data.keys:"",list(f_data.keys()))\n    train_X=f_data[\'train_X\'] # np.array(\n    print(""train_X.shape:"",train_X.shape)\n    train_Y=f_data[\'train_Y\'] # np.array(\n    print(""train_Y.shape:"",train_Y.shape,"";"")\n    vaild_X=f_data[\'vaild_X\'] # np.array(\n    valid_Y=f_data[\'valid_Y\'] # np.array(\n    test_X=f_data[\'test_X\'] # np.array(\n    test_Y=f_data[\'test_Y\'] # np.array(\n    #print(train_X)\n    #f_data.close()\n\n    word2index, label2index=None,None\n    with open(cache_file_pickle, \'rb\') as data_f_pickle:\n        word2index, label2index=pickle.load(data_f_pickle)\n    print(""INFO. cache file load successful..."")\n    return word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
a02_TextCNN/p7_temp.py,0,"b'# -*- coding: utf-8 -*-\nimport random\ndef read_write(source_file_path,target_file_path):\n    # 1.read file\n    source_file_object=open(source_file_path,mode=\'r\')\n    target_file_object=open(target_file_path,mode=\'a\')\n\n    lines=source_file_object.readlines()\n    random.shuffle(lines)\n    # 2.write file\n    for i,line in enumerate(lines):\n        # line=w29368 w6 w7851 w24869 w35610 w111  __label__-5732564720446782766 2787171473654490487\n        raw_list = line.strip().split(""__label__"")\n        input_list = raw_list[0].strip().split("" "")\n        input_list = [x.strip().replace("" "", """") for x in input_list if x != \'\']\n        label_list = raw_list[1:]\n        label_list=label_list[0].split("" "")\n        label_list=[l.strip().replace("" "", """") for l in label_list if l != \'\']\n        strings="" "".join(input_list)+\' __label__\'+"" __label__"".join(label_list)+""\\n""\n        target_file_object.write(strings)\n        if i%10000==0:\n           print(i,strings)\n    target_file_object.close()\n    source_file_object.close()\n#source_file_path=\'/Users/xuliang/Downloads/train-zhihu6-title-desc-folder.txt/train-zhihu6-title-desc.txt\'\n#target_file_path=\'/Users/xuliang/Downloads/train-zhihu6-title-desc-folder.txt/train-zhihu-title-desc-multiple-label-v6.txt\'\n\nsource_file_path=\'/Users/xuliang/Downloads/train-zhihu6-title-desc-folder.txt/test-zhihu6-title-desc.txt\'\ntarget_file_path=\'/Users/xuliang/Downloads/train-zhihu6-title-desc-folder.txt/test-zhihu-title-desc-multiple-label-v6.txt\'\nread_write(source_file_path,target_file_path)\n'"
a03_TextRNN/p8_TextRNN_model.py,36,"b'# -*- coding: utf-8 -*-\n#TextRNN: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\n\nclass TextRNN:\n    def __init__(self,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,\n                 vocab_size,embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1)):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.hidden_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.initializer=initializer\n        self.num_sampled=20\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        self.input_y = tf.placeholder(tf.int32,[None], name=""input_y"")  # y [None,num_classes]\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        self.loss_val = self.loss() #-->self.loss_nce()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")  # shape:[None,]\n        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.hidden_size*2, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n    def inference(self):\n        """"""main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat, 4.FC layer 5.softmax """"""\n        #1.get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n        #2. Bi-lstm layer\n        # define lstm cess:get lstm cell output\n        lstm_fw_cell=rnn.BasicLSTMCell(self.hidden_size) #forward direction cell\n        lstm_bw_cell=rnn.BasicLSTMCell(self.hidden_size) #backward direction cell\n        if self.dropout_keep_prob is not None:\n            lstm_fw_cell=rnn.DropoutWrapper(lstm_fw_cell,output_keep_prob=self.dropout_keep_prob)\n            lstm_bw_cell=rnn.DropoutWrapper(lstm_bw_cell,output_keep_prob=self.dropout_keep_prob)\n        # bidirectional_dynamic_rnn: input: [batch_size, max_time, input_size]\n        #                            output: A tuple (outputs, output_states)\n        #                                    where:outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output `Tensor`.\n        outputs,_=tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell,self.embedded_words,dtype=tf.float32) #[batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\n        print(""outputs:===>"",outputs) #outputs:(<tf.Tensor \'bidirectional_rnn/fw/fw/transpose:0\' shape=(?, 5, 100) dtype=float32>, <tf.Tensor \'ReverseV2:0\' shape=(?, 5, 100) dtype=float32>))\n        #3. concat output\n        output_rnn=tf.concat(outputs,axis=2) #[batch_size,sequence_length,hidden_size*2]\n        #self.output_rnn_last=tf.reduce_mean(output_rnn,axis=1) #[batch_size,hidden_size*2] \n        self.output_rnn_last=output_rnn[:,-1,:] ##[batch_size,hidden_size*2] #TODO\n        print(""output_rnn_last:"", self.output_rnn_last) # <tf.Tensor \'strided_slice:0\' shape=(?, 200) dtype=float32>\n        #4. logits(use linear layer)\n        with tf.name_scope(""output""): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n            logits = tf.matmul(self.output_rnn_last, self.W_projection) + self.b_projection  # [batch_size,num_classes]\n        return logits\n\n    def loss(self,l2_lambda=0.0001):\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss_nce(self,l2_lambda=0.0001): #0.0001-->0.001\n        """"""calculate loss using (NCE)cross entropy here""""""\n        # Compute the average NCE loss for the batch.\n        # tf.nce_loss automatically draws a new sample of the negative labels each\n        # time we evaluate the loss.\n        if self.is_training: #training\n            #labels=tf.reshape(self.input_y,[-1])               #[batch_size,1]------>[batch_size,]\n            labels=tf.expand_dims(self.input_y,1)                   #[batch_size,]----->[batch_size,1]\n            loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n                tf.nn.nce_loss(weights=tf.transpose(self.W_projection),#[hidden_size*2, num_classes]--->[num_classes,hidden_size*2]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n                               biases=self.b_projection,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n                               labels=labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n                               inputs=self.output_rnn_last,# [batch_size,hidden_size*2] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n                               num_sampled=self.num_sampled,  #scalar. 100\n                               num_classes=self.num_classes,partition_strategy=""div""))  #scalar. 1999\n        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n        loss = loss + l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=10\n    learning_rate=0.01\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.9\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1#0.5\n    textRNN=TextRNN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            input_x=np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n            input_y=input_y=np.array([1,0,1,1,1,2,1,1]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n            print(""loss:"",loss,""acc:"",acc,""label:"",input_y,""prediction:"",predict)\ntest()\n'"
a03_TextRNN/p8_TextRNN_model_multi_layers.py,36,"b'# -*- coding: utf-8 -*-\n#TextRNN: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\n\nclass TextRNN:\n    def __init__(self,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,\n                 vocab_size,embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1)):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.hidden_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.initializer=initializer\n        self.num_sampled=20\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        self.input_y = tf.placeholder(tf.int32,[None], name=""input_y"")  # y [None,num_classes]\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        self.loss_val = self.loss() #-->self.loss_nce()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")  # shape:[None,]\n        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.hidden_size*2, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n    def inference(self):\n        """"""main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer-->dropout,3.LSTM layer-->dropout 4.FC layer 6.softmax layer """"""\n        #1.get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n        \n        #2. Bi-lstm layer\n        # define lstm cess:get lstm cell output\n        lstm_fw_cell=rnn.BasicLSTMCell(self.hidden_size) #forward direction cell\n        lstm_bw_cell=rnn.BasicLSTMCell(self.hidden_size) #backward direction cell\n        if self.dropout_keep_prob is not None:\n            lstm_fw_cell=rnn.DropoutWrapper(lstm_fw_cell,output_keep_prob=self.dropout_keep_prob)\n            lstm_bw_cell=rnn.DropoutWrapper(lstm_bw_cell,output_keep_prob=self.dropout_keep_prob)\n        # bidirectional_dynamic_rnn: input: [batch_size, max_time, input_size]\n        #                            output: A tuple (outputs, output_states)\n        #                                    where:outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output `Tensor`.\n        outputs,_=tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell,self.embedded_words,dtype=tf.float32) #[batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\n        print(""outputs:===>"",outputs) #outputs:(<tf.Tensor \'bidirectional_rnn/fw/fw/transpose:0\' shape=(?, 5, 100) dtype=float32>, <tf.Tensor \'ReverseV2:0\' shape=(?, 5, 100) dtype=float32>))\n        output_rnn=tf.concat(outputs,axis=2) #[batch_size,sequence_length,hidden_size*2]\n\n        #3. Second LSTM layer\n        rnn_cell=rnn.BasicLSTMCell(self.hidden_size*2)\n        if self.dropout_keep_prob is not None:\n            rnn_cell=rnn.DropoutWrapper(rnn_cell,output_keep_prob=self.dropout_keep_prob)\n        _,final_state_c_h=tf.nn.dynamic_rnn(rnn_cell,output_rnn,dtype=tf.float32)\n        final_state=final_state_c_h[1]\n\n        #4 .FC layer\n        output=tf.layers.dense(final_state,self.hidden_size*2,activation=tf.nn.tanh)\n        \n        #5. logits(use linear layer)\n        with tf.name_scope(""output""): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n            logits = tf.matmul(output, self.W_projection) + self.b_projection  # [batch_size,num_classes]\n        return logits\n\n    def loss(self,l2_lambda=0.0001):\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss_nce(self,l2_lambda=0.0001): #0.0001-->0.001\n        """"""calculate loss using (NCE)cross entropy here""""""\n        # Compute the average NCE loss for the batch.\n        # tf.nce_loss automatically draws a new sample of the negative labels each\n        # time we evaluate the loss.\n        if self.is_training: #training\n            #labels=tf.reshape(self.input_y,[-1])               #[batch_size,1]------>[batch_size,]\n            labels=tf.expand_dims(self.input_y,1)                   #[batch_size,]----->[batch_size,1]\n            loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n                tf.nn.nce_loss(weights=tf.transpose(self.W_projection),#[hidden_size*2, num_classes]--->[num_classes,hidden_size*2]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n                               biases=self.b_projection,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n                               labels=labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n                               inputs=self.output_rnn_last,# [batch_size,hidden_size*2] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n                               num_sampled=self.num_sampled,  #scalar. 100\n                               num_classes=self.num_classes,partition_strategy=""div""))  #scalar. 1999\n        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n        loss = loss + l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=10\n    learning_rate=0.001\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.9\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1#0.5\n    textRNN=TextRNN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            input_x=np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n            input_y=input_y=np.array([1,0,1,1,1,2,1,1]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n            print(""loss:"",loss,""acc:"",acc,""label:"",input_y,""prediction:"",predict)\ntest()\n'"
a03_TextRNN/p8_TextRNN_predict.py,19,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p8_TextRNN_model import TextRNN\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 80, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_rnn_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #train-zhihu4-only-title-all.txt.training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec.bin-100"",""word2vec\'s vocabulary and vectors"")\ntf.app.flags.DEFINE_string(""predict_target_file"",""text_rnn_checkpoint/zhihu_result_rnn5.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-v4only-title.txt\',""target file path for final prediction"")\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""rnn"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""rnn"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textRNN=TextRNN(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\n                        vocab_size, FLAGS.embed_size, FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint for TextRNN"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n       #for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textRNN.logits,feed_dict={textRNN.input_x:testX2[start:end],textRNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            #predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label) #logits[0]\n            # 7. write question id and labels to file system.\n            #write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            #############################################################################################################\n            print(""start:"",start,"";end:"",end)\n            question_id_sublist=question_id_list[start:end]\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\n            ########################################################################################################\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    #print(""get_label_using_logits:"",logits)\n    print(""get_label_using_logits.shape:"", logits.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    print(""get_label_using_logits.label_list"",label_list)\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\n    #print(""get_label_using_logits.shape:"", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\n    for i,logits in enumerate(logits_batch):\n        index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n        index_list=index_list[::-1]\n        label_list=[]\n        for index in index_list:\n            label=vocabulary_index2word_label[index]\n            label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        #print(""get_label_using_logits.label_list"",label_list)\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\n    f.flush()\n    #return label_list\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a03_TextRNN/p8_TextRNN_train.py,23,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p8_TextRNN_model import TextRNN\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1024, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_rnn_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",60,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #train-zhihu4-only-title-all.txt===>training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec.bin-100"",""word2vec\'s vocabulary and vectors"")\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        #1.  get vocabulary of X and label.\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""rnn"")\n        vocab_size = len(vocabulary_word2index)\n        print(""rnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""rnn"")\n        train, test, _ =  load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=False,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        ###############################################################################################\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        ###############################################################################################\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textRNN=TextRNN(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\n        vocab_size, FLAGS.embed_size, FLAGS.is_training)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint for rnn model."")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textRNN,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textRNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                curr_loss,curr_acc,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.train_op],feed_dict={textRNN.input_x:trainX[start:end],textRNN.input_y:trainY[start:end]\n                    ,textRNN.dropout_keep_prob:1}) #curr_acc--->TextCNN.accuracy -->,textRNN.dropout_keep_prob:1\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %500==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textRNN.epoch_increment)\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,textRNN,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, textRNN, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textRNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textRNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textRNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textRNN.loss_val,textRNN.logits,textRNN.accuracy],#curr_eval_acc--->textCNN.accuracy\n                                          feed_dict={textRNN.input_x: evalX[start:end],textRNN.input_y: evalY[start:end]\n                                              ,textRNN.dropout_keep_prob:1})\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a04_TextRCNN/p71_TextRCNN_mode2.py,65,"b'# -*- coding: utf-8 -*-\n#TextRNN: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\nimport tensorflow as tf\nimport numpy as np\nimport copy\nclass TextRCNN:\n    def __init__(self,num_classes, learning_rate, decay_steps, decay_rate,sequence_length,\n                 vocab_size,embed_size,is_training,batch_size,initializer=tf.random_normal_initializer(stddev=0.1),multi_label_flag=False):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.hidden_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.initializer=initializer\n        self.activation=tf.nn.relu #TODO tf.nn.tanh\n        self.multi_label_flag=multi_label_flag\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        self.input_y = tf.placeholder(tf.int32, [None,], name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32, [None, self.num_classes], name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n        #self.batch_size=tf.placeholder(tf.int32,name=""batch_size"")\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        if multi_label_flag:\n            print(""going to use multi label loss."")\n            self.loss_val = self.loss_multilabel()\n        else:\n            print(""going to use single label loss."")\n            self.loss_val = self.loss()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")  # shape:[None,]\n        if not self.multi_label_flag:\n            correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy =tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n        else:\n            self.accuracy = tf.constant(0.5) #fake accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""weights""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n\n            self.left_side_first_word= tf.get_variable(""left_side_first_word"",shape=[self.batch_size, self.embed_size],initializer=self.initializer) #TODO removed. replaced with zero vector\n            self.right_side_last_word = tf.get_variable(""right_side_last_word"",shape=[self.batch_size, self.embed_size],initializer=self.initializer) #TODO removed. replaced with zero vector\n            #self.left_side_context_first= tf.get_variable(""left_side_context_first"",shape=[self.batch_size, self.embed_size],initializer=self.initializer) #TODO removed. replaced with zero vector\n            #self.right_side_context_last=tf.get_variable(""right_side_context_last"",shape=[self.batch_size, self.embed_size],initializer=self.initializer) #TODO removed. replaced with zero vector\n\n            self.W_l=tf.get_variable(""W_l"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_r=tf.get_variable(""W_r"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_sl=tf.get_variable(""W_sl"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_sr=tf.get_variable(""W_sr"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n\n            self.b = tf.get_variable(""b"", [self.embed_size])\n\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.hidden_size*3, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n        #b = tf.get_variable(""b"", [self.embed_size*3])\n        #h = tf.nn.relu(tf.nn.bias_add(output_conv, b), ""relu"")\n    def get_context_left(self,context_left,embedding_previous):\n        """"""\n        :param context_left:\n        :param embedding_previous:\n        :return: output:[None,embed_size]\n        """"""\n        left_c=tf.matmul(context_left,self.W_l)       #shape:[batch_size,embed_size]<---------context_left:[batch_size,embed_size];W_l:[embed_size,embed_size]\n        left_e=tf.matmul(embedding_previous,self.W_sl)#shape:[batch_size,embed_size]<---------embedding_previous;[batch_size,embed_size];W_sl:[embed_size, embed_size]\n        left_h=left_c+left_e #shape:[batch_size,embed_size]\n        #context_left=self.activation(left_h) #shape:[batch_size,embed_size] #TODO\n        context_left = tf.nn.relu(tf.nn.bias_add(left_h, self.b), ""relu"") #TODO\n        return context_left #shape:[batch_size,embed_size]\n\n    def get_context_right(self,context_right,embedding_afterward):\n        """"""\n        :param context_right:\n        :param embedding_afterward:\n        :return: output:[None,embed_size]\n        """"""\n        right_c=tf.matmul(context_right,self.W_r)        #shape:[batch_size,embed_size]<---------context_right:[batch_size,embed_size];W_r:[embed_size,embed_size]\n        right_e=tf.matmul(embedding_afterward,self.W_sr) #shape:[batch_size,embed_size]<----------embedding_afterward:[batch_size,embed_size];W_sr:[embed_size,embed_size]\n        right_h=right_c+right_e #shape:[batch_size,embed_size]\n        #context_right=self.activation(right_h) #shape:[batch_size,embed_size] #TODO\n        context_right = tf.nn.relu(tf.nn.bias_add(right_h, self.b), ""relu"") #TODO\n        return context_right #shape:[batch_size,embed_size]\n\n    def conv_layer_with_recurrent_structure(self):\n        """"""\n        input:self.embedded_words:[None,sentence_length,embed_size]\n        :return: shape:[None,sentence_length,embed_size*3]\n        """"""\n        #1. get splitted list of word embeddings\n        embedded_words_split=tf.split(self.embedded_words,self.sequence_length,axis=1) #sentence_length\xe4\xb8\xaa[None,1,embed_size]\n        embedded_words_squeezed=[tf.squeeze(x,axis=1) for x in embedded_words_split]#sentence_length\xe4\xb8\xaa[None,embed_size]\n        embedding_previous=self.left_side_first_word #tf.zeros((self.batch_size,self.embed_size)) #TODO SHOULD WE ASSIGN A VARIABLE HERE\n        context_left_previous=tf.zeros((self.batch_size,self.embed_size)) #self.left_side_context_first# TODO SHOULD WE ASSIGN A VARIABLE HERE\n\n        #2. get list of context left\n        context_left_list=[]\n        for i,current_embedding_word in enumerate(embedded_words_squeezed):#sentence_length\xe4\xb8\xaa[None,embed_size]\n            context_left=self.get_context_left(context_left_previous, embedding_previous) #[None,embed_size]\n            context_left_list.append(context_left) #append result to list\n            embedding_previous=current_embedding_word #assign embedding_previous\n            context_left_previous=context_left #assign context_left_previous\n\n        #3. get context right\n        embedded_words_squeezed2=copy.copy(embedded_words_squeezed)\n        embedded_words_squeezed2.reverse()\n        embedding_afterward=self.right_side_last_word #tf.zeros((self.batch_size,self.embed_size)) # TODO self.right_side_last_word SHOULD WE ASSIGN A VARIABLE HERE\n        context_right_afterward = tf.zeros((self.batch_size, self.embed_size)) #self.right_side_context_last # TODO SHOULD WE ASSIGN A VARIABLE HERE\n        context_right_list=[]\n        for j,current_embedding_word in enumerate(embedded_words_squeezed2):\n            context_right=self.get_context_right(context_right_afterward,embedding_afterward)\n            context_right_list.append(context_right)\n            embedding_afterward=current_embedding_word\n            context_right_afterward=context_right\n\n        #4.ensemble ""left,embedding,right"" to output\n        output_list=[]\n        for index,current_embedding_word in enumerate(embedded_words_squeezed):\n            representation=tf.concat([context_left_list[index],current_embedding_word,context_right_list[index]],axis=1) #representation\'s shape:[None,embed_size*3]\n            output_list.append(representation) #shape:sentence_length\xe4\xb8\xaa[None,embed_size*3]\n\n        #5. stack list to a tensor\n        output=tf.stack(output_list,axis=1) #shape:[None,sentence_length,embed_size*3]\n        return output\n\n\n    def inference(self):\n        """"""main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer, 3.max pooling, 4.FC layer 5.softmax """"""\n        #1.get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n        #2. Bi-lstm layer\n        output_conv=self.conv_layer_with_recurrent_structure() #shape:[None,sentence_length,embed_size*3]\n        #2.1 apply nolinearity\n        #b = tf.get_variable(""b"", [self.embed_size*3])\n        #h = tf.nn.relu(tf.nn.bias_add(output_conv, b), ""relu"")\n\n        #3. max pooling\n        output_pooling=tf.reduce_max(output_conv,axis=1) #shape:[None,embed_size*3]\n        #4. logits(use linear layer)\n        with tf.name_scope(""dropout""):\n            h_drop=tf.nn.dropout(output_pooling,keep_prob=self.dropout_keep_prob) #[None,embed_size*3]\n\n        with tf.name_scope(""output""): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n            logits = tf.matmul(h_drop, self.W_projection) + self.b_projection  #shape:[batch_size,num_classes]<-----h_drop:[None,embed_size*3];b_projection:[hidden_size*3, self.num_classes]\n        return logits\n\n    def loss(self,l2_lambda=0.0001):#0.001\n        with tf.name_scope(""loss""):\n            #input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss_multilabel(self,l2_lambda=0.00001): #0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            #input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            #losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"",losses) #shape=(?, 1999).\n            losses=tf.reduce_sum(losses,axis=1) #shape=(?,). loss for all data in the batch\n            loss=tf.reduce_mean(losses)         #shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=10\n    learning_rate=0.01\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.9\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1#0.5\n    textRNN=TextRCNN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            input_x=np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n            input_y=input_y=np.array([1,0,1,1,1,2,1,1]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],\n                                        feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n            print(""loss:"",loss,""acc:"",acc,""label:"",input_y,""prediction:"",predict)\n#test()'"
a04_TextRCNN/p71_TextRCNN_model.py,53,"b'# -*- coding: utf-8 -*-\n#TextRNN: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\nimport tensorflow as tf\nimport numpy as np\nimport copy\nclass TextRCNN:\n    def __init__(self,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,\n                 vocab_size,embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1),multi_label_flag=False):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.hidden_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.initializer=initializer\n        self.activation=tf.nn.tanh\n        self.multi_label_flag=multi_label_flag\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        self.input_y = tf.placeholder(tf.int32, [None,], name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32, [None, self.num_classes], name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        if multi_label_flag:\n            print(""going to use multi label loss."")\n            self.loss_val = self.loss_multilabel()\n        else:\n            print(""going to use single label loss."")\n            self.loss_val = self.loss()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")  # shape:[None,]\n        if not self.multi_label_flag:\n            correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy =tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n        else:\n            self.accuracy = tf.constant(0.5) #fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""weights""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n\n            self.left_side_first_word= tf.get_variable(""left_side_first_word"",shape=[self.batch_size, self.embed_size],initializer=self.initializer) #TODO\n            self.right_side_last_word = tf.get_variable(""right_side_last_word"",shape=[self.batch_size, self.embed_size],initializer=self.initializer) #TODO\n            self.W_l=tf.get_variable(""W_l"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_r=tf.get_variable(""W_r"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_sl=tf.get_variable(""W_sl"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_sr=tf.get_variable(""W_sr"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.hidden_size*3, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n    def get_context_left(self,context_left,embedding_previous):\n        """"""\n        :param context_left:\n        :param embedding_previous:\n        :return: output:[None,embed_size]\n        """"""\n        left_c=tf.matmul(context_left,self.W_l) #context_left:[batch_size,embed_size];W_l:[embed_size,embed_size]\n        left_e=tf.matmul(embedding_previous,self.W_sl)#embedding_previous;[batch_size,embed_size]\n        left_h=left_c+left_e\n        context_left=self.activation(left_h)\n        return context_left\n\n    def get_context_right(self,context_right,embedding_afterward):\n        """"""\n        :param context_right:\n        :param embedding_afterward:\n        :return: output:[None,embed_size]\n        """"""\n        right_c=tf.matmul(context_right,self.W_r)\n        right_e=tf.matmul(embedding_afterward,self.W_sr)\n        right_h=right_c+right_e\n        context_right=self.activation(right_h)\n        return context_right\n\n    def conv_layer_with_recurrent_structure(self):\n        """"""\n        input:self.embedded_words:[None,sentence_length,embed_size]\n        :return: shape:[None,sentence_length,embed_size*3]\n        """"""\n        #1. get splitted list of word embeddings\n        embedded_words_split=tf.split(self.embedded_words,self.sequence_length,axis=1) #sentence_length\xe4\xb8\xaa[None,1,embed_size]\n        embedded_words_squeezed=[tf.squeeze(x,axis=1) for x in embedded_words_split]#sentence_length\xe4\xb8\xaa[None,embed_size]\n        embedding_previous=self.left_side_first_word\n        context_left_previous=tf.zeros((self.batch_size,self.embed_size))\n        #2. get list of context left\n        context_left_list=[]\n        for i,current_embedding_word in enumerate(embedded_words_squeezed):#sentence_length\xe4\xb8\xaa[None,embed_size]\n            context_left=self.get_context_left(context_left_previous, embedding_previous) #[None,embed_size]\n            context_left_list.append(context_left) #append result to list\n            embedding_previous=current_embedding_word #assign embedding_previous\n            context_left_previous=context_left #assign context_left_previous\n        #3. get context right\n        embedded_words_squeezed2=copy.copy(embedded_words_squeezed)\n        embedded_words_squeezed2.reverse()\n        embedding_afterward=self.right_side_last_word\n        context_right_afterward = tf.zeros((self.batch_size, self.embed_size))\n        context_right_list=[]\n        for j,current_embedding_word in enumerate(embedded_words_squeezed2):\n            context_right=self.get_context_right(context_right_afterward,embedding_afterward)\n            context_right_list.append(context_right)\n            embedding_afterward=current_embedding_word\n            context_right_afterward=context_right\n        #4.ensemble left,embedding,right to output\n        output_list=[]\n        for index,current_embedding_word in enumerate(embedded_words_squeezed):\n            representation=tf.concat([context_left_list[index],current_embedding_word,context_right_list[index]],axis=1)\n            #print(i,""representation:"",representation)\n            output_list.append(representation) #shape:sentence_length\xe4\xb8\xaa[None,embed_size*3]\n        #5. stack list to a tensor\n        #print(""output_list:"",output_list) #(3, 5, 8, 100)\n        output=tf.stack(output_list,axis=1) #shape:[None,sentence_length,embed_size*3]\n        #print(""output:"",output)\n        return output\n\n\n    def inference(self):\n        """"""main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer, 3.max pooling, 4.FC layer 5.softmax """"""\n        #1.get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n        #2. Bi-lstm layer\n        output_conv=self.conv_layer_with_recurrent_structure() #shape:[None,sentence_length,embed_size*3]\n        #3. max pooling\n        #print(""output_conv:"",output_conv) #(3, 5, 8, 100)\n        output_pooling=tf.reduce_max(output_conv,axis=1) #shape:[None,embed_size*3]\n        #print(""output_pooling:"",output_pooling) #(3, 8, 100)\n        #4. logits(use linear layer)\n        with tf.name_scope(""dropout""):\n            h_drop=tf.nn.dropout(output_pooling,keep_prob=self.dropout_keep_prob) #[None,num_filters_total]\n\n        with tf.name_scope(""output""): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n            logits = tf.matmul(h_drop, self.W_projection) + self.b_projection  # [batch_size,num_classes]\n        return logits\n\n    def loss(self,l2_lambda=0.0001):#0.001\n        with tf.name_scope(""loss""):\n            #input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss_multilabel(self,l2_lambda=0.00001): #0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            #input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            #losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"",losses) #shape=(?, 1999).\n            losses=tf.reduce_sum(losses,axis=1) #shape=(?,). loss for all data in the batch\n            loss=tf.reduce_mean(losses)         #shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=10\n    learning_rate=0.01\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.9\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1#0.5\n    textRNN=TextRCNN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            input_x=np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n            input_y=input_y=np.array([1,0,1,1,1,2,1,1]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],\n                                        feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n            print(""loss:"",loss,""acc:"",acc,""label:"",input_y,""prediction:"",predict)\n#test()'"
a04_TextRCNN/p71_TextRCNN_predict.py,19,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p71_TextRCNN_mode2 import TextRCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 80, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_rcnn_title_desc_checkpoint2/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_string(""predict_target_file"",""text_rcnn_title_desc_checkpoint2/zhihu_result_rcnn_multilabel.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n# 1.load data with vocabulary of words and labels\n\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""rcnn"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""rcnn"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_length, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textRCNN=TextRCNN(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sentence_length,\n                 vocab_size,FLAGS.embed_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir)) #TODO\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textRCNN.logits,feed_dict={textRCNN.input_x:testX2[start:end],textRCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            #predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            #write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n\n            question_id_sublist=question_id_list[start:end]\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\n\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\n# get label using logits\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\n    #print(""get_label_using_logits.shape:"", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\n    for i,logits in enumerate(logits_batch):\n        index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n        index_list=index_list[::-1]\n        label_list=[]\n        for index in index_list:\n            label=vocabulary_index2word_label[index]\n            label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        #print(""get_label_using_logits.label_list"",label_list)\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\n    f.flush()\n    #return label_list\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a04_TextRCNN/p71_TextRCNN_train.py,25,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p71_TextRCNN_mode2 import TextRCNN\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_rcnn_title_desc_checkpoint2/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",60,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""rcnn"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""cnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""rcnn"")\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #test-zhihu5-only-title-multilabel.txt\n        train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textRCNN=TextRCNN(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sequence_length,\n                 vocab_size,FLAGS.embed_size,FLAGS.is_training,FLAGS.batch_size,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textRCNN,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textRCNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {textRCNN.input_x: trainX[start:end],textRCNN.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[textRCNN.input_y] = trainY[start:end]\n                else:\n                    feed_dict[textRCNN.input_y_multilabel]=trainY[start:end]\n                curr_loss,curr_acc,_=sess.run([textRCNN.loss_val,textRCNN.accuracy,textRCNN.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %50==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textRCNN.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,textRCNN,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, textRCNN, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textRCNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textRCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textCNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[textCNN.input_y] = evalY[start:end]\n        else:\n            feed_dict[textCNN.input_y_multilabel] = evalY[start:end]\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textCNN.loss_val,textCNN.logits,textCNN.accuracy],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a05_HierarchicalAttentionNetwork/HAN_model.py,59,"b'# -*- coding: utf-8 -*-\n# HierarchicalAttention: 1.Word Encoder. 2.Word Attention. 3.Sentence Encoder 4.Sentence Attention 5.linear classifier. 2017-06-13\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.contrib as tf_contrib\nfrom tensorflow.contrib import rnn\n\nclass HierarchicalAttention:\n    def __init__(self,  accusation_num_classes,article_num_classes, deathpenalty_num_classes,lifeimprisonment_num_classes,learning_rate,\n                        batch_size, decay_steps, decay_rate, sequence_length, num_sentences,vocab_size, embed_size,hidden_size, is_training,\n                        initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,max_pooling_style=\'max_pooling\'):#0.01\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.accusation_num_classes = accusation_num_classes\n        self.article_num_classes=article_num_classes\n        self.deathpenalty_num_classes=deathpenalty_num_classes\n        self.lifeimprisonment_num_classes=lifeimprisonment_num_classes\n        self.batch_size = batch_size\n        self.total_sequence_length = sequence_length\n        self.num_sentences = num_sentences\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.is_training = is_training\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")#TODO ADD learning_rate\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\n        self.initializer = initializer\n        self.hidden_size = hidden_size\n        self.clip_gradients=clip_gradients\n        self.max_pooling_style=max_pooling_style\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.total_sequence_length], name=""input_x"")\n        self.input_y_accusation=tf.placeholder(tf.float32, [None, self.accusation_num_classes],name=""input_y_accusation"")\n        self.input_y_article = tf.placeholder(tf.float32, [None, self.article_num_classes],name=""input_y_article"")\n        self.input_y_deathpenalty = tf.placeholder(tf.float32, [None, self.deathpenalty_num_classes], name=""input_y_deathpenalty"")\n        self.input_y_lifeimprisonment = tf.placeholder(tf.float32, [None, self.lifeimprisonment_num_classes], name=""input_y_lifeimprisonment"")\n        self.input_y_imprisonment = tf.placeholder(tf.float32, [None], name=""input_y_imprisonment"")\n\n        self.sequence_length = int(self.total_sequence_length / self.num_sentences)\n        print(""self.single_sequence_length:"",self.sequence_length,"";self.total_sequence_length:"",self.total_sequence_length,"";self.num_sentences:"",self.num_sentences)\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits_accusation,self.logits_article,self.logits_deathpenalty,self.logits_lifeimprisonment,self.logits_imprisonment = self.inference()  # [None, self.label_size]. main computation graph is here.\n\n        if not is_training:\n            return\n        self.loss_val = self.loss()\n        self.train_op = self.train()\n\n    def inference(self):\n        """"""main computation graph here: 1.Word Encoder. 2.Word Attention. 3.Sentence Encoder 4.Sentence Attention 5.dropout 6.transform for each task 7.linear classifier""""""\n        # 1.Word Encoder: use bilstm to encoder the sentence.\n        embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x)  # [None,num_sentences,sentence_length,embed_size]\n        embedded_words=[tf.squeeze(x) for x in tf.split(embedded_words,self.num_sentences,axis=1)] #a list.length is num_sentence, each element is:[None,sentence_length,embed_size]\n        word_attention_list=[]\n        for i in range(self.num_sentences):\n            sentence=embedded_words[i]\n            #sentence is:[batch_size,seqence_length,embed_size]\n            resue_flag=True if i>0 else False\n            # 2. word encoder\n            num_units1=self.embed_size\n            sentence=tf.reshape(sentence,(-1,self.sequence_length,num_units1))\n            word_encoded=self.bi_lstm(sentence, \'word_level\', num_units1,reuse_flag=resue_flag) #[batch_size,seq_length,num_units*2]\n            # 3. word attention\n            word_attention=self.attention( word_encoded, \'word_level\', reuse_flag=resue_flag)  #[batch_size,num_units*2]\n            word_attention_list.append(word_attention)\n        sentence_encoder_input=tf.stack(word_attention_list,axis=1) #[batch_size,num_sentence,num_units*2]\n        # 4. sentence encoder\n        num_units2 = self.hidden_size*2\n        sentence_encoder_input=tf.reshape(sentence_encoder_input,(-1,self.num_sentences,num_units2))\n        sentence_encoded = self.bi_lstm(sentence_encoder_input, \'sentence_level\',num_units2)  # [batch_size,seq_length,num_units*4]\n        # 5. sentence attention\n        document_representation=self.attention(sentence_encoded,\'sentence_level\') # [batch_size,num_units*4]\n        # 6. dropout\n        h = tf.nn.dropout(document_representation,keep_prob=self.dropout_keep_prob)  # [batch_size,num_units*4]\n\n        # 7. transoform each sub task using one-layer MLP ,then get logits\n        # train_Y_accusation, train_Y_article, train_Y_deathpenalty, train_Y_lifeimprisonment, train_Y_imprisonment\n        h_accusation = tf.layers.dense(h, self.hidden_size, activation=tf.nn.relu, use_bias=True)\n        logits_accusation = tf.layers.dense(h_accusation, self.accusation_num_classes,use_bias=True)  # shape:[None,self.num_classes]\n\n        h_article = tf.layers.dense(h, self.hidden_size, activation=tf.nn.relu, use_bias=True)\n        logits_article = tf.layers.dense(h_article, self.article_num_classes,use_bias=True)  # shape:[None,self.num_classes]\n\n        h_deathpenalty = tf.layers.dense(h, self.hidden_size, activation=tf.nn.relu, use_bias=True)\n        logits_deathpenalty = tf.layers.dense(h_deathpenalty,self.deathpenalty_num_classes,use_bias=True)  # shape:[None,self.num_classes]\n\n        h_lifeimprisonment = tf.layers.dense(h, self.hidden_size, activation=tf.nn.relu, use_bias=True)\n        logits_lifeimprisonment = tf.layers.dense(h_lifeimprisonment, self.lifeimprisonment_num_classes,use_bias=True)  # shape:[None,self.num_classes]\n        print(""logits_lifeimprisonment:"",logits_lifeimprisonment)\n        logits_imprisonment = tf.layers.dense(h, 1,use_bias=True)  # imprisonment is a continuous value, no need to use activation function\n        logits_imprisonment = tf.reshape(logits_imprisonment, [-1]) #[batch_size]\n\n        return logits_accusation, logits_article, logits_deathpenalty, logits_lifeimprisonment, logits_imprisonment\n\n    def attention(self,input_sequences,attention_level,reuse_flag=False):\n        """"""\n        :param input_sequence: [batch_size,seq_length,num_units]\n        :param attention_level: word or sentence level\n        :return: [batch_size,hidden_size]\n        """"""\n        num_units=input_sequences.get_shape().as_list()[-1] #get last dimension\n        with tf.variable_scope(""attention_"" + str(attention_level),reuse=reuse_flag):\n            v_attention = tf.get_variable(""u_attention"" + attention_level, shape=[num_units],initializer=self.initializer)\n            #1.one-layer MLP\n            u=tf.layers.dense(input_sequences,num_units,activation=tf.nn.tanh,use_bias=True) #[batch_size,seq_legnth,num_units].no-linear\n            #2.compute weight by compute simility of u and attention vector v\n            score=tf.multiply(u,v_attention) #[batch_size,seq_length,num_units]\n            weight=tf.reduce_sum(score,axis=2,keepdims=True) #[batch_size,seq_length,1]\n            #3.weight sum\n            attention_representation=tf.reduce_sum(tf.multiply(u,weight),axis=1) #[batch_size,num_units]\n        return attention_representation\n\n    def bi_lstm(self, input_sequences, level,num_units, reuse_flag=False):\n        """"""\n        :param input_sequences: [batch_size,seq_lenght,num_units]\n        :param level: word or sentence\n        :param reuse_flag: resuse or not\n        :return: encoded representation:[batch_size,seq_lenght,num_units*2]\n        """"""\n        #num_units=input_sequences.get_shape().as_list()[-1] #get last dimension\n        with tf.variable_scope(""bi_lstm_"" + str(level), reuse=reuse_flag):\n            lstm_fw_cell = rnn.BasicLSTMCell(num_units)  # forward direction cell\n            lstm_bw_cell = rnn.BasicLSTMCell(num_units)  # backward direction cell\n            outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, input_sequences,dtype=tf.float32)  # [batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\n        #concat output\n        encdoded_inputs = tf.concat(outputs, axis=2)  #[batch_size,sequence_length,hidden_size*2]\n        return encdoded_inputs  #[batch_size,sequence_length,num_units*2]\n\n    def loss(self,l2_lambda=0.0001):\n        # input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n        # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n        # input_y:shape=(?, 1999); logits:shape=(?, 1999)\n        # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n        # losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n        #loss1: accusation\n        losses_accusation = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_accusation,logits=self.logits_accusation)\n        loss_accusation = tf.reduce_mean((tf.reduce_sum(losses_accusation, axis=1)))# shape=(?,)-->(). loss for all data in the batch-->single loss\n        #loss2:relevant article\n        losses_article = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_article,logits=self.logits_article)\n        loss_article = tf.reduce_mean((tf.reduce_sum(losses_article, axis=1))) # shape=(?,)-->(). loss for all data in the batch-->single loss\n        #loss3:death penalty\n        losses_deathpenalty = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_deathpenalty,logits=self.logits_deathpenalty)\n        loss_deathpenalty = tf.reduce_mean((tf.reduce_sum(losses_deathpenalty, axis=1))) # shape=(?,)-->(). loss for all data in the batch-->single loss\n        #loss4:life imprisonment\n        losses_lifeimprisonment = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_lifeimprisonment,logits=self.logits_lifeimprisonment)\n        loss_lifeimprisonment = tf.reduce_mean((tf.reduce_sum(losses_lifeimprisonment, axis=1))) # shape=(?,)-->(). loss for all data in the batch-->single loss\n        #loss5: imprisonment: how many year in prison.\n        #print(""====>logits_imprisonment.shape:"",self.logits_imprisonment.shape,"";self.input_y_imprisonment"",self.input_y_imprisonment)\n        loss_imprisonment=tf.reduce_sum(tf.pow((self.logits_imprisonment - self.input_y_imprisonment), 2))/50.0\n\n        print(""sigmoid_cross_entropy_with_logits.losses:"", losses_accusation)  # shape=(?, 1999).\n        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n        #set max weight of accusation and article(each is 1/3);others share the result of weights\n        self.weights_accusation = tf.nn.sigmoid(tf.cast(self.global_step / 1000, dtype=tf.float32)) / 3.0    # 0--1/3\n        self.weights_article = tf.nn.sigmoid(tf.cast(self.global_step / 1000, dtype=tf.float32)) / 3.0       # 0--1/3\n        self.weights_deathpenalty = tf.nn.sigmoid(tf.cast(self.global_step / 1000, dtype=tf.float32)) / 9.0   #0--1/9\n        self.weights_lifeimprisonment = tf.nn.sigmoid(tf.cast(self.global_step / 1000, dtype=tf.float32)) / 9.0 #0--1/9\n        self.weights_imprisonment=1-(self.weights_accusation+self.weights_article+self.weights_deathpenalty+self.weights_lifeimprisonment) #0-1/9\n        loss = self.weights_accusation*loss_accusation+self.weights_article*loss_article+self.weights_deathpenalty*loss_deathpenalty +\\\n               self.weights_lifeimprisonment*loss_lifeimprisonment+self.weights_imprisonment*loss_imprisonment+l2_lambda*l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\n                                                   self.decay_rate, staircase=True)\n        self.learning_rate_=learning_rate\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\n        train_op = tf_contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\n        return train_op\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding_projection""):  # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n\n\n# test started\ndef test():\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes = 3\n    learning_rate = 0.01\n    batch_size = 8\n    decay_steps = 1000\n    decay_rate = 0.9\n    sequence_length = 30\n    num_sentences = 6  # number of sentences\n    vocab_size = 10000\n    embed_size = 100 #100\n    hidden_size = 100\n    is_training = True\n    dropout_keep_prob = 1  # 0.5 #num_sentences\n    textRNN = HierarchicalAttention(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\n                                    num_sentences, vocab_size, embed_size,\n                                    hidden_size, is_training,multi_label_flag=False)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            # input_x should be:[batch_size, num_sentences,self.sequence_length]\n            input_x = np.zeros((batch_size, sequence_length)) #num_sentences\n            input_x[input_x > 0.5] = 1\n            input_x[input_x <= 0.5] = 0\n            input_y = np.array(\n                [1, 0, 1, 1, 1, 2, 1, 1])  # np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss, acc, predict, W_projection_value, _ = sess.run(\n                [textRNN.loss_val, textRNN.accuracy, textRNN.predictions, textRNN.W_projection, textRNN.train_op],\n                feed_dict={textRNN.input_x: input_x, textRNN.input_y: input_y,\n                           textRNN.dropout_keep_prob: dropout_keep_prob})\n            print(""loss:"", loss, ""acc:"", acc, ""label:"", input_y, ""prediction:"", predict)\n            # print(""W_projection_value_:"",W_projection_value)\n#test()'"
a05_HierarchicalAttentionNetwork/p1_HierarchicalAttention_model.py,125,"b'# -*- coding: utf-8 -*-\n# HierarchicalAttention: 1.Word Encoder. 2.Word Attention. 3.Sentence Encoder 4.Sentence Attention 5.linear classifier. 2017-06-13\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.contrib as tf_contrib\n\nclass HierarchicalAttention:\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length, num_sentences,\n                 vocab_size, embed_size,\n                 hidden_size, is_training, need_sentence_level_attention_encoder_flag=True, multi_label_flag=False,\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0):#0.01\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length = sequence_length\n        self.num_sentences = num_sentences\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.is_training = is_training\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")#TODO ADD learning_rate\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\n        self.initializer = initializer\n        self.multi_label_flag = multi_label_flag\n        self.hidden_size = hidden_size\n        self.need_sentence_level_attention_encoder_flag = need_sentence_level_attention_encoder_flag\n        self.clip_gradients=clip_gradients\n\n        # add placeholder (X,label)\n        # self.input_x = tf.placeholder(tf.int32, [None, self.num_sentences,self.sequence_length], name=""input_x"")  # X\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")\n\n        self.sequence_length = int(self.sequence_length / self.num_sentences) # TODO\n        self.input_y = tf.placeholder(tf.int32, [None, ], name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32, [None, self.num_classes],name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference()  # [None, self.label_size]. main computation graph is here.\n\n        self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\n        if not self.multi_label_flag:\n            correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32),\n                                          self.input_y)  # tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"")  # shape=()\n        else:\n            self.accuracy = tf.constant(\n                0.5)  # fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n\n        if not is_training:\n            return\n        if multi_label_flag:\n            print(""going to use multi label loss."")\n            self.loss_val = self.loss_multilabel()\n        else:\n            print(""going to use single label loss."")\n            self.loss_val = self.loss()\n        self.train_op = self.train()\n\n    def attention_word_level(self, hidden_state):\n        """"""\n        input1:self.hidden_state: hidden_state:list,len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n        input2:sentence level context vector:[batch_size*num_sentences,hidden_size*2]\n        :return:representation.shape:[batch_size*num_sentences,hidden_size*2]\n        """"""\n        hidden_state_ = tf.stack(hidden_state, axis=1)  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        # 0) one layer of feed forward network\n        hidden_state_2 = tf.reshape(hidden_state_, shape=[-1,\n                                                          self.hidden_size * 2])  # shape:[batch_size*num_sentences*sequence_length,hidden_size*2]\n        # hidden_state_:[batch_size*num_sentences*sequence_length,hidden_size*2];W_w_attention_sentence:[,hidden_size*2,,hidden_size*2]\n        hidden_representation = tf.nn.tanh(tf.matmul(hidden_state_2,\n                                                     self.W_w_attention_word) + self.W_b_attention_word)  # shape:[batch_size*num_sentences*sequence_length,hidden_size*2]\n        hidden_representation = tf.reshape(hidden_representation, shape=[-1, self.sequence_length,\n                                                                         self.hidden_size * 2])  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        # attention process:1.get logits for each word in the sentence. 2.get possibility distribution for each word in the sentence. 3.get weighted sum for the sentence as sentence representation.\n        # 1) get logits for each word in the sentence.\n        hidden_state_context_similiarity = tf.multiply(hidden_representation,\n                                                       self.context_vecotor_word)  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        attention_logits = tf.reduce_sum(hidden_state_context_similiarity,\n                                         axis=2)  # shape:[batch_size*num_sentences,sequence_length]\n        # subtract max for numerical stability (softmax is shift invariant). tf.reduce_max:Computes the maximum of elements across dimensions of a tensor.\n        attention_logits_max = tf.reduce_max(attention_logits, axis=1,\n                                             keep_dims=True)  # shape:[batch_size*num_sentences,1]\n        # 2) get possibility distribution for each word in the sentence.\n        p_attention = tf.nn.softmax(\n            attention_logits - attention_logits_max)  # shape:[batch_size*num_sentences,sequence_length]\n        # 3) get weighted hidden state by attention vector\n        p_attention_expanded = tf.expand_dims(p_attention, axis=2)  # shape:[batch_size*num_sentences,sequence_length,1]\n        # below sentence_representation\'shape:[batch_size*num_sentences,sequence_length,hidden_size*2]<----p_attention_expanded:[batch_size*num_sentences,sequence_length,1];hidden_state_:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        sentence_representation = tf.multiply(p_attention_expanded,\n                                              hidden_state_)  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        sentence_representation = tf.reduce_sum(sentence_representation,\n                                                axis=1)  # shape:[batch_size*num_sentences,hidden_size*2]\n        return sentence_representation  # shape:[batch_size*num_sentences,hidden_size*2]\n\n    def attention_sentence_level(self, hidden_state_sentence):\n        """"""\n        input1: hidden_state_sentence: a list,len:num_sentence,element:[None,hidden_size*4]\n        input2: sentence level context vector:[self.hidden_size*2]\n        :return:representation.shape:[None,hidden_size*4]\n        """"""\n        hidden_state_ = tf.stack(hidden_state_sentence, axis=1)  # shape:[None,num_sentence,hidden_size*4]\n\n        # 0) one layer of feed forward\n        hidden_state_2 = tf.reshape(hidden_state_,\n                                    shape=[-1, self.hidden_size * 4])  # [None*num_sentence,hidden_size*4]\n        hidden_representation = tf.nn.tanh(tf.matmul(hidden_state_2,\n                                                     self.W_w_attention_sentence) + self.W_b_attention_sentence)  # shape:[None*num_sentence,hidden_size*2]\n        hidden_representation = tf.reshape(hidden_representation, shape=[-1, self.num_sentences,\n                                                                         self.hidden_size * 2])  # [None,num_sentence,hidden_size*2]\n        # attention process:1.get logits for each sentence in the doc.2.get possibility distribution for each sentence in the doc.3.get weighted sum for the sentences as doc representation.\n        # 1) get logits for each word in the sentence.\n        hidden_state_context_similiarity = tf.multiply(hidden_representation,\n                                                       self.context_vecotor_sentence)  # shape:[None,num_sentence,hidden_size*2]\n        attention_logits = tf.reduce_sum(hidden_state_context_similiarity,\n                                         axis=2)  # shape:[None,num_sentence]. that is get logit for each num_sentence.\n        # subtract max for numerical stability (softmax is shift invariant). tf.reduce_max:computes the maximum of elements across dimensions of a tensor.\n        attention_logits_max = tf.reduce_max(attention_logits, axis=1, keep_dims=True)  # shape:[None,1]\n        # 2) get possibility distribution for each word in the sentence.\n        p_attention = tf.nn.softmax(attention_logits - attention_logits_max)  # shape:[None,num_sentence]\n        # 3) get weighted hidden state by attention vector(sentence level)\n        p_attention_expanded = tf.expand_dims(p_attention, axis=2)  # shape:[None,num_sentence,1]\n        sentence_representation = tf.multiply(p_attention_expanded,\n                                              hidden_state_)  # shape:[None,num_sentence,hidden_size*2]<---p_attention_expanded:[None,num_sentence,1];hidden_state_:[None,num_sentence,hidden_size*2]\n        sentence_representation = tf.reduce_sum(sentence_representation, axis=1)  # shape:[None,hidden_size*2]\n        return sentence_representation  # shape:[None,hidden_size*2]\n\n    def inference(self):\n        """"""main computation graph here: 1.Word Encoder. 2.Word Attention. 3.Sentence Encoder 4.Sentence Attention 5.linear classifier""""""\n        # 1.Word Encoder\n        # 1.1 embedding of words\n        input_x = tf.split(self.input_x, self.num_sentences,axis=1)  # a list. length:num_sentences.each element is:[None,self.sequence_length/num_sentences]\n        input_x = tf.stack(input_x, axis=1)  # shape:[None,self.num_sentences,self.sequence_length/num_sentences]\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,input_x)  # [None,num_sentences,sentence_length,embed_size]\n        embedded_words_reshaped = tf.reshape(self.embedded_words, shape=[-1, self.sequence_length,self.embed_size])  # [batch_size*num_sentences,sentence_length,embed_size]\n        # 1.2 forward gru\n        hidden_state_forward_list = self.gru_forward_word_level(embedded_words_reshaped)  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        # 1.3 backward gru\n        hidden_state_backward_list = self.gru_backward_word_level(embedded_words_reshaped)  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        # 1.4 concat forward hidden state and backward hidden state. hidden_state: a list.len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n        self.hidden_state = [tf.concat([h_forward, h_backward], axis=1) for h_forward, h_backward in\n                             zip(hidden_state_forward_list, hidden_state_backward_list)]  # hidden_state:list,len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n\n        # 2.Word Attention\n        # for each sentence.\n        sentence_representation = self.attention_word_level(self.hidden_state)  # output:[batch_size*num_sentences,hidden_size*2]\n        sentence_representation = tf.reshape(sentence_representation, shape=[-1, self.num_sentences, self.hidden_size * 2])  # shape:[batch_size,num_sentences,hidden_size*2]\n        #with tf.name_scope(""dropout""):#TODO\n        #    sentence_representation = tf.nn.dropout(sentence_representation,keep_prob=self.dropout_keep_prob)  # shape:[None,hidden_size*4]\n\n        # 3.Sentence Encoder\n        # 3.1) forward gru for sentence\n        hidden_state_forward_sentences = self.gru_forward_sentence_level(sentence_representation)  # a list.length is sentence_length, each element is [None,hidden_size]\n        # 3.2) backward gru for sentence\n        hidden_state_backward_sentences = self.gru_backward_sentence_level(sentence_representation)  # a list,length is sentence_length, each element is [None,hidden_size]\n        # 3.3) concat forward hidden state and backward hidden state\n        # below hidden_state_sentence is a list,len:sentence_length,element:[None,hidden_size*2]\n        self.hidden_state_sentence = [tf.concat([h_forward, h_backward], axis=1) for h_forward, h_backward in zip(hidden_state_forward_sentences, hidden_state_backward_sentences)]\n\n        # 4.Sentence Attention\n        document_representation = self.attention_sentence_level(self.hidden_state_sentence)  # shape:[None,hidden_size*4]\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(document_representation,keep_prob=self.dropout_keep_prob)  # shape:[None,hidden_size*4]\n        # 5. logits(use linear layer)and predictions(argmax)\n        with tf.name_scope(""output""):\n            logits = tf.matmul(self.h_drop, self.W_projection) + self.b_projection  # shape:[None,self.num_classes]==tf.matmul([None,hidden_size*2],[hidden_size*2,self.num_classes])\n        return logits\n\n    def loss(self, l2_lambda=0.0001):  # 0.001\n        with tf.name_scope(""loss""):\n            # input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n                                                                    logits=self.logits);  # sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            # print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss = tf.reduce_mean(losses)  # print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n(\n                [tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss = loss + l2_losses\n        return loss\n\n    def loss_multilabel(self, l2_lambda=0.00001*10): #*3#0.00001 #TODO 0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            # input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            # input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel,\n                                                             logits=self.logits);  # losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            # losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"", losses)  # shape=(?, 1999).\n            losses = tf.reduce_sum(losses, axis=1)  # shape=(?,). loss for all data in the batch\n            loss = tf.reduce_mean(losses)  # shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss = loss + l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\n                                                   self.decay_rate, staircase=True)\n        self.learning_rate_=learning_rate\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\n        train_op = tf_contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\n        return train_op\n\n    def gru_single_step_word_level(self, Xt, h_t_minus_1):\n        """"""\n        single step of gru for word level\n        :param Xt: Xt:[batch_size*num_sentences,embed_size]\n        :param h_t_minus_1:[batch_size*num_sentences,embed_size]\n        :return:\n        """"""\n        # update gate: decides how much past information is kept and how much new information is added.\n        z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z) + tf.matmul(h_t_minus_1,\n                                                                self.U_z) + self.b_z)  # z_t:[batch_size*num_sentences,self.hidden_size]\n        # reset gate: controls how much the past state contributes to the candidate state.\n        r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r) + tf.matmul(h_t_minus_1,\n                                                                self.U_r) + self.b_r)  # r_t:[batch_size*num_sentences,self.hidden_size]\n        # candiate state h_t~\n        h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h) +r_t * (tf.matmul(h_t_minus_1, self.U_h)) + self.b_h)  # h_t_candiate:[batch_size*num_sentences,self.hidden_size]\n        # new state: a linear combine of pervious hidden state and the current new state h_t~\n        h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate  # h_t:[batch_size*num_sentences,hidden_size]\n        return h_t\n\n    def gru_single_step_sentence_level(self, Xt,\n                                       h_t_minus_1):  # Xt:[batch_size, hidden_size*2]; h_t:[batch_size, hidden_size*2]\n        """"""\n        single step of gru for sentence level\n        :param Xt:[batch_size, hidden_size*2]\n        :param h_t_minus_1:[batch_size, hidden_size*2]\n        :return:h_t:[batch_size,hidden_size]\n        """"""\n        # update gate: decides how much past information is kept and how much new information is added.\n        z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z_sentence) + tf.matmul(h_t_minus_1,\n                                                                         self.U_z_sentence) + self.b_z_sentence)  # z_t:[batch_size,self.hidden_size]\n        # reset gate: controls how much the past state contributes to the candidate state.\n        r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r_sentence) + tf.matmul(h_t_minus_1,\n                                                                         self.U_r_sentence) + self.b_r_sentence)  # r_t:[batch_size,self.hidden_size]\n        # candiate state h_t~\n        h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h_sentence) + r_t * (\n        tf.matmul(h_t_minus_1, self.U_h_sentence)) + self.b_h_sentence)  # h_t_candiate:[batch_size,self.hidden_size]\n        # new state: a linear combine of pervious hidden state and the current new state h_t~\n        h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate\n        return h_t\n\n    # forward gru for first level: word levels\n    def gru_forward_word_level(self, embedded_words):\n        """"""\n        :param embedded_words:[batch_size*num_sentences,sentence_length,embed_size]\n        :return:forward hidden state: a list.length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        """"""\n        # split embedded_words\n        embedded_words_splitted = tf.split(embedded_words, self.sequence_length,\n                                           axis=1)  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,1,embed_size]\n        embedded_words_squeeze = [tf.squeeze(x, axis=1) for x in\n                                  embedded_words_splitted]  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,embed_size]\n        # demension_1=embedded_words_squeeze[0].get_shape().dims[0]\n        h_t = tf.ones((self.batch_size * self.num_sentences,\n                       self.hidden_size))  #TODO self.hidden_size h_t =int(tf.get_shape(embedded_words_squeeze[0])[0]) # tf.ones([self.batch_size*self.num_sentences, self.hidden_size]) # [batch_size*num_sentences,embed_size]\n        h_t_forward_list = []\n        for time_step, Xt in enumerate(embedded_words_squeeze):  # Xt: [batch_size*num_sentences,embed_size]\n            h_t = self.gru_single_step_word_level(Xt,h_t)  # [batch_size*num_sentences,embed_size]<------Xt:[batch_size*num_sentences,embed_size];h_t:[batch_size*num_sentences,embed_size]\n            h_t_forward_list.append(h_t)\n        return h_t_forward_list  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n\n    # backward gru for first level: word level\n    def gru_backward_word_level(self, embedded_words):\n        """"""\n        :param   embedded_words:[batch_size*num_sentences,sentence_length,embed_size]\n        :return: backward hidden state:a list.length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        """"""\n        # split embedded_words\n        embedded_words_splitted = tf.split(embedded_words, self.sequence_length,\n                                           axis=1)  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,1,embed_size]\n        embedded_words_squeeze = [tf.squeeze(x, axis=1) for x in\n                                  embedded_words_splitted]  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,embed_size]\n        embedded_words_squeeze.reverse()  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,embed_size]\n        # demension_1=int(tf.get_shape(embedded_words_squeeze[0])[0]) #h_t = tf.ones([self.batch_size*self.num_sentences, self.hidden_size])\n        h_t = tf.ones((self.batch_size * self.num_sentences, self.hidden_size))\n        h_t_backward_list = []\n        for time_step, Xt in enumerate(embedded_words_squeeze):\n            h_t = self.gru_single_step_word_level(Xt, h_t)\n            h_t_backward_list.append(h_t)\n        h_t_backward_list.reverse() #ADD 2017.06.14\n        return h_t_backward_list\n\n    # forward gru for second level: sentence level\n    def gru_forward_sentence_level(self, sentence_representation):\n        """"""\n        :param sentence_representation: [batch_size,num_sentences,hidden_size*2]\n        :return:forward hidden state: a list,length is num_sentences, each element is [batch_size,hidden_size]\n        """"""\n        # split embedded_words\n        sentence_representation_splitted = tf.split(sentence_representation, self.num_sentences,\n                                                    axis=1)  # it is a list.length is num_sentences,each element is [batch_size,1,hidden_size*2]\n        sentence_representation_squeeze = [tf.squeeze(x, axis=1) for x in\n                                           sentence_representation_splitted]  # it is a list.length is num_sentences,each element is [batch_size, hidden_size*2]\n        # demension_1 = int(tf.get_shape(sentence_representation_squeeze[0])[0]) #scalar: batch_size\n        h_t = tf.ones((self.batch_size, self.hidden_size * 2))  # TODO\n        h_t_forward_list = []\n        for time_step, Xt in enumerate(sentence_representation_squeeze):  # Xt:[batch_size, hidden_size*2]\n            h_t = self.gru_single_step_sentence_level(Xt,\n                                                      h_t)  # h_t:[batch_size,hidden_size]<---------Xt:[batch_size, hidden_size*2]; h_t:[batch_size, hidden_size*2]\n            h_t_forward_list.append(h_t)\n        return h_t_forward_list  # a list,length is num_sentences, each element is [batch_size,hidden_size]\n\n    # backward gru for second level: sentence level\n    def gru_backward_sentence_level(self, sentence_representation):\n        """"""\n        :param sentence_representation: [batch_size,num_sentences,hidden_size*2]\n        :return:forward hidden state: a list,length is num_sentences, each element is [batch_size,hidden_size]\n        """"""\n        # split embedded_words\n        sentence_representation_splitted = tf.split(sentence_representation, self.num_sentences,\n                                                    axis=1)  # it is a list.length is num_sentences,each element is [batch_size,1,hidden_size*2]\n        sentence_representation_squeeze = [tf.squeeze(x, axis=1) for x in\n                                           sentence_representation_splitted]  # it is a list.length is num_sentences,each element is [batch_size, hidden_size*2]\n        sentence_representation_squeeze.reverse()\n        # demension_1 = int(tf.get_shape(sentence_representation_squeeze[0])[0])  # scalar: batch_size\n        h_t = tf.ones((self.batch_size, self.hidden_size * 2))\n        h_t_forward_list = []\n        for time_step, Xt in enumerate(sentence_representation_squeeze):  # Xt:[batch_size, hidden_size*2]\n            h_t = self.gru_single_step_sentence_level(Xt,h_t)  # h_t:[batch_size,hidden_size]<---------Xt:[batch_size, hidden_size*2]; h_t:[batch_size, hidden_size*2]\n            h_t_forward_list.append(h_t)\n        h_t_forward_list.reverse() #ADD 2017.06.14\n        return h_t_forward_list  # a list,length is num_sentences, each element is [batch_size,hidden_size]\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding_projection""):  # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],\n                                             initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"", shape=[self.hidden_size * 4, self.num_classes],\n                                                initializer=self.initializer)  # [embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"", shape=[self.num_classes])  #TODO [label_size]\n\n        # GRU parameters:update gate related\n        with tf.name_scope(""gru_weights_word_level""):\n            self.W_z = tf.get_variable(""W_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_z = tf.get_variable(""U_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_z = tf.get_variable(""b_z"", shape=[self.hidden_size])\n            # GRU parameters:reset gate related\n            self.W_r = tf.get_variable(""W_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_r = tf.get_variable(""U_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_r = tf.get_variable(""b_r"", shape=[self.hidden_size])\n\n            self.W_h = tf.get_variable(""W_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_h = tf.get_variable(""U_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_h = tf.get_variable(""b_h"", shape=[self.hidden_size])\n\n        with tf.name_scope(""gru_weights_sentence_level""):\n            self.W_z_sentence = tf.get_variable(""W_z_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.U_z_sentence = tf.get_variable(""U_z_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.b_z_sentence = tf.get_variable(""b_z_sentence"", shape=[self.hidden_size * 2])\n            # GRU parameters:reset gate related\n            self.W_r_sentence = tf.get_variable(""W_r_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.U_r_sentence = tf.get_variable(""U_r_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.b_r_sentence = tf.get_variable(""b_r_sentence"", shape=[self.hidden_size * 2])\n\n            self.W_h_sentence = tf.get_variable(""W_h_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.U_h_sentence = tf.get_variable(""U_h_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.b_h_sentence = tf.get_variable(""b_h_sentence"", shape=[self.hidden_size * 2])\n\n        with tf.name_scope(""attention""):\n            self.W_w_attention_word = tf.get_variable(""W_w_attention_word"",\n                                                      shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                      initializer=self.initializer)\n            self.W_b_attention_word = tf.get_variable(""W_b_attention_word"", shape=[self.hidden_size * 2])\n\n            self.W_w_attention_sentence = tf.get_variable(""W_w_attention_sentence"",\n                                                          shape=[self.hidden_size * 4, self.hidden_size * 2],\n                                                          initializer=self.initializer)\n            self.W_b_attention_sentence = tf.get_variable(""W_b_attention_sentence"", shape=[self.hidden_size * 2])\n            self.context_vecotor_word = tf.get_variable(""what_is_the_informative_word"", shape=[self.hidden_size * 2],\n                                                        initializer=self.initializer)  # TODO o.k to use batch_size in first demension?\n            self.context_vecotor_sentence = tf.get_variable(""what_is_the_informative_sentence"",\n                                                            shape=[self.hidden_size * 2], initializer=self.initializer)\n\n\n# test started\ndef test():\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes = 3\n    learning_rate = 0.01\n    batch_size = 8\n    decay_steps = 1000\n    decay_rate = 0.9\n    sequence_length = 30\n    num_sentences = 6  # number of sentences\n    vocab_size = 10000\n    embed_size = 100 #100\n    hidden_size = 100\n    is_training = True\n    dropout_keep_prob = 1  # 0.5 #num_sentences\n    textRNN = HierarchicalAttention(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\n                                    num_sentences, vocab_size, embed_size,\n                                    hidden_size, is_training,multi_label_flag=False)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            # input_x should be:[batch_size, num_sentences,self.sequence_length]\n            input_x = np.zeros((batch_size, sequence_length)) #num_sentences\n            input_x[input_x > 0.5] = 1\n            input_x[input_x <= 0.5] = 0\n            input_y = np.array(\n                [1, 0, 1, 1, 1, 2, 1, 1])  # np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss, acc, predict, W_projection_value, _ = sess.run(\n                [textRNN.loss_val, textRNN.accuracy, textRNN.predictions, textRNN.W_projection, textRNN.train_op],\n                feed_dict={textRNN.input_x: input_x, textRNN.input_y: input_y,\n                           textRNN.dropout_keep_prob: dropout_keep_prob})\n            print(""loss:"", loss, ""acc:"", acc, ""label:"", input_y, ""prediction:"", predict)\n            # print(""W_projection_value_:"",W_projection_value)\n#test()'"
a05_HierarchicalAttentionNetwork/p1_HierarchicalAttention_model_transformer.py,135,"b'# -*- coding: utf-8 -*-\n# HierarchicalAttention: 1.Word Encoder. 2.Word Attention. 3.Sentence Encoder 4.Sentence Attention 5.linear classifier. 2017-06-13\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.contrib as tf_contrib\nfrom  a2_multi_head_attention import MultiHeadAttention\nfrom a2_poistion_wise_feed_forward import PositionWiseFeedFoward\nclass HierarchicalAttention:\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length, num_sentences,\n                 vocab_size, embed_size,\n                 hidden_size, is_training, need_sentence_level_attention_encoder_flag=True, multi_label_flag=False,\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0):#0.01\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length = sequence_length\n        self.num_sentences = num_sentences\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.is_training = is_training\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")#TODO ADD learning_rate\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\n        self.initializer = initializer\n        self.multi_label_flag = multi_label_flag\n        self.hidden_size = hidden_size\n        self.need_sentence_level_attention_encoder_flag = need_sentence_level_attention_encoder_flag\n        self.clip_gradients=clip_gradients\n\n        # add placeholder (X,label)\n        # self.input_x = tf.placeholder(tf.int32, [None, self.num_sentences,self.sequence_length], name=""input_x"")  # X\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")\n\n        self.sequence_length = int(self.sequence_length / self.num_sentences) # TODO\n        self.input_y = tf.placeholder(tf.int32, [None, ], name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32, [None, self.num_classes],name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference()  # [None, self.label_size]. main computation graph is here.\n\n        self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\n        if not self.multi_label_flag:\n            correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32),\n                                          self.input_y)  # tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"")  # shape=()\n        else:\n            self.accuracy = tf.constant(\n                0.5)  # fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n\n        if not is_training:\n            return\n        if multi_label_flag:\n            print(""going to use multi label loss."")\n            self.loss_val = self.loss_multilabel()\n        else:\n            print(""going to use single label loss."")\n            self.loss_val = self.loss()\n        self.train_op = self.train()\n\n    def attention_word_level(self, hidden_state):\n        """"""\n        input1:self.hidden_state: hidden_state:list,len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n        input2:sentence level context vector:[batch_size*num_sentences,hidden_size*2]\n        :return:representation.shape:[batch_size*num_sentences,hidden_size*2]\n        """"""\n        hidden_state_ = tf.stack(hidden_state, axis=1)  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        # 0) one layer of feed forward network\n        hidden_state_2 = tf.reshape(hidden_state_, shape=[-1,self.hidden_size * 2])  # shape:[batch_size*num_sentences*sequence_length,hidden_size*2]\n        # hidden_state_:[batch_size*num_sentences*sequence_length,hidden_size*2];W_w_attention_sentence:[,hidden_size*2,,hidden_size*2]\n        hidden_representation = tf.nn.tanh(tf.matmul(hidden_state_2,self.W_w_attention_word) + self.W_b_attention_word)  # shape:[batch_size*num_sentences*sequence_length,hidden_size*2]\n        hidden_representation = tf.reshape(hidden_representation, shape=[-1, self.sequence_length,self.hidden_size * 2])  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        # attention process:1.get logits for each word in the sentence. 2.get possibility distribution for each word in the sentence. 3.get weighted sum for the sentence as sentence representation.\n        # 1) get logits for each word in the sentence.\n        hidden_state_context_similiarity = tf.multiply(hidden_representation,self.context_vecotor_word)  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        attention_logits = tf.reduce_sum(hidden_state_context_similiarity,axis=2)  # shape:[batch_size*num_sentences,sequence_length]\n        # subtract max for numerical stability (softmax is shift invariant). tf.reduce_max:Computes the maximum of elements across dimensions of a tensor.\n        attention_logits_max = tf.reduce_max(attention_logits, axis=1,keep_dims=True)  # shape:[batch_size*num_sentences,1]\n        # 2) get possibility distribution for each word in the sentence.\n        p_attention = tf.nn.softmax(attention_logits - attention_logits_max)  # shape:[batch_size*num_sentences,sequence_length]\n        # 3) get weighted hidden state by attention vector\n        p_attention_expanded = tf.expand_dims(p_attention, axis=2)  # shape:[batch_size*num_sentences,sequence_length,1]\n        # below sentence_representation\'shape:[batch_size*num_sentences,sequence_length,hidden_size*2]<----p_attention_expanded:[batch_size*num_sentences,sequence_length,1];hidden_state_:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        sentence_representation = tf.multiply(p_attention_expanded,hidden_state_)  # shape:[batch_size*num_sentences,sequence_length,hidden_size*2]\n        sentence_representation = tf.reduce_sum(sentence_representation,axis=1)  # shape:[batch_size*num_sentences,hidden_size*2]\n        return sentence_representation  # shape:[batch_size*num_sentences,hidden_size*2]\n\n    def attention_sentence_level(self, hidden_state_sentence):\n        """"""\n        input1: hidden_state_sentence: a list,len:num_sentence,element:[None,hidden_size*4]\n        input2: sentence level context vector:[self.hidden_size*2]\n        :return:representation.shape:[None,hidden_size*4]\n        """"""\n        hidden_state_ = tf.stack(hidden_state_sentence, axis=1)  # shape:[None,num_sentence,hidden_size*4]\n\n        # 0) one layer of feed forward\n        hidden_state_2 = tf.reshape(hidden_state_,\n                                    shape=[-1, self.hidden_size * 4])  # [None*num_sentence,hidden_size*4]\n        hidden_representation = tf.nn.tanh(tf.matmul(hidden_state_2,\n                                                     self.W_w_attention_sentence) + self.W_b_attention_sentence)  # shape:[None*num_sentence,hidden_size*2]\n        hidden_representation = tf.reshape(hidden_representation, shape=[-1, self.num_sentences,\n                                                                         self.hidden_size * 2])  # [None,num_sentence,hidden_size*2]\n        # attention process:1.get logits for each sentence in the doc.2.get possibility distribution for each sentence in the doc.3.get weighted sum for the sentences as doc representation.\n        # 1) get logits for each word in the sentence.\n        hidden_state_context_similiarity = tf.multiply(hidden_representation,\n                                                       self.context_vecotor_sentence)  # shape:[None,num_sentence,hidden_size*2]\n        attention_logits = tf.reduce_sum(hidden_state_context_similiarity,\n                                         axis=2)  # shape:[None,num_sentence]. that is get logit for each num_sentence.\n        # subtract max for numerical stability (softmax is shift invariant). tf.reduce_max:computes the maximum of elements across dimensions of a tensor.\n        attention_logits_max = tf.reduce_max(attention_logits, axis=1, keep_dims=True)  # shape:[None,1]\n        # 2) get possibility distribution for each word in the sentence.\n        p_attention = tf.nn.softmax(attention_logits - attention_logits_max)  # shape:[None,num_sentence]\n        # 3) get weighted hidden state by attention vector(sentence level)\n        p_attention_expanded = tf.expand_dims(p_attention, axis=2)  # shape:[None,num_sentence,1]\n        sentence_representation = tf.multiply(p_attention_expanded,\n                                              hidden_state_)  # shape:[None,num_sentence,hidden_size*2]<---p_attention_expanded:[None,num_sentence,1];hidden_state_:[None,num_sentence,hidden_size*2]\n        sentence_representation = tf.reduce_sum(sentence_representation, axis=1)  # shape:[None,hidden_size*2]\n        return sentence_representation  # shape:[None,hidden_size*2]\n\n    def inference(self):\n        """"""main computation graph here: 1.Word Encoder. 2.Word Attention. 3.Sentence Encoder 4.Sentence Attention 5.linear classifier""""""\n        # 1.Word Encoder\n        # 1.1 embedding of words\n        input_x = tf.split(self.input_x, self.num_sentences,axis=1)  # a list. length:num_sentences.each element is:[None,self.sequence_length/num_sentences]\n        input_x = tf.stack(input_x, axis=1)  # shape:[None,self.num_sentences,self.sequence_length/num_sentences]\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,input_x)  # [None,num_sentences,sentence_length,embed_size]\n        embedded_words_reshaped = tf.reshape(self.embedded_words, shape=[-1, self.sequence_length,self.embed_size])  # [batch_size*num_sentences,sentence_length,embed_size]\n        # 1.2 forward gru\n        hidden_state_forward_list = self.gru_forward_word_level(embedded_words_reshaped)  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        # 1.3 backward gru\n        hidden_state_backward_list = self.gru_backward_word_level(embedded_words_reshaped)  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        # 1.4 concat forward hidden state and backward hidden state. hidden_state: a list.len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n        self.hidden_state = [tf.concat([h_forward, h_backward], axis=1) for h_forward, h_backward in\n                             zip(hidden_state_forward_list, hidden_state_backward_list)]  # hidden_state:list,len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n\n        # 2.Word Attention\n        # for each sentence.\n        sentence_representation = self.attention_word_level(self.hidden_state)  # output:[batch_size*num_sentences,hidden_size*2]\n        sentence_representation = tf.reshape(sentence_representation, shape=[-1, self.num_sentences, self.hidden_size * 2])  # shape:[batch_size,num_sentences,hidden_size*2]\n\n        # 2.====================== Multi Head Attention + Layer normalization================================\n        V_sentence= tf.get_variable(""V_sentence"", shape=sentence_representation.get_shape().as_list(),initializer=self.initializer)\n        multi_head_attention_class = MultiHeadAttention(sentence_representation, sentence_representation, V_sentence, 200, 25, 25, self.sequence_length, 8,type=\'word_level\')\n        sentence_representation=multi_head_attention_class.multi_head_attention_fn() #shape:[sequence_length,d_model]\n\n        #postion_wise_feed_forward = PositionWiseFeedFoward(sentence_representation,\'word_level\',d_model=sentence_representation.get_shape()[-1])\n        #sentence_representation = postion_wise_feed_forward.position_wise_feed_forward_fn()\n\n        #sentence_representation=self.layer_normalization(sentence_representation,""sentence_representation"") #add layer_normalization\n        #with tf.name_scope(""dropout""):#\n        #    sentence_representation = tf.nn.dropout(sentence_representation,keep_prob=self.dropout_keep_prob)  # shape:[None,hidden_size*4]\n\n        # 3.Sentence Encoder\n        # 3.1) forward gru for sentence\n        hidden_state_forward_sentences = self.gru_forward_sentence_level(sentence_representation)  # a list.length is sentence_length, each element is [None,hidden_size]\n        # 3.2) backward gru for sentence\n        hidden_state_backward_sentences = self.gru_backward_sentence_level(sentence_representation)  # a list,length is sentence_length, each element is [None,hidden_size]\n        # 3.3) concat forward hidden state and backward hidden state\n        # below hidden_state_sentence is a list,len:sentence_length,element:[None,hidden_size*2]\n        self.hidden_state_sentence = [tf.concat([h_forward, h_backward], axis=1) for h_forward, h_backward in zip(hidden_state_forward_sentences, hidden_state_backward_sentences)]\n\n        # 3. ======================Multi-head Attention + Layer Normalization================================\n        #####self.hidden_state_sentence=tf.stack(self.hidden_state_sentence,axis=1) #[batch,sentence_length,hidden_size*2]\n        #####print(""self.hidden_state_sentence0:"", self.hidden_state_sentence)  # 8, 6, 200\n        ####V_encoder= tf.get_variable(""V_encoder"", shape=self.hidden_state_sentence.get_shape().as_list(),initializer=self.initializer)\n        #####multi_head_attention_class = MultiHeadAttention(self.hidden_state_sentence, self.hidden_state_sentence, V_encoder, 400, 50, 50, self.sequence_length, 8,type=\'word_encoder\')\n        ####hidden_state_sentence=multi_head_attention_class.multi_head_attention_fn() #shape:[sequence_length,d_model]\n\n        ######postion_wise_feed_forward = PositionWiseFeedFoward(hidden_state_sentence,\'sentence_level\',d_model=hidden_state_sentence.get_shape()[-1])\n        ######hidden_state_sentence = postion_wise_feed_forward.position_wise_feed_forward_fn()\n\n        ######hidden_state_sentence=self.layer_normalization(self.hidden_state_sentence,""hidden_state_sentence"") #add layer_normalization\n        ######print(""hidden_state_sentence:"",hidden_state_sentence) #[8, 6, 200]\n        ######self.hidden_state_sentence=[tf.squeeze(e,axis=1) for e in tf.split(hidden_state_sentence,hidden_state_sentence.get_shape().as_list()[1],axis=1)]\n\n        # 4.Sentence Attention\n        print(""self.hidden_state_sentence:"", self.hidden_state_sentence)  # 8, 6, 200\n        document_representation = self.attention_sentence_level(self.hidden_state_sentence)  # shape:[None,hidden_size*4]\n        document_representation = self.layer_normalization(document_representation,""document_representation"")  # add layer_normalization\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(document_representation,keep_prob=self.dropout_keep_prob)  # shape:[None,hidden_size*4]\n        # 5. logits(use linear layer)and predictions(argmax)\n        with tf.name_scope(""output""):\n            logits = tf.matmul(self.h_drop, self.W_projection) + self.b_projection  # shape:[None,self.num_classes]==tf.matmul([None,hidden_size*2],[hidden_size*2,self.num_classes])\n        return logits\n\n    def loss(self, l2_lambda=0.0001):  # 0.001\n        with tf.name_scope(""loss""):\n            # input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n                                                                    logits=self.logits);  # sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            # print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss = tf.reduce_mean(losses)  # print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n(\n                [tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss = loss + l2_losses\n        return loss\n\n    def loss_multilabel(self, l2_lambda=0.00001*10): #*3#0.00001 #TODO 0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            # input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            # input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel,\n                                                             logits=self.logits);  # losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            # losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"", losses)  # shape=(?, 1999).\n            losses = tf.reduce_sum(losses, axis=1)  # shape=(?,). loss for all data in the batch\n            loss = tf.reduce_mean(losses)  # shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss = loss + l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\n                                                   self.decay_rate, staircase=True)\n        self.learning_rate_=learning_rate\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\n        train_op = tf_contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\n        return train_op\n\n    def gru_single_step_word_level(self, Xt, h_t_minus_1):\n        """"""\n        single step of gru for word level\n        :param Xt: Xt:[batch_size*num_sentences,embed_size]\n        :param h_t_minus_1:[batch_size*num_sentences,embed_size]\n        :return:\n        """"""\n        # update gate: decides how much past information is kept and how much new information is added.\n        z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z) + tf.matmul(h_t_minus_1,\n                                                                self.U_z) + self.b_z)  # z_t:[batch_size*num_sentences,self.hidden_size]\n        # reset gate: controls how much the past state contributes to the candidate state.\n        r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r) + tf.matmul(h_t_minus_1,\n                                                                self.U_r) + self.b_r)  # r_t:[batch_size*num_sentences,self.hidden_size]\n        # candiate state h_t~\n        h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h) +r_t * (tf.matmul(h_t_minus_1, self.U_h)) + self.b_h)  # h_t_candiate:[batch_size*num_sentences,self.hidden_size]\n        # new state: a linear combine of pervious hidden state and the current new state h_t~\n        h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate  # h_t:[batch_size*num_sentences,hidden_size]\n        return h_t\n\n    def gru_single_step_sentence_level(self, Xt,\n                                       h_t_minus_1):  # Xt:[batch_size, hidden_size*2]; h_t:[batch_size, hidden_size*2]\n        """"""\n        single step of gru for sentence level\n        :param Xt:[batch_size, hidden_size*2]\n        :param h_t_minus_1:[batch_size, hidden_size*2]\n        :return:h_t:[batch_size,hidden_size]\n        """"""\n        # update gate: decides how much past information is kept and how much new information is added.\n        z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z_sentence) + tf.matmul(h_t_minus_1,\n                                                                         self.U_z_sentence) + self.b_z_sentence)  # z_t:[batch_size,self.hidden_size]\n        # reset gate: controls how much the past state contributes to the candidate state.\n        r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r_sentence) + tf.matmul(h_t_minus_1,\n                                                                         self.U_r_sentence) + self.b_r_sentence)  # r_t:[batch_size,self.hidden_size]\n        # candiate state h_t~\n        h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h_sentence) + r_t * (\n            tf.matmul(h_t_minus_1, self.U_h_sentence)) + self.b_h_sentence)  # h_t_candiate:[batch_size,self.hidden_size]\n        # new state: a linear combine of pervious hidden state and the current new state h_t~\n        h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate\n        return h_t\n\n    # forward gru for first level: word levels\n    def gru_forward_word_level(self, embedded_words):\n        """"""\n        :param embedded_words:[batch_size*num_sentences,sentence_length,embed_size]\n        :return:forward hidden state: a list.length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        """"""\n        # split embedded_words\n        embedded_words_splitted = tf.split(embedded_words, self.sequence_length,\n                                           axis=1)  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,1,embed_size]\n        embedded_words_squeeze = [tf.squeeze(x, axis=1) for x in\n                                  embedded_words_splitted]  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,embed_size]\n        # demension_1=embedded_words_squeeze[0].get_shape().dims[0]\n        h_t = tf.ones((self.batch_size * self.num_sentences,\n                       self.hidden_size))  #TODO self.hidden_size h_t =int(tf.get_shape(embedded_words_squeeze[0])[0]) # tf.ones([self.batch_size*self.num_sentences, self.hidden_size]) # [batch_size*num_sentences,embed_size]\n        h_t_forward_list = []\n        for time_step, Xt in enumerate(embedded_words_squeeze):  # Xt: [batch_size*num_sentences,embed_size]\n            h_t = self.gru_single_step_word_level(Xt,h_t)  # [batch_size*num_sentences,embed_size]<------Xt:[batch_size*num_sentences,embed_size];h_t:[batch_size*num_sentences,embed_size]\n            h_t_forward_list.append(h_t)\n        return h_t_forward_list  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n\n    # backward gru for first level: word level\n    def gru_backward_word_level(self, embedded_words):\n        """"""\n        :param   embedded_words:[batch_size*num_sentences,sentence_length,embed_size]\n        :return: backward hidden state:a list.length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        """"""\n        # split embedded_words\n        embedded_words_splitted = tf.split(embedded_words, self.sequence_length,\n                                           axis=1)  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,1,embed_size]\n        embedded_words_squeeze = [tf.squeeze(x, axis=1) for x in\n                                  embedded_words_splitted]  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,embed_size]\n        embedded_words_squeeze.reverse()  # it is a list,length is sentence_length, each element is [batch_size*num_sentences,embed_size]\n        # demension_1=int(tf.get_shape(embedded_words_squeeze[0])[0]) #h_t = tf.ones([self.batch_size*self.num_sentences, self.hidden_size])\n        h_t = tf.ones((self.batch_size * self.num_sentences, self.hidden_size))\n        h_t_backward_list = []\n        for time_step, Xt in enumerate(embedded_words_squeeze):\n            h_t = self.gru_single_step_word_level(Xt, h_t)\n            h_t_backward_list.append(h_t)\n        h_t_backward_list.reverse() #ADD 2017.06.14\n        return h_t_backward_list\n\n    # forward gru for second level: sentence level\n    def gru_forward_sentence_level(self, sentence_representation):\n        """"""\n        :param sentence_representation: [batch_size,num_sentences,hidden_size*2]\n        :return:forward hidden state: a list,length is num_sentences, each element is [batch_size,hidden_size]\n        """"""\n        # split embedded_words\n        sentence_representation_splitted = tf.split(sentence_representation, self.num_sentences,\n                                                    axis=1)  # it is a list.length is num_sentences,each element is [batch_size,1,hidden_size*2]\n        sentence_representation_squeeze = [tf.squeeze(x, axis=1) for x in\n                                           sentence_representation_splitted]  # it is a list.length is num_sentences,each element is [batch_size, hidden_size*2]\n        # demension_1 = int(tf.get_shape(sentence_representation_squeeze[0])[0]) #scalar: batch_size\n        h_t = tf.ones((self.batch_size, self.hidden_size * 2))  # TODO\n        h_t_forward_list = []\n        for time_step, Xt in enumerate(sentence_representation_squeeze):  # Xt:[batch_size, hidden_size*2]\n            h_t = self.gru_single_step_sentence_level(Xt,\n                                                      h_t)  # h_t:[batch_size,hidden_size]<---------Xt:[batch_size, hidden_size*2]; h_t:[batch_size, hidden_size*2]\n            h_t_forward_list.append(h_t)\n        return h_t_forward_list  # a list,length is num_sentences, each element is [batch_size,hidden_size]\n\n    # backward gru for second level: sentence level\n    def gru_backward_sentence_level(self, sentence_representation):\n        """"""\n        :param sentence_representation: [batch_size,num_sentences,hidden_size*2]\n        :return:forward hidden state: a list,length is num_sentences, each element is [batch_size,hidden_size]\n        """"""\n        # split embedded_words\n        sentence_representation_splitted = tf.split(sentence_representation, self.num_sentences,\n                                                    axis=1)  # it is a list.length is num_sentences,each element is [batch_size,1,hidden_size*2]\n        sentence_representation_squeeze = [tf.squeeze(x, axis=1) for x in\n                                           sentence_representation_splitted]  # it is a list.length is num_sentences,each element is [batch_size, hidden_size*2]\n        sentence_representation_squeeze.reverse()\n        # demension_1 = int(tf.get_shape(sentence_representation_squeeze[0])[0])  # scalar: batch_size\n        h_t = tf.ones((self.batch_size, self.hidden_size * 2))\n        h_t_forward_list = []\n        for time_step, Xt in enumerate(sentence_representation_squeeze):  # Xt:[batch_size, hidden_size*2]\n            h_t = self.gru_single_step_sentence_level(Xt,h_t)  # h_t:[batch_size,hidden_size]<---------Xt:[batch_size, hidden_size*2]; h_t:[batch_size, hidden_size*2]\n            h_t_forward_list.append(h_t)\n        h_t_forward_list.reverse() #ADD 2017.06.14\n        return h_t_forward_list  # a list,length is num_sentences, each element is [batch_size,hidden_size]\n\n    def layer_normalization(self,x,scope):\n        """"""\n        x should be:[batch_size,sequence_length,d_model]\n        :return:[batch_size,sequence_length,d_model]\n        """"""\n        filter=x.get_shape()[-1] #last dimension of x. e.g. 512\n        with tf.variable_scope(""layer_normalization""+scope):\n            # 1. normalize input by using  mean and variance according to last dimension\n            mean=tf.reduce_mean(x,axis=-1,keep_dims=True) #[batch_size,sequence_length,1]\n            variance=tf.reduce_mean(tf.square(x-mean),axis=-1,keep_dims=True) #[batch_size,sequence_length,1]\n            norm_x=(x-mean)*tf.rsqrt(variance+1e-6) #[batch_size,sequence_length,d_model]\n            # 2. re-scale normalized input back\n            scale=tf.get_variable(""layer_norm_scale"",[filter],initializer=tf.ones_initializer) #[filter]\n            bias=tf.get_variable(""layer_norm_bias"",[filter],initializer=tf.ones_initializer) #[filter]\n            output=norm_x*scale+bias #[batch_size,sequence_length,d_model]\n            return output #[batch_size,sequence_length,d_model]\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding_projection""):  # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],\n                                             initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"", shape=[self.hidden_size * 4, self.num_classes],\n                                                initializer=self.initializer)  # [embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"", shape=[self.num_classes])  #TODO [label_size]\n\n        # GRU parameters:update gate related\n        with tf.name_scope(""gru_weights_word_level""):\n            self.W_z = tf.get_variable(""W_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_z = tf.get_variable(""U_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_z = tf.get_variable(""b_z"", shape=[self.hidden_size])\n            # GRU parameters:reset gate related\n            self.W_r = tf.get_variable(""W_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_r = tf.get_variable(""U_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_r = tf.get_variable(""b_r"", shape=[self.hidden_size])\n\n            self.W_h = tf.get_variable(""W_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_h = tf.get_variable(""U_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_h = tf.get_variable(""b_h"", shape=[self.hidden_size])\n\n        with tf.name_scope(""gru_weights_sentence_level""):\n            self.W_z_sentence = tf.get_variable(""W_z_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.U_z_sentence = tf.get_variable(""U_z_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.b_z_sentence = tf.get_variable(""b_z_sentence"", shape=[self.hidden_size * 2])\n            # GRU parameters:reset gate related\n            self.W_r_sentence = tf.get_variable(""W_r_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.U_r_sentence = tf.get_variable(""U_r_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.b_r_sentence = tf.get_variable(""b_r_sentence"", shape=[self.hidden_size * 2])\n\n            self.W_h_sentence = tf.get_variable(""W_h_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.U_h_sentence = tf.get_variable(""U_h_sentence"", shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                initializer=self.initializer)\n            self.b_h_sentence = tf.get_variable(""b_h_sentence"", shape=[self.hidden_size * 2])\n\n        with tf.name_scope(""attention""):\n            self.W_w_attention_word = tf.get_variable(""W_w_attention_word"",\n                                                      shape=[self.hidden_size * 2, self.hidden_size * 2],\n                                                      initializer=self.initializer)\n            self.W_b_attention_word = tf.get_variable(""W_b_attention_word"", shape=[self.hidden_size * 2])\n\n            self.W_w_attention_sentence = tf.get_variable(""W_w_attention_sentence"",\n                                                          shape=[self.hidden_size * 4, self.hidden_size * 2],\n                                                          initializer=self.initializer)\n            self.W_b_attention_sentence = tf.get_variable(""W_b_attention_sentence"", shape=[self.hidden_size * 2])\n            self.context_vecotor_word = tf.get_variable(""what_is_the_informative_word"", shape=[self.hidden_size * 2],\n                                                        initializer=self.initializer)  # TODO o.k to use batch_size in first demension?\n            self.context_vecotor_sentence = tf.get_variable(""what_is_the_informative_sentence"",\n                                                            shape=[self.hidden_size * 2], initializer=self.initializer)\n\n\n# test started\n# given a sequence of inputs, if the sum of inputs is greater than a threshold, then output will be 2, if equals, then output is 1, else 0.\ndef test():\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes = 3\n    learning_rate = 0.0001\n    batch_size = 8\n    decay_steps = 1000\n    decay_rate = 0.9\n    sequence_length = 30\n    num_sentences = 6  # number of sentences\n    vocab_size = 10000\n    embed_size = 100 #100\n    hidden_size = 100\n    is_training = True\n    dropout_keep_prob = 1  # 0.5 #num_sentences\n    textRNN = HierarchicalAttention(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\n                                    num_sentences, vocab_size, embed_size,\n                                    hidden_size, is_training,multi_label_flag=False)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(1500):\n            # input_x should be:[batch_size, num_sentences,self.sequence_length]\n            input_x = np.random.randn(batch_size, sequence_length) #num_sentences\n            input_x[input_x > 0.5] = 1\n            input_x[input_x <= 0.5] = 0\n            input_y=get_input_y(i,batch_size)\n            loss, acc, predict, W_projection_value, _ = sess.run(\n                [textRNN.loss_val, textRNN.accuracy, textRNN.predictions, textRNN.W_projection, textRNN.train_op],\n                feed_dict={textRNN.input_x: input_x, textRNN.input_y: input_y,\n                           textRNN.dropout_keep_prob: dropout_keep_prob})\n            print(i,""loss:"", loss, ""acc:"", acc, ""label:"", input_y, ""prediction:"", predict)\n            # print(""W_projection_value_:"",W_projection_value)\n\ndef get_input_y(i,input_x,batch_size):\n    input_y = [0 for j in range(batch_size)]\n    for k in range(batch_size):\n        input_sum = np.sum(input_x[k])\n        #print(i, ""input_sum:"", input_sum)\n        if input_sum < 10:\n            input_y[k] = 0\n        elif input_sum == 10:\n            input_y[k] = 1\n        else:\n            input_y[k] = 2\n    input_y = np.array(input_y)\n    return input_y\n\n#test()\n'"
a05_HierarchicalAttentionNetwork/p1_HierarchicalAttention_predict.py,26,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p1_HierarchicalAttention_model import HierarchicalAttention\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 80, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_hier_atten_title/text_hier_atten_title_desc_checkpoint_0609/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\ntf.app.flags.DEFINE_integer(""num_sentences"", 4, ""number of sentences in the document"") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\ntf.app.flags.DEFINE_string(""predict_target_file"",""checkpoint_hier_atten_title/text_hier_atten_title_desc_checkpoint_0609/zhihu_result_hier_atten_multilabel_b512_DROPOUT4.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n# 1.load data with vocabulary of words and labels\n\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""hierAtten"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""hierAtten"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        model = HierarchicalAttention(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                                      FLAGS.decay_rate, FLAGS.sequence_length,\n                                      FLAGS.num_sentences, vocab_size, FLAGS.embed_size, FLAGS.hidden_size,\n                                      FLAGS.is_training, multi_label_flag=FLAGS.multi_label_flag)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(model.logits,feed_dict={model.input_x:testX2[start:end],model.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            #predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            #write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            question_id_sublist=question_id_list[start:end]\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\n\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\n# get label using logits\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\n    #print(""get_label_using_logits.shape:"", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\n    for i,logits in enumerate(logits_batch):\n        index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n        index_list=index_list[::-1]\n        label_list=[]\n        for index in index_list:\n            label=vocabulary_index2word_label[index]\n            label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        #print(""get_label_using_logits.label_list"",label_list)\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\n    f.flush()\n    #return label_list\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a05_HierarchicalAttentionNetwork/p1_HierarchicalAttention_train.py,28,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p1_HierarchicalAttention_model import HierarchicalAttention\nfrom p1_HierarchicalAttention_model_transformer import HierarchicalAttention\n\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"") #TODO 0.01\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128 #TODO\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_hier_atten_title/text_hier_atten_title_desc_checkpoint_MHA/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",20,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""validate_step"", 1000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c TODO\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\ntf.app.flags.DEFINE_integer(""num_sentences"", 4, ""number of sentences in the document"") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""hierAtten"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""cnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""hierAtten"")\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #test-zhihu5-only-title-multilabel.txt\n        train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        #num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,num_sentences,vocab_size,embed_size,\n        #hidden_size,is_training\n        model=HierarchicalAttention(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sequence_length,\n                                       FLAGS.num_sentences,vocab_size,FLAGS.embed_size,FLAGS.hidden_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(model.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        print(""number_of_training_data:"",number_of_training_data)\n        previous_eval_loss=10000\n        best_eval_loss=10000\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {model.input_x: trainX[start:end],model.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[model.input_y] = trainY[start:end]\n                else:\n                    feed_dict[model.input_y_multilabel]=trainY[start:end]\n                curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %50==0:\n                    print(""HierAtten_0609drate0.75==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n                if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0): #(epoch % FLAGS.validate_every) or  if epoch % FLAGS.validate_every == 0:\n                    eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\n                    print(""validation.part. previous_eval_loss:"", previous_eval_loss,"";current_eval_loss:"", eval_loss)\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\n                        # reduce the learning rate by a factor of 0.5\n                        print(""HierAtten_0609drate0.75==>validation.part.going to reduce the learning rate."")\n                        learning_rate1 = sess.run(model.learning_rate)\n                        lrr=sess.run([model.learning_rate_decay_half_op])\n                        learning_rate2 = sess.run(model.learning_rate)\n                        print(""HierAtten_0609drate0.75==>validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\n                    #print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_acc))\n                    else:# loss is decreasing\n                        if eval_loss<best_eval_loss:\n                            print(""HierAtten_0609drate0.75==>going to save the model.eval_loss:"",eval_loss,"";best_eval_loss:"",best_eval_loss)\n                            # save model to checkpoint\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                            saver.save(sess, save_path, global_step=epoch)\n                            best_eval_loss=eval_loss\n                    previous_eval_loss = eval_loss\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(model.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,model,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,model,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(model.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textCNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[textCNN.input_y] = evalY[start:end]\n        else:\n            feed_dict[textCNN.input_y_multilabel] = evalY[start:end]\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textCNN.loss_val,textCNN.logits,textCNN.accuracy],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(label_nozero)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
a05_HierarchicalAttentionNetwork/p1_seq2seq.py,20,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\n# \xe3\x80\x90\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x91\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe6\xa0\xb9\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xef\xbc\x8c\xe5\x86\x8d\xe5\xbe\x97\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84embedding.\ndef extract_argmax_and_embed(embedding, output_projection=None):\n    """"""\n    Get a loop_function that extracts the previous symbol and embeds it. Used by decoder.\n    :param embedding: embedding tensor for symbol\n    :param output_projection: None or a pair (W, B). If provided, each fed previous output will\n    first be multiplied by W and added B.\n    :return: A loop function\n    """"""\n    def loop_function(prev, _):\n        if output_projection is not None:\n            prev = tf.matmul(prev, output_projection[0]) + output_projection[1]\n        prev_symbol = tf.argmax(prev, 1) #\xe5\xbe\x97\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84INDEX\n        emb_prev = tf.gather(embedding, prev_symbol) #\xe5\xbe\x97\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaaINDEX\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84embedding\n        return emb_prev\n    return loop_function\n\n# RNN\xe7\x9a\x84\xe8\xa7\xa3\xe7\xa0\x81\xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82\n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaftest,\xe5\xb0\x86t\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xbat+1\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84s\xe8\xbe\x93\xe5\x85\xa5\ndef rnn_decoder_with_attention(decoder_inputs, initial_state, cell, loop_function,attention_states,scope=None):#3D Tensor [batch_size x attn_length x attn_size]\n    """"""RNN decoder for the sequence-to-sequence model.\n    Args:\n        decoder_inputs: A list of 2D Tensors [batch_size x input_size].it is target Y, but shift by one.\n        initial_state: 2D Tensor with shape [batch_size x cell.state_size].it is the encoded vector of input sentences, which represent \'thought vector\'\n        cell: core_rnn_cell.RNNCell defining the cell function and size.\n        loop_function: If not None, this function will be applied to the i-th output\n            in order to generate the i+1-st input, and decoder_inputs will be ignored,\n            except for the first element (""GO"" symbol). This can be used for decoding,\n            but also for training to emulate http://arxiv.org/abs/1506.03099.\n            Signature -- loop_function(prev, i) = next\n                * prev is a 2D Tensor of shape [batch_size x output_size],\n                * i is an integer, the step number (when advanced control is needed),\n                * next is a 2D Tensor of shape [batch_size x input_size].\n        attention_states: 3D Tensor [batch_size x attn_length x attn_size].it is a input X.\n        scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n    Returns:\n        A tuple of the form (outputs, state), where:\n        outputs: A list of the same length as decoder_inputs of 2D Tensors with\n            shape [batch_size x output_size] containing generated outputs.\n        state: The state of each cell at the final time-step.\n            It is a 2D Tensor of shape [batch_size x cell.state_size].\n            (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n            states can be the same. They are different for LSTM cells though.)\n    """"""\n    with tf.variable_scope(scope or ""rnn_decoder""):\n        print(""rnn_decoder_with_attention started..."")\n        state = initial_state  #[batch_size x cell.state_size]\n        _, hidden_size = state.get_shape().as_list()\n        batch_size,sequence_length,embed_size=attention_states.get_shape().as_list()\n        outputs = []\n        prev = None\n        W_a = tf.get_variable(""W_a"", shape=[embed_size, hidden_size],initializer=tf.random_normal_initializer(stddev=0.1))\n        attention_states=tf.reshape(attention_states,shape=(-1,embed_size)) #attention_states:[batch_size*sequence_length,embed_size]\n        attention_states = tf.nn.tanh(tf.matmul(attention_states, W_a)) #attention_states:[batch_size*sequence_length,hidden_size]\n        attention_states=tf.reshape(attention_states,shape=(-1,sequence_length,hidden_size)) #attention_states:[batch_size,sequence_length,hidden_size]\n        for i, inp in enumerate(decoder_inputs):#\xe5\xbe\xaa\xe7\x8e\xaf\xe8\xa7\xa3\xe7\xa0\x81\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x82\xe5\xa6\x82sentence_length\xe4\xb8\xaa[batch_size x input_size]\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaftest, \xe5\xb0\x86t\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xbat + 1 \xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84s\xe8\xbe\x93\xe5\x85\xa5\n            if loop_function is not None and prev is not None:#\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x9a\xe5\xa6\x82\xe6\x9e\x9cloop_function\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\xe4\xb8\x94\xe5\x89\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84\xe5\x80\xbc\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe4\xbd\xbf\xe7\x94\xa8\xe5\x89\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9a\x84\xe5\x80\xbc\xe4\xbd\x9c\xe4\xb8\xbaRNN\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n                with tf.variable_scope(""loop_function"", reuse=True):\n                    inp = loop_function(prev, i)\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n            ##ATTENTION#################################################################################################################################################\n            # 1.use Full connected layer to match dimension for two parts of attention.<Wx*X,Wy*y>\n            W_s = tf.get_variable(""W_s_attention"", shape=[hidden_size, hidden_size], initializer=tf.random_normal_initializer(stddev=0.1))\n\n            state_transfered=tf.nn.tanh(tf.matmul(state,W_s))\n            # 2.get possibility attention for each encoder input. attention_states:[batch_size x attn_length x attn_size]; query=state:[batch_size x cell.state_size]\n            query=tf.expand_dims(state_transfered,axis=1)                        #[batch_size x 1 x cell.state_size]\n            # get logits using attention_states and query\n            attention_logits=tf.multiply(attention_states,query)      #TODO [batch_size x attn_length x attn_size]. notice: cell.state_size=atten_size=embedding_size\n            attention_logits=tf.reduce_sum(attention_logits,2)        #[batch_size x attn_length]\n            attention_logits_max=tf.reduce_max(attention_logits,axis=1,keep_dims=True) #[batch_size x 1]\n            # possibility distribution for each encoder input.it means how much attention or focus for each encoder input\n            p_attention=tf.nn.softmax(attention_logits-attention_logits_max)#[batch_size x attn_length]\n\n            # 3.get weighted sum of hidden state for each encoder input as attention state\n            p_attention=tf.expand_dims(p_attention,axis=2)            #[batch_size x attn_length x 1]\n            # attention_states:[batch_size x attn_length x attn_size]; p_attention:[batch_size x attn_length];\n            # final attention\n            attention_final=tf.multiply(attention_states,p_attention) #[batch_size x attn_length x attn_size]\n            attention_final=tf.reduce_sum(attention_final,axis=1)     #[batch_size x attn_size]\n            ############################################################################################################################################################\n            output, state = cell(inp+attention_final, state)          #\xe4\xbd\xbf\xe7\x94\xa8RNN\xe8\xb5\xb0\xe4\xb8\x80\xe6\xad\xa5 #TODO SHOULD WE ADD OR CONCAT THESE TWO PARTS.\n            outputs.append(output) # \xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe7\xbb\x93\xe6\x9e\x9c\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\n            if loop_function is not None:\n                prev = output\n    print(""rnn_decoder_with_attention ended..."")\n    return outputs, state'"
a06_Seq2seqWithAttention/a1_seq2seq.py,25,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\n# \xe3\x80\x90\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x91\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe6\xa0\xb9\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xef\xbc\x8c\xe5\x86\x8d\xe5\xbe\x97\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84embedding.\ndef extract_argmax_and_embed(embedding, output_projection=None):\n    """"""\n    Get a loop_function that extracts the previous symbol and embeds it. Used by decoder.\n    :param embedding: embedding tensor for symbol\n    :param output_projection: None or a pair (W, B). If provided, each fed previous output will\n    first be multiplied by W and added B.\n    :return: A loop function\n    """"""\n    def loop_function(prev, _):\n        if output_projection is not None:\n            prev = tf.matmul(prev, output_projection[0]) + output_projection[1]\n        prev_symbol = tf.argmax(prev, 1) #\xe5\xbe\x97\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84INDEX\n        emb_prev = tf.gather(embedding, prev_symbol) #\xe5\xbe\x97\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaaINDEX\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84embedding\n        return emb_prev\n    return loop_function\n\n# RNN\xe7\x9a\x84\xe8\xa7\xa3\xe7\xa0\x81\xe9\x83\xa8\xe5\x88\x86\xe3\x80\x82\n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaftest,\xe5\xb0\x86t\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xbat+1\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84s\xe8\xbe\x93\xe5\x85\xa5\ndef rnn_decoder_with_attention(decoder_inputs, initial_state, cell, loop_function,attention_states,scope=None):#3D Tensor [batch_size x attn_length x attn_size]\n    """"""RNN decoder for the sequence-to-sequence model.\n    Args:\n        decoder_inputs: A list of 2D Tensors [batch_size x input_size].it is decoder input.\n        initial_state: 2D Tensor with shape [batch_size x cell.state_size].it is the encoded vector of input sentences, which represent \'thought vector\'\n        cell: core_rnn_cell.RNNCell defining the cell function and size.\n        loop_function: If not None, this function will be applied to the i-th output\n            in order to generate the i+1-st input, and decoder_inputs will be ignored,\n            except for the first element (""GO"" symbol). This can be used for decoding,\n            but also for training to emulate http://arxiv.org/abs/1506.03099.\n            Signature -- loop_function(prev, i) = next\n                * prev is a 2D Tensor of shape [batch_size x output_size],\n                * i is an integer, the step number (when advanced control is needed),\n                * next is a 2D Tensor of shape [batch_size x input_size].\n        attention_states: 3D Tensor [batch_size x attn_length x attn_size].it is represent input X.\n        scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".\n    Returns:\n        A tuple of the form (outputs, state), where:\n        outputs: A list of the same length as decoder_inputs of 2D Tensors with\n            shape [batch_size x output_size] containing generated outputs.\n        state: The state of each cell at the final time-step.\n            It is a 2D Tensor of shape [batch_size x cell.state_size].\n            (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n            states can be the same. They are different for LSTM cells though.)\n    """"""\n    with tf.variable_scope(scope or ""rnn_decoder""):\n        print(""rnn_decoder_with_attention started..."")\n        state = initial_state  #[batch_size x cell.state_size].\n        _, hidden_size = state.get_shape().as_list() #200\n        attention_states_original=attention_states\n        batch_size,sequence_length,_=attention_states.get_shape().as_list()\n        outputs = []\n        prev = None\n        #################################################\n        for i, inp in enumerate(decoder_inputs):#\xe5\xbe\xaa\xe7\x8e\xaf\xe8\xa7\xa3\xe7\xa0\x81\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe3\x80\x82\xe5\xa6\x82sentence_length\xe4\xb8\xaa[batch_size x input_size]\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaftest, \xe5\xb0\x86t\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xbat + 1 \xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84s\xe8\xbe\x93\xe5\x85\xa5\n            if loop_function is not None and prev is not None:#\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x9a\xe5\xa6\x82\xe6\x9e\x9cloop_function\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\xe4\xb8\x94\xe5\x89\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84\xe5\x80\xbc\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe4\xbd\xbf\xe7\x94\xa8\xe5\x89\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9a\x84\xe5\x80\xbc\xe4\xbd\x9c\xe4\xb8\xbaRNN\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n                with tf.variable_scope(""loop_function"", reuse=True):\n                    inp = loop_function(prev, i)\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n            ##ATTENTION#################################################################################################################################################\n            # 1.get logits of attention for each encoder input. attention_states:[batch_size x attn_length x attn_size]; query=state:[batch_size x cell.state_size]\n            query=state\n            W_a = tf.get_variable(""W_a"", shape=[hidden_size, hidden_size],initializer=tf.random_normal_initializer(stddev=0.1))\n            query=tf.matmul(query, W_a) #[batch_size,hidden_size]\n            query=tf.expand_dims(query,axis=1) #[batch_size, 1, hidden_size]\n            U_a = tf.get_variable(""U_a"", shape=[hidden_size, hidden_size],initializer=tf.random_normal_initializer(stddev=0.1))\n            U_aa = tf.get_variable(""U_aa"", shape=[ hidden_size])\n            attention_states=tf.reshape(attention_states,shape=(-1,hidden_size)) #[batch_size*sentence_length,hidden_size]\n            attention_states=tf.matmul(attention_states, U_a) #[batch_size*sentence_length,hidden_size]\n            #print(""batch_size"",batch_size,"" ;sequence_length:"",sequence_length,"" ;hidden_size:"",hidden_size) #print(""attention_states:"", attention_states) #(?, 200)\n            attention_states=tf.reshape(attention_states,shape=(-1,sequence_length,hidden_size)) # TODO [batch_size,sentence_length,hidden_size]\n            #query_expanded:            [batch_size,1,             hidden_size]\n            #attention_states_reshaped: [batch_size,sentence_length,hidden_size]\n            attention_logits=tf.nn.tanh(query+attention_states+U_aa) #[batch_size,sentence_length,hidden_size]. additive style\n\n            # 2.get possibility of attention\n            attention_logits=tf.reshape(attention_logits,shape=(-1,hidden_size)) #batch_size*sequence_length [batch_size*sentence_length,hidden_size]\n            V_a = tf.get_variable(""V_a"", shape=[hidden_size,1],initializer=tf.random_normal_initializer(stddev=0.1)) #[hidden_size,1]\n            attention_logits=tf.matmul(attention_logits,V_a) #\xe6\x9c\x80\xe7\xbb\x88\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x98\xaf[batch_size*sentence_length,1]<-----[batch_size*sentence_length,hidden_size],[hidden_size,1]\n            attention_logits=tf.reshape(attention_logits,shape=(-1,sequence_length)) #attention_logits:[batch_size,sequence_length]\n            ##########################################################################################################################################################\n            #attention_logits=tf.reduce_sum(attention_logits,2)        #[batch_size x attn_length]\n            attention_logits_max=tf.reduce_max(attention_logits,axis=1,keep_dims=True) #[batch_size x 1]\n            # possibility distribution for each encoder input.it means how much attention or focus for each encoder input\n            p_attention=tf.nn.softmax(attention_logits-attention_logits_max)#[batch_size x attn_length]\n\n            # 3.get weighted sum of hidden state for each encoder input as attention state\n            p_attention=tf.expand_dims(p_attention,axis=2)            #[batch_size x attn_length x 1]\n            # attention_states:[batch_size x attn_length x attn_size]; p_attention:[batch_size x attn_length];\n            attention_final=tf.multiply(attention_states_original,p_attention) #[batch_size x attn_length x attn_size]\n            context_vector=tf.reduce_sum(attention_final,axis=1)     #[batch_size x attn_size]\n            ############################################################################################################################################################\n            #inp:[batch_size x input_size].it is decoder input;  attention_final:[batch_size x attn_size]\n            output, state = cell(inp, state,context_vector)          #attention_final TODO \xe4\xbd\xbf\xe7\x94\xa8RNN\xe8\xb5\xb0\xe4\xb8\x80\xe6\xad\xa5\n            outputs.append(output) # \xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe7\xbb\x93\xe6\x9e\x9c\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\n            if loop_function is not None:\n                prev = output\n    print(""rnn_decoder_with_attention ended..."")\n    return outputs, state'"
a06_Seq2seqWithAttention/a1_seq2seq_attention_model.py,79,"b'# -*- coding: utf-8 -*-\n# seq2seq_attention: 1.word embedding 2.encoder 3.decoder(optional with attention). for more detail, please check:Neural Machine Translation By Jointly Learning to Align And Translate\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.contrib as tf_contrib\nimport random\nimport copy\nfrom a1_seq2seq import rnn_decoder_with_attention,extract_argmax_and_embed\n\nclass seq2seq_attention_model:\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\n                 vocab_size, embed_size,hidden_size, is_training,decoder_sent_length=6,\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,l2_lambda=0.0001):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.is_training = is_training\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\n        self.initializer = initializer\n        self.decoder_sent_length=decoder_sent_length\n        self.hidden_size = hidden_size\n        self.clip_gradients=clip_gradients\n        self.l2_lambda=l2_lambda\n\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")                 #x\n        self.decoder_input = tf.placeholder(tf.int32, [None, self.decoder_sent_length],name=""decoder_input"")  #y, but shift\n        self.input_y_label = tf.placeholder(tf.int32, [None, self.decoder_sent_length], name=""input_y_label"") #y, but shift\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #logits shape:[batch_size,decoder_sent_length,self.num_classes]\n\n        self.predictions = tf.argmax(self.logits, axis=2, name=""predictions"")\n        self.accuracy = tf.constant(0.5)  # fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n        if not is_training:\n            return\n        self.loss_val = self.loss_seq2seq()\n        self.train_op = self.train()\n\n    def inference(self):\n        """"""main computation graph here:\n        #1.Word embedding. 2.Encoder with GRU 3.Decoder using GRU(optional with attention).""""""\n        ###################################################################################################################################\n        # 1.embedding of words\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x)  #[None, self.sequence_length, self.embed_size]\n        # 2.encoder with GRU\n        # 2.1 forward gru\n        hidden_state_forward_list = self.gru_forward(self.embedded_words,self.gru_cell)  # a list,length is sentence_length, each element is [batch_size,hidden_size]\n        # 2.2 backward gru\n        hidden_state_backward_list = self.gru_forward(self.embedded_words,self.gru_cell,reverse=True)  # a list,length is sentence_length, each element is [batch_size*num_sentences,hidden_size]\n        # 2.3 concat forward hidden state and backward hidden state. hidden_state: a list.len:sentence_length,element:[batch_size*num_sentences,hidden_size*2]\n        thought_vector_list=[tf.concat([h_forward,h_backward],axis=1) for h_forward,h_backward in zip(hidden_state_forward_list,hidden_state_backward_list)]#list,len:sent_len,e:[batch_size,hidden_size*2]\n\n        # 3.Decoder using GRU with attention\n        thought_vector=tf.stack(thought_vector_list,axis=1) #shape:[batch_size,sentence_length,hidden_size*2]\n        #initial_state=tf.reduce_sum(thought_vector,axis=1) #[batch_size,hidden_size*2] #TODO NEED TO TEST WHICH ONE IS BETTER: SUM UP OR USE LAST HIDDEN STATE==>similiarity.\n        initial_state=tf.nn.tanh(tf.matmul(hidden_state_backward_list[0],self.W_initial_state)+self.b_initial_state) #initial_state:[batch_size,hidden_size*2]. TODO this is follow paper\'s way.\n        cell=self.gru_cell_decoder #this is a special cell. because it beside previous hidden state, current input, it also has a context vecotor, which represent attention result.\n\n        output_projection=(self.W_projection,self.b_projection) #W_projection:[self.hidden_size * 2, self.num_classes]; b_projection:[self.num_classes]\n        loop_function = extract_argmax_and_embed(self.Embedding_label,output_projection) if not self.is_training else None #loop function will be used only at testing, not training.\n        attention_states=thought_vector #[None, self.sequence_length, self.embed_size]\n        decoder_input_embedded=tf.nn.embedding_lookup(self.Embedding_label,self.decoder_input) #[batch_size,self.decoder_sent_length,embed_size]\n        decoder_input_splitted = tf.split(decoder_input_embedded, self.decoder_sent_length,axis=1)  # it is a list,length is decoder_sent_length, each element is [batch_size,1,embed_size]\n        decoder_input_squeezed = [tf.squeeze(x, axis=1) for x in decoder_input_splitted]  # it is a list,length is decoder_sent_length, each element is [batch_size,embed_size]\n\n        #rnn_decoder_with_attention(decoder_inputs, initial_state, cell, loop_function,attention_states,scope=None):\n            #input1:decoder_inputs:target, shift by one. for example.the target is:""X Y Z"",then decoder_inputs should be:""START X Y Z"" A list of 2D Tensors [batch_size x input_size].\n            #input2:initial_state: 2D Tensor with shape  [batch_size x cell.state_size].\n            #input3:attention_states:represent X. 3D Tensor [batch_size x attn_length x attn_size].\n            #output:?\n        outputs, final_state=rnn_decoder_with_attention(decoder_input_squeezed, initial_state, cell, loop_function, attention_states, scope=None) # A list.length:decoder_sent_length.each element is:[batch_size x output_size]\n        decoder_output=tf.stack(outputs,axis=1) #decoder_output:[batch_size,decoder_sent_length,hidden_size*2]\n        decoder_output=tf.reshape(decoder_output,shape=(-1,self.hidden_size*2)) #decoder_output:[batch_size*decoder_sent_length,hidden_size*2]\n\n        with tf.name_scope(""dropout""):\n            decoder_output = tf.nn.dropout(decoder_output,keep_prob=self.dropout_keep_prob)  # shape:[None,hidden_size*4]\n        # 4. get logits\n        with tf.name_scope(""output""):\n            logits = tf.matmul(decoder_output, self.W_projection) + self.b_projection  # logits shape:[batch_size*decoder_sent_length,self.num_classes]==tf.matmul([batch_size*decoder_sent_length,hidden_size*2],[hidden_size*2,self.num_classes])\n            logits=tf.reshape(logits,shape=(self.batch_size,self.decoder_sent_length,self.num_classes)) #logits shape:[batch_size,decoder_sent_length,self.num_classes]\n        ###################################################################################################################################\n        return logits\n\n    def loss_seq2seq(self):\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y_label, logits=self.logits);#losses:[batch_size,self.decoder_sent_length]\n            loss_batch=tf.reduce_sum(losses,axis=1)/self.decoder_sent_length #loss_batch:[batch_size]\n            loss=tf.reduce_mean(loss_batch)\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * self.l2_lambda\n            loss = loss + l2_losses\n            return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        self.learning_rate_=learning_rate\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\n        train_op = tf_contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\n        return train_op\n\n    def gru_cell(self, Xt, h_t_minus_1):\n        """"""\n        single step of gru for word level\n        :param Xt: Xt:[batch_size,embed_size]\n        :param h_t_minus_1:[batch_size,embed_size]\n        :return:\n        """"""\n        # 1.update gate: decides how much past information is kept and how much new information is added.\n        z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z) + tf.matmul(h_t_minus_1,self.U_z) + self.b_z)  # z_t:[batch_size,self.hidden_size]\n        # 2.reset gate: controls how much the past state contributes to the candidate state.\n        r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r) + tf.matmul(h_t_minus_1,self.U_r) + self.b_r)  # r_t:[batch_size,self.hidden_size]\n        # candiate state h_t~\n        h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h) +r_t * (tf.matmul(h_t_minus_1, self.U_h)) + self.b_h)  # h_t_candiate:[batch_size,self.hidden_size]\n        # new state: a linear combine of pervious hidden state and the current new state h_t~\n        h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate  # h_t:[batch_size*num_sentences,hidden_size]\n        return h_t\n\n    def gru_cell_decoder(self, Xt, h_t_minus_1,context_vector):\n        """"""\n        single step of gru for word level\n        :param Xt: Xt:[batch_size,embed_size]\n        :param h_t_minus_1:[batch_size,embed_size]\n        :param context_vector. [batch_size,embed_size].this represent the result from attention( weighted sum of input during current decoding step)\n        :return:\n        """"""\n        # 1.update gate: decides how much past information is kept and how much new information is added.\n        z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z_decoder) + tf.matmul(h_t_minus_1,self.U_z_decoder) +tf.matmul(context_vector,self.C_z_decoder)+self.b_z_decoder)  # z_t:[batch_size,self.hidden_size]\n        # 2.reset gate: controls how much the past state contributes to the candidate state.\n        r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r_decoder) + tf.matmul(h_t_minus_1,self.U_r_decoder) +tf.matmul(context_vector,self.C_r_decoder)+self.b_r_decoder)  # r_t:[batch_size,self.hidden_size]\n        # candiate state h_t~\n        h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h_decoder) +r_t * (tf.matmul(h_t_minus_1, self.U_h_decoder)) +tf.matmul(context_vector, self.C_h_decoder)+ self.b_h_decoder)  # h_t_candiate:[batch_size,self.hidden_size]\n        # new state: a linear combine of pervious hidden state and the current new state h_t~\n        h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate  # h_t:[batch_size*num_sentences,hidden_size]\n        return h_t,h_t\n\n    # forward gru for first level: word levels\n    def gru_forward(self, embedded_words,gru_cell, reverse=False):\n        """"""\n        :param embedded_words:[None,sequence_length, self.embed_size]\n        :return:forward hidden state: a list.length is sentence_length, each element is [batch_size,hidden_size]\n        """"""\n        # split embedded_words\n        embedded_words_splitted = tf.split(embedded_words, self.sequence_length,axis=1)  # it is a list,length is sentence_length, each element is [batch_size,1,embed_size]\n        embedded_words_squeeze = [tf.squeeze(x, axis=1) for x in embedded_words_splitted]  # it is a list,length is sentence_length, each element is [batch_size,embed_size]\n        h_t = tf.ones((self.batch_size,self.hidden_size))\n        h_t_list = []\n        if reverse:\n            embedded_words_squeeze.reverse()\n        for time_step, Xt in enumerate(embedded_words_squeeze):  # Xt: [batch_size,embed_size]\n            h_t = gru_cell(Xt,h_t) #h_t:[batch_size,embed_size]<------Xt:[batch_size,embed_size];h_t:[batch_size,embed_size]\n            h_t_list.append(h_t)\n        if reverse:\n            h_t_list.reverse()\n        return h_t_list  # a list,length is sentence_length, each element is [batch_size,hidden_size]\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""decoder_init_state""):\n            self.W_initial_state = tf.get_variable(""W_initial_state"", shape=[self.hidden_size, self.hidden_size*2], initializer=self.initializer)\n            self.b_initial_state = tf.get_variable(""b_initial_state"", shape=[self.hidden_size*2])\n        with tf.name_scope(""embedding_projection""):  # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.Embedding_label = tf.get_variable(""Embedding_label"", shape=[self.num_classes, self.embed_size*2],dtype=tf.float32) #,initializer=self.initializer\n            self.W_projection = tf.get_variable(""W_projection"", shape=[self.hidden_size*2, self.num_classes],\n                                                initializer=self.initializer)  # [embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"", shape=[self.num_classes])\n\n        # GRU parameters:update gate related\n        with tf.name_scope(""gru_weights_encoder""):\n            self.W_z = tf.get_variable(""W_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_z = tf.get_variable(""U_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_z = tf.get_variable(""b_z"", shape=[self.hidden_size])\n            # GRU parameters:reset gate related\n            self.W_r = tf.get_variable(""W_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_r = tf.get_variable(""U_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_r = tf.get_variable(""b_r"", shape=[self.hidden_size])\n\n            self.W_h = tf.get_variable(""W_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.U_h = tf.get_variable(""U_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\n            self.b_h = tf.get_variable(""b_h"", shape=[self.hidden_size])\n\n        with tf.name_scope(""gru_weights_decoder""):\n            self.W_z_decoder = tf.get_variable(""W_z_decoder"", shape=[self.embed_size*2, self.hidden_size*2], initializer=self.initializer)\n            self.U_z_decoder = tf.get_variable(""U_z_decoder"", shape=[self.embed_size*2, self.hidden_size*2], initializer=self.initializer)\n            self.C_z_decoder = tf.get_variable(""C_z_decoder"", shape=[self.embed_size * 2, self.hidden_size * 2],initializer=self.initializer) #TODO\n            self.b_z_decoder = tf.get_variable(""b_z_decoder"", shape=[self.hidden_size*2])\n            # GRU parameters:reset gate related\n            self.W_r_decoder = tf.get_variable(""W_r_decoder"", shape=[self.embed_size*2, self.hidden_size*2], initializer=self.initializer)\n            self.U_r_decoder = tf.get_variable(""U_r_decoder"", shape=[self.embed_size*2, self.hidden_size*2], initializer=self.initializer)\n            self.C_r_decoder = tf.get_variable(""C_r_decoder"", shape=[self.embed_size * 2, self.hidden_size * 2],initializer=self.initializer) #TODO\n            self.b_r_decoder = tf.get_variable(""b_r_decoder"", shape=[self.hidden_size*2])\n\n            self.W_h_decoder = tf.get_variable(""W_h_decoder"", shape=[self.embed_size*2, self.hidden_size*2], initializer=self.initializer)\n            self.U_h_decoder = tf.get_variable(""U_h_decoder"", shape=[self.embed_size*2, self.hidden_size*2], initializer=self.initializer)   #TODO\n            self.C_h_decoder = tf.get_variable(""C_h_decoder"", shape=[self.embed_size * 2, self.hidden_size * 2],initializer=self.initializer)\n            self.b_h_decoder = tf.get_variable(""b_h_decoder"", shape=[self.hidden_size*2])\n\n        with tf.name_scope(""full_connected""):\n            self.W_fc=tf.get_variable(""W_fc"",shape=[self.hidden_size*2,self.hidden_size])\n            self.a_fc=tf.get_variable(""a_fc"",shape=[self.hidden_size])\n\n# test started: learn to output reverse sequence of itself.\ndef test():\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes = 9+2 #additional two classes:one is for _GO, another is for _END\n    learning_rate = 0.0001\n    batch_size = 1\n    decay_steps = 1000\n    decay_rate = 0.9\n    sequence_length = 5\n    vocab_size = 300\n    embed_size = 100 #100\n    hidden_size = 100\n    is_training = True\n    dropout_keep_prob = 1  # 0.5 #num_sentences\n    decoder_sent_length=6\n    l2_lambda=0.0001\n    model = seq2seq_attention_model(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\n                                    vocab_size, embed_size,hidden_size, is_training,decoder_sent_length=decoder_sent_length,l2_lambda=l2_lambda)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(1500):\n            #input_x = np.zeros((batch_size, sequence_length),dtype=np.int32) #[None, self.sequence_length]\n            label_list=get_unique_labels()\n            input_x = np.array([label_list],dtype=np.int32) #[2,3,4,5,6]\n            label_list_original=copy.deepcopy(label_list)\n            label_list.reverse()\n            decoder_input=np.array([[0]+label_list],dtype=np.int32) #[[0,2,3,4,5,6]]\n            input_y_label=np.array([label_list+[1]],dtype=np.int32) #[[2,3,4,5,6,1]]\n            loss, acc, predict, W_projection_value, _ = sess.run([model.loss_val, model.accuracy, model.predictions, model.W_projection, model.train_op],\n                                                     feed_dict={model.input_x:input_x,model.decoder_input:decoder_input, model.input_y_label: input_y_label,\n                                                                model.dropout_keep_prob: dropout_keep_prob})\n            print(i,""loss:"", loss, ""acc:"", acc, ""label_list_original as input x:"",label_list_original,"";input_y_label:"", input_y_label, ""prediction:"", predict)\n\ndef get_unique_labels():\n    x=[2,3,4,5,6]\n    random.shuffle(x)\n    return x\ntest()'"
a06_Seq2seqWithAttention/a1_seq2seq_attention_predict.py,29,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\nfrom a1_seq2seq_attention_model import seq2seq_attention_model\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999+3,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_seq2seq_attention/seq2seq_attention1/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""validate_step"", 1000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\ntf.app.flags.DEFINE_integer(""num_sentences"", 4, ""number of sentences in the document"") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\ntf.app.flags.DEFINE_string(""predict_target_file"",""checkpoint_seq2seq_attention/seq2seq_attention1//zhihu_result_seq2seq_attention.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_integer(""decoder_sent_length"",6,""length of decoder inputs"")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n# 1.load data with vocabulary of words and labels\n_GO=""_GO""\n_END=""_END""\n_PAD=""_PAD""\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""seq2seq_attention"")  # simple=\'simple\'\n    vocab_size = len(vocabulary_word2index)\n    print(""seq2seq_attention.vocab_size:"", vocab_size)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""seq2seq_attention"",use_seq2seq=True)\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        model=seq2seq_attention_model(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\n                                      vocab_size, FLAGS.embed_size,FLAGS.hidden_size, FLAGS.is_training,decoder_sent_length=FLAGS.decoder_sent_length,l2_lambda=FLAGS.l2_lambda)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        decoder_input=np.reshape(np.array([vocabulary_word2index_label[_GO]]+[vocabulary_word2index_label[_PAD]]*(FLAGS.decoder_sent_length-1)),[-1,FLAGS.decoder_sent_length])\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            predictions,logits=sess.run([model.predictions,model.logits],feed_dict={model.input_x:testX2[start:end],model.decoder_input:decoder_input,model.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],predictions,vocabulary_index2word_label,vocabulary_word2index_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\ndef get_label_using_logits(logits, predictions,vocabulary_index2word_label,vocabulary_word2index_label, top_number=5):\n    #print(""logits:"",logits.shape) #(6, 2002)\n    result_list=[]\n    for i,row in enumerate(logits):\n        #print(""i,"",i,""row:"",row)\n        if i!=len(logits)-1: #not include result from last column, which usually it should be <END> TOKEN.\n            label=process_each_row_get_lable(row,vocabulary_index2word_label,vocabulary_word2index_label,result_list)\n            result_list.append(label)\n    return result_list\n\ndef process_each_row_get_lable(row,vocabulary_index2word_label,vocabulary_word2index_label,result_list):\n    """"""\n    :param row: it is a list.length is number of labels. e.g. 2002\n    :param vocabulary_index2word_label\n    :param result_list\n    :return: a lable\n    """"""\n    label_list=list(np.argsort(row))\n    label_list.reverse()\n    #print(""label_list:"",label_list) # a list,length is number of labels.\n    for i,index in enumerate(label_list): # if index is not exists, and not _PAD,_END, then it is the label we want.\n        #print(i,""index:"",index)\n        flag1=vocabulary_index2word_label[index] not in result_list\n        flag2=index!=vocabulary_word2index_label[_PAD]\n        flag3=index!=vocabulary_word2index_label[_END]\n        if flag1 and flag2 and flag3:\n            #print(""going to return "")\n            return vocabulary_index2word_label[index]\n\ndef get_label_using_logitsO(pred_list, vocabulary_index2word_label,vocabulary_word2index_label, top_number=5):\n    print(""pred_list[0]:"",pred_list[0]) #(6, 2002)  for example.e.g. array([ 310, 1541,   75,    1,    1,    1])\n    result_list=[]\n    pred_list_=pred_list.tolist()[0]\n    print(""pred_list_:"",pred_list_)\n    for index in pred_list_:\n        print(""index:"",index)\n        word=vocabulary_index2word_label[index]\n        print(""word:"",word) #(\'index:\', 2, \';word:\', \'_PAD\')\n        result_list.append(word)\n    return result_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a06_Seq2seqWithAttention/a1_seq2seq_attention_train.py,29,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom a1_seq2seq_attention_model import  seq2seq_attention_model\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os,math\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999+3,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_seq2seq_attention/seq2seq_attention1/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""validate_step"", 1000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\ntf.app.flags.DEFINE_integer(""num_sentences"", 4, ""number of sentences in the document"") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""seq2seq_attention"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""seq2seq_attention.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""seq2seq_attention"",use_seq2seq=True)\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #train\n        train,test,_=load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,use_seq2seq=True,traning_data_path=FLAGS.traning_data_path)\n        trainX, trainY,train_decoder_input = train\n        testX, testY,test_decoder_input = test\n\n        print(""trainY:"",trainY[0:10])\n        print(""train_decoder_input:"",train_decoder_input[0:10])\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        #num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,num_sentences,vocab_size,embed_size,\n        #hidden_size,is_training\n        model=seq2seq_attention_model(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\n                                      vocab_size, FLAGS.embed_size,FLAGS.hidden_size, FLAGS.is_training,l2_lambda=FLAGS.l2_lambda)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(model.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        print(""number_of_training_data:"",number_of_training_data)\n        previous_eval_loss=10000\n        best_eval_loss=10000\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {model.input_x: trainX[start:end],model.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[model.input_y] = trainY[start:end]\n                else:\n                    feed_dict[model.input_y_label]=trainY[start:end]\n                    feed_dict[model.decoder_input] = train_decoder_input[start:end]\n                curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %50==0:\n                    print(""seq2seq_with_attention==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,math.exp(loss/float(counter)) if (loss/float(counter))<20 else 10000.000,acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n                if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0): #(epoch % FLAGS.validate_every) or  if epoch % FLAGS.validate_every == 0:\n                    eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label,eval_decoder_input=test_decoder_input)\n                    print(""seq2seq_with_attention.validation.part. previous_eval_loss:"", math.exp(previous_eval_loss) if previous_eval_loss<20 else 10000.000,"";current_eval_loss:"", math.exp(eval_loss) if eval_loss<20 else 10000.000)\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\n                        # reduce the learning rate by a factor of 0.5\n                        print(""seq2seq_with_attention==>validation.part.going to reduce the learning rate."")\n                        learning_rate1 = sess.run(model.learning_rate)\n                        lrr=sess.run([model.learning_rate_decay_half_op])\n                        learning_rate2 = sess.run(model.learning_rate)\n                        print(""seq2seq_with_attention==>validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\n                    #print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_acc))\n                    else:# loss is decreasing\n                        if eval_loss<best_eval_loss:\n                            print(""seq2seq_with_attention==>going to save the model.eval_loss:"",math.exp(eval_loss) if eval_loss<20 else 10000.000,"";best_eval_loss:"",math.exp(best_eval_loss) if best_eval_loss<20 else 10000.000)\n                            # save model to checkpoint\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                            saver.save(sess, save_path, global_step=epoch)\n                            best_eval_loss=eval_loss\n                    previous_eval_loss = eval_loss\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(model.epoch_increment)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label,eval_decoder_input=test_decoder_input)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,model,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(model.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,model,evalX,evalY,batch_size,vocabulary_index2word_label,eval_decoder_input=None):\n    #ii=0\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {model.input_x: evalX[start:end], model.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[model.input_y] = evalY[start:end]\n        else:\n            feed_dict[model.input_y_label] = evalY[start:end]\n            feed_dict[model.decoder_input] = eval_decoder_input[start:end]\n        curr_eval_loss, logits,curr_eval_acc,pred= sess.run([model.loss_val,model.logits,model.accuracy,model.predictions],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n        #if ii<20:\n            #print(""1.evalX[start:end]:"",evalX[start:end])\n            #print(""2.evalY[start:end]:"", evalY[start:end])\n            #print(""3.pred:"",pred)\n            #ii=ii+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a07_Transformer/a2_attention_between_enc_dec.py,6,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nattention connect encoder and decoder\r\nIn ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,\r\nand the memory keys and values come from the output of the encoder. This allows every\r\nposition in the decoder to attend over all positions in the input sequence. This mimics the\r\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models.\r\n""""""\r\nimport tensorflow as tf\r\nfrom a2_base_model import BaseClass\r\nfrom  a2_multi_head_attention import MultiHeadAttention\r\nimport time\r\nclass AttentionEncoderDecoder(BaseClass):\r\n    def __init__(self,d_model, d_k, d_v, sequence_length, h, batch_size,Q, K_s,layer_index,decoder_sent_length,type=""attention"",mask=None,dropout_keep_prob=None):\r\n        """"""\r\n        :param d_model:\r\n        :param d_k:\r\n        :param d_v:\r\n        :param sequence_length:\r\n        :param h:\r\n        :param batch_size:\r\n        :param Q: value from decoder\r\n        :param K_s: output of encoder\r\n        """"""\r\n        super(AttentionEncoderDecoder, self).__init__(d_model, d_k, d_v, sequence_length, h, batch_size)\r\n        self.Q=Q\r\n        self.K_s=K_s\r\n        self.layer_index=layer_index\r\n        self.type=type\r\n        self.decoder_sent_length=decoder_sent_length\r\n        self.initializer = tf.random_normal_initializer(stddev=0.1)\r\n        self.mask=mask\r\n        self.dropout_keep_prob=dropout_keep_prob\r\n\r\n    def attention_encoder_decoder_fn(self):\r\n        #call multi head attention function to perform this task.\r\n        return self.sub_layer_multi_head_attention(self.layer_index,self.Q,self.K_s,self.type,mask=self.mask,dropout_keep_prob=self.dropout_keep_prob)\r\n\r\n\r\n#test attention between encoder and decoder\r\ndef test():\r\n    start = time.time()\r\n    d_model = 512\r\n    d_k = 64\r\n    d_v = 64\r\n    sequence_length = 600\r\n    decoder_sent_length=600 #6\r\n    h = 8\r\n    batch_size=128\r\n    initializer = tf.random_normal_initializer(stddev=0.1)\r\n    # 2.set Q,K,V\r\n    vocab_size=1000\r\n    embed_size=d_model\r\n    Embedding = tf.get_variable(""Embedding"", shape=[vocab_size, embed_size],initializer=initializer)\r\n    input_x = tf.placeholder(tf.int32, [batch_size,decoder_sent_length], name=""input_x"") #[4,10]\r\n    embedded_words = tf.nn.embedding_lookup(Embedding, input_x) #[batch_size,decoder_sent_length,embed_size]\r\n\r\n    Q = embedded_words  #[batch_size*decoder_sent_length,embed_size]\r\n    K_s = tf.ones((batch_size,sequence_length,embed_size),dtype=tf.float32)  # [batch_size*sequence_length,embed_size]\r\n    layer_index = 0\r\n    mask=None #TODO\r\n    attention_between_encoder_decoder_class=AttentionEncoderDecoder(d_model, d_k, d_v, sequence_length, h, batch_size,Q, K_s,layer_index,decoder_sent_length,mask=mask)\r\n    attention_output=attention_between_encoder_decoder_class.attention_encoder_decoder_fn()\r\n    end = time.time()\r\n    print(""embedded_words:"",embedded_words,"" ;attention_output:"",attention_output,"";time spent:"",(end-start))\r\n#test()'"
a07_Transformer/a2_base_model.py,3,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nfrom  a2_multi_head_attention import MultiHeadAttention\r\nfrom a2_poistion_wise_feed_forward import PositionWiseFeedFoward\r\nfrom a2_layer_norm_residual_conn import LayerNormResidualConnection\r\nclass BaseClass(object):\r\n    """"""\r\n    base class has some common fields and functions.\r\n    """"""\r\n    def __init__(self,d_model,d_k,d_v,sequence_length,h,batch_size,num_layer=6,type=\'encoder\',decoder_sent_length=None):\r\n        """"""\r\n        :param d_model:\r\n        :param d_k:\r\n        :param d_v:\r\n        :param sequence_length:\r\n        :param h:\r\n        :param batch_size:\r\n        :param embedded_words: shape:[batch_size,sequence_length,embed_size]\r\n        """"""\r\n        self.d_model=d_model\r\n        self.d_k=d_k\r\n        self.d_v=d_v\r\n        self.sequence_length=sequence_length\r\n        self.h=h\r\n        self.num_layer=num_layer\r\n        self.batch_size=batch_size\r\n        self.type=type\r\n        self.decoder_sent_length=decoder_sent_length\r\n\r\n    def sub_layer_postion_wise_feed_forward(self ,x ,layer_index,type)  :# COMMON FUNCTION\r\n        """"""\r\n        :param x: shape should be:[batch_size,sequence_length,d_model]\r\n        :param layer_index: index of layer number\r\n        :param type: encoder,decoder or encoder_decoder_attention\r\n        :return: [batch_size,sequence_length,d_model]\r\n        """"""\r\n        with tf.variable_scope(""sub_layer_postion_wise_feed_forward"" + type + str(layer_index)):\r\n            postion_wise_feed_forward = PositionWiseFeedFoward(x, layer_index)\r\n            postion_wise_feed_forward_output = postion_wise_feed_forward.position_wise_feed_forward_fn()\r\n        return postion_wise_feed_forward_output\r\n\r\n    def sub_layer_multi_head_attention(self ,layer_index ,Q ,K_s,type,mask=None,is_training=None,dropout_keep_prob=None)  :# COMMON FUNCTION\r\n        """"""\r\n        multi head attention as sub layer\r\n        :param layer_index: index of layer number\r\n        :param Q: shape should be: [batch_size,sequence_length,embed_size]\r\n        :param k_s: shape should be: [batch_size,sequence_length,embed_size]\r\n        :param type: encoder,decoder or encoder_decoder_attention\r\n        :param mask: when use mask,illegal connection will be mask as huge big negative value.so it\'s possiblitity will become zero.\r\n        :return: output of multi head attention.shape:[batch_size,sequence_length,d_model]\r\n        """"""\r\n        with tf.variable_scope(""base_mode_sub_layer_multi_head_attention_"" + type+str(layer_index)):\r\n            # below is to handle attention for encoder and decoder with difference length:\r\n            #length=self.decoder_sent_length if (type!=\'encoder\' and self.sequence_length!=self.decoder_sent_length) else self.sequence_length #TODO this may be useful\r\n            length=self.sequence_length\r\n            #1. get V as learned parameters\r\n            #V_s = tf.get_variable(""V_s"", shape=(self.batch_size,length,self.d_model),initializer=self.initializer) # REMOVED 2018.11.16\r\n            V_s=K\r\n            #2. call function of multi head attention to get result\r\n            multi_head_attention_class = MultiHeadAttention(Q, K_s, V_s, self.d_model, self.d_k, self.d_v, self.sequence_length,\r\n                                                            self.h,type=type,is_training=is_training,mask=mask,dropout_rate=(1.0-dropout_keep_prob))\r\n            sub_layer_multi_head_attention_output = multi_head_attention_class.multi_head_attention_fn()  # [batch_size*sequence_length,d_model]\r\n        return sub_layer_multi_head_attention_output  # [batch_size,sequence_length,d_model]\r\n\r\n    def sub_layer_layer_norm_residual_connection(self,layer_input ,layer_output,layer_index,type,dropout_keep_prob=None,use_residual_conn=True): # COMMON FUNCTION\r\n        """"""\r\n        layer norm & residual connection\r\n        :param input: [batch_size,equence_length,d_model]\r\n        :param output:[batch_size,sequence_length,d_model]\r\n        :return:\r\n        """"""\r\n        print(""@@@========================>layer_input:"",layer_input,"";layer_output:"",layer_output)\r\n        #assert layer_input.get_shape().as_list()==layer_output.get_shape().as_list()\r\n        #layer_output_new= layer_input+ layer_output\r\n        layer_norm_residual_conn=LayerNormResidualConnection(layer_input,layer_output,layer_index,type,residual_dropout=(1-dropout_keep_prob),use_residual_conn=use_residual_conn)\r\n        output = layer_norm_residual_conn.layer_norm_residual_connection()\r\n        return output  # [batch_size,sequence_length,d_model]'"
a07_Transformer/a2_decoder.py,10,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDecoder:\r\n1. The decoder is composed of a stack of N= 6 identical layers.\r\n2. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\r\nattention over the output of the encoder stack.\r\n3. Similar to the encoder, we employ residual connections\r\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\r\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This\r\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\r\npredictions for position i can depend only on the known outputs at positions less than i.\r\n""""""\r\nimport tensorflow as tf\r\nfrom a2_base_model import BaseClass\r\nfrom a2_attention_between_enc_dec import AttentionEncoderDecoder\r\nimport time\r\n\r\nclass Decoder(BaseClass):\r\n    def __init__(self,d_model,d_k,d_v,sequence_length,h,batch_size,Q,K_s,K_v_encoder,decoder_sent_length,\r\n                 num_layer=6,type=\'decoder\',is_training=True,mask=None,dropout_keep_prob=None):\r\n        """"""\r\n        :param d_model:\r\n        :param d_k:\r\n        :param d_v:\r\n        :param sequence_length:\r\n        :param h:\r\n        :param batch_size:\r\n        :param Q:\r\n        :param K_s:\r\n        :param K_v_encoder: shape:[batch_size,sequence_length,embed_size]. it is the output from encoder\r\n        """"""\r\n        super(Decoder, self).__init__(d_model, d_k, d_v, sequence_length, h, batch_size, num_layer=num_layer)\r\n        self.Q=Q\r\n        self.K_s=K_s\r\n        self.K_v_encoder=K_v_encoder\r\n        self.type=type\r\n        self.initializer = tf.random_normal_initializer(stddev=0.1)\r\n        self.is_training=is_training\r\n        self.decoder_sent_length=decoder_sent_length\r\n        self.mask=mask\r\n        self.dropout_keep_prob=dropout_keep_prob\r\n\r\n    def decoder_fn(self):\r\n        start = time.time()\r\n        print(""decoder.decoder_fn.started."")\r\n        Q=self.Q\r\n        K_s=self.K_s\r\n        for layer_index in range(self.num_layer):\r\n            Q,K_s=self.decoder_single_layer(Q, K_s, layer_index)\r\n        end = time.time()\r\n        print(""decoder.decoder_fn.ended.Q:"", Q, "" ;K_s:"", K_s,"";time spent:"",(end-start))\r\n        return Q,K_s\r\n\r\n    def decoder_single_layer(self,Q,K_s,layer_index):\r\n        """"""\r\n        singel layer for decoder. each layers has three sub-layers:\r\n        the first is multi-head self-attention(mask) mechanism;\r\n        the second is multi-head attention over output of encoder\r\n        the third is position-wise fully connected feed-forward network.\r\n        for each sublayer. use LayerNorm(x+Sublayer(x)). input and output of last dimension: d_model=512s.\r\n        :param Q: shape should be:       [batch_size,sequence_length,d_model]\r\n        :param K_s: shape should be:     [batch_size,sequence_length,d_model]\r\n        :param layer_index: index of layer\r\n        :param mask: mask is a list. length is sequence_length. each element is a scaler value. e.g. [1,1,1,-1000000,-1000000,-1000000,....-1000000]\r\n        :return:output: shape should be:[batch_size*sequence_length,d_model]\r\n        """"""\r\n        print(""#decoder#decoder_single_layer"",layer_index,""====================================>"")\r\n        # 1.1 the first is masked multi-head self-attention mechanism\r\n        multi_head_attention_output=self.sub_layer_multi_head_attention(layer_index,Q,K_s,self.type,is_training=self.is_training,mask=self.mask,dropout_keep_prob=self.dropout_keep_prob) #[batch_size*sequence_length,d_model]\r\n        #1.2 use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        multi_head_attention_output=self.sub_layer_layer_norm_residual_connection(K_s,multi_head_attention_output,layer_index,\'decoder_multi_head_attention\',dropout_keep_prob=self.dropout_keep_prob)\r\n\r\n\r\n        # 2.1 the second is multi-head attention over output of encoder\r\n        # IMPORTANT!!! check two parameters below: Q: should from decoder; K_s: should be the output of encoder\r\n        attention_enc_dec=AttentionEncoderDecoder(self.d_model,self.d_k,self.d_v,self.sequence_length,self.h,self.batch_size,\r\n                                                  multi_head_attention_output,self.K_v_encoder,layer_index,self.decoder_sent_length,dropout_keep_prob=self.dropout_keep_prob)\r\n        attention_enc_dec_output=attention_enc_dec.attention_encoder_decoder_fn()\r\n        print(""decoder.2.1.attention_enc_dec_output:"",attention_enc_dec_output)\r\n        # 2.2 use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        attention_enc_dec_output=self.sub_layer_layer_norm_residual_connection(multi_head_attention_output,attention_enc_dec_output, layer_index,\r\n                                                                               \'decoder_attention_encoder_decoder\',dropout_keep_prob=self.dropout_keep_prob)\r\n\r\n        # 3.1 the second is position-wise fully connected feed-forward network.\r\n        postion_wise_feed_forward_output=self.sub_layer_postion_wise_feed_forward(attention_enc_dec_output,layer_index,self.type)\r\n        print(""decoder.3.1.postion_wise_feed_forward_output:"",postion_wise_feed_forward_output)\r\n        #3.2 use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        postion_wise_feed_forward_output=self.sub_layer_layer_norm_residual_connection(attention_enc_dec_output,postion_wise_feed_forward_output, layer_index, \'decoder_position_ff\',dropout_keep_prob=self.dropout_keep_prob)\r\n        return postion_wise_feed_forward_output,postion_wise_feed_forward_output\r\n\r\n#####################BELOW IS TEST METHOD FOR DECODER: FROM SMALL FUNCTION  TO WHOLE FUNCTION OF DECODER\xef\xbc\x9a################################################\r\n#test decoder for single layer\r\n\r\ndef get_mask(sequence_length):\r\n    lower_triangle=tf.matrix_band_part(tf.ones([sequence_length,sequence_length]),-1,0)\r\n    result=-1e9*(1.0-lower_triangle)\r\n    print(""get_mask==>result:"",result)\r\n    return result\r\n\r\ndef init():\r\n    d_model = 512\r\n    d_k = 64\r\n    d_v = 64\r\n    sequence_length =6 #5\r\n    decoder_sent_length=6\r\n    h = 8\r\n    batch_size = 4*32\r\n    num_layer=6\r\n    # 2.set Q,K,V\r\n    vocab_size = 1000\r\n    embed_size = d_model\r\n    initializer = tf.random_normal_initializer(stddev=0.1)\r\n    Embedding = tf.get_variable(""Embedding_d"", shape=[vocab_size, embed_size], initializer=initializer)\r\n    decoder_input_x = tf.placeholder(tf.int32, [batch_size, decoder_sent_length], name=""input_x"")  # [4,10]\r\n    print(""1.decoder_input_x:"", decoder_input_x)\r\n    decoder_input_embedding = tf.nn.embedding_lookup(Embedding, decoder_input_x)  # [batch_size*sequence_length,embed_size]\r\n    #Q = embedded_words  # [batch_size*sequence_length,embed_size]\r\n    #K_s = embedded_words  # [batch_size*sequence_length,embed_size]\r\n    #K_v_encoder = tf.placeholder(tf.float32, [batch_size,decoder_sent_length, d_model], name=""input_x"") #sequence_length\r\n    Q = tf.placeholder(tf.float32, [batch_size,sequence_length, d_model], name=""input_x"")\r\n    K_s=decoder_input_embedding\r\n    K_v_encoder= tf.get_variable(""v_variable"",shape=[batch_size,decoder_sent_length, d_model],initializer=initializer) #tf.float32,\r\n    print(""2.output from encoder:"",K_v_encoder)\r\n    mask = get_mask(decoder_sent_length) #sequence_length\r\n    decoder = Decoder(d_model, d_k, d_v, sequence_length, h, batch_size, Q, K_s, K_v_encoder,decoder_sent_length,mask=mask,num_layer=num_layer)\r\n    return decoder,Q, K_s\r\n\r\ndecoder,Q, K_s=init()\r\n\r\ndef test_decoder_single_layer():\r\n    layer_index=0\r\n    print(""Q.get_shape().as_list():"",Q.get_shape().as_list())\r\n    sequence_length_unfold=Q.get_shape().as_list()[0]\r\n    mask=tf.ones((sequence_length_unfold)) #,dtype=tf.float32\r\n    decoder.decoder_single_layer(Q,K_s,layer_index,mask)\r\n\r\ndef test_decoder():\r\n    output=decoder.decoder_fn()\r\n    return output\r\n\r\n#1. test for single layer of decoder\r\n#test_decoder_single_layer()\r\n\r\n#2. test for decoder\r\n#decoder_output=test_decoder(); print(""3.decoder_output:"",decoder_output)\r\n\r\n\r\n'"
a07_Transformer/a2_encoder.py,8,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nencoder for the transformer:\r\n6 layers.each layers has two sub-layers.\r\nthe first is multi-head self-attention mechanism;\r\nthe second is position-wise fully connected feed-forward network.\r\nfor each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n""""""\r\n#TODO LAYER NORMALIZATION\r\nimport tensorflow as tf\r\nfrom a2_base_model import BaseClass\r\nimport time\r\nclass Encoder(BaseClass):\r\n    def __init__(self,d_model,d_k,d_v,sequence_length,h,batch_size,num_layer,Q,K_s,type=\'encoder\',mask=None,dropout_keep_prob=None,use_residual_conn=True):\r\n        """"""\r\n        :param d_model:\r\n        :param d_k:\r\n        :param d_v:\r\n        :param sequence_length:\r\n        :param h:\r\n        :param batch_size:\r\n        :param embedded_words: shape:[batch_size*sequence_length,embed_size]\r\n        """"""\r\n        super(Encoder, self).__init__(d_model,d_k,d_v,sequence_length,h,batch_size,num_layer=num_layer)\r\n        self.Q=Q\r\n        self.K_s=K_s\r\n        self.type=type\r\n        self.mask=mask\r\n        self.initializer = tf.random_normal_initializer(stddev=0.1)\r\n        self.dropout_keep_prob=dropout_keep_prob\r\n        self.use_residual_conn=use_residual_conn\r\n\r\n    def encoder_fn(self):\r\n        start = time.time()\r\n        print(""encoder_fn.started."")\r\n        Q=self.Q\r\n        K_s=self.K_s\r\n        for layer_index in range(self.num_layer):\r\n            Q, K_s=self.encoder_single_layer(Q,K_s,layer_index)\r\n            print(""encoder_fn."",layer_index,"".Q:"",Q,"";K_s:"",K_s)\r\n        end = time.time()\r\n        print(""encoder_fn.ended.Q:"",Q,"";K_s:"",K_s,"";time spent:"",(end-start))\r\n        return Q,K_s\r\n\r\n    def encoder_single_layer(self,Q,K_s,layer_index):\r\n        """"""\r\n        singel layer for encoder.each layers has two sub-layers:\r\n        the first is multi-head self-attention mechanism; the second is position-wise fully connected feed-forward network.\r\n        for each sublayer. use LayerNorm(x+Sublayer(x)). input and output of last dimension: d_model\r\n        :param Q: shape should be:       [batch_size*sequence_length,d_model]\r\n        :param K_s: shape should be:     [batch_size*sequence_length,d_model]\r\n        :return:output: shape should be:[batch_size*sequence_length,d_model]\r\n        """"""\r\n        #1.1 the first is multi-head self-attention mechanism\r\n        multi_head_attention_output=self.sub_layer_multi_head_attention(layer_index,Q,K_s,self.type,mask=self.mask,dropout_keep_prob=self.dropout_keep_prob) #[batch_size,sequence_length,d_model]\r\n        #1.2 use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        multi_head_attention_output=self.sub_layer_layer_norm_residual_connection(K_s ,multi_head_attention_output,layer_index,\'encoder_multi_head_attention\',dropout_keep_prob=self.dropout_keep_prob,use_residual_conn=self.use_residual_conn)\r\n\r\n        #2.1 the second is position-wise fully connected feed-forward network.\r\n        postion_wise_feed_forward_output=self.sub_layer_postion_wise_feed_forward(multi_head_attention_output,layer_index,self.type)\r\n        #2.2 use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        postion_wise_feed_forward_output= self.sub_layer_layer_norm_residual_connection(multi_head_attention_output,postion_wise_feed_forward_output,layer_index,\'encoder_postion_wise_ff\',dropout_keep_prob=self.dropout_keep_prob)\r\n        return  postion_wise_feed_forward_output,postion_wise_feed_forward_output\r\n\r\n\r\ndef init():\r\n    #1. assign value to fields\r\n    vocab_size=1000\r\n    d_model = 512\r\n    d_k = 64\r\n    d_v = 64\r\n    sequence_length = 5*10\r\n    h = 8\r\n    batch_size=4*32\r\n    initializer = tf.random_normal_initializer(stddev=0.1)\r\n    # 2.set values for Q,K,V\r\n    vocab_size=1000\r\n    embed_size=d_model\r\n    Embedding = tf.get_variable(""Embedding_E"", shape=[vocab_size, embed_size],initializer=initializer)\r\n    input_x = tf.placeholder(tf.int32, [batch_size,sequence_length], name=""input_x"") #[4,10]\r\n    print(""input_x:"",input_x)\r\n    embedded_words = tf.nn.embedding_lookup(Embedding, input_x) #[batch_size*sequence_length,embed_size]\r\n    Q = embedded_words  # [batch_size*sequence_length,embed_size]\r\n    K_s = embedded_words  # [batch_size*sequence_length,embed_size]\r\n    num_layer=6\r\n    mask = get_mask(batch_size, sequence_length)\r\n    #3. get class object\r\n    encoder_class=Encoder(d_model,d_k,d_v,sequence_length,h,batch_size,num_layer,Q,K_s,mask=mask) #Q,K_s,embedded_words\r\n    return encoder_class,Q,K_s\r\n\r\ndef get_mask(batch_size,sequence_length):\r\n    lower_triangle=tf.matrix_band_part(tf.ones([sequence_length,sequence_length]),-1,0)\r\n    result=-1e9*(1.0-lower_triangle)\r\n    print(""get_mask==>result:"",result)\r\n    return result\r\n\r\ndef test_sub_layer_multi_head_attention(encoder_class,index_layer,Q,K_s):\r\n    sub_layer_multi_head_attention_output=encoder_class.sub_layer_multi_head_attention(index_layer,Q,K_s)\r\n    return sub_layer_multi_head_attention_output\r\n\r\ndef test_postion_wise_feed_forward(encoder_class,x,layer_index):\r\n    sub_layer_postion_wise_feed_forward_output=encoder_class.sub_layer_postion_wise_feed_forward(x, layer_index)\r\n    return sub_layer_postion_wise_feed_forward_output\r\n\r\nencoder_class,Q,K_s=init()\r\n#index_layer=0\r\n\r\n#below is 4 callable codes for testing functions:from small function to whole function of encoder.\r\n\r\n#1.test 1: for sub layer of multi head attention\r\n#sub_layer_multi_head_attention_output=test_sub_layer_multi_head_attention(encoder_class,index_layer,Q,K_s)\r\n#print(""sub_layer_multi_head_attention_output1:"",sub_layer_multi_head_attention_output)\r\n\r\n#2. test 2: for sub layer of multi head attention with poistion-wise feed forward\r\n#d1,d2,d3=sub_layer_multi_head_attention_output.get_shape().as_list()\r\n#postion_wise_ff_input=tf.reshape(sub_layer_multi_head_attention_output,shape=[-1,d3])\r\n#sub_layer_postion_wise_feed_forward_output=test_postion_wise_feed_forward(encoder_class,postion_wise_ff_input,index_layer)\r\n#sub_layer_postion_wise_feed_forward_output=tf.reshape(sub_layer_postion_wise_feed_forward_output,shape=(d1,d2,d3))\r\n#print(""sub_layer_postion_wise_feed_forward_output2:"",sub_layer_postion_wise_feed_forward_output)\r\n\r\n#3.test 3: test for single layer of encoder\r\n#encoder_class.encoder_single_layer(Q,K_s,index_layer)\r\n\r\n#4.test 4: test for encoder. with N layers\r\n\r\n#Q,K_s = encoder_class.encoder_fn()'"
a07_Transformer/a2_layer_norm_residual_conn.py,8,"b'import tensorflow as tf\r\nimport time\r\n""""""\r\nWe employ a residual connection around each of the two sub-layers, followed by layer normalization.\r\nThat is, the output of each sub-layer is LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. """"""\r\nclass LayerNormResidualConnection(object):\r\n    def __init__(self,x,y,layer_index,type,residual_dropout=0.1,use_residual_conn=True):\r\n        self.x=x\r\n        self.y=y\r\n        self.layer_index=layer_index\r\n        self.type=type\r\n        self.residual_dropout=residual_dropout\r\n        self.use_residual_conn=use_residual_conn\r\n\r\n    #call residual connection and layer normalization\r\n    def layer_norm_residual_connection(self):\r\n        print(""LayerNormResidualConnection.use_residual_conn:"",self.use_residual_conn)\r\n        ##if self.use_residual_conn:\r\n        #    x_residual=self.residual_connection()\r\n        #    x_layer_norm=self.layer_normalization(x_residual)\r\n        #else:\r\n        x_layer_norm = self.layer_normalization(self.x)\r\n        return x_layer_norm\r\n\r\n    def residual_connection(self):\r\n        output=self.x + tf.nn.dropout(self.y, 1.0 - self.residual_dropout)\r\n        return output\r\n\r\n    # layer normalize the tensor x, averaging over the last dimension.\r\n    def layer_normalization(self,x):\r\n        """"""\r\n        x should be:[batch_size,sequence_length,d_model]\r\n        :return:\r\n        """"""\r\n        filter=x.get_shape()[-1] #last dimension of x. e.g. 512\r\n        print(""layer_normalization:==================>variable_scope:"",""layer_normalization""+str(self.layer_index)+self.type)\r\n        with tf.variable_scope(""layer_normalization""+str(self.layer_index)+self.type):\r\n            # 1. normalize input by using  mean and variance according to last dimension\r\n            mean=tf.reduce_mean(x,axis=-1,keep_dims=True) #[batch_size,sequence_length,1]\r\n            variance=tf.reduce_mean(tf.square(x-mean),axis=-1,keep_dims=True) #[batch_size,sequence_length,1]\r\n            norm_x=(x-mean)*tf.rsqrt(variance+1e-6) #[batch_size,sequence_length,d_model]\r\n            # 2. re-scale normalized input back\r\n            scale=tf.get_variable(""layer_norm_scale"",[filter],initializer=tf.ones_initializer) #[filter]\r\n            bias=tf.get_variable(""layer_norm_bias"",[filter],initializer=tf.ones_initializer) #[filter]\r\n            output=norm_x*scale+bias #[batch_size,sequence_length,d_model]\r\n            return output #[batch_size,sequence_length,d_model]\r\n\r\ndef test():\r\n    start = time.time()\r\n    batch_size=128\r\n    sequence_length=1000\r\n    d_model=512\r\n    x=tf.ones((batch_size,sequence_length,d_model))\r\n    y=x*3-0.5\r\n    layer_norm_residual_conn=LayerNormResidualConnection(x,y,0,\'encoder\')\r\n    output=layer_norm_residual_conn.layer_norm_residual_connection()\r\n    end = time.time()\r\n    print(""x:"",x,"";y:"",y,"";output:"",output,"";time spent:"",(end-start))\r\n\r\n#test()\r\n\r\n'"
a07_Transformer/a2_multi_head_attention.py,33,"b'# -*- coding: utf-8 -*-\r\n#test self-attention\r\nimport tensorflow as tf\r\nimport time\r\n""""""\r\nmulti head attention.\r\n1.linearly project the queries,keys and values h times(with different,learned linear projections to d_k,d_k,d_v dimensions)\r\n2.scaled dot product attention for each projected version of Q,K,V\r\n3.concatenated result\r\n4.linear projection to get final result\r\n\r\nthree kinds of usage:\r\n1. attention for encoder\r\n2. attention for decoder(need a mask to pay attention for only known position)\r\n3. attention as bridge of encoder and decoder\r\n""""""\r\nclass MultiHeadAttention(object):\r\n    """""" multi head attention""""""\r\n    def __init__(self,Q,K_s,V_s,d_model,d_k,d_v,sequence_length,h,type=None,is_training=None,mask=None,dropout_rate=0.1):\r\n        self.d_model=d_model\r\n        self.d_k=d_k\r\n        self.d_v=d_v\r\n        self.sequence_length=sequence_length\r\n        self.h=h\r\n        self.Q=Q\r\n        self.K_s=K_s\r\n        self.V_s=V_s\r\n        self.type=type\r\n        self.is_training=is_training\r\n        self.mask=mask\r\n        self.dropout_rate=dropout_rate\r\n        print(""MultiHeadAttention.self.dropout_rate:"",self.dropout_rate)\r\n\r\n    def multi_head_attention_fn(self):\r\n        """"""\r\n        multi head attention\r\n        :param Q: query.  shape:[batch,sequence_length,d_model]\r\n        :param K_s: keys. shape:[batch,sequence_length,d_model].\r\n        :param V_s:values.shape:[batch,sequence_length,d_model].\r\n        :param h: h times\r\n        :return: result of scaled dot product attention. shape:[sequence_length,d_model]\r\n        """"""\r\n        # 1. linearly project the queries,keys and values h times(with different,learned linear projections to d_k,d_k,d_v dimensions)\r\n        Q_projected   = tf.layers.dense(self.Q,units=self.d_model)     # [batch,sequence_length,d_model]\r\n        K_s_projected = tf.layers.dense(self.K_s, units=self.d_model)  # [batch,sequence_length,d_model]\r\n        V_s_projected = tf.layers.dense(self.V_s, units=self.d_model)  # [batch,sequence_length,d_model]\r\n        # 2. scaled dot product attention for each projected version of Q,K,V\r\n        dot_product=self.scaled_dot_product_attention_batch(Q_projected,K_s_projected,V_s_projected) # [batch,h,sequence_length,d_k]\r\n        # 3. concatenated\r\n        print(""dot_product:====================================================================================>"",dot_product) #dot_product:(128, 8, 6, 64)\r\n        batch_size,h,length,d_k=dot_product.get_shape().as_list()\r\n        print(""self.sequence_length:"",self.sequence_length) #5\r\n        dot_product=tf.reshape(dot_product,shape=(-1,length,self.d_model))\r\n        # 4. linear projection\r\n        output=tf.layers.dense(dot_product,units=self.d_model) # [batch,sequence_length,d_model]\r\n        return output  #[batch,sequence_length,d_model]\r\n\r\n    def scaled_dot_product_attention_batch_mine(self,Q,K_s,V_s): #my own implementation of scaled dot product attention.\r\n        """"""\r\n        scaled dot product attention\r\n        :param Q:  query.  shape:[batch,sequence_length,d_model]\r\n        :param K_s: keys.  shape:[batch,sequence_length,d_model]\r\n        :param V_s:values. shape:[batch,sequence_length,d_model]\r\n        :param mask:       shape:[batch,sequence_length]\r\n        :return: result of scaled dot product attention. shape:[batch,h,sequence_length,d_k]\r\n        """"""\r\n        # 1. split Q,K,V\r\n        Q_heads = tf.stack(tf.split(Q,self.h,axis=2),axis=1)         # [batch,h,sequence_length,d_k]\r\n        K_heads = tf.stack(tf.split(K_s, self.h, axis=2), axis=1)    # [batch,h,sequence_length,d_k]\r\n        V_heads = tf.stack(tf.split(V_s, self.h, axis=2), axis=1)    # [batch,h,sequence_length,d_k]\r\n        dot_product=tf.multiply(Q_heads,K_heads)                     # [batch,h,sequence_length,d_k]\r\n        # 2. dot product\r\n        dot_product=dot_product*(1.0/tf.sqrt(tf.cast(self.d_model,tf.float32))) # [batch,h,sequence_length,d_k]\r\n        dot_product=tf.reduce_sum(dot_product,axis=-1,keep_dims=True) # [batch,h,sequence_length,1]\r\n        # 3. add mask if it is none\r\n        if self.mask is not None:\r\n            mask = tf.expand_dims(self.mask, axis=-1)  # [batch,sequence_length,1]\r\n            mask = tf.expand_dims(mask, axis=1)  # [batch,1,sequence_length,1]\r\n            dot_product=dot_product+mask   # [batch,h,sequence_length,1]\r\n        # 4. get possibility\r\n        p=tf.nn.softmax(dot_product)                                  # [batch,h,sequence_length,1]\r\n        # 5. final output\r\n        output=tf.multiply(p,V_heads)                                 # [batch,h,sequence_length,d_k]\r\n        return output                                                 # [batch,h,sequence_length,d_k]\r\n\r\n    def scaled_dot_product_attention_batch(self, Q, K_s, V_s):# scaled dot product attention: implementation style like tensor2tensor from google\r\n        """"""\r\n        scaled dot product attention\r\n        :param Q:  query.  shape:[batch,sequence_length,d_model]\r\n        :param K_s: keys.  shape:[batch,sequence_length,d_model]\r\n        :param V_s:values. shape:[batch,sequence_length,d_model]\r\n        :param mask:       shape:[sequence_length,sequence_length]\r\n        :return: result of scaled dot product attention. shape:[batch,h,sequence_length,d_k]\r\n        """"""\r\n        # 1. split Q,K,V\r\n        Q_heads = tf.stack(tf.split(Q,self.h,axis=2),axis=1)                    # [batch,h,sequence_length,d_k]\r\n        K_heads = tf.stack(tf.split(K_s, self.h, axis=2), axis=1)               # [batch,h,sequence_length,d_k]\r\n        V_heads = tf.stack(tf.split(V_s, self.h, axis=2), axis=1)               # [batch,h,sequence_length,d_k]\r\n        # 2. dot product of Q,K\r\n        dot_product=tf.matmul(Q_heads,K_heads,transpose_b=True)                 # [batch,h,sequence_length,sequence_length]\r\n        dot_product=dot_product*(1.0/tf.sqrt(tf.cast(self.d_model,tf.float32))) # [batch,h,sequence_length,sequence_length]\r\n        # 3. add mask if it is none\r\n        print(""scaled_dot_product_attention_batch.===============================================================>mask is not none?"",self.mask is not None)\r\n        if self.mask is not None:\r\n            mask_expand=tf.expand_dims(tf.expand_dims(self.mask,axis=0),axis=0) # [1,1,sequence_length,sequence_length]\r\n            #dot_product:(128, 8, 6, 6);mask_expand:(1, 1, 5, 5)\r\n            print(""scaled_dot_product_attention_batch.===============================================================>dot_product:"",dot_product,"";mask_expand:"",mask_expand)\r\n            dot_product=dot_product+mask_expand                                 # [batch,h,sequence_length,sequence_length]\r\n        # 4.get possibility\r\n        weights=tf.nn.softmax(dot_product)                                      # [batch,h,sequence_length,sequence_length]\r\n        # drop out weights\r\n        weights=tf.nn.dropout(weights,1.0-self.dropout_rate)                    # [batch,h,sequence_length,sequence_length]\r\n        # 5. final output\r\n        output=tf.matmul(weights,V_heads)                                       # [batch,h,sequence_length,d_model]\r\n        return output\r\n\r\n\r\n#vectorized implementation of multi head attention for sentences with batch\r\ndef multi_head_attention_for_sentence_vectorized(layer_number):\r\n    print(""started..."")\r\n    start = time.time()\r\n    # 1.set parameter\r\n    d_model = 512\r\n    d_k = 64\r\n    d_v = 64\r\n    sequence_length = 1000\r\n    h = 8\r\n    batch_size=128\r\n    initializer = tf.random_normal_initializer(stddev=0.1)\r\n    # 2.set Q,K,V\r\n    vocab_size=1000\r\n    embed_size=d_model\r\n    type=\'decoder\'\r\n    Embedding = tf.get_variable(""Embedding_"", shape=[vocab_size, embed_size],initializer=initializer)\r\n    input_x = tf.placeholder(tf.int32, [batch_size,sequence_length], name=""input_x"")\r\n    embedded_words = tf.nn.embedding_lookup(Embedding, input_x) #[batch_size,sequence_length,embed_size]\r\n    mask=get_mask(batch_size,sequence_length) #tf.ones((batch_size,sequence_length))*-1e8  #[batch,sequence_length]\r\n    with tf.variable_scope(""query_at_each_sentence""+str(layer_number)):\r\n        Q = embedded_words  # [batch_size*sequence_length,embed_size]\r\n        K_s=embedded_words #[batch_size*sequence_length,embed_size]\r\n        #V_s=tf.get_variable(""V_s_original_"", shape=embedded_words.get_shape().as_list(),initializer=initializer) #[batch_size,sequence_length,embed_size]\r\n        V_s=K_s\r\n        # 3.call method to get result\r\n        multi_head_attention_class = MultiHeadAttention(Q, K_s, V_s, d_model, d_k, d_v, sequence_length, h,type=\'decoder\',mask=mask)\r\n        encoder_output=multi_head_attention_class.multi_head_attention_fn() #shape:[sequence_length,d_model]\r\n        encoder_output=tf.reshape(encoder_output,shape=(batch_size,sequence_length,d_model))\r\n    end = time.time()\r\n    print(""input_x:"",input_x)\r\n    print(""encoder_output:"",encoder_output,"";time_spent:"",(end-start))\r\n\r\ndef get_mask(batch_size,sequence_length):\r\n    lower_triangle=tf.matrix_band_part(tf.ones([sequence_length,sequence_length]),-1,0)\r\n    result=-1e9*(1.0-lower_triangle)\r\n    print(""get_mask==>result:"",result)\r\n    return result\r\n\r\n#multi_head_attention_for_sentence_vectorized(0)'"
a07_Transformer/a2_poistion_wise_feed_forward.py,10,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport time\r\n""""""\r\nPosition-wise Feed-Forward Networks\r\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\r\nconnected feed-forward network, which is applied to each position separately and identically. This\r\nconsists of two linear transformations with a ReLU activation in between.\r\n\r\nFFN(x) = max(0,xW1+b1)W2+b2\r\n\r\nWhile the linear transformations are the same across different positions, they use different parameters\r\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\r\nThe dimensionality of input and output is d_model= 512, and the inner-layer has dimensionalityd_ff= 2048.\r\n""""""\r\nclass PositionWiseFeedFoward(object): #TODO make it parallel\r\n    """"""\r\n    position-wise feed forward networks. formula as below:\r\n    FFN(x)=max(0,xW1+b1)W2+b2\r\n    """"""\r\n    def __init__(self,x,layer_index,d_model=512,d_ff=2048):\r\n        """"""\r\n        :param x: shape should be:[batch,sequence_length,d_model]\r\n        :param layer_index:  index of layer\r\n        :return: shape:[sequence_length,d_model]\r\n        """"""\r\n        shape_list=x.get_shape().as_list()\r\n        assert(len(shape_list)==3)\r\n        self.x=x\r\n        self.layer_index=layer_index\r\n        self.d_model=d_model\r\n        self.d_ff=d_ff\r\n        self.initializer = tf.random_normal_initializer(stddev=0.1)\r\n\r\n    def position_wise_feed_forward_fn(self):\r\n        """"""\r\n        x:       [batch,sequence_length,d_model]\r\n        :return: [batch,sequence_length,d_model]\r\n        """"""\r\n        output=None\r\n        #1.conv1\r\n        input=tf.expand_dims(self.x,axis=3) #[batch,sequence_length,d_model,1]\r\n        # conv2d.input:       [None,sentence_length,embed_size,1]. filter=[filter_size,self.embed_size,1,self.num_filters]\r\n        # output with padding:[None,sentence_length,1,1]\r\n        output_conv1=tf.layers.conv2d(\r\n            input,filters=self.d_ff,kernel_size=[1,self.d_model],padding=""VALID"",\r\n            name=\'conv1\',kernel_initializer=self.initializer,activation=tf.nn.relu\r\n        )\r\n        output_conv1 = tf.transpose(output_conv1, [0,1,3,2])\r\n        print(""output_conv1:"",output_conv1)\r\n\r\n        #2.conv2\r\n        output_conv2 = tf.layers.conv2d(\r\n            output_conv1,filters=self.d_model,kernel_size=[1,self.d_ff],padding=""VALID"",\r\n            name=\'conv2\',kernel_initializer=self.initializer,activation=None\r\n        )\r\n        output=tf.squeeze(output_conv2) #[batch,sequence_length,d_model]\r\n        return output #[batch,sequence_length,d_model]\r\n\r\n#test function of position_wise_feed_forward_fn\r\n#time spent:OLD VERSION: length=8000,time spent:35.6s; NEW VERSION:0.03s\r\ndef test_position_wise_feed_forward_fn():\r\n    start=time.time()\r\n    x=tf.ones((8,1000,512)) #batch_size=8,sequence_length=10 ;\r\n    layer_index=0\r\n    postion_wise_feed_forward=PositionWiseFeedFoward(x,layer_index)\r\n    output=postion_wise_feed_forward.position_wise_feed_forward_fn()\r\n    end=time.time()\r\n    print(""x:"",x,"";output:"",output,"";time spent:"",(end-start))\r\n    return output\r\n\r\ndef test():\r\n    with tf.Session() as sess:\r\n        result=test_position_wise_feed_forward_fn()\r\n        sess.run(tf.global_variables_initializer())\r\n        result_=sess.run(result)\r\n        print(""result_:"",result_)\r\n#test()\r\n\r\n'"
a07_Transformer/a2_predict.py,28,"b'# -*- coding: utf-8 -*-\r\n#prediction using model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom a07_Transformer import  Transformer\r\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import pad_sequences #to_categorical\r\nimport os\r\nimport codecs\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999+3,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128 #16\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_transformer/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",25,""max sentence length"") #100-->25\r\ntf.app.flags.DEFINE_integer(""embed_size"",512,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\n#train-zhihu4-only-title-all.txt\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-512"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\ntf.app.flags.DEFINE_string(""predict_target_file"",""checkpoint_transformer/zhihu_result_transformer.csv"",""target file path for final prediction"")\r\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\r\ntf.app.flags.DEFINE_integer(""decoder_sent_length"",25,""length of decoder inputs"")\r\n\r\ntf.app.flags.DEFINE_integer(""d_model"",512,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_k"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_v"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""h"",8,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""num_layer"",1,""hidden size"") #6\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\n# 1.load data with vocabulary of words and labels\r\n_GO=""_GO""\r\n_END=""_END""\r\n_PAD=""_PAD""\r\n\r\ndef main(_):\r\n    # 1.load data with vocabulary of words and labels\r\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""transformer"")  # simple=\'simple\'\r\n    vocab_size = len(vocabulary_word2index)\r\n    print(""transformer.vocab_size:"", vocab_size)\r\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""transformer"",use_seq2seq=True)\r\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\r\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\r\n    testX=[]\r\n    question_id_list=[]\r\n    for tuple in test:\r\n        question_id,question_string_list=tuple\r\n        question_id_list.append(question_id)\r\n        testX.append(question_string_list)\r\n    # 2.Data preprocessing: Sequence padding\r\n    print(""start padding...."")\r\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n    print(""end padding..."")\r\n   # 3.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        # 4.Instantiate Model\r\n        model=Transformer(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                 vocab_size, FLAGS.embed_size,FLAGS.d_model,FLAGS.d_k,FLAGS.d_v,FLAGS.h,FLAGS.num_layer,FLAGS.is_training,decoder_sent_length=FLAGS.decoder_sent_length,l2_lambda=FLAGS.l2_lambda)\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop"")\r\n            return\r\n        # 5.feed data, to get logits\r\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\r\n        index=0\r\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\r\n        #decoder_input=np.reshape(np.array([vocabulary_word2index_label[_GO]]+[vocabulary_word2index_label[_PAD]]*(FLAGS.decoder_sent_length-1)),[-1,FLAGS.decoder_sent_length])\r\n        decoder_input=np.full((FLAGS.batch_size,FLAGS.decoder_sent_length),vocabulary_word2index_label[_PAD])\r\n        decoder_input[:,0:1]=vocabulary_word2index_label[_GO] #set all values in first column to _GO\r\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\r\n            predictions,logits=sess.run([model.predictions,model.logits],\r\n                                        feed_dict={model.input_x:testX2[start:end],\r\n                                                   model.decoder_input:decoder_input,\r\n                                                   model.dropout_keep_prob:1\r\n                                                   })\r\n            ####################################################################################\r\n            #for j in range(FLAGS.decoder_sent_length):\r\n            #    predict = sess.run(model.predictions, #model.loss_val,--->loss, model.train_op\r\n            #                        feed_dict={model.input_x:testX2[start:end],\r\n            #                                   model.decoder_input:decoder_input,\r\n            #                                   #model.input_y_label: input_y_label,\r\n            #                                   model.dropout_keep_prob: 1.0,\r\n            #                                   })\r\n            #    decoder_input[:,j] = predict[:,j]\r\n           ####################################################################################\r\n            print(""===============>"",start,""predict:"",predict)\r\n            # 6. get lable using logtis\r\n            for _,logit in enumerate(logits):\r\n                predicted_labels=get_label_using_logits(logit,predictions,vocabulary_index2word_label,vocabulary_word2index_label)\r\n                print(index,"" ;predicted_labels:"",predicted_labels)\r\n                # 7. write question id and labels to file system.\r\n                write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\r\n                index=index+1\r\n        predict_target_file_f.close()\r\n\r\ndef get_label_using_logits(logits, predictions,vocabulary_index2word_label,vocabulary_word2index_label, top_number=5):\r\n    print(""logits:"",logits.shape) #(6, 2002)\r\n    result_list=[]\r\n    for i,row in enumerate(logits):\r\n        #print(""i,"",i,""row:"",row)\r\n        if i!=len(logits)-1: #not include result from last column, which usually it should be <END> TOKEN.\r\n            label=process_each_row_get_lable(row,vocabulary_index2word_label,vocabulary_word2index_label,result_list)\r\n            result_list.append(label)\r\n    return result_list\r\n\r\ndef process_each_row_get_lable(row,vocabulary_index2word_label,vocabulary_word2index_label,result_list):\r\n    """"""\r\n    :param row: it is a list.length is number of labels. e.g. 2002\r\n    :param vocabulary_index2word_label\r\n    :param result_list\r\n    :return: a lable\r\n    """"""\r\n    label_list=list(np.argsort(row))\r\n    label_list.reverse()\r\n    #print(""label_list:"",label_list) # a list,length is number of labels.\r\n    for i,index in enumerate(label_list): # if index is not exists, and not _PAD,_END, then it is the label we want.\r\n        #print(i,""index:"",index)\r\n        flag1=vocabulary_index2word_label[index] not in result_list\r\n        flag2=index!=vocabulary_word2index_label[_PAD]\r\n        flag3=index!=vocabulary_word2index_label[_END]\r\n        if flag1 and flag2 and flag3:\r\n            #print(""going to return "")\r\n            return vocabulary_index2word_label[index]\r\n\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()'"
a07_Transformer/a2_predict_classification.py,26,"b'# -*- coding: utf-8 -*-\r\n#prediction using model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom a2_transformer_classification import  Transformer\r\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import pad_sequences #to_categorical\r\nimport os\r\nimport codecs\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999+3,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128 #16\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""../checkpoint_transformer_classification/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",60,""max sentence length"") #100-->25\r\ntf.app.flags.DEFINE_integer(""embed_size"",512,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\n#train-zhihu4-only-title-all.txt\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-512"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",False,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\ntf.app.flags.DEFINE_string(""predict_target_file"",""../checkpoint_transformer_classification/zhihu_result_transformer_classification.csv"",""target file path for final prediction"")\r\ntf.app.flags.DEFINE_string(""predict_source_file"",\'../test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\r\ntf.app.flags.DEFINE_integer(""d_model"",512,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_k"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_v"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""h"",8,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""num_layer"",1,""hidden size"") #6\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\n# 1.load data with vocabulary of words and labels\r\n_GO=""_GO""\r\n_END=""_END""\r\n_PAD=""_PAD""\r\n\r\ndef main(_):\r\n    # 1.load data with vocabulary of words and labels\r\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""transformer_classification"")  # simple=\'simple\'\r\n    vocab_size = len(vocabulary_word2index)\r\n    print(""transformer_classification.vocab_size:"", vocab_size)\r\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""transformer_classification"")\r\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\r\n    print(""list of total questions:"",len(questionid_question_lists))\r\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\r\n    print(""list of total questions2:"",len(test))\r\n    testX=[]\r\n    question_id_list=[]\r\n    for tuple in test:\r\n        question_id,question_string_list=tuple\r\n        question_id_list.append(question_id)\r\n        testX.append(question_string_list)\r\n    # 2.Data preprocessing: Sequence padding\r\n    print(""start padding...."")\r\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n    print(""list of total questions3:"", len(testX2))\r\n    print(""end padding..."")\r\n   # 3.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        # 4.Instantiate Model\r\n        model=Transformer(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                 vocab_size, FLAGS.embed_size,FLAGS.d_model,FLAGS.d_k,FLAGS.d_v,FLAGS.h,FLAGS.num_layer,FLAGS.is_training,l2_lambda=FLAGS.l2_lambda)\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop"")\r\n            return\r\n        # 5.feed data, to get logits\r\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\r\n        index=0\r\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\r\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\r\n            logits=sess.run(model.logits,feed_dict={model.input_x:testX2[start:end],model.dropout_keep_prob:1}) #logits:[batch_size,self.num_classes]\r\n\r\n            question_id_sublist=question_id_list[start:end]\r\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\r\n\r\n            # 6. get lable using logtis\r\n            #predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\r\n            #print(index,"" ;predicted_labels:"",predicted_labels)\r\n            # 7. write question id and labels to file system.\r\n            #write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\r\n            index=index+1\r\n        predict_target_file_f.close()\r\n\r\n# get label using logits\r\ndef get_label_using_logits_batch(question_id_sublist, logits_batch, vocabulary_index2word_label, f, top_number=5):\r\n    print(""get_label_using_logits.shape:"", np.array(logits_batch).shape) # (1, 128, 2002))\r\n    for i, logits in enumerate(logits_batch):\r\n        index_list = np.argsort(logits)[-top_number:]\r\n        #print(""index_list:"",index_list)\r\n        index_list = index_list[::-1]\r\n        label_list = []\r\n        for index in index_list:\r\n            #print(""index:"",index)\r\n            label = vocabulary_index2word_label[index]\r\n            label_list.append(\r\n                label)  # (\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        # print(""get_label_using_logits.label_list"",label_list)\r\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\r\n    f.flush()\r\n\r\n# get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #a list.length is top_number\r\n    index_list=index_list[::-1] ##a list.length is top_number\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n    return label_list\r\n\r\ndef process_each_row_get_lable(row,vocabulary_index2word_label,vocabulary_word2index_label,result_list):\r\n    """"""\r\n    :param row: it is a list.length is number of labels. e.g. 2002\r\n    :param vocabulary_index2word_label\r\n    :param result_list\r\n    :return: a lable\r\n    """"""\r\n    label_list=list(np.argsort(row))\r\n    label_list.reverse()\r\n    #print(""label_list:"",label_list) # a list,length is number of labels.\r\n    for i,index in enumerate(label_list): # if index is not exists, and not _PAD,_END, then it is the label we want.\r\n        #print(i,""index:"",index)\r\n        flag1=vocabulary_index2word_label[index] not in result_list\r\n        flag2=index!=vocabulary_word2index_label[_PAD]\r\n        flag3=index!=vocabulary_word2index_label[_END]\r\n        if flag1 and flag2 and flag3:\r\n            #print(""going to return "")\r\n            return vocabulary_index2word_label[index]\r\n\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()'"
a07_Transformer/a2_split_traning_data.py,0,"b'# -*- coding: utf-8 -*-\r\nimport codecs\r\n\r\nfile=\'training-data/test-zhihu6-title-desc.txt\'\r\nfile_x=\'training-data/test_x.txt\'\r\nfile_y=\'training-data/test_y.txt\'\r\nfile_object = codecs.open(file, \'r\', \'utf8\')\r\nx_object = codecs.open(file_x, \'a\', \'utf8\')\r\ny_object = codecs.open(file_y, \'a\', \'utf8\')\r\n\r\nlines=file_object.readlines()\r\nfor i,line in enumerate(lines):\r\n    x,y=line.strip().split(""__label__"")\r\n    #print(i,""x:"",x,"";y:"",y)\r\n    x_object.write(x.strip()+""\\n"")\r\n    y_object.write(y.strip() + ""\\n"")\r\n    #if i>=10:\r\n    #    break\r\nfile_object.close()\r\nx_object.close()\r\nx_object.close()\r\n'"
a07_Transformer/a2_train.py,33,"b'# -*- coding: utf-8 -*-\r\n#training the model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom a07_Transformer import  Transformer\r\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import to_categorical, pad_sequences\r\nimport os,math\r\nimport word2vec\r\nimport pickle\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999+3,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128-->512\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_transformer/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",25,""max sentence length"") #25\r\ntf.app.flags.DEFINE_integer(""embed_size"",512,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_integer(""validate_step"", 1000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\n#train-zhihu4-only-title-all.txt\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-512"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\n\r\ntf.app.flags.DEFINE_integer(""d_model"",512,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_k"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_v"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""h"",8,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""num_layer"",1,""hidden size"") #6\r\ntf.app.flags.DEFINE_integer(""decoder_sent_length"",25,""length of decoder inputs"") #decoder sentence length should be 6 here.\r\n\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\ndef main(_):\r\n    #1.load data(X:list of lint,y:int).\r\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\r\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\r\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\r\n    #        vocab_size=len(vocabulary_index2word)\r\n    #else:\r\n    if 1==1:\r\n        trainX, trainY, testX, testY = None, None, None, None\r\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""transformer"") #simple=\'simple\'\r\n        vocab_size = len(vocabulary_word2index)\r\n        print(""transformer.vocab_size:"",vocab_size)\r\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""transformer"",use_seq2seq=True)\r\n        if FLAGS.multi_label_flag:\r\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #train\r\n        train,test,_=load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,\r\n                                              use_seq2seq=True,traning_data_path=FLAGS.traning_data_path,seq2seq_label_length=FLAGS.decoder_sent_length) #TODO\r\n        trainX, trainY,train_decoder_input = train\r\n        testX, testY,test_decoder_input = test\r\n\r\n        print(""trainY:"",trainY[0:10])\r\n        print(""train_decoder_input:"",train_decoder_input[0:10])\r\n        # 2.Data preprocessing.Sequence padding\r\n        print(""start padding & transform to one hot..."")\r\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\r\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\r\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\r\n        # Converting labels to binary vectors\r\n        print(""end padding & transform to one hot..."")\r\n    #2.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        #Instantiate Model\r\n        model=Transformer(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                 vocab_size, FLAGS.embed_size,FLAGS.d_model,FLAGS.d_k,FLAGS.d_v,FLAGS.h,FLAGS.num_layer,FLAGS.is_training,\r\n                          decoder_sent_length=FLAGS.sequence_length,l2_lambda=FLAGS.l2_lambda) #TODO decoder_sent_length=FLAGS.sequence_length\r\n        #Initialize Save\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(\'Initializing Variables\')\r\n            sess.run(tf.global_variables_initializer())\r\n            if FLAGS.use_embedding: #load pre-trained word embedding\r\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\r\n        curr_epoch=sess.run(model.epoch_step)\r\n        #3.feed data & training\r\n        number_of_training_data=len(trainX)\r\n        print(""number_of_training_data:"",number_of_training_data)\r\n        previous_eval_loss=10000\r\n        best_eval_loss=10000\r\n        batch_size=FLAGS.batch_size\r\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\r\n            loss, acc, counter = 0.0, 0.0, 0\r\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\r\n                if epoch==0 and counter==0:\r\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\r\n                feed_dict = {model.input_x: trainX[start:end],model.dropout_keep_prob: 0.5}\r\n                if not FLAGS.multi_label_flag:\r\n                    feed_dict[model.input_y] = trainY[start:end]\r\n                else:\r\n                    feed_dict[model.input_y_label]=trainY[start:end]\r\n                    feed_dict[model.decoder_input] = train_decoder_input[start:end]\r\n                curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\r\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\r\n                if counter %50==0:\r\n                    print(""transformer==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,math.exp(loss/float(counter)) if (loss/float(counter))<20 else 10000.000,acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n                if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0): #(epoch % FLAGS.validate_every) or  if epoch % FLAGS.validate_every == 0:\r\n                    eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label,eval_decoder_input=test_decoder_input)\r\n                    print(""transformer.validation.part. previous_eval_loss:"", math.exp(previous_eval_loss) if previous_eval_loss<20 else 10000.000,"";current_eval_loss:"", math.exp(eval_loss) if eval_loss<20 else 10000.000)\r\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\r\n                        # reduce the learning rate by a factor of 0.5\r\n                        print(""transformer==>validation.part.going to reduce the learning rate."")\r\n                        learning_rate1 = sess.run(model.learning_rate)\r\n                        lrr=sess.run([model.learning_rate_decay_half_op])\r\n                        learning_rate2 = sess.run(model.learning_rate)\r\n                        print(""transformer==>validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\r\n                    #print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_acc))\r\n                    else:# loss is decreasing\r\n                        if eval_loss<best_eval_loss:\r\n                            print(""transformer==>going to save the model.eval_loss:"",math.exp(eval_loss) if eval_loss<20 else 10000.000,"";best_eval_loss:"",math.exp(best_eval_loss) if best_eval_loss<20 else 10000.000)\r\n                            # save model to checkpoint\r\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\r\n                            saver.save(sess, save_path, global_step=epoch)\r\n                            best_eval_loss=eval_loss\r\n                    previous_eval_loss = eval_loss\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n\r\n            #epoch increment\r\n            print(""going to increment epoch counter...."")\r\n            sess.run(model.epoch_increment)\r\n\r\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\r\n        test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label,eval_decoder_input=test_decoder_input)\r\n    pass\r\n\r\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,model,word2vec_model_path=None):\r\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\r\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\r\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\r\n    word2vec_dict = {}\r\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\r\n        word2vec_dict[word] = vector\r\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\r\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\r\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\r\n    count_exist = 0;\r\n    count_not_exist = 0\r\n    for i in range(1, vocab_size):  # loop each word\r\n        word = vocabulary_index2word[i]  # get a word\r\n        embedding = None\r\n        try:\r\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\r\n        except Exception:\r\n            embedding = None\r\n        if embedding is not None:  # the \'word\' exist a embedding\r\n            word_embedding_2dlist[i] = embedding;\r\n            count_exist = count_exist + 1  # assign array to this word.\r\n        else:  # no embedding for this word\r\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\r\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\r\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\r\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\r\n    t_assign_embedding = tf.assign(model.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\r\n    sess.run(t_assign_embedding);\r\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\r\n    print(""using pre-trained word emebedding.ended..."")\r\n\r\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\r\ndef do_eval(sess,model,evalX,evalY,batch_size,vocabulary_index2word_label,eval_decoder_input=None):\r\n    #ii=0\r\n    number_examples=len(evalX)\r\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\r\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\r\n        feed_dict = {model.input_x: evalX[start:end], model.dropout_keep_prob: 1.0}\r\n        if not FLAGS.multi_label_flag:\r\n            feed_dict[model.input_y] = evalY[start:end]\r\n        else:\r\n            feed_dict[model.input_y_label] = evalY[start:end]\r\n            feed_dict[model.decoder_input] = eval_decoder_input[start:end]\r\n        curr_eval_loss, logits,curr_eval_acc,pred= sess.run([model.loss_val,model.logits,model.accuracy,model.predictions],feed_dict)#curr_eval_acc--->textCNN.accuracy\r\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\r\n        #if ii<20:\r\n            #print(""1.evalX[start:end]:"",evalX[start:end])\r\n            #print(""2.evalY[start:end]:"", evalY[start:end])\r\n            #print(""3.pred:"",pred)\r\n            #ii=ii+1\r\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()'"
a07_Transformer/a2_train_classification.py,32,"b'# -*- coding: utf-8 -*-\r\n#training the model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom a2_transformer_classification import  Transformer\r\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import to_categorical, pad_sequences\r\nimport os,math\r\nimport word2vec\r\nimport pickle\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999+3,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 256, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128-->512\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""../checkpoint_transformer_classification/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",60,""max sentence length"")\r\ntf.app.flags.DEFINE_integer(""embed_size"",512,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_integer(""validate_step"", 2000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\n#train-zhihu4-only-title-all.txt\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""../train-zhihu4-only-title-all.txt"",""path of traning data."") #test-zhihu4-only-title-all.txt.one record like this:\'w183364 w11 w3484 w3125 w155457 w111 __label__-2086863971949478092\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-512"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",False,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\n\r\ntf.app.flags.DEFINE_integer(""d_model"",512,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_k"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""d_v"",64,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""h"",8,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""num_layer"",1,""hidden size"") #6\r\n\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\ndef main(_):\r\n    #1.load data(X:list of lint,y:int).\r\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\r\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\r\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\r\n    #        vocab_size=len(vocabulary_index2word)\r\n    #else:\r\n    if 1==1:\r\n        trainX, trainY, testX, testY = None, None, None, None\r\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""transformer_classification"") #simple=\'simple\'\r\n        vocab_size = len(vocabulary_word2index)\r\n        print(""transformer.vocab_size:"",vocab_size)\r\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""transformer_classification"")\r\n        if FLAGS.multi_label_flag:\r\n            FLAGS.traning_data_path=\'training-data/test-zhihu6-title-desc.txt\' #one record like this:\'w35620 w1097 w111 c278 c150 c150 c285 c278 c43 __label__7756633728210171144 3195914392210930723\'\r\n        train,test,_=load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,\r\n                                              traning_data_path=FLAGS.traning_data_path)\r\n        trainX, trainY, = train\r\n        testX, testY = test\r\n\r\n        print(""trainY:"",trainY[0:10])\r\n        # 2.Data preprocessing.Sequence padding\r\n        print(""start padding & transform to one hot..."")\r\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\r\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\r\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\r\n        # Converting labels to binary vectors\r\n        print(""end padding & transform to one hot..."")\r\n    #2.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        #Instantiate Model\r\n        model=Transformer(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                 vocab_size, FLAGS.embed_size,FLAGS.d_model,FLAGS.d_k,FLAGS.d_v,FLAGS.h,FLAGS.num_layer,FLAGS.is_training,l2_lambda=FLAGS.l2_lambda)\r\n        #Initialize Save\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(\'Initializing Variables\')\r\n            sess.run(tf.global_variables_initializer())\r\n            if FLAGS.use_embedding: #load pre-trained word embedding\r\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\r\n        curr_epoch=sess.run(model.epoch_step)\r\n        #3.feed data & training\r\n        number_of_training_data=len(trainX)\r\n        print(""number_of_training_data:"",number_of_training_data)\r\n        previous_eval_loss=10000\r\n        best_eval_loss=10000\r\n        batch_size=FLAGS.batch_size\r\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\r\n            loss, acc, counter = 0.0, 0.0, 0\r\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\r\n                if epoch==0 and counter==0:\r\n                    print(""trainX[start:end]:"",trainX[start:end])\r\n                feed_dict = {model.input_x: trainX[start:end],model.dropout_keep_prob: 0.5}\r\n                feed_dict[model.input_y_label]=trainY[start:end]\r\n                curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\r\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\r\n                if counter %50==0:\r\n                    print(""transformer.classification==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n                if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0):\r\n                    eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\r\n                    print(""transformer.classification.validation.part. previous_eval_loss:"", previous_eval_loss,"";current_eval_loss:"",eval_loss)\r\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\r\n                        # reduce the learning rate by a factor of 0.5\r\n                        print(""transformer.classification.==>validation.part.going to reduce the learning rate."")\r\n                        learning_rate1 = sess.run(model.learning_rate)\r\n                        lrr=sess.run([model.learning_rate_decay_half_op])\r\n                        learning_rate2 = sess.run(model.learning_rate)\r\n                        print(""transformer.classification==>validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\r\n                    #print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_acc))\r\n                    else:# loss is decreasing\r\n                        if eval_loss<best_eval_loss:\r\n                            print(""transformer.classification==>going to save the model.eval_loss:"",eval_loss,"";best_eval_loss:"",best_eval_loss)\r\n                            # save model to checkpoint\r\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\r\n                            saver.save(sess, save_path, global_step=epoch)\r\n                            best_eval_loss=eval_loss\r\n                    previous_eval_loss = eval_loss\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n\r\n            #epoch increment\r\n            print(""going to increment epoch counter...."")\r\n            sess.run(model.epoch_increment)\r\n\r\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\r\n        test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\r\n    pass\r\n\r\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,model,word2vec_model_path=None):\r\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\r\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\r\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\r\n    word2vec_dict = {}\r\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\r\n        word2vec_dict[word] = vector\r\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\r\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\r\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\r\n    count_exist = 0;\r\n    count_not_exist = 0\r\n    for i in range(1, vocab_size):  # loop each word\r\n        word = vocabulary_index2word[i]  # get a word\r\n        embedding = None\r\n        try:\r\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\r\n        except Exception:\r\n            embedding = None\r\n        if embedding is not None:  # the \'word\' exist a embedding\r\n            word_embedding_2dlist[i] = embedding;\r\n            count_exist = count_exist + 1  # assign array to this word.\r\n        else:  # no embedding for this word\r\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\r\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\r\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\r\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\r\n    t_assign_embedding = tf.assign(model.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\r\n    sess.run(t_assign_embedding);\r\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\r\n    print(""using pre-trained word emebedding.ended..."")\r\n\r\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\r\ndef do_eval(sess,model,evalX,evalY,batch_size,vocabulary_index2word_label,eval_decoder_input=None):\r\n    #ii=0\r\n    number_examples=len(evalX)\r\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\r\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\r\n        feed_dict = {model.input_x: evalX[start:end], model.dropout_keep_prob: 1.0}\r\n        #if not FLAGS.multi_label_flag:\r\n        #    feed_dict[model.input_y] = evalY[start:end]\r\n        #else:\r\n        feed_dict[model.input_y_label] = evalY[start:end]\r\n        #    feed_dict[model.decoder_input] = eval_decoder_input[start:end]\r\n        curr_eval_loss, logits,curr_eval_acc,pred= sess.run([model.loss_val,model.logits,model.accuracy,model.predictions],feed_dict)#curr_eval_acc--->textCNN.accuracy\r\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\r\n        #if ii<20:\r\n            #print(""1.evalX[start:end]:"",evalX[start:end])\r\n            #print(""2.evalY[start:end]:"", evalY[start:end])\r\n            #print(""3.pred:"",pred)\r\n            #ii=ii+1\r\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()'"
a07_Transformer/a2_transformer.py,50,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\nimport copy\r\nfrom a2_base_model import BaseClass\r\nfrom a2_encoder import Encoder\r\nfrom a2_decoder import Decoder\r\nimport os\r\n""""""\r\nTransformer: perform sequence to sequence solely on attention mechanism. do it fast and better.\r\nfor more detail, check paper: ""Attention Is All You Need""\r\n1. position embedding for encoder input and decoder input\r\n2. encoder with multi-head attention, position-wise feed forward\r\n3. decoder with multi-head attention for decoder input,position-wise feed forward, mulit-head attention between encoder and decoder.\r\n\r\nencoder:\r\n6 layers.each layers has two sub-layers.\r\nthe first is multi-head self-attention mechanism;\r\nthe second is position-wise fully connected feed-forward network.\r\nfor each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n\r\nDecoder:\r\n1. The decoder is composed of a stack of N= 6 identical layers.\r\n2. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\r\nattention over the output of the encoder stack.\r\n3. Similar to the encoder, we employ residual connections\r\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\r\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This\r\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\r\npredictions for position i can depend only on the known outputs at positions less than i.\r\n""""""\r\nclass Transformer(BaseClass):\r\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                 vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,decoder_sent_length=6,\r\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,l2_lambda=0.0001):\r\n        """"""init all hyperparameter here""""""\r\n        super(Transformer, self).__init__(d_model, d_k, d_v, sequence_length, h, batch_size, num_layer=num_layer) #init some fields by using parent class.\r\n\r\n        self.num_classes = num_classes\r\n        self.sequence_length = sequence_length\r\n        self.vocab_size = vocab_size\r\n        self.embed_size = d_model\r\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")\r\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\r\n        self.initializer = initializer\r\n        self.decoder_sent_length=decoder_sent_length\r\n        self.clip_gradients=clip_gradients\r\n        self.l2_lambda=l2_lambda\r\n\r\n        self.is_training=is_training #self.is_training=tf.placeholder(tf.bool,name=""is_training"") #tf.bool #is_training\r\n        self.input_x = tf.placeholder(tf.int32, [self.batch_size, self.sequence_length], name=""input_x"")                 #x  batch_size\r\n        self.decoder_input = tf.placeholder(tf.int32, [self.batch_size, self.decoder_sent_length],name=""decoder_input"")  #y, but shift None\r\n        self.input_y_label = tf.placeholder(tf.int32, [self.batch_size, self.decoder_sent_length], name=""input_y_label"") #y, but shift None\r\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\r\n\r\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\r\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\r\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\r\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\r\n\r\n        self.instantiate_weights()\r\n        self.logits = self.inference() #logits shape:[batch_size,decoder_sent_length,self.num_classes]\r\n\r\n        self.predictions = tf.argmax(self.logits, axis=2, name=""predictions"")\r\n        self.accuracy = tf.constant(0.5)  # fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\r\n        if self.is_training is False:# if it is not training, then no need to calculate loss and back-propagation.\r\n            return\r\n        self.loss_val = self.loss_seq2seq()\r\n        self.train_op = self.train()\r\n\r\n    def inference(self):\r\n        """""" building blocks:\r\n        encoder:6 layers.each layers has two   sub-layers. the first is multi-head self-attention mechanism; the second is position-wise fully connected feed-forward network.\r\n               for each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        decoder:6 layers.each layers has three sub-layers. the second layer is performs multi-head attention over the ouput of the encoder stack.\r\n               for each sublayer. use LayerNorm(x+Sublayer(x)).\r\n        """"""\r\n        # 1.embedding for encoder input & decoder input\r\n        # 1.1 position embedding for encoder input\r\n        input_x_embeded = tf.nn.embedding_lookup(self.Embedding,self.input_x)  #[None,sequence_length, embed_size]\r\n        input_x_embeded=tf.multiply(input_x_embeded,tf.sqrt(tf.cast(self.d_model,dtype=tf.float32)))\r\n        input_mask=tf.get_variable(""input_mask"",[self.sequence_length,1],initializer=self.initializer)\r\n        input_x_embeded=tf.add(input_x_embeded,input_mask) #[None,sequence_length,embed_size].position embedding.\r\n        # 1.2 position embedding for decoder input\r\n        decoder_input_embedded = tf.nn.embedding_lookup(self.Embedding_label, self.decoder_input) #[None,decoder_sent_length,embed_size]\r\n        decoder_input_embedded = tf.multiply(decoder_input_embedded, tf.sqrt(tf.cast(self.d_model,dtype=tf.float32)))\r\n        decoder_input_mask=tf.get_variable(""decoder_input_mask"",[self.decoder_sent_length,1],initializer=self.initializer)\r\n        decoder_input_embedded=tf.add(decoder_input_embedded,decoder_input_mask)\r\n\r\n        # 2. encoder\r\n        encoder_class=Encoder(self.d_model,self.d_k,self.d_v,self.sequence_length,self.h,self.batch_size,self.num_layer,input_x_embeded,input_x_embeded,dropout_keep_prob=self.dropout_keep_prob)\r\n        Q_encoded,K_encoded = encoder_class.encoder_fn() #K_v_encoder\r\n\r\n        # 3. decoder with attention ==>get last of output(hidden state)====>prepare to get logits\r\n        mask = self.get_mask(self.decoder_sent_length)\r\n                           #d_model, d_k, d_v, sequence_length, h, batch_size, Q, K_s, K_v_encoder, decoder_sent_length,\r\n                           #num_layer = 6, type = \'decoder\', is_training = True, mask = None\r\n        decoder = Decoder(self.d_model, self.d_k, self.d_v, self.sequence_length, self.h, self.batch_size,\r\n                          decoder_input_embedded, decoder_input_embedded, K_encoded,self.decoder_sent_length,\r\n                          num_layer=self.num_layer,is_training=self.is_training,mask=mask,dropout_keep_prob=self.dropout_keep_prob) #,extract_word_vector_fn=extract_word_vector_fn\r\n        Q_decoded, K_decoded=decoder.decoder_fn() #[batch_size,decoder_sent_length,d_model]\r\n        K_decoded=tf.reshape(K_decoded,shape=(-1,self.d_model))\r\n        with tf.variable_scope(""output""):\r\n            print(""self.W_projection2:"",self.W_projection,"" ;K_decoded:"",K_decoded)\r\n            logits = tf.matmul(K_decoded, self.W_projection) + self.b_projection #logits shape:[batch_size*decoder_sent_length,self.num_classes]\r\n            logits=tf.reshape(logits,shape=(self.batch_size,self.decoder_sent_length,self.num_classes)) #logits shape:[batch_size,decoder_sent_length,self.num_classes]\r\n        return logits\r\n\r\n    def loss_seq2seq(self):\r\n        with tf.variable_scope(""loss""):\r\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y_label, logits=self.logits);#losses:[batch_size,self.decoder_sent_length]\r\n            loss_batch=tf.reduce_sum(losses,axis=1)/self.decoder_sent_length #loss_batch:[batch_size]\r\n            loss=tf.reduce_mean(loss_batch)\r\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * self.l2_lambda\r\n            loss = loss + l2_losses\r\n            return loss\r\n\r\n    def train(self):\r\n        """"""based on the loss, use SGD to update parameter""""""\r\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\r\n        self.learning_rate_=learning_rate\r\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\r\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\r\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\r\n        return train_op\r\n\r\n    def instantiate_weights(self):\r\n        """"""define all weights here""""""\r\n        with tf.variable_scope(""embedding_projection""):  # embedding matrix\r\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\r\n            self.Embedding_label = tf.get_variable(""Embedding_label"", shape=[self.num_classes, self.embed_size],dtype=tf.float32) #,initializer=self.initializer\r\n            self.W_projection = tf.get_variable(""W_projection"", shape=[self.d_model, self.num_classes],initializer=self.initializer)  # [embed_size,label_size]\r\n            self.b_projection = tf.get_variable(""b_projection"", shape=[self.num_classes])\r\n\r\n    def get_mask(self,sequence_length):\r\n        lower_triangle = tf.matrix_band_part(tf.ones([sequence_length, sequence_length]), -1, 0)\r\n        result = -1e9 * (1.0 - lower_triangle)\r\n        print(""get_mask==>result:"", result)\r\n        return result\r\n# test started: learn to output reverse sequence of itself.\r\ndef test_training():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 9+2 #additional two classes:one is for _GO, another is for _END\r\n    learning_rate = 0.0001/10.0\r\n    batch_size = 1\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 6 #5 TODO\r\n    vocab_size = 300\r\n    is_training = True #True\r\n    dropout_keep_prob = 0.9  # 0.5 #num_sentences\r\n    decoder_sent_length=6\r\n    l2_lambda=0.0001#0.0001\r\n    d_model=512 #512\r\n    d_k=64\r\n    d_v=64\r\n    h=8\r\n    num_layer=1#6\r\n    embed_size = d_model\r\n\r\n    model = Transformer(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                                    vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,\r\n                                    decoder_sent_length=decoder_sent_length,l2_lambda=l2_lambda)\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        ckpt_dir = \'checkpoint_transformer/sequence_reverse/\'\r\n        if os.path.exists(ckpt_dir+""checkpoint""):\r\n            saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\r\n        for i in range(150000):\r\n            label_list=get_unique_labels()\r\n            input_x = np.array([label_list+[9]],dtype=np.int32) #TODO [2,3,4,5,6]\r\n            label_list_original=copy.deepcopy(label_list)\r\n            label_list.reverse()\r\n            decoder_input=np.array([[0]+label_list],dtype=np.int32) #TODO [[0,2,3,4,5,6]]\r\n            input_y_label=np.array([label_list+[1]],dtype=np.int32) #TODO [[2,3,4,5,6,1]]\r\n\r\n            loss, acc, predict, W_projection_value, _ = sess.run([model.loss_val, model.accuracy, model.predictions, model.W_projection, model.train_op],\r\n                                                     feed_dict={model.input_x:input_x,model.decoder_input:decoder_input, model.input_y_label: input_y_label,\r\n                                                                model.dropout_keep_prob: dropout_keep_prob}) #model.dropout_keep_prob: dropout_keep_prob\r\n            print(i,""loss:"", loss, ""acc:"", acc, ""label_list_original as input x:"",label_list_original,"";input_y_label:"", input_y_label, ""prediction:"", predict)\r\n            if i%1500==0:\r\n                save_path = ckpt_dir + ""model.ckpt""\r\n                saver.save(sess, save_path, global_step=i)\r\n\r\ndef test_predict():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 9+2 #additional two classes:one is for _GO, another is for _END\r\n    learning_rate = 0.0001\r\n    batch_size = 1\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 6 #5\r\n    vocab_size = 300\r\n    is_training = False #True\r\n    dropout_keep_prob = 1  # 0.5 #num_sentences\r\n    decoder_sent_length=6\r\n    l2_lambda=0.0001\r\n    d_model=512 #512\r\n    d_k=64\r\n    d_v=64\r\n    h=8\r\n    num_layer=1#6\r\n    embed_size = d_model\r\n    model = Transformer(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                                    vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,\r\n                                    decoder_sent_length=decoder_sent_length,l2_lambda=l2_lambda)\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        ckpt_dir = \'checkpoint_transformer/sequence_reverse/\'\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\r\n        print(""=================restored."")\r\n        for i in range(15000):\r\n            label_list=get_unique_labels()\r\n            input_x = np.array([label_list+[9]],dtype=np.int32) #[2,3,4,5,6]\r\n            label_list_original=copy.deepcopy(label_list)\r\n            label_list.reverse()\r\n            decoder_input=np.array([[0]*decoder_sent_length],dtype=np.int32) #[0]+label_list]--->[[0,2,3,4,5,6]]\r\n            #input_y_label=np.array([label_list+[1]],dtype=np.int32) #[[2,3,4,5,6,1]]\r\n            predict, W_projection_value = sess.run([ model.predictions, model.W_projection], #model.loss_val,--->loss, model.train_op\r\n                                feed_dict={model.input_x:input_x,\r\n                                           model.decoder_input:decoder_input,\r\n                                           #model.input_y_label: input_y_label,\r\n                                           model.dropout_keep_prob: dropout_keep_prob,\r\n                                           })\r\n            print(i, ""label_list_original as input x:"",label_list_original, ""prediction:"", predict) #""acc:"", acc, ""loss:"", loss "";input_y_label:"", input_y_label\r\n\r\n# test started: learn to output reverse sequence of itself using batch input.\r\ndef test_training_batch():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 9+2 #additional two classes:one is for _GO, another is for _END\r\n    learning_rate = 0.001\r\n    batch_size = 16\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 5\r\n    vocab_size = 300\r\n    is_training = True #True\r\n    dropout_keep_prob = 1  # 0.5 #num_sentences\r\n    decoder_sent_length=6\r\n    l2_lambda=0.0001\r\n    d_model=512 #512\r\n    d_k=64\r\n    d_v=64\r\n    h=8\r\n    num_layer=1#6\r\n    embed_size = d_model\r\n    ckpt_dir=\'checkpoint_transformer/sequence_reverse_batch/\'\r\n    model = Transformer(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length+1,\r\n                                    vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,\r\n                                    decoder_sent_length=decoder_sent_length,l2_lambda=l2_lambda)\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for i in range(1500):\r\n            label_list=get_unique_labels_batch(batch_size,length=sequence_length)\r\n            input_x = np.array(label_list,dtype=np.int32) #INPUT_X of each element should be 6.\r\n            label_list_original=copy.deepcopy(label_list)\r\n\r\n            decoder_input_list=[]\r\n            input_y_label_list=[]\r\n            for _,sub_label_list in enumerate(label_list):\r\n                sub_label_list.reverse()\r\n                decoder_input_list.append([0]+sub_label_list)\r\n                input_y_label_list.append(sub_label_list+[1])\r\n\r\n            decoder_input=np.array(decoder_input_list,dtype=np.int32)\r\n            input_y_label=np.array(input_y_label_list,dtype=np.int32)\r\n\r\n            loss, acc, predict, W_projection_value, _ = sess.run([model.loss_val, model.accuracy, model.predictions, model.W_projection, model.train_op],\r\n                                                       feed_dict={model.input_x:input_x,model.decoder_input:decoder_input, model.input_y_label: input_y_label,\r\n                                                                  model.dropout_keep_prob: dropout_keep_prob})\r\n            print(i,""loss:"", loss, ""acc:"", acc)\r\n            if i%100==0:\r\n                print(""label_list_original as input x:"",label_list_original,"";input_y_label:"", input_y_label, ""prediction:"", predict)\r\n            if i%(int(1500/batch_size))==0:\r\n                save_path = ckpt_dir + ""model.ckpt""\r\n                saver.save(sess, save_path, global_step=i*batch_size)\r\n\r\ndef get_unique_labels(length=5):\r\n    #if length is  None:\r\n    #    x=[2,3,4,5,6]\r\n    #else:\r\n    x=[i for i in range(2,2+length)]\r\n    random.shuffle(x)\r\n    return x\r\n\r\ndef get_unique_labels_batch(batch_size,length=None):\r\n    x=[]\r\n    for i in range(batch_size):\r\n        labels=get_unique_labels(length=length)\r\n        x.append(labels)\r\n    return x\r\n\r\n\r\n#test_training()\r\n#test_predict()\r\n#test_training_batch()\r\n#test_training()'"
a07_Transformer/a2_transformer_classification.py,46,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\nimport copy\r\nfrom a2_base_model import BaseClass\r\nfrom a2_encoder import Encoder\r\nimport os\r\n""""""\r\nTransformer_classification: originally it perform sequence to sequence solely on attention mechanism. do it fast and better. now we use it to do text classification.\r\nfor more detail, check paper: ""Attention Is All You Need""\r\n1. position embedding for encoder input and decoder input\r\n2. encoder with multi-head attention, position-wise feed forward\r\n3. decoder with multi-head attention for decoder input,position-wise feed forward, mulit-head attention between encoder and decoder.\r\n\r\nencoder:\r\n6 layers.each layers has two sub-layers.\r\nthe first is multi-head self-attention mechanism;\r\nthe second is position-wise fully connected feed-forward network.\r\nfor each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n\r\nDecoder:\r\n1. The decoder is composed of a stack of N= 6 identical layers.\r\n2. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\r\nattention over the output of the encoder stack.\r\n3. Similar to the encoder, we employ residual connections\r\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\r\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This\r\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\r\npredictions for position i can depend only on the known outputs at positions less than i.\r\n""""""\r\nclass Transformer(BaseClass):\r\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                 vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,\r\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,l2_lambda=0.0001,use_residual_conn=False):\r\n        """"""init all hyperparameter here""""""\r\n        super(Transformer, self).__init__(d_model, d_k, d_v, sequence_length, h, batch_size, num_layer=num_layer) #init some fields by using parent class.\r\n\r\n        self.num_classes = num_classes\r\n        self.sequence_length = sequence_length\r\n        self.vocab_size = vocab_size\r\n        self.embed_size = d_model\r\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")\r\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\r\n        self.initializer = initializer\r\n        self.clip_gradients=clip_gradients\r\n        self.l2_lambda=l2_lambda\r\n\r\n        self.is_training=is_training #self.is_training=tf.placeholder(tf.bool,name=""is_training"") #tf.bool #is_training\r\n        self.input_x = tf.placeholder(tf.int32, [self.batch_size, self.sequence_length], name=""input_x"")                 #x  batch_size\r\n        self.input_y_label = tf.placeholder(tf.int32, [self.batch_size], name=""input_y_label"")\r\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\r\n\r\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\r\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\r\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\r\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\r\n        self.use_residual_conn=use_residual_conn\r\n\r\n        self.instantiate_weights()\r\n        self.logits = self.inference() #logits shape:[batch_size,self.num_classes]\r\n\r\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")\r\n        correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32),self.input_y_label)\r\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"")  # shape=()\r\n        if self.is_training is False:# if it is not training, then no need to calculate loss and back-propagation.\r\n            return\r\n        self.loss_val = self.loss()\r\n        self.train_op = self.train()\r\n\r\n    def inference(self):\r\n        """""" building blocks:\r\n        encoder:6 layers.each layers has two   sub-layers. the first is multi-head self-attention mechanism; the second is position-wise fully connected feed-forward network.\r\n               for each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\r\n        decoder:6 layers.each layers has three sub-layers. the second layer is performs multi-head attention over the ouput of the encoder stack.\r\n               for each sublayer. use LayerNorm(x+Sublayer(x)).\r\n        """"""\r\n        # 1.embedding for encoder input & decoder input\r\n        # 1.1 position embedding for encoder input\r\n        input_x_embeded = tf.nn.embedding_lookup(self.Embedding,self.input_x)  #[None,sequence_length, embed_size]\r\n        input_x_embeded=tf.multiply(input_x_embeded,tf.sqrt(tf.cast(self.d_model,dtype=tf.float32)))\r\n        input_mask=tf.get_variable(""input_mask"",[self.sequence_length,1],initializer=self.initializer)\r\n        input_x_embeded=tf.add(input_x_embeded,input_mask) #[None,sequence_length,embed_size].position embedding.\r\n\r\n        # 2. encoder\r\n        encoder_class=Encoder(self.d_model,self.d_k,self.d_v,self.sequence_length,self.h,self.batch_size,self.num_layer,input_x_embeded,input_x_embeded,dropout_keep_prob=self.dropout_keep_prob,use_residual_conn=self.use_residual_conn)\r\n        Q_encoded,K_encoded = encoder_class.encoder_fn() #K_v_encoder\r\n\r\n        Q_encoded=tf.reshape(Q_encoded,shape=(self.batch_size,-1)) #[batch_size,sequence_length*d_model]\r\n        with tf.variable_scope(""output""):\r\n            logits = tf.matmul(Q_encoded, self.W_projection) + self.b_projection #logits shape:[batch_size*decoder_sent_length,self.num_classes]\r\n        print(""logits:"",logits)\r\n        return logits\r\n\r\n    def loss(self, l2_lambda=0.0001):  # 0.001\r\n        with tf.name_scope(""loss""):\r\n            # input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\r\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\r\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y_label,logits=self.logits);  # sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\r\n            # print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\r\n            loss = tf.reduce_mean(losses)  # print(""2.loss.loss:"", loss) #shape=()\r\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if (\'bias\' not in v.name ) and (\'alpha\' not in v.name)]) * l2_lambda\r\n            loss = loss + l2_losses\r\n        return loss\r\n\r\n    #def loss_seq2seq(self):\r\n    #    with tf.variable_scope(""loss""):\r\n    #        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y_label, logits=self.logits);#losses:[batch_size,self.decoder_sent_length]\r\n    #        loss_batch=tf.reduce_sum(losses,axis=1)/self.decoder_sent_length #loss_batch:[batch_size]\r\n    #        loss=tf.reduce_mean(loss_batch)\r\n    #        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * self.l2_lambda\r\n    #        loss = loss + l2_losses\r\n    #        return loss\r\n\r\n    def train(self):\r\n        """"""based on the loss, use SGD to update parameter""""""\r\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\r\n        self.learning_rate_=learning_rate\r\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\r\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\r\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\r\n        return train_op\r\n\r\n    def instantiate_weights(self):\r\n        """"""define all weights here""""""\r\n        with tf.variable_scope(""embedding_projection""):  # embedding matrix\r\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\r\n            self.Embedding_label = tf.get_variable(""Embedding_label"", shape=[self.num_classes, self.embed_size],dtype=tf.float32) #,initializer=self.initializer\r\n            self.W_projection = tf.get_variable(""W_projection"", shape=[self.sequence_length*self.d_model, self.num_classes],initializer=self.initializer)  # [embed_size,label_size]\r\n            self.b_projection = tf.get_variable(""b_projection"", shape=[self.num_classes])\r\n\r\n    def get_mask(self,sequence_length):\r\n        lower_triangle = tf.matrix_band_part(tf.ones([sequence_length, sequence_length]), -1, 0)\r\n        result = -1e9 * (1.0 - lower_triangle)\r\n        print(""get_mask==>result:"", result)\r\n        return result\r\n# test started: learn to predict the bigger number in two numbers from specific location of array.\r\ndef test_training():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 9+2 #additional two classes:one is for _GO, another is for _END\r\n    learning_rate = 0.0001 #/10.0\r\n    batch_size = 1\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 6 #5 TODO\r\n    vocab_size = 300\r\n    is_training = True #True\r\n    dropout_keep_prob = 0.9  # 0.5 #num_sentences\r\n    #decoder_sent_length=6\r\n    l2_lambda=0.0001#0.0001\r\n    d_model=512 #512\r\n    d_k=64\r\n    d_v=64\r\n    h=8\r\n    num_layer=1\r\n    embed_size = d_model\r\n\r\n    model = Transformer(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                        vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,l2_lambda=l2_lambda)\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        ckpt_dir = \'checkpoint_transformer/sequence_reverse/\'\r\n        if os.path.exists(ckpt_dir+""checkpoint""):\r\n            saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\r\n        for i in range(15000):\r\n            label_list=get_unique_labels()\r\n            input_x = np.array([label_list+[9]],dtype=np.int32)\r\n            label_list_original=copy.deepcopy(label_list)\r\n            label_list.reverse()\r\n            input_y_label=np.array([np.max([label_list[0],label_list[1]])],dtype=np.int32)\r\n\r\n            loss, acc, predict, W_projection_value, _ = sess.run([model.loss_val, model.accuracy, model.predictions, model.W_projection, model.train_op],\r\n                                                     feed_dict={model.input_x:input_x, model.input_y_label: input_y_label,\r\n                                                                model.dropout_keep_prob: dropout_keep_prob}) #model.dropout_keep_prob: dropout_keep_prob\r\n            print(i,""loss:"", loss, ""acc:"", acc, ""label_list_original as input x:"",label_list_original,"";input_y_label:"", input_y_label, ""prediction:"", predict)\r\n            if i%1500==0:\r\n                save_path = ckpt_dir + ""model.ckpt""\r\n                saver.save(sess, save_path, global_step=i)\r\n\r\ndef test_predict():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 9+2 #additional two classes:one is for _GO, another is for _END\r\n    learning_rate = 0.001\r\n    batch_size = 1\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 6 #5\r\n    vocab_size = 300\r\n    is_training = False #True\r\n    dropout_keep_prob = 1  # 0.5 #num_sentences\r\n    l2_lambda=0.0001\r\n    d_model=512 #512\r\n    d_k=64\r\n    d_v=64\r\n    h=8\r\n    num_layer=1#6\r\n    embed_size = d_model\r\n    model = Transformer(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                                    vocab_size, embed_size,d_model,d_k,d_v,h,num_layer,is_training,l2_lambda=l2_lambda)\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        ckpt_dir = \'checkpoint_transformer/sequence_reverse/\'\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\r\n        print(""=================restored."")\r\n        for i in range(1500):\r\n            label_list=get_unique_labels()\r\n            input_x = np.array([label_list+[9]],dtype=np.int32)\r\n            label_list_original=copy.deepcopy(label_list)\r\n            label_list.reverse()\r\n            input_y_label=np.array([np.max([label_list[0],label_list[1]])],dtype=np.int32)\r\n\r\n            predict, W_projection_value = sess.run([ model.predictions, model.W_projection], #model.loss_val,--->loss, model.train_op\r\n                                feed_dict={model.input_x:input_x,\r\n                                           model.dropout_keep_prob: dropout_keep_prob})\r\n            print(i, ""label_list_original as input x:"",label_list_original, ""prediction:"", predict,"";label:"",input_y_label) #""acc:"", acc, ""loss:"", loss "";input_y_label:"", input_y_label\r\n\r\ndef get_unique_labels(length=5):\r\n    #if length is  None:\r\n    #    x=[2,3,4,5,6]\r\n    #else:\r\n    x=[i for i in range(2,2+length)]\r\n    random.shuffle(x)\r\n    return x\r\n\r\ndef get_unique_labels_batch(batch_size,length=None):\r\n    x=[]\r\n    for i in range(batch_size):\r\n        labels=get_unique_labels(length=length)\r\n        x.append(labels)\r\n    return x\r\n\r\n\r\n#test_training()\r\n#test_predict()\r\n#test_training_batch()\r\n#test_training()'"
a07_Transformer/data_util_zhihu.py,0,"b'# -*- coding: utf-8 -*-\r\nimport codecs\r\nimport numpy as np\r\n#load data of zhihu\r\nimport word2vec\r\nimport os\r\nimport pickle\r\nPAD_ID = 0\r\nfrom tflearn.data_utils import pad_sequences\r\n_GO=""_GO""\r\n_END=""_END""\r\n_PAD=""_PAD""\r\ndef create_voabulary(simple=None,word2vec_model_path=\'../zhihu-word2vec-title-desc.bin-100\',name_scope=\'\'): #zhihu-word2vec-multilabel.bin-100\r\n    cache_path =\'../cache_vocabulary_label_pik/\'+ name_scope + ""_word_voabulary.pik""\r\n    print(""cache_path:"",cache_path,""file_exists:"",os.path.exists(cache_path))\r\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\r\n        with open(cache_path, \'r\') as data_f:\r\n            vocabulary_word2index, vocabulary_index2word=pickle.load(data_f)\r\n            return vocabulary_word2index, vocabulary_index2word\r\n    else:\r\n        vocabulary_word2index={}\r\n        vocabulary_index2word={}\r\n        if simple is not None:\r\n            word2vec_model_path=\'../zhihu-word2vec.bin-100\'\r\n        print(""create vocabulary. word2vec_model_path:"",word2vec_model_path)\r\n        model=word2vec.load(word2vec_model_path,kind=\'bin\')\r\n        vocabulary_word2index[\'PAD_ID\']=0\r\n        vocabulary_index2word[0]=\'PAD_ID\'\r\n        special_index=0\r\n        if \'biLstmTextRelation\' in name_scope:\r\n            vocabulary_word2index[\'EOS\']=1 # a special token for biLstTextRelation model. which is used between two sentences.\r\n            vocabulary_index2word[1]=\'EOS\'\r\n            special_index=1\r\n        for i,vocab in enumerate(model.vocab):\r\n            vocabulary_word2index[vocab]=i+1+special_index\r\n            vocabulary_index2word[i+1+special_index]=vocab\r\n\r\n        #save to file system if vocabulary of words is not exists.\r\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n            with open(cache_path, \'a\') as data_f:\r\n                pickle.dump((vocabulary_word2index,vocabulary_index2word), data_f)\r\n    return vocabulary_word2index,vocabulary_index2word\r\n\r\n# create vocabulary of lables. label is sorted. 1 is high frequency, 2 is low frequency.\r\ndef create_voabulary_label(voabulary_label=\'../train-zhihu4-only-title-all.txt\',name_scope=\'\',use_seq2seq=False):#\'train-zhihu.txt\'\r\n    print(""create_voabulary_label_sorted.started.traning_data_path:"",voabulary_label)\r\n    cache_path =\'../cache_vocabulary_label_pik/\'+ name_scope + ""_label_voabulary.pik""\r\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\r\n        with open(cache_path, \'r\') as data_f:\r\n            vocabulary_word2index_label, vocabulary_index2word_label=pickle.load(data_f)\r\n            return vocabulary_word2index_label, vocabulary_index2word_label\r\n    else:\r\n        zhihu_f_train = codecs.open(voabulary_label, \'r\', \'utf8\')\r\n        lines=zhihu_f_train.readlines()\r\n        count=0\r\n        vocabulary_word2index_label={}\r\n        vocabulary_index2word_label={}\r\n        vocabulary_label_count_dict={} #{label:count}\r\n        for i,line in enumerate(lines):\r\n            if \'__label__\' in line:  #\'__label__-2051131023989903826\r\n                label=line[line.index(\'__label__\')+len(\'__label__\'):].strip().replace(""\\n"","""")\r\n                if vocabulary_label_count_dict.get(label,None) is not None:\r\n                    vocabulary_label_count_dict[label]=vocabulary_label_count_dict[label]+1\r\n                else:\r\n                    vocabulary_label_count_dict[label]=1\r\n        list_label=sort_by_value(vocabulary_label_count_dict)\r\n\r\n        print(""length of list_label:"",len(list_label));#print("";list_label:"",list_label)\r\n        countt=0\r\n\r\n        ##########################################################################################\r\n        if use_seq2seq:#if used for seq2seq model,insert two special label(token):_GO AND _END\r\n            i_list=[0,1,2];label_special_list=[_GO,_END,_PAD]\r\n            for i,label in zip(i_list,label_special_list):\r\n                vocabulary_word2index_label[label] = i\r\n                vocabulary_index2word_label[i] = label\r\n        #########################################################################################\r\n        for i,label in enumerate(list_label):\r\n            if i<10:\r\n                count_value=vocabulary_label_count_dict[label]\r\n                print(""label:"",label,""count_value:"",count_value)\r\n                countt=countt+count_value\r\n            indexx = i + 3 if use_seq2seq else i\r\n            vocabulary_word2index_label[label]=indexx\r\n            vocabulary_index2word_label[indexx]=label\r\n        print(""count top10:"",countt)\r\n\r\n        #save to file system if vocabulary of words is not exists.\r\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n            with open(cache_path, \'a\') as data_f:\r\n                pickle.dump((vocabulary_word2index_label,vocabulary_index2word_label), data_f)\r\n    print(""create_voabulary_label_sorted.ended.len of vocabulary_label:"",len(vocabulary_index2word_label))\r\n    return vocabulary_word2index_label,vocabulary_index2word_label\r\n\r\ndef sort_by_value(d):\r\n    items=d.items()\r\n    backitems=[[v[1],v[0]] for v in items]\r\n    backitems.sort(reverse=True)\r\n    return [ backitems[i][1] for i in range(0,len(backitems))]\r\n\r\ndef create_voabulary_labelO():\r\n    model = word2vec.load(\'zhihu-word2vec-multilabel.bin-100\', kind=\'bin\') #zhihu-word2vec.bin-100\r\n    count=0\r\n    vocabulary_word2index_label={}\r\n    vocabulary_index2word_label={}\r\n    label_unique={}\r\n    for i,vocab in enumerate(model.vocab):\r\n        if \'__label__\' in vocab:  #\'__label__-2051131023989903826\r\n            label=vocab[vocab.index(\'__label__\')+len(\'__label__\'):]\r\n            if label_unique.get(label,None) is None: #\xe4\xb8\x8d\xe6\x9b\xbe\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe4\xbf\x9d\xe6\x8c\x81\xe5\x88\xb0\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\r\n                vocabulary_word2index_label[label]=count\r\n                vocabulary_index2word_label[count]=label #ADD\r\n                count=count+1\r\n                label_unique[label]=label\r\n    return vocabulary_word2index_label,vocabulary_index2word_label\r\n\r\ndef load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\r\n                             traning_data_path=\'../train-zhihu4-only-title-all.txt\',multi_label_flag=True,use_seq2seq=False,seq2seq_label_length=6):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\r\n    print(""load_data.started..."")\r\n    print(""load_data_multilabel_new.training_data_path:"",traning_data_path)\r\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    Y = []\r\n    Y_decoder_input=[] #ADD 2017-06-15\r\n    for i, line in enumerate(lines):\r\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        y=y.strip().replace(\'\\n\',\'\')\r\n        x = x.strip()\r\n        if i<1:\r\n            print(i,""x0:"",x) #get raw x\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<2:\r\n            print(i,""x1:"",x) #word to index\r\n        if use_seq2seq:        # 1)prepare label for seq2seq format(ADD _GO,_END,_PAD for seq2seq)\r\n            ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\r\n            _PAD_INDEX=vocabulary_word2index_label[_PAD]\r\n            ys_mulithot_list=[_PAD_INDEX]*seq2seq_label_length #[3,2,11,14,1]\r\n            ys_decoder_input=[_PAD_INDEX]*seq2seq_label_length\r\n            # below is label.\r\n            for j,y in enumerate(ys):\r\n                if j<seq2seq_label_length-1:\r\n                    ys_mulithot_list[j]=vocabulary_word2index_label[y]\r\n            if len(ys)>seq2seq_label_length-1:\r\n                ys_mulithot_list[seq2seq_label_length-1]=vocabulary_word2index_label[_END]#ADD END TOKEN\r\n            else:\r\n                ys_mulithot_list[len(ys)] = vocabulary_word2index_label[_END]\r\n\r\n            # below is input for decoder.\r\n            ys_decoder_input[0]=vocabulary_word2index_label[_GO]\r\n            for j,y in enumerate(ys):\r\n                if j < seq2seq_label_length - 1:\r\n                    ys_decoder_input[j+1]=vocabulary_word2index_label[y]\r\n            if i<10:\r\n                print(i,""ys:==========>0"", ys)\r\n                print(i,""ys_mulithot_list:==============>1"", ys_mulithot_list)\r\n                print(i,""ys_decoder_input:==============>2"", ys_decoder_input)\r\n        else:\r\n            if multi_label_flag: # 2)prepare multi-label format for classification\r\n                ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\r\n                ys_index=[]\r\n                for y in ys:\r\n                    y_index = vocabulary_word2index_label[y]\r\n                    ys_index.append(y_index)\r\n                ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\r\n            else:                #3)prepare single label format for classification\r\n                ys_mulithot_list=vocabulary_word2index_label[y]\r\n        if i<=3:\r\n            print(""ys_index:"")\r\n            #print(ys_index)\r\n            print(i,""y:"",y,"" ;ys_mulithot_list:"",ys_mulithot_list) #,"" ;ys_decoder_input:"",ys_decoder_input)\r\n        X.append(x)\r\n        Y.append(ys_mulithot_list)\r\n        if use_seq2seq:\r\n            Y_decoder_input.append(ys_decoder_input) #decoder input\r\n        #if i>50000:\r\n        #    break\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    print(""number_examples:"",number_examples) #\r\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\r\n    if use_seq2seq:\r\n        train=train+(Y_decoder_input[0:int((1 - valid_portion) * number_examples)],)\r\n        test=test+(Y_decoder_input[int((1 - valid_portion) * number_examples) + 1:],)\r\n    # 5.return\r\n    print(""load_data.ended..."")\r\n    return train, test, test\r\n\r\ndef load_data_multilabel_new_twoCNN(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\r\n                                    traning_data_path=\'train-zhihu4-only-title-all.txt\',multi_label_flag=True):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\r\n    print(""load_data.twoCNN.started..."")\r\n    print(""load_data_multilabel_new_twoCNN.training_data_path:"",traning_data_path)\r\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    X2=[]\r\n    Y = []\r\n    count_error=0\r\n    for i, line in enumerate(lines):\r\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        y=y.strip().replace(\'\\n\',\'\')\r\n        x = x.strip()\r\n        #print(""x:===============>"",x)\r\n        try:\r\n            x,x2=x.split(""\\t"")\r\n        except Exception:\r\n            print(""x.split.error."",x,""count_error:"",count_error)\r\n            count_error+=1\r\n            continue\r\n        if i<1:\r\n            print(i,""x0:"",x) #get raw x\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        x2=x2.split("" "")\r\n        x2 =[vocabulary_word2index.get(e, 0) for e in x2]\r\n        if i<1:\r\n            print(i,""x1:"",x,""x2:"",x2) #word to index\r\n        if multi_label_flag:\r\n            ys = y.replace(\'\\n\', \'\').split("" "") #ys is a list\r\n            ys_index=[]\r\n            for y in ys:\r\n                y_index = vocabulary_word2index_label[y]\r\n                ys_index.append(y_index)\r\n            ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\r\n        else:\r\n            ys_mulithot_list=int(y) #vocabulary_word2index_label[y]\r\n        if i<1:\r\n            print(i,""y:"",y,""ys_mulithot_list:"",ys_mulithot_list)\r\n        X.append(x)\r\n        X2.append(x2)\r\n        Y.append(ys_mulithot_list)\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    print(""number_examples:"",number_examples) #\r\n    train = (X[0:int((1 - valid_portion) * number_examples)],X2[0:int((1 - valid_portion) * number_examples)],Y[0:int((1 - valid_portion) * number_examples)])\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], X2[int((1 - valid_portion) * number_examples) + 1:],Y[int((1 - valid_portion) * number_examples) + 1:])\r\n    # 5.return\r\n    print(""load_data.ended..."")\r\n    return train, test, test\r\n\r\ndef load_data(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,training_data_path=\'train-zhihu4-only-title-all.txt\'):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\r\n    print(""load_data.started..."")\r\n    zhihu_f = codecs.open(training_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    Y = []\r\n    for i, line in enumerate(lines):\r\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        y=y.replace(\'\\n\',\'\')\r\n        x = x.replace(""\\t"",\' EOS \').strip()\r\n        if i<5:\r\n            print(""x0:"",x) #get raw x\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        #if i<5:\r\n        #    print(""x1:"",x_) #\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<5:\r\n            print(""x1:"",x) #word to index\r\n        y = vocabulary_word2index_label[y] #np.abs(hash(y))\r\n        X.append(x)\r\n        Y.append(y)\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    print(""number_examples:"",number_examples) #\r\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\r\n    # 5.return\r\n    print(""load_data.ended..."")\r\n    return train, test, test\r\n\r\n # \xe5\xb0\x86\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba(uigram,bigram,trigram)\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\r\ndef process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3):\r\n    """"""\r\n    :param sentence: string. example:\'w17314 w5521 w7729 w767 w10147 w111\'\r\n    :param n_gram:\r\n    :return:string. example:\'w17314 w17314w5521 w17314w5521w7729 w5521 w5521w7729 w5521w7729w767 w7729 w7729w767 w7729w767w10147 w767 w767w10147 w767w10147w111 w10147 w10147w111 w111\'\r\n    """"""\r\n    result=[]\r\n    word_list=sentence.split("" "") #[sentence[i] for i in range(len(sentence))]\r\n    unigram=\'\';bigram=\'\';trigram=\'\';fourgram=\'\'\r\n    length_sentence=len(word_list)\r\n    for i,word in enumerate(word_list):\r\n        unigram=word                           #ui-gram\r\n        word_i=unigram\r\n        if n_gram>=2 and i+2<=length_sentence: #bi-gram\r\n            bigram="""".join(word_list[i:i+2])\r\n            word_i=word_i+\' \'+bigram\r\n        if n_gram>=3 and i+3<=length_sentence: #tri-gram\r\n            trigram="""".join(word_list[i:i+3])\r\n            word_i = word_i + \' \' + trigram\r\n        if n_gram>=4 and i+4<=length_sentence: #four-gram\r\n            fourgram="""".join(word_list[i:i+4])\r\n            word_i = word_i + \' \' + fourgram\r\n        if n_gram>=5 and i+5<=length_sentence: #five-gram\r\n            fivegram="""".join(word_list[i:i+5])\r\n            word_i = word_i + \' \' + fivegram\r\n        result.append(word_i)\r\n    result="" "".join(result)\r\n    return result\r\n\r\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xa0\x87\xe7\xad\xbe\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaalabel\xef\xbc\x9aload data with multi-labels\r\ndef load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,traning_path,valid_portion=0.05,max_training_data=1000000):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example: \'w140 w13867 w10344 w2673 w9514 w269 w460 w6 w35053 w844 w10147 w111 __label__-2379261820462209275 -5535923551616745326 6038661761506862294\'\r\n    print(""load_data_with_multilabels.ended..."")\r\n    zhihu_f = codecs.open(traning_path,\'r\',\'utf8\') #(\'/home/xul/xul/9_ZhihuCup/\'+data_type+\'-zhihu5-only-title-multilabel.txt\', \'r\', \'utf8\') #home/xul/xul/9_ZhihuCup/\'\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    Y = []\r\n    Y_label1999=[]\r\n    for i, line in enumerate(lines):\r\n        #if i>max_training_data:\r\n        #    break\r\n        x, ys = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        ys=ys.replace(\'\\n\',\'\').split("" "")\r\n        x = x.strip()\r\n        if i < 5:\r\n            print(""x0:"", x) # u\'w4260 w4260w86860 w4260w86860w30907 w86860 w86860w30907 w86860w30907w11 w30907 w30907w11 w30907w11w31 w11 w11w31 w11w31w72 w31 w31w72 w31w72w166 w72 w72w166 w72w166w346 w166 w166w346 w166w346w2182 w346 w346w2182 w346w2182w224 w2182 w2182w224 w2182w224w2148 w224 w224w2148 w224w2148w6 w2148 w2148w6 w2148w6w2566 w6 w6w2566 w6w2566w25 w2566 w2566w25 w2566w25w1110 w25 w25w1110 w25w1110w111 w1110 w1110w111 w111\'\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        #if i < 5:\r\n        #    print(""x1:"", x_)\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<5:\r\n            print(""x2:"", x)\r\n        #print(""ys:"",ys) #[\'501174938575526146\', \'-4317515119936650885\']\r\n        ys_list=[]\r\n        for y in ys:\r\n            y_ = vocabulary_word2index_label[y]\r\n            ys_list.append(y_)\r\n        X.append(x)\r\n        #TODO ys_list_array=transform_multilabel_as_multihot(ys_list) #it is 2-d array. [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]...]\r\n        ys_list_=proces_label_to_algin(ys_list)\r\n        Y.append(ys_list_)\r\n        #TODO Y_label1999.append(ys_list_array)\r\n        if i==0:\r\n            print(X,Y)\r\n            print(Y_label1999)\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)]) #TODO Y_label1999[0:int((1 - valid_portion) * number_examples)]\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:]) #TODO ,Y_label1999[int((1 - valid_portion) * number_examples) + 1:]\r\n    print(""load_data_with_multilabels.ended..."")\r\n    return train, test\r\n\r\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\r\ndef transform_multilabel_as_multihot(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\r\n    """"""\r\n    :param label_list: e.g.[0,1,4]\r\n    :param label_size: e.g.199\r\n    :return:e.g.[1,1,0,1,0,0,........]\r\n    """"""\r\n    result=np.zeros(label_size)\r\n    #set those location as 1, all else place as 0.\r\n    result[label_list] = 1\r\n    return result\r\n\r\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\r\ndef transform_multilabel_as_multihotO(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\r\n    batch_size=len(label_list)\r\n    result=np.zeros((batch_size,label_size))\r\n    #set those location as 1, all else place as 0.\r\n    result[(range(batch_size),label_list)]=1\r\n    return result\r\n\r\ndef load_final_test_data(file_path):\r\n    final_test_file_predict_object = codecs.open(file_path, \'r\', \'utf8\')\r\n    lines=final_test_file_predict_object.readlines()\r\n    question_lists_result=[]\r\n    for i,line in enumerate(lines):\r\n        question_id,question_string=line.split(""\\t"")\r\n        question_string=question_string.strip().replace(""\\n"","""")\r\n        question_lists_result.append((question_id,question_string))\r\n    print(""length of total question lists:"",len(question_lists_result))\r\n    return question_lists_result\r\n\r\ndef load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists,uni_to_tri_gram=False):  # n_words=100000,\r\n    final_list=[]\r\n    for i, tuplee in enumerate(questionid_question_lists):\r\n        queston_id,question_string_list=tuplee\r\n        if uni_to_tri_gram:\r\n            x_=process_one_sentence_to_get_ui_bi_tri_gram(question_string_list)\r\n            x=x_.split("" "")\r\n        else:\r\n            x=question_string_list.split("" "")\r\n        x = [vocabulary_word2index.get(e, 0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<=2:\r\n            print(""question_id:"",queston_id);print(""question_string_list:"",question_string_list);print(""x_indexed:"",x)\r\n        final_list.append((queston_id,x))\r\n    number_examples = len(final_list)\r\n    print(""number_examples:"",number_examples) #\r\n    return  final_list\r\n\r\n\r\ndef proces_label_to_algin(ys_list,require_size=5):\r\n    """"""\r\n    :param ys_list: a list\r\n    :return: a list\r\n    """"""\r\n    ys_list_result=[0 for x in range(require_size)]\r\n    if len(ys_list)>=require_size: #\xe8\xb6\x85\xe9\x95\xbf\r\n        ys_list_result=ys_list[0:require_size]\r\n    else:#\xe5\xa4\xaa\xe7\x9f\xad\r\n       if len(ys_list)==1:\r\n           ys_list_result =[ys_list[0] for x in range(require_size)]\r\n       elif len(ys_list)==2:\r\n           ys_list_result = [ys_list[0],ys_list[0],ys_list[0],ys_list[1],ys_list[1]]\r\n       elif len(ys_list) == 3:\r\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[1], ys_list[2]]\r\n       elif len(ys_list) == 4:\r\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[2], ys_list[3]]\r\n    return ys_list_result\r\n\r\ndef write_uigram_to_trigram():\r\n    pass\r\n    #1.read file.\r\n    #2.uigram--->trigram\r\n    #3.write each line to file system.\r\n\r\ndef test_pad():\r\n    trainX=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\r\n    trainX=trainX.split("" "")\r\n    trainX = pad_sequences([[trainX]], maxlen=100, value=0.)\r\n    print(""trainX:"",trainX)\r\n\r\ntopic_info_file_path=\'topic_info.txt\'\r\ndef read_topic_info():\r\n    f = codecs.open(topic_info_file_path, \'r\', \'utf8\')\r\n    lines=f.readlines()\r\n    dict_questionid_title={}\r\n    for i,line in enumerate(lines):\r\n        topic_id,partent_ids,title_character,title_words,desc_character,decs_words=line.split(""\\t"").strip()\r\n        # print(i,""------------------------------------------------------"")\r\n        # print(""topic_id:"",topic_id)\r\n        # print(""partent_ids:"",partent_ids)\r\n        # print(""title_character:"",title_character)\r\n        # print(""title_words:"",title_words)\r\n        # print(""desc_character:"",desc_character)\r\n        # print(""decs_words:"",decs_words)\r\n        dict_questionid_title[topic_id]=title_words+"" ""+decs_words\r\n    print(""len(dict_questionid_title):"",len(dict_questionid_title))\r\n    return dict_questionid_title\r\n\r\ndef stat_training_data_length():\r\n    training_data=\'train-zhihu4-only-title-all.txt\'\r\n    f = codecs.open(training_data, \'r\', \'utf8\')\r\n    lines=f.readlines()\r\n    length_dict={0:0,5:0,10:0,15:0,20:0,25:0,30:0,35:0,40:0,100:0,150:0,200:0,1500:0}\r\n    length_list=[0,5,10,15,20,25,30,35,40,100,150,200,1500]\r\n    for i,line in enumerate(lines):\r\n        line_list=line.split(\'__label__\')[0].strip().split("" "")\r\n        length=len(line_list)\r\n        #print(i,""length:"",length)\r\n        for l in length_list:\r\n            if length<l:\r\n                length=l\r\n                #print(""length.assigned:"",length)\r\n                break\r\n        #print(""length.before dict assign:"", length)\r\n        length_dict[length]=length_dict[length]+1\r\n    print(""length_dict:"",length_dict)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    if __name__ == \'__main__\':\r\n        if __name__ == \'__main__\':\r\n            #1.\r\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\r\n            #vocabulary_word2index_label, vocabulary_index2word_label=create_voabulary_label()\r\n            #load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,data_type=\'test\')\r\n            #2.\r\n            #sentence=u\'\xe6\x88\x91\xe6\x83\xb3\xe5\xbc\x80\xe9\x80\x9a\xe5\x88\x9b\xe4\xb8\x9a\xe6\x9d\xbf\'\r\n            #sentence=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\r\n            #result=process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3)\r\n            #print(len(result),""result:"",result)\r\n\r\n            #3. transform to multilabel\r\n            #label_list=[0,1,4,9,5]\r\n            #result=transform_multilabel_as_multihot(label_list,label_size=15)\r\n            #print(""result:"",result)\r\n\r\n            #4.load data for predict-----------------------------------------------------------------\r\n            #file_path=\'test-zhihu-forpredict-v4only-title.txt\'\r\n            #questionid_question_lists=load_final_test_data(file_path)\r\n\r\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\r\n            #vocabulary_word2index_label,_=create_voabulary_label()\r\n            #final_list=load_data_predict(vocabulary_word2index, vocabulary_word2index_label, questionid_question_lists)\r\n\r\n            #5.process label require lengh\r\n            #ys_list=[99999]\r\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\r\n            #print(ys_list,""ys_list_result1.:"",ys_list_result)\r\n            #ys_list=[99999,23423432,67566765]\r\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\r\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\r\n            #ys_list=[99999,23423432,67566765,23333333]\r\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\r\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\r\n            #ys_list = [99999, 23423432, 67566765,44543543,546546546,323423434]\r\n            #ys_list_result = proces_label_to_algin(ys_list, require_size=5)\r\n            #print(ys_list, ""ys_list_result3.:"", ys_list_result)\r\n\r\n            #6.create vocabulary label. sorted.\r\n            #create_voabulary_label()\r\n\r\n            #d={\'a\':3,\'b\':2,\'c\':11}\r\n            #d_=sort_by_value(d)\r\n            #print(""d_"",d_)\r\n\r\n            #7.\r\n            #test_pad()\r\n\r\n            #8.read topic info\r\n            #read_topic_info()\r\n\r\n            #9\xe3\x80\x82\r\n            stat_training_data_length()\r\n'"
a08_EntityNetwork/a3_entity_network.py,99,"b'# -*- coding: utf-8 -*-\r\n# EntityNet:1.input encoder  2. dynamic emeory 3.output layer\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib as tf_contrib\r\nimport numpy as np\r\nfrom tensorflow.contrib import rnn\r\n#from  a07_Transformer.a2_multi_head_attention import MultiHeadAttention\r\n\r\nclass EntityNetwork:\r\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length, story_length,\r\n                 vocab_size, embed_size,hidden_size, is_training, multi_label_flag=False,block_size=20,\r\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,use_bi_lstm=False,use_additive_attention=False):#0.01\r\n        """"""init all hyperparameter here""""""\r\n        # set hyperparamter\r\n        self.num_classes = num_classes\r\n        self.batch_size = batch_size\r\n        self.sequence_length = sequence_length\r\n        self.vocab_size = vocab_size\r\n        self.embed_size = embed_size\r\n        self.is_training = is_training\r\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")#TODO ADD learning_rate\r\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\r\n        self.initializer = initializer\r\n        self.multi_label_flag = multi_label_flag\r\n        self.hidden_size = hidden_size\r\n        self.clip_gradients=clip_gradients\r\n        self.story_length=story_length\r\n        self.block_size=block_size\r\n        self.use_bi_lstm=use_bi_lstm\r\n        self.dimension=self.hidden_size*2 if self.use_bi_lstm else self.hidden_size #if use bi-lstm, set dimension value, so it can be used later for parameter.\r\n        self.use_additive_attention=use_additive_attention\r\n\r\n        # add placeholder (X,label)\r\n        # self.input_x = tf.placeholder(tf.int32, [None, self.num_sentences,self.sequence_length], name=""input_x"")  # X\r\n        self.story=tf.placeholder(tf.int32,[None,self.story_length,self.sequence_length],name=""story"")\r\n        self.query = tf.placeholder(tf.int32, [None, self.sequence_length], name=""question"")\r\n\r\n        self.answer_single = tf.placeholder(tf.int32, [None,], name=""input_y"")  # y:[None,num_classes]\r\n        self.answer_multilabel = tf.placeholder(tf.float32, [None, self.num_classes],name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\r\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\r\n\r\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\r\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\r\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\r\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\r\n\r\n        self.instantiate_weights()\r\n        self.logits = self.inference()  # [None, self.label_size]. main computation graph is here.\r\n\r\n        self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\r\n        if not self.multi_label_flag:\r\n            correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32),self.answer_single)  # tf.argmax(self.logits, 1)-->[batch_size]\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"")  # shape=()\r\n        else:\r\n            self.accuracy = tf.constant(0.5)  # fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\r\n\r\n        if not is_training:\r\n            return\r\n        if multi_label_flag:\r\n            print(""going to use multi label loss."")\r\n            self.loss_val = self.loss_multilabel()\r\n        else:\r\n            print(""going to use single label loss."")\r\n            self.loss_val = self.loss()\r\n        self.train_op = self.train()\r\n\r\n    def inference(self):\r\n        """"""main computation graph here: 1.input encoder 2.dynamic emeory 3.output layer """"""\r\n        # 1.input encoder\r\n        self.embedding_with_mask()\r\n        if self.use_bi_lstm:\r\n            self.input_encoder_bi_lstm()\r\n        else:\r\n            self.input_encoder_bow()\r\n        # 2. dynamic emeory\r\n        self.hidden_state=self.rnn_story() #[batch_size,block_size,hidden_size]. get hidden state after process the story\r\n\r\n        # 3.output layer\r\n        logits=self.output_module() #[batch_size,vocab_size]\r\n        return logits\r\n\r\n    def embedding_with_mask(self):\r\n        # 1.1 embedding for story and query\r\n        story_embedding = tf.nn.embedding_lookup(self.Embedding,self.story)  # [batch_size,story_length,sequence_length,embed_size]\r\n        query_embedding=tf.nn.embedding_lookup(self.Embedding,self.query)    # [batch_size,sequence_length,embed_size]\r\n        # 1.2 mask for story and query\r\n        story_mask=tf.get_variable(""story_mask"",[self.sequence_length,1],initializer=tf.constant_initializer(1.0))\r\n        query_mask=tf.get_variable(""query_mask"",[self.sequence_length,1],initializer=tf.constant_initializer(1.0))\r\n        # 1.3 multiply of embedding and mask for story and query\r\n        self.story_embedding=tf.multiply(story_embedding,story_mask)  # [batch_size,story_length,sequence_length,embed_size]\r\n        self.query_embedding=tf.multiply(query_embedding,query_mask)  # [batch_size,sequence_length,embed_size]\r\n\r\n    def input_encoder_bow(self):\r\n        # 1.4 use bag of words to encoder story and query\r\n        self.story_embedding=tf.reduce_sum(self.story_embedding,axis=2) #[batch_size,story_length,embed_size]\r\n        self.query_embedding=tf.reduce_sum(self.query_embedding,axis=1)  #[batch_size,embed_size]\r\n\r\n    def input_encoder_bi_lstm(self):\r\n        """"""use bi-directional lstm to encode query_embedding:[batch_size,sequence_length,embed_size]\r\n                                         and story_embedding:[batch_size,story_length,sequence_length,embed_size]\r\n        output:query_embedding:[batch_size,hidden_size*2]  story_embedding:[batch_size,self.story_length,self.hidden_size*2]\r\n        """"""\r\n        #1. encode query: bi-lstm layer\r\n        lstm_fw_cell = rnn.BasicLSTMCell(self.hidden_size)  # forward direction cell\r\n        lstm_bw_cell = rnn.BasicLSTMCell(self.hidden_size)  # backward direction cell\r\n        if self.dropout_keep_prob is not None:\r\n            lstm_fw_cell = rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\r\n            lstm_bw_cell == rnn.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\r\n        query_hidden_output, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, self.query_embedding,dtype=tf.float32,scope=""query_rnn"")  # [batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\r\n        query_hidden_output = tf.concat(query_hidden_output, axis=2) #[batch_size,sequence_length,hidden_size*2]\r\n        self.query_embedding=tf.reduce_sum(query_hidden_output,axis=1) #[batch_size,hidden_size*2]\r\n        print(""input_encoder_bi_lstm.self.query_embedding:"",self.query_embedding)\r\n\r\n        #2. encode story\r\n        # self.story_embedding:[batch_size,story_length,sequence_length,embed_size]\r\n        self.story_embedding=tf.reshape(self.story_embedding,shape=(-1,self.story_length*self.sequence_length,self.embed_size)) #[self.story_length*self.sequence_length,self.embed_size]\r\n        lstm_fw_cell_story = rnn.BasicLSTMCell(self.hidden_size)  # forward direction cell\r\n        lstm_bw_cell_story = rnn.BasicLSTMCell(self.hidden_size)  # backward direction cell\r\n        if self.dropout_keep_prob is not None:\r\n            lstm_fw_cell_story = rnn.DropoutWrapper(lstm_fw_cell_story, output_keep_prob=self.dropout_keep_prob)\r\n            lstm_bw_cell_story == rnn.DropoutWrapper(lstm_bw_cell_story, output_keep_prob=self.dropout_keep_prob)\r\n        story_hidden_output, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell_story, lstm_bw_cell_story, self.story_embedding,dtype=tf.float32,scope=""story_rnn"")\r\n        story_hidden_output=tf.concat(story_hidden_output,axis=2) #[batch_size,story_length*sequence_length,hidden_size*2]\r\n        story_hidden_output=tf.reshape(story_hidden_output,shape=(-1,self.story_length,self.sequence_length,self.hidden_size*2))\r\n        self.story_embedding = tf.reduce_sum(story_hidden_output, axis=2)  # [batch_size,self.story_length,self.hidden_size*2]\r\n\r\n    def activation(self,features, scope=None):  # scope=None\r\n        with tf.variable_scope(scope, \'PReLU\', initializer=self.initializer):\r\n            alpha = tf.get_variable(\'alpha\', features.get_shape().as_list()[1:])\r\n            pos = tf.nn.relu(features)\r\n            neg = alpha * (features - tf.abs(features)) * 0.5\r\n            return pos + neg\r\n\r\n    def output_module(self):\r\n        """"""\r\n        1.use attention mechanism between query and hidden states, to get weighted sum of hidden state. 2.non-linearity of query and hidden state to get label.\r\n        input: query_embedding:[batch_size,embed_size], hidden state:[batch_size,block_size,hidden_size] of memory\r\n        :return:y: predicted label.[]\r\n        """"""\r\n        # 1.use attention mechanism between query and hidden states, to get weighted sum of hidden state.\r\n        # 1.1 get possibility distribution (of similiarity)\r\n        p=tf.nn.softmax(tf.multiply(tf.expand_dims(self.query_embedding,axis=1),self.hidden_state)) #shape:[batch_size,block_size,hidden_size]<---query_embedding_expand:[batch_size,1,hidden_size]; hidden_state:[batch_size,block_size,hidden_size]\r\n        # 1.2 get weighted sum of hidden state\r\n        u=tf.reduce_sum(tf.multiply(p,self.hidden_state),axis=1) #shape:[batch_size,hidden_size]<----------([batch_size,block_size,hidden_size],[batch_size,block_size,hidden_size])\r\n\r\n        # 2.non-linearity of query and hidden state to get label\r\n        H_u_matmul=tf.matmul(u,self.H)+self.h_u_bias #shape:[batch_size,hidden_size]<----([batch_size,hidden_size],[hidden_size,hidden_size])\r\n        activation=self.activation(self.query_embedding + H_u_matmul,scope=""query_add_hidden"")           #shape:[batch_size,hidden_size]\r\n        activation = tf.nn.dropout(activation,keep_prob=self.dropout_keep_prob) #shape:[batch_size,hidden_size]\r\n        y=tf.matmul(activation,self.R)+self.y_bias #shape:[batch_size,vocab_size]<-----([batch_size,hidden_size],[hidden_size,vocab_size])\r\n        return y #shape:[batch_size,vocab_size]\r\n\r\n    def rnn_story(self):\r\n        """"""\r\n        run rnn for story to get last hidden state\r\n        input is:  story:                 [batch_size,story_length,embed_size]\r\n        :return:   last hidden state.     [batch_size,embed_size]\r\n        """"""\r\n        # 1.split input to get lists.\r\n        input_split=tf.split(self.story_embedding,self.story_length,axis=1) #a list.length is:story_length.each element is:[batch_size,1,embed_size]\r\n        input_list=[tf.squeeze(x,axis=1) for x in input_split]           #a list.length is:story_length.each element is:[batch_size,embed_size]\r\n        # 2.init keys(w_all) and values(h_all) of memory\r\n        h_all=tf.get_variable(""hidden_states"",shape=[self.block_size,self.dimension],initializer=self.initializer)# [block_size,hidden_size]\r\n        w_all=tf.get_variable(""keys"",          shape=[self.block_size,self.dimension],initializer=self.initializer)# [block_size,hidden_size]\r\n        # 3.expand keys and values to prepare operation of rnn\r\n        w_all_expand=tf.tile(tf.expand_dims(w_all,axis=0),[self.batch_size,1,1]) #[batch_size,block_size,hidden_size]\r\n        h_all_expand=tf.tile(tf.expand_dims(h_all,axis=0),[self.batch_size,1,1]) #[batch_size,block_size,hidden_size]\r\n        # 4. run rnn using input with cell.\r\n        for i,input in enumerate(input_list):\r\n            h_all_expand=self.cell(input,h_all_expand,w_all_expand,i) #w_all:[batch_size,block_size,hidden_size]; h_all:[batch_size,block_size,hidden_size]\r\n        return h_all_expand #[batch_size,block_size,hidden_size]\r\n\r\n    def cell(self,s_t,h_all,w_all,i):\r\n        """"""\r\n        parallel implementation of single time step for compute of input with memory\r\n        :param s_t:   [batch_size,hidden_size].vector representation of current input(is a sentence).notice:hidden_size=embedding_size\r\n        :param w_all: [batch_size,block_size,hidden_size]\r\n        :param h_all: [batch_size,block_size,hidden_size]\r\n        :return: new hidden state: [batch_size,block_size,hidden_size]\r\n        """"""\r\n        # 1.gate\r\n        s_t_expand=tf.expand_dims(s_t, axis=1)       #[batch_size,1,hidden_size]\r\n        g=tf.nn.sigmoid(tf.multiply(s_t_expand,h_all)+tf.multiply(s_t_expand,w_all))#shape:[batch_size,block_size,hidden_size]\r\n\r\n        # 2.candidate hidden state\r\n        #below\' shape:[batch_size*block_size,hidden_size]\r\n        h_candidate_part1=tf.matmul(tf.reshape(h_all,shape=(-1,self.dimension)), self.U) + tf.matmul(tf.reshape(w_all,shape=(-1,self.dimension)), self.V)+self.h_bias\r\n        print(""======>h_candidate_part1:"",h_candidate_part1) #(160, 100)\r\n        h_candidate_part1=tf.reshape(h_candidate_part1,shape=(self.batch_size,self.block_size,self.dimension)) #[batch_size,block_size,hidden_size]\r\n        h_candidate_part2=tf.expand_dims(tf.matmul(s_t,self.W)+self.h2_bias,axis=1)              #shape:[batch_size,1,hidden_size]\r\n        h_candidate=self.activation(h_candidate_part1+h_candidate_part2,scope=""h_candidate""+str(i))   #shape:[batch_size,block_size,hidden_size]\r\n\r\n        # 3.update hidden state\r\n        h_all=h_all+tf.multiply(g,h_candidate) #shape:[batch_size,block_size,hidden_size]\r\n\r\n        # 4.normalized hidden state\r\n        h_all=tf.nn.l2_normalize(h_all,-1) #shape:[batch_size,block_size,hidden_size]\r\n        return h_all  #shape:[batch_size,block_size,hidden_size]\r\n\r\n    def loss(self, l2_lambda=0.0001):  # 0.001\r\n        with tf.name_scope(""loss""):\r\n            # input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\r\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\r\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.answer_single,logits=self.logits);  # sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\r\n            # print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\r\n            loss = tf.reduce_mean(losses)  # print(""2.loss.loss:"", loss) #shape=()\r\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if (\'bias\' not in v.name ) and (\'alpha\' not in v.name)]) * l2_lambda\r\n            loss = loss + l2_losses\r\n        return loss\r\n\r\n    def loss_multilabel(self, l2_lambda=0.0001): #this loss function is for multi-label classification\r\n        with tf.name_scope(""loss""):\r\n            # input_y:shape=(?, 1999); logits:shape=(?, 1999)\r\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\r\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.answer_multilabel,logits=self.logits);  #[None,self.num_classes]. losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\r\n            #losses=self.smoothing_cross_entropy(self.logits,self.answer_multilabel,self.num_classes) #shape=(512,)\r\n            losses = tf.reduce_sum(losses, axis=1)  # shape=(?,). loss for all data in the batch\r\n            loss = tf.reduce_mean(losses)  # shape=().   average loss in the batch\r\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if(\'bias\' not in v.name ) and (\'alpha\' not in v.name)]) * l2_lambda\r\n            loss = loss + l2_losses\r\n        return loss\r\n\r\n    def smoothing_cross_entropy(self,logits, labels, vocab_size, confidence=0.9): #confidence = 1.0 - label_smoothing. where label_smooth=0.1. from http://github.com/tensorflow/tensor2tensor\r\n        """"""Cross entropy with label smoothing to limit over-confidence.""""""\r\n        with tf.name_scope(""smoothing_cross_entropy"", [logits, labels]):\r\n            # Low confidence is given to all non-true labels, uniformly.\r\n            low_confidence = (1.0 - confidence) / tf.to_float(vocab_size - 1)\r\n            # Normalizing constant is the best cross-entropy value with soft targets.\r\n            # We subtract it just for readability, makes no difference on learning.\r\n            normalizing = -(confidence * tf.log(confidence) + tf.to_float(vocab_size - 1) * low_confidence * tf.log(low_confidence + 1e-20))\r\n            # Soft targets.\r\n            soft_targets = tf.one_hot(\r\n                tf.cast(labels, tf.int32),\r\n                depth=vocab_size,\r\n                on_value=confidence,\r\n                off_value=low_confidence)\r\n            xentropy = tf.nn.softmax_cross_entropy_with_logits(\r\n                logits=logits, labels=soft_targets)\r\n        return xentropy - normalizing\r\n\r\n    def train(self):\r\n        """"""based on the loss, use SGD to update parameter""""""\r\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\r\n                                                   self.decay_rate, staircase=True)\r\n        self.learning_rate_=learning_rate\r\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\r\n        train_op = tf_contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\r\n                                                   learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\r\n        return train_op\r\n\r\n    #:param s_t: vector representation of current input(is a sentence). shape:[batch_size,sequence_length,embed_size]\r\n    #:param h: value(hidden state).shape:[hidden_size]\r\n    #:param w: key.shape:[hidden_size]\r\n    def instantiate_weights(self):\r\n        """"""define all weights here""""""\r\n        with tf.variable_scope(""output_module""):\r\n            self.H=tf.get_variable(""H"",shape=[self.dimension,self.dimension],initializer=self.initializer)\r\n            self.R = tf.get_variable(""R"", shape=[self.dimension, self.num_classes], initializer=self.initializer)\r\n            self.y_bias=tf.get_variable(""y_bias"",shape=[self.num_classes])\r\n            self.b_projected = tf.get_variable(""b_projection"", shape=[self.num_classes])\r\n            self.h_u_bias=tf.get_variable(""h_u_bias"",shape=[self.dimension])\r\n\r\n        with tf.variable_scope(""dynamic_memory""):\r\n            self.U=tf.get_variable(""U"",shape=[self.dimension,self.dimension],initializer=self.initializer)\r\n            self.V=tf.get_variable(""V"",shape=[self.dimension,self.dimension],initializer=self.initializer)\r\n            self.W=tf.get_variable(""W"",shape=[self.dimension,self.dimension],initializer=self.initializer)\r\n            self.h_bias=tf.get_variable(""h_bias"",shape=[self.dimension])\r\n            self.h2_bias = tf.get_variable(""h2_bias"", shape=[self.dimension])\r\n\r\n        with tf.variable_scope(""embedding_projection""):  # embedding matrix\r\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)\r\n            self.Embedding_label = tf.get_variable(""Embedding_label"", shape=[self.num_classes, self.embed_size],dtype=tf.float32) #,initializer=self.initializer\r\n            #self.W_projection = tf.get_variable(""W_projection"", shape=[self.hidden_size * 4, self.num_classes],initializer=self.initializer)  # [embed_size,label_size]\r\n            #self.b_projection = tf.get_variable(""b_projection"", shape=[self.num_classes])\r\n\r\n        with tf.variable_scope(""attention""):\r\n            self.W_w_attention_word = tf.get_variable(""W_w_attention_word"",shape=[self.hidden_size * 2, self.hidden_size * 2],initializer=self.initializer)\r\n            self.W_b_attention_word = tf.get_variable(""W_b_attention_word"", shape=[self.hidden_size * 2])\r\n            self.context_vecotor_word = tf.get_variable(""what_is_the_informative_word"", shape=[self.hidden_size * 2],initializer=self.initializer)  # TODO o.k to use batch_size in first demension?\r\n\r\n# test: learn to count. weight of query and story is different\r\ndef test():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 15\r\n    learning_rate = 0.001\r\n    batch_size = 8\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 10\r\n    vocab_size = 10000\r\n    embed_size = 100\r\n    hidden_size = 100\r\n    is_training = True\r\n    story_length = 3\r\n    dropout_keep_prob = 1\r\n    use_bi_lstm=False\r\n    model = EntityNetwork(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                          story_length, vocab_size, embed_size, hidden_size, is_training,\r\n                          multi_label_flag=False, block_size=20,use_bi_lstm=use_bi_lstm)\r\n    ckpt_dir = \'checkpoint_entity_network/dummy_test/\'\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for i in range(1500):\r\n            # input_x should be:[batch_size, num_sentences,self.sequence_length]\r\n            story = np.random.randn(batch_size, story_length, sequence_length)\r\n            story[story > 0] = 1\r\n            story[story <= 0] = 0\r\n            query = np.random.randn(batch_size, sequence_length)  # [batch_size, sequence_length]\r\n            query[query > 0] = 1\r\n            query[query <= 0] = 0\r\n            answer_single = np.sum(query, axis=1) + np.round(0.1 * np.sum(np.sum(story, axis=1),\r\n                                                                          axis=1))  # [batch_size].e.g. np.array([1, 0, 1, 1, 1, 2, 1, 1])\r\n            loss, acc, predict, _ = sess.run(\r\n                [model.loss_val, model.accuracy, model.predictions, model.train_op],\r\n                feed_dict={model.query: query, model.story: story, model.answer_single: answer_single,\r\n                           model.dropout_keep_prob: dropout_keep_prob})\r\n            print(i, ""query:"", query, ""=====================>"")\r\n            print(i, ""loss:"", loss, ""acc:"", acc, ""label:"", answer_single, ""prediction:"", predict)\r\n            if i % 300 == 0:\r\n                save_path = ckpt_dir + ""model.ckpt""\r\n                saver.save(sess, save_path, global_step=i * 300)\r\n\r\ndef predict():\r\n    num_classes = 15\r\n    learning_rate = 0.001\r\n    batch_size = 8\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 10\r\n    vocab_size = 10000\r\n    embed_size = 100\r\n    hidden_size = 100\r\n    is_training = False\r\n    story_length = 3\r\n    dropout_keep_prob = 1\r\n    model = EntityNetwork(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                          story_length, vocab_size, embed_size, hidden_size, is_training,\r\n                          multi_label_flag=False, block_size=20)\r\n    ckpt_dir = \'checkpoint_entity_network/dummy_test/\'\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\r\n        for i in range(1500):\r\n            story = np.random.randn(batch_size, story_length, sequence_length)\r\n            story[story > 0] = 1\r\n            story[story <= 0] = 0\r\n            query = np.random.randn(batch_size, sequence_length)  # [batch_size, sequence_length]\r\n            query[query > 0] = 1\r\n            query[query <= 0] = 0\r\n            answer_single = np.sum(query, axis=1) + np.round(0.1 * np.sum(np.sum(story, axis=1),\r\n                                                                          axis=1))  # [batch_size].e.g. np.array([1, 0, 1, 1, 1, 2, 1, 1])\r\n            predict = sess.run([model.predictions], feed_dict={model.query: query, model.story: story,\r\n                                                               model.dropout_keep_prob: dropout_keep_prob})\r\n            print(i, ""query:"", query, ""=====================>"")\r\n            print(i, ""label:"", answer_single, ""prediction:"", predict)\r\n\r\n#test()\r\n#predict()'"
a08_EntityNetwork/a3_predict.py,27,"b'# -*- coding: utf-8 -*-\r\n#prediction using model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import pad_sequences #to_categorical\r\nimport os\r\nimport codecs\r\nfrom a3_entity_network import EntityNetwork\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 80, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""../checkpoint_entity_network/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",50,""max sentence length"")\r\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\n#train-zhihu4-only-title-all.txt\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""../train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\r\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\r\ntf.app.flags.DEFINE_string(""predict_target_file"",""../checkpoint_entity_network/zhihu_result_entity_network_multilabel_0712.csv"",""target file path for final prediction"")\r\ntf.app.flags.DEFINE_string(""predict_source_file"",\'../test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\r\ntf.app.flags.DEFINE_integer(""story_length"",1,""story length"")\r\ntf.app.flags.DEFINE_integer(""block_size"",20,""block size"")\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\n# 1.load data with vocabulary of words and labels\r\n\r\n\r\ndef main(_):\r\n    # 1.load data with vocabulary of words and labels\r\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""entity_network"")\r\n    vocab_size = len(vocabulary_word2index)\r\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""entity_network"")\r\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\r\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\r\n    testX=[]\r\n    question_id_list=[]\r\n    for tuple in test:\r\n        question_id,question_string_list=tuple\r\n        question_id_list.append(question_id)\r\n        testX.append(question_string_list)\r\n    # 2.Data preprocessing: Sequence padding\r\n    print(""start padding...."")\r\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n    print(""end padding..."")\r\n   # 3.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        # 4.Instantiate Model\r\n        model = EntityNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                              FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training, multi_label_flag=True, block_size=FLAGS.block_size)\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop"")\r\n            return\r\n        # 5.feed data, to get logits\r\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\r\n        index=0\r\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\r\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\r\n            logits=sess.run(model.logits,feed_dict={model.query:testX2[start:end],model.story: np.expand_dims(testX2[start:end],axis=1),model.dropout_keep_prob:1.0}) #\'shape of logits:\', ( 1, 1999)\r\n            # 6. get lable using logtis\r\n            #predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\r\n            # 7. write question id and labels to file system.\r\n            #write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\r\n            question_id_sublist=question_id_list[start:end]\r\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\r\n\r\n            index=index+1\r\n        predict_target_file_f.close()\r\n\r\n# get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n    return label_list\r\n\r\n# get label using logits\r\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    value_list=[]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        value_list.append(logits[index])\r\n    return label_list,value_list\r\n\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\n# get label using logits\r\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\r\n    #print(""get_label_using_logits.shape:"", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\r\n    for i,logits in enumerate(logits_batch):\r\n        index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n        index_list=index_list[::-1]\r\n        label_list=[]\r\n        for index in index_list:\r\n            label=vocabulary_index2word_label[index]\r\n            label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        #print(""get_label_using_logits.label_list"",label_list)\r\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\r\n    f.flush()\r\n    #return label_list\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()'"
a08_EntityNetwork/a3_train.py,31,"b'# -*- coding: utf-8 -*-\r\n#training the model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom a3_entity_network import EntityNetwork\r\n#from aa1_data_util.\\\r\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import to_categorical, pad_sequences\r\nimport os,math\r\nimport word2vec\r\nimport pickle\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.015,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 256, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""../checkpoint_entity_network2/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",50,""max sentence length"") #100\r\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",10,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_integer(""validate_step"", 2000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\n#train-zhihu4-only-title-all.txt\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""../train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\r\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\r\n#tf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\ntf.app.flags.DEFINE_integer(""story_length"",1,""story length"")\r\ntf.app.flags.DEFINE_integer(""block_size"",20,""block size"")\r\ntf.app.flags.DEFINE_boolean(""use_bi_lstm"",True,""whether to use bi-directional lstm for encode of story and query"")\r\n\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\ndef main(_):\r\n    #1.load data(X:list of lint,y:int).\r\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\r\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\r\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\r\n    #        vocab_size=len(vocabulary_index2word)\r\n    #else:\r\n    if 1==1:\r\n        trainX, trainY, testX, testY = None, None, None, None\r\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""entity_network"") #simple=\'simple\'\r\n        vocab_size = len(vocabulary_word2index)\r\n        print(""entity_network.vocab_size:"",vocab_size)\r\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""entity_networks"")\r\n        if FLAGS.multi_label_flag:\r\n            FLAGS.traning_data_path=\'../training-data/test-zhihu6-title-desc.txt\' #train\r\n        train,test,_=load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,\r\n                                              traning_data_path=FLAGS.traning_data_path)\r\n        trainX, trainY = train\r\n        testX, testY = test\r\n\r\n        print(""trainY:"",trainY[0:10])\r\n        # 2.Data preprocessing.Sequence padding\r\n        print(""start padding & transform to one hot..."")\r\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\r\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\r\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\r\n        # Converting labels to binary vectors\r\n        print(""end padding & transform to one hot..."")\r\n    #2.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        #Instantiate Model\r\n        #num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,num_sentences,vocab_size,embed_size,\r\n        #hidden_size,is_training\r\n        model = EntityNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                              FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,\r\n                              multi_label_flag=True, block_size=FLAGS.block_size,use_bi_lstm=FLAGS.use_bi_lstm)\r\n        #Initialize Save\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(\'Initializing Variables\')\r\n            sess.run(tf.global_variables_initializer())\r\n            if FLAGS.use_embedding: #load pre-trained word embedding\r\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\r\n        curr_epoch=sess.run(model.epoch_step)\r\n        #3.feed data & training\r\n        number_of_training_data=len(trainX)\r\n        print(""number_of_training_data:"",number_of_training_data)\r\n        previous_eval_loss=10000\r\n        best_eval_loss=10000\r\n        batch_size=FLAGS.batch_size\r\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\r\n            loss, acc, counter = 0.0, 0.0, 0\r\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\r\n                if epoch==0 and counter==0:\r\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\r\n                feed_dict = {model.query: trainX[start:end],model.story: np.expand_dims(trainX[start:end],axis=1),model.dropout_keep_prob: 1.0}\r\n                if not FLAGS.multi_label_flag:\r\n                    feed_dict[model.answer_single] = trainY[start:end]\r\n                else:\r\n                    feed_dict[model.answer_multilabel]=trainY[start:end]\r\n                curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\r\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\r\n                if counter %50==0:\r\n                    print(""entity_network2==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f""\r\n                          %(epoch,counter,math.exp(loss/float(counter)) if (loss/float(counter))<20 else 10000.000,acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n                if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0): #(epoch % FLAGS.validate_every) or  if epoch % FLAGS.validate_every == 0:\r\n                    eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\r\n                    print(""entity_network2.validation.part. previous_eval_loss:"", math.exp(previous_eval_loss) if previous_eval_loss<20 else 10000.000,"";current_eval_loss:"", math.exp(eval_loss) if eval_loss<20 else 10000.000)\r\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\r\n                        # reduce the learning rate by a factor of 0.5\r\n                        print(""entity_network2==>validation.part.going to reduce the learning rate."")\r\n                        learning_rate1 = sess.run(model.learning_rate)\r\n                        lrr=sess.run([model.learning_rate_decay_half_op])\r\n                        learning_rate2 = sess.run(model.learning_rate)\r\n                        print(""entity_network2==>validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\r\n                    #print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_acc))\r\n                    else:# loss is decreasing\r\n                        if eval_loss<best_eval_loss:\r\n                            print(""entity_network2==>going to save the model.eval_loss:"",math.exp(eval_loss) if eval_loss<20 else 10000.000,"";best_eval_loss:"",math.exp(best_eval_loss) if best_eval_loss<20 else 10000.000)\r\n                            # save model to checkpoint\r\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\r\n                            saver.save(sess, save_path, global_step=epoch)\r\n                            best_eval_loss=eval_loss\r\n                    previous_eval_loss = eval_loss\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n\r\n            #epoch increment\r\n            print(""going to increment epoch counter...."")\r\n            sess.run(model.epoch_increment)\r\n\r\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\r\n        test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\r\n    pass\r\n\r\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,model,word2vec_model_path=None):\r\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\r\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\r\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\r\n    word2vec_dict = {}\r\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\r\n        word2vec_dict[word] = vector\r\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\r\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\r\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\r\n    count_exist = 0;\r\n    count_not_exist = 0\r\n    for i in range(1, vocab_size):  # loop each word\r\n        word = vocabulary_index2word[i]  # get a word\r\n        embedding = None\r\n        try:\r\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\r\n        except Exception:\r\n            embedding = None\r\n        if embedding is not None:  # the \'word\' exist a embedding\r\n            word_embedding_2dlist[i] = embedding;\r\n            count_exist = count_exist + 1  # assign array to this word.\r\n        else:  # no embedding for this word\r\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\r\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\r\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\r\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\r\n    t_assign_embedding = tf.assign(model.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\r\n    sess.run(t_assign_embedding);\r\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\r\n    print(""using pre-trained word emebedding.ended..."")\r\n\r\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\r\ndef do_eval(sess,model,evalX,evalY,batch_size,vocabulary_index2word_label,eval_decoder_input=None):\r\n    #ii=0\r\n    number_examples=len(evalX)\r\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\r\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\r\n        feed_dict = {model.query: evalX[start:end],model.story:np.expand_dims(evalX[start:end],axis=1), model.dropout_keep_prob: 1}\r\n        if not FLAGS.multi_label_flag:\r\n            feed_dict[model.answer_single] = evalY[start:end]\r\n        else:\r\n            feed_dict[model.answer_multilabel] = evalY[start:end]\r\n        curr_eval_loss, logits,curr_eval_acc,pred= sess.run([model.loss_val,model.logits,model.accuracy,model.predictions],feed_dict)#curr_eval_acc--->textCNN.accuracy\r\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\r\n        #if ii<20:\r\n            #print(""1.evalX[start:end]:"",evalX[start:end])\r\n            #print(""2.evalY[start:end]:"", evalY[start:end])\r\n            #print(""3.pred:"",pred)\r\n            #ii=ii+1\r\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\r\n\r\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\r\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\r\n    index_list=np.argsort(logits)[-top_number:]\r\n    index_list=index_list[::-1]\r\n    #label_list=[]\r\n    #for index in index_list:\r\n    #    label=vocabulary_index2word_label[index]\r\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n    return index_list\r\n\r\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\r\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\r\n    label_nozero=[]\r\n    #print(""labels:"",labels)\r\n    labels=list(labels)\r\n    for index,label in enumerate(labels):\r\n        if label>0:\r\n            label_nozero.append(index)\r\n    if eval_counter<2:\r\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\r\n    count = 0\r\n    label_dict = {x: x for x in label_nozero}\r\n    for label_predict in labels_predicted:\r\n        flag = label_dict.get(label_predict, None)\r\n    if flag is not None:\r\n        count = count + 1\r\n    return count / len(labels)\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()'"
a08_EntityNetwork/data_util_zhihu.py,0,"b'# -*- coding: utf-8 -*-\r\nimport codecs\r\nimport numpy as np\r\n#load data of zhihu\r\nimport word2vec\r\nimport os\r\nimport pickle\r\nPAD_ID = 0\r\nfrom tflearn.data_utils import pad_sequences\r\n_GO=""_GO""\r\n_END=""_END""\r\n_PAD=""_PAD""\r\ndef create_voabulary(simple=None,word2vec_model_path=\'../zhihu-word2vec-title-desc.bin-100\',name_scope=\'\'): #zhihu-word2vec-multilabel.bin-100\r\n    cache_path =\'../cache_vocabulary_label_pik/\'+ name_scope + ""_word_voabulary.pik""\r\n    print(""cache_path:"",cache_path,""file_exists:"",os.path.exists(cache_path))\r\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\r\n        with open(cache_path, \'r\') as data_f:\r\n            vocabulary_word2index, vocabulary_index2word=pickle.load(data_f)\r\n            return vocabulary_word2index, vocabulary_index2word\r\n    else:\r\n        vocabulary_word2index={}\r\n        vocabulary_index2word={}\r\n        if simple is not None:\r\n            word2vec_model_path=\'../zhihu-word2vec.bin-100\'\r\n        print(""create vocabulary. word2vec_model_path:"",word2vec_model_path)\r\n        model=word2vec.load(word2vec_model_path,kind=\'bin\')\r\n        vocabulary_word2index[\'PAD_ID\']=0\r\n        vocabulary_index2word[0]=\'PAD_ID\'\r\n        special_index=0\r\n        if \'biLstmTextRelation\' in name_scope:\r\n            vocabulary_word2index[\'EOS\']=1 # a special token for biLstTextRelation model. which is used between two sentences.\r\n            vocabulary_index2word[1]=\'EOS\'\r\n            special_index=1\r\n        for i,vocab in enumerate(model.vocab):\r\n            vocabulary_word2index[vocab]=i+1+special_index\r\n            vocabulary_index2word[i+1+special_index]=vocab\r\n\r\n        #save to file system if vocabulary of words is not exists.\r\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n            with open(cache_path, \'a\') as data_f:\r\n                pickle.dump((vocabulary_word2index,vocabulary_index2word), data_f)\r\n    return vocabulary_word2index,vocabulary_index2word\r\n\r\n# create vocabulary of lables. label is sorted. 1 is high frequency, 2 is low frequency.\r\ndef create_voabulary_label(voabulary_label=\'train-zhihu4-only-title-all.txt\',name_scope=\'\',use_seq2seq=False):#\'train-zhihu.txt\'\r\n    print(""create_voabulary_label_sorted.started.traning_data_path:"",voabulary_label)\r\n    cache_path =\'../cache_vocabulary_label_pik/\'+ name_scope + ""_label_voabulary.pik""\r\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\r\n        with open(cache_path, \'r\') as data_f:\r\n            vocabulary_word2index_label, vocabulary_index2word_label=pickle.load(data_f)\r\n            return vocabulary_word2index_label, vocabulary_index2word_label\r\n    else:\r\n        zhihu_f_train = codecs.open(voabulary_label, \'r\', \'utf8\')\r\n        lines=zhihu_f_train.readlines()\r\n        count=0\r\n        vocabulary_word2index_label={}\r\n        vocabulary_index2word_label={}\r\n        vocabulary_label_count_dict={} #{label:count}\r\n        for i,line in enumerate(lines):\r\n            if \'__label__\' in line:  #\'__label__-2051131023989903826\r\n                label=line[line.index(\'__label__\')+len(\'__label__\'):].strip().replace(""\\n"","""")\r\n                if vocabulary_label_count_dict.get(label,None) is not None:\r\n                    vocabulary_label_count_dict[label]=vocabulary_label_count_dict[label]+1\r\n                else:\r\n                    vocabulary_label_count_dict[label]=1\r\n        list_label=sort_by_value(vocabulary_label_count_dict)\r\n\r\n        print(""length of list_label:"",len(list_label));#print("";list_label:"",list_label)\r\n        countt=0\r\n\r\n        ##########################################################################################\r\n        if use_seq2seq:#if used for seq2seq model,insert two special label(token):_GO AND _END\r\n            i_list=[0,1,2];label_special_list=[_GO,_END,_PAD]\r\n            for i,label in zip(i_list,label_special_list):\r\n                vocabulary_word2index_label[label] = i\r\n                vocabulary_index2word_label[i] = label\r\n        #########################################################################################\r\n        for i,label in enumerate(list_label):\r\n            if i<10:\r\n                count_value=vocabulary_label_count_dict[label]\r\n                print(""label:"",label,""count_value:"",count_value)\r\n                countt=countt+count_value\r\n            indexx = i + 3 if use_seq2seq else i\r\n            vocabulary_word2index_label[label]=indexx\r\n            vocabulary_index2word_label[indexx]=label\r\n        print(""count top10:"",countt)\r\n\r\n        #save to file system if vocabulary of words is not exists.\r\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n            with open(cache_path, \'a\') as data_f:\r\n                pickle.dump((vocabulary_word2index_label,vocabulary_index2word_label), data_f)\r\n    print(""create_voabulary_label_sorted.ended.len of vocabulary_label:"",len(vocabulary_index2word_label))\r\n    return vocabulary_word2index_label,vocabulary_index2word_label\r\n\r\ndef sort_by_value(d):\r\n    items=d.items()\r\n    backitems=[[v[1],v[0]] for v in items]\r\n    backitems.sort(reverse=True)\r\n    return [ backitems[i][1] for i in range(0,len(backitems))]\r\n\r\ndef create_voabulary_labelO():\r\n    model = word2vec.load(\'zhihu-word2vec-multilabel.bin-100\', kind=\'bin\') #zhihu-word2vec.bin-100\r\n    count=0\r\n    vocabulary_word2index_label={}\r\n    vocabulary_index2word_label={}\r\n    label_unique={}\r\n    for i,vocab in enumerate(model.vocab):\r\n        if \'__label__\' in vocab:  #\'__label__-2051131023989903826\r\n            label=vocab[vocab.index(\'__label__\')+len(\'__label__\'):]\r\n            if label_unique.get(label,None) is None: #\xe4\xb8\x8d\xe6\x9b\xbe\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe4\xbf\x9d\xe6\x8c\x81\xe5\x88\xb0\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\r\n                vocabulary_word2index_label[label]=count\r\n                vocabulary_index2word_label[count]=label #ADD\r\n                count=count+1\r\n                label_unique[label]=label\r\n    return vocabulary_word2index_label,vocabulary_index2word_label\r\n\r\ndef load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\r\n                             traning_data_path=\'../train-zhihu4-only-title-all.txt\',multi_label_flag=True,use_seq2seq=False,seq2seq_label_length=6):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\r\n    print(""load_data.started..."")\r\n    print(""load_data_multilabel_new.training_data_path:"",traning_data_path)\r\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    Y = []\r\n    Y_decoder_input=[] #ADD 2017-06-15\r\n    for i, line in enumerate(lines):\r\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        y=y.strip().replace(\'\\n\',\'\')\r\n        x = x.strip()\r\n        if i<1:\r\n            print(i,""x0:"",x) #get raw x\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<2:\r\n            print(i,""x1:"",x) #word to index\r\n        if use_seq2seq:        # 1)prepare label for seq2seq format(ADD _GO,_END,_PAD for seq2seq)\r\n            ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\r\n            _PAD_INDEX=vocabulary_word2index_label[_PAD]\r\n            ys_mulithot_list=[_PAD_INDEX]*seq2seq_label_length #[3,2,11,14,1]\r\n            ys_decoder_input=[_PAD_INDEX]*seq2seq_label_length\r\n            # below is label.\r\n            for j,y in enumerate(ys):\r\n                if j<seq2seq_label_length-1:\r\n                    ys_mulithot_list[j]=vocabulary_word2index_label[y]\r\n            if len(ys)>seq2seq_label_length-1:\r\n                ys_mulithot_list[seq2seq_label_length-1]=vocabulary_word2index_label[_END]#ADD END TOKEN\r\n            else:\r\n                ys_mulithot_list[len(ys)] = vocabulary_word2index_label[_END]\r\n\r\n            # below is input for decoder.\r\n            ys_decoder_input[0]=vocabulary_word2index_label[_GO]\r\n            for j,y in enumerate(ys):\r\n                if j < seq2seq_label_length - 1:\r\n                    ys_decoder_input[j+1]=vocabulary_word2index_label[y]\r\n            if i<10:\r\n                print(i,""ys:==========>0"", ys)\r\n                print(i,""ys_mulithot_list:==============>1"", ys_mulithot_list)\r\n                print(i,""ys_decoder_input:==============>2"", ys_decoder_input)\r\n        else:\r\n            if multi_label_flag: # 2)prepare multi-label format for classification\r\n                ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\r\n                ys_index=[]\r\n                for y in ys:\r\n                    y_index = vocabulary_word2index_label[y]\r\n                    ys_index.append(y_index)\r\n                ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\r\n            else:                #3)prepare single label format for classification\r\n                ys_mulithot_list=vocabulary_word2index_label[y]\r\n        if i<=3:\r\n            print(""ys_index:"")\r\n            #print(ys_index)\r\n            print(i,""y:"",y,"" ;ys_mulithot_list:"",ys_mulithot_list) #,"" ;ys_decoder_input:"",ys_decoder_input)\r\n        X.append(x)\r\n        Y.append(ys_mulithot_list)\r\n        if use_seq2seq:\r\n            Y_decoder_input.append(ys_decoder_input) #decoder input\r\n        #if i>50000:\r\n        #    break\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    print(""number_examples:"",number_examples) #\r\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\r\n    if use_seq2seq:\r\n        train=train+(Y_decoder_input[0:int((1 - valid_portion) * number_examples)],)\r\n        test=test+(Y_decoder_input[int((1 - valid_portion) * number_examples) + 1:],)\r\n    # 5.return\r\n    print(""load_data.ended..."")\r\n    return train, test, test\r\n\r\ndef load_data_multilabel_new_twoCNN(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\r\n                                    traning_data_path=\'train-zhihu4-only-title-all.txt\',multi_label_flag=True):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\r\n    print(""load_data.twoCNN.started..."")\r\n    print(""load_data_multilabel_new_twoCNN.training_data_path:"",traning_data_path)\r\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    X2=[]\r\n    Y = []\r\n    count_error=0\r\n    for i, line in enumerate(lines):\r\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        y=y.strip().replace(\'\\n\',\'\')\r\n        x = x.strip()\r\n        #print(""x:===============>"",x)\r\n        try:\r\n            x,x2=x.split(""\\t"")\r\n        except Exception:\r\n            print(""x.split.error."",x,""count_error:"",count_error)\r\n            count_error+=1\r\n            continue\r\n        if i<1:\r\n            print(i,""x0:"",x) #get raw x\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        x2=x2.split("" "")\r\n        x2 =[vocabulary_word2index.get(e, 0) for e in x2]\r\n        if i<1:\r\n            print(i,""x1:"",x,""x2:"",x2) #word to index\r\n        if multi_label_flag:\r\n            ys = y.replace(\'\\n\', \'\').split("" "") #ys is a list\r\n            ys_index=[]\r\n            for y in ys:\r\n                y_index = vocabulary_word2index_label[y]\r\n                ys_index.append(y_index)\r\n            ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\r\n        else:\r\n            ys_mulithot_list=int(y) #vocabulary_word2index_label[y]\r\n        if i<1:\r\n            print(i,""y:"",y,""ys_mulithot_list:"",ys_mulithot_list)\r\n        X.append(x)\r\n        X2.append(x2)\r\n        Y.append(ys_mulithot_list)\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    print(""number_examples:"",number_examples) #\r\n    train = (X[0:int((1 - valid_portion) * number_examples)],X2[0:int((1 - valid_portion) * number_examples)],Y[0:int((1 - valid_portion) * number_examples)])\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], X2[int((1 - valid_portion) * number_examples) + 1:],Y[int((1 - valid_portion) * number_examples) + 1:])\r\n    # 5.return\r\n    print(""load_data.ended..."")\r\n    return train, test, test\r\n\r\ndef load_data(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,training_data_path=\'train-zhihu4-only-title-all.txt\'):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\r\n    print(""load_data.started..."")\r\n    zhihu_f = codecs.open(training_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    Y = []\r\n    for i, line in enumerate(lines):\r\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        y=y.replace(\'\\n\',\'\')\r\n        x = x.replace(""\\t"",\' EOS \').strip()\r\n        if i<5:\r\n            print(""x0:"",x) #get raw x\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        #if i<5:\r\n        #    print(""x1:"",x_) #\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<5:\r\n            print(""x1:"",x) #word to index\r\n        y = vocabulary_word2index_label[y] #np.abs(hash(y))\r\n        X.append(x)\r\n        Y.append(y)\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    print(""number_examples:"",number_examples) #\r\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\r\n    # 5.return\r\n    print(""load_data.ended..."")\r\n    return train, test, test\r\n\r\n # \xe5\xb0\x86\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba(uigram,bigram,trigram)\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\r\ndef process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3):\r\n    """"""\r\n    :param sentence: string. example:\'w17314 w5521 w7729 w767 w10147 w111\'\r\n    :param n_gram:\r\n    :return:string. example:\'w17314 w17314w5521 w17314w5521w7729 w5521 w5521w7729 w5521w7729w767 w7729 w7729w767 w7729w767w10147 w767 w767w10147 w767w10147w111 w10147 w10147w111 w111\'\r\n    """"""\r\n    result=[]\r\n    word_list=sentence.split("" "") #[sentence[i] for i in range(len(sentence))]\r\n    unigram=\'\';bigram=\'\';trigram=\'\';fourgram=\'\'\r\n    length_sentence=len(word_list)\r\n    for i,word in enumerate(word_list):\r\n        unigram=word                           #ui-gram\r\n        word_i=unigram\r\n        if n_gram>=2 and i+2<=length_sentence: #bi-gram\r\n            bigram="""".join(word_list[i:i+2])\r\n            word_i=word_i+\' \'+bigram\r\n        if n_gram>=3 and i+3<=length_sentence: #tri-gram\r\n            trigram="""".join(word_list[i:i+3])\r\n            word_i = word_i + \' \' + trigram\r\n        if n_gram>=4 and i+4<=length_sentence: #four-gram\r\n            fourgram="""".join(word_list[i:i+4])\r\n            word_i = word_i + \' \' + fourgram\r\n        if n_gram>=5 and i+5<=length_sentence: #five-gram\r\n            fivegram="""".join(word_list[i:i+5])\r\n            word_i = word_i + \' \' + fivegram\r\n        result.append(word_i)\r\n    result="" "".join(result)\r\n    return result\r\n\r\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xa0\x87\xe7\xad\xbe\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaalabel\xef\xbc\x9aload data with multi-labels\r\ndef load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,traning_path,valid_portion=0.05,max_training_data=1000000):  # n_words=100000,\r\n    """"""\r\n    input: a file path\r\n    :return: train, test, valid. where train=(trainX, trainY). where\r\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\r\n    """"""\r\n    # 1.load a zhihu data from file\r\n    # example: \'w140 w13867 w10344 w2673 w9514 w269 w460 w6 w35053 w844 w10147 w111 __label__-2379261820462209275 -5535923551616745326 6038661761506862294\'\r\n    print(""load_data_with_multilabels.ended..."")\r\n    zhihu_f = codecs.open(traning_path,\'r\',\'utf8\') #(\'/home/xul/xul/9_ZhihuCup/\'+data_type+\'-zhihu5-only-title-multilabel.txt\', \'r\', \'utf8\') #home/xul/xul/9_ZhihuCup/\'\r\n    lines = zhihu_f.readlines()\r\n    # 2.transform X as indices\r\n    # 3.transform  y as scalar\r\n    X = []\r\n    Y = []\r\n    Y_label1999=[]\r\n    for i, line in enumerate(lines):\r\n        #if i>max_training_data:\r\n        #    break\r\n        x, ys = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\r\n        ys=ys.replace(\'\\n\',\'\').split("" "")\r\n        x = x.strip()\r\n        if i < 5:\r\n            print(""x0:"", x) # u\'w4260 w4260w86860 w4260w86860w30907 w86860 w86860w30907 w86860w30907w11 w30907 w30907w11 w30907w11w31 w11 w11w31 w11w31w72 w31 w31w72 w31w72w166 w72 w72w166 w72w166w346 w166 w166w346 w166w346w2182 w346 w346w2182 w346w2182w224 w2182 w2182w224 w2182w224w2148 w224 w224w2148 w224w2148w6 w2148 w2148w6 w2148w6w2566 w6 w6w2566 w6w2566w25 w2566 w2566w25 w2566w25w1110 w25 w25w1110 w25w1110w111 w1110 w1110w111 w111\'\r\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\r\n        #if i < 5:\r\n        #    print(""x1:"", x_)\r\n        x=x.split("" "")\r\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<5:\r\n            print(""x2:"", x)\r\n        #print(""ys:"",ys) #[\'501174938575526146\', \'-4317515119936650885\']\r\n        ys_list=[]\r\n        for y in ys:\r\n            y_ = vocabulary_word2index_label[y]\r\n            ys_list.append(y_)\r\n        X.append(x)\r\n        #TODO ys_list_array=transform_multilabel_as_multihot(ys_list) #it is 2-d array. [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]...]\r\n        ys_list_=proces_label_to_algin(ys_list)\r\n        Y.append(ys_list_)\r\n        #TODO Y_label1999.append(ys_list_array)\r\n        if i==0:\r\n            print(X,Y)\r\n            print(Y_label1999)\r\n    # 4.split to train,test and valid data\r\n    number_examples = len(X)\r\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)]) #TODO Y_label1999[0:int((1 - valid_portion) * number_examples)]\r\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:]) #TODO ,Y_label1999[int((1 - valid_portion) * number_examples) + 1:]\r\n    print(""load_data_with_multilabels.ended..."")\r\n    return train, test\r\n\r\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\r\ndef transform_multilabel_as_multihot(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\r\n    """"""\r\n    :param label_list: e.g.[0,1,4]\r\n    :param label_size: e.g.199\r\n    :return:e.g.[1,1,0,1,0,0,........]\r\n    """"""\r\n    result=np.zeros(label_size)\r\n    #set those location as 1, all else place as 0.\r\n    result[label_list] = 1\r\n    return result\r\n\r\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\r\ndef transform_multilabel_as_multihotO(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\r\n    batch_size=len(label_list)\r\n    result=np.zeros((batch_size,label_size))\r\n    #set those location as 1, all else place as 0.\r\n    result[(range(batch_size),label_list)]=1\r\n    return result\r\n\r\ndef load_final_test_data(file_path):\r\n    final_test_file_predict_object = codecs.open(file_path, \'r\', \'utf8\')\r\n    lines=final_test_file_predict_object.readlines()\r\n    question_lists_result=[]\r\n    for i,line in enumerate(lines):\r\n        question_id,question_string=line.split(""\\t"")\r\n        question_string=question_string.strip().replace(""\\n"","""")\r\n        question_lists_result.append((question_id,question_string))\r\n    print(""length of total question lists:"",len(question_lists_result))\r\n    return question_lists_result\r\n\r\ndef load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists,uni_to_tri_gram=False):  # n_words=100000,\r\n    final_list=[]\r\n    for i, tuplee in enumerate(questionid_question_lists):\r\n        queston_id,question_string_list=tuplee\r\n        if uni_to_tri_gram:\r\n            x_=process_one_sentence_to_get_ui_bi_tri_gram(question_string_list)\r\n            x=x_.split("" "")\r\n        else:\r\n            x=question_string_list.split("" "")\r\n        x = [vocabulary_word2index.get(e, 0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\r\n        if i<=2:\r\n            print(""question_id:"",queston_id);print(""question_string_list:"",question_string_list);print(""x_indexed:"",x)\r\n        final_list.append((queston_id,x))\r\n    number_examples = len(final_list)\r\n    print(""number_examples:"",number_examples) #\r\n    return  final_list\r\n\r\n\r\ndef proces_label_to_algin(ys_list,require_size=5):\r\n    """"""\r\n    :param ys_list: a list\r\n    :return: a list\r\n    """"""\r\n    ys_list_result=[0 for x in range(require_size)]\r\n    if len(ys_list)>=require_size: #\xe8\xb6\x85\xe9\x95\xbf\r\n        ys_list_result=ys_list[0:require_size]\r\n    else:#\xe5\xa4\xaa\xe7\x9f\xad\r\n       if len(ys_list)==1:\r\n           ys_list_result =[ys_list[0] for x in range(require_size)]\r\n       elif len(ys_list)==2:\r\n           ys_list_result = [ys_list[0],ys_list[0],ys_list[0],ys_list[1],ys_list[1]]\r\n       elif len(ys_list) == 3:\r\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[1], ys_list[2]]\r\n       elif len(ys_list) == 4:\r\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[2], ys_list[3]]\r\n    return ys_list_result\r\n\r\ndef write_uigram_to_trigram():\r\n    pass\r\n    #1.read file.\r\n    #2.uigram--->trigram\r\n    #3.write each line to file system.\r\n\r\ndef test_pad():\r\n    trainX=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\r\n    trainX=trainX.split("" "")\r\n    trainX = pad_sequences([[trainX]], maxlen=100, value=0.)\r\n    print(""trainX:"",trainX)\r\n\r\ntopic_info_file_path=\'topic_info.txt\'\r\ndef read_topic_info():\r\n    f = codecs.open(topic_info_file_path, \'r\', \'utf8\')\r\n    lines=f.readlines()\r\n    dict_questionid_title={}\r\n    for i,line in enumerate(lines):\r\n        topic_id,partent_ids,title_character,title_words,desc_character,decs_words=line.split(""\\t"").strip()\r\n        # print(i,""------------------------------------------------------"")\r\n        # print(""topic_id:"",topic_id)\r\n        # print(""partent_ids:"",partent_ids)\r\n        # print(""title_character:"",title_character)\r\n        # print(""title_words:"",title_words)\r\n        # print(""desc_character:"",desc_character)\r\n        # print(""decs_words:"",decs_words)\r\n        dict_questionid_title[topic_id]=title_words+"" ""+decs_words\r\n    print(""len(dict_questionid_title):"",len(dict_questionid_title))\r\n    return dict_questionid_title\r\n\r\ndef stat_training_data_length():\r\n    training_data=\'train-zhihu4-only-title-all.txt\'\r\n    f = codecs.open(training_data, \'r\', \'utf8\')\r\n    lines=f.readlines()\r\n    length_dict={0:0,5:0,10:0,15:0,20:0,25:0,30:0,35:0,40:0,100:0,150:0,200:0,1500:0}\r\n    length_list=[0,5,10,15,20,25,30,35,40,100,150,200,1500]\r\n    for i,line in enumerate(lines):\r\n        line_list=line.split(\'__label__\')[0].strip().split("" "")\r\n        length=len(line_list)\r\n        #print(i,""length:"",length)\r\n        for l in length_list:\r\n            if length<l:\r\n                length=l\r\n                #print(""length.assigned:"",length)\r\n                break\r\n        #print(""length.before dict assign:"", length)\r\n        length_dict[length]=length_dict[length]+1\r\n    print(""length_dict:"",length_dict)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    if __name__ == \'__main__\':\r\n        if __name__ == \'__main__\':\r\n            #1.\r\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\r\n            #vocabulary_word2index_label, vocabulary_index2word_label=create_voabulary_label()\r\n            #load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,data_type=\'test\')\r\n            #2.\r\n            #sentence=u\'\xe6\x88\x91\xe6\x83\xb3\xe5\xbc\x80\xe9\x80\x9a\xe5\x88\x9b\xe4\xb8\x9a\xe6\x9d\xbf\'\r\n            #sentence=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\r\n            #result=process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3)\r\n            #print(len(result),""result:"",result)\r\n\r\n            #3. transform to multilabel\r\n            #label_list=[0,1,4,9,5]\r\n            #result=transform_multilabel_as_multihot(label_list,label_size=15)\r\n            #print(""result:"",result)\r\n\r\n            #4.load data for predict-----------------------------------------------------------------\r\n            #file_path=\'test-zhihu-forpredict-v4only-title.txt\'\r\n            #questionid_question_lists=load_final_test_data(file_path)\r\n\r\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\r\n            #vocabulary_word2index_label,_=create_voabulary_label()\r\n            #final_list=load_data_predict(vocabulary_word2index, vocabulary_word2index_label, questionid_question_lists)\r\n\r\n            #5.process label require lengh\r\n            #ys_list=[99999]\r\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\r\n            #print(ys_list,""ys_list_result1.:"",ys_list_result)\r\n            #ys_list=[99999,23423432,67566765]\r\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\r\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\r\n            #ys_list=[99999,23423432,67566765,23333333]\r\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\r\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\r\n            #ys_list = [99999, 23423432, 67566765,44543543,546546546,323423434]\r\n            #ys_list_result = proces_label_to_algin(ys_list, require_size=5)\r\n            #print(ys_list, ""ys_list_result3.:"", ys_list_result)\r\n\r\n            #6.create vocabulary label. sorted.\r\n            #create_voabulary_label()\r\n\r\n            #d={\'a\':3,\'b\':2,\'c\':11}\r\n            #d_=sort_by_value(d)\r\n            #print(""d_"",d_)\r\n\r\n            #7.\r\n            #test_pad()\r\n\r\n            #8.read topic info\r\n            #read_topic_info()\r\n\r\n            #9\xe3\x80\x82\r\n            stat_training_data_length()\r\n'"
a09_DynamicMemoryNet/a8_dynamic_memory_network.py,93,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nDynamic Memory Network: a.Input Module,b.Question Module,c.Episodic Memory Module,d.Answer Module.\r\n  1.Input Module: encode raw texts into vector representation\r\n  2.Question Module: encode question into vector representation\r\n  3.Episodic Memory Module: with inputs,it chooses which parts of inputs to focus on through the attention mechanism,\r\n                            taking into account of question and previous memory====>it poduce a \'memory\' vecotr.\r\n  4.Answer Module:generate an answer from the final memory vector.\r\n""""""\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib as tf_contrib\r\nimport numpy as np\r\nfrom tensorflow.contrib import rnn\r\n\r\nclass DynamicMemoryNetwork:\r\n    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length, story_length,\r\n                 vocab_size, embed_size,hidden_size, is_training, num_pass=2,use_gated_gru=True,decode_with_sequences=False,multi_label_flag=False,\r\n                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,l2_lambda=0.0001):\r\n        """"""init all hyperparameter here""""""\r\n        # set hyperparamter\r\n        self.num_classes = num_classes\r\n        self.batch_size = batch_size\r\n        self.sequence_length = sequence_length\r\n        self.vocab_size = vocab_size\r\n        self.embed_size = embed_size\r\n        self.is_training = is_training\r\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")\r\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\r\n        self.initializer = initializer\r\n        self.multi_label_flag = multi_label_flag\r\n        self.hidden_size = hidden_size\r\n        self.clip_gradients=clip_gradients\r\n        self.story_length=story_length\r\n        #self.dimension=self.hidden_size*2 if self.use_bi_lstm else self.hidden_size #if use bi-lstm, set dimension value, so it can be used later for parameter.\r\n        self.num_pass=num_pass #number of pass to run for episodic memory module. for example, num_pass=2\r\n        self.use_gated_gru=use_gated_gru #if this is True. we will use gated gru as our \'Memory Update Mechanism\'\r\n        self.decode_with_sequences=decode_with_sequences\r\n        self.l2_lambda=l2_lambda\r\n\r\n        # add placeholder (X,label)\r\n        self.story=tf.placeholder(tf.int32,[None,self.story_length,self.sequence_length],name=""story"")\r\n        self.query = tf.placeholder(tf.int32, [None, self.sequence_length], name=""question"")\r\n\r\n        self.answer_single = tf.placeholder(tf.int32, [None,], name=""input_y"")  # y:[None,num_classes]\r\n        self.answer_multilabel = tf.placeholder(tf.float32, [None, self.num_classes],name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\r\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\r\n\r\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\r\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\r\n        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\r\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\r\n\r\n        self.instantiate_weights()\r\n        self.logits = self.inference()  # [None, self.label_size]. main computation graph is here.\r\n\r\n        self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\r\n        if not self.multi_label_flag:\r\n            correct_prediction = tf.equal(tf.cast(self.predictions, tf.int32),self.answer_single)  # tf.argmax(self.logits, 1)-->[batch_size]\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"")  # shape=()\r\n        else:\r\n            self.accuracy = tf.constant(0.5)  # fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\r\n\r\n        if not is_training:\r\n            return\r\n        if multi_label_flag:\r\n            print(""going to use multi label loss."")\r\n            self.loss_val = self.loss_multilabel()\r\n        else:\r\n            print(""going to use single label loss."")\r\n            self.loss_val = self.loss()\r\n        self.train_op = self.train()\r\n\r\n    def inference(self):\r\n        """"""main computation graph here: a.Input Module,b.Question Module,c.Episodic Memory Module,d.Answer Module """"""\r\n        # 1.Input Module\r\n        self.input_module() #[batch_size,story_length,hidden_size\r\n        # 2.question module\r\n        self.question_module() #[batch_size,hidden_size]\r\n        # 3.episodic memory module\r\n        self.episodic_memory_module() #[batch_size,hidden_size]\r\n        # 4. answer module\r\n        logits=self.answer_module() #[batch_size,vocab_size]\r\n        return logits\r\n\r\n    def input_module(self):\r\n        """"""encode raw texts into vector representation""""""\r\n        story_embedding=tf.nn.embedding_lookup(self.Embedding,self.story)  # [batch_size,story_length,sequence_length,embed_size]\r\n        story_embedding=tf.reshape(story_embedding,(self.batch_size,self.story_length,self.sequence_length*self.embed_size))\r\n        hidden_state=tf.ones((self.batch_size,self.hidden_size),dtype=tf.float32)\r\n        cell = rnn.GRUCell(self.hidden_size)\r\n        self.story_embedding,hidden_state=tf.nn.dynamic_rnn(cell,story_embedding,dtype=tf.float32,scope=""input_module"")\r\n\r\n    def question_module(self):\r\n        """"""\r\n        input:tokens of query:[batch_size,sequence_length]\r\n        :return: representation of question:[batch_size,hidden_size]\r\n        """"""\r\n        query_embedding = tf.nn.embedding_lookup(self.Embedding, self.query)  # [batch_size,sequence_length,embed_size]\r\n        cell=rnn.GRUCell(self.hidden_size)\r\n        _,self.query_embedding=tf.nn.dynamic_rnn(cell,query_embedding,dtype=tf.float32,scope=""question_module"") #query_embedding:[batch_size,hidden_size]\r\n\r\n    def episodic_memory_module(self):#input(story):[batch_size,story_length,hidden_size]\r\n        """"""\r\n        episodic memory module\r\n        1.combine features\r\n        1.attention mechansim using gate function.take fact representation c,question q,previous memory m_previous\r\n        2.use gated-gru to update hidden state\r\n        3.set last hidden state as episode result\r\n        4.use gru to update final memory using episode result\r\n\r\n        input: story(from input module):[batch_size,story_length,hidden_size]\r\n        output: last hidden state:[batch_size,hidden_size]\r\n        """"""\r\n        candidate_inputs=tf.split(self.story_embedding,self.story_length,axis=1) # a list. length is: story_length. each element is:[batch_size,1,embedding_size]\r\n        candidate_list=[tf.squeeze(x,axis=1) for x in candidate_inputs]          # a list. length is: story_length. each element is:[batch_size  ,embedding_size]\r\n        m_current=self.query_embedding\r\n        h_current = tf.zeros((self.batch_size, self.hidden_size))\r\n        for pass_number in range(self.num_pass):#for each candidate sentence in the list,do loop.\r\n            # 1. attention mechansim.take fact representation c,question q,previous memory m_previous\r\n            g = self.attention_mechanism_parallel(self.story_embedding, m_current,self.query_embedding,pass_number)  # [batch_size,story_length]\r\n            # 2.below is Memory Update Mechanism\r\n            if self.use_gated_gru: #use gated gru to update episode. this is default method.\r\n                g = tf.split(g, self.story_length,axis=1)  # a list. length is: sequence_length. each element is:[batch_size,1]\r\n                # 2.1 use gated-gru to update hidden state\r\n                for i,c_current in enumerate(candidate_list):\r\n                    g_current=g[i] #[batch_size,1]\r\n                    h_current=self.gated_gru(c_current,h_current,g_current) #h_current:[batch_size,hidden_size]. g[i] represent score( a scalar) for current candidate sentence:c_current.\r\n                # 2.2 assign last hidden state to e(episodic)\r\n                e_i=h_current #[batch_size,hidden_size]\r\n            else: #use weighted sum to get episode(e.g. used in question answering)\r\n                p_gate=tf.nn.softmax(g,dim=1)                #[batch_size,story_length]. compute weight\r\n                p_gate=tf.expand_dims(p_gate,axis=2)         #[batch_size,story_length,1]\r\n                e_i=tf.multiply(p_gate,self.story_embedding) #[batch_size,story_length,hidden_size]\r\n                e_i=tf.reduce_sum(e_i,axis=1)                #[batch_size,story_length]\r\n            #3. use gru to update episodic memory m_i\r\n            m_current=self.gru_cell(e_i, m_current,""gru_episodic_memory"") #[batch_size,hidden_size]\r\n        self.m_T=m_current #[batch_size,hidden_size]\r\n\r\n    def answer_module(self):\r\n        """""" Answer Module:generate an answer from the final memory vector.\r\n        Input:\r\n            hidden state from episodic memory module:[batch_size,hidden_size]\r\n            question:[batch_size, embedding_size]\r\n        """"""\r\n        steps=self.sequence_length if self.decode_with_sequences else 1 #decoder for a list of tokens with sequence. e.g.""x1 x2 x3 x4...""\r\n        a=self.m_T #init hidden state\r\n        y_pred=tf.zeros((self.batch_size,self.hidden_size)) #TODO usually we will init this as a special token \'<GO>\', you can change this line by pass embedding of \'<GO>\' from outside.\r\n        logits_list=[]\r\n        logits_return=None\r\n        for i in range(steps):\r\n            cell = rnn.GRUCell(self.hidden_size)\r\n            y_previous_q=tf.concat([y_pred,self.query_embedding],axis=1) #[batch_hidden_size*2]\r\n            _, a = cell( y_previous_q,a)\r\n            logits=tf.layers.dense(a,units=self.num_classes) #[batch_size,vocab_size]\r\n            logits_list.append(logits)\r\n        if self.decode_with_sequences:#need to get sequences.\r\n            logits_return = tf.stack(logits_list, axis=1)  # [batch_size,sequence_length,num_classes]\r\n        else:#only need to get an answer, not sequences\r\n            logits_return = logits_list[0]  #[batcj_size,num_classes]\r\n\r\n        return logits_return\r\n    def gated_gru(self,c_current,h_previous,g_current):\r\n        """"""\r\n        gated gru to get updated hidden state\r\n        :param  c_current: [batch_size,embedding_size]\r\n        :param  h_previous:[batch_size,hidden_size]\r\n        :param  g_current: [batch_size,1]\r\n        :return h_current: [batch_size,hidden_size]\r\n        """"""\r\n        # 1.compute candidate hidden state using GRU.\r\n        h_candidate=self.gru_cell(c_current, h_previous,""gru_candidate_sentence"") #[batch_size,hidden_size]\r\n        # 2.combine candidate hidden state and previous hidden state using weight(a gate) to get updated hidden state.\r\n        h_current=tf.multiply(g_current,h_candidate)+tf.multiply(1-g_current,h_previous) #[batch_size,hidden_size]\r\n        return h_current\r\n\r\n    def attention_mechanism_parallel(self,c_full,m,q,i):\r\n        """""" parallel implemtation of gate function given a list of candidate sentence, a query, and previous memory.\r\n        Input:\r\n           c_full: candidate fact. shape:[batch_size,story_length,hidden_size]\r\n           m: previous memory. shape:[batch_size,hidden_size]\r\n           q: question. shape:[batch_size,hidden_size]\r\n        Output: a scalar score (in batch). shape:[batch_size,story_length]\r\n        """"""\r\n        q=tf.expand_dims(q,axis=1) #[batch_size,1,hidden_size]\r\n        m=tf.expand_dims(m,axis=1) #[batch_size,1,hidden_size]\r\n\r\n        # 1.define a large feature vector that captures a variety of similarities between input,memory and question vector: z(c,m,q)\r\n        c_q_elementwise=tf.multiply(c_full,q)          #[batch_size,story_length,hidden_size]\r\n        c_m_elementwise=tf.multiply(c_full,m)          #[batch_size,story_length,hidden_size]\r\n        c_q_minus=tf.abs(tf.subtract(c_full,q))        #[batch_size,story_length,hidden_size]\r\n        c_m_minus=tf.abs(tf.subtract(c_full,m))        #[batch_size,story_length,hidden_size]\r\n        # c_transpose Wq\r\n        c_w_q=self.x1Wx2_parallel(c_full,q,""c_w_q""+str(i))   #[batch_size,story_length,hidden_size]\r\n        c_w_m=self.x1Wx2_parallel(c_full,m,""c_w_m""+str(i))   #[batch_size,story_length,hidden_size]\r\n        # c_transposeWm\r\n        q_tile=tf.tile(q,[1,self.story_length,1])     #[batch_size,story_length,hidden_size]\r\n        m_tile=tf.tile(m,[1,self.story_length,1])     #[batch_size,story_length,hidden_size]\r\n        z=tf.concat([c_full,m_tile,q_tile,c_q_elementwise,c_m_elementwise,c_q_minus,c_m_minus,c_w_q,c_w_m],2) #[batch_size,story_length,hidden_size*9]\r\n        # 2. two layer feed foward\r\n        g=tf.layers.dense(z,self.hidden_size*3,activation=tf.nn.tanh)  #[batch_size,story_length,hidden_size*3]\r\n        g=tf.layers.dense(g,1,activation=tf.nn.sigmoid)                #[batch_size,story_length,1]\r\n        g=tf.squeeze(g,axis=2)                                         #[batch_size,story_length]\r\n        return g\r\n\r\n    def x1Wx2_parallel(self,x1,x2,scope):\r\n        """"""\r\n        :param x1: [batch_size,story_length,hidden_size]\r\n        :param x2: [batch_size,1,hidden_size]\r\n        :param scope: a string\r\n        :return:  [batch_size,story_length,hidden_size]\r\n        """"""\r\n        with tf.variable_scope(scope):\r\n            x1=tf.reshape(x1,shape=(self.batch_size,-1)) #[batch_size,story_length*hidden_size]\r\n            x1_w=tf.layers.dense(x1,self.story_length*self.hidden_size,use_bias=False) #[self.hidden_size, story_length*self.hidden_size]\r\n            x1_w_expand=tf.expand_dims(x1_w,axis=2)     #[batch_size,story_length*self.hidden_size,1]\r\n            x1_w_x2=tf.matmul(x1_w_expand,x2)           #[batch_size,story_length*self.hidden_size,hidden_size]\r\n            x1_w_x2=tf.reshape(x1_w_x2,shape=(self.batch_size,self.story_length,self.hidden_size,self.hidden_size))\r\n            x1_w_x2=tf.reduce_sum(x1_w_x2,axis=3)      #[batch_size,story_length,hidden_size]\r\n            return x1_w_x2\r\n\r\n    def gru_cell(self, Xt, h_t_minus_1,variable_scope):\r\n        """"""\r\n        single step of gru\r\n        :param Xt: Xt:[batch_size,hidden_size]\r\n        :param h_t_minus_1:[batch_size,hidden_size]\r\n        :return:[batch_size,hidden_size]\r\n        """"""\r\n        with tf.variable_scope(variable_scope):\r\n            # 1.update gate: decides how much past information is kept and how much new information is added.\r\n            z_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_z) + tf.matmul(h_t_minus_1,self.U_z) + self.b_z)  # z_t:[batch_size,self.hidden_size]\r\n            # 2.reset gate: controls how much the past state contributes to the candidate state.\r\n            r_t = tf.nn.sigmoid(tf.matmul(Xt, self.W_r) + tf.matmul(h_t_minus_1,self.U_r) + self.b_r)  # r_t:[batch_size,self.hidden_size]\r\n            # 3.compute candiate state h_t~\r\n            h_t_candiate = tf.nn.tanh(tf.matmul(Xt, self.W_h) +r_t * (tf.matmul(h_t_minus_1, self.U_h)) + self.b_h)  # h_t_candiate:[batch_size,self.hidden_size]\r\n            # 4.compute new state: a linear combine of pervious hidden state and the current new state h_t~\r\n            h_t = (1 - z_t) * h_t_minus_1 + z_t * h_t_candiate  # h_t:[batch_size,hidden_size]\r\n        return h_t\r\n\r\n    def loss(self, l2_lambda=0.0001):  # 0.001\r\n        with tf.name_scope(""loss""):\r\n            # input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\r\n            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\r\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.answer_single,logits=self.logits);  # sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\r\n            # print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\r\n            loss = tf.reduce_mean(losses)  # print(""2.loss.loss:"", loss) #shape=()\r\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if (\'bias\' not in v.name ) and (\'alpha\' not in v.name)]) * l2_lambda\r\n            loss = loss + l2_losses\r\n        return loss\r\n\r\n    def loss_multilabel(self, l2_lambda=0.0001): #0.0001 this loss function is for multi-label classification\r\n        with tf.name_scope(""loss""):\r\n            # input_y:shape=(?, 1999); logits:shape=(?, 1999)\r\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\r\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.answer_multilabel,logits=self.logits);  #[None,self.num_classes]. losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\r\n            #losses=self.smoothing_cross_entropy(self.logits,self.answer_multilabel,self.num_classes) #shape=(512,)\r\n            losses = tf.reduce_sum(losses, axis=1)  # shape=(?,). loss for all data in the batch\r\n            loss = tf.reduce_mean(losses)  # shape=().   average loss in the batch\r\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if(\'bias\' not in v.name ) and (\'alpha\' not in v.name)]) * l2_lambda\r\n            loss = loss + l2_losses\r\n        return loss\r\n\r\n    def smoothing_cross_entropy(self,logits, labels, vocab_size, confidence=0.9): #confidence = 1.0 - label_smoothing. where label_smooth=0.1. from http://github.com/tensorflow/tensor2tensor\r\n        """"""Cross entropy with label smoothing to limit over-confidence.""""""\r\n        with tf.name_scope(""smoothing_cross_entropy"", [logits, labels]):\r\n            # Low confidence is given to all non-true labels, uniformly.\r\n            low_confidence = (1.0 - confidence) / tf.to_float(vocab_size - 1)\r\n            # Normalizing constant is the best cross-entropy value with soft targets.\r\n            # We subtract it just for readability, makes no difference on learning.\r\n            normalizing = -(confidence * tf.log(confidence) + tf.to_float(vocab_size - 1) * low_confidence * tf.log(low_confidence + 1e-20))\r\n            # Soft targets.\r\n            soft_targets = tf.one_hot(\r\n                tf.cast(labels, tf.int32),\r\n                depth=vocab_size,\r\n                on_value=confidence,\r\n                off_value=low_confidence)\r\n            xentropy = tf.nn.softmax_cross_entropy_with_logits(\r\n                logits=logits, labels=soft_targets)\r\n        return xentropy - normalizing\r\n\r\n    def train(self):\r\n        """"""based on the loss, use SGD to update parameter""""""\r\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,\r\n                                                   self.decay_rate, staircase=True)\r\n        self.learning_rate_=learning_rate\r\n        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\r\n        train_op = tf_contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\r\n        return train_op\r\n\r\n    #:param s_t: vector representation of current input(is a sentence). shape:[batch_size,sequence_length,embed_size]\r\n    #:param h: value(hidden state).shape:[hidden_size]\r\n    #:param w: key.shape:[hidden_size]\r\n    def instantiate_weights(self):\r\n        """"""define all weights here""""""\r\n        with tf.variable_scope(""gru_cell""):\r\n            self.W_z = tf.get_variable(""W_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\r\n            self.U_z = tf.get_variable(""U_z"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\r\n            self.b_z = tf.get_variable(""b_z"", shape=[self.hidden_size])\r\n            # GRU parameters:reset gate related\r\n            self.W_r = tf.get_variable(""W_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\r\n            self.U_r = tf.get_variable(""U_r"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\r\n            self.b_r = tf.get_variable(""b_r"", shape=[self.hidden_size])\r\n\r\n            self.W_h = tf.get_variable(""W_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\r\n            self.U_h = tf.get_variable(""U_h"", shape=[self.embed_size, self.hidden_size], initializer=self.initializer)\r\n            self.b_h = tf.get_variable(""b_h"", shape=[self.hidden_size])\r\n\r\n        with tf.variable_scope(""embedding_projection""):  # embedding matrix\r\n            self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)\r\n\r\n# test: learn to count. weight of query and story is different\r\n#two step to test\r\n#step1. run train function to train the model. it will save checkpoint\r\n#step2. run predict function to make a prediction based on the model restore from the checkpoint.\r\ndef train():\r\n    # below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\r\n    num_classes = 15\r\n    learning_rate = 0.001\r\n    batch_size = 8\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 10\r\n    vocab_size = 10000\r\n    embed_size = 100\r\n    hidden_size = 100\r\n    is_training = True\r\n    story_length = 3\r\n    dropout_keep_prob = 1\r\n    model = DynamicMemoryNetwork(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                          story_length, vocab_size, embed_size, hidden_size, is_training,\r\n                          multi_label_flag=False)\r\n    ckpt_dir = \'checkpoint_dmn/dummy_test/\'\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for i in range(1500):\r\n            # input_x should be:[batch_size, num_sentences,self.sequence_length]\r\n            story = np.random.randn(batch_size, story_length, sequence_length)\r\n            story[story > 0] = 1\r\n            story[story <= 0] = 0\r\n            query = np.random.randn(batch_size, sequence_length)  # [batch_size, sequence_length]\r\n            query[query > 0] = 1\r\n            query[query <= 0] = 0\r\n            answer_single = np.sum(query, axis=1) + np.round(0.1 * np.sum(np.sum(story, axis=1),\r\n                                                                          axis=1))  # [batch_size].e.g. np.array([1, 0, 1, 1, 1, 2, 1, 1])\r\n            loss, acc, predict, _ = sess.run(\r\n                [model.loss_val, model.accuracy, model.predictions, model.train_op],\r\n                feed_dict={model.query: query, model.story: story, model.answer_single: answer_single,\r\n                           model.dropout_keep_prob: dropout_keep_prob})\r\n            print(i, ""query:"", query, ""=====================>"")\r\n            print(i, ""loss:"", loss, ""acc:"", acc, ""label:"", answer_single, ""prediction:"", predict)\r\n            if i % 300 == 0:\r\n                save_path = ckpt_dir + ""model.ckpt""\r\n                saver.save(sess, save_path, global_step=i * 300)\r\n\r\ndef predict():\r\n    num_classes = 15\r\n    learning_rate = 0.001\r\n    batch_size = 8\r\n    decay_steps = 1000\r\n    decay_rate = 0.9\r\n    sequence_length = 10\r\n    vocab_size = 10000\r\n    embed_size = 100\r\n    hidden_size = 100\r\n    is_training = False\r\n    story_length = 3\r\n    dropout_keep_prob = 1\r\n    model = DynamicMemoryNetwork(num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\r\n                          story_length, vocab_size, embed_size, hidden_size, is_training,\r\n                          multi_label_flag=False, block_size=20)\r\n    ckpt_dir = \'checkpoint_dmn/dummy_test/\'\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\r\n        for i in range(100):\r\n            story = np.random.randn(batch_size, story_length, sequence_length)\r\n            story[story > 0] = 1\r\n            story[story <= 0] = 0\r\n            query = np.random.randn(batch_size, sequence_length)  # [batch_size, sequence_length]\r\n            query[query > 0] = 1\r\n            query[query <= 0] = 0\r\n            answer_single = np.sum(query, axis=1) + np.round(0.1 * np.sum(np.sum(story, axis=1),axis=1))  # [batch_size].e.g. np.array([1, 0, 1, 1, 1, 2, 1, 1])\r\n            predict = sess.run([model.predictions], feed_dict={model.query: query, model.story: story,\r\n                                                               model.dropout_keep_prob: dropout_keep_prob})\r\n            print(i, ""query:"", query, ""=====================>"")\r\n            print(i, ""label:"", answer_single, ""prediction:"", predict)\r\n#1.train the model\r\n#train()\r\n#2.make a prediction based on the learned model.\r\n#predict()\r\n'"
a09_DynamicMemoryNet/a8_predict.py,30,"b'# -*- coding: utf-8 -*-\r\n#prediction using model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import pad_sequences #to_categorical\r\nimport os\r\nimport codecs\r\nfrom a8_dynamic_memory_network import DynamicMemoryNetwork\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 80, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""../checkpoint_dynamic_memory_network/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",60,""max sentence length"")\r\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",1,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""../train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\r\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\r\ntf.app.flags.DEFINE_string(""predict_target_file"",""../checkpoint_dynamic_memory_network/zhihu_result_dynamic_memory_network.csv"",""target file path for final prediction"")\r\ntf.app.flags.DEFINE_string(""predict_source_file"",\'../test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\r\ntf.app.flags.DEFINE_integer(""story_length"",1,""story length"")\r\ntf.app.flags.DEFINE_boolean(""use_gated_gru"",False,""whether to use gated gru as  memory update mechanism. if false,use weighted sum of candidate sentences according to gate"")\r\ntf.app.flags.DEFINE_integer(""num_pass"",2,""number of pass to run"") #e.g. num_pass=1,2,3,4.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\ntf.app.flags.DEFINE_boolean(""decode_with_sequences"",False,""if your task is sequence generating, you need to set this true.default is false, for predict a label"")\r\n\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\n# 1.load data with vocabulary of words and labels\r\n\r\n\r\ndef main(_):\r\n    # 1.load data with vocabulary of words and labels\r\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""dynamic_memory_network"")\r\n    vocab_size = len(vocabulary_word2index)\r\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""dynamic_memory_network"")\r\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\r\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\r\n    testX=[]\r\n    question_id_list=[]\r\n    for tuple in test:\r\n        question_id,question_string_list=tuple\r\n        question_id_list.append(question_id)\r\n        testX.append(question_string_list)\r\n    # 2.Data preprocessing: Sequence padding\r\n    print(""start padding...."")\r\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n    print(""end padding..."")\r\n   # 3.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        # 4.Instantiate Model\r\n        model = DynamicMemoryNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                                     FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,num_pass=FLAGS.num_pass,\r\n                                     use_gated_gru=FLAGS.use_gated_gru,decode_with_sequences=FLAGS.decode_with_sequences,multi_label_flag=FLAGS.multi_label_flag,l2_lambda=FLAGS.l2_lambda)\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint of EntityNet."")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(""Can\'t find the checkpoint.going to stop"")\r\n            return\r\n        # 5.feed data, to get logits\r\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\r\n        index=0\r\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\r\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\r\n            logits=sess.run(model.logits,feed_dict={model.query:testX2[start:end],model.story: np.expand_dims(testX2[start:end],axis=1),\r\n                                                    model.dropout_keep_prob:1.0}) #\'shape of logits:\', ( 1, 1999)\r\n            # 6. get lable using logtis\r\n            #predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\r\n            # 7. write question id and labels to file system.\r\n            #write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\r\n            question_id_sublist=question_id_list[start:end]\r\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\r\n\r\n            index=index+1\r\n        predict_target_file_f.close()\r\n\r\n# get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n    return label_list\r\n\r\n# get label using logits\r\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    value_list=[]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        value_list.append(logits[index])\r\n    return label_list,value_list\r\n\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\n# get label using logits\r\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\r\n    #print(""get_label_using_logits.shape:"", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>\xe9\x9c\x80\xe8\xa6\x81(10,5)\r\n    for i,logits in enumerate(logits_batch):\r\n        index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\r\n        index_list=index_list[::-1]\r\n        label_list=[]\r\n        for index in index_list:\r\n            label=vocabulary_index2word_label[index]\r\n            label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n        #print(""get_label_using_logits.label_list"",label_list)\r\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\r\n    f.flush()\r\n    #return label_list\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string="","".join(labels_list)\r\n    f.write(question_id+"",""+labels_string+""\\n"")\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()\r\n'"
a09_DynamicMemoryNet/a8_train.py,31,"b'# -*- coding: utf-8 -*-\r\n#training the model.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding(\'utf8\')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom a8_dynamic_memory_network import DynamicMemoryNetwork\r\n#from aa1_data_util.\\\r\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import to_categorical, pad_sequences\r\nimport os,math\r\nimport word2vec\r\nimport pickle\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"") #3 ADDITIONAL TOKEN: _GO,_END,_PAD\r\ntf.app.flags.DEFINE_float(""learning_rate"",0.015,""learning rate"")\r\ntf.app.flags.DEFINE_integer(""batch_size"", 256, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\r\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.87\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\r\ntf.app.flags.DEFINE_string(""ckpt_dir"",""../checkpoint_dynamic_memory_network/"",""checkpoint location for the model"")\r\ntf.app.flags.DEFINE_integer(""sequence_length"",60,""max sentence length"") #100\r\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\r\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\r\ntf.app.flags.DEFINE_integer(""num_epochs"",16,""number of epochs to run."")\r\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\r\ntf.app.flags.DEFINE_integer(""validate_step"", 2000, ""how many step to validate."") #1500\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe6\xa3\x80\xe9\xaa\x8c\r\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\r\ntf.app.flags.DEFINE_string(""traning_data_path"",""../train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\r\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""../zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."") #set this false. becase we are using it is a sequence of token here.\r\ntf.app.flags.DEFINE_integer(""hidden_size"",100,""hidden size"")\r\ntf.app.flags.DEFINE_integer(""story_length"",1,""story length"")\r\n# you can do experiment by change below two hyperparameter, performance may be changed.\r\ntf.app.flags.DEFINE_boolean(""use_gated_gru"",False,""whether to use gated gru as  memory update mechanism. if false,use weighted sum of candidate sentences according to gate"")\r\ntf.app.flags.DEFINE_integer(""num_pass"",2,""number of pass to run"") #e.g. num_pass=1,2,3,4.\r\ntf.app.flags.DEFINE_float(""l2_lambda"", 0.0001, ""l2 regularization"")\r\n\r\ntf.app.flags.DEFINE_boolean(""decode_with_sequences"",False,""if your task is sequence generating, you need to set this true.default is false, for predict a label"")\r\n\r\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\r\ndef main(_):\r\n    #1.load data(X:list of lint,y:int).\r\n    #if os.path.exists(FLAGS.cache_path):  # load training data from cache file.\r\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\r\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\r\n    #        vocab_size=len(vocabulary_index2word)\r\n    #else:\r\n    if 1==1:\r\n        trainX, trainY, testX, testY = None, None, None, None\r\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""dynamic_memory_network"") #simple=\'simple\'\r\n        vocab_size = len(vocabulary_word2index)\r\n        print(""dynamic_memory_network.vocab_size:"",vocab_size)\r\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""dynamic_memory_network"")\r\n        if FLAGS.multi_label_flag:\r\n            FLAGS.traning_data_path=\'../training-data/train-zhihu6-title-desc.txt\' #change this line if want to train in a small dataset. e.g. dataset from \'test-zhihu6-title-desc.txt\'\r\n        train,test,_=load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,\r\n                                              traning_data_path=FLAGS.traning_data_path)\r\n        trainX, trainY = train\r\n        testX, testY = test\r\n\r\n        print(""trainY:"",trainY[0:10])\r\n        # 2.Data preprocessing.Sequence padding\r\n        print(""start padding & transform to one hot..."")\r\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\r\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\r\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\r\n        # Converting labels to binary vectors\r\n        print(""end padding & transform to one hot..."")\r\n    #2.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    with tf.Session(config=config) as sess:\r\n        #Instantiate Model\r\n        model = DynamicMemoryNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                                     FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,num_pass=FLAGS.num_pass,\r\n                                     use_gated_gru=FLAGS.use_gated_gru,decode_with_sequences=FLAGS.decode_with_sequences,multi_label_flag=FLAGS.multi_label_flag,l2_lambda=FLAGS.l2_lambda)\r\n        #Initialize Save\r\n        saver=tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\r\n            print(""Restoring Variables from Checkpoint"")\r\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\r\n        else:\r\n            print(\'Initializing Variables\')\r\n            sess.run(tf.global_variables_initializer())\r\n            if FLAGS.use_embedding: #load pre-trained word embedding\r\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, model,word2vec_model_path=FLAGS.word2vec_model_path)\r\n        curr_epoch=sess.run(model.epoch_step)\r\n        #3.feed data & training\r\n        number_of_training_data=len(trainX)\r\n        print(""number_of_training_data:"",number_of_training_data)\r\n        previous_eval_loss=10000\r\n        best_eval_loss=10000\r\n        batch_size=FLAGS.batch_size\r\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\r\n            loss, acc, counter = 0.0, 0.0, 0\r\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\r\n                if epoch==0 and counter==0:\r\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\r\n                feed_dict = {model.query: trainX[start:end],model.story: np.expand_dims(trainX[start:end],axis=1),model.dropout_keep_prob: 1.0}\r\n                if not FLAGS.multi_label_flag:\r\n                    feed_dict[model.answer_single] = trainY[start:end]\r\n                else:\r\n                    feed_dict[model.answer_multilabel]=trainY[start:end]\r\n                curr_loss,curr_acc,_=sess.run([model.loss_val,model.accuracy,model.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\r\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\r\n                if counter %50==0:\r\n                    print(""dynamic_memory_network[use_gated_gru=False,num_pass=2]==>Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f""\r\n                          %(epoch,counter,math.exp(loss/float(counter)) if (loss/float(counter))<20 else 10000.000,acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n                if FLAGS.batch_size!=0 and (start%(FLAGS.validate_step*FLAGS.batch_size)==0): #(epoch % FLAGS.validate_every) or  if epoch % FLAGS.validate_every == 0:\r\n                    eval_loss, eval_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\r\n                    print(""dynamic_memory_network[use_gated_gru=False,num_pass=2].validation.part. previous_eval_loss:"", math.exp(previous_eval_loss) if previous_eval_loss<20 else 10000.000,"";current_eval_loss:"", math.exp(eval_loss) if eval_loss<20 else 10000.000)\r\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\r\n                        # reduce the learning rate by a factor of 0.5\r\n                        print(""dynamic_memory_network[use_gated_gru=False,num_pass=2]==>validation.part.going to reduce the learning rate."")\r\n                        learning_rate1 = sess.run(model.learning_rate)\r\n                        lrr=sess.run([model.learning_rate_decay_half_op])\r\n                        learning_rate2 = sess.run(model.learning_rate)\r\n                        print(""dynamic_memory_network[use_gated_gru=False,num_pass=2]==>validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\r\n                    else:# loss is decreasing\r\n                        if eval_loss<best_eval_loss:\r\n                            print(""dynamic_memory_network[use_gated_gru=False,num_pass=2]==>going to save the model.eval_loss:"",math.exp(eval_loss) if eval_loss<20 else 10000.000,"";best_eval_loss:"",math.exp(best_eval_loss) if best_eval_loss<20 else 10000.000)\r\n                            # save model to checkpoint\r\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\r\n                            saver.save(sess, save_path, global_step=epoch)\r\n                            best_eval_loss=eval_loss\r\n                    previous_eval_loss = eval_loss\r\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\r\n\r\n            #epoch increment\r\n            print(""going to increment epoch counter...."")\r\n            sess.run(model.epoch_increment)\r\n\r\n        # 5.test on test set\r\n        test_loss, test_acc = do_eval(sess, model, testX, testY, batch_size,vocabulary_index2word_label)\r\n    pass\r\n\r\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,model,word2vec_model_path=None):\r\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\r\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\r\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\r\n    word2vec_dict = {}\r\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\r\n        word2vec_dict[word] = vector\r\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\r\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\r\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\r\n    count_exist = 0;\r\n    count_not_exist = 0\r\n    for i in range(1, vocab_size):  # loop each word\r\n        word = vocabulary_index2word[i]  # get a word\r\n        embedding = None\r\n        try:\r\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\r\n        except Exception:\r\n            embedding = None\r\n        if embedding is not None:  # the \'word\' exist a embedding\r\n            word_embedding_2dlist[i] = embedding;\r\n            count_exist = count_exist + 1  # assign array to this word.\r\n        else:  # no embedding for this word\r\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\r\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\r\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\r\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\r\n    t_assign_embedding = tf.assign(model.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\r\n    sess.run(t_assign_embedding);\r\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\r\n    print(""using pre-trained word emebedding.ended..."")\r\n\r\n# do evalation on validation dataset, report loss and accuracy\r\ndef do_eval(sess,model,evalX,evalY,batch_size,vocabulary_index2word_label,eval_decoder_input=None):\r\n    #ii=0\r\n    number_examples=len(evalX)\r\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\r\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\r\n        feed_dict = {model.query: evalX[start:end],model.story:np.expand_dims(evalX[start:end],axis=1), model.dropout_keep_prob: 1}\r\n        if not FLAGS.multi_label_flag:\r\n            feed_dict[model.answer_single] = evalY[start:end]\r\n        else:\r\n            feed_dict[model.answer_multilabel] = evalY[start:end]\r\n        curr_eval_loss, logits,curr_eval_acc,pred= sess.run([model.loss_val,model.logits,model.accuracy,model.predictions],feed_dict)#curr_eval_acc--->textCNN.accuracy\r\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\r\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\r\n\r\n# get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\r\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\r\n    index_list=np.argsort(logits)[-top_number:]\r\n    index_list=index_list[::-1]\r\n    #label_list=[]\r\n    #for index in index_list:\r\n    #    label=vocabulary_index2word_label[index]\r\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\r\n    return index_list\r\n\r\n# calcuate accuracy\r\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\r\n    label_nozero=[]\r\n    #print(""labels:"",labels)\r\n    labels=list(labels)\r\n    for index,label in enumerate(labels):\r\n        if label>0:\r\n            label_nozero.append(index)\r\n    if eval_counter<2:\r\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\r\n    count = 0\r\n    label_dict = {x: x for x in label_nozero}\r\n    for label_predict in labels_predicted:\r\n        flag = label_dict.get(label_predict, None)\r\n    if flag is not None:\r\n        count = count + 1\r\n    return count / len(labels)\r\n\r\nif __name__ == ""__main__"":\r\n    tf.app.run()\r\n'"
aa1_data_util/1_process_zhihu.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\n#reload(sys)\n#sys.setdefaultencoding(\'utf8\')\n#1.\xe5\xb0\x86\xe9\x97\xae\xe9\xa2\x98ID\xe5\x92\x8cTOPIC\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\xb3\xe7\xb3\xbb\xe4\xbf\x9d\xe6\x8c\x81\xe5\x88\xb0\xe5\xad\x97\xe5\x85\xb8\xe9\x87\x8c\xef\xbc\x9aprocess question_topic_train_set.txt\n#from:question_id,topics(topic_id1,topic_id2,topic_id3,topic_id4,topic_id5)\n#  to:(question_id,topic_id1)\n#     (question_id,topic_id2)\n#read question_topic_train_set.txt\nimport codecs\n#1.################################################################################################################\nprint(""process question_topic_train_set.txt,started..."")\nq_t=\'question_topic_train_set.txt\'\nq_t_file = codecs.open(q_t, \'r\', \'utf8\')\nlines=q_t_file.readlines()\nquestion_topic_dict={}\nfor i,line in enumerate(lines):\n    if i%300000==0:\n        print(i)\n    #print(line)\n    question_id,topic_list_string=line.split(\'\\t\')\n    #print(question_id)\n    #print(topic_list_string)\n    topic_list=topic_list_string.replace(""\\n"","""").split("","")\n    question_topic_dict[question_id]=topic_list\n    #for ii,topic in enumerate(topic_list):\n    #    print(ii,topic)\n    #print(""====================================="")\n    #if i>10:\n    #   print(question_topic_dict)\n    #   break\nprint(""process question_topic_train_set.txt,ended..."")\n###################################################################################################################\n###################################################################################################################\n#2.\xe5\xa4\x84\xe7\x90\x86\xe9\x97\xae\xe9\xa2\x98--\xe5\xbe\x97\xe5\x88\xb0\xe9\x97\xae\xe9\xa2\x98ID\xef\xbc\x9a\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x8c\xe5\xad\x98\xe6\x88\x90\xe5\xad\x97\xe5\x85\xb8\xe3\x80\x82proces question. for every question form a a list of string to reprensent it.\nimport codecs\nprint(""process question started11..."")\nq=\'question_train_set.txt\'\nq_file = codecs.open(q, \'r\', \'utf8\')\nq_lines=q_file.readlines()\nquestionid_words_representation={}\nquestion_representation=[]\nlength_desc=30\nfor i,line in enumerate(q_lines):\n    #print(""line:"")\n    #print(line)\n    element_lists=line.split(\'\\t\') #[\'c324,c39\',\'w305...\',\'c\']\n    question_id=element_lists[0]\n    #print(""question_id:"",element_lists[0])\n    #for i,q_e in enumerate(element_lists):\n    #    print(""e:"",q_e)\n    #question_representation=[x for x in element_lists[2].split("","")] #+  #TODO this is only for title\'s word. no more.\n    title_words=[x for x in element_lists[2].strip().split("","")][-length_desc:]\n    #print(""title_words:"",title_words)\n    title_c=[x for x in element_lists[1].strip().split("","")][-length_desc:]\n    #print(""title_c:"", title_c)\n    desc_words=[x for x in element_lists[4].strip().split("","")][-length_desc:]\n    #print(""desc_words:"", desc_words)\n    desc_c=[x for x in element_lists[3].strip().split("","")][-length_desc:]\n    #print(""desc_c:"", desc_c)\n    question_representation =title_words+ title_c+desc_words+ desc_c\n    question_representation="" "".join(question_representation)\n    #print(""question_representation:"",question_representation)\n    #print(""question_representation:"",question_representation)\n    questionid_words_representation[question_id]=question_representation\nq_file.close()\nprint(""proces question ended2..."")\n#####################################################################################################################\n###################################################################################################################\n# 3.\xe8\x8e\xb7\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe4\xbb\xa5{\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x9aTOPIC_ID}\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8\n# save training data,testing data: question __label__topic_id\nimport codecs\nimport random\n\nprint(""saving traininig data.started1..."")\ncount = 0\ntrain_zhihu = \'train-zhihu6-title-desc.txt\'\ntest_zhihu = \'test-zhihu6-title-desc.txt\'\nvalid_zhihu = \'valid-zhihu6-title-desc.txt\'\ndata_list = []\nmulti_label_flag=True\n\ndef split_list(listt):\n    random.shuffle(listt)\n    list_len = len(listt)\n    train_len = 0.95\n    valid_len = 0.025\n    train = listt[0:int(list_len * train_len)]\n    valid = listt[int(list_len * train_len):int(list_len * (train_len + valid_len))]\n    test = listt[int(list_len * (train_len + valid_len)):]\n    return train, valid, test\n\nfor question_id, question_representation in questionid_words_representation.items():\n    # print(""===================>"")\n    # print(\'question_id\',question_id)\n    # print(""question_representation:"",question_representation)\n    # get label_id for this question_id by using:question_topic_dict\n    topic_list = question_topic_dict[question_id]\n    # print(""topic_list:"",topic_list)\n    # if count>5:\n    #    ii=0\n    #    ii/0\n    if not multi_label_flag:\n        for topic_id in topic_list:\n            data_list.append((question_representation, topic_id)) #single-label\n    else:\n        data_list.append((question_representation, topic_list)) #multi-label\n    count = count + 1\n\n# random shuffle list\nrandom.shuffle(data_list)\n\ndef write_data_to_file_system(file_name, data):\n    file = codecs.open(file_name, \'a\', \'utf8\')\n    for d in data:\n        # print(d)\n        question_representation, topic_id = d\n        question_representation_ = "" "".join(question_representation)\n        file.write(question_representation_ + "" __label__"" + str(topic_id) + ""\\n"")\n    file.close()\n\ndef write_data_to_file_system_multilabel(file_name, data):\n    file = codecs.open(file_name, \'a\', \'utf8\')\n    for d in data:\n        question_representation, topic_id_list = d\n        topic_id_list_="" "".join(topic_id_list)\n        file.write(question_representation + "" __label__"" + str(topic_id_list_) + ""\\n"")\n    file.close()\n\ntrain_data, valid_data, test_data = split_list(data_list)\nif not multi_label_flag:#single label\n    write_data_to_file_system(train_zhihu, train_data)\n    write_data_to_file_system(valid_zhihu, valid_data)\n    write_data_to_file_system(test_zhihu, test_data)\nelse:#multi-label\n    write_data_to_file_system_multilabel(train_zhihu, train_data)\n    write_data_to_file_system_multilabel(valid_zhihu, valid_data)\n    write_data_to_file_system_multilabel(test_zhihu, test_data)\n\nprint(""saving traininig data.ended..."")\n######################################################################################################################'"
aa1_data_util/2_predict_zhihu_get_question_representation.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\n\n#\xe5\x87\x86\xe5\xa4\x87\xe9\xa2\x84\xe6\xb5\x8b\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae.\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xa1\xa8\xe7\xa4\xba,\xe5\x86\x99\xe5\x88\xb0\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad.\n#prepreing prediction data. structure is just the same as training data.\n#proces question. for every question form a a list of string to reprensent it.\nimport codecs\nprint(""proces question started. get question representation..."")\ntarget_filename=\'test-muying-forpredict-v4only-title.txt\'\ntarget_file_predict = codecs.open(target_filename, \'a\', \'utf8\')\nqv=\'question_eval_set.txt\'\nq_filev = codecs.open(qv, \'r\', \'utf8\')\nq_linesv=q_filev.readlines()\nquestionid_words_representationv={}\nquestion_representationv=[]\nquestion_representationv_list=[]\nfor i,line in enumerate(q_linesv):\n    element_lists=line.split(\'\\t\') #[\'c324,c39\',\'w305...\',\'c\']\n    question_id=element_lists[0]\n    question_representationv=[x for x in element_lists[2].split("","")] # TODO +[x for x in element_lists[1].split("","")]\n    #print(""question_representation:"",question_representationv)\n    questionid_words_representationv[question_id]=question_representationv\n    question_representationv_list.append(question_representationv)\n    #if i>5:\n    #    break\n    question_representationv_="" "".join(question_representationv)\n    target_file_predict.write(question_representationv_+""\\n"")\ntarget_file_predict.close()\nprint(""proces question ended..."")'"
aa1_data_util/3_process_zhihu_question_topic_relation.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\n#\xe6\x9c\x80\xe7\xbb\x88\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9ax1=question_representation,x2=topic_representation,y=0(or 1)--->(x1,x2,y)\n\nimport codecs\n#1.\xe5\xb0\x86\xe9\x97\xae\xe9\xa2\x98ID\xe5\x92\x8cTOPIC\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\xb3\xe7\xb3\xbb\xe4\xbf\x9d\xe6\x8c\x81\xe5\x88\xb0\xe5\xad\x97\xe5\x85\xb8\xe9\x87\x8c.################################################################################\nprint(""process question_topic_train_set.txt,started..."")\nq_t=\'question_topic_train_set.txt\'\nq_t_file = codecs.open(q_t, \'r\', \'utf8\')\nlines=q_t_file.readlines()\nquestion_topic_dict={}\nfor i,line in enumerate(lines):\n    if i%300000==0:\n        print(i)\n    #print(line)\n    question_id,topic_list_string=line.split(\'\\t\')\n    #print(question_id)\n    #print(topic_list_string)\n    topic_list=topic_list_string.replace(""\\n"","""").split("","")\n    question_topic_dict[question_id]=topic_list\n    #for ii,topic in enumerate(topic_list):\n    #    print(ii,topic)\n    #print(""====================================="")\n    #if i>10:\n    #   print(question_topic_dict)\n    #   break\nprint(""process question_topic_train_set.txt,ended..."")\n###################################################################################################################\n###################################################################################################################\n#2.\xe5\xa4\x84\xe7\x90\x86\xe9\x97\xae\xe9\xa2\x98--\xe5\xbe\x97\xe5\x88\xb0{\xe9\x97\xae\xe9\xa2\x98ID\xef\xbc\x9a\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xa1\xa8\xe7\xa4\xba}\xef\xbc\x8c\xe5\xad\x98\xe6\x88\x90\xe5\xad\x97\xe5\x85\xb8\xe3\x80\x82\nimport codecs\nprint(""process question started11..."")\nq=\'question_train_set.txt\'\nq_file = codecs.open(q, \'r\', \'utf8\')\nq_lines=q_file.readlines()\nquestionid_words_representation={}\nquestion_representation=[]\nlength_desc=30\nfor i,line in enumerate(q_lines):\n    #print(""line:"")\n    #print(line)\n    element_lists=line.split(\'\\t\') #[\'c324,c39\',\'w305...\',\'c\']\n    question_id=element_lists[0]\n    #print(""question_id:"",element_lists[0])\n    #for i,q_e in enumerate(element_lists):\n    #    print(""e:"",q_e)\n    question_representation=[x for x in element_lists[2].split("","")] #+ \\\n    #[x for x in element_lists[1].split("","")]+ \\\n    #[x for x in element_lists[4][-length_desc:].split("","")] + \\\n    #[x for x in element_lists[3][-length_desc*2:].split("","")] #character:\xe5\xad\x97\n    #print(""question_representation:"",question_representation)\n    questionid_words_representation[question_id]=question_representation\nq_file.close()\nprint(""proces question ended2..."")\n\n###################################################################################################################\n###################################################################################################################\n#3.\xe5\xa4\x84\xe7\x90\x86topic\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0{TOPIC_ID,TOPIC\xe7\x9a\x84\xe8\xa1\xa8\xe7\xa4\xba}\xef\xbc\x8c\xe5\xad\x98\xe6\x88\x90\xe5\xad\x97\xe5\x85\xb8\ntopic_info_file_path=\'topic_info.txt\'\ndef read_topic_info():\n    f = codecs.open(topic_info_file_path, \'r\', \'utf8\')\n    lines=f.readlines()\n    dict_questionid_title={}\n    for i,line in enumerate(lines):\n        topic_id,partent_ids,title_character,title_words,desc_character,decs_words=line.split(""\\t"")\n        # print(i,""------------------------------------------------------"")\n        # print(""topic_id:"",topic_id)\n        # print(""partent_ids:"",partent_ids)\n        # print(""title_character:"",title_character)\n        # print(""title_words:"",title_words)\n        # print(""desc_character:"",desc_character)\n        # print(""decs_words:"",decs_words)\n        decs_words=decs_words.strip()\n        decs_words = decs_words.strip().split("","");decs_words = "" "".join(decs_words)\n\n        title_words=title_words.strip().split("","");title_words=title_words[0:30];title_words="" "".join(title_words);\n        dict_questionid_title[topic_id]=title_words+"" ""+decs_words\n    print(""len(dict_questionid_title):"",len(dict_questionid_title))\n    return dict_questionid_title\ndict_questionid_title=read_topic_info()\n#####################################################################################################################\n#####################################################################################################################\n# 4.\xe8\x8e\xb7\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe4\xbb\xa5{\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x9aTOPIC_ID}\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8\n# save training data,testing data: question __label__topic_id\nimport codecs\nimport random\n\nprint(""saving traininig data.started1..."")\ncount = 0\ntrain_zhihu = \'train_twoCNN_zhihu.txt\'\ntest_zhihu =  \'test_twoCNN_zhihu.txt\'\nvalid_zhihu = \'valid_twoCNN_zhihu.txt\'\ndata_list = []\n\n\ndef split_list(listt):\n    random.shuffle(listt)\n    list_len = len(listt)\n    train_len = 0.9\n    valid_len = 0.05\n    train = listt[0:int(list_len * train_len)]\n    valid = listt[int(list_len * train_len):int(list_len * (train_len + valid_len))]\n    test = listt[int(list_len * (train_len + valid_len)):]\n    return train, valid, test\n\ntopic_list_previous=None\nfrom random import choice\nfor question_id, question_representation in questionid_words_representation.items():\n    # print(""===================>"")\n    # print(\'question_id\',question_id)\n    # print(""question_representation:"",question_representation)\n    # get label_id for this question_id by using:question_topic_dict\n\n    topic_list = question_topic_dict[question_id]\n    # print(""topic_list:"",topic_list)\n    # if count>5:\n    #    ii=0\n    #    ii/0\n    for topic_id in topic_list:\n        topic_representation=dict_questionid_title[topic_id]\n        data_list.append((question_representation, topic_representation,1))\n        if topic_list_previous is not None:\n            topic_representation_negative = dict_questionid_title[choice(topic_list_previous)]\n            data_list.append((question_representation,topic_representation_negative,0))\n    topic_list_previous=topic_list\n    count = count + 1\n\n# random shuffle list\nrandom.shuffle(data_list)\n\n\ndef write_data_to_file_system(file_name, data):\n    file = codecs.open(file_name, \'a\', \'utf8\')\n    for d in data:\n        # print(d)\n        question_representation,topic_representation, label = d\n        question_representation_ = "" "".join(question_representation)\n        file.write(question_representation_ + ""\\t""+topic_representation+"" __label__"" + str(label) + ""\\n"")\n    file.close()\n\n\ntrain_data, valid_data, test_data = split_list(data_list)\nwrite_data_to_file_system(train_zhihu, train_data)\nwrite_data_to_file_system(valid_zhihu, valid_data)\nwrite_data_to_file_system(test_zhihu, test_data)\nprint(""saving traininig data.ended..."")\n######################################################################################################################'"
aa1_data_util/data_util_zhihu.py,0,"b'# -*- coding: utf-8 -*-\nimport codecs\nimport numpy as np\n#load data of zhihu\nimport word2vec\nimport os\nimport pickle\nPAD_ID = 0\nfrom tflearn.data_utils import pad_sequences\n_GO=""_GO""\n_END=""_END""\n_PAD=""_PAD""\ndef create_voabulary(simple=None,word2vec_model_path=\'zhihu-word2vec-title-desc.bin-100\',name_scope=\'\'): #zhihu-word2vec-multilabel.bin-100\n    cache_path =\'cache_vocabulary_label_pik/\'+ name_scope + ""_word_voabulary.pik""\n    print(""cache_path:"",cache_path,""file_exists:"",os.path.exists(cache_path))\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\n        with open(cache_path, \'r\') as data_f:\n            vocabulary_word2index, vocabulary_index2word=pickle.load(data_f)\n            return vocabulary_word2index, vocabulary_index2word\n    else:\n        vocabulary_word2index={}\n        vocabulary_index2word={}\n        if simple is not None:\n            word2vec_model_path=\'zhihu-word2vec.bin-100\'\n        print(""create vocabulary. word2vec_model_path:"",word2vec_model_path)\n        model=word2vec.load(word2vec_model_path,kind=\'bin\')\n        vocabulary_word2index[\'PAD_ID\']=0\n        vocabulary_index2word[0]=\'PAD_ID\'\n        special_index=0\n        if \'biLstmTextRelation\' in name_scope:\n            vocabulary_word2index[\'EOS\']=1 # a special token for biLstTextRelation model. which is used between two sentences.\n            vocabulary_index2word[1]=\'EOS\'\n            special_index=1\n        for i,vocab in enumerate(model.vocab):\n            vocabulary_word2index[vocab]=i+1+special_index\n            vocabulary_index2word[i+1+special_index]=vocab\n\n        #save to file system if vocabulary of words is not exists.\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n            with open(cache_path, \'a\') as data_f:\n                pickle.dump((vocabulary_word2index,vocabulary_index2word), data_f)\n    return vocabulary_word2index,vocabulary_index2word\n\n# create vocabulary of lables. label is sorted. 1 is high frequency, 2 is low frequency.\ndef create_voabulary_label(voabulary_label=\'train-zhihu4-only-title-all.txt\',name_scope=\'\',use_seq2seq=False):#\'train-zhihu.txt\'\n    print(""create_voabulary_label_sorted.started.traning_data_path:"",voabulary_label)\n    cache_path =\'cache_vocabulary_label_pik/\'+ name_scope + ""_label_voabulary.pik""\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\n        with open(cache_path, \'r\') as data_f:\n            vocabulary_word2index_label, vocabulary_index2word_label=pickle.load(data_f)\n            return vocabulary_word2index_label, vocabulary_index2word_label\n    else:\n        zhihu_f_train = codecs.open(voabulary_label, \'r\', \'utf8\')\n        lines=zhihu_f_train.readlines()\n        count=0\n        vocabulary_word2index_label={}\n        vocabulary_index2word_label={}\n        vocabulary_label_count_dict={} #{label:count}\n        for i,line in enumerate(lines):\n            if \'__label__\' in line:  #\'__label__-2051131023989903826\n                label=line[line.index(\'__label__\')+len(\'__label__\'):].strip().replace(""\\n"","""")\n                if vocabulary_label_count_dict.get(label,None) is not None:\n                    vocabulary_label_count_dict[label]=vocabulary_label_count_dict[label]+1\n                else:\n                    vocabulary_label_count_dict[label]=1\n        list_label=sort_by_value(vocabulary_label_count_dict)\n\n        print(""length of list_label:"",len(list_label));#print("";list_label:"",list_label)\n        countt=0\n\n        ##########################################################################################\n        if use_seq2seq:#if used for seq2seq model,insert two special label(token):_GO AND _END\n            i_list=[0,1,2];label_special_list=[_GO,_END,_PAD]\n            for i,label in zip(i_list,label_special_list):\n                vocabulary_word2index_label[label] = i\n                vocabulary_index2word_label[i] = label\n        #########################################################################################\n        for i,label in enumerate(list_label):\n            if i<10:\n                count_value=vocabulary_label_count_dict[label]\n                print(""label:"",label,""count_value:"",count_value)\n                countt=countt+count_value\n            indexx = i + 3 if use_seq2seq else i\n            vocabulary_word2index_label[label]=indexx\n            vocabulary_index2word_label[indexx]=label\n        print(""count top10:"",countt)\n\n        #save to file system if vocabulary of words is not exists.\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n            with open(cache_path, \'a\') as data_f:\n                pickle.dump((vocabulary_word2index_label,vocabulary_index2word_label), data_f)\n    print(""create_voabulary_label_sorted.ended.len of vocabulary_label:"",len(vocabulary_index2word_label))\n    return vocabulary_word2index_label,vocabulary_index2word_label\n\ndef sort_by_value(d):\n    items=d.items()\n    backitems=[[v[1],v[0]] for v in items]\n    backitems.sort(reverse=True)\n    return [ backitems[i][1] for i in range(0,len(backitems))]\n\ndef create_voabulary_labelO():\n    model = word2vec.load(\'zhihu-word2vec-multilabel.bin-100\', kind=\'bin\') #zhihu-word2vec.bin-100\n    count=0\n    vocabulary_word2index_label={}\n    vocabulary_index2word_label={}\n    label_unique={}\n    for i,vocab in enumerate(model.vocab):\n        if \'__label__\' in vocab:  #\'__label__-2051131023989903826\n            label=vocab[vocab.index(\'__label__\')+len(\'__label__\'):]\n            if label_unique.get(label,None) is None: #\xe4\xb8\x8d\xe6\x9b\xbe\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe4\xbf\x9d\xe6\x8c\x81\xe5\x88\xb0\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\n                vocabulary_word2index_label[label]=count\n                vocabulary_index2word_label[count]=label #ADD\n                count=count+1\n                label_unique[label]=label\n    return vocabulary_word2index_label,vocabulary_index2word_label\n\ndef load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\n                             traning_data_path=\'train-zhihu4-only-title-all.txt\',multi_label_flag=True,use_seq2seq=False,seq2seq_label_length=6):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\n    print(""load_data.started..."")\n    print(""load_data_multilabel_new.training_data_path:"",traning_data_path)\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    Y = []\n    Y_decoder_input=[] #ADD 2017-06-15\n    for i, line in enumerate(lines):\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        y=y.strip().replace(\'\\n\',\'\')\n        x = x.strip()\n        if i<1:\n            print(i,""x0:"",x) #get raw x\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<2:\n            print(i,""x1:"",x) #word to index\n        if use_seq2seq:        # 1)prepare label for seq2seq format(ADD _GO,_END,_PAD for seq2seq)\n            ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\n            _PAD_INDEX=vocabulary_word2index_label[_PAD]\n            ys_mulithot_list=[_PAD_INDEX]*seq2seq_label_length #[3,2,11,14,1]\n            ys_decoder_input=[_PAD_INDEX]*seq2seq_label_length\n            # below is label.\n            for j,y in enumerate(ys):\n                if j<seq2seq_label_length-1:\n                    ys_mulithot_list[j]=vocabulary_word2index_label[y]\n            if len(ys)>seq2seq_label_length-1:\n                ys_mulithot_list[seq2seq_label_length-1]=vocabulary_word2index_label[_END]#ADD END TOKEN\n            else:\n                ys_mulithot_list[len(ys)] = vocabulary_word2index_label[_END]\n\n            # below is input for decoder.\n            ys_decoder_input[0]=vocabulary_word2index_label[_GO]\n            for j,y in enumerate(ys):\n                if j < seq2seq_label_length - 1:\n                    ys_decoder_input[j+1]=vocabulary_word2index_label[y]\n            if i<10:\n                print(i,""ys:==========>0"", ys)\n                print(i,""ys_mulithot_list:==============>1"", ys_mulithot_list)\n                print(i,""ys_decoder_input:==============>2"", ys_decoder_input)\n        else:\n            if multi_label_flag: # 2)prepare multi-label format for classification\n                ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\n                ys_index=[]\n                for y in ys:\n                    y_index = vocabulary_word2index_label[y]\n                    ys_index.append(y_index)\n                ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\n            else:                #3)prepare single label format for classification\n                ys_mulithot_list=vocabulary_word2index_label[y]\n        if i<=3:\n            print(""ys_index:"")\n            #print(ys_index)\n            print(i,""y:"",y,"" ;ys_mulithot_list:"",ys_mulithot_list) #,"" ;ys_decoder_input:"",ys_decoder_input)\n        X.append(x)\n        Y.append(ys_mulithot_list)\n        if use_seq2seq:\n            Y_decoder_input.append(ys_decoder_input) #decoder input\n        #if i>50000:\n        #    break\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    print(""number_examples:"",number_examples) #\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\n    if use_seq2seq:\n        train=train+(Y_decoder_input[0:int((1 - valid_portion) * number_examples)],)\n        test=test+(Y_decoder_input[int((1 - valid_portion) * number_examples) + 1:],)\n    # 5.return\n    print(""load_data.ended..."")\n    return train, test, test\n\ndef load_data_multilabel_new_twoCNN(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\n                                    traning_data_path=\'train-zhihu4-only-title-all.txt\',multi_label_flag=True):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\n    print(""load_data.twoCNN.started..."")\n    print(""load_data_multilabel_new_twoCNN.training_data_path:"",traning_data_path)\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    X2=[]\n    Y = []\n    count_error=0\n    for i, line in enumerate(lines):\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        y=y.strip().replace(\'\\n\',\'\')\n        x = x.strip()\n        #print(""x:===============>"",x)\n        try:\n            x,x2=x.split(""\\t"")\n        except Exception:\n            print(""x.split.error."",x,""count_error:"",count_error)\n            count_error+=1\n            continue\n        if i<1:\n            print(i,""x0:"",x) #get raw x\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        x2=x2.split("" "")\n        x2 =[vocabulary_word2index.get(e, 0) for e in x2]\n        if i<1:\n            print(i,""x1:"",x,""x2:"",x2) #word to index\n        if multi_label_flag:\n            ys = y.replace(\'\\n\', \'\').split("" "") #ys is a list\n            ys_index=[]\n            for y in ys:\n                y_index = vocabulary_word2index_label[y]\n                ys_index.append(y_index)\n            ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\n        else:\n            ys_mulithot_list=int(y) #vocabulary_word2index_label[y]\n        if i<1:\n            print(i,""y:"",y,""ys_mulithot_list:"",ys_mulithot_list)\n        X.append(x)\n        X2.append(x2)\n        Y.append(ys_mulithot_list)\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    print(""number_examples:"",number_examples) #\n    train = (X[0:int((1 - valid_portion) * number_examples)],X2[0:int((1 - valid_portion) * number_examples)],Y[0:int((1 - valid_portion) * number_examples)])\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], X2[int((1 - valid_portion) * number_examples) + 1:],Y[int((1 - valid_portion) * number_examples) + 1:])\n    # 5.return\n    print(""load_data.ended..."")\n    return train, test, test\n\ndef load_data(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,training_data_path=\'train-zhihu4-only-title-all.txt\'):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\n    print(""load_data.started..."")\n    zhihu_f = codecs.open(training_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    Y = []\n    for i, line in enumerate(lines):\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        y=y.replace(\'\\n\',\'\')\n        x = x.replace(""\\t"",\' EOS \').strip()\n        if i<5:\n            print(""x0:"",x) #get raw x\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        #if i<5:\n        #    print(""x1:"",x_) #\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<5:\n            print(""x1:"",x) #word to index\n        y = vocabulary_word2index_label[y] #np.abs(hash(y))\n        X.append(x)\n        Y.append(y)\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    print(""number_examples:"",number_examples) #\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\n    # 5.return\n    print(""load_data.ended..."")\n    return train, test, test\n\n # \xe5\xb0\x86\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba(uigram,bigram,trigram)\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\ndef process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3):\n    """"""\n    :param sentence: string. example:\'w17314 w5521 w7729 w767 w10147 w111\'\n    :param n_gram:\n    :return:string. example:\'w17314 w17314w5521 w17314w5521w7729 w5521 w5521w7729 w5521w7729w767 w7729 w7729w767 w7729w767w10147 w767 w767w10147 w767w10147w111 w10147 w10147w111 w111\'\n    """"""\n    result=[]\n    word_list=sentence.split("" "") #[sentence[i] for i in range(len(sentence))]\n    unigram=\'\';bigram=\'\';trigram=\'\';fourgram=\'\'\n    length_sentence=len(word_list)\n    for i,word in enumerate(word_list):\n        unigram=word                           #ui-gram\n        word_i=unigram\n        if n_gram>=2 and i+2<=length_sentence: #bi-gram\n            bigram="""".join(word_list[i:i+2])\n            word_i=word_i+\' \'+bigram\n        if n_gram>=3 and i+3<=length_sentence: #tri-gram\n            trigram="""".join(word_list[i:i+3])\n            word_i = word_i + \' \' + trigram\n        if n_gram>=4 and i+4<=length_sentence: #four-gram\n            fourgram="""".join(word_list[i:i+4])\n            word_i = word_i + \' \' + fourgram\n        if n_gram>=5 and i+5<=length_sentence: #five-gram\n            fivegram="""".join(word_list[i:i+5])\n            word_i = word_i + \' \' + fivegram\n        result.append(word_i)\n    result="" "".join(result)\n    return result\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xa0\x87\xe7\xad\xbe\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaalabel\xef\xbc\x9aload data with multi-labels\ndef load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,traning_path,valid_portion=0.05,max_training_data=1000000):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example: \'w140 w13867 w10344 w2673 w9514 w269 w460 w6 w35053 w844 w10147 w111 __label__-2379261820462209275 -5535923551616745326 6038661761506862294\'\n    print(""load_data_with_multilabels.ended..."")\n    zhihu_f = codecs.open(traning_path,\'r\',\'utf8\') #(\'/home/xul/xul/9_ZhihuCup/\'+data_type+\'-zhihu5-only-title-multilabel.txt\', \'r\', \'utf8\') #home/xul/xul/9_ZhihuCup/\'\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    Y = []\n    Y_label1999=[]\n    for i, line in enumerate(lines):\n        #if i>max_training_data:\n        #    break\n        x, ys = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        ys=ys.replace(\'\\n\',\'\').split("" "")\n        x = x.strip()\n        if i < 5:\n            print(""x0:"", x) # u\'w4260 w4260w86860 w4260w86860w30907 w86860 w86860w30907 w86860w30907w11 w30907 w30907w11 w30907w11w31 w11 w11w31 w11w31w72 w31 w31w72 w31w72w166 w72 w72w166 w72w166w346 w166 w166w346 w166w346w2182 w346 w346w2182 w346w2182w224 w2182 w2182w224 w2182w224w2148 w224 w224w2148 w224w2148w6 w2148 w2148w6 w2148w6w2566 w6 w6w2566 w6w2566w25 w2566 w2566w25 w2566w25w1110 w25 w25w1110 w25w1110w111 w1110 w1110w111 w111\'\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        #if i < 5:\n        #    print(""x1:"", x_)\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<5:\n            print(""x2:"", x)\n        #print(""ys:"",ys) #[\'501174938575526146\', \'-4317515119936650885\']\n        ys_list=[]\n        for y in ys:\n            y_ = vocabulary_word2index_label[y]\n            ys_list.append(y_)\n        X.append(x)\n        #TODO ys_list_array=transform_multilabel_as_multihot(ys_list) #it is 2-d array. [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]...]\n        ys_list_=proces_label_to_algin(ys_list)\n        Y.append(ys_list_)\n        #TODO Y_label1999.append(ys_list_array)\n        if i==0:\n            print(X,Y)\n            print(Y_label1999)\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)]) #TODO Y_label1999[0:int((1 - valid_portion) * number_examples)]\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:]) #TODO ,Y_label1999[int((1 - valid_portion) * number_examples) + 1:]\n    print(""load_data_with_multilabels.ended..."")\n    return train, test\n\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\ndef transform_multilabel_as_multihot(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\n    """"""\n    :param label_list: e.g.[0,1,4]\n    :param label_size: e.g.199\n    :return:e.g.[1,1,0,1,0,0,........]\n    """"""\n    result=np.zeros(label_size)\n    #set those location as 1, all else place as 0.\n    result[label_list] = 1\n    return result\n\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\ndef transform_multilabel_as_multihotO(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\n    batch_size=len(label_list)\n    result=np.zeros((batch_size,label_size))\n    #set those location as 1, all else place as 0.\n    result[(range(batch_size),label_list)]=1\n    return result\n\ndef load_final_test_data(file_path):\n    final_test_file_predict_object = codecs.open(file_path, \'r\', \'utf8\')\n    lines=final_test_file_predict_object.readlines()\n    question_lists_result=[]\n    for i,line in enumerate(lines):\n        question_id,question_string=line.split(""\\t"")\n        question_string=question_string.strip().replace(""\\n"","""")\n        question_lists_result.append((question_id,question_string))\n    print(""length of total question lists:"",len(question_lists_result))\n    return question_lists_result\n\ndef load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists,uni_to_tri_gram=False):  # n_words=100000,\n    final_list=[]\n    for i, tuplee in enumerate(questionid_question_lists):\n        queston_id,question_string_list=tuplee\n        if uni_to_tri_gram:\n            x_=process_one_sentence_to_get_ui_bi_tri_gram(question_string_list)\n            x=x_.split("" "")\n        else:\n            x=question_string_list.split("" "")\n        x = [vocabulary_word2index.get(e, 0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<=2:\n            print(""question_id:"",queston_id);print(""question_string_list:"",question_string_list);print(""x_indexed:"",x)\n        final_list.append((queston_id,x))\n    number_examples = len(final_list)\n    print(""number_examples:"",number_examples) #\n    return  final_list\n\n\ndef proces_label_to_algin(ys_list,require_size=5):\n    """"""\n    given a list of labels, process it to fixed size(\'require_size\')\n    :param ys_list: a list\n    :return: a list\n    """"""\n    ys_list_result=[0 for x in range(require_size)]\n    if len(ys_list)>=require_size: #\xe8\xb6\x85\xe9\x95\xbf\n        ys_list_result=ys_list[0:require_size]\n    else:#\xe5\xa4\xaa\xe7\x9f\xad\n       if len(ys_list)==1:\n           ys_list_result =[ys_list[0] for x in range(require_size)]\n       elif len(ys_list)==2:\n           ys_list_result = [ys_list[0],ys_list[0],ys_list[0],ys_list[1],ys_list[1]]\n       elif len(ys_list) == 3:\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[1], ys_list[2]]\n       elif len(ys_list) == 4:\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[2], ys_list[3]]\n    return ys_list_result\n\ndef write_uigram_to_trigram():\n    pass\n    #1.read file.\n    #2.uigram--->trigram\n    #3.write each line to file system.\n\ndef test_pad():\n    trainX=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\n    trainX=trainX.split("" "")\n    trainX = pad_sequences([[trainX]], maxlen=100, value=0.)\n    print(""trainX:"",trainX)\n\ntopic_info_file_path=\'topic_info.txt\'\ndef read_topic_info():\n    f = codecs.open(topic_info_file_path, \'r\', \'utf8\')\n    lines=f.readlines()\n    dict_questionid_title={}\n    for i,line in enumerate(lines):\n        topic_id,partent_ids,title_character,title_words,desc_character,decs_words=line.split(""\\t"").strip()\n        # print(i,""------------------------------------------------------"")\n        # print(""topic_id:"",topic_id)\n        # print(""partent_ids:"",partent_ids)\n        # print(""title_character:"",title_character)\n        # print(""title_words:"",title_words)\n        # print(""desc_character:"",desc_character)\n        # print(""decs_words:"",decs_words)\n        dict_questionid_title[topic_id]=title_words+"" ""+decs_words\n    print(""len(dict_questionid_title):"",len(dict_questionid_title))\n    return dict_questionid_title\n\ndef stat_training_data_length():\n    training_data=\'train-zhihu4-only-title-all.txt\'\n    f = codecs.open(training_data, \'r\', \'utf8\')\n    lines=f.readlines()\n    length_dict={0:0,5:0,10:0,15:0,20:0,25:0,30:0,35:0,40:0,100:0,150:0,200:0,1500:0}\n    length_list=[0,5,10,15,20,25,30,35,40,100,150,200,1500]\n    for i,line in enumerate(lines):\n        line_list=line.split(\'__label__\')[0].strip().split("" "")\n        length=len(line_list)\n        #print(i,""length:"",length)\n        for l in length_list:\n            if length<l:\n                length=l\n                #print(""length.assigned:"",length)\n                break\n        #print(""length.before dict assign:"", length)\n        length_dict[length]=length_dict[length]+1\n    print(""length_dict:"",length_dict)\n\n\nif __name__ == \'__main__\':\n    if __name__ == \'__main__\':\n        if __name__ == \'__main__\':\n            #1.\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\n            #vocabulary_word2index_label, vocabulary_index2word_label=create_voabulary_label()\n            #load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,data_type=\'test\')\n            #2.\n            #sentence=u\'\xe6\x88\x91\xe6\x83\xb3\xe5\xbc\x80\xe9\x80\x9a\xe5\x88\x9b\xe4\xb8\x9a\xe6\x9d\xbf\'\n            #sentence=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\n            #result=process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3)\n            #print(len(result),""result:"",result)\n\n            #3. transform to multilabel\n            #label_list=[0,1,4,9,5]\n            #result=transform_multilabel_as_multihot(label_list,label_size=15)\n            #print(""result:"",result)\n\n            #4.load data for predict-----------------------------------------------------------------\n            #file_path=\'test-zhihu-forpredict-v4only-title.txt\'\n            #questionid_question_lists=load_final_test_data(file_path)\n\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\n            #vocabulary_word2index_label,_=create_voabulary_label()\n            #final_list=load_data_predict(vocabulary_word2index, vocabulary_word2index_label, questionid_question_lists)\n\n            #5.process label require lengh\n            #ys_list=[99999]\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\n            #print(ys_list,""ys_list_result1.:"",ys_list_result)\n            #ys_list=[99999,23423432,67566765]\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\n            #ys_list=[99999,23423432,67566765,23333333]\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\n            #ys_list = [99999, 23423432, 67566765,44543543,546546546,323423434]\n            #ys_list_result = proces_label_to_algin(ys_list, require_size=5)\n            #print(ys_list, ""ys_list_result3.:"", ys_list_result)\n\n            #6.create vocabulary label. sorted.\n            #create_voabulary_label()\n\n            #d={\'a\':3,\'b\':2,\'c\':11}\n            #d_=sort_by_value(d)\n            #print(""d_"",d_)\n\n            #7.\n            #test_pad()\n\n            #8.read topic info\n            #read_topic_info()\n\n            #9\xe3\x80\x82\n            stat_training_data_length()\n'"
aa2_ClassificationTflearn/p2_classification_tflearn.py,0,"b'print(""started..."")\nimport tflearn\nimport numpy as np\nimport tensorflow as tf\nclass_number=3 #10\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 784]) #set input data\'s shape\nnet = tflearn.fully_connected(net, 64) #one layer of FC\nnet = tflearn.dropout(net, 0.5) #dropout\nnet = tflearn.fully_connected(net, class_number, activation=\'softmax\') #one layer of FC(this layer will be output possibility distribution )\nnet = tflearn.regression(net, optimizer=\'adam\', loss=\'categorical_crossentropy\') #classification\n\nmodel = tflearn.DNN(net) #deep Neural Network Model\n\n# invoke method with some data--------------------------------------------------------------------\ndef convert_int_to_one_hot(number,label_size):\n    listt=[0 for x in range(label_size)]\n    listt[number]=1\n    return listt\n\nbatch_size=32\nX=np.random.randn(batch_size,784)\ny_=[convert_int_to_one_hot(np.random.choice(class_number),class_number) for xx in range(batch_size)]\ny=np.array(y_)\n\nX_test=np.random.randn(batch_size,784)\ny_2=[convert_int_to_one_hot(np.random.choice(class_number),class_number) for xx in range(batch_size)]\nY_test=np.array(y_2)\nmodel.fit(X, y,validation_set=(X_test, Y_test),show_metric=True)\n#-------------------------------------------------------------------------------------------------\nprint(""ended..."")'"
aa2_ClassificationTflearn/p2_classification_tflearn_demo.py,0,"b'print(""started..."")\nimport tflearn\nimport numpy as np\nimport tensorflow as tf\nclass_number=3 #10\ntflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n\nnet = tflearn.input_data(shape=[None, 784]) #set input data\'s shape\nnet = tflearn.fully_connected(net, 64) #one layer of FC\nnet = tflearn.dropout(net, 0.5) #dropout\nnet = tflearn.fully_connected(net, class_number, activation=\'softmax\') #one layer of FC(this layer will be output possibility distribution )\nnet = tflearn.regression(net, optimizer=\'adam\', loss=\'categorical_crossentropy\') #classification\n\nmodel = tflearn.DNN(net) #deep Neural Network Model\n\n# invoke method with some data--------------------------------------------------------------------\ndef convert_int_to_one_hot(number,label_size):\n    listt=[0 for x in range(label_size)]\n    listt[number]=1\n    return listt\n\nbatch_size=32\nX=np.random.randn(batch_size,784)\ny_=[convert_int_to_one_hot(np.random.choice(class_number),class_number) for xx in range(batch_size)]\ny=np.array(y_)\n\nX_test=np.random.randn(batch_size,784)\ny_2=[convert_int_to_one_hot(np.random.choice(class_number),class_number) for xx in range(batch_size)]\nY_test=np.array(y_2)\nmodel.fit(X, y,validation_set=(X_test, Y_test),show_metric=True)\n#-------------------------------------------------------------------------------------------------\nprint(""ended..."")'"
aa3_CNNSentenceClassificationTflearn/p4_cnn_sentence_classification.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\n""""""\nSimple example using convolutional neural network to classify IMDB\nsentiment dataset.\nReferences:\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n    - Kim Y. Convolutional Neural Networks for Sentence Classification[C].\n    Empirical Methods in Natural Language Processing, 2014.\nLinks:\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n    - http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf\n""""""\nimport tensorflow as tf\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_1d, global_max_pool\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_utils import to_categorical, pad_sequences\nfrom tflearn.datasets import imdb\nimport numpy as np\n\nprint(""started..."")\n# 1.IMDB Dataset loading\ntrain, test, _ = imdb.load_data(path=\'imdb.pkl\', n_words=10000,valid_portion=0.1)\ntrainX, trainY = train\ntestX, testY = test\nprint(""testX.shape:"",np.array(testX).shape) #2500\xe4\xb8\xaalist.\xe6\xaf\x8f\xe4\xb8\xaalist\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\nprint(""testY.shape:"",np.array(testY).shape) #2500\xe4\xb8\xaalabel\nprint(""testX[0]:"",testX[0]) #[17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\nprint(""testY[0]:"",testY[0]) #0\n\n# 2.Data preprocessing\n# Sequence padding\ntrainX = pad_sequences(trainX, maxlen=100, value=0.) #padding to max length\ntestX = pad_sequences(testX, maxlen=100, value=0.)   #padding to max length\n# Converting labels to binary vectors\ntrainY = to_categorical(trainY, nb_classes=2) #y as one hot\ntestY = to_categorical(testY, nb_classes=2)   #y as one hot\n\n# 3.Building convolutional network\n#(shape=None, placeholder=None, dtype=tf.float32,data_preprocessing=None, data_augmentation=None,name=""InputData"")\nnetwork = input_data(shape=[None, 100], name=\'input\') #[None, 100] `input_data` is used as a data entry (placeholder) of a network. This placeholder will be feeded with data when training\nnetwork = tflearn.embedding(network, input_dim=10000, output_dim=128) #[None, 100,128].embedding layer for a sequence of ids. network: Incoming 2-D Tensor. input_dim: vocabulary size, oput_dim:embedding size\n         #conv_1d(incoming,nb_filter,filter_size)\nbranch1 = conv_1d(network, 128, 3, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps1, nb_filters]. padding:""VALID"",only ever drops the right-most columns\nbranch2 = conv_1d(network, 128, 4, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps2, nb_filters]\nbranch3 = conv_1d(network, 128, 5, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps3, nb_filters]\nnetwork = merge([branch1, branch2, branch3], mode=\'concat\', axis=1) # merge a list of `Tensor` into a single one.===>[batch_size, new steps1+new step2+new step3, nb_filters]\nnetwork = tf.expand_dims(network, 2) #[batch_size, new steps1+new step2+new step3,1, nb_filters] Inserts a dimension of 1 into a tensor\'s shape\nnetwork = global_max_pool(network) #[batch_size, pooled dim]\nnetwork = dropout(network, 0.5) #[batch_size, pooled dim]\nnetwork = fully_connected(network, 2, activation=\'softmax\') #matmul([batch_size, pooled_dim],[pooled_dim,2])---->[batch_size,2]\nnetwork = regression(network, optimizer=\'adam\', learning_rate=0.001,\n                     loss=\'categorical_crossentropy\', name=\'target\')\n# Training\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(trainX, trainY, n_epoch = 5, shuffle=True, validation_set=(testX, testY), show_metric=True, batch_size=32)\nprint(""ended..."")'"
aa3_CNNSentenceClassificationTflearn/p4_cnn_sentence_classification_zhihu.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\n""""""\nSimple example using convolutional neural network to classify IMDB\nsentiment dataset.\nReferences:\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n    - Kim Y. Convolutional Neural Networks for Sentence Classification[C].\n    Empirical Methods in Natural Language Processing, 2014.\nLinks:\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n    - http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf\n""""""\nimport tensorflow as tf\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_1d, global_max_pool\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_utils import to_categorical, pad_sequences\n#from tflearn.datasets import imdb\nfrom p4_zhihu_load_data import load_data,create_voabulary,create_voabulary_label\nimport numpy as np\nimport pickle\n\nprint(""started..."")\nf_cache=\'data_zhihu.pik\'\n# 1. loading dataset\nwith open(f_cache, \'r\') as f:\n    trainX,trainY,testX,testY=pickle.load(f)\nif trainX is not None and trainY is not None: #\xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\n    print(""training data not exist==>load data, and dump it to file system"")\n    vocabulary_word2index, vocabulary_index2word = create_voabulary()\n    vocabulary_word2index_label = create_voabulary_label()\n    train, test, _ =load_data(vocabulary_word2index, vocabulary_word2index_label)\n    trainX, trainY = train\n    testX, testY = test\n    nb_classes=1999\n    print(""testX.shape:"",np.array(testX).shape) #2500\xe4\xb8\xaalist.\xe6\xaf\x8f\xe4\xb8\xaalist\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\n    print(""testY.shape:"",np.array(testY).shape) #2500\xe4\xb8\xaalabel\n    print(""testX[0]:"",testX[0]) #[17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\n    print(""testX[1]:"",testX[1]);print(""testY[0]:"",testY[0]) #0 ;print(""testY[1]:"",testY[1]) #0\n\n    # 2.Data preprocessing\n    # Sequence padding\n    print(""start padding & transform to one hot..."")\n    trainX = pad_sequences(trainX, maxlen=100, value=0.) #padding to max length\n    testX = pad_sequences(testX, maxlen=100, value=0.)   #padding to max length\n    # Converting labels to binary vectors\n    trainY = to_categorical(trainY, nb_classes=nb_classes) #y as one hot\n    testY = to_categorical(testY, nb_classes=nb_classes)   #y as one hot\n    print(""end padding & transform to one hot..."")\n    #cahe trainX,trainY,testX,testY for next time use.\n    pickle.dump((trainX,trainY,testX,testY))\nelse:\n    print(""traning data exists in cache. going to use it."")\n\n# 3.Building convolutional network\n######################################MODEL:1.conv-2.conv-3.conv-4.max_pool-5.dropout-6.FC##############################################################################################\n#(shape=None, placeholder=None, dtype=tf.float32,data_preprocessing=None, data_augmentation=None,name=""InputData"")\nnetwork = input_data(shape=[None, 100], name=\'input\') #[None, 100] `input_data` is used as a data entry (placeholder) of a network. This placeholder will be feeded with data when training\nnetwork = tflearn.embedding(network, input_dim=10000, output_dim=128) #[None, 100,128].embedding layer for a sequence of ids. network: Incoming 2-D Tensor. input_dim: vocabulary size, oput_dim:embedding size\n         #conv_1d(incoming,nb_filter,filter_size)\nbranch1 = conv_1d(network, 128, 3, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps1, nb_filters]. padding:""VALID"",only ever drops the right-most columns\nbranch2 = conv_1d(network, 128, 4, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps2, nb_filters]\nbranch3 = conv_1d(network, 128, 5, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps3, nb_filters]\nnetwork = merge([branch1, branch2, branch3], mode=\'concat\', axis=1) # merge a list of `Tensor` into a single one.===>[batch_size, new steps1+new step2+new step3, nb_filters]\nnetwork = tf.expand_dims(network, 2) #[batch_size, new steps1+new step2+new step3,1, nb_filters] Inserts a dimension of 1 into a tensor\'s shape\nnetwork = global_max_pool(network) #[batch_size, pooled dim]\nnetwork = dropout(network, 0.5) #[batch_size, pooled dim]\nnetwork = fully_connected(network, nb_classes, activation=\'softmax\') #matmul([batch_size, pooled_dim],[pooled_dim,2])---->[batch_size,2]\nnetwork = regression(network, optimizer=\'adam\', learning_rate=0.001,loss=\'categorical_crossentropy\', name=\'target\')\n######################################MODEL:1.conv-2.conv-3.conv-4.max_pool-5.dropout-6.FC################################################################################################\n# 4.Training\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(trainX, trainY, n_epoch = 5, shuffle=True, validation_set=(testX, testY), show_metric=True, batch_size=256) #32\nprint(""ended..."")'"
aa3_CNNSentenceClassificationTflearn/p4_cnn_sentence_classification_zhihu2.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\n""""""\nSimple example using convolutional neural network to classify IMDB\nsentiment dataset.\nReferences:\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n    - Kim Y. Convolutional Neural Networks for Sentence Classification[C].\n    Empirical Methods in Natural Language Processing, 2014.\nLinks:\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n    - http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf\n""""""\nimport tensorflow as tf\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_1d, global_max_pool\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_utils import to_categorical, pad_sequences\n#from tflearn.datasets import imdb\nfrom p4_zhihu_load_data import load_data,create_voabulary,create_voabulary_label\nimport numpy as np\nimport pickle\nimport os\n#import tflearn.metrics.Top_k as Top_k\n\nprint(""started..."")\nf_cache=\'data_zhihu.pik\'\n# 1. loading dataset\ntrainX,trainY,testX,testY=None,None,None,None\nnumber_classes=1999\n#if os.path.exists(f_cache):\n#    with open(f_cache, \'r\') as f:\n#        trainX,trainY,testX,testY,vocab_size=pickle.load(f)\n#if trainX is None or trainY is None: #\xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\n#-------------------------------------------------------------------------------------------------\nprint(""training data not exist==>load data, and dump it to file system"")\nvocabulary_word2index, vocabulary_index2word = create_voabulary()\nvocab_size=len(vocabulary_word2index)\nvocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label()\ntrain, test, _ =load_data(vocabulary_word2index, vocabulary_word2index_label)\ntrainX, trainY = train\ntestX, testY = test\nprint(""testX.shape:"",np.array(testX).shape) #2500\xe4\xb8\xaalist.\xe6\xaf\x8f\xe4\xb8\xaalist\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\nprint(""testY.shape:"",np.array(testY).shape) #2500\xe4\xb8\xaalabel\nprint(""testX[0]:"",testX[0]) #[17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\nprint(""testX[1]:"",testX[1]);print(""testY[0]:"",testY[0]) #0 ;print(""testY[1]:"",testY[1]) #0\n\n# 2.Data preprocessing\n# Sequence padding\nprint(""start padding & transform to one hot..."")\ntrainX = pad_sequences(trainX, maxlen=100, value=0.) #padding to max length\ntestX = pad_sequences(testX, maxlen=100, value=0.)   #padding to max length\n# Converting labels to binary vectors\ntrainY = to_categorical(trainY, nb_classes=number_classes) #y as one hot\ntestY = to_categorical(testY, nb_classes=number_classes)   #y as one hot\nprint(""end padding & transform to one hot..."")\n#--------------------------------------------------------------------------------------------------\n    # cache trainX,trainY,testX,testY for next time use.\n#    with open(f_cache, \'w\') as f:\n#        pickle.dump((trainX,trainY,testX,testY,vocab_size),f)\n#else:\n#    print(""traning data exists in cache. going to use it."")\n\n# 3.Building convolutional network\n######################################MODEL:1.conv-2.conv-3.conv-4.max_pool-5.dropout-6.FC##############################################################################################\n#(shape=None, placeholder=None, dtype=tf.float32,data_preprocessing=None, data_augmentation=None,name=""InputData"")\nnetwork = input_data(shape=[None, 100], name=\'input\') #[None, 100] `input_data` is used as a data entry (placeholder) of a network. This placeholder will be feeded with data when training\nnetwork = tflearn.embedding(network, input_dim=vocab_size, output_dim=256) #TODO 128 [None, 100,128].embedding layer for a sequence of ids. network: Incoming 2-D Tensor. input_dim: vocabulary size, oput_dim:embedding size\n         #conv_1d(incoming,nb_filter,filter_size)\nbranch1 = conv_1d(network, 256, 1, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128\nbranch2 = conv_1d(network, 256, 2, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128\nbranch3 = conv_1d(network, 256, 3, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps1, nb_filters]. padding:""VALID"",only ever drops the right-most columns\nbranch4 = conv_1d(network, 256, 4, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps2, nb_filters]\nbranch5 = conv_1d(network, 256, 5, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps3, nb_filters]\nbranch6 = conv_1d(network, 256, 6, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps3, nb_filters] #ADD\nbranch7 = conv_1d(network, 256, 7, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps3, nb_filters] #ADD\nbranch8 = conv_1d(network, 256, 7, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps3, nb_filters] #ADD\nbranch9 = conv_1d(network, 256, 8, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps3, nb_filters] #ADD\nbranch10 = conv_1d(network,256, 9, padding=\'valid\', activation=\'relu\', regularizer=""L2"") #128 [batch_size, new steps3, nb_filters] #ADD\nnetwork = merge([branch1, branch2, branch3,branch4,branch5,branch6, branch7, branch8,branch9,branch10], mode=\'concat\', axis=1) # merge a list of `Tensor` into a single one.===>[batch_size, new steps1+new step2+new step3, nb_filters]\nnetwork = tf.expand_dims(network, 2) #[batch_size, new steps1+new step2+new step3,1, nb_filters] Inserts a dimension of 1 into a tensor\'s shape\nnetwork = global_max_pool(network) #input: 4-D tensors,[batch_size,height,width,in_channels]; output:2-D Tensor,[batch_size, pooled dim]\nnetwork = dropout(network, 0.5) #[batch_size, pooled dim]\nnetwork = fully_connected(network, number_classes, activation=\'softmax\') #matmul([batch_size, pooled_dim],[pooled_dim,2])---->[batch_size,number_classes]\n#top5 = tflearn.metrics.Top_k(k=5)\nnetwork = regression(network, optimizer=\'adam\', learning_rate=0.001,loss=\'categorical_crossentropy\', name=\'target\') #,metric=top5\n######################################MODEL:1.conv-2.conv-3.conv-4.max_pool-5.dropout-6.FC################################################################################################\n# 4.Training\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(trainX, trainY, n_epoch = 10, shuffle=True, validation_set=(testX, testY), show_metric=True, batch_size=256) #32\nmodel.save(\'model_zhihu_cnn12345\')\n\nprint(""going to make a prediction..."")\nmodel.predict(testX[0:1000])\nprint(""ended..."")'"
aa3_CNNSentenceClassificationTflearn/p4_cnn_sentence_classification_zhihu2_predict.py,2,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\n""""""\nSimple example using convolutional neural network to classify IMDB\nsentiment dataset.\nReferences:\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n    - Kim Y. Convolutional Neural Networks for Sentence Classification[C].\n    Empirical Methods in Natural Language Processing, 2014.\nLinks:\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n    - http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf\n""""""\nimport tensorflow as tf\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_1d, global_max_pool\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_utils import to_categorical, pad_sequences\n#from tflearn.datasets import imdb\nfrom p4_zhihu_load_data import load_data,create_voabulary,create_voabulary_label\nimport numpy as np\nimport pickle\nimport os\n#import tflearn.metrics.Metric.Top_k as Top_k\n\nprint(""started..."")\nf_cache=\'data_zhihu.pik\'\n# 1. loading dataset\ntrainX,trainY,testX,testY=None,None,None,None\nnumber_classes=1999\n#if os.path.exists(f_cache):\n#    with open(f_cache, \'r\') as f:\n#        trainX,trainY,testX,testY,vocab_size=pickle.load(f)\n#if trainX is None or trainY is None: #\xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\n#-------------------------------------------------------------------------------------------------\nprint(""training data not exist==>load data, and dump it to file system"")\nvocabulary_word2index, vocabulary_index2word = create_voabulary()\nvocab_size=len(vocabulary_word2index)\nvocabulary_word2index_label = create_voabulary_label()\ntrain, test, _ =load_data(vocabulary_word2index, vocabulary_word2index_label)\ntrainX, trainY = train\ntestX, testY = test\nprint(""testX.shape:"",np.array(testX).shape) #2500\xe4\xb8\xaalist.\xe6\xaf\x8f\xe4\xb8\xaalist\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\nprint(""testY.shape:"",np.array(testY).shape) #2500\xe4\xb8\xaalabel\nprint(""testX[0]:"",testX[0]) #[17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\nprint(""testX[1]:"",testX[1]);print(""testY[0]:"",testY[0]) #0 ;print(""testY[1]:"",testY[1]) #0\n\n# 2.Data preprocessing\n# Sequence padding\nprint(""start padding & transform to one hot..."")\ntrainX = pad_sequences(trainX, maxlen=100, value=0.) #padding to max length\ntestX = pad_sequences(testX, maxlen=100, value=0.)   #padding to max length\n# Converting labels to binary vectors\ntrainY = to_categorical(trainY, nb_classes=number_classes) #y as one hot\ntestY = to_categorical(testY, nb_classes=number_classes)   #y as one hot\nprint(""end padding & transform to one hot..."")\n#--------------------------------------------------------------------------------------------------\n    # cache trainX,trainY,testX,testY for next time use.\n#    with open(f_cache, \'w\') as f:\n#        pickle.dump((trainX,trainY,testX,testY,vocab_size),f)\n#else:\n#    print(""traning data exists in cache. going to use it."")\n\n# 3.Building convolutional network\n######################################MODEL:1.conv-2.conv-3.conv-4.max_pool-5.dropout-6.FC##############################################################################################\n#(shape=None, placeholder=None, dtype=tf.float32,data_preprocessing=None, data_augmentation=None,name=""InputData"")\nnetwork = input_data(shape=[None, 100], name=\'input\') #[None, 100] `input_data` is used as a data entry (placeholder) of a network. This placeholder will be feeded with data when training\nnetwork = tflearn.embedding(network, input_dim=vocab_size, output_dim=128) #TODO [None, 100,128].embedding layer for a sequence of ids. network: Incoming 2-D Tensor. input_dim: vocabulary size, oput_dim:embedding size\n         #conv_1d(incoming,nb_filter,filter_size)\nbranch1 = conv_1d(network, 128, 1, padding=\'valid\', activation=\'relu\', regularizer=""L2"")\nbranch2 = conv_1d(network, 128, 2, padding=\'valid\', activation=\'relu\', regularizer=""L2"")\nbranch3 = conv_1d(network, 128, 3, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps1, nb_filters]. padding:""VALID"",only ever drops the right-most columns\nbranch4 = conv_1d(network, 128, 4, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps2, nb_filters]\nbranch5 = conv_1d(network, 128, 5, padding=\'valid\', activation=\'relu\', regularizer=""L2"") # [batch_size, new steps3, nb_filters]\nnetwork = merge([branch1, branch2, branch3,branch4,branch5], mode=\'concat\', axis=1) # merge a list of `Tensor` into a single one.===>[batch_size, new steps1+new step2+new step3, nb_filters]\nnetwork = tf.expand_dims(network, 2) #[batch_size, new steps1+new step2+new step3,1, nb_filters] Inserts a dimension of 1 into a tensor\'s shape\nnetwork = global_max_pool(network) #[batch_size, pooled dim]\nnetwork = dropout(network, 0.5) #[batch_size, pooled dim]\nnetwork = fully_connected(network, number_classes, activation=\'softmax\') #matmul([batch_size, pooled_dim],[pooled_dim,2])---->[batch_size,number_classes]\ntop5 = tflearn.metrics.Top_k(k=5)\nnetwork = regression(network, optimizer=\'adam\', learning_rate=0.001,loss=\'categorical_crossentropy\', name=\'target\') #metric=top5\n######################################MODEL:1.conv-2.conv-3.conv-4.max_pool-5.dropout-6.FC################################################################################################\n# 4.Training\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\n#model.fit(trainX, trainY, n_epoch = 10, shuffle=True, validation_set=(testX, testY), show_metric=True, batch_size=256) #32\n#model.save(\'model_zhihu_cnn12345\')\nmodel.load(\'model_zhihu_cnn12345\')\nprint(""going to make a prediction..."")\npredict_result=model.predict(testX[0:1000])\nprint(""predict_result:"",predict_result)\nprint(""ended..."")'"
aa3_CNNSentenceClassificationTflearn/p4_conv_classification_tflearn.py,0,"b'from __future__ import division, print_function, absolute_import\n\nimport tensorflow as tf\n\n# -*- coding: utf-8 -*-\n\n"""""" Convolutional network applied to CIFAR-10 dataset classification task.\nReferences:\n    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.\nLinks:\n    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n""""""\n\nimport tflearn\nfrom tflearn.data_utils import shuffle, to_categorical\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.estimator import regression\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\n\nprint(""started..."")\n# Data loading and preprocessing\nfrom tflearn.datasets import cifar10\n(X, Y), (X_test, Y_test) = cifar10.load_data()\nX, Y = shuffle(X, Y)\nY = to_categorical(Y, 10)\nY_test = to_categorical(Y_test, 10)\n\n# Real-time data preprocessing\nimg_prep = ImagePreprocessing()\nimg_prep.add_featurewise_zero_center()\nimg_prep.add_featurewise_stdnorm()\n\n# Real-time data augmentation\nimg_aug = ImageAugmentation()\nimg_aug.add_random_flip_leftright()\nimg_aug.add_random_rotation(max_angle=25.)\n\n# Convolutional network building\n#-------------------------------------------------------------------------------------------\nnetwork = input_data(shape=[None, 32, 32, 3],\n                     data_preprocessing=img_prep,\n                     data_augmentation=img_aug)\nnetwork = conv_2d(network, 32, 3, activation=\'relu\')\nnetwork = max_pool_2d(network, 2)\nnetwork = conv_2d(network, 64, 3, activation=\'relu\')\nnetwork = conv_2d(network, 64, 3, activation=\'relu\')\nnetwork = max_pool_2d(network, 2)\nnetwork = fully_connected(network, 512, activation=\'relu\')\nnetwork = dropout(network, 0.5)\nnetwork = fully_connected(network, 10, activation=\'softmax\')\nnetwork = regression(network, optimizer=\'adam\',\n                     loss=\'categorical_crossentropy\',\n                     learning_rate=0.001)\n#-----------------------------------------------------------------------------------------\n# Train using classifier\nmodel = tflearn.DNN(network, tensorboard_verbose=0)\nmodel.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),\nshow_metric=True, batch_size=96, run_id=\'cifar10_cnn\')\nprint(""end..."")'"
aa4_TextCNN_with_RCNN/p72_TextCNN_with_RCNN_model.py,79,"b'# -*- coding: utf-8 -*-\n#TextCNN: 1. embeddding layers, 2.convolutional layer, 3.max-pooling, 4.softmax layer.\n# print(""started..."")\nimport tensorflow as tf\nimport numpy as np\nimport copy\n\nclass TextCNN_with_RCNN:\n    def __init__(self, filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,\n                 is_training,initializer=tf.random_normal_initializer(stddev=0.1),multi_label_flag=False):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.filter_sizes=filter_sizes # it is a list of int. e.g. [3,4,5]\n        self.num_filters=num_filters\n        self.initializer=initializer\n        self.num_filters_total=self.num_filters * len(filter_sizes) #how many filters totally.\n        self.multi_label_flag=multi_label_flag\n        self.hidden_size = embed_size\n        self.activation = tf.nn.tanh\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        self.input_y = tf.placeholder(tf.int32, [None,],name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32,[None,self.num_classes], name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights_cnn()\n        self.instantiate_weights_rcnn()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        if multi_label_flag:\n            print(""going to use multi label loss."")\n            self.loss_val = self.loss_multilabel()\n        else:\n            print(""going to use single label loss."")\n            self.loss_val = self.loss()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\n\n        if not self.multi_label_flag:\n            correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy =tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n        else:\n            self.accuracy = tf.constant(0.5) #fuke accuracy. (you can calcuate accuracy outside of graph using method calculate_accuracy(...) in train.py)\n\n    def instantiate_weights_cnn(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""projection_cnn""): # embedding matrix\n            #self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection_cnn = tf.get_variable(""W_projection_cnn"",shape=[self.num_filters_total, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection_cnn = tf.get_variable(""b_projection_cnn"",shape=[self.num_classes])       #[label_size]\n\n    def instantiate_weights_rcnn(self):\n        """"""define all weights here""""""\n        self.Embedding = tf.get_variable(""Embedding"", shape=[self.vocab_size, self.embed_size],\n                                         initializer=self.initializer)  # [vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n\n        with tf.name_scope(""weights_rcnn""): # embedding matrix\n            self.left_side_first_word= tf.get_variable(""left_side_first_word"",shape=[self.batch_size, self.embed_size],initializer=self.initializer)\n            self.right_side_last_word = tf.get_variable(""right_side_last_word"",shape=[self.batch_size, self.embed_size],initializer=self.initializer)\n            self.W_l=tf.get_variable(""W_l"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_r=tf.get_variable(""W_r"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_sl=tf.get_variable(""W_sl"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n            self.W_sr=tf.get_variable(""W_sr"",shape=[self.embed_size, self.embed_size],initializer=self.initializer)\n\n            self.W_projection_rcnn = tf.get_variable(""W_projection"",shape=[self.hidden_size*3, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection_rcnn = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n    def inference1(self):\n        """"""main computation graph here: 1.embedding-->2.average-->3.linear classifier""""""\n        # 1.=====>get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x)#[None,sentence_length,embed_size]\n        self.sentence_embeddings_expanded=tf.expand_dims(self.embedded_words,-1) #[None,sentence_length,embed_size,1). expand dimension so meet input requirement of 2d-conv\n\n        # 2.=====>loop each filter size. for each filter, do:convolution-pooling layer(a.create filters,b.conv,c.apply nolinearity,d.max-pooling)--->\n        # you can use:tf.nn.conv2d;tf.nn.relu;tf.nn.max_pool; feature shape is 4-d. feature is a new variable\n        pooled_outputs = []\n        for i,filter_size in enumerate(self.filter_sizes):\n            with tf.name_scope(""convolution-pooling-%s"" %filter_size):\n                # ====>a.create filter\n                filter=tf.get_variable(""filter-%s""%filter_size,[filter_size,self.embed_size,1,self.num_filters],initializer=self.initializer)\n                # ====>b.conv operation: conv2d===>computes a 2-D convolution given 4-D `input` and `filter` tensors.\n                #Conv.Input: given an input tensor of shape `[batch, in_height, in_width, in_channels]` and a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`\n                #Conv.Returns: A `Tensor`. Has the same type as `input`.\n                #         A 4-D tensor. The dimension order is determined by the value of `data_format`, see below for details.\n                #1)each filter with conv2d\'s output a shape:[1,sequence_length-filter_size+1,1,1];2)*num_filters--->[1,sequence_length-filter_size+1,1,num_filters];3)*batch_size--->[batch_size,sequence_length-filter_size+1,1,num_filters]\n                #input data format:NHWC:[batch, height, width, channels];output:4-D\n                conv=tf.nn.conv2d(self.sentence_embeddings_expanded, filter, strides=[1,1,1,1], padding=""VALID"",name=""conv"") #shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]\n                # ====>c. apply nolinearity\n                b=tf.get_variable(""b-%s""%filter_size,[self.num_filters])\n                h=tf.nn.relu(tf.nn.bias_add(conv,b),""relu"") #shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n                # ====>. max-pooling.  value: A 4-D `Tensor` with shape `[batch, height, width, channels]\n                #                  ksize: A list of ints that has length >= 4.  The size of the window for each dimension of the input tensor.\n                #                  strides: A list of ints that has length >= 4.  The stride of the sliding window for each dimension of the input tensor.\n                pooled=tf.nn.max_pool(h, ksize=[1,self.sequence_length-filter_size+1,1,1], strides=[1,1,1,1], padding=\'VALID\',name=""pool"")#shape:[batch_size, 1, 1, num_filters].max_pool:performs the max pooling on the input.\n                pooled_outputs.append(pooled)\n        # 3.=====>combine all pooled features, and flatten the feature.output\' shape is a [1,None]\n        #e.g. >>> x1=tf.ones([3,3]);x2=tf.ones([3,3]);x=[x1,x2]\n        #         x12_0=tf.concat(x,0)---->x12_0\' shape:[6,3]\n        #         x12_1=tf.concat(x,1)---->x12_1\' shape;[3,6]\n        self.h_pool=tf.concat(pooled_outputs,3) #shape:[batch_size, 1, 1, num_filters_total]. tf.concat=>concatenates tensors along one dimension.where num_filters_total=num_filters_1+num_filters_2+num_filters_3\n        self.h_pool_flat=tf.reshape(self.h_pool,[-1,self.num_filters_total]) #shape should be:[None,num_filters_total]. here this operation has some result as tf.sequeeze().e.g. x\'s shape:[3,3];tf.reshape(-1,x) & (3, 3)---->(1,9)\n\n        #4.=====>add dropout: use tf.nn.dropout\n        with tf.name_scope(""dropout_cnn""):\n            self.h_drop=tf.nn.dropout(self.h_pool_flat,keep_prob=self.dropout_keep_prob) #[None,num_filters_total]\n\n        #5. logits(use linear layer)and predictions(argmax)\n        with tf.name_scope(""output_cnn""):\n            logits = tf.matmul(self.h_drop,self.W_projection_cnn) + self.b_projection_cnn  #shape:[None, self.num_classes]==tf.matmul([None,self.embed_size],[self.embed_size,self.num_classes])\n        return logits\n    #########################################################################################################################################################\n    def get_context_left(self,context_left,embedding_previous):\n        """"""\n        :param context_left:[batch_size,self.embed_size]\n        :param embedding_previous:[batch_size,self.embed_size]\n        :return: output:[None,embed_size]\n        """"""\n        left_c=tf.matmul(context_left,self.W_l) #context_left:[batch_size,embed_size];W_l:[embed_size,embed_size]\n        left_e=tf.matmul(embedding_previous,self.W_sl)#embedding_previous;[batch_size,embed_size]\n        left_h=left_c+left_e\n        context_left=self.activation(left_h)\n        return context_left\n\n    def get_context_right(self,context_right,embedding_afterward):\n        """"""\n        :param context_right:[batch_size,self.embed_size]\n        :param embedding_afterward:[batch_size,self.embed_size]\n        :return: output:[batch_size,embed_size]\n        """"""\n        right_c=tf.matmul(context_right,self.W_r)\n        right_e=tf.matmul(embedding_afterward,self.W_sr)\n        right_h=right_c+right_e\n        context_right=self.activation(right_h)\n        return context_right\n\n    def conv_layer_with_recurrent_structure(self):\n        """"""\n        input:self.embedded_words:[None,sentence_length,embed_size]\n        :return: shape:[None,sentence_length,embed_size*3]\n        """"""\n        #1. get splitted list of word embeddings\n        embedded_words_split=tf.split(self.embedded_words,self.sequence_length,axis=1) #sentence_length\xe4\xb8\xaa[None,1,embed_size]\n        embedded_words_squeezed=[tf.squeeze(x,axis=1) for x in embedded_words_split]#sentence_length\xe4\xb8\xaa[None,embed_size]\n        embedding_previous=self.left_side_first_word\n        context_left_previous=tf.zeros((self.batch_size,self.embed_size))\n        #2. get list of context left\n        context_left_list=[]\n        for i,current_embedding_word in enumerate(embedded_words_squeezed):#sentence_length\xe4\xb8\xaa[None,embed_size]\n            context_left=self.get_context_left(context_left_previous, embedding_previous) #[None,embed_size]\n            context_left_list.append(context_left) #append result to list\n            embedding_previous=current_embedding_word #assign embedding_previous\n            context_left_previous=context_left #assign context_left_previous\n        #3. get context right\n        embedded_words_squeezed2=copy.copy(embedded_words_squeezed)\n        embedded_words_squeezed2.reverse()\n        embedding_afterward=self.right_side_last_word\n        context_right_afterward = tf.zeros((self.batch_size, self.embed_size))\n        context_right_list=[]\n        for j,current_embedding_word in enumerate(embedded_words_squeezed2):\n            context_right=self.get_context_right(context_right_afterward,embedding_afterward)\n            context_right_list.append(context_right)\n            embedding_afterward=current_embedding_word\n            context_right_afterward=context_right\n        #4.ensemble left,embedding,right to output\n        output_list=[]\n        for index,current_embedding_word in enumerate(embedded_words_squeezed):\n            representation=tf.concat([context_left_list[index],current_embedding_word,context_right_list[index]],axis=1)\n            #print(i,""representation:"",representation)\n            output_list.append(representation) #shape:sentence_length\xe4\xb8\xaa[None,embed_size*3]\n        #5. stack list to a tensor\n        #print(""output_list:"",output_list) #(3, 5, 8, 100)\n        output=tf.stack(output_list,axis=1) #shape:[None,sentence_length,embed_size*3]\n        #print(""output:"",output)\n        return output\n\n\n    def inference2(self):\n        """"""main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer, 3.max pooling, 4.FC layer 5.softmax """"""\n        #1.get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n        #2. Bi-lstm layer\n        output_conv=self.conv_layer_with_recurrent_structure() #shape:[None,sentence_length,embed_size*3]\n        #3. max pooling\n        #print(""output_conv:"",output_conv) #(3, 5, 8, 100)\n        output_pooling=tf.reduce_max(output_conv,axis=1) #shape:[None,embed_size*3]\n        #print(""output_pooling:"",output_pooling) #(3, 8, 100)\n        #4. logits(use linear layer)\n        with tf.name_scope(""dropout_rcnn""):\n            h_drop=tf.nn.dropout(output_pooling,keep_prob=self.dropout_keep_prob) #[None,embed_size*3]\n\n        #with tf.name_scope(""output""): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n            logits = tf.matmul(h_drop, self.W_projection_rcnn) + self.b_projection_rcnn  # [batch_size,num_classes]\n        return logits\n\n    def inference(self):\n        weight1=tf.get_variable(""weight1"", shape=(),initializer=self.initializer)\n        self.p_weight1=tf.nn.sigmoid(weight1)\n        self.p_weight2=1.0-self.p_weight1\n        logits1=self.inference1()\n        logits2=self.inference2()\n        logits=self.p_weight1*logits1+self.p_weight2*logits2\n        return logits\n\n    #########################################################################################################################################################\n    def loss(self,l2_lambda=0.0001):#0.001\n        with tf.name_scope(""loss""):\n            #input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss_multilabel(self,l2_lambda=0.00001): #0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            #input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            #losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"",losses) #shape=(?, 1999).\n            losses=tf.reduce_sum(losses,axis=1) #shape=(?,). loss for all data in the batch\n            loss=tf.reduce_mean(losses)         #shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=3\n    learning_rate=0.01\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.9\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1 #0.5\n    filter_sizes=[3,4,5]\n    num_filters=128\n    textRNN=TextCNN_with_RCNN(filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      for i in range(100):\n           input_x=np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n           input_x[input_x>0.5]=1\n           input_x[input_x <= 0.5] = 0\n           input_y=np.array([1,0,1,1,1,2,1,1])#np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n           loss,acc,predict,p_weight1_value,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,\n                                                        textRNN.p_weight1 ,textRNN.train_op],#textRNN.W_projection_rcnn\n                                feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n           print(""loss:"",loss,""acc:"",acc,""label:"",input_y,""prediction:"",predict)\n           print(""p_weight1_value:"",p_weight1_value)\n#test()'"
aa4_TextCNN_with_RCNN/p72_TextCNN_with_RCNN_train.py,27,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p72_TextCNN_with_RCNN_model import TextCNN_with_RCNN\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\n#tf.app.flags.DEFINE_integer(""num_sampled"",50,""number of noise sampling"") #100\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_cnn_with_rcnn_title_desc_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",40,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_integer(""num_filters"", 256, ""number of filters"") #256--->512\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\nfilter_sizes=[3,4,5,7,10,15,20,25] #[1,2,3,4,5,6,7] #[1,2,3]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn_with_rcnn"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""cnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn_with_rcnn"")\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/test-zhihu6-title-desc.txt\' #test-zhihu5-only-title-multilabel.txt\n        train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textCNN=TextCNN_with_RCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                        FLAGS.decay_rate,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textCNN,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textCNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {textCNN.input_x: trainX[start:end],textCNN.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[textCNN.input_y] = trainY[start:end]\n                else:\n                    feed_dict[textCNN.input_y_multilabel]=trainY[start:end]\n                curr_loss,curr_acc,_=sess.run([textCNN.loss_val,textCNN.accuracy,textCNN.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %50==0:\n                    print(""CNN&RCNN: Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textCNN.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,textCNN,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""CNN&RCNN: Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, textCNN, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textCNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textCNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[textCNN.input_y] = evalY[start:end]\n        else:\n            feed_dict[textCNN.input_y_multilabel] = evalY[start:end]\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textCNN.loss_val,textCNN.logits,textCNN.accuracy],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
aa5_BiLstmTextRelation/p9_BiLstmTextRelation_model.py,30,"b'# -*- coding: utf-8 -*-\n""""""\nBiLstmTextRelation: check reationship of two questions(Qi,Qj),result(0 or 1). 1 means related,0 means no relation\ninput_x e.g. ""how much is the computer? EOS price of laptop"".two different questions were splitted by a special token: EOS\nmain graph:1. embeddding layer, 2.Bi-LSTM layer, 3.mean pooling, 4.FC layer, 5.softmax\n""""""\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\nimport random\n\nclass BiLstmTextRelation:\n    def __init__(self,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,\n                 vocab_size,embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1)):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.hidden_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.initializer=initializer\n\n        # add placeholder (X,label)\n        #X:input_x e.g. ""how much is the computer? EOS price of laptop""\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X:  concat of two sentence, split by EOS.\n        self.input_y = tf.placeholder(tf.int32,[None], name=""input_y"")  # y [None,num_classes]\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        self.loss_val = self.loss()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")  # shape:[None,]\n        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.hidden_size*2, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n    def inference(self):\n        """"""main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer, 3.mean pooling, 4.FC layer, 5.softmax """"""\n        #1.get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n        #2. Bi-lstm layer\n        # define lstm cess:get lstm cell output\n        lstm_fw_cell=rnn.BasicLSTMCell(self.hidden_size) #forward direction cell\n        lstm_bw_cell=rnn.BasicLSTMCell(self.hidden_size) #backward direction cell\n        if self.dropout_keep_prob is not None:\n            lstm_fw_cell=rnn.DropoutWrapper(lstm_fw_cell,output_keep_prob=self.dropout_keep_prob)\n            lstm_bw_cell==rnn.DropoutWrapper(lstm_bw_cell,output_keep_prob=self.dropout_keep_prob)\n        # bidirectional_dynamic_rnn: input: [batch_size, max_time, input_size]\n        #                            output: A tuple (outputs, output_states)\n        #                                    where:outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output `Tensor`.\n        outputs,_=tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell,self.embedded_words,dtype=tf.float32) #[batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\n        print(""outputs:===>"",outputs) #outputs:(<tf.Tensor \'bidirectional_rnn/fw/fw/transpose:0\' shape=(?, 5, 100) dtype=float32>, <tf.Tensor \'ReverseV2:0\' shape=(?, 5, 100) dtype=float32>))\n        #3. concat output\n        output_rnn=tf.concat(outputs,axis=2) #[batch_size,sequence_length,hidden_size*2]\n        output_rnn_pooled=tf.reduce_mean(output_rnn,axis=1) #[batch_size,hidden_size*2] #output_rnn_last=output_rnn[:,-1,:] ##[batch_size,hidden_size*2] #TODO\n        print(""output_rnn_pooled:"", output_rnn_pooled) # <tf.Tensor \'strided_slice:0\' shape=(?, 200) dtype=float32>\n        #4. logits(use linear layer)\n        with tf.name_scope(""output""): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n            logits = tf.matmul(output_rnn_pooled, self.W_projection) + self.b_projection  # [batch_size,num_classes]\n        return logits\n\n    def loss(self,l2_lambda=0.0001):\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n\n# def test():\n#     #below is a function test; if you use this for text related task, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n#     #input_x e.g. ""how much is the computer? EOS price of laptop""--->""1 2 3 4 5 6 0 11 12 13 14""; label:1\n#     #input_x e.g. ""how much is the computer? EOS what\'s the weather today?""--->""1 2 3 4 5 6 0 15 16 17 18 19""; label:0\n#     num_classes=2\n#     learning_rate=0.001\n#     batch_size=1\n#     decay_steps=1000\n#     decay_rate=0.9\n#     sequence_length=3\n#     vocab_size=10000\n#     embed_size=100\n#     is_training=True\n#     dropout_keep_prob=1#0.5\n#     textRNN=BiLstmTextRelation(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n#     with tf.Session() as sess:\n#         sess.run(tf.global_variables_initializer())\n#         for i in range(10000):\n#             x1=random.choice([1,2,3,4,5,6,7,8,9])\n#             x2 = random.choice([1,2,3,4,5,6,7,8,9])\n#             input_x=np.array([[x1,0,x2]]) #np.zeros((batch_size,sequence_length)) #[None, self.sequence_length]\n#             label=0\n#             if np.abs(x1-x2)<3:label=1\n#             input_y=np.array([label]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n#             loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n#             if i%100==0:\n#                 print(i,""x1:"",x1,"";x2:"",x2,"";loss:"",loss,"";acc:"",acc,"";label:"",input_y,"";prediction:"",predict)\n# test()'"
aa5_BiLstmTextRelation/p9_BiLstmTextRelation_train.py,23,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p9_BiLstmTextRelation_model import BiLstmTextRelation\nfrom data_util_zhihu import load_data,create_voabulary #,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1024, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""biLstm_text_relation_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",20,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\ntf.app.flags.DEFINE_string(""traning_data_path"",""/home/xul/xul/9_ZhihuCup/test_twoCNN_zhihu.txt"",""path of traning data."") #train-zhihu4-only-title-all.txt===>training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec.bin-100"",""word2vec\'s vocabulary and vectors"")\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        #1.  get vocabulary of X and label.\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""biLstmTextRelation"")\n        vocab_size = len(vocabulary_word2index)\n        print(""rnn_model.vocab_size:"",vocab_size)\n        #vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""biLstmTextRelation"")\n        vocabulary_word2index_label={\'1\':1,\'0\':0}\n        vocabulary_index2word_label={0:\'0\',1:\'1\'}\n        train, test, _= load_data(vocabulary_word2index, vocabulary_word2index_label,valid_portion=0.005,training_data_path=FLAGS.traning_data_path)\n        #train, test, _ =  load_data_multilabel_new_twoCNN(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=False,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        #train, test, _ =  load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=False,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        ###############################################################################################\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        ###############################################################################################\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        biLstmTR = BiLstmTextRelation(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                                     FLAGS.decay_rate, FLAGS.sequence_length,vocab_size, FLAGS.embed_size, FLAGS.is_training)\n\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint for rnn model."")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, biLstmTR,\n                                                 word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(biLstmTR.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                curr_loss,curr_acc,_=sess.run([biLstmTR.loss_val,biLstmTR.accuracy,biLstmTR.train_op],\n                                              feed_dict={biLstmTR.input_x:trainX[start:end],biLstmTR.input_y:trainY[start:end]\n                                              ,biLstmTR.dropout_keep_prob:1.0}) #curr_acc--->TextCNN.accuracy -->,textRNN.dropout_keep_prob:1\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %500==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(biLstmTR.epoch_increment)\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,biLstmTR,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                if not os.path.exists(FLAGS.ckpt_dir):\n                    os.mkdir(FLAGS.ckpt_dir)\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, biLstmTR, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textRNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textRNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,biLstmTR,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        curr_eval_loss, logits,curr_eval_acc= sess.run([biLstmTR.loss_val,biLstmTR.logits,biLstmTR.accuracy],#curr_eval_acc--->textCNN.accuracy\n                                          feed_dict={biLstmTR.input_x: evalX[start:end],biLstmTR.input_y: evalY[start:end],\n                                                    biLstmTR.dropout_keep_prob:1})\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
aa6_TwoCNNTextRelation/p9_twoCNNTextRelation_model.py,53,"b'# -*- coding: utf-8 -*-\n#TextCNN: for each of two sentences,do(1. embeddding layers, 2.convolutional layer, 3.max-pooling,),4. concat features,5.FC, 6.softmax layer.\n# print(""started..."")\nimport tensorflow as tf\nimport numpy as np\nimport random\nfrom tflearn.data_utils import pad_sequences #to_categorical\n\nclass TwoCNNTextRelation:\n    def __init__(self, filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,\n                 embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1)):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n        self.filter_sizes=filter_sizes # it is a list of int. e.g. [3,4,5]\n        self.num_filters=num_filters\n        self.initializer=initializer\n        self.num_filters_total=self.num_filters * len(filter_sizes) #how many filters totally.\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X: first sentence\n        self.input_x2 = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x2"")  # X: second sentence\n        self.input_y = tf.placeholder(tf.int32, [None,],name=""input_y"")  # y: 0 or 1. 1 means two sentences related to each other;0 means no relation.\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        if not is_training:\n            return\n        self.loss_val = self.loss()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\n\n        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.num_filters_total*2, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size]\n\n    def inference(self):\n        """"""main computation graph here: 1. embeddding layers, 2.convolutional layer, 3.max-pooling, 4.softmax layer.""""""\n        # 1.=====>get emebedding of words in the sentence\n        self.embedded_words1 = tf.nn.embedding_lookup(self.Embedding,self.input_x)#[None,sentence_length,embed_size]\n        self.sentence_embeddings_expanded1=tf.expand_dims(self.embedded_words1,-1) #[None,sentence_length,embed_size,1). expand dimension so meet input requirement of 2d-conv\n        self.embedded_words2 = tf.nn.embedding_lookup(self.Embedding,self.input_x2)#[None,sentence_length,embed_size]\n        self.sentence_embeddings_expanded2=tf.expand_dims(self.embedded_words2,-1) #[None,sentence_length,embed_size,1). expand dimension so meet input requirement of 2d-conv\n        #2.1 get features of sentence1\n        h1=self.conv_relu_pool_dropout(self.sentence_embeddings_expanded1,name_scope_prefix=""s1"") #[None,num_filters_total]\n        #2.2 get features of sentence2\n        h2 =self.conv_relu_pool_dropout(self.sentence_embeddings_expanded2,name_scope_prefix=""s2"")  # [None,num_filters_total]\n        #3. concat features\n        h=tf.concat([h1,h2],axis=1) #[None,num_filters_total*2]\n        #4. logits(use linear layer)and predictions(argmax)\n        with tf.name_scope(""output""):\n            logits = tf.matmul(h,self.W_projection) + self.b_projection  #shape:[None, self.num_classes]==tf.matmul([None,self.num_filters_total*2],[self.num_filters_total*2,self.num_classes])\n        return logits\n\n    def conv_relu_pool_dropout(self,sentence_embeddings_expanded, name_scope_prefix=None):\n        # 1.loop each filter size. for each filter, do:convolution-pooling layer(a.create filters,b.conv,c.apply nolinearity,d.max-pooling)--->\n        # you can use:tf.nn.conv2d;tf.nn.relu;tf.nn.max_pool; feature shape is 4-d. feature is a new variable\n        pooled_outputs = []\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.name_scope(name_scope_prefix + ""convolution-pooling-%s"" % filter_size):\n                # ====>a.create filter\n                filter = tf.get_variable(name_scope_prefix+""filter-%s"" % filter_size, [filter_size, self.embed_size, 1, self.num_filters],\n                                         initializer=self.initializer)\n                # ====>b.conv operation: conv2d===>computes a 2-D convolution given 4-D `input` and `filter` tensors.\n                # Conv.Input: given an input tensor of shape `[batch, in_height, in_width, in_channels]` and a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`\n                # Conv.Returns: A `Tensor`. Has the same type as `input`.\n                #         A 4-D tensor. The dimension order is determined by the value of `data_format`, see below for details.\n                # 1)each filter with conv2d\'s output a shape:[1,sequence_length-filter_size+1,1,1];2)*num_filters--->[1,sequence_length-filter_size+1,1,num_filters];3)*batch_size--->[batch_size,sequence_length-filter_size+1,1,num_filters]\n                # input data format:NHWC:[batch, height, width, channels];output:4-D\n                conv = tf.nn.conv2d(sentence_embeddings_expanded, filter, strides=[1, 1, 1, 1], padding=""VALID"",\n                                    name=""conv"")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]\n                # ====>c. apply nolinearity\n                b = tf.get_variable(name_scope_prefix+""b-%s"" % filter_size, [self.num_filters])\n                h = tf.nn.relu(tf.nn.bias_add(conv, b),\n                               ""relu"")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n                # ====>. max-pooling.  value: A 4-D `Tensor` with shape `[batch, height, width, channels]\n                #                  ksize: A list of ints that has length >= 4.  The size of the window for each dimension of the input tensor.\n                #                  strides: A list of ints that has length >= 4.  The stride of the sliding window for each dimension of the input tensor.\n                pooled = tf.nn.max_pool(h, ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding=\'VALID\',\n                                        name=""pool"")  # shape:[batch_size, 1, 1, num_filters].max_pool:performs the max pooling on the input.\n                pooled_outputs.append(pooled)\n        # 2.=====>combine all pooled features, and flatten the feature.output\' shape is a [1,None]\n        # e.g. >>> x1=tf.ones([3,3]);x2=tf.ones([3,3]);x=[x1,x2]\n        #         x12_0=tf.concat(x,0)---->x12_0\' shape:[6,3]\n        #         x12_1=tf.concat(x,1)---->x12_1\' shape;[3,6]\n        h_pool = tf.concat(pooled_outputs,\n                           3)  # shape:[batch_size, 1, 1, num_filters_total]. tf.concat=>concatenates tensors along one dimension.where num_filters_total=num_filters_1+num_filters_2+num_filters_3\n        h_pool_flat = tf.reshape(h_pool, [-1,\n                                          self.num_filters_total])  # shape should be:[None,num_filters_total]. here this operation has some result as tf.sequeeze().e.g. x\'s shape:[3,3];tf.reshape(-1,x) & (3, 3)---->(1,9)\n        # 3.=====>add dropout: use tf.nn.dropout\n        with tf.name_scope(""dropout""):\n            h_drop = tf.nn.dropout(h_pool_flat, keep_prob=self.dropout_keep_prob)  # [None,num_filters_total]\n        return h_drop\n\n    def loss(self,l2_lambda=0.0001):#0.001\n        with tf.name_scope(""loss""):\n            #input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss_multilabel(self,l2_lambda=0.001): #this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            #input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"",losses) #shape=(?, 1999)\n            losses=tf.reduce_sum(losses,axis=1) #shape=(?,)\n            loss=tf.reduce_mean(losses)\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\n# def test():\n#     #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n#     filter_sizes=[3,4,5]\n#     num_filters=128\n#     num_classes=2\n#     learning_rate=0.001\n#     batch_size=1\n#     decay_steps=1000\n#     decay_rate=0.9\n#     sequence_length=15\n#     vocab_size=100\n#     embed_size=100\n#     is_training=True\n#     dropout_keep_prob=1\n#     twoCNNTR=TwoCNNTextRelation(filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,\n#                                           sequence_length,vocab_size,embed_size,is_training)\n#     with tf.Session() as sess:\n#         sess.run(tf.global_variables_initializer())\n#         for i in range(10000):\n#             x1=random.choice([1,2,3,4,5,6,7,8,9])\n#             x2 = random.choice([1,2,3,4,5,6,7,8,9])\n#             #input_x=np.array([[x1,x1,x1,0,x2,x2,x2]])\n#             label=0\n#             if np.abs(x1-x2)<3:label=1\n#             input_y=np.array([label]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n#             x1 = pad_sequences([[x1,x1,x1,x1,x1]], maxlen=sequence_length, value=0)\n#             x2 = pad_sequences([[x2,x2,x2,x2,x2]], maxlen=sequence_length, value=0)\n#             loss_sum=0.0\n#             acc_sum=0.0\n#             loss,acc,predict,_=sess.run([twoCNNTR.loss_val,twoCNNTR.accuracy,twoCNNTR.predictions,twoCNNTR.train_op],\n#                                         feed_dict={twoCNNTR.input_x:x1,twoCNNTR.input_x2:x2,twoCNNTR.input_y:input_y,\n#                                                    twoCNNTR.dropout_keep_prob:dropout_keep_prob})\n#             loss_sum=loss_sum+loss\n#             acc_sum=acc_sum+acc\n#             if i%100==0:\n#                 print(i,""x1:"",x1,"";x2:"",x2,""loss:"",loss,""acc:"",acc,""avg loss:"",loss_sum/(i+1),"";avg acc:"",acc_sum/(i+1),""label:"",input_y,""prediction:"",predict)\n# test()'"
aa6_TwoCNNTextRelation/p9_twoCNNTextRelation_train.py,24,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p9_twoCNNTextRelation_model import TwoCNNTextRelation\nfrom data_util_zhihu import load_data_multilabel_new_twoCNN,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1024, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_twoCNN_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sequence_length"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",20,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\ntf.app.flags.DEFINE_string(""traning_data_path"",""/home/xul/xul/9_ZhihuCup/test_twoCNN_zhihu.txt"",""path of traning data."") #TODO #train-zhihu4-only-title-all.txt===>training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec.bin-100"",""word2vec\'s vocabulary and vectors"")\ntf.app.flags.DEFINE_integer(""num_filters"", 128, ""number of filters"") #128\n\nfilter_sizes=[1,2,3,4,5]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        #1.  get vocabulary of X and label.\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""twoRNN"")\n        vocab_size = len(vocabulary_word2index)\n        print(""twoCNN_model.vocab_size:"",vocab_size)\n        #vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""twoRNN"")\n        vocabulary_word2index_label={\'1\':1,\'0\':0}\n        vocabulary_index2word_label={0:\'0\',1:\'1\'}\n        train, test, _ =  load_data_multilabel_new_twoCNN(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=False,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX,trainX2,trainY = train\n        testX, testX2,testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        trainX2 = pad_sequences(trainX2, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n        testX2 = pad_sequences(testX2, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\n\n        ###############################################################################################\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        ###############################################################################################\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        twoCNNTR = TwoCNNTextRelation(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                                      FLAGS.decay_rate,FLAGS.sequence_length, vocab_size, FLAGS.embed_size, FLAGS.is_training)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint for two CNN model."")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, twoCNNTR,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(twoCNNTR.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                curr_loss,curr_acc,_=sess.run([twoCNNTR.loss_val,twoCNNTR.accuracy,twoCNNTR.train_op],\n                                              feed_dict={twoCNNTR.input_x:trainX[start:end],twoCNNTR.input_x2:trainX2[start:end],\n                                                         twoCNNTR.input_y:trainY[start:end],twoCNNTR.dropout_keep_prob:0.5}) #curr_acc--->TextCNN.accuracy -->,textRNN.dropout_keep_prob:1\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %500==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(twoCNNTR.epoch_increment)\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,twoCNNTR,testX,testX2,testY,batch_size,vocabulary_index2word_label)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, twoCNNTR, testX,testX2,testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textRNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textRNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,twoCNNTR,evalX,evalX2,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        curr_eval_loss, logits,curr_eval_acc= sess.run([twoCNNTR.loss_val,twoCNNTR.logits,twoCNNTR.accuracy],#curr_eval_acc--->textCNN.accuracy\n                                          feed_dict={twoCNNTR.input_x: evalX[start:end],twoCNNTR.input_x2: evalX2[start:end],\n                                                     twoCNNTR.input_y: evalY[start:end],twoCNNTR.dropout_keep_prob:1})\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
data/__init__.py,0,b''
a00_Bert/unused/run_classifier_multi_labels_bert.py,117,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BERT finetuning runner.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport os\nfrom . import bert_modeling as modeling\nfrom . import optimization\nfrom . import tokenization\nimport tensorflow as tf\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\n    ""data_dir"", None,\n    ""The input data dir. Should contain the .tsv files (or other data files) ""\n    ""for the task."")\n\nflags.DEFINE_string(\n    ""bert_config_file"", None,\n    ""The config json file corresponding to the pre-trained BERT model. ""\n    ""This specifies the model architecture."")\n\nflags.DEFINE_string(""task_name"", None, ""The name of the task to train."")\n\nflags.DEFINE_string(""vocab_file"", None,\n                    ""The vocabulary file that the BERT model was trained on."")\n\nflags.DEFINE_string(\n    ""output_dir"", None,\n    ""The output directory where the model checkpoints will be written."")\n\n## Other parameters\n\nflags.DEFINE_string(\n    ""init_checkpoint"", None,\n    ""Initial checkpoint (usually from a pre-trained BERT model)."")\n\nflags.DEFINE_bool(\n    ""do_lower_case"", True,\n    ""Whether to lower case the input text. Should be True for uncased ""\n    ""models and False for cased models."")\n\nflags.DEFINE_integer(\n    ""max_seq_length"", 128,\n    ""The maximum total input sequence length after WordPiece tokenization. ""\n    ""Sequences longer than this will be truncated, and sequences shorter ""\n    ""than this will be padded."")\n\nflags.DEFINE_bool(""do_train"", False, ""Whether to run training."")\n\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\n\nflags.DEFINE_bool(\n    ""do_predict"", False,\n    ""Whether to run the model in inference mode on the test set."")\n\nflags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")\n\nflags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")\n\nflags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")\n\nflags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")\n\nflags.DEFINE_float(""num_train_epochs"", 3.0,\n                   ""Total number of training epochs to perform."")\n\nflags.DEFINE_float(\n    ""warmup_proportion"", 0.1,\n    ""Proportion of training to perform linear learning rate warmup for. ""\n    ""E.g., 0.1 = 10% of training."")\n\nflags.DEFINE_integer(""save_checkpoints_steps"", 1000,\n                     ""How often to save the model checkpoint."")\n\nflags.DEFINE_integer(""iterations_per_loop"", 1000,\n                     ""How many steps to make in each estimator call."")\n\nflags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")\n\ntf.flags.DEFINE_string(\n    ""tpu_name"", None,\n    ""The Cloud TPU to use for training. This should be either the name ""\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""\n    ""url."")\n\ntf.flags.DEFINE_string(\n    ""tpu_zone"", None,\n    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""\n    ""specified, we will attempt to automatically detect the GCE project from ""\n    ""metadata."")\n\ntf.flags.DEFINE_string(\n    ""gcp_project"", None,\n    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""\n    ""specified, we will attempt to automatically detect the GCE project from ""\n    ""metadata."")\n\ntf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")\n\nflags.DEFINE_integer(\n    ""num_tpu_cores"",    8,\n    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")\n\n\n# task specific parameter( sentiment analysis)\nflags.DEFINE_integer(""num_classes"", 80, ""Total number of labels for sentiment analysis"")\nflags.DEFINE_integer(""num_aspects"", 20, ""Total number of aspect"")\n\nflags.DEFINE_list(""aspect_value_list"", [-2,-1,0,1], ""Values that a aspect can have"")\n\n\n\nclass InputExample(object):\n  """"""A single training/test example for simple sequence classification.""""""\n\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    """"""Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    """"""\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass InputFeatures(object):\n  """"""A single set of features of data.""""""\n\n  def __init__(self, input_ids, input_mask, segment_ids, label_id):\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.segment_ids = segment_ids\n    self.label_id = label_id\n\n\nclass DataProcessor(object):\n  """"""Base class for data converters for sequence classification data sets.""""""\n\n  def get_train_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for the train set.""""""\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for the dev set.""""""\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for prediction.""""""\n    raise NotImplementedError()\n\n  def get_labels(self):\n    """"""Gets the list of labels for this data set.""""""\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    """"""Reads a tab separated value file.""""""\n    with tf.gfile.Open(input_file, ""r"") as f:\n      reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\n\nclass XnliProcessor(DataProcessor):\n  """"""Processor for the XNLI data set.""""""\n\n  def __init__(self):\n    self.language = ""zh""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    lines = self._read_tsv(\n        os.path.join(data_dir, ""multinli"",\n                     ""multinli.train.%s.tsv"" % self.language))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""train-%d"" % (i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[1])\n      label = tokenization.convert_to_unicode(line[2])\n      if label == tokenization.convert_to_unicode(""contradictory""):\n        label = tokenization.convert_to_unicode(""contradiction"")\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    lines = self._read_tsv(os.path.join(data_dir, ""xnli.dev.tsv""))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""dev-%d"" % (i)\n      language = tokenization.convert_to_unicode(line[0])\n      if language != tokenization.convert_to_unicode(self.language):\n        continue\n      text_a = tokenization.convert_to_unicode(line[6])\n      text_b = tokenization.convert_to_unicode(line[7])\n      label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""contradiction"", ""entailment"", ""neutral""]\n\n\nclass MnliProcessor(DataProcessor):\n  """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n        ""dev_matched"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""contradiction"", ""entailment"", ""neutral""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, tokenization.convert_to_unicode(line[0]))\n      text_a = tokenization.convert_to_unicode(line[8])\n      text_b = tokenization.convert_to_unicode(line[9])\n      if set_type == ""test"":\n        label = ""contradiction""\n      else:\n        label = tokenization.convert_to_unicode(line[-1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass MrpcProcessor(DataProcessor):\n  """"""Processor for the MRPC data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""0"", ""1""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[3])\n      text_b = tokenization.convert_to_unicode(line[4])\n      if set_type == ""test"":\n        label = ""0""\n      else:\n        label = tokenization.convert_to_unicode(line[0])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass ColaProcessor(DataProcessor):\n  """"""Processor for the CoLA data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""0"", ""1""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      # Only the test set has a header\n      if set_type == ""test"" and i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, i)\n      if set_type == ""test"":\n        text_a = tokenization.convert_to_unicode(line[1])\n        label = ""0""\n      else:\n        text_a = tokenization.convert_to_unicode(line[3])\n        label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\nclass SentimentAnalysisFineGrainProcessor(DataProcessor):\n  """"""Processor for the CoLA data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    label_list=[]\n    #num_aspect=FLAGS.num_aspects\n    aspect_value_list=FLAGS.aspect_value_list #[-2,-1,0,1]\n    for i in range(20):\n        for value in aspect_value_list:\n            label_list.append(str(i) + ""_"" + str(value))\n    return label_list #[ {\'0_-2\': 0, \'0_-1\': 1, \'0_0\': 2, \'0_1\': 3,....\'19_-2\': 76, \'19_-1\': 77, \'19_0\': 78, \'19_1\': 79}]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      # Only the test set has a header\n      if set_type == ""test"" and i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, i)\n      #if set_type == ""test"":\n      #  text_a = tokenization.convert_to_unicode(line[1])\n      #  label = ""0""\n      #else:\n      #  text_a = tokenization.convert_to_unicode(line[3])\n      #  label = tokenization.convert_to_unicode(line[1])\n      label = tokenization.convert_to_unicode(line[0])\n      text_a = tokenization.convert_to_unicode(line[1])\n\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\nclass SentencePairClassificationProcessor(DataProcessor):\n  """"""Processor for the internal data set. sentence pair classification""""""\n  def __init__(self):\n    self.language = ""zh""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""0"", ""1""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, i)\n      label = tokenization.convert_to_unicode(line[0])\n      text_a = tokenization.convert_to_unicode(line[1])\n      text_b = tokenization.convert_to_unicode(line[2])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\ndef convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""\n  label_map = {}\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with ""- 3""\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with ""- 2""\n    if len(tokens_a) > max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where ""type_ids"" are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as as the ""sentence vector"". Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(""[CLS]"")\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(""[SEP]"")\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(""[SEP]"")\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) < max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n  #print(""label_map:"",label_map,"";length of label_map:"",len(label_map))\n  label_id=None\n  if "","" in example.label: # multiple label\n      # get list of label\n      label_id_list=[]\n      label_list=example.label.split("","")\n      for label_ in label_list:\n          label_id_list.append(label_map[label_])\n      #print(""label_id_list:"",label_id_list)\n      # convert to multi-hot style\n      label_id=[0 for l in range(len(label_map))]\n      for j, label_index in enumerate(label_id_list):\n          label_id[label_index]=1\n  else: # single label\n      label_id = label_map[example.label]\n  if ex_index < 5:\n    tf.logging.info(""*** Example ***"")\n    tf.logging.info(""guid: %s"" % (example.guid))\n    tf.logging.info(""tokens: %s"" % "" "".join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n    if "","" in example.label: tf.logging.info(""label: %s (id_list = %s)"" % (str(example.label), str(label_id_list))) # if label_id is a list, try print multi-hot value: label_id_list\n    tf.logging.info(""label: %s (id = %s)"" % (str(example.label), str(label_id))) # %d\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id)\n  return feature\n\n\ndef file_based_convert_examples_to_features(\n    examples, label_list, max_seq_length, tokenizer, output_file):\n  """"""Convert a set of `InputExample`s to a TFRecord file.""""""\n\n  writer = tf.python_io.TFRecordWriter(output_file)\n\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    def create_int_feature(values):\n      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n      return f\n\n    features = collections.OrderedDict()\n    features[""input_ids""] = create_int_feature(feature.input_ids)\n    features[""input_mask""] = create_int_feature(feature.input_mask)\n    features[""segment_ids""] = create_int_feature(feature.segment_ids)\n\n    # if feature.label_id is already a list, then no need to add [].\n    if isinstance(feature.label_id, list):\n        label_ids=feature.label_id\n    else:\n        label_ids = [feature.label_id]\n    features[""label_ids""] = create_int_feature(label_ids)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n    writer.write(tf_example.SerializeToString())\n\n\ndef file_based_input_fn_builder(input_file, seq_length, is_training,\n                                drop_remainder):\n  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n  # task specific parameter\n  name_to_features = {\n      ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n      ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),\n      ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n      ""label_ids"": tf.FixedLenFeature([FLAGS.num_classes], tf.int64), # ADD TO A FIXED LENGTH\n  }\n\n  def _decode_record(record, name_to_features):\n    """"""Decodes a record to a TensorFlow example.""""""\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    """"""The actual input function.""""""\n    batch_size = params[""batch_size""]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn\'t matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n\ndef create_model_original(bert_config, is_training, input_ids, input_mask, segment_ids,\n                 labels, num_labels, use_one_hot_embeddings):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  # In the demo, we are doing a simple classification task on the entire\n  # segment.\n  #\n  # If you want to use the token-level output, use model.get_sequence_output()\n  # instead.\n  output_layer = model.get_pooled_output() # \xe4\xbb\x8e\xe4\xb8\xbb\xe5\xb9\xb2\xe6\xa8\xa1\xe5\x9e\x8b\xe8\x8e\xb7\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n\n  hidden_size = output_layer.shape[-1].value\n\n  output_weights = tf.get_variable( # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x89\xb9\xe6\x9c\x89\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n      ""output_weights"", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable( # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x89\xb9\xe6\x9c\x89\xe7\x9a\x84bias\n      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True) # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x89\xb9\xe6\x9c\x89\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1) # \xe5\x88\xa9\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe5\xb0\xb1\xe5\x92\x8c\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n                 labels, num_labels, use_one_hot_embeddings):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  # In the demo, we are doing a simple classification task on the entire\n  # segment.\n  #\n  # If you want to use the token-level output, use model.get_sequence_output()\n  # instead.\n  output_layer = model.get_pooled_output() # \xe4\xbb\x8e\xe4\xb8\xbb\xe5\xb9\xb2\xe6\xa8\xa1\xe5\x9e\x8b\xe8\x8e\xb7\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n\n  hidden_size = output_layer.shape[-1].value\n\n  output_weights = tf.get_variable( # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x89\xb9\xe6\x9c\x89\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n      ""output_weights"", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable( # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x89\xb9\xe6\x9c\x89\xe7\x9a\x84bias\n      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True) # \xe5\x88\x86\xe7\xb1\xbb\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x89\xb9\xe6\x9c\x89\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    #print(""labels:"",labels,"";logits:"",logits,""isinstance(labels,list):"",isinstance(labels,list))\n    # mulit-label classification: 1.multi-hot==> then use sigmoid to transform it to possibility\n    probabilities=tf.nn.sigmoid(logits)\n    #log_probs=tf.log(probabilities)\n    labels=tf.cast(labels,tf.float32)\n    #  below is for single label classification\n    #  one-hot for single label classification\n    #  probabilities = tf.nn.softmax(logits, axis=-1)\n    #log_probs = tf.nn.log_softmax(logits, axis=-1)\n    #  one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    print(""num_labels:"",num_labels,"";logits:"",logits,"";labels:"",labels)\n    #print(""log_probs:"",log_probs)\n    #per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1) # \xe5\x88\xa9\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe5\xb0\xb1\xe5\x92\x8c\n    per_example_loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  """"""Returns `model_fn` closure for TPUEstimator.""""""\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    """"""The `model_fn` for TPUEstimator.""""""\n\n    tf.logging.info(""*** Features ***"")\n    for name in sorted(features.keys()):\n      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))\n\n    input_ids = features[""input_ids""]\n    input_mask = features[""input_mask""]\n    segment_ids = features[""segment_ids""]\n    label_ids = features[""label_ids""]\n\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n    (total_loss, per_example_loss, logits, probabilities) = create_model(\n        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n        num_labels, use_one_hot_embeddings)\n\n    tvars = tf.trainable_variables()\n\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(""**** Trainable Variables ****"")\n    for var in tvars:\n      init_string = """"\n      if var.name in initialized_variable_names:\n        init_string = "", *INIT_FROM_CKPT*""\n      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      logging_hook = tf.train.LoggingTensorHook({""loss"": total_loss}, every_n_iter=300)\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          training_hooks=[logging_hook],\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(per_example_loss, label_ids, logits):\n        #predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n        logits_split=tf.split(logits,FLAGS.num_aspects,axis=-1) # a list. length is num_aspects\n        label_ids_split=tf.split(logits,FLAGS.num_aspects,axis=-1) # a list. length is num_aspects\n        accuracy=tf.constant(0.0,dtype=tf.float64)\n        for j,logits in enumerate(logits_split): #\n            # accuracy = tf.metrics.accuracy(label_ids, predictions)\n            predictions=tf.argmax(logits, axis=-1, output_type=tf.int32) # should be [batch_size,]\n            label_id_=tf.cast(tf.argmax(label_ids_split[j],axis=-1),dtype=tf.int32)\n            print(""label_ids_split_sub:"",label_ids_split[j],"";predictions:"",predictions,"";label_id_:"",label_id_)\n            current_accuracy,update_op_accuracy=tf.metrics.accuracy(label_id_,predictions)\n            accuracy+=tf.cast(current_accuracy,dtype=tf.float64)\n        accuracy=accuracy/tf.constant(FLAGS.num_aspects,dtype=tf.float64)\n        loss = tf.metrics.mean(per_example_loss)\n        return {\n            ""eval_accuracy"": (accuracy,update_op_accuracy),\n            ""eval_loss"": loss,\n        }\n\n      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode, predictions=probabilities, scaffold_fn=scaffold_fn)\n    return output_spec\n\n  return model_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef input_fn_builder(features, seq_length, is_training, drop_remainder):\n  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    """"""The actual input function.""""""\n    batch_size = params[""batch_size""]\n\n    num_examples = len(features)\n\n    # This is for demo purposes and does NOT scale to large data sets. We do\n    # not use Dataset.from_generator() because that uses tf.py_func which is\n    # not TPU compatible. The right way to load data is with TFRecordReader.\n    d = tf.data.Dataset.from_tensor_slices({\n        ""input_ids"":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        ""input_mask"":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        ""segment_ids"":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        ""label_ids"":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer):\n  """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  processors = {\n      ""cola"": ColaProcessor,\n      ""mnli"": MnliProcessor,\n      ""mrpc"": MrpcProcessor,\n      ""xnli"": XnliProcessor,\n      ""sentence_pair"":SentencePairClassificationProcessor,\n      ""sentiment_analysis"":SentimentAnalysisFineGrainProcessor,\n  }\n\n  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n    raise ValueError(\n        ""At least one of `do_train`, `do_eval` or `do_predict\' must be True."")\n\n  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\n  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n    raise ValueError(\n        ""Cannot use sequence length %d because the BERT model ""\n        ""was only trained up to sequence length %d"" %\n        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  task_name = FLAGS.task_name.lower()\n\n  if task_name not in processors:\n    raise ValueError(""Task not found: %s"" % (task_name))\n\n  processor = processors[task_name]()\n\n  label_list = processor.get_labels()\n\n  tokenizer = tokenization.FullTokenizer(\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:\n    train_examples = processor.get_train_examples(FLAGS.data_dir)\n    num_train_steps = int(\n        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n  model_fn = model_fn_builder(\n      bert_config=bert_config,\n      num_labels=len(label_list),\n      init_checkpoint=FLAGS.init_checkpoint,\n      learning_rate=FLAGS.learning_rate,\n      num_train_steps=num_train_steps,\n      num_warmup_steps=num_warmup_steps,\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_tpu)\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      predict_batch_size=FLAGS.predict_batch_size)\n\n  if FLAGS.do_train:\n    train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")\n    file_based_convert_examples_to_features(\n        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n    tf.logging.info(""***** Running training *****"")\n    tf.logging.info(""  Num examples = %d"", len(train_examples))\n    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)\n    tf.logging.info(""  Num steps = %d"", num_train_steps)\n    train_input_fn = file_based_input_fn_builder(\n        input_file=train_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n\n  if FLAGS.do_eval:\n    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n    eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")\n    file_based_convert_examples_to_features(\n        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n\n    tf.logging.info(""***** Running evaluation *****"")\n    tf.logging.info(""  Num examples = %d"", len(eval_examples))\n    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)\n\n    # This tells the estimator to run through the entire set.\n    eval_steps = None\n    # However, if running eval on the TPU, you will need to specify the\n    # number of steps.\n    if FLAGS.use_tpu:\n      # Eval will be slightly WRONG on the TPU because it will truncate\n      # the last batch.\n      eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\n\n    eval_drop_remainder = True if FLAGS.use_tpu else False\n    eval_input_fn = file_based_input_fn_builder(\n        input_file=eval_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=eval_drop_remainder)\n\n    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n\n    output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")\n    with tf.gfile.GFile(output_eval_file, ""w"") as writer:\n      tf.logging.info(""***** Eval results *****"")\n      for key in sorted(result.keys()):\n        tf.logging.info(""  %s = %s"", key, str(result[key]))\n        writer.write(""%s = %s\\n"" % (key, str(result[key])))\n\n  if FLAGS.do_predict:\n    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n    predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")\n    file_based_convert_examples_to_features(predict_examples, label_list,\n                                            FLAGS.max_seq_length, tokenizer,\n                                            predict_file)\n\n    tf.logging.info(""***** Running prediction*****"")\n    tf.logging.info(""  Num examples = %d"", len(predict_examples))\n    tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)\n\n    if FLAGS.use_tpu:\n      # Warning: According to tpu_estimator.py Prediction on TPU is an\n      # experimental feature and hence not supported here\n      raise ValueError(""Prediction in TPU not supported"")\n\n    predict_drop_remainder = True if FLAGS.use_tpu else False\n    predict_input_fn = file_based_input_fn_builder(\n        input_file=predict_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=predict_drop_remainder)\n\n    result = estimator.predict(input_fn=predict_input_fn)\n\n    output_predict_file = os.path.join(FLAGS.output_dir, ""test_results.tsv"")\n    with tf.gfile.GFile(output_predict_file, ""w"") as writer:\n      tf.logging.info(""***** Predict results *****"")\n      for prediction in result:\n        output_line = ""\\t"".join(\n            str(class_probability) for class_probability in prediction) + ""\\n""\n        writer.write(output_line)\n\n\nif __name__ == ""__main__"":\n  flags.mark_flag_as_required(""data_dir"")\n  flags.mark_flag_as_required(""task_name"")\n  flags.mark_flag_as_required(""vocab_file"")\n  flags.mark_flag_as_required(""bert_config_file"")\n  flags.mark_flag_as_required(""output_dir"")\n  tf.app.run()\n'"
a00_Bert/unused/train_bert_multi-label_old.py,36,"b'# coding=utf-8\n""""""\ntrain bert model\n\n1.get training data and vocabulary & labels dict\n2. create model\n3. train the model and report f1 score\n""""""\nimport bert_modeling as modeling\nimport tensorflow as tf\nimport os\nimport numpy as np\n\nfrom utils import load_data,init_label_dict,get_label_using_logits,get_target_label_short,compute_confuse_matrix,\\\n    compute_micro_macro,compute_confuse_matrix_batch,get_label_using_logits_batch,get_target_label_short_batch\n\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(""cache_file_h5py"",""../data/ieee_zhihu_cup/data.h5"",""path of training/validation/test data."") #../data/sample_multiple_label.txt\ntf.app.flags.DEFINE_string(""cache_file_pickle"",""../data/ieee_zhihu_cup/vocab_label.pik"",""path of vocabulary and label files"") #../data/sample_multiple_label.txt\n\ntf.app.flags.DEFINE_float(""learning_rate"",0.0001,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 32, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is training.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs to run."")\n\n# below hyper-parameter is for bert model\n# for a middel size model, train fast. use hidden_size=128, num_hidden_layers=4, num_attention_heads=8, intermediate_size=1024\ntf.app.flags.DEFINE_integer(""hidden_size"",768,""hidden size"")\ntf.app.flags.DEFINE_integer(""num_hidden_layers"",12,""number of hidden layers"")\ntf.app.flags.DEFINE_integer(""num_attention_heads"",12,""number of attention headers"")\ntf.app.flags.DEFINE_integer(""intermediate_size"",3072,""intermediate size of hidden layer"")\ntf.app.flags.DEFINE_integer(""max_seq_length"",200,""max sequence length"")\n\ndef main(_):\n    # 1. get training data and vocabulary & labels dict\n    word2index, label2index, trainX, trainY, vaildX, vaildY, testX, testY = load_data(FLAGS.cache_file_h5py,FLAGS.cache_file_pickle)\n    vocab_size = len(word2index); print(""bert model.vocab_size:"", vocab_size);\n    num_labels = len(label2index); print(""num_labels:"", num_labels); cls_id=word2index[\'CLS\'];print(""id of \'CLS\':"",word2index[\'CLS\'])\n    num_examples, FLAGS.max_seq_length = trainX.shape;print(""num_examples of training:"", num_examples, "";max_seq_length:"", FLAGS.max_seq_length)\n\n    # 2. create model, define train operation\n    bert_config = modeling.BertConfig(vocab_size=len(word2index), hidden_size=FLAGS.hidden_size, num_hidden_layers=FLAGS.num_hidden_layers,\n                                      num_attention_heads=FLAGS.num_attention_heads,intermediate_size=FLAGS.intermediate_size)\n    input_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=""input_ids"") # FLAGS.batch_size\n    input_mask = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name=""input_mask"")\n    segment_ids = tf.placeholder(tf.int32, [None,FLAGS.max_seq_length],name=""segment_ids"")\n    label_ids = tf.placeholder(tf.float32, [None,num_labels], name=""label_ids"")\n    is_training = FLAGS.is_training #tf.placeholder(tf.bool, name=""is_training"")\n\n    use_one_hot_embeddings = False\n    loss, per_example_loss, logits, probabilities, model = create_model(bert_config, is_training, input_ids, input_mask,\n                                                            segment_ids, label_ids, num_labels,use_one_hot_embeddings)\n    # define train operation\n    #num_train_steps = int(float(num_examples) / float(FLAGS.batch_size * FLAGS.num_epochs)); use_tpu=False; num_warmup_steps = int(num_train_steps * 0.1)\n    #train_op = optimization.create_optimizer(loss, FLAGS.learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n    global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n    train_op = tf.contrib.layers.optimize_loss(loss, global_step=global_step, learning_rate=FLAGS.learning_rate,optimizer=""Adam"", clip_gradients=3.0)\n\n    is_training_eval=False\n    # 3. train the model by calling create model, get loss\n    gpu_config = tf.ConfigProto()\n    gpu_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=gpu_config)\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    if os.path.exists(FLAGS.ckpt_dir + ""checkpoint""):\n        print(""Checkpoint Exists. Restoring Variables from Checkpoint."")\n        saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n    number_of_training_data = len(trainX)\n    iteration = 0\n    curr_epoch = 0 #sess.run(textCNN.epoch_step)\n    batch_size = FLAGS.batch_size\n    for epoch in range(curr_epoch, FLAGS.num_epochs):\n        loss_total, counter = 0.0, 0\n        for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n            iteration = iteration + 1\n            input_ids_,input_mask_,segment_ids_=get_input_mask_segment_ids(trainX[start:end],cls_id)\n            feed_dict = {input_ids: input_ids_, input_mask: input_mask_, segment_ids:segment_ids_,\n                         label_ids:trainY[start:end]}\n            curr_loss,_ = sess.run([loss,train_op], feed_dict)\n            loss_total, counter = loss_total + curr_loss, counter + 1\n            if counter % 30 == 0:\n                print(epoch,""\\t"",iteration,""\\tloss:"",loss_total/float(counter),""\\tcurrent_loss:"",curr_loss)\n            if counter % 1000==0:\n                print(""trainX["",start,""]:"",trainX[start]);#print(""trainY[start:end]:"",trainY[start:end])\n                try:\n                    target_labels = get_target_label_short_batch(trainY[start:end]);#print(""target_labels:"",target_labels)\n                    print(""trainY["",start,""]:"",target_labels[0])\n                except:\n                    pass\n            # evaulation\n            if start!=0 and start % (3000 * FLAGS.batch_size) == 0:\n                eval_loss, f1_score, f1_micro, f1_macro = do_eval(sess,input_ids,input_mask,segment_ids,label_ids,is_training_eval,loss,\n                                                                  probabilities,vaildX, vaildY, num_labels,batch_size,cls_id)\n                print(""Epoch %d Validation Loss:%.3f\\tF1 Score:%.3f\\tF1_micro:%.3f\\tF1_macro:%.3f"" % (\n                    epoch, eval_loss, f1_score, f1_micro, f1_macro))\n                # save model to checkpoint\n                #if start % (4000 * FLAGS.batch_size)==0:\n                save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                print(""Going to save model.."")\n                saver.save(sess, save_path, global_step=epoch)\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,labels, num_labels, use_one_hot_embeddings,reuse_flag=False):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  output_layer = model.get_pooled_output()\n  hidden_size = output_layer.shape[-1].value\n  with tf.variable_scope(""weights"",reuse=reuse_flag):\n      output_weights = tf.get_variable(""output_weights"", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n      output_bias = tf.get_variable(""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    if is_training:\n        print(""###create_model.is_training:"",is_training)\n        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    print(""output_layer:"",output_layer.shape,"";output_weights:"",output_weights.shape,"";logits:"",logits.shape)\n\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    per_example_loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return loss, per_example_loss, logits, probabilities,model\n\n\ndef do_eval(sess,input_ids,input_mask,segment_ids,label_ids,is_training,loss,probabilities,vaildX, vaildY, num_labels,batch_size,cls_id):\n    """"""\n    evalution on model using validation data\n    :param sess:\n    :param input_ids:\n    :param input_mask:\n    :param segment_ids:\n    :param label_ids:\n    :param is_training:\n    :param loss:\n    :param probabilities:\n    :param vaildX:\n    :param vaildY:\n    :param num_labels:\n    :param batch_size:\n    :return:\n    """"""\n    num_eval=1000\n    vaildX = vaildX[0:num_eval]\n    vaildY = vaildY[0:num_eval]\n    number_examples = len(vaildX)\n    eval_loss, eval_counter, eval_f1_score, eval_p, eval_r = 0.0, 0, 0.0, 0.0, 0.0\n    label_dict = init_label_dict(num_labels)\n    f1_score_micro_sklearn_total=0.0\n    # batch_size=1 # TODO\n    for start, end in zip(range(0, number_examples, batch_size), range(batch_size, number_examples, batch_size)):\n        input_ids_,input_mask_, segment_ids_ = get_input_mask_segment_ids(vaildX[start:end],cls_id)\n        feed_dict = {input_ids: input_ids_,input_mask:input_mask_,segment_ids:segment_ids_,\n                     label_ids:vaildY[start:end]}\n        curr_eval_loss, prob = sess.run([loss, probabilities],feed_dict)\n        target_labels=get_target_label_short_batch(vaildY[start:end])\n        predict_labels=get_label_using_logits_batch(prob)\n        #print(""predict_labels:"",predict_labels)\n        label_dict=compute_confuse_matrix_batch(target_labels,predict_labels,label_dict,name=\'bert\')\n        eval_loss, eval_counter = eval_loss + curr_eval_loss, eval_counter + 1\n\n    f1_micro, f1_macro = compute_micro_macro(label_dict)  # label_dictis a dict, key is: accusation,value is: (TP,FP,FN). where TP is number of True Positive\n    f1_score_result = (f1_micro + f1_macro) / 2.0\n    return eval_loss / float(eval_counter), f1_score_result, f1_micro, f1_macro\n\ndef get_input_mask_segment_ids(train_x_batch,cls_id):\n    """"""\n    get input mask and segment ids given a batch of input x.\n    if sequence length of input x is max_sequence_length, then shape of both input_mask and segment_ids should be\n    [batch_size, max_sequence_length]. for those padding tokens, input_mask will be zero, value for all other place is one.\n    :param train_x_batch:\n    :return: input_mask_,segment_ids\n    """"""\n    batch_size,max_sequence_length=train_x_batch.shape\n    input_mask=np.ones((batch_size,max_sequence_length),dtype=np.int32)\n    # set 0 for token in padding postion\n    for i in range(batch_size):\n        input_x_=train_x_batch[i] # a list, length is max_sequence_length\n        input_x=list(input_x_)\n        for j in range(len(input_x)):\n            if input_x[j]==0:\n                input_mask[i][j:]=0\n                break\n    # insert CLS token for classification\n    input_ids=np.zeros((batch_size,max_sequence_length),dtype=np.int32)\n    #print(""input_ids.shape1:"",input_ids.shape)\n    for k in range(batch_size):\n        input_id_list=list(train_x_batch[k])\n        input_id_list.insert(0,cls_id)\n        del input_id_list[-1]\n        input_ids[k]=input_id_list\n    #print(""input_ids.shape2:"",input_ids.shape)\n\n    segment_ids=np.ones((batch_size,max_sequence_length),dtype=np.int32)\n    return input_mask, segment_ids,input_ids\n\n#train_x_batch=np.ones((3,5))\n#train_x_batch[0,4]=0\n#train_x_batch[1,3]=0\n#train_x_batch[1,4]=0\n#cls_id=2\n#print(""train_x_batch:"",train_x_batch)\n#input_mask, segment_ids,input_ids=get_input_mask_segment_ids(train_x_batch,cls_id)\n#print(""input_mask:"",input_mask, ""segment_ids:"",segment_ids,""input_ids:"",input_ids)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a01_FastText/old_single_label/p5_fastTextB_model.py,30,"b'# fast text. using: very simple model;n-gram to captrue location information;h-softmax to speed up training/inference\n# for the n-gram you can use data_util to generate. see method process_one_sentence_to_get_ui_bi_tri_gram under aa1_data_util/data_util_zhihu.py\nprint(""started..."")\nimport tensorflow as tf\nimport numpy as np\n\nclass fastTextB:\n    def __init__(self, label_size, learning_rate, batch_size, decay_steps, decay_rate,num_sampled,sentence_len,vocab_size,embed_size,is_training):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.label_size = label_size\n        self.batch_size = batch_size\n        self.num_sampled = num_sampled\n        self.sentence_len=sentence_len\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.is_training=is_training\n        self.learning_rate=learning_rate\n\n        # add placeholder (X,label)\n        self.sentence = tf.placeholder(tf.int32, [None, self.sentence_len], name=""sentence"")  # X\n        self.labels = tf.placeholder(tf.int32, [None], name=""Labels"")  # y\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.epoch_step = tf.Variable(0, trainable=False, name=""Epoch_Step"")\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]\n        if not is_training:\n            return\n        self.loss_val = self.loss()\n        self.train_op = self.train()\n        self.predictions = tf.argmax(self.logits, axis=1, name=""predictions"")  # shape:[None,]\n        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.labels) #tf.argmax(self.logits, 1)-->[batch_size]\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        # embedding matrix\n        self.Embedding = tf.get_variable(""Embedding"", [self.vocab_size, self.embed_size])\n        self.W = tf.get_variable(""W"", [self.embed_size, self.label_size])\n        self.b = tf.get_variable(""b"", [self.label_size])\n\n    def inference(self):\n        """"""main computation graph here: 1.embedding-->2.average-->3.linear classifier""""""\n        # 1.get emebedding of words in the sentence\n        sentence_embeddings = tf.nn.embedding_lookup(self.Embedding,self.sentence)  # [None,self.sentence_len,self.embed_size]\n\n        # 2.average vectors, to get representation of the sentence\n        self.sentence_embeddings = tf.reduce_mean(sentence_embeddings, axis=1)  # [None,self.embed_size]\n\n        # 3.linear classifier layer\n        logits = tf.matmul(self.sentence_embeddings, self.W) + self.b #[None, self.label_size]==tf.matmul([None,self.embed_size],[self.embed_size,self.label_size])\n        return logits\n\n    def loss(self,l2_lambda=0.01): #0.0001-->0.001\n        """"""calculate loss using (NCE)cross entropy here""""""\n        # Compute the average NCE loss for the batch.\n        # tf.nce_loss automatically draws a new sample of the negative labels each\n        # time we evaluate the loss.\n        if self.is_training: #training\n            labels=tf.reshape(self.labels,[-1])               #[batch_size,1]------>[batch_size,]\n            labels=tf.expand_dims(labels,1)                   #[batch_size,]----->[batch_size,1]\n            loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n                tf.nn.nce_loss(weights=tf.transpose(self.W),  #[embed_size, label_size]--->[label_size,embed_size]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n                               biases=self.b,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n                               labels=labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n                               inputs=self.sentence_embeddings,# [None,self.embed_size] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n                               num_sampled=self.num_sampled,  #scalar. 100\n                               num_classes=self.label_size,partition_strategy=""div""))  #scalar. 1999\n        else:#eval/inference\n            #logits = tf.matmul(self.sentence_embeddings, tf.transpose(self.W)) #matmul([None,self.embed_size])--->\n            #logits = tf.nn.bias_add(logits, self.b)\n            labels_one_hot = tf.one_hot(self.labels, self.label_size) #[batch_size]---->[batch_size,label_size]\n            #sigmoid_cross_entropy_with_logits:Computes sigmoid cross entropy given `logits`.Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive.  For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_one_hot,logits=self.logits) #labels:[batch_size,label_size];logits:[batch, label_size]\n            print(""loss0:"", loss) #shape=(?, 1999)\n            loss = tf.reduce_sum(loss, axis=1)\n            print(""loss1:"",loss)  #shape=(?,)\n        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n        return loss\n\n    def train(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"")\n        return train_op\n\n#test started\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=19\n    learning_rate=0.01\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.9\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1\n    fastText=fastTextB(num_classes, learning_rate, batch_size, decay_steps, decay_rate,5,sequence_length,vocab_size,embed_size,is_training)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(100):\n            input_x=np.zeros((batch_size,sequence_length),dtype=np.int32) #[None, self.sequence_length]\n            input_y=input_y=np.array([1,0,1,1,1,2,1,1],dtype=np.int32) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n            loss,acc,predict,_=sess.run([fastText.loss_val,fastText.accuracy,fastText.predictions,fastText.train_op],\n                                        feed_dict={fastText.sentence:input_x,fastText.labels:input_y})\n            print(""loss:"",loss,""acc:"",acc,""label:"",input_y,""prediction:"",predict)\n#test()\nprint(""ended..."")\n'"
a01_FastText/old_single_label/p5_fastTextB_predict.py,20,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\ntry:\n    reload                        # Python 2\nexcept NameError:\n    from importlib import reload  # Python 3\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport codecs\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""label_size"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_integer(""num_sampled"",100,""number of noise sampling"")\ntf.app.flags.DEFINE_string(""ckpt_dir"",""fast_text_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",300,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 3, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""fast_text_checkpoint/zhihu_result_ftB2.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-v4only-title.txt\',""target file path for final prediction"")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary()\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label()\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n\n    # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        fast_text=fastText(FLAGS.label_size, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.num_sampled,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        batch_size=1\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data+1, batch_size)):\n            logits=sess.run(fast_text.logits,feed_dict={fast_text.sentence:testX2[start:end]}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    # test\n    #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
a01_FastText/old_single_label/p5_fastTextB_train.py,23,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p5_fastTextB_model import fastTextB as fastText\nfrom p4_zhihu_load_data import load_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""label_size"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 128, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 20000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.8, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_integer(""num_sampled"",50,""number of noise sampling"") #100\ntf.app.flags.DEFINE_string(""ckpt_dir"",""fast_text_checkpoint/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",200,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""embedding size"")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\ntf.app.flags.DEFINE_string(""cache_path"",""fast_text_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary()\n        vocab_size = len(vocabulary_word2index)\n        vocabulary_word2index_label,_ = create_voabulary_label()\n        train, test, _ = load_data(vocabulary_word2index, vocabulary_word2index_label,data_type=\'train\')\n        trainX, trainY = train\n        testX, testY = test\n        print(""testX.shape:"", np.array(testX).shape)  # 2500\xe4\xb8\xaalist.\xe6\xaf\x8f\xe4\xb8\xaalist\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\n        print(""testY.shape:"", np.array(testY).shape)  # 2500\xe4\xb8\xaalabel\n        print(""testX[0]:"", testX[0])  # [17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\n        print(""testX[1]:"", testX[1]);\n        print(""testY[0]:"", testY[0])  # 0 ;print(""testY[1]:"",testY[1]) #0\n\n        # 2.Data preprocessing\n        # Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        ###############################################################################################\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        ###############################################################################################\n    print(""testX[0]:"", testX[0]) ;print(""testX[1]:"", testX[1]); #[17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4]\n    # Converting labels to binary vectors\n    print(""testY[0]:"", testY[0])  # 0 ;print(""testY[1]:"",testY[1]) #0\n    print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        fast_text=fastText(FLAGS.label_size, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.num_sampled,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, fast_text)\n\n        curr_epoch=sess.run(fast_text.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):#range(start,stop,step_size)\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])\n                    print(""trainY[start:end]:"",trainY[start:end])\n                curr_loss,curr_acc,_=sess.run([fast_text.loss_val,fast_text.accuracy,fast_text.train_op],feed_dict={fast_text.sentence:trainX[start:end],fast_text.labels:trainY[start:end]})\n                loss,acc,counter=loss+curr_loss,acc+curr_acc,counter+1\n                if counter %500==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter)))\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(fast_text.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,fast_text,testX,testY,batch_size)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=fast_text.epoch_step) #fast_text.epoch_step\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, fast_text, testX, testY, batch_size)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,fast_text):\n    print(""using pre-trained word emebedding.started..."")\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(\'zhihu-word2vec-multilabel.bin-100\', kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(fast_text.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,fast_text,evalX,evalY,batch_size):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        curr_eval_loss, curr_eval_acc, = sess.run([fast_text.loss_val, fast_text.accuracy],\n                                          feed_dict={fast_text.sentence: evalX[start:end],fast_text.labels: evalY[start:end]})\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\ndef load_data(cache_file_h5py,cache_file_pickle):\n    """"""\n    load data from h5py and pickle cache files, which is generate by take step by step of pre-processing.ipynb\n    :param cache_file_h5py:\n    :param cache_file_pickle:\n    :return:\n    """"""\n    if not os.path.exists(cache_file_h5py) or not os.path.exists(cache_file_pickle):\n        raise RuntimeError(""############################ERROR##############################\\n. ""\n                           ""please download cache file, it include training data and vocabulary & labels. ""\n                           ""link can be found in README.md\\n download zip file, unzip it, then put cache files as FLAGS.""\n                           ""cache_file_h5py and FLAGS.cache_file_pickle suggested location."")\n    print(""INFO. cache file exists. going to load cache file"")\n    f_data = h5py.File(cache_file_h5py, \'r\')\n    print(""f_data.keys:"",list(f_data.keys()))\n    train_X=f_data[\'train_X\'] # np.array(\n    print(""train_X.shape:"",train_X.shape)\n    train_Y=f_data[\'train_Y\'] # np.array(\n    print(""train_Y.shape:"",train_Y.shape,"";"")\n    vaild_X=f_data[\'vaild_X\'] # np.array(\n    valid_Y=f_data[\'valid_Y\'] # np.array(\n    test_X=f_data[\'test_X\'] # np.array(\n    test_Y=f_data[\'test_Y\'] # np.array(\n    #print(train_X)\n    #f_data.close()\n\n    word2index, label2index=None,None\n    with open(cache_file_pickle, \'rb\') as data_f_pickle:\n        word2index, label2index=pickle.load(data_f_pickle)\n    print(""INFO. cache file load successful..."")\n    return word2index, label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a02_TextCNN/other_experiement/__init__.py,0,b''
a02_TextCNN/other_experiement/data_util_zhihu.py,0,"b'# -*- coding: utf-8 -*-\nimport codecs\nimport numpy as np\n#load data of zhihu\nimport word2vec\nimport os\nimport pickle\nPAD_ID = 0\nfrom tflearn.data_utils import pad_sequences\n_GO=""_GO""\n_END=""_END""\n_PAD=""_PAD""\n\n#use pretrained word embedding to get word vocabulary and labels, and its relationship with index\ndef create_voabulary(simple=None,word2vec_model_path=\'zhihu-word2vec-title-desc.bin-100\',name_scope=\'\'):\n    cache_path =\'cache_vocabulary_label_pik/\'+ name_scope + ""_word_voabulary.pik""\n    print(""cache_path:"",cache_path,""file_exists:"",os.path.exists(cache_path))\n    if os.path.exists(cache_path):#if exists, load it; otherwise create it.\n        with open(cache_path, \'r\') as data_f:\n            vocabulary_word2index, vocabulary_index2word=pickle.load(data_f)\n            return vocabulary_word2index, vocabulary_index2word\n    else:\n        vocabulary_word2index={}\n        vocabulary_index2word={}\n        if simple is not None:\n            word2vec_model_path=\'zhihu-word2vec.bin-100\'\n        print(""create vocabulary. word2vec_model_path:"",word2vec_model_path)\n        model=word2vec.load(word2vec_model_path,kind=\'bin\')\n        vocabulary_word2index[\'PAD_ID\']=0\n        vocabulary_index2word[0]=\'PAD_ID\'\n        special_index=0\n        if \'biLstmTextRelation\' in name_scope:\n            vocabulary_word2index[\'EOS\']=1 # a special token for biLstTextRelation model. which is used between two sentences.\n            vocabulary_index2word[1]=\'EOS\'\n            special_index=1\n        for i,vocab in enumerate(model.vocab):\n            vocabulary_word2index[vocab]=i+1+special_index\n            vocabulary_index2word[i+1+special_index]=vocab\n\n        #save to file system if vocabulary of words is not exists.\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n            with open(cache_path, \'a\') as data_f:\n                pickle.dump((vocabulary_word2index,vocabulary_index2word), data_f)\n    return vocabulary_word2index,vocabulary_index2word\n\n# create vocabulary of lables. label is sorted. 1 is high frequency, 2 is low frequency.\ndef create_voabulary_label(voabulary_label=\'train-zhihu4-only-title-all.txt\',name_scope=\'\',use_seq2seq=False):#\'train-zhihu.txt\'\n    print(""create_voabulary_label_sorted.started.traning_data_path:"",voabulary_label)\n    cache_path =\'cache_vocabulary_label_pik/\'+ name_scope + ""_label_voabulary.pik""\n    if os.path.exists(cache_path):#\xe5\xa6\x82\xe6\x9e\x9c\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\n        with open(cache_path, \'r\') as data_f:\n            vocabulary_word2index_label, vocabulary_index2word_label=pickle.load(data_f)\n            return vocabulary_word2index_label, vocabulary_index2word_label\n    else:\n        zhihu_f_train = codecs.open(voabulary_label, \'r\', \'utf8\')\n        lines=zhihu_f_train.readlines()\n        count=0\n        vocabulary_word2index_label={}\n        vocabulary_index2word_label={}\n        vocabulary_label_count_dict={} #{label:count}\n        for i,line in enumerate(lines):\n            if \'__label__\' in line:  #\'__label__-2051131023989903826\n                label=line[line.index(\'__label__\')+len(\'__label__\'):].strip().replace(""\\n"","""")\n                if vocabulary_label_count_dict.get(label,None) is not None:\n                    vocabulary_label_count_dict[label]=vocabulary_label_count_dict[label]+1\n                else:\n                    vocabulary_label_count_dict[label]=1\n        list_label=sort_by_value(vocabulary_label_count_dict)\n\n        print(""length of list_label:"",len(list_label));#print("";list_label:"",list_label)\n        countt=0\n\n        ##########################################################################################\n        if use_seq2seq:#if used for seq2seq model,insert two special label(token):_GO AND _END\n            i_list=[0,1,2];label_special_list=[_GO,_END,_PAD]\n            for i,label in zip(i_list,label_special_list):\n                vocabulary_word2index_label[label] = i\n                vocabulary_index2word_label[i] = label\n        #########################################################################################\n        for i,label in enumerate(list_label):\n            if i<10:\n                count_value=vocabulary_label_count_dict[label]\n                print(""label:"",label,""count_value:"",count_value)\n                countt=countt+count_value\n            indexx = i + 3 if use_seq2seq else i\n            vocabulary_word2index_label[label]=indexx\n            vocabulary_index2word_label[indexx]=label\n        print(""count top10:"",countt)\n\n        #save to file system if vocabulary of words is not exists.\n        if not os.path.exists(cache_path): #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x86\x99\xe5\x88\xb0\xe7\xbc\x93\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n            with open(cache_path, \'a\') as data_f:\n                pickle.dump((vocabulary_word2index_label,vocabulary_index2word_label), data_f)\n    print(""create_voabulary_label_sorted.ended.len of vocabulary_label:"",len(vocabulary_index2word_label))\n    return vocabulary_word2index_label,vocabulary_index2word_label\n\ndef sort_by_value(d):\n    items=d.items()\n    backitems=[[v[1],v[0]] for v in items]\n    backitems.sort(reverse=True)\n    return [ backitems[i][1] for i in range(0,len(backitems))]\n\ndef create_voabulary_labelO():\n    model = word2vec.load(\'zhihu-word2vec-multilabel.bin-100\', kind=\'bin\') #zhihu-word2vec.bin-100\n    count=0\n    vocabulary_word2index_label={}\n    vocabulary_index2word_label={}\n    label_unique={}\n    for i,vocab in enumerate(model.vocab):\n        if \'__label__\' in vocab:  #\'__label__-2051131023989903826\n            label=vocab[vocab.index(\'__label__\')+len(\'__label__\'):]\n            if label_unique.get(label,None) is None: #\xe4\xb8\x8d\xe6\x9b\xbe\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe4\xbf\x9d\xe6\x8c\x81\xe5\x88\xb0\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\n                vocabulary_word2index_label[label]=count\n                vocabulary_index2word_label[count]=label #ADD\n                count=count+1\n                label_unique[label]=label\n    return vocabulary_word2index_label,vocabulary_index2word_label\n\ndef load_data_multilabel_new(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\n                             traning_data_path=\'train-zhihu4-only-title-all.txt\',multi_label_flag=True,use_seq2seq=False,seq2seq_label_length=6):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\n    print(""load_data.started..."")\n    print(""load_data_multilabel_new.training_data_path:"",traning_data_path)\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    Y = []\n    Y_decoder_input=[] #ADD 2017-06-15\n    for i, line in enumerate(lines):\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        y=y.strip().replace(\'\\n\',\'\')\n        x = x.strip()\n        if i<1:\n            print(i,""x0:"",x) #get raw x\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<2:\n            print(i,""x1:"",x) #word to index\n        if use_seq2seq:        # 1)prepare label for seq2seq format(ADD _GO,_END,_PAD for seq2seq)\n            ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\n            _PAD_INDEX=vocabulary_word2index_label[_PAD]\n            ys_mulithot_list=[_PAD_INDEX]*seq2seq_label_length #[3,2,11,14,1]\n            ys_decoder_input=[_PAD_INDEX]*seq2seq_label_length\n            # below is label.\n            for j,y in enumerate(ys):\n                if j<seq2seq_label_length-1:\n                    ys_mulithot_list[j]=vocabulary_word2index_label[y]\n            if len(ys)>seq2seq_label_length-1:\n                ys_mulithot_list[seq2seq_label_length-1]=vocabulary_word2index_label[_END]#ADD END TOKEN\n            else:\n                ys_mulithot_list[len(ys)] = vocabulary_word2index_label[_END]\n\n            # below is input for decoder.\n            ys_decoder_input[0]=vocabulary_word2index_label[_GO]\n            for j,y in enumerate(ys):\n                if j < seq2seq_label_length - 1:\n                    ys_decoder_input[j+1]=vocabulary_word2index_label[y]\n            if i<10:\n                print(i,""ys:==========>0"", ys)\n                print(i,""ys_mulithot_list:==============>1"", ys_mulithot_list)\n                print(i,""ys_decoder_input:==============>2"", ys_decoder_input)\n        else:\n            if multi_label_flag: # 2)prepare multi-label format for classification\n                ys = y.replace(\'\\n\', \'\').split("" "")  # ys is a list\n                ys_index=[]\n                for y in ys:\n                    y_index = vocabulary_word2index_label[y]\n                    ys_index.append(y_index)\n                ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\n            else:                #3)prepare single label format for classification\n                ys_mulithot_list=vocabulary_word2index_label[y]\n        if i<=3:\n            print(""ys_index:"")\n            #print(ys_index)\n            print(i,""y:"",y,"" ;ys_mulithot_list:"",ys_mulithot_list) #,"" ;ys_decoder_input:"",ys_decoder_input)\n        X.append(x)\n        Y.append(ys_mulithot_list)\n        if use_seq2seq:\n            Y_decoder_input.append(ys_decoder_input) #decoder input\n        #if i>50000:\n        #    break\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    print(""number_examples:"",number_examples) #\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\n    if use_seq2seq:\n        train=train+(Y_decoder_input[0:int((1 - valid_portion) * number_examples)],)\n        test=test+(Y_decoder_input[int((1 - valid_portion) * number_examples) + 1:],)\n    # 5.return\n    print(""load_data.ended..."")\n    return train, test, test\n\ndef load_data_multilabel_new_twoCNN(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,\n                                    traning_data_path=\'train-zhihu4-only-title-all.txt\',multi_label_flag=True):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\n    print(""load_data.twoCNN.started..."")\n    print(""load_data_multilabel_new_twoCNN.training_data_path:"",traning_data_path)\n    zhihu_f = codecs.open(traning_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    X2=[]\n    Y = []\n    count_error=0\n    for i, line in enumerate(lines):\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        y=y.strip().replace(\'\\n\',\'\')\n        x = x.strip()\n        #print(""x:===============>"",x)\n        try:\n            x,x2=x.split(""\\t"")\n        except Exception:\n            print(""x.split.error."",x,""count_error:"",count_error)\n            count_error+=1\n            continue\n        if i<1:\n            print(i,""x0:"",x) #get raw x\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        x2=x2.split("" "")\n        x2 =[vocabulary_word2index.get(e, 0) for e in x2]\n        if i<1:\n            print(i,""x1:"",x,""x2:"",x2) #word to index\n        if multi_label_flag:\n            ys = y.replace(\'\\n\', \'\').split("" "") #ys is a list\n            ys_index=[]\n            for y in ys:\n                y_index = vocabulary_word2index_label[y]\n                ys_index.append(y_index)\n            ys_mulithot_list=transform_multilabel_as_multihot(ys_index)\n        else:\n            ys_mulithot_list=int(y) #vocabulary_word2index_label[y]\n        if i<1:\n            print(i,""y:"",y,""ys_mulithot_list:"",ys_mulithot_list)\n        X.append(x)\n        X2.append(x2)\n        Y.append(ys_mulithot_list)\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    print(""number_examples:"",number_examples) #\n    train = (X[0:int((1 - valid_portion) * number_examples)],X2[0:int((1 - valid_portion) * number_examples)],Y[0:int((1 - valid_portion) * number_examples)])\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], X2[int((1 - valid_portion) * number_examples) + 1:],Y[int((1 - valid_portion) * number_examples) + 1:])\n    # 5.return\n    print(""load_data.ended..."")\n    return train, test, test\n\ndef load_data(vocabulary_word2index,vocabulary_word2index_label,valid_portion=0.05,max_training_data=1000000,training_data_path=\'train-zhihu4-only-title-all.txt\'):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example:""w305 w6651 w3974 w1005 w54 w109 w110 w3974 w29 w25 w1513 w3645 w6 w111 __label__-400525901828896492""\n    print(""load_data.started..."")\n    zhihu_f = codecs.open(training_data_path, \'r\', \'utf8\') #-zhihu4-only-title.txt\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    Y = []\n    for i, line in enumerate(lines):\n        x, y = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        y=y.replace(\'\\n\',\'\')\n        x = x.replace(""\\t"",\' EOS \').strip()\n        if i<5:\n            print(""x0:"",x) #get raw x\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        #if i<5:\n        #    print(""x1:"",x_) #\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<5:\n            print(""x1:"",x) #word to index\n        y = vocabulary_word2index_label[y] #np.abs(hash(y))\n        X.append(x)\n        Y.append(y)\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    print(""number_examples:"",number_examples) #\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)])\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:])\n    # 5.return\n    print(""load_data.ended..."")\n    return train, test, test\n\n # \xe5\xb0\x86\xe4\xb8\x80\xe5\x8f\xa5\xe8\xaf\x9d\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba(uigram,bigram,trigram)\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\ndef process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3):\n    """"""\n    :param sentence: string. example:\'w17314 w5521 w7729 w767 w10147 w111\'\n    :param n_gram:\n    :return:string. example:\'w17314 w17314w5521 w17314w5521w7729 w5521 w5521w7729 w5521w7729w767 w7729 w7729w767 w7729w767w10147 w767 w767w10147 w767w10147w111 w10147 w10147w111 w111\'\n    """"""\n    result=[]\n    word_list=sentence.split("" "") #[sentence[i] for i in range(len(sentence))]\n    unigram=\'\';bigram=\'\';trigram=\'\';fourgram=\'\'\n    length_sentence=len(word_list)\n    for i,word in enumerate(word_list):\n        unigram=word                           #ui-gram\n        word_i=unigram\n        if n_gram>=2 and i+2<=length_sentence: #bi-gram\n            bigram="""".join(word_list[i:i+2])\n            word_i=word_i+\' \'+bigram\n        if n_gram>=3 and i+3<=length_sentence: #tri-gram\n            trigram="""".join(word_list[i:i+3])\n            word_i = word_i + \' \' + trigram\n        if n_gram>=4 and i+4<=length_sentence: #four-gram\n            fourgram="""".join(word_list[i:i+4])\n            word_i = word_i + \' \' + fourgram\n        if n_gram>=5 and i+5<=length_sentence: #five-gram\n            fivegram="""".join(word_list[i:i+5])\n            word_i = word_i + \' \' + fivegram\n        result.append(word_i)\n    result="" "".join(result)\n    return result\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\xa0\x87\xe7\xad\xbe\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaalabel\xef\xbc\x9aload data with multi-labels\ndef load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,traning_path,valid_portion=0.05,max_training_data=1000000):  # n_words=100000,\n    """"""\n    input: a file path\n    :return: train, test, valid. where train=(trainX, trainY). where\n                trainX: is a list of list.each list representation a sentence.trainY: is a list of label. each label is a number\n    """"""\n    # 1.load a zhihu data from file\n    # example: \'w140 w13867 w10344 w2673 w9514 w269 w460 w6 w35053 w844 w10147 w111 __label__-2379261820462209275 -5535923551616745326 6038661761506862294\'\n    print(""load_data_with_multilabels.ended..."")\n    zhihu_f = codecs.open(traning_path,\'r\',\'utf8\') #(\'/home/xul/xul/9_ZhihuCup/\'+data_type+\'-zhihu5-only-title-multilabel.txt\', \'r\', \'utf8\') #home/xul/xul/9_ZhihuCup/\'\n    lines = zhihu_f.readlines()\n    # 2.transform X as indices\n    # 3.transform  y as scalar\n    X = []\n    Y = []\n    Y_label1999=[]\n    for i, line in enumerate(lines):\n        #if i>max_training_data:\n        #    break\n        x, ys = line.split(\'__label__\') #x=\'w17314 w5521 w7729 w767 w10147 w111\'\n        ys=ys.replace(\'\\n\',\'\').split("" "")\n        x = x.strip()\n        if i < 5:\n            print(""x0:"", x) # u\'w4260 w4260w86860 w4260w86860w30907 w86860 w86860w30907 w86860w30907w11 w30907 w30907w11 w30907w11w31 w11 w11w31 w11w31w72 w31 w31w72 w31w72w166 w72 w72w166 w72w166w346 w166 w166w346 w166w346w2182 w346 w346w2182 w346w2182w224 w2182 w2182w224 w2182w224w2148 w224 w224w2148 w224w2148w6 w2148 w2148w6 w2148w6w2566 w6 w6w2566 w6w2566w25 w2566 w2566w25 w2566w25w1110 w25 w25w1110 w25w1110w111 w1110 w1110w111 w111\'\n        #x_=process_one_sentence_to_get_ui_bi_tri_gram(x)\n        #if i < 5:\n        #    print(""x1:"", x_)\n        x=x.split("" "")\n        x = [vocabulary_word2index.get(e,0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<5:\n            print(""x2:"", x)\n        #print(""ys:"",ys) #[\'501174938575526146\', \'-4317515119936650885\']\n        ys_list=[]\n        for y in ys:\n            y_ = vocabulary_word2index_label[y]\n            ys_list.append(y_)\n        X.append(x)\n        #TODO ys_list_array=transform_multilabel_as_multihot(ys_list) #it is 2-d array. [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]...]\n        ys_list_=proces_label_to_algin(ys_list)\n        Y.append(ys_list_)\n        #TODO Y_label1999.append(ys_list_array)\n        if i==0:\n            print(X,Y)\n            print(Y_label1999)\n    # 4.split to train,test and valid data\n    number_examples = len(X)\n    train = (X[0:int((1 - valid_portion) * number_examples)], Y[0:int((1 - valid_portion) * number_examples)]) #TODO Y_label1999[0:int((1 - valid_portion) * number_examples)]\n    test = (X[int((1 - valid_portion) * number_examples) + 1:], Y[int((1 - valid_portion) * number_examples) + 1:]) #TODO ,Y_label1999[int((1 - valid_portion) * number_examples) + 1:]\n    print(""load_data_with_multilabels.ended..."")\n    return train, test\n\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\ndef transform_multilabel_as_multihot(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\n    """"""\n    :param label_list: e.g.[0,1,4]\n    :param label_size: e.g.199\n    :return:e.g.[1,1,0,1,0,0,........]\n    """"""\n    result=np.zeros(label_size)\n    #set those location as 1, all else place as 0.\n    result[label_list] = 1\n    return result\n\n#\xe5\xb0\x86LABEL\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaMULTI-HOT\ndef transform_multilabel_as_multihotO(label_list,label_size=1999): #1999label_list=[0,1,4,9,5]\n    batch_size=len(label_list)\n    result=np.zeros((batch_size,label_size))\n    #set those location as 1, all else place as 0.\n    result[(range(batch_size),label_list)]=1\n    return result\n\ndef load_final_test_data(file_path):\n    final_test_file_predict_object = codecs.open(file_path, \'r\', \'utf8\')\n    lines=final_test_file_predict_object.readlines()\n    question_lists_result=[]\n    for i,line in enumerate(lines):\n        question_id,question_string=line.split(""\\t"")\n        question_string=question_string.strip().replace(""\\n"","""")\n        question_lists_result.append((question_id,question_string))\n    print(""length of total question lists:"",len(question_lists_result))\n    return question_lists_result\n\ndef load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists,uni_to_tri_gram=False):  # n_words=100000,\n    final_list=[]\n    for i, tuplee in enumerate(questionid_question_lists):\n        queston_id,question_string_list=tuplee\n        if uni_to_tri_gram:\n            x_=process_one_sentence_to_get_ui_bi_tri_gram(question_string_list)\n            x=x_.split("" "")\n        else:\n            x=question_string_list.split("" "")\n        x = [vocabulary_word2index.get(e, 0) for e in x] #if can\'t find the word, set the index as \'0\'.(equal to PAD_ID = 0)\n        if i<=2:\n            print(""question_id:"",queston_id);print(""question_string_list:"",question_string_list);print(""x_indexed:"",x)\n        final_list.append((queston_id,x))\n    number_examples = len(final_list)\n    print(""number_examples:"",number_examples) #\n    return  final_list\n\n\ndef proces_label_to_algin(ys_list,require_size=5):\n    """"""\n    :param ys_list: a list\n    :return: a list\n    """"""\n    ys_list_result=[0 for x in range(require_size)]\n    if len(ys_list)>=require_size: #\xe8\xb6\x85\xe9\x95\xbf\n        ys_list_result=ys_list[0:require_size]\n    else:#\xe5\xa4\xaa\xe7\x9f\xad\n       if len(ys_list)==1:\n           ys_list_result =[ys_list[0] for x in range(require_size)]\n       elif len(ys_list)==2:\n           ys_list_result = [ys_list[0],ys_list[0],ys_list[0],ys_list[1],ys_list[1]]\n       elif len(ys_list) == 3:\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[1], ys_list[2]]\n       elif len(ys_list) == 4:\n           ys_list_result = [ys_list[0], ys_list[0], ys_list[1], ys_list[2], ys_list[3]]\n    return ys_list_result\n\ndef write_uigram_to_trigram():\n    pass\n    #1.read file.\n    #2.uigram--->trigram\n    #3.write each line to file system.\n\ndef test_pad():\n    trainX=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\n    trainX=trainX.split("" "")\n    trainX = pad_sequences([[trainX]], maxlen=100, value=0.)\n    print(""trainX:"",trainX)\n\ntopic_info_file_path=\'topic_info.txt\'\ndef read_topic_info():\n    f = codecs.open(topic_info_file_path, \'r\', \'utf8\')\n    lines=f.readlines()\n    dict_questionid_title={}\n    for i,line in enumerate(lines):\n        topic_id,partent_ids,title_character,title_words,desc_character,decs_words=line.split(""\\t"").strip()\n        # print(i,""------------------------------------------------------"")\n        # print(""topic_id:"",topic_id)\n        # print(""partent_ids:"",partent_ids)\n        # print(""title_character:"",title_character)\n        # print(""title_words:"",title_words)\n        # print(""desc_character:"",desc_character)\n        # print(""decs_words:"",decs_words)\n        dict_questionid_title[topic_id]=title_words+"" ""+decs_words\n    print(""len(dict_questionid_title):"",len(dict_questionid_title))\n    return dict_questionid_title\n\ndef stat_training_data_length():\n    training_data=\'train-zhihu4-only-title-all.txt\'\n    f = codecs.open(training_data, \'r\', \'utf8\')\n    lines=f.readlines()\n    length_dict={0:0,5:0,10:0,15:0,20:0,25:0,30:0,35:0,40:0,100:0,150:0,200:0,1500:0}\n    length_list=[0,5,10,15,20,25,30,35,40,100,150,200,1500]\n    for i,line in enumerate(lines):\n        line_list=line.split(\'__label__\')[0].strip().split("" "")\n        length=len(line_list)\n        #print(i,""length:"",length)\n        for l in length_list:\n            if length<l:\n                length=l\n                #print(""length.assigned:"",length)\n                break\n        #print(""length.before dict assign:"", length)\n        length_dict[length]=length_dict[length]+1\n    print(""length_dict:"",length_dict)\n\n\nif __name__ == \'__main__\':\n    if __name__ == \'__main__\':\n        if __name__ == \'__main__\':\n            #1.\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\n            #vocabulary_word2index_label, vocabulary_index2word_label=create_voabulary_label()\n            #load_data_with_multilabels(vocabulary_word2index,vocabulary_word2index_label,data_type=\'test\')\n            #2.\n            #sentence=u\'\xe6\x88\x91\xe6\x83\xb3\xe5\xbc\x80\xe9\x80\x9a\xe5\x88\x9b\xe4\xb8\x9a\xe6\x9d\xbf\'\n            #sentence=\'w18476 w4454 w1674 w6 w25 w474 w1333 w1467 w863 w6 w4430 w11 w813 w4463 w863 w6 w4430 w111\'\n            #result=process_one_sentence_to_get_ui_bi_tri_gram(sentence,n_gram=3)\n            #print(len(result),""result:"",result)\n\n            #3. transform to multilabel\n            #label_list=[0,1,4,9,5]\n            #result=transform_multilabel_as_multihot(label_list,label_size=15)\n            #print(""result:"",result)\n\n            #4.load data for predict-----------------------------------------------------------------\n            #file_path=\'test-zhihu-forpredict-v4only-title.txt\'\n            #questionid_question_lists=load_final_test_data(file_path)\n\n            #vocabulary_word2index, vocabulary_index2word=create_voabulary()\n            #vocabulary_word2index_label,_=create_voabulary_label()\n            #final_list=load_data_predict(vocabulary_word2index, vocabulary_word2index_label, questionid_question_lists)\n\n            #5.process label require lengh\n            #ys_list=[99999]\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\n            #print(ys_list,""ys_list_result1.:"",ys_list_result)\n            #ys_list=[99999,23423432,67566765]\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\n            #ys_list=[99999,23423432,67566765,23333333]\n            #ys_list_result=proces_label_to_algin(ys_list,require_size=5)\n            #print(ys_list,""ys_list_result2.:"",ys_list_result)\n            #ys_list = [99999, 23423432, 67566765,44543543,546546546,323423434]\n            #ys_list_result = proces_label_to_algin(ys_list, require_size=5)\n            #print(ys_list, ""ys_list_result3.:"", ys_list_result)\n\n            #6.create vocabulary label. sorted.\n            #create_voabulary_label()\n\n            #d={\'a\':3,\'b\':2,\'c\':11}\n            #d_=sort_by_value(d)\n            #print(""d_"",d_)\n\n            #7.\n            #test_pad()\n\n            #8.read topic info\n            #read_topic_info()\n\n            #9\xe3\x80\x82\n            stat_training_data_length()\n'"
a02_TextCNN/other_experiement/p7_TextCNN_model_multilayers.py,57,"b'# -*- coding: utf-8 -*-\n#TextCNN: 1. embeddding layers, 2.convolutional layer, 3.max-pooling, 4.softmax layer.\n# print(""started..."")\nimport tensorflow as tf\nimport numpy as np\n\nclass TextCNNMultilayers:\n    def __init__(self, filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,\n                 is_training,initializer=tf.random_normal_initializer(stddev=0.1),multi_label_flag=False,clip_gradients=5.0,decay_rate_big=0.50):\n        """"""init all hyperparameter here""""""\n        # set hyperparamter\n        self.num_classes = num_classes\n        self.batch_size = batch_size\n        self.sequence_length=sequence_length\n        self.vocab_size=vocab_size\n        self.embed_size=embed_size\n        self.is_training=is_training\n        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=""learning_rate"")#ADD learning_rate\n        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * decay_rate_big)\n        self.filter_sizes=filter_sizes # it is a list of int. e.g. [3,4,5]\n        self.num_filters=num_filters\n        self.initializer=initializer\n        self.num_filters_total=self.num_filters * len(filter_sizes) #how many filters totally.\n        self.multi_label_flag=multi_label_flag\n        self.clip_gradients = clip_gradients\n\n        # add placeholder (X,label)\n        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=""input_x"")  # X\n        #self.input_y = tf.placeholder(tf.int32, [None,],name=""input_y"")  # y:[None,num_classes]\n        self.input_y_multilabel = tf.placeholder(tf.float32,[None,self.num_classes], name=""input_y_multilabel"")  # y:[None,num_classes]. this is for multi-label classification only.\n        self.dropout_keep_prob=tf.placeholder(tf.float32,name=""dropout_keep_prob"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n        self.epoch_step=tf.Variable(0,trainable=False,name=""Epoch_Step"")\n        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n\n        self.instantiate_weights()\n        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n        self.possibility=tf.nn.sigmoid(self.logits)\n        if not is_training:\n            return\n        if multi_label_flag:print(""going to use multi label loss."");self.loss_val = self.loss_multilabel()\n        else:print(""going to use single label loss."");self.loss_val = self.loss()\n        self.train_op = self.train()\n        if not self.multi_label_flag:\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")  # shape:[None,]\n            print(""self.predictions:"", self.predictions)\n            correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n            self.accuracy =tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""Accuracy"") # shape=()\n\n    def instantiate_weights(self):\n        """"""define all weights here""""""\n        with tf.name_scope(""embedding""): # embedding matrix\n            self.Embedding = tf.get_variable(""Embedding"",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n            self.W_projection = tf.get_variable(""W_projection"",shape=[self.num_filters_total, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n            self.b_projection = tf.get_variable(""b_projection"",shape=[self.num_classes])       #[label_size] #ADD 2017.06.09\n\n    def inference(self):\n        """"""main computation graph here: 1.embedding-->2.CONV-RELU-MAX_POOLING-->3.linear classifier""""""\n        # 1.=====>get emebedding of words in the sentence\n        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x)#[None,sentence_length,embed_size]\n        self.sentence_embeddings_expanded=tf.expand_dims(self.embedded_words,-1) #[None,sentence_length,embed_size,1). expand dimension so meet input requirement of 2d-conv\n\n        # 2.=====>loop each filter size. for each filter, do:convolution-pooling layer(a.create filters,b.conv,c.apply nolinearity,d.max-pooling)--->\n        # you can use:tf.nn.conv2d;tf.nn.relu;tf.nn.max_pool; feature shape is 4-d. feature is a new variable\n        pooled_outputs = []\n        for i,filter_size in enumerate(self.filter_sizes):\n            with tf.name_scope(""convolution-pooling-%s"" %filter_size):\n                # ====>a.create filter\n                #Layer1:CONV-RELU\n                filter=tf.get_variable(""filter-%s""%filter_size,[filter_size,self.embed_size,1,self.num_filters],initializer=self.initializer)\n                # ====>b.conv operation: conv2d===>computes a 2-D convolution given 4-D `input` and `filter` tensors.\n                #Conv.Input: given an input tensor of shape `[batch, in_height, in_width, in_channels]` and a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`\n                #Conv.Returns: A `Tensor`. Has the same type as `input`.\n                #         A 4-D tensor. The dimension order is determined by the value of `data_format`, see below for details.\n                #1)each filter with conv2d\'s output a shape:[1,sequence_length-filter_size+1,1,1];2)*num_filters--->[1,sequence_length-filter_size+1,1,num_filters];3)*batch_size--->[batch_size,sequence_length-filter_size+1,1,num_filters]\n                #input data format:NHWC:[batch, height, width, channels];output:4-D\n                conv=tf.nn.conv2d(self.sentence_embeddings_expanded, filter, strides=[1,1,1,1], padding=""VALID"",name=""conv"") #shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]\n                print(""conv1:"",conv)\n                # ====>c. apply nolinearity\n                b=tf.get_variable(""b-%s""%filter_size,[self.num_filters]) #ADD 2017-06-09\n                h=tf.nn.relu(tf.nn.bias_add(conv,b),""relu"") #shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n\n                #h=tf.reshape(h,[-1,self.sequence_length-filter_size+1,self.num_filters,1]) #shape:[batch_size,sequence_length-filter_size+1,num_filters,1]\n                #Layer2:CONV-RELU\n                #filter2 = tf.get_variable(""filter2-%s"" % filter_size, [filter_size, self.num_filters, 1, self.num_filters],initializer=self.initializer)\n                #conv2=tf.nn.conv2d(h,filter2,strides=[1,1,1,1],padding=""VALID"",name=""conv2"") #shape:[batch_size,sequence_length-filter_size*2+2,1,num_filters]\n                #print(""conv2:"",conv2)\n                #b2 = tf.get_variable(""b2-%s"" % filter_size, [self.num_filters])  # ADD 2017-06-09\n                #h = tf.nn.relu(tf.nn.bias_add(conv2, b2),""relu2"")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n\n                # ====>. max-pooling.  value: A 4-D `Tensor` with shape `[batch, height, width, channels]\n                #                  ksize: A list of ints that has length >= 4.  The size of the window for each dimension of the input tensor.\n                #                  strides: A list of ints that has length >= 4.  The stride of the sliding window for each dimension of the input tensor.\n                #pooled=tf.nn.max_pool(h, ksize=[1,self.sequence_length-filter_size*2+2,1,1], strides=[1,1,1,1], padding=\'VALID\',name=""pool"")#shape:[batch_size, 1, 1, num_filters].max_pool:performs the max pooling on the input.\n\n                pooled=tf.nn.max_pool(h, ksize=[1,self.sequence_length-filter_size+1,1,1], strides=[1,1,1,1], padding=\'VALID\',name=""pool"")#shape:[batch_size, 1, 1, num_filters].max_pool:performs the max pooling on the input.\n                pooled_outputs.append(pooled)\n        # 3.=====>combine all pooled features, and flatten the feature.output\' shape is a [1,None]\n        #e.g. >>> x1=tf.ones([3,3]);x2=tf.ones([3,3]);x=[x1,x2]\n        #         x12_0=tf.concat(x,0)---->x12_0\' shape:[6,3]\n        #         x12_1=tf.concat(x,1)---->x12_1\' shape;[3,6]\n        self.h_pool=tf.concat(pooled_outputs,3) #shape:[batch_size, 1, 1, num_filters_total]. tf.concat=>concatenates tensors along one dimension.where num_filters_total=num_filters_1+num_filters_2+num_filters_3\n        self.h_pool_flat=tf.reshape(self.h_pool,[-1,self.num_filters_total]) #shape should be:[None,num_filters_total]. here this operation has some result as tf.sequeeze().e.g. x\'s shape:[3,3];tf.reshape(-1,x) & (3, 3)---->(1,9)\n        print(""self.h_pool_flat:"",self.h_pool_flat)\n        #4.=====>add dropout: use tf.nn.dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop=tf.nn.dropout(self.h_pool_flat,keep_prob=self.dropout_keep_prob) #[None,num_filters_total]\n\n        #5. logits(use linear layer)and predictions(argmax)\n        with tf.name_scope(""output""):\n            logits = tf.matmul(self.h_drop,self.W_projection) + self.b_projection  #shape:[None, self.num_classes]==tf.matmul([None,self.embed_size],[self.embed_size,self.num_classes])\n        return logits\n\n    def loss_multilabel(self,l2_lambda=0.0001): #0.0001#this loss function is for multi-label classification\n        with tf.name_scope(""loss""):\n            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            #input_y:shape=(?, 1999); logits:shape=(?, 1999)\n            # let `x = logits`, `z = labels`.  The logistic loss is:z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y_multilabel, logits=self.logits);#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input__y,logits=self.logits)\n            #losses=-self.input_y_multilabel*tf.log(self.logits)-(1-self.input_y_multilabel)*tf.log(1-self.logits)\n            print(""sigmoid_cross_entropy_with_logits.losses:"",losses) #shape=(?, 1999).\n            losses=tf.reduce_sum(losses,axis=1) #shape=(?,). loss for all data in the batch\n            loss=tf.reduce_mean(losses)         #shape=().   average loss in the batch\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def loss(self,l2_lambda=0.0001):#0.001\n        with tf.name_scope(""loss""):\n            #input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n            #print(""1.sparse_softmax_cross_entropy_with_logits.losses:"",losses) # shape=(?,)\n            loss=tf.reduce_mean(losses)#print(""2.loss.loss:"", loss) #shape=()\n            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \'bias\' not in v.name]) * l2_lambda\n            loss=loss+l2_losses\n        return loss\n\n    def train_old(self):\n        """"""based on the loss, use SGD to update parameter""""""\n        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=""Adam"",clip_gradients=self.clip_gradients)\n        return train_op\n\n\n#test started. toy task: given a sequence of data. compute it\'s label: sum of its previous element,itself and next element greater than a threshold, it\'s label is 1,otherwise 0.\n#e.g. given inputs:[1,0,1,1,0]; outputs:[0,1,1,1,0].\n#invoke test() below to test the model in this toy task.\ndef test():\n    #below is a function test; if you use this for text classifiction, you need to transform sentence to indices of vocabulary first. then feed data to the graph.\n    num_classes=5\n    learning_rate=0.001\n    batch_size=8\n    decay_steps=1000\n    decay_rate=0.95\n    sequence_length=5\n    vocab_size=10000\n    embed_size=100\n    is_training=True\n    dropout_keep_prob=1.0 #0.5\n    filter_sizes=[2,3,4]\n    num_filters=128\n    multi_label_flag=True\n    textRNN=TextCNN(filter_sizes,num_filters,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training,multi_label_flag=multi_label_flag)\n    with tf.Session() as sess:\n       sess.run(tf.global_variables_initializer())\n       for i in range(500):\n           input_x=np.random.randn(batch_size,sequence_length) #[None, self.sequence_length]\n           input_x[input_x>=0]=1\n           input_x[input_x <0] = 0\n           input_y_multilabel=get_label_y(input_x)\n           loss,possibility,W_projection_value,_=sess.run([textRNN.loss_val,textRNN.possibility,textRNN.W_projection,textRNN.train_op],\n                                                    feed_dict={textRNN.input_x:input_x,textRNN.input_y_multilabel:input_y_multilabel,textRNN.dropout_keep_prob:dropout_keep_prob})\n           print(i,""loss:"",loss,""-------------------------------------------------------"")\n           print(""label:"",input_y_multilabel);print(""possibility:"",possibility)\n\ndef get_label_y(input_x):\n    length=input_x.shape[0]\n    input_y=np.zeros((input_x.shape))\n    for i in range(length):\n        element=input_x[i,:] #[5,]\n        result=compute_single_label(element)\n        input_y[i,:]=result\n    return input_y\n\ndef compute_single_label(listt):\n    result=[]\n    length=len(listt)\n    for i,e in enumerate(listt):\n        previous=listt[i-1] if i>0 else 0\n        current=listt[i]\n        next=listt[i+1] if i<length-1 else 0\n        summ=previous+current+next\n        if summ>=2:\n            summ=1\n        else:\n            summ=0\n        result.append(summ)\n    return result\n#test()'"
a02_TextCNN/other_experiement/p7_TextCNN_predict_ensemble.py,1,"b'from  p7_TextCNN_predict import get_logits_with_value_by_input\nfrom p7_TextCNN_predict_exp import get_logits_with_value_by_input_exp\nimport tensorflow as tf\ndef main(_):\n    for start in range(217360):\n        end=start+1\n        label_list,p_list=get_logits_with_value_by_input(start,end)\n        label_list_exp, p_list_exp=get_logits_with_value_by_input_exp(start,end)\n\n        if start<5:\n            print(""----------------------------------------------------"")\n            print(start,""label_list0:"",label_list,""p_list0:"",p_list)\n            print(start,""label_list1:"", label_list_exp, ""p_list1:"", p_list_exp)\n        else:\n            break\n\n\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a02_TextCNN/other_experiement/p7_TextCNN_predict_exp.py,27,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\n# tf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\n# tf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\n# tf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\n# tf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\n# tf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\n#tf.app.flags.DEFINE_string(""ckpt_dir2"",""text_cnn_title_desc_checkpoint_exp/"",""checkpoint location for the model"")\n# tf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\n# tf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\n# tf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\n# tf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs."")\n# tf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\n# tf.app.flags.DEFINE_string(""predict_target_file"",""text_cnn_title_desc_checkpoint_exp/zhihu_result_cnn_multilabel_v6_exp.csv"",""target file path for final prediction"")\n# tf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\n# tf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\n# tf.app.flags.DEFINE_integer(""num_filters"", 256, ""number of filters"") #128\n#tf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n\n##################################################################################################################################\nfilter_sizes=[3,4,5,7,15,20,25]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n# 1.load data with vocabulary of words and labels\nvocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',\n                                                                word2vec_model_path=FLAGS.word2vec_model_path,\n                                                                name_scope=""cnn2"")\nvocab_size = len(vocabulary_word2index)\nvocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\nquestionid_question_lists = load_final_test_data(FLAGS.predict_source_file)\ntest = load_data_predict(vocabulary_word2index, vocabulary_word2index_label, questionid_question_lists)\ntestX = []\nquestion_id_list = []\nfor tuple in test:\n    question_id, question_string_list = tuple\n    question_id_list.append(question_id)\n    testX.append(question_string_list)\n# 2.Data preprocessing: Sequence padding\nprint(""start padding...."")\ntestX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\nprint(""end padding..."")\n# 3.create session.\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\ngraph=tf.Graph().as_default()\nglobal sess\nglobal textCNN\nwith graph:\n    sess=tf.Session(config=config)\n# 4.Instantiate Model\n    textCNN = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\n                  FLAGS.decay_steps, FLAGS.decay_rate,\n                  FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training)\n    saver = tf.train.Saver()\n    if os.path.exists(FLAGS.ckpt_dir2 + ""checkpoint""):\n        print(""Restoring Variables from Checkpoint"")\n        saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir2))\n    else:\n        print(""Can\'t find the checkpoint.going to stop"")\n    #return\n# 5.feed data, to get logits\nnumber_of_training_data = len(testX2);\nprint(""number_of_training_data:"", number_of_training_data)\n#index = 0\n#predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n##################################################################################################################################\ndef get_logits_with_value_by_input_exp(start,end):\n    x=testX2[start:end]\n    global sess\n    global textCNN\n    logits = sess.run(textCNN.logits, feed_dict={textCNN.input_x: x, textCNN.dropout_keep_prob: 1})\n    predicted_labels,value_labels = get_label_using_logits_with_value(logits[0], vocabulary_index2word_label)\n    value_labels_exp= np.exp(value_labels)\n    p_labels=value_labels_exp/np.sum(value_labels_exp)\n    return predicted_labels,p_labels\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,FLAGS.decay_rate,\n                        FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir2+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir2))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textCNN.logits,feed_dict={textCNN.input_x:testX2[start:end],textCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    #tf.app.run()\n    labels,list_value=get_logits_with_value_by_input_exp(0, 1)\n    print(""labels:"",labels)\n    print(""list_value:"", list_value)'"
a02_TextCNN/other_experiement/p7_TextCNN_predict_exp512.py,23,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512/zhihu_result_cnn_multilabel_v7_exp512_20170616.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\ntf.app.flags.DEFINE_integer(""num_filters"", 600, ""number of filters"") #128-->512\ntf.app.flags.DEFINE_string(""ckpt_dir2"",""text_cnn_title_desc_checkpoint_exp/"",""checkpoint location for the model"")\n\n#tf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n\n##############################################################################################################################################\nfilter_sizes=[3,4,5,7,10,15,20,25]#[1,2,3,4,5,6,7]\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,FLAGS.decay_rate,\n                        FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textCNN.logits,feed_dict={textCNN.input_x:testX2[start:end],textCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()\n    #labels,list_value=get_logits_with_value_by_input(0, 1)\n    #print(""labels:"",labels)\n    #print(""list_value:"", list_value)'"
a02_TextCNN/other_experiement/p7_TextCNN_predict_exp512_0609.py,23,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_cnn_title_desc_checkpoint_exp512_0609/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""text_cnn_title_desc_checkpoint_exp512_0609/zhihu_result_cnn_multilabel_exp512_0609.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\ntf.app.flags.DEFINE_integer(""num_filters"", 256, ""number of filters"") #128\ntf.app.flags.DEFINE_string(""ckpt_dir2"",""text_cnn_title_desc_checkpoint_exp/"",""checkpoint location for the model"")\n\n#tf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n\n##############################################################################################################################################\nfilter_sizes=[2,3,5,6,7,8] #[3,4,5,7,10,15,20,25]#[1,2,3,4,5,6,7]\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,FLAGS.decay_rate,\n                        FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textCNN.logits,feed_dict={textCNN.input_x:testX2[start:end],textCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()\n    #labels,list_value=get_logits_with_value_by_input(0, 1)\n    #print(""labels:"",labels)\n    #print(""list_value:"", list_value)'"
a02_TextCNN/other_experiement/p7_TextCNN_predict_exp512_simple.py,23,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_simple/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_simple/zhihu_result_cnn_multilabel_exp512_simple.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\ntf.app.flags.DEFINE_integer(""num_filters"", 256, ""number of filters"") #128\ntf.app.flags.DEFINE_string(""ckpt_dir2"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp/"",""checkpoint location for the model"")\n\n#tf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n\n##############################################################################################################################################\nfilter_sizes=[7] #[3,4,5,7,10,15,20,25]#[1,2,3,4,5,6,7]\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,FLAGS.decay_rate,\n                        FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textCNN.logits,feed_dict={textCNN.input_x:testX2[start:end],textCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    tf.app.run()\n    #labels,list_value=get_logits_with_value_by_input(0, 1)\n    #print(""labels:"",labels)\n    #print(""list_value:"", list_value)\n'"
a02_TextCNN/other_experiement/p7_TextCNN_train_exp.py,27,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p7_TextCNN_model import TextCNN\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 512, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 6000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.65, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\n#tf.app.flags.DEFINE_integer(""num_sampled"",50,""number of noise sampling"") #100\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_cnn_title_desc_checkpoint_exp/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",25,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_integer(""num_filters"", 256, ""number of filters"") #256--->512\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\nfilter_sizes=[3,4,5,7,15,20,25] #[1,2,3,4,5,6,7]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""cnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #test-zhihu5-only-title-multilabel.txt\n        train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                        FLAGS.decay_rate,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textCNN,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textCNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {textCNN.input_x: trainX[start:end],textCNN.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[textCNN.input_y] = trainY[start:end]\n                else:\n                    feed_dict[textCNN.input_y_multilabel]=trainY[start:end]\n                curr_loss,curr_acc,_=sess.run([textCNN.loss_val,textCNN.accuracy,textCNN.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %50==0:\n                    print(""Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textCNN.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,textCNN,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                save_path=FLAGS.ckpt_dir+""model.ckpt""\n                saver.save(sess,save_path,global_step=epoch)\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, textCNN, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textCNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textCNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[textCNN.input_y] = evalY[start:end]\n        else:\n            feed_dict[textCNN.input_y_multilabel] = evalY[start:end]\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textCNN.loss_val,textCNN.logits,textCNN.accuracy],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a02_TextCNN/other_experiement/p7_TextCNN_train_exp512.py,27,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p7_TextCNN_model import TextCNN\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1024, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 3000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.8, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\n#tf.app.flags.DEFINE_integer(""num_sampled"",50,""number of noise sampling"") #100\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",35,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_integer(""num_filters"", 512, ""number of filters"") #256--->512-->\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\nfilter_sizes=[3,4,5,7,10,15,20,25] #[1,2,3,4,5,6,7]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""cnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #test-zhihu5-only-title-multilabel.txt\n        train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                        FLAGS.decay_rate,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textCNN,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textCNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        best_loss=10000\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {textCNN.input_x: trainX[start:end],textCNN.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[textCNN.input_y] = trainY[start:end]\n                else:\n                    feed_dict[textCNN.input_y_multilabel]=trainY[start:end]\n                curr_loss,curr_acc,_=sess.run([textCNN.loss_val,textCNN.accuracy,textCNN.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %100==0:\n                    print(""Exp512:Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textCNN.epoch_increment)\n\n            # 4.validation\n            print(epoch,FLAGS.validate_every,(epoch % FLAGS.validate_every==0))\n            if epoch % FLAGS.validate_every==0:\n                eval_loss, eval_acc=do_eval(sess,textCNN,testX,testY,batch_size,vocabulary_index2word_label)\n                print(""Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch,eval_loss,eval_acc))\n                #save model to checkpoint\n                if curr_loss<best_loss:\n                    save_path=FLAGS.ckpt_dir+""model.ckpt""\n                    saver.save(sess,save_path,global_step=epoch)\n                    best_loss=curr_loss\n\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, textCNN, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textCNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textCNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[textCNN.input_y] = evalY[start:end]\n        else:\n            feed_dict[textCNN.input_y_multilabel] = evalY[start:end]\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textCNN.loss_val,textCNN.logits,textCNN.accuracy],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a02_TextCNN/other_experiement/p7_TextCNN_train_exp_512_0609.py,29,"b'# -*- coding: utf-8 -*-\n#training the model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\nfrom p7_TextCNN_model import TextCNN\nfrom data_util_zhihu import load_data_multilabel_new,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import to_categorical, pad_sequences\nimport os\nimport word2vec\nimport pickle\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.05,""learning rate"")#0.01\ntf.app.flags.DEFINE_integer(""batch_size"", 256, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 12000, ""how many steps before decay learning rate."") #6000\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 1.0, ""Rate of decay for learning rate."") #0.65\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\n#tf.app.flags.DEFINE_integer(""num_sampled"",50,""number of noise sampling"") #100\ntf.app.flags.DEFINE_string(""ckpt_dir"",""checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_0609/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",60,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",True,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",30,""number of epochs to run."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_integer(""validate_step"", 2000, ""how many step to validate."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_boolean(""use_embedding"",True,""whether to use embedding or not."")\n#tf.app.flags.DEFINE_string(""cache_path"",""text_cnn_checkpoint/data_cache.pik"",""checkpoint location for the model"")\n#train-zhihu4-only-title-all.txt\ntf.app.flags.DEFINE_string(""traning_data_path"",""train-zhihu4-only-title-all.txt"",""path of traning data."") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\ntf.app.flags.DEFINE_integer(""num_filters"", 512, ""number of filters"") #256--->512--->600\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\ntf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\ntf.app.flags.DEFINE_integer(""fz"",6,""filter size"") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->\'training-data/train-zhihu5-only-title-multilabel.txt\'\n\nfilter_sizes=[3,4,5,6,7,8,9,10,15,20,25] #[1,2,3,4,5,6,7,8,9,10]#[1,2,3,4,5,6,7,8,9]#[5,6,7,8,9] #[2,3,5,6,7,8]#[3,4,5,7,10,15,20,25] #[1,2,3,4,5,6,7][3,5,7]#[7,8,9,10,15,20,25] #[3,4,5,7,10,15,20,25]-->[6,7,8,10,15,20,25,30,35]BAD EPOCH2:13.2  #\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n\ndef main(_):\n    #1.load data(X:list of lint,y:int).\n    #if os.path.exists(FLAGS.cache_path):  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\xad\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\x85\xe4\xba\x8b\xef\xbc\x88\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x89\n    #    with open(FLAGS.cache_path, \'r\') as data_f:\n    #        trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n    #        vocab_size=len(vocabulary_index2word)\n    #else:\n    if 1==1:\n        trainX, trainY, testX, testY = None, None, None, None\n        vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"") #simple=\'simple\'\n        vocab_size = len(vocabulary_word2index)\n        print(""cnn_model.vocab_size:"",vocab_size)\n        vocabulary_word2index_label,vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n        if FLAGS.multi_label_flag:\n            FLAGS.traning_data_path=\'training-data/train-zhihu6-title-desc.txt\' #test-zhihu5-only-title-multilabel.txt\n        train, test, _ = load_data_multilabel_new(vocabulary_word2index, vocabulary_word2index_label,multi_label_flag=FLAGS.multi_label_flag,traning_data_path=FLAGS.traning_data_path) #,traning_data_path=FLAGS.traning_data_path\n        trainX, trainY = train\n        testX, testY = test\n        # 2.Data preprocessing.Sequence padding\n        print(""start padding & transform to one hot..."")\n        trainX = pad_sequences(trainX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        testX = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n        #with open(FLAGS.cache_path, \'w\') as data_f: #save data to cache file, so we can use it next time quickly.\n        #    pickle.dump((trainX,trainY,testX,testY,vocabulary_index2word),data_f)\n        print(""trainX[0]:"", trainX[0]) #;print(""trainY[0]:"", trainY[0])\n        # Converting labels to binary vectors\n        print(""end padding & transform to one hot..."")\n    #2.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        #Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,\n                        FLAGS.decay_rate,FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training,multi_label_flag=FLAGS.multi_label_flag)\n        #Initialize Save\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(\'Initializing Variables\')\n            sess.run(tf.global_variables_initializer())\n            if FLAGS.use_embedding: #load pre-trained word embedding\n                assign_pretrained_word_embedding(sess, vocabulary_index2word, vocab_size, textCNN,word2vec_model_path=FLAGS.word2vec_model_path)\n        curr_epoch=sess.run(textCNN.epoch_step)\n        #3.feed data & training\n        number_of_training_data=len(trainX)\n        batch_size=FLAGS.batch_size\n        previous_eval_loss=10000\n        best_eval_loss=10000\n        for epoch in range(curr_epoch,FLAGS.num_epochs):\n            loss, acc, counter = 0.0, 0.0, 0\n            for start, end in zip(range(0, number_of_training_data, batch_size),range(batch_size, number_of_training_data, batch_size)):\n                if epoch==0 and counter==0:\n                    print(""trainX[start:end]:"",trainX[start:end])#;print(""trainY[start:end]:"",trainY[start:end])\n                feed_dict = {textCNN.input_x: trainX[start:end],textCNN.dropout_keep_prob: 0.5}\n                if not FLAGS.multi_label_flag:\n                    feed_dict[textCNN.input_y] = trainY[start:end]\n                else:\n                    feed_dict[textCNN.input_y_multilabel]=trainY[start:end]\n                curr_loss,curr_acc,_=sess.run([textCNN.loss_val,textCNN.accuracy,textCNN.train_op],feed_dict) #curr_acc--->TextCNN.accuracy\n                loss,counter,acc=loss+curr_loss,counter+1,acc+curr_acc\n                if counter %50==0:\n                    print(""Exp512-0609-fitersize[3,4,5,6,7,8,9,10,15,20,25]:Epoch %d\\tBatch %d\\tTrain Loss:%.3f\\tTrain Accuracy:%.3f"" %(epoch,counter,loss/float(counter),acc/float(counter))) #tTrain Accuracy:%.3f---\xe3\x80\x8bacc/float(counter)\n\n                # 4.validation\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n                if  start%(FLAGS.validate_step*FLAGS.batch_size)==0: #if epoch % FLAGS.validate_every == 0: #(epoch % FLAGS.validate_every) or\n                    eval_loss, eval_acc = do_eval(sess, textCNN, testX, testY, batch_size,vocabulary_index2word_label)\n                    print(""Exp512-0609[[3,4,5,6,7,8,9,10,15,20,25]]:validation.part. previous_eval_loss:"", previous_eval_loss,"";current_eval_loss:"", eval_loss)\n                    if eval_loss > previous_eval_loss: #if loss is not decreasing\n                        # reduce the learning rate by a factor of 0.5\n                        print(""Exp512-0609[[3,4,5,6,7,8,9,10,15,20,25]]:validation.part.going to reduce the learning rate."")\n                        learning_rate1 = sess.run(textCNN.learning_rate)\n                        lrr=sess.run([textCNN.learning_rate_decay_half_op])\n                        learning_rate2 = sess.run(textCNN.learning_rate)\n                        print(""Exp512-0609[[3,4,5,6,7,8,9,10,15,20,25]]:validation.part.learning_rate1:"", learning_rate1, "" ;learning_rate2:"",learning_rate2)\n                    #print(""HierAtten==>Epoch %d Validation Loss:%.3f\\tValidation Accuracy: %.3f"" % (epoch, eval_loss, eval_acc))\n                    else:# loss is decreasing\n                        if eval_loss<best_eval_loss:\n                            print(""Exp512-0609[[3,4,5,6,7,8,9,10,15,20,25]]:going to save the model.eval_loss:"",eval_loss,"";best_eval_loss:"",best_eval_loss)\n                            # save model to checkpoint\n                            save_path = FLAGS.ckpt_dir + ""model.ckpt""\n                            saver.save(sess, save_path, global_step=epoch)\n                            best_eval_loss=eval_loss\n                    previous_eval_loss = eval_loss\n                ##VALIDATION VALIDATION VALIDATION PART######################################################################################################\n\n            #epoch increment\n            print(""going to increment epoch counter...."")\n            sess.run(textCNN.epoch_increment)\n        # 5.\xe6\x9c\x80\xe5\x90\x8e\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8a\xa5\xe5\x91\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 Test\n        test_loss, test_acc = do_eval(sess, textCNN, testX, testY, batch_size,vocabulary_index2word_label)\n    pass\n\ndef assign_pretrained_word_embedding(sess,vocabulary_index2word,vocab_size,textCNN,word2vec_model_path=None):\n    print(""using pre-trained word emebedding.started.word2vec_model_path:"",word2vec_model_path)\n    # word2vecc=word2vec.load(\'word_embedding.txt\') #load vocab-vector fiel.word2vecc[\'w91874\']\n    word2vec_model = word2vec.load(word2vec_model_path, kind=\'bin\')\n    word2vec_dict = {}\n    for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n        word2vec_dict[word] = vector\n    word_embedding_2dlist = [[]] * vocab_size  # create an empty word_embedding list.\n    word_embedding_2dlist[0] = np.zeros(FLAGS.embed_size)  # assign empty for first word:\'PAD\'\n    bound = np.sqrt(6.0) / np.sqrt(vocab_size)  # bound for random variables.\n    count_exist = 0;\n    count_not_exist = 0\n    for i in range(1, vocab_size):  # loop each word\n        word = vocabulary_index2word[i]  # get a word\n        embedding = None\n        try:\n            embedding = word2vec_dict[word]  # try to get vector:it is an array.\n        except Exception:\n            embedding = None\n        if embedding is not None:  # the \'word\' exist a embedding\n            word_embedding_2dlist[i] = embedding;\n            count_exist = count_exist + 1  # assign array to this word.\n        else:  # no embedding for this word\n            word_embedding_2dlist[i] = np.random.uniform(-bound, bound, FLAGS.embed_size);\n            count_not_exist = count_not_exist + 1  # init a random value for the word.\n    word_embedding_final = np.array(word_embedding_2dlist)  # covert to 2d array.\n    word_embedding = tf.constant(word_embedding_final, dtype=tf.float32)  # convert to tensor\n    t_assign_embedding = tf.assign(textCNN.Embedding,word_embedding)  # assign this value to our embedding variables of our model.\n    sess.run(t_assign_embedding);\n    print(""word. exists embedding:"", count_exist, "" ;word not exist embedding:"", count_not_exist)\n    print(""using pre-trained word emebedding.ended..."")\n\n# \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe5\x81\x9a\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\x8a\xa5\xe5\x91\x8a\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe7\xb2\xbe\xe7\xa1\xae\xe5\xba\xa6\ndef do_eval(sess,textCNN,evalX,evalY,batch_size,vocabulary_index2word_label):\n    number_examples=len(evalX)\n    eval_loss,eval_acc,eval_counter=0.0,0.0,0\n    for start,end in zip(range(0,number_examples,batch_size),range(batch_size,number_examples,batch_size)):\n        feed_dict = {textCNN.input_x: evalX[start:end], textCNN.dropout_keep_prob: 1}\n        if not FLAGS.multi_label_flag:\n            feed_dict[textCNN.input_y] = evalY[start:end]\n        else:\n            feed_dict[textCNN.input_y_multilabel] = evalY[start:end]\n        curr_eval_loss, logits,curr_eval_acc= sess.run([textCNN.loss_val,textCNN.logits,textCNN.accuracy],feed_dict)#curr_eval_acc--->textCNN.accuracy\n        #label_list_top5 = get_label_using_logits(logits_[0], vocabulary_index2word_label)\n        #curr_eval_acc=calculate_accuracy(list(label_list_top5), evalY[start:end][0],eval_counter)\n        eval_loss,eval_acc,eval_counter=eval_loss+curr_eval_loss,eval_acc+curr_eval_acc,eval_counter+1\n    return eval_loss/float(eval_counter),eval_acc/float(eval_counter)\n\n#\xe4\xbb\x8elogits\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe5\x89\x8d\xe4\xba\x94 get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=1):\n    #print(""get_label_using_logits.logits:"",logits) #1-d array: array([-5.69036102, -8.54903221, -5.63954401, ..., -5.83969498,-5.84496021, -6.13911009], dtype=float32))\n    index_list=np.argsort(logits)[-top_number:]\n    index_list=index_list[::-1]\n    #label_list=[]\n    #for index in index_list:\n    #    label=vocabulary_index2word_label[index]\n    #    label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return index_list\n\n#\xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\ndef calculate_accuracy(labels_predicted, labels,eval_counter):\n    label_nozero=[]\n    #print(""labels:"",labels)\n    labels=list(labels)\n    for index,label in enumerate(labels):\n        if label>0:\n            label_nozero.append(index)\n    if eval_counter<2:\n        print(""labels_predicted:"",labels_predicted,"" ;labels_nozero:"",label_nozero)\n    count = 0\n    label_dict = {x: x for x in label_nozero}\n    for label_predict in labels_predicted:\n        flag = label_dict.get(label_predict, None)\n    if flag is not None:\n        count = count + 1\n    return count / len(labels)\n\nif __name__ == ""__main__"":\n    tf.app.run()'"
a02_TextCNN/other_experiement/p8_TextCNN_predict_exp.py,26,"b'# -*- coding: utf-8 -*-\n#prediction using model.\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\nimport tensorflow as tf\nimport numpy as np\n#from p5_fastTextB_model import fastTextB as fastText\nfrom data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\nfrom tflearn.data_utils import pad_sequences #to_categorical\nimport os\nimport codecs\nfrom p7_TextCNN_model import TextCNN\n\n#configuration\nFLAGS=tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(""num_classes"",1999,""number of label"")\ntf.app.flags.DEFINE_float(""learning_rate"",0.01,""learning rate"")\ntf.app.flags.DEFINE_integer(""batch_size"", 1, ""Batch size for training/evaluating."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_integer(""decay_steps"", 5000, ""how many steps before decay learning rate."") #\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 32-->128\ntf.app.flags.DEFINE_float(""decay_rate"", 0.9, ""Rate of decay for learning rate."") #0.5\xe4\xb8\x80\xe6\xac\xa1\xe8\xa1\xb0\xe5\x87\x8f\xe5\xa4\x9a\xe5\xb0\x91\ntf.app.flags.DEFINE_string(""ckpt_dir"",""text_cnn_title_desc_checkpoint_exp/"",""checkpoint location for the model"")\ntf.app.flags.DEFINE_integer(""sentence_len"",100,""max sentence length"")\ntf.app.flags.DEFINE_integer(""embed_size"",100,""embedding size"")\ntf.app.flags.DEFINE_boolean(""is_training"",False,""is traning.true:tranining,false:testing/inference"")\ntf.app.flags.DEFINE_integer(""num_epochs"",15,""number of epochs."")\ntf.app.flags.DEFINE_integer(""validate_every"", 1, ""Validate every validate_every epochs."") #\xe6\xaf\x8f10\xe8\xbd\xae\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe9\xaa\x8c\xe8\xaf\x81\ntf.app.flags.DEFINE_string(""predict_target_file"",""text_cnn_title_desc_checkpoint_exp/zhihu_result_cnn_multilabel_v6_exp.csv"",""target file path for final prediction"")\ntf.app.flags.DEFINE_string(""predict_source_file"",\'test-zhihu-forpredict-title-desc-v6.txt\',""target file path for final prediction"") #test-zhihu-forpredict-v4only-title.txt\ntf.app.flags.DEFINE_string(""word2vec_model_path"",""zhihu-word2vec-title-desc.bin-100"",""word2vec\'s vocabulary and vectors"") #zhihu-word2vec.bin-100\ntf.app.flags.DEFINE_integer(""num_filters"", 256, ""number of filters"") #128\n#tf.app.flags.DEFINE_boolean(""multi_label_flag"",True,""use multi label or single label."")\n\n##################################################################################################################################\nfilter_sizes=[3,4,5,7,15,20,25]\n#1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.training (5.validation) ,(6.prediction)\n# 1.load data with vocabulary of words and labels\nvocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',\n                                                                word2vec_model_path=FLAGS.word2vec_model_path,\n                                                                name_scope=""cnn2"")\nvocab_size = len(vocabulary_word2index)\nvocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\nquestionid_question_lists = load_final_test_data(FLAGS.predict_source_file)\ntest = load_data_predict(vocabulary_word2index, vocabulary_word2index_label, questionid_question_lists)\ntestX = []\nquestion_id_list = []\nfor tuple in test:\n    question_id, question_string_list = tuple\n    question_id_list.append(question_id)\n    testX.append(question_string_list)\n# 2.Data preprocessing: Sequence padding\nprint(""start padding...."")\ntestX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\nprint(""end padding..."")\n# 3.create session.\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess=tf.Session(config=config)\n# 4.Instantiate Model\ntextCNN = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\n                  FLAGS.decay_steps, FLAGS.decay_rate,\n                  FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training)\nsaver = tf.train.Saver()\nif os.path.exists(FLAGS.ckpt_dir + ""checkpoint""):\n    print(""Restoring Variables from Checkpoint"")\n    saver.restore(sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir))\nelse:\n    print(""Can\'t find the checkpoint.going to stop"")\n    #return\n# 5.feed data, to get logits\nnumber_of_training_data = len(testX2);\nprint(""number_of_training_data:"", number_of_training_data)\n#index = 0\n#predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n##################################################################################################################################\ndef get_logits_by_input_exp(start,end):\n    x=testX2[start:end]\n    logits = sess.run(textCNN.logits, feed_dict={textCNN.input_x: x, textCNN.dropout_keep_prob: 1})\n    predicted_labels,value_labels = get_label_using_logits_with_value(logits[0], vocabulary_index2word_label)\n    value_labels_exp= np.exp(value_labels)\n    p_labels=value_labels_exp/np.sum(value_labels_exp)\n    return predicted_labels,p_labels\n\ndef main(_):\n    # 1.load data with vocabulary of words and labels\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(simple=\'simple\',word2vec_model_path=FLAGS.word2vec_model_path,name_scope=""cnn2"")\n    vocab_size = len(vocabulary_word2index)\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=""cnn2"")\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\n    testX=[]\n    question_id_list=[]\n    for tuple in test:\n        question_id,question_string_list=tuple\n        question_id_list.append(question_id)\n        testX.append(question_string_list)\n    # 2.Data preprocessing: Sequence padding\n    print(""start padding...."")\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length\n    print(""end padding..."")\n   # 3.create session.\n    config=tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n        # 4.Instantiate Model\n        textCNN=TextCNN(filter_sizes,FLAGS.num_filters,FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps,FLAGS.decay_rate,\n                        FLAGS.sentence_len,vocab_size,FLAGS.embed_size,FLAGS.is_training)\n        saver=tf.train.Saver()\n        if os.path.exists(FLAGS.ckpt_dir+""checkpoint""):\n            print(""Restoring Variables from Checkpoint"")\n            saver.restore(sess,tf.train.latest_checkpoint(FLAGS.ckpt_dir))\n        else:\n            print(""Can\'t find the checkpoint.going to stop"")\n            return\n        # 5.feed data, to get logits\n        number_of_training_data=len(testX2);print(""number_of_training_data:"",number_of_training_data)\n        index=0\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, \'a\', \'utf8\')\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\n            logits=sess.run(textCNN.logits,feed_dict={textCNN.input_x:testX2[start:end],textCNN.dropout_keep_prob:1}) #\'shape of logits:\', ( 1, 1999)\n            # 6. get lable using logtis\n            predicted_labels=get_label_using_logits(logits[0],vocabulary_index2word_label)\n            # 7. write question id and labels to file system.\n            write_question_id_with_labels(question_id_list[index],predicted_labels,predict_target_file_f)\n            index=index+1\n        predict_target_file_f.close()\n\n# get label using logits\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n    return label_list\n\n# get label using logits\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\n    index_list=np.argsort(logits)[-top_number:] #print(""sum_p"", np.sum(1.0 / (1 + np.exp(-logits))))\n    index_list=index_list[::-1]\n    value_list=[]\n    label_list=[]\n    for index in index_list:\n        label=vocabulary_index2word_label[index]\n        label_list.append(label) #(\'get_label_using_logits.label_list:\', [u\'-3423450385060590478\', u\'2838091149470021485\', u\'-3174907002942471215\', u\'-1812694399780494968\', u\'6815248286057533876\'])\n        value_list.append(logits[index])\n    return label_list,value_list\n\n# write question id and labels to file system.\ndef write_question_id_with_labels(question_id,labels_list,f):\n    labels_string="","".join(labels_list)\n    f.write(question_id+"",""+labels_string+""\\n"")\n\nif __name__ == ""__main__"":\n    #tf.app.run()\n    labels,list_value=get_logits_by_input_exp(0, 1)\n    print(""labels:"",labels)\n    print(""list_value:"", list_value)'"
data/old/__init__.py,0,b''
