file_path,api_count,code
setup.py,0,"b'import setuptools\n\nsetuptools.setup(\n    author=""Allen Goodman"",\n    author_email=""allen.goodman@icloud.com"",\n    extras_require={\n        ""dev"": [""black>=19.10b0"", ""check-manifest>=0.41"", ""pre-commit>=2.2.0""],\n        ""test"": [\n            ""codecov"",\n            ""mock"",\n            ""pytest"",\n            ""pytest-cov"",\n            ""pytest-pep8"",\n            ""pytest-runner"",\n        ],\n    },\n    install_requires=[\n        ""jsonschema>=3.2.0"",\n        ""keras>=2.3.1"",\n        ""keras-resnet>=0.2.0"",\n        ""scikit-image>=0.17.2"",\n    ],\n    license=""MIT"",\n    name=""keras-rcnn"",\n    package_data={\n        ""keras-rcnn"": [\n            ""data/checkpoints/*/*.hdf5"",\n            ""data/logs/*/*.csv"",\n            ""data/notebooks/*/*.ipynb"",\n        ]\n    },\n    packages=setuptools.find_packages(exclude=[""tests""]),\n    url=""https://github.com/broadinstitute/keras-rcnn"",\n    version=""0.1.0"",\n    zip_safe=True,\n)\n'"
keras_rcnn/__init__.py,0,b''
tests/__init__.py,0,b''
tests/conftest.py,0,"b'import pytest\n\nimport keras_rcnn.datasets.shape\nimport keras_rcnn.layers\nimport keras_rcnn.preprocessing\n\n\n@pytest.fixture()\ndef anchor_layer():\n    return keras_rcnn.layers.Anchor()\n\n\n@pytest.fixture()\ndef generator(training_dictionary):\n    object_detection_generator = keras_rcnn.preprocessing.ObjectDetectionGenerator()\n\n    categories = {""circle"": 1, ""rectangle"": 2, ""triangle"": 3}\n\n    return object_detection_generator.flow_from_dictionary(\n        dictionary=training_dictionary,\n        categories=categories,\n        target_size=(224, 224),\n        shuffle=False,\n    )\n\n\n@pytest.fixture()\ndef training_dictionary():\n    return keras_rcnn.datasets.shape.load_data()[0]\n\n\n@pytest.fixture()\ndef test_dictionary():\n    return keras_rcnn.datasets.shape.load_data()[1]\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# keras-rcnn documentation build configuration file, created by\n# sphinx-quickstart on Mon Apr 24 14:22:30 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\nimport pkg_resources\nimport sphinx_rtd_theme\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""numpydoc"",\n    ""sphinx_gallery.gen_gallery"",\n]\n\nsphinx_gallery_conf = {\n    ""backreferences_dir"": False,\n    ""examples_dirs"": ""../../examples"",\n    ""gallery_dirs"": ""auto_examples"",\n}\n\nautosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = ""keras-rcnn""\ncopyright = ""2017, Allen Goodman""\nauthor = ""Allen Goodman""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = pkg_resources.get_distribution(""keras-rcnn"").version\n# The full version, including alpha/beta/rc tags.\nrelease = pkg_resources.get_distribution(""keras-rcnn"").version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n\nhtml_theme_options = {""navigation_depth"": 2}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""keras-rcnndoc""\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        ""keras-rcnn.tex"",\n        ""keras-rcnn Documentation"",\n        ""Allen Goodman"",\n        ""manual"",\n    ),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""keras-rcnn"", ""keras-rcnn Documentation"", [author], 1)]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""keras-rcnn"",\n        ""keras-rcnn Documentation"",\n        author,\n        ""keras-rcnn"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    ),\n]\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [""search.html""]\n'"
examples/training/nan_and_inf.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nNaN (and Inf)\n=============\n\nA neural network whose layers or losses yield NaN or Inf values are a common\nmachine learning problem. They are especially obnoxious because it\xe2\x80\x99s\ndifficult for experienced and inexperienced users alike to find the source (\nor sources) of the problem. They usually originate from: a bug in Keras-RCNN\n(or a Keras-RCNN dependency), numerical stability problems in a user\xe2\x80\x99s\nruntime environment (like CUDA or a related library), or, most commonly,\na fundamental problem in a model\xe2\x80\x99s architecture or the underlying algorithms\n(like an objective function). In the following example, I\xe2\x80\x99ve summarized the\naforementioned issues alongside techniques to diagnose and resolve them.\n\nHyperparameters\n---------------\n\nInappropriately configured hyperparameters are a common source of NaN and\nInf problems.\n\nLearning rate\n~~~~~~~~~~~~~\n\nInappropriate learning rates are a frequent source of NaN and Inf\nfrustration. A learning rate that\xe2\x80\x99s too large will quickly, if not\nimmediately, yield NaN values.\n\n.. code:: python\n\n    model = keras_rcnn.models.RCNN(\n        categories=categories.keys(),\n        dense_units=512,\n        input_shape=(224, 224, 3)\n    )\n\n    optimizer = keras.optimizers.SGD(0.1)\n\n    model.compile(optimizer)\n\n    model.fit_generator(\n        epochs=1,\n        generator=generator,\n        validation_data=generator\n    )\n\nThe easiest solution to remedy this problem is to use Keras\xe2\x80\x99\nLearningRateScheduler callback to enforce a schedule that steadily decrease\nthe learning rate:\n\n.. code:: python\n\n    import math\n\n    def schedule(epoch_index):\n        return 0.1 * numpy.power(0.5, numpy.floor((1 + epoch_index) / 1.0))\n\n    learning_rates = [schedule(epoch_index) for epoch_index in range(0, 10)]\n\n    matplotlib.pyplot.plot(learning_rates)\n\n    model.fit_generator(\n        callbacks = [\n            keras.callbacks.LearningRateScheduler(schedule)\n        ],\n        epochs=10,\n        generator=generator,\n        validation_data=generator\n    )\n\nYou could also consider writing a custom Keras callback that halves the\nlearning rate until your objective function yields reasonable values:\n\nPenalties\n~~~~~~~~~\n\nOther hyperparameters may also play a role. For example, are your training\nalgorithms involve regularization terms? If so, are their corresponding\npenalties set reasonably? Search a wider hyperparameter space with a few (\none or two) training epochs each to see if the NaNs could disappear.\n\nWeight initialization\n~~~~~~~~~~~~~~~~~~~~~\n\nSome models can be very sensitive to the initialization of weight vectors.\nIf the weights are not properly initialized, then it is not surprising that\nthe model ends up with yielding NaNs.\n\nNumerical stability\n-------------------\n""""""\n\n\ndef main():\n    pass\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/training/object_detection.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nObject detection\n================\n\nA simple example for ploting two figures of a exponential\nfunction in order to test the autonomy of the gallery\nstacking multiple images.\n""""""\n\nimport keras\n\nimport keras_rcnn.datasets.shape\nimport keras_rcnn.models\nimport keras_rcnn.preprocessing\n\n\ndef main():\n    training_dictionary, test_dictionary = keras_rcnn.datasets.shape.load_data()\n\n    categories = {""circle"": 1, ""rectangle"": 2, ""triangle"": 3}\n\n    generator = keras_rcnn.preprocessing.ObjectDetectionGenerator()\n\n    generator = generator.flow_from_dictionary(\n        dictionary=training_dictionary, categories=categories, target_size=(224, 224)\n    )\n\n    validation_data = keras_rcnn.preprocessing.ObjectDetectionGenerator()\n\n    validation_data = validation_data.flow_from_dictionary(\n        dictionary=test_dictionary, categories=categories, target_size=(224, 224)\n    )\n\n    keras.backend.set_learning_phase(1)\n\n    model = keras_rcnn.models.RCNN(\n        categories=[""circle"", ""rectangle"", ""triangle""],\n        dense_units=512,\n        input_shape=(224, 224, 3),\n    )\n\n    optimizer = keras.optimizers.Adam()\n\n    model.compile(optimizer,)\n\n    model.fit_generator(epochs=1, generator=generator, validation_data=validation_data)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/visualization/bounding_boxes.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nBounding boxes\n==============\n\nA simple example for ploting two figures of a exponential\nfunction in order to test the autonomy of the gallery\nstacking multiple images.\n""""""\n\nimport numpy\nimport keras_rcnn.datasets.shape\nimport keras_rcnn.preprocessing\nimport keras_rcnn.utils\n\n\ndef main():\n    training_dictionary, test_dictionary = keras_rcnn.datasets.shape.load_data()\n\n    categories = {""circle"": 1, ""rectangle"": 2, ""triangle"": 3}\n\n    generator = keras_rcnn.preprocessing.ObjectDetectionGenerator()\n\n    generator = generator.flow_from_dictionary(\n        dictionary=training_dictionary,\n        categories=categories,\n        target_size=(224, 224),\n        shuffle=False,\n    )\n\n    target, _ = generator.next()\n\n    target_bounding_boxes, target_categories, target_images, target_masks, _ = target\n\n    target_bounding_boxes = numpy.squeeze(target_bounding_boxes)\n\n    target_images = numpy.squeeze(target_images)\n\n    target_categories = numpy.argmax(target_categories, -1)\n\n    target_categories = numpy.squeeze(target_categories)\n\n    keras_rcnn.utils.show_bounding_boxes(\n        target_images, target_bounding_boxes, target_categories\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
keras_rcnn/applications/__init__.py,0,b'from ._hollandi2019 import Hollandi2019\n\nfrom ._jhung2019 import JHung2019\n'
keras_rcnn/applications/_hollandi2019.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.models\n\n\nclass Hollandi2019(keras_rcnn.models.MaskRCNN):\n    def compile(self, optimizer, **kwargs):\n        super(Hollandi2019, self).compile(optimizer)\n\n        origin = ""http://keras-rcnn.storage.googleapis.com/Hollandi2019.tar.gz""\n\n        pathname = tensorflow.keras.utils.get_file(\n            cache_subdir=""models"", fname=""Hollandi2019"", origin=origin, untar=True,\n        )\n\n        self.load_weights(pathname, by_name=True)\n\n    def predict(self, x, batch_size=None, verbose=0, steps=None, **kwargs):\n        super(Hollandi2019, self).predict(\n            x, batch_size, verbose, steps,\n        )\n'"
keras_rcnn/applications/_jhung2019.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy\nimport tensorflow\n\nimport keras_rcnn.models\n\n\nclass JHung2019(keras_rcnn.models.RCNN):\n    def compile(self, optimizer, **kwargs):\n        super(JHung2019, self).compile(optimizer,)\n\n        origin = (\n            ""http://keras-rcnn-applications.storage.googleapis.com/JHung2019.tar.gz""\n        )\n\n        pathname = tensorflow.keras.utils.get_file(\n            cache_subdir=""models"", fname=""JHung2019.hdf5"", origin=origin, untar=True,\n        )\n\n        self.load_weights(pathname, by_name=True)\n\n    def predict(self, x, batch_size=None, verbose=0, steps=None, **kwargs):\n        prediction = super(JHung2019, self).predict(x, batch_size, verbose, steps,)\n\n        predicted_bounding_boxes, predicted_categories = prediction\n\n        predicted_bounding_boxes = numpy.squeeze(predicted_bounding_boxes)\n\n        predicted_categories = numpy.squeeze(predicted_categories)\n\n        predicted_categories = numpy.argmax(predicted_categories, axis=-1)\n\n        indices = numpy.where(predicted_categories > 0)\n\n        predicted_bounding_boxes = predicted_bounding_boxes[indices]\n\n        predicted_categories = predicted_categories[indices]\n\n        return predicted_bounding_boxes, predicted_categories\n'"
keras_rcnn/backend/__init__.py,0,"b'import os\n\nfrom .common import *\n\n_BACKEND = ""tensorflow""\n\nif ""KERAS_BACKEND"" in os.environ:\n    _backend = os.environ[""KERAS_BACKEND""]\n\n    backends = {\n        ""tensorflow"",\n    }\n\n    assert _backend in backends\n\n    _BACKEND = _backend\n\nif _BACKEND == ""tensorflow"":\n    from .tensorflow_backend import *\nelse:\n    raise ValueError(""Unknown backend: "" + str(_BACKEND))\n'"
keras_rcnn/backend/common.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\n\n\ndef anchor(base_size=16, ratios=None, scales=None):\n    """"""\n    Generates a regular grid of multi-aspect and multi-scale anchor boxes.\n    """"""\n    if ratios is None:\n        ratios = tensorflow.keras.backend.cast(\n            [0.5, 1, 2], tensorflow.keras.backend.floatx()\n        )\n\n    if scales is None:\n        scales = tensorflow.keras.backend.cast(\n            [1, 2, 4, 8, 16], tensorflow.keras.backend.floatx()\n        )\n\n    base_anchor = tensorflow.keras.backend.cast(\n        [-base_size / 2, -base_size / 2, base_size / 2, base_size / 2],\n        tensorflow.keras.backend.floatx(),\n    )\n\n    base_anchor = tensorflow.keras.backend.expand_dims(base_anchor, 0)\n\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n\n    anchors = _scale_enum(ratio_anchors, scales)\n\n    anchors = tensorflow.keras.backend.round(anchors)\n\n    return anchors\n\n\ndef bbox_transform(ex_rois, gt_rois):\n    """"""\n    Args:\n        ex_rois: proposed bounding box coordinates (x1, y1, x2, y2)\n        gt_rois: ground truth bounding box coordinates (x1, y1, x2, y2)\n\n    Returns:\n        Computed bounding-box regression targets for an image.\n    """"""\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0]\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1]\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0]\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1]\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = tensorflow.keras.backend.log(gt_widths / ex_widths)\n    targets_dh = tensorflow.keras.backend.log(gt_heights / ex_heights)\n\n    targets = tensorflow.keras.backend.stack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)\n    )\n\n    targets = tensorflow.keras.backend.transpose(targets)\n\n    return tensorflow.keras.backend.cast(targets, ""float32"")\n\n\ndef clip(boxes, shape):\n    """"""\n    Clips box coordinates to be within the width and height as defined in shape\n\n    """"""\n    indices = tensorflow.keras.backend.tile(\n        tensorflow.keras.backend.arange(0, tensorflow.keras.backend.shape(boxes)[0]),\n        [4],\n    )\n    indices = tensorflow.keras.backend.reshape(indices, (-1, 1))\n    indices = tensorflow.keras.backend.tile(\n        indices, [1, tensorflow.keras.backend.shape(boxes)[1] // 4]\n    )\n    indices = tensorflow.keras.backend.reshape(indices, (-1, 1))\n\n    indices_coords = tensorflow.keras.backend.tile(\n        tensorflow.keras.backend.arange(\n            0, tensorflow.keras.backend.shape(boxes)[1], step=4\n        ),\n        [tensorflow.keras.backend.shape(boxes)[0]],\n    )\n    indices_coords = tensorflow.keras.backend.concatenate(\n        [indices_coords, indices_coords + 1, indices_coords + 2, indices_coords + 3], 0\n    )\n    indices = tensorflow.keras.backend.concatenate(\n        [indices, tensorflow.keras.backend.expand_dims(indices_coords)], axis=1\n    )\n\n    updates = tensorflow.keras.backend.concatenate(\n        [\n            tensorflow.keras.backend.maximum(\n                tensorflow.keras.backend.minimum(boxes[:, 0::4], shape[1] - 1), 0\n            ),\n            tensorflow.keras.backend.maximum(\n                tensorflow.keras.backend.minimum(boxes[:, 1::4], shape[0] - 1), 0\n            ),\n            tensorflow.keras.backend.maximum(\n                tensorflow.keras.backend.minimum(boxes[:, 2::4], shape[1] - 1), 0\n            ),\n            tensorflow.keras.backend.maximum(\n                tensorflow.keras.backend.minimum(boxes[:, 3::4], shape[0] - 1), 0\n            ),\n        ],\n        axis=0,\n    )\n    updates = tensorflow.keras.backend.reshape(updates, (-1,))\n    pred_boxes = keras_rcnn.backend.scatter_add_tensor(\n        tensorflow.keras.backend.zeros_like(boxes), indices, updates\n    )\n\n    return pred_boxes\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n    x_ctr = tensorflow.keras.backend.reshape(x_ctr, (-1, 1))\n    y_ctr = tensorflow.keras.backend.reshape(y_ctr, (-1, 1))\n\n    col1 = tensorflow.keras.backend.reshape(x_ctr - 0.5 * ws, (-1, 1))\n    col2 = tensorflow.keras.backend.reshape(y_ctr - 0.5 * hs, (-1, 1))\n    col3 = tensorflow.keras.backend.reshape(x_ctr + 0.5 * ws, (-1, 1))\n    col4 = tensorflow.keras.backend.reshape(y_ctr + 0.5 * hs, (-1, 1))\n    anchors = tensorflow.keras.backend.concatenate((col1, col2, col3, col4), axis=1)\n\n    return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size * ratios\n    ws = tensorflow.keras.backend.sqrt(size_ratios)\n    hs = ws / ratios\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = tensorflow.keras.backend.expand_dims(w, 1) * scales\n    hs = tensorflow.keras.backend.expand_dims(h, 1) * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n    w = anchor[:, 2] - anchor[:, 0]\n    h = anchor[:, 3] - anchor[:, 1]\n    x_ctr = anchor[:, 0] + 0.5 * w\n    y_ctr = anchor[:, 1] + 0.5 * h\n    return w, h, x_ctr, y_ctr\n\n\ndef shift(shape, stride, base_size=16, ratios=None, scales=None):\n    """"""\n    Produce shifted anchors based on shape of the map and stride size\n    """"""\n    shift_x = tensorflow.keras.backend.arange(0, shape[0] * stride, stride)\n    shift_y = tensorflow.keras.backend.arange(0, shape[1] * stride, stride)\n\n    shift_x, shift_y = keras_rcnn.backend.meshgrid(shift_x, shift_y)\n\n    shift_x = tensorflow.keras.backend.reshape(shift_x, [-1])\n    shift_y = tensorflow.keras.backend.reshape(shift_y, [-1])\n\n    shifts = tensorflow.keras.backend.stack(\n        [shift_x, shift_y, shift_x, shift_y], axis=0\n    )\n\n    shifts = tensorflow.keras.backend.transpose(shifts)\n\n    anchors = anchor(base_size=base_size, ratios=ratios, scales=scales)\n\n    number_of_anchors = tensorflow.keras.backend.shape(anchors)[0]\n\n    k = tensorflow.keras.backend.shape(shifts)[\n        0\n    ]  # number of base points = feat_h * feat_w\n\n    shifted_anchors = tensorflow.keras.backend.reshape(\n        anchors, [1, number_of_anchors, 4]\n    )\n\n    b = tensorflow.keras.backend.cast(\n        tensorflow.keras.backend.reshape(shifts, [k, 1, 4]),\n        tensorflow.keras.backend.floatx(),\n    )\n\n    shifted_anchors = shifted_anchors + b\n\n    shifted_anchors = tensorflow.keras.backend.reshape(\n        shifted_anchors, [k * number_of_anchors, 4]\n    )\n\n    return shifted_anchors\n\n\ndef intersection_over_union(output, target):\n    """"""\n    Parameters\n    ----------\n    output: (N, 4) ndarray of float\n    target: (K, 4) ndarray of float\n\n    Returns\n    -------\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    intersection_area = (target[:, 2] - target[:, 0] + 1) * (\n        target[:, 3] - target[:, 1] + 1\n    )\n\n    intersection_c_minimum = tensorflow.keras.backend.minimum(\n        tensorflow.keras.backend.expand_dims(output[:, 2], 1), target[:, 2]\n    )\n    intersection_c_maximum = tensorflow.keras.backend.maximum(\n        tensorflow.keras.backend.expand_dims(output[:, 0], 1), target[:, 0]\n    )\n\n    intersection_r_minimum = tensorflow.keras.backend.minimum(\n        tensorflow.keras.backend.expand_dims(output[:, 3], 1), target[:, 3]\n    )\n    intersection_r_maximum = tensorflow.keras.backend.maximum(\n        tensorflow.keras.backend.expand_dims(output[:, 1], 1), target[:, 1]\n    )\n\n    intersection_c = intersection_c_minimum - intersection_c_maximum + 1\n    intersection_r = intersection_r_minimum - intersection_r_maximum + 1\n\n    intersection_c = tensorflow.keras.backend.maximum(intersection_c, 0)\n    intersection_r = tensorflow.keras.backend.maximum(intersection_r, 0)\n\n    union_area = (\n        tensorflow.keras.backend.expand_dims(\n            (output[:, 2] - output[:, 0] + 1) * (output[:, 3] - output[:, 1] + 1), 1\n        )\n        + intersection_area\n        - intersection_c * intersection_r\n    )\n\n    union_area = tensorflow.keras.backend.maximum(\n        union_area, tensorflow.keras.backend.epsilon()\n    )\n\n    intersection_area = intersection_c * intersection_r\n\n    return intersection_area / union_area\n\n\ndef smooth_l1(output, target, anchored=False, weights=None):\n    difference = tensorflow.keras.backend.abs(output - target)\n\n    p = difference < 1\n    q = 0.5 * tensorflow.keras.backend.square(difference)\n    r = difference - 0.5\n\n    difference = tensorflow.keras.backend.switch(p, q, r)\n\n    loss = tensorflow.keras.backend.sum(difference, axis=2)\n\n    if weights is not None:\n        loss *= weights\n\n    if anchored:\n        return loss\n\n    return tensorflow.keras.backend.sum(loss)\n\n\ndef focal_loss(target, output, gamma=2):\n    output /= tensorflow.keras.backend.sum(output, axis=-1, keepdims=True)\n\n    epsilon = tensorflow.keras.backend.epsilon()\n\n    output = tensorflow.keras.backend.clip(output, epsilon, 1.0 - epsilon)\n\n    loss = tensorflow.keras.backend.pow(1.0 - output, gamma)\n\n    output = tensorflow.keras.backend.log(output)\n\n    loss = -tensorflow.keras.backend.sum(loss * target * output, axis=-1)\n\n    return loss\n\n\ndef softmax_classification(target, output, anchored=False, weights=None):\n    classes = tensorflow.keras.backend.int_shape(output)[-1]\n\n    target = tensorflow.keras.backend.reshape(target, [-1, classes])\n    output = tensorflow.keras.backend.reshape(output, [-1, classes])\n\n    loss = tensorflow.keras.backend.categorical_crossentropy(\n        target, output, from_logits=False\n    )\n\n    if anchored:\n        if weights is not None:\n            loss = tensorflow.keras.backend.reshape(\n                loss, tensorflow.keras.backend.shape(weights)\n            )\n\n            loss *= weights\n\n        return loss\n\n    if weights is not None:\n        loss *= tensorflow.keras.backend.reshape(weights, [-1])\n\n    return loss\n\n\ndef bbox_transform_inv(boxes, deltas):\n    """"""\n\n    Args:\n        boxes: roi or proposal coordinates\n        deltas: regression targets\n\n    Returns:\n        coordinates as a result of deltas applied to boxes (should be equal to ground truth)\n    """"""\n\n    a = boxes[:, 2] - boxes[:, 0]\n    b = boxes[:, 3] - boxes[:, 1]\n\n    ctr_x = boxes[:, 0] + 0.5 * a\n    ctr_y = boxes[:, 1] + 0.5 * b\n\n    dx = deltas[:, 0::4]\n    dy = deltas[:, 1::4]\n    dw = deltas[:, 2::4]\n    dh = deltas[:, 3::4]\n\n    pred_ctr_x = (\n        dx * a[:, keras_rcnn.backend.newaxis] + ctr_x[:, keras_rcnn.backend.newaxis]\n    )\n    pred_ctr_y = (\n        dy * b[:, keras_rcnn.backend.newaxis] + ctr_y[:, keras_rcnn.backend.newaxis]\n    )\n\n    pred_w = tensorflow.keras.backend.exp(dw) * a[:, keras_rcnn.backend.newaxis]\n    pred_h = tensorflow.keras.backend.exp(dh) * b[:, keras_rcnn.backend.newaxis]\n\n    indices = tensorflow.keras.backend.tile(\n        tensorflow.keras.backend.arange(0, tensorflow.keras.backend.shape(deltas)[0]),\n        [4],\n    )\n    indices = tensorflow.keras.backend.reshape(indices, (-1, 1))\n    indices = tensorflow.keras.backend.tile(\n        indices, [1, tensorflow.keras.backend.shape(deltas)[-1] // 4]\n    )\n    indices = tensorflow.keras.backend.reshape(indices, (-1, 1))\n    indices_coords = tensorflow.keras.backend.tile(\n        tensorflow.keras.backend.arange(\n            0, tensorflow.keras.backend.shape(deltas)[1], step=4\n        ),\n        [tensorflow.keras.backend.shape(deltas)[0]],\n    )\n    indices_coords = tensorflow.keras.backend.concatenate(\n        [indices_coords, indices_coords + 1, indices_coords + 2, indices_coords + 3], 0\n    )\n    indices = tensorflow.keras.backend.concatenate(\n        [indices, tensorflow.keras.backend.expand_dims(indices_coords)], axis=1\n    )\n\n    updates = tensorflow.keras.backend.concatenate(\n        [\n            tensorflow.keras.backend.reshape(pred_ctr_x - 0.5 * pred_w, (-1,)),\n            tensorflow.keras.backend.reshape(pred_ctr_y - 0.5 * pred_h, (-1,)),\n            tensorflow.keras.backend.reshape(pred_ctr_x + 0.5 * pred_w, (-1,)),\n            tensorflow.keras.backend.reshape(pred_ctr_y + 0.5 * pred_h, (-1,)),\n        ],\n        axis=0,\n    )\n    pred_boxes = keras_rcnn.backend.scatter_add_tensor(\n        tensorflow.keras.backend.zeros_like(deltas), indices, updates\n    )\n\n    return pred_boxes\n'"
keras_rcnn/backend/tensorflow_backend.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\n\ndef resize(image, output_shape):\n    """"""\n    Resize an image or images to match a certain size.\n\n    :param image: Input image or images with the shape:\n\n        (rows, columns, channels)\n\n    or:\n\n        (batch, rows, columns, channels).\n\n    :param output_shape: Shape of the output image:\n\n        (rows, columns).\n\n    :return: If an image is provided a resized image with the shape:\n\n        (resized rows, resized columns, channels)\n\n    is returned.\n\n    If more than one image is provided then resized images with the shape:\n\n        (batch size, resized rows, resized columns, channels)\n\n    are returned.\n    """"""\n    return tensorflow.compat.v1.image.resize_images(image, output_shape)\n\n\ndef transpose(x, axes=None):\n    """"""\n    Permute the dimensions of an array.\n    """"""\n    return tensorflow.compat.v1.transpose(x, axes)\n\n\ndef shuffle(x):\n    """"""\n    Modify a sequence by shuffling its contents. This function only shuffles\n    the array along the first axis of a multi-dimensional array. The order of\n    sub-arrays is changed but their contents remains the same.\n    """"""\n    return tensorflow.compat.v1.random_shuffle(x)\n\n\ndef gather_nd(params, indices):\n    return tensorflow.compat.v1.gather_nd(params, indices)\n\n\ndef matmul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n):\n    return tensorflow.compat.v1.matmul(\n        a,\n        b,\n        transpose_a=transpose_a,\n        transpose_b=transpose_b,\n        adjoint_a=adjoint_a,\n        adjoint_b=adjoint_b,\n        a_is_sparse=a_is_sparse,\n        b_is_sparse=b_is_sparse,\n    )\n\n\n# TODO: emulate NumPy semantics\ndef argsort(a):\n    _, indices = tensorflow.compat.v1.nn.top_k(a, tensorflow.keras.backend.shape(a)[-1])\n\n    return indices\n\n\ndef scatter_add_tensor(ref, indices, updates, name=None):\n    """"""\n    Adds sparse updates to a variable reference.\n\n    This operation outputs ref after the update is done. This makes it\n    easier to chain operations that need to use the reset value.\n\n    Duplicate indices: if multiple indices reference the same location,\n    their contributions add.\n\n    Requires updates.shape = indices.shape + ref.shape[1:].\n\n    :param ref: A Tensor. Must be one of the following types: float32,\n    float64, int64, int32, uint8, uint16, int16, int8, complex64, complex128,\n    qint8, quint8, qint32, half.\n\n    :param indices: A Tensor. Must be one of the following types: int32,\n    int64. A tensor of indices into the first dimension of ref.\n\n    :param updates: A Tensor. Must have the same dtype as ref. A tensor of\n    updated values to add to ref\n\n    :param name: A name for the operation (optional).\n\n    :return: Same as ref. Returned as a convenience for operations that want\n    to use the updated values after the update is done.\n    """"""\n    return tensorflow.tensor_scatter_nd_add(ref, indices, updates, name)\n\n\ndef meshgrid(*args, **kwargs):\n    return tensorflow.compat.v1.meshgrid(*args, **kwargs)\n\n\nnewaxis = tensorflow.newaxis\n\n\ndef where(condition, x=None, y=None):\n    return tensorflow.compat.v1.where(condition, x, y)\n\n\ndef non_maximum_suppression(boxes, scores, maximum, threshold=0.5):\n    return tensorflow.compat.v1.image.non_max_suppression(\n        boxes=boxes, iou_threshold=threshold, max_output_size=maximum, scores=scores\n    )\n\n\ndef crop_and_resize(image, boxes, size):\n    """"""Crop the image given boxes and resize with bilinear interplotation.\n    # Parameters\n    image: Input image of shape (1, image_height, image_width, depth)\n    boxes: Regions of interest of shape (num_boxes, 4),\n    each row [y1, x1, y2, x2]\n    size: Fixed size [h, w], e.g. [7, 7], for the output slices.\n    # Returns\n    4D Tensor (number of regions, slice_height, slice_width, channels)\n    """"""\n\n    box_ind = tensorflow.keras.backend.zeros_like(boxes, ""int32"")\n    box_ind = box_ind[:, 0]\n    box_ind = tensorflow.keras.backend.reshape(box_ind, [-1])\n\n    boxes = tensorflow.keras.backend.cast(\n        tensorflow.keras.backend.reshape(boxes, [-1, 4]), ""float32""\n    )\n\n    return tensorflow.compat.v1.image.crop_and_resize(image, boxes, box_ind, size)\n\n\ndef smooth_l1(output, target, anchored=False, weights=None):\n    difference = tensorflow.keras.backend.abs(output - target)\n\n    p = difference < 1\n    q = 0.5 * tensorflow.keras.backend.square(difference)\n    r = difference - 0.5\n\n    difference = tensorflow.compat.v1.where(p, q, r)\n\n    loss = tensorflow.keras.backend.sum(difference, axis=2)\n\n    if weights is not None:\n        loss *= weights\n\n    if anchored:\n        return loss\n\n    return tensorflow.keras.backend.sum(loss)\n\n\ndef squeeze(a, axis=None):\n    """"""\n    Remove single-dimensional entries from the shape of an array.\n    """"""\n    return tensorflow.squeeze(a, axis)\n\n\ndef unique(x, return_index=False):\n    """"""\n    Find the unique elements of an array.\n\n    Returns the sorted unique elements of an array. There are three optional\n    outputs in addition to the unique elements: the indices of the input array\n    that give the unique values, the indices of the unique array that\n    reconstruct the input array, and the number of times each unique value\n    comes up in the input array.\n    """"""\n    y, indices = tensorflow.compat.v1.unique(x)\n\n    if return_index:\n        return y, indices\n    else:\n        return y\n\n\ndef pad(x, pad_width, mode):\n    return tensorflow.compat.v1.pad(x, pad_width, mode)\n'"
keras_rcnn/callbacks/__init__.py,0,b'from ._tensorboard import TensorBoard\n'
keras_rcnn/callbacks/_tensorboard.py,0,"b'import io\n\nimport matplotlib.pyplot\nimport numpy\nimport tensorflow\n\nimport keras_rcnn.utils\n\n\n# TODO: use skimage.draw to circumvent matplotlib\ndef _generate_image(image, bounding_boxes):\n    figure = matplotlib.pyplot.figure()\n\n    axis = figure.gca()\n\n    axis.set_axis_off()\n\n    bbox_inches = axis.get_window_extent().transformed(\n        matplotlib.pyplot.gcf().dpi_scale_trans.inverted()\n    )\n\n    keras_rcnn.utils.show_bounding_boxes(image, bounding_boxes)\n\n    buffer = io.BytesIO()\n\n    matplotlib.pyplot.savefig(buffer, bbox_inches=bbox_inches)\n\n    buffer.seek(0)\n\n    return buffer\n\n\nclass TensorBoard(tensorflow.keras.callbacks.TensorBoard):\n    """"""\n\n    """"""\n\n    def __init__(self, generator):\n        self.generator = generator\n\n        super(TensorBoard, self).__init__()\n\n    def on_epoch_end(self, epoch, logs=None):\n        score = 0.0\n\n        summary = tensorflow.Summary()\n\n        summary_value = summary.value.add()\n\n        summary_value.simple_value = score\n\n        summary_value.tag = ""mean average precision (mAP)""\n\n        self.writer.add_summary(summary, epoch)\n\n        summary = self._summarize_image()\n\n        summary = tensorflow.keras.backend.eval(summary)\n\n        self.writer.add_summary(summary)\n\n    def _summarize_image(self):\n        shape = (self.generator.n, *self.generator.target_size, self.generator.channels)\n\n        images = numpy.zeros((self.generator.n, 224, 341, 4))\n\n        for generator_index in range(self.generator.n):\n            x, _ = self.generator.next()\n\n            target_bounding_boxes, _, target_images, _, _ = x\n\n            buffer = _generate_image(target_images[0], target_bounding_boxes[0])\n\n            image = tensorflow.image.decode_png(buffer.getvalue(), channels=4)\n\n            image = tensorflow.expand_dims(image, 0)\n\n            image = tensorflow.keras.backend.eval(image)\n\n            images[generator_index] = image\n\n        return tensorflow.summary.image(""training"", images)\n'"
keras_rcnn/datasets/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport os\n\nimport tensorflow\n\n\ndef load_data(name):\n    origin = ""http://keras-rcnn.storage.googleapis.com/{}.tar.gz"".format(name)\n\n    pathname = tensorflow.keras.utils.get_file(fname=name, origin=origin, untar=True)\n\n    filename = os.path.join(pathname, ""training.json"")\n\n    training = get_file_data(filename, pathname)\n\n    filename = os.path.join(pathname, ""test.json"")\n\n    test = get_file_data(filename, pathname)\n\n    return training, test\n\n\ndef get_file_data(filename, image_path):\n    if os.path.exists(filename):\n        with open(filename) as data:\n            partition = json.load(data)\n    else:\n        partition = []\n\n    for dictionary in partition:\n\n        dictionary[""image""][""pathname""] = image_path + dictionary[""image""][""pathname""]\n\n    return partition\n'"
keras_rcnn/datasets/dsb2018.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\nimport os.path\n\nimport tensorflow\n\n\ndef load_data():\n    origin = ""http://keras-rcnn.storage.googleapis.com/{}.tar.gz"".format(""DSB2018"")\n\n    pathname = tensorflow.keras.utils.get_file(\n        fname=""DSB2018"", origin=origin, untar=True\n    )\n\n    filename = os.path.join(pathname, ""training.json"")\n\n    if os.path.exists(filename):\n        with open(filename) as data:\n            training_dictionaries = json.load(data)\n    else:\n        training_dictionaries = None\n\n    if training_dictionaries:\n        for training_dictionary in training_dictionaries:\n            training_dictionary[""image""][\n                ""pathname""\n            ] = f""{pathname}{training_dictionary[\'image\'][\'pathname\']}""\n\n            for instance in training_dictionary[""objects""]:\n                instance[""mask""][\n                    ""pathname""\n                ] = f""{pathname}{instance[\'mask\'][\'pathname\']}""\n\n    filename = os.path.join(pathname, ""test.json"")\n\n    if os.path.exists(filename):\n        with open(filename) as data:\n            test_dictionaries = json.load(data)\n    else:\n        test_dictionaries = None\n\n    if test_dictionaries:\n        for test_dictionary in test_dictionaries:\n            test_dictionary[""image""][\n                ""pathname""\n            ] = f""{pathname}{training_dictionary[\'image\'][\'pathname\']}""\n\n            for instance in test_dictionary[""objects""]:\n                instance[""mask""][\n                    ""pathname""\n                ] = f""{pathname}{instance[\'mask\'][\'pathname\']}""\n\n    return training_dictionaries, test_dictionaries\n'"
keras_rcnn/datasets/malaria_phenotypes.py,0,"b'# -*- coding: utf-8 -*-\n\nimport keras_rcnn.datasets\n\n\ndef load_data():\n    """"""\n    Load Hung, et al.\xe2\x80\x99s malaria dataset.\n\n    Hung, et al.\xe2\x80\x99s malaria dataset is a collection of blood smears from\n    multiple patients captured by a variety of microscopes. Images are\n    accompanied by bounding boxes that capture each cell\xe2\x80\x99s location and\n    corresponding class labels that describe each cell\xe2\x80\x99s phenotype.\n    """"""\n\n    return keras_rcnn.datasets.load_data(""malaria_phenotypes"")\n'"
keras_rcnn/datasets/nuclei.py,0,"b'# -*- coding: utf-8 -*-\nimport keras_rcnn.datasets\n\n""""""\nLoad Broad Institute\xe2\x80\x99s nuclei dataset.\n\nBroad Institute\xe2\x80\x99s nuclei dataset is a collection of images from the BBBC022 \ndata set of 100 manually annotated images (~10,000 nuclei) taken from a \ncompound profiling experiment of human U2OS cells. \nThe cells were stained with Hoechst 33342 to highlight their DNA.\n""""""\n\n\ndef load_data():\n    return keras_rcnn.datasets.load_data(""nuclei"")\n'"
keras_rcnn/datasets/pascal.py,0,"b'# -*- coding: utf-8 -*-\n\nimport keras_rcnn.datasets\n\n\ndef load_data():\n    """"""\n    Load Everingham, et al.\xe2\x80\x99s PASCAL Visual Object Classes (VOC) dataset.\n    """"""\n    return keras_rcnn.datasets.load_data(""pascal"")\n'"
keras_rcnn/datasets/shape.py,0,"b'# -*- coding: utf-8 -*-\n\nimport json\n\nimport jsonschema\nimport pkg_resources\n\n\ndef load_data():\n    resource_path = ""/"".join([""data"", ""schema.json""])\n\n    with open(pkg_resources.resource_filename(""keras_rcnn"", resource_path)) as stream:\n        schema = json.load(stream)\n\n    resource_path = ""/"".join([""data"", ""shape"", ""training.json""])\n\n    with open(pkg_resources.resource_filename(""keras_rcnn"", resource_path)) as stream:\n        training_dictionary = json.load(stream)\n\n    jsonschema.validate(training_dictionary, schema)\n\n    for dictionary in training_dictionary:\n        resource_path = ""/"".join([""data"", ""shape"", dictionary[""image""][""pathname""]])\n        pathname = pkg_resources.resource_filename(""keras_rcnn"", resource_path)\n        dictionary[""image""][""pathname""] = pathname\n\n    resource_path = ""/"".join([""data"", ""shape"", ""test.json""])\n\n    with open(pkg_resources.resource_filename(""keras_rcnn"", resource_path)) as stream:\n        test_dictionary = json.load(stream)\n\n    jsonschema.validate(test_dictionary, schema)\n\n    for dictionary in test_dictionary:\n        resource_path = ""/"".join([""data"", ""shape"", dictionary[""image""][""pathname""]])\n        pathname = pkg_resources.resource_filename(""keras_rcnn"", resource_path)\n        dictionary[""image""][""pathname""] = pathname\n\n    return training_dictionary, test_dictionary\n'"
keras_rcnn/layers/__init__.py,0,"b'from ._object_detection import ObjectDetection\nfrom ._object_segmentation import ObjectSegmentation\nfrom ._pooling import RegionOfInterest, RegionOfInterestAlignPyramid\nfrom ._upsample import Upsample\nfrom .losses import RCNN, RPN\nfrom .object_detection import Anchor, ObjectProposal, ProposalTarget\n'"
keras_rcnn/layers/_object_detection.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\n\n\nclass ObjectDetection(tensorflow.keras.layers.Layer):\n    def __init__(self, padding=300, **kwargs):\n        self.padding = padding\n\n        super(ObjectDetection, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(ObjectDetection, self).build(input_shape)\n\n    def call(self, x, training=None, **kwargs):\n        """"""\n        # Inputs\n        metadata: image information (1, 3)\n        deltas: predicted deltas (1, N, 4*classes)\n        proposals: output of proposal target (1, N, 4)\n        scores: score distributions (1, N, classes)\n\n        # Returns\n        bounding_boxes: predicted boxes (1, N, 4 * classes)\n        scores: score distribution over all classes (1, N, classes),\n        note the box only corresponds to the most probable class, not the\n        other classes\n        """"""\n\n        metadata, deltas, proposals, scores = x[0], x[1], x[2], x[3]\n\n        bounding_boxes = tensorflow.keras.backend.in_train_phase(\n            proposals,\n            lambda: self.detections(0, metadata, deltas, proposals, scores),\n            training=training,\n        )\n\n        scores = tensorflow.keras.backend.in_train_phase(\n            scores,\n            lambda: self.detections(1, metadata, deltas, proposals, scores),\n            training=training,\n        )\n\n        return [bounding_boxes, scores]\n\n    def compute_output_shape(self, input_shape):\n        return [\n            (1, input_shape[0][0], input_shape[1][2]),\n            (1, input_shape[0][0], input_shape[2][2]),\n        ]\n\n    def compute_mask(self, inputs, mask=None):\n        return 2 * [None]\n\n    def detections(self, num_output, metadata, deltas, proposals, scores):\n        proposals = tensorflow.keras.backend.reshape(proposals, (-1, 4))\n\n        # unscale back to raw image space\n        bounding_boxes = proposals / metadata[0][2]\n\n        num_objects = tensorflow.keras.backend.shape(proposals)[0]\n\n        deltas = tensorflow.keras.backend.reshape(deltas, (num_objects, -1))\n\n        # Apply bounding-box regression deltas\n        predicted_bounding_boxes = keras_rcnn.backend.bbox_transform_inv(\n            bounding_boxes, deltas\n        )\n\n        predicted_bounding_boxes = keras_rcnn.backend.clip(\n            predicted_bounding_boxes, metadata[0][:2]\n        )\n\n        scores = tensorflow.keras.backend.reshape(scores, (num_objects, -1))\n\n        # Arg max\n        inds = tensorflow.keras.backend.expand_dims(\n            tensorflow.keras.backend.arange(0, num_objects, dtype=""int64"")\n        )\n\n        top_classes = tensorflow.keras.backend.expand_dims(\n            tensorflow.keras.backend.argmax(scores, axis=1)\n        )\n\n        coordinate_0 = tensorflow.keras.backend.concatenate([inds, top_classes * 4], 1)\n\n        coordinate_1 = tensorflow.keras.backend.concatenate(\n            [inds, top_classes * 4 + 1], 1\n        )\n\n        coordinate_2 = tensorflow.keras.backend.concatenate(\n            [inds, top_classes * 4 + 2], 1\n        )\n\n        coordinate_3 = tensorflow.keras.backend.concatenate(\n            [inds, top_classes * 4 + 3], 1\n        )\n\n        predicted_bounding_boxes = keras_rcnn.backend.gather_nd(\n            predicted_bounding_boxes,\n            tensorflow.keras.backend.reshape(\n                tensorflow.keras.backend.concatenate(\n                    [coordinate_0, coordinate_1, coordinate_2, coordinate_3], 1\n                ),\n                (-1, 2),\n            ),\n        )\n\n        predicted_bounding_boxes = tensorflow.keras.backend.reshape(\n            predicted_bounding_boxes, (-1, 4)\n        )\n\n        max_scores = tensorflow.keras.backend.max(scores[:, 1:], axis=1)\n\n        suppressed_indices = keras_rcnn.backend.non_maximum_suppression(\n            boxes=predicted_bounding_boxes,\n            scores=max_scores,\n            maximum=num_objects,\n            threshold=0.5,\n        )\n\n        predicted_bounding_boxes = tensorflow.keras.backend.gather(\n            predicted_bounding_boxes, suppressed_indices\n        )\n\n        scores = tensorflow.keras.backend.gather(scores, suppressed_indices)\n\n        predicted_bounding_boxes = tensorflow.keras.backend.expand_dims(\n            predicted_bounding_boxes, 0\n        )\n\n        scores = tensorflow.keras.backend.expand_dims(scores, 0)\n\n        predicted_bounding_boxes = self.pad_bounding_boxes(\n            predicted_bounding_boxes, self.padding\n        )\n\n        scores = self.pad_bounding_boxes(scores, self.padding)\n\n        detections = [predicted_bounding_boxes, scores]\n\n        return detections[num_output]\n\n    @staticmethod\n    def pad_bounding_boxes(x, padding):\n        detections = tensorflow.keras.backend.shape(x)[1]\n\n        difference = padding - detections\n\n        difference = tensorflow.keras.backend.max([0, difference])\n\n        paddings = ((0, 0), (0, difference), (0, 0))\n\n        # TODO: replace with `tensorflow.keras.backend.pad`\n        return tensorflow.pad(x, paddings, mode=""constant"")\n\n    def get_config(self):\n        configuration = {""padding"": self.padding}\n\n        return {**super(ObjectDetection, self).get_config(), **configuration}\n'"
keras_rcnn/layers/_object_segmentation.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\n\n\nclass ObjectSegmentation(tensorflow.keras.layers.Layer):\n    def __init__(self, padding=300, **kwargs):\n        self.padding = padding\n\n        super(ObjectSegmentation, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(ObjectSegmentation, self).build(input_shape)\n\n    def call(self, x, training=None, **kwargs):\n        """"""\n        # Inputs\n        metadata: image information (1, 3)\n        deltas: predicted deltas (1, N, 4*classes)\n        proposals: output of proposal target (1, N, 4)\n        scores: score distributions (1, N, classes)\n\n        # Returns\n        bounding_boxes: predicted boxes (1, N, 4 * classes)\n        scores: score distribution over all classes (1, N, classes),\n        note the box only corresponds to the most probable class, not the\n        other classes\n        """"""\n\n        metadata, deltas, proposals, scores, masks = x[0], x[1], x[2], x[3], x[4]\n\n        bounding_boxes = tensorflow.keras.backend.in_train_phase(\n            proposals,\n            lambda: self.detections(0, metadata, deltas, proposals, scores, masks),\n            training=training,\n        )\n\n        masks = tensorflow.keras.backend.in_train_phase(\n            masks,\n            lambda: self.detections(2, metadata, deltas, proposals, scores, masks),\n            training=training,\n        )\n\n        scores = tensorflow.keras.backend.in_train_phase(\n            scores,\n            lambda: self.detections(1, metadata, deltas, proposals, scores, masks),\n            training=training,\n        )\n\n        return [bounding_boxes, scores, masks]\n\n    def compute_output_shape(self, input_shape):\n        return [\n            (1, input_shape[0][0], input_shape[1][2]),\n            (1, input_shape[0][0], input_shape[2][2]),\n            (\n                1,\n                input_shape[0][0],\n                input_shape[2][2],\n                input_shape[4][2],\n                input_shape[4][3],\n                input_shape[4][4],\n            ),\n        ]\n\n    def compute_mask(self, inputs, mask=None):\n        return 3 * [None]\n\n    def detections(self, num_output, metadata, deltas, proposals, scores, masks):\n        proposals = tensorflow.keras.backend.reshape(proposals, (-1, 4))\n\n        # unscale back to raw image space\n        bounding_boxes = proposals / metadata[0][2]\n\n        num_objects = tensorflow.keras.backend.shape(proposals)[0]\n\n        deltas = tensorflow.keras.backend.reshape(deltas, (num_objects, -1))\n\n        # Apply bounding-box regression deltas\n        predicted_bounding_boxes = keras_rcnn.backend.bbox_transform_inv(\n            bounding_boxes, deltas\n        )\n\n        predicted_bounding_boxes = keras_rcnn.backend.clip(\n            predicted_bounding_boxes, metadata[0][:2]\n        )\n\n        scores = tensorflow.keras.backend.reshape(scores, (num_objects, -1))\n\n        # Arg max\n        inds = tensorflow.keras.backend.expand_dims(\n            tensorflow.keras.backend.arange(0, num_objects, dtype=""int64"")\n        )\n\n        top_classes = tensorflow.keras.backend.expand_dims(\n            tensorflow.keras.backend.argmax(scores, axis=1)\n        )\n\n        coordinate_0 = tensorflow.keras.backend.concatenate([inds, top_classes * 4], 1)\n\n        coordinate_1 = tensorflow.keras.backend.concatenate(\n            [inds, top_classes * 4 + 1], 1\n        )\n\n        coordinate_2 = tensorflow.keras.backend.concatenate(\n            [inds, top_classes * 4 + 2], 1\n        )\n\n        coordinate_3 = tensorflow.keras.backend.concatenate(\n            [inds, top_classes * 4 + 3], 1\n        )\n\n        predicted_bounding_boxes = keras_rcnn.backend.gather_nd(\n            predicted_bounding_boxes,\n            tensorflow.keras.backend.reshape(\n                tensorflow.keras.backend.concatenate(\n                    [coordinate_0, coordinate_1, coordinate_2, coordinate_3], 1\n                ),\n                (-1, 2),\n            ),\n        )\n\n        predicted_bounding_boxes = tensorflow.keras.backend.reshape(\n            predicted_bounding_boxes, (-1, 4)\n        )\n\n        max_scores = tensorflow.keras.backend.max(scores[:, 1:], axis=1)\n\n        suppressed_indices = keras_rcnn.backend.non_maximum_suppression(\n            boxes=predicted_bounding_boxes,\n            scores=max_scores,\n            maximum=num_objects,\n            threshold=0.5,\n        )\n\n        predicted_bounding_boxes = tensorflow.keras.backend.gather(\n            predicted_bounding_boxes, suppressed_indices\n        )\n\n        scores = tensorflow.keras.backend.gather(scores, suppressed_indices)\n\n        predicted_bounding_boxes = tensorflow.keras.backend.expand_dims(\n            predicted_bounding_boxes, 0\n        )\n\n        scores = tensorflow.keras.backend.expand_dims(scores, 0)\n\n        predicted_bounding_boxes = self.pad_bounding_boxes(\n            predicted_bounding_boxes, self.padding\n        )\n\n        scores = self.pad_bounding_boxes(scores, self.padding)\n\n        masks = tensorflow.keras.backend.squeeze(masks, axis=0)\n\n        masks = tensorflow.keras.backend.gather(masks, suppressed_indices)\n\n        masks = tensorflow.keras.backend.expand_dims(masks, axis=0)\n\n        masks = self.pad_masks(masks, self.padding)\n\n        detections = [predicted_bounding_boxes, scores, masks]\n\n        return detections[num_output]\n\n    @staticmethod\n    def pad_bounding_boxes(x, padding):\n        detections = tensorflow.keras.backend.shape(x)[1]\n\n        difference = padding - detections\n\n        difference = tensorflow.keras.backend.max([0, difference])\n\n        paddings = ((0, 0), (0, difference), (0, 0))\n\n        # TODO: replace with `tensorflow.keras.backend.pad`\n        return tensorflow.pad(x, paddings, mode=""constant"")\n\n    @staticmethod\n    def pad_masks(x, padding):\n        detections = tensorflow.keras.backend.shape(x)[1]\n\n        difference = padding - detections\n\n        difference = tensorflow.keras.backend.max([0, difference])\n\n        paddings = ((0, 0), (0, difference), (0, 0), (0, 0), (0, 0))\n\n        # TODO: replace with `tensorflow.keras.backend.pad`\n        return tensorflow.pad(x, paddings, mode=""constant"")\n\n    def get_config(self):\n        configuration = {""padding"": self.padding}\n\n        return {**super(ObjectSegmentation, self).get_config(), **configuration}\n'"
keras_rcnn/layers/_pooling.py,3,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\n\n\nclass RegionOfInterest(tensorflow.keras.layers.Layer):\n    """"""\n    ROI pooling layer proposed in Mask R-CNN (Kaiming He et. al.).\n\n    :param size: Fixed size [h, w], e.g. [7, 7], for the output slices.\n    :param stride: Integer, pooling stride.\n    :return: slices: 5D Tensor (number of regions, slice_height,\n    slice_width, channels)\n    """"""\n\n    def __init__(self, extent=(7, 7), strides=1, **kwargs):\n        self.channels = None\n\n        self.extent = extent\n\n        self.stride = strides\n\n        super(RegionOfInterest, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.channels = input_shape[1][3]\n\n        super(RegionOfInterest, self).build(input_shape)\n\n    def call(self, x, **kwargs):\n        """"""\n\n        :rtype: `(samples, proposals, width, height, channels)`\n        """"""\n        metadata, image, boxes = x[0], x[1], x[2]\n\n        # convert regions from (x, y, w, h) to (x1, y1, x2, y2)\n        boxes = tensorflow.keras.backend.cast(boxes, tensorflow.keras.backend.floatx())\n\n        boxes = boxes / self.stride\n\n        x1 = boxes[..., 0]\n        y1 = boxes[..., 1]\n        x2 = boxes[..., 2]\n        y2 = boxes[..., 3]\n\n        # normalize the boxes\n        shape = metadata[0]\n\n        h = tensorflow.keras.backend.cast(shape[0], tensorflow.keras.backend.floatx())\n        w = tensorflow.keras.backend.cast(shape[1], tensorflow.keras.backend.floatx())\n\n        x1 /= w - 1\n        y1 /= h - 1\n        x2 /= w - 1\n        y2 /= h - 1\n\n        x1 = tensorflow.keras.backend.expand_dims(x1, axis=2)\n        y1 = tensorflow.keras.backend.expand_dims(y1, axis=2)\n        x2 = tensorflow.keras.backend.expand_dims(x2, axis=2)\n        y2 = tensorflow.keras.backend.expand_dims(y2, axis=2)\n\n        boxes = tensorflow.keras.backend.concatenate([y1, x1, y2, x2], axis=2)\n        boxes = tensorflow.keras.backend.reshape(boxes, (-1, 4))\n\n        slices = keras_rcnn.backend.crop_and_resize(image, boxes, self.extent)\n\n        return tensorflow.keras.backend.expand_dims(slices, axis=0)\n\n    def compute_output_shape(self, input_shape):\n        return 1, input_shape[2][1], self.extent[0], self.extent[1], self.channels\n\n    def get_config(self):\n        configuration = {""extent"": self.extent, ""strides"": self.stride}\n\n        return {**super(RegionOfInterest, self).get_config(), **configuration}\n\n\ndef log2_graph(x):\n    """"""Implementation of Log2. TF doesn\'t have a native implementation.""""""\n    return tensorflow.keras.backend.log(x) / tensorflow.keras.backend.log(2.0)\n\n\nclass RegionOfInterestAlignPyramid(tensorflow.keras.layers.Layer):\n    def __init__(self, extent=(7, 7), strides=1, **kwargs):\n        self.channels = None\n\n        self.extent = extent\n\n        self.stride = strides\n\n        super(RegionOfInterestAlignPyramid, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.channels = input_shape[2][3]\n\n        super(RegionOfInterestAlignPyramid, self).build(input_shape)\n\n    def call(self, x, **kwargs):\n        metadata, boxes, images = x[0], x[1], x[2:]\n\n        x1 = boxes[..., 0]\n        y1 = boxes[..., 1]\n        x2 = boxes[..., 2]\n        y2 = boxes[..., 3]\n\n        h = y2 - y1\n        w = x2 - x1\n\n        image_rows = metadata[0, 0]\n        image_cols = metadata[0, 1]\n\n        image_area = tensorflow.keras.backend.cast(image_rows * image_cols, ""float32"")\n        roi_level = log2_graph(\n            tensorflow.keras.backend.sqrt(h * w)\n            / (tensorflow.keras.backend.sqrt(image_area))\n        )\n        roiInt = tensorflow.keras.backend.round(roi_level)\n        roi_level = tensorflow.keras.backend.minimum(\n            5.0, tensorflow.keras.backend.maximum(2.0, 4.0 + roiInt)\n        )\n        roi_level = tensorflow.keras.backend.squeeze(roi_level, 0)\n\n        pooled = []\n        box_to_level = []\n        for i in range(0, len(images)):\n            level = i + 2\n\n            ix = keras_rcnn.backend.where(\n                tensorflow.keras.backend.equal(roi_level, level)\n            )\n\n            level_boxes = tensorflow.keras.backend.tf.gather_nd(\n                tensorflow.keras.backend.squeeze(boxes, axis=0), ix\n            )\n\n            level_boxes = tensorflow.keras.backend.expand_dims(level_boxes, axis=0)\n\n            pool = keras_rcnn.layers.RegionOfInterest(\n                extent=self.extent, strides=self.stride\n            )([metadata, images[i], level_boxes])\n\n            pooled.append(pool)\n            box_to_level.append(ix)\n\n        pooled = tensorflow.keras.backend.concatenate(pooled, axis=1)\n        box_to_level = tensorflow.keras.backend.concatenate(box_to_level, axis=0)\n\n        box_range = tensorflow.keras.backend.expand_dims(\n            tensorflow.keras.backend.tf.range(\n                tensorflow.keras.backend.shape(box_to_level)[0]\n            ),\n            1,\n        )\n        box_to_level_int = tensorflow.keras.backend.cast(\n            tensorflow.keras.backend.round(box_to_level), ""int32""\n        )\n        box_to_level = tensorflow.keras.backend.concatenate(\n            [box_to_level_int, box_range], axis=1\n        )\n\n        pooled = tensorflow.keras.backend.squeeze(pooled, axis=0)\n        sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]\n        ix = tensorflow.keras.backend.tf.nn.top_k(\n            sorting_tensor, k=tensorflow.keras.backend.shape(box_to_level)[0]\n        ).indices[::-1]\n\n        ix = tensorflow.keras.backend.gather(box_to_level[:, 1], ix)\n\n        pooled = tensorflow.keras.backend.gather(pooled, ix)\n\n        shape = tensorflow.keras.backend.concatenate(\n            [\n                tensorflow.keras.backend.shape(boxes)[:2],\n                tensorflow.keras.backend.shape(pooled)[1:],\n            ],\n            axis=0,\n        )\n        pooled = tensorflow.keras.backend.reshape(pooled, shape)\n\n        tensorflow.keras.backend.expand_dims(pooled, axis=0)\n\n        return pooled\n\n    def compute_output_shape(self, input_shape):\n        return 1, input_shape[1][1], self.extent[0], self.extent[1], self.channels\n\n    def get_config(self):\n        configuration = {""extent"": self.extent, ""strides"": self.stride}\n\n        return {\n            **super(RegionOfInterestAlignPyramid, self).get_config(),\n            **configuration,\n        }\n'"
keras_rcnn/layers/_upsample.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\n\n\nclass Upsample(tensorflow.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Upsample, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(Upsample, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        output, target = inputs\n\n        shape = tensorflow.keras.backend.shape(target)\n\n        return keras_rcnn.backend.resize(output, (shape[1], shape[2]))\n\n    def compute_output_shape(self, input_shape):\n        output_shape = input_shape[1][1:3]\n\n        return (input_shape[0][0],) + output_shape + (input_shape[0][-1],)\n'"
keras_rcnn/metrics/mean_average_precision.py,0,"b'import numpy\n\n\ndef intersection_over_union(y_true, y_pred):\n    """"""\n    :param y_pred: [minimum_r, minimum_c, maximum_r, maximum_c]\n    :param y_true: [minimum_r, minimum_c, maximum_r, maximum_c]\n\n    :return:\n\n    """"""\n    y_true_minimum_r, y_true_minimum_c, y_true_maximum_r, y_true_maximum_c = y_true\n    y_pred_minimum_r, y_pred_minimum_c, y_pred_maximum_r, y_pred_maximum_c = y_pred\n\n    if y_true_maximum_r < y_pred_minimum_r:\n        return 0.0\n\n    if y_pred_maximum_r < y_true_minimum_r:\n        return 0.0\n\n    if y_true_maximum_c < y_pred_minimum_c:\n        return 0.0\n\n    if y_pred_maximum_c < y_true_minimum_c:\n        return 0.0\n\n    minimum_r = numpy.maximum(y_true_minimum_r, y_pred_minimum_r)\n    minimum_c = numpy.maximum(y_true_minimum_c, y_pred_minimum_c)\n\n    maximum_r = numpy.minimum(y_true_maximum_r, y_pred_maximum_r)\n    maximum_c = numpy.minimum(y_true_maximum_c, y_pred_maximum_c)\n\n    intersection = (maximum_r - minimum_r + 1) * (maximum_c - minimum_c + 1)\n\n    y_true_area = (y_true_maximum_r - y_true_minimum_r + 1) * (\n        y_true_maximum_c - y_true_minimum_c + 1\n    )\n    y_pred_area = (y_pred_maximum_r - y_pred_minimum_r + 1) * (\n        y_pred_maximum_c - y_pred_minimum_c + 1\n    )\n\n    union = y_true_area + y_pred_area - intersection\n\n    return intersection / union\n\n\ndef evaluate(y_true, y_pred, threshold=0.5):\n    y_true_indices = []\n    y_pred_indices = []\n\n    scores = []\n\n    for y_pred_detection_index, y_pred_detection in enumerate(y_pred):\n        for y_true_detection_index, y_true_detection in enumerate(y_true):\n            score = intersection_over_union(y_true_detection, y_pred_detection)\n\n            if score > threshold:\n                y_true_indices += [y_true_detection_index]\n                y_pred_indices += [y_pred_detection_index]\n\n                scores += [score]\n\n    scores = numpy.argsort(scores)[::-1]\n\n    if scores.size == 0:\n        tp = 0\n        fp = len(y_pred)\n        fn = len(y_true)\n    else:\n        y_true_matched_indices = []\n        y_pred_matched_indices = []\n\n        for score in scores:\n            y_true_index = y_true_indices[score]\n            y_pred_index = y_pred_indices[score]\n\n            if (y_true_index not in y_true_matched_indices) and (\n                y_pred_index not in y_pred_matched_indices\n            ):\n                y_true_matched_indices += [y_true_index]\n                y_pred_matched_indices += [y_pred_index]\n\n        tp = len(y_true_matched_indices)\n        fp = len(y_pred) - len(y_pred_matched_indices)\n        fn = len(y_true) - len(y_true_matched_indices)\n\n    try:\n        precision = tp / (tp + fp)\n    except ZeroDivisionError:\n        precision = 0.0\n\n    try:\n        recall = tp / (tp + fn)\n    except ZeroDivisionError:\n        recall = 0.0\n\n    return {\n        ""false negatives"": fn,\n        ""false positives"": fp,\n        ""precision"": precision,\n        ""recall"": recall,\n        ""threshold"": threshold,\n        ""true positives"": tp,\n    }\n'"
keras_rcnn/models/__init__.py,0,b'from ._mask_rcnn import MaskRCNN\n\nfrom ._rcnn import RCNN\n'
keras_rcnn/models/_mask_rcnn.py,0,"b'# -*- coding: utf-8 -*-\n\nimport keras_resnet\nimport keras_resnet.models\nimport numpy\nimport tensorflow\n\nimport keras_rcnn.layers\nimport keras_rcnn.models.backbone\n\n\nclass MaskRCNN(tensorflow.keras.models.Model):\n    """"""\n    A Region-based Convolutional Neural Network (RCNN)\n\n    Parameters\n    ----------\n\n    input_shape : A shape tuple (integer) without the batch dimension.\n\n        For example:\n\n            `input_shape=(224, 224, 3)`\n\n        specifies that the input are batches of $224 \xc3\x97 224$ RGB images.\n\n        Likewise:\n\n            `input_shape=(224, 224)`\n\n        specifies that the input are batches of $224 \xc3\x97 224$ grayscale\n        images.\n\n    categories : An array-like with shape:\n\n            $$(categories,)$$.\n\n        For example:\n\n            `categories=[""circle"", ""square"", ""triangle""]`\n\n        specifies that the detected objects belong to either the\n        \xe2\x80\x9ccircle,\xe2\x80\x9d \xe2\x80\x9csquare,\xe2\x80\x9d or \xe2\x80\x9ctriangle\xe2\x80\x9d category.\n\n    anchor_aspect_ratios : An array-like with shape:\n\n            $$(aspect_ratios,)$$\n\n        used to generate anchors.\n\n        For example:\n\n            `aspect_ratios=[0.5, 1., 2.]`\n\n        corresponds to 1:2, 1:1, and 2:1 respectively.\n\n    anchor_base_size : Integer that specifies an anchor\xe2\x80\x99s base area:\n\n            $$base_area = base_size^{2}$$.\n\n    anchor_scales : An array-like with shape:\n\n            $$(scales,)$$\n\n        used to generate anchors. A scale corresponds to:\n\n            $$area_{scale}=\\sqrt{\\frac{area_{anchor}}{area_{base}}}$$.\n\n    anchor_stride : A positive integer\n\n    backbone :\n\n    dense_units : A positive integer that specifies the dimensionality of\n        the fully-connected layers.\n\n        The fully-connected layers are the layers that precede the\n        fully-connected layers for the classification, regression and\n        segmentation target functions.\n\n        Increasing the number of dense units will increase the\n        expressiveness of the network and consequently the ability to\n        correctly learn the target functions, but it\xe2\x80\x99ll substantially\n        increase the number of learnable parameters and memory needed by\n        the model.\n\n    mask_shape : A shape tuple (integer).\n\n    maximum_proposals : A positive integer that specifies the maximum\n        number of object proposals returned from the model.\n\n        The model always return an array-like with shape:\n\n            $$(maximum_proposals, 4)$$\n\n        regardless of the number of object proposals returned after\n        non-maximum suppression is performed. If the number of object\n        proposals returned from non-maximum suppression is less than the\n        number of objects specified by the `maximum_proposals` parameter,\n        the model will return bounding boxes with the value:\n\n            `[0., 0., 0., 0.]`\n\n        and scores with the value `[0.]`.\n\n    minimum_size : A positive integer that specifies the maximum width\n        or height for each object proposal.\n    """"""\n\n    def __init__(\n        self,\n        input_shape,\n        categories,\n        anchor_aspect_ratios=None,\n        anchor_base_size=16,\n        anchor_padding=1,\n        anchor_scales=None,\n        anchor_stride=16,\n        backbone=None,\n        dense_units=1024,\n        mask_shape=(28, 28),\n        maximum_proposals=300,\n        minimum_size=16,\n    ):\n        if anchor_aspect_ratios is None:\n            anchor_aspect_ratios = [0.5, 1.0, 2.0]\n\n        if anchor_scales is None:\n            anchor_scales = [32, 64, 128, 256, 512]\n\n        self.mask_shape = mask_shape\n\n        self.n_categories = len(categories) + 1\n\n        k = len(anchor_aspect_ratios)\n\n        target_bounding_boxes = tensorflow.keras.layers.Input(\n            shape=(None, 4), name=""target_bounding_boxes""\n        )\n\n        target_categories = tensorflow.keras.layers.Input(\n            shape=(None, self.n_categories), name=""target_categories""\n        )\n\n        target_image = tensorflow.keras.layers.Input(\n            shape=input_shape, name=""target_image""\n        )\n\n        target_masks = tensorflow.keras.layers.Input(\n            shape=(None,) + mask_shape, name=""target_masks""\n        )\n\n        target_metadata = tensorflow.keras.layers.Input(\n            shape=(3,), name=""target_metadata""\n        )\n\n        options = {""activation"": ""relu"", ""kernel_size"": (3, 3), ""padding"": ""same""}\n\n        inputs = [\n            target_bounding_boxes,\n            target_categories,\n            target_image,\n            target_masks,\n            target_metadata,\n        ]\n\n        backbone = keras_resnet.models.FPN2D50(target_image)\n\n        pyramid_2, pyramid_3, pyramid_4, pyramid_5, pyramid_6 = backbone.outputs\n\n        levels = backbone.outputs\n\n        target_proposal_bounding_boxes_list = []\n        target_proposal_categories_list = []\n        output_proposal_bounding_boxes_list = []\n\n        for index_lvl in range(0, len(levels)):\n            level = levels[len(levels) - index_lvl - 1]\n\n            convolution_3x3 = tensorflow.keras.layers.Conv2D(\n                kernel_size=(3, 3),\n                filters=64,\n                name=""3x3_"" + str(index_lvl),\n                kernel_initializer=tensorflow.keras.initializers.RandomNormal(\n                    mean=0.0, stddev=0.01, seed=None\n                ),\n                bias_initializer=tensorflow.keras.initializers.Constant(value=0.0),\n                padding=""same"",\n            )(level)\n\n            output_deltas = tensorflow.keras.layers.Conv2D(\n                filters=k * 4,\n                kernel_size=(1, 1),\n                activation=""linear"",\n                kernel_initializer=tensorflow.keras.initializers.RandomNormal(\n                    mean=0.0, stddev=0.01, seed=None\n                ),\n                bias_initializer=tensorflow.keras.initializers.Constant(value=0.0),\n                name=""deltas1_"" + str(index_lvl),\n                padding=""same"",\n            )(convolution_3x3)\n\n            output_scores = tensorflow.keras.layers.Conv2D(\n                filters=k * 2,\n                kernel_size=(1, 1),\n                activation=""sigmoid"",\n                kernel_initializer=tensorflow.keras.initializers.RandomNormal(\n                    mean=0.0, stddev=0.01, seed=None\n                ),\n                bias_initializer=tensorflow.keras.initializers.Constant(value=0.0),\n                name=""scores1_"" + str(index_lvl),\n                padding=""valid"",\n            )(convolution_3x3)\n\n            (\n                target_anchors,\n                target_proposal_bounding_boxes,\n                target_proposal_categories,\n            ) = keras_rcnn.layers.Anchor(\n                base_size=minimum_size,\n                padding=anchor_padding,\n                aspect_ratios=anchor_aspect_ratios,\n                scales=[\n                    32\n                    * (2.0 ** (len(levels) - 1 - index_lvl))\n                    / (4 * 2 ** (len(levels) - 1 - index_lvl))\n                ],\n                stride=4 * 2 ** (len(levels) - 1 - index_lvl),\n            )(\n                [target_bounding_boxes, target_metadata, output_scores]\n            )\n\n            output_deltas, output_scores = keras_rcnn.layers.RPN()(\n                [\n                    target_proposal_bounding_boxes,\n                    target_proposal_categories,\n                    output_deltas,\n                    output_scores,\n                ]\n            )\n\n            output_proposal_bounding_boxes = keras_rcnn.layers.ObjectProposal(\n                maximum_proposals=maximum_proposals, minimum_size=minimum_size\n            )([target_anchors, target_metadata, output_deltas, output_scores])\n\n            (\n                target_proposal_bounding_boxes,\n                target_proposal_categories,\n                output_proposal_bounding_boxes,\n            ) = keras_rcnn.layers.ProposalTarget()(\n                [\n                    target_bounding_boxes,\n                    target_categories,\n                    output_proposal_bounding_boxes,\n                ]\n            )\n\n            output_proposal_bounding_boxes_list += [output_proposal_bounding_boxes]\n            target_proposal_bounding_boxes_list += [target_proposal_bounding_boxes]\n            target_proposal_categories_list += [target_proposal_categories]\n\n        output_proposal_bounding_boxes = tensorflow.keras.layers.concatenate(\n            inputs=output_proposal_bounding_boxes_list, axis=1\n        )\n\n        target_proposal_bounding_boxes = tensorflow.keras.layers.concatenate(\n            inputs=target_proposal_bounding_boxes_list, axis=1\n        )\n\n        target_proposal_categories = tensorflow.keras.layers.concatenate(\n            inputs=target_proposal_categories_list, axis=1\n        )\n\n        mask_features = keras_rcnn.layers.RegionOfInterestAlignPyramid(\n            extent=(14, 14), strides=2,\n        )(\n            [\n                target_metadata,\n                output_proposal_bounding_boxes,\n                pyramid_2,\n                pyramid_3,\n                pyramid_4,\n                pyramid_5,\n            ]\n        )\n\n        output_features = keras_rcnn.layers.RegionOfInterestAlignPyramid(\n            extent=(7, 7), strides=1\n        )(\n            [\n                target_metadata,\n                output_proposal_bounding_boxes,\n                pyramid_2,\n                pyramid_3,\n                pyramid_4,\n                pyramid_5,\n            ]\n        )\n\n        mask_features = self._mask_network()(\n            [target_metadata, mask_features, output_proposal_bounding_boxes]\n        )\n        output_features = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=dense_units, activation=""relu"", name=""fc1""\n            )\n        )(output_features)\n\n        output_features = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=dense_units, activation=""relu"", name=""fc2""\n            )\n        )(output_features)\n\n        output_features = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Flatten()\n        )(output_features)\n\n        output_deltas = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=4 * self.n_categories,\n                activation=""linear"",\n                kernel_initializer=""zero"",\n                name=""deltas2"",\n            )\n        )(output_features)\n\n        output_scores = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=1 * self.n_categories,\n                activation=""softmax"",\n                kernel_initializer=""zero"",\n                name=""scores2"",\n            )\n        )(output_features)\n\n        output_deltas, output_scores = keras_rcnn.layers.RCNN()(\n            [\n                target_proposal_bounding_boxes,\n                target_proposal_categories,\n                output_deltas,\n                output_scores,\n            ]\n        )\n\n        (\n            output_bounding_boxes,\n            output_categories,\n            mask_features,\n        ) = keras_rcnn.layers.ObjectSegmentation()(\n            [\n                target_metadata,\n                output_deltas,\n                output_proposal_bounding_boxes,\n                output_scores,\n                mask_features,\n            ]\n        )\n\n        output_masks = keras_rcnn.layers.losses.RCNNMaskLoss()(\n            [target_bounding_boxes, output_bounding_boxes, target_masks, mask_features]\n        )\n\n        outputs = [output_bounding_boxes, output_categories, output_masks]\n\n        super(MaskRCNN, self).__init__(inputs, outputs)\n\n    def _mask_network(self):\n        def f(x):\n            target_metadata, output_features, output_proposal_bounding_boxes = x\n\n            mask_features = tensorflow.keras.layers.TimeDistributed(\n                tensorflow.keras.layers.Conv2D(\n                    activation=""relu"", filters=256, kernel_size=(3, 3), padding=""same""\n                )\n            )(output_features)\n\n            mask_features = tensorflow.keras.layers.TimeDistributed(\n                tensorflow.keras.layers.Conv2D(\n                    activation=""relu"", filters=256, kernel_size=(3, 3), padding=""same""\n                )\n            )(mask_features)\n\n            mask_features = tensorflow.keras.layers.TimeDistributed(\n                tensorflow.keras.layers.Conv2D(\n                    activation=""relu"", filters=256, kernel_size=(3, 3), padding=""same""\n                )\n            )(mask_features)\n\n            mask_features = tensorflow.keras.layers.TimeDistributed(\n                tensorflow.keras.layers.Conv2D(\n                    activation=""relu"", filters=256, kernel_size=(3, 3), padding=""same""\n                )\n            )(mask_features)\n\n            mask_features = tensorflow.keras.layers.TimeDistributed(\n                tensorflow.keras.layers.Conv2DTranspose(\n                    activation=""relu"", filters=256, kernel_size=(2, 2), strides=2\n                )\n            )(mask_features)\n\n            mask_features = tensorflow.keras.layers.TimeDistributed(\n                tensorflow.keras.layers.Conv2D(\n                    activation=""sigmoid"",\n                    filters=self.n_categories - 1,\n                    kernel_size=(1, 1),\n                    strides=1,\n                )\n            )(mask_features)\n\n            return mask_features\n\n        return f\n\n    def compile(self, optimizer, **kwargs):\n        super(MaskRCNN, self).compile(optimizer, None)\n\n    def predict(self, x, batch_size=None, verbose=0, steps=None, **kwargs):\n        target_bounding_boxes = numpy.zeros((x.shape[0], 1, 4))\n\n        target_categories = numpy.zeros((x.shape[0], 1, self.n_categories))\n\n        target_mask = numpy.zeros((1, 1, *self.mask_shape))\n\n        target_metadata = numpy.array([[x.shape[1], x.shape[2], 1.0]])\n\n        x = [target_bounding_boxes, target_categories, x, target_mask, target_metadata]\n\n        return super(MaskRCNN, self).predict(x, batch_size, verbose, steps)\n'"
keras_rcnn/models/_rcnn.py,0,"b'# -*- coding: utf-8 -*-\n\nimport keras_resnet\nimport keras_resnet.models\nimport numpy\nimport tensorflow\n\nimport keras_rcnn.layers\nimport keras_rcnn.models.backbone\n\n\nclass RCNN(tensorflow.keras.models.Model):\n    """"""\n    A Region-based Convolutional Neural Network (RCNN)\n\n    Parameters\n    ----------\n\n    input_shape : A shape tuple (integer) without the batch dimension.\n\n        For example:\n\n            `input_shape=(224, 224, 3)`\n\n        specifies that the input are batches of $224 \xc3\x97 224$ RGB images.\n\n        Likewise:\n\n            `input_shape=(224, 224)`\n\n        specifies that the input are batches of $224 \xc3\x97 224$ grayscale\n        images.\n\n    categories : An array-like with shape:\n\n            $$(categories,)$$.\n\n        For example:\n\n            `categories=[""circle"", ""square"", ""triangle""]`\n\n        specifies that the detected objects belong to either the\n        \xe2\x80\x9ccircle,\xe2\x80\x9d \xe2\x80\x9csquare,\xe2\x80\x9d or \xe2\x80\x9ctriangle\xe2\x80\x9d category.\n\n    anchor_aspect_ratios : An array-like with shape:\n\n            $$(aspect_ratios,)$$\n\n        used to generate anchors.\n\n        For example:\n\n            `aspect_ratios=[0.5, 1., 2.]`\n\n        corresponds to 1:2, 1:1, and 2:1 respectively.\n\n    anchor_base_size : Integer that specifies an anchor\xe2\x80\x99s base area:\n\n            $$base_area = base_size^{2}$$.\n\n    anchor_scales : An array-like with shape:\n\n            $$(scales,)$$\n\n        used to generate anchors. A scale corresponds to:\n\n            $$area_{scale}=\\sqrt{\\frac{area_{anchor}}{area_{base}}}$$.\n\n    anchor_stride : A positive integer\n\n    backbone :\n\n    dense_units : A positive integer that specifies the dimensionality of\n        the fully-connected layers.\n\n        The fully-connected layers are the layers that precede the\n        fully-connected layers for the classification, regression and\n        segmentation target functions.\n\n        Increasing the number of dense units will increase the\n        expressiveness of the network and consequently the ability to\n        correctly learn the target functions, but it\xe2\x80\x99ll substantially\n        increase the number of learnable parameters and memory needed by\n        the model.\n\n    mask_shape : A shape tuple (integer).\n\n    maximum_proposals : A positive integer that specifies the maximum\n        number of object proposals returned from the model.\n\n        The model always return an array-like with shape:\n\n            $$(maximum_proposals, 4)$$\n\n        regardless of the number of object proposals returned after\n        non-maximum suppression is performed. If the number of object\n        proposals returned from non-maximum suppression is less than the\n        number of objects specified by the `maximum_proposals` parameter,\n        the model will return bounding boxes with the value:\n\n            `[0., 0., 0., 0.]`\n\n        and scores with the value `[0.]`.\n\n    minimum_size : A positive integer that specifies the maximum width\n        or height for each object proposal.\n    """"""\n\n    def __init__(\n        self,\n        input_shape,\n        categories,\n        anchor_aspect_ratios=None,\n        anchor_base_size=16,\n        anchor_padding=1,\n        anchor_scales=None,\n        anchor_stride=16,\n        backbone=None,\n        dense_units=1024,\n        mask_shape=(28, 28),\n        maximum_proposals=300,\n        minimum_size=16,\n    ):\n        if anchor_aspect_ratios is None:\n            anchor_aspect_ratios = [0.5, 1.0, 2.0]\n\n        if anchor_scales is None:\n            anchor_scales = [32, 64, 128, 256, 512]\n\n        self.mask_shape = mask_shape\n\n        self.n_categories = len(categories) + 1\n\n        k = len(anchor_aspect_ratios)\n\n        target_bounding_boxes = tensorflow.keras.layers.Input(\n            shape=(None, 4), name=""target_bounding_boxes""\n        )\n\n        target_categories = tensorflow.keras.layers.Input(\n            shape=(None, self.n_categories), name=""target_categories""\n        )\n\n        target_image = tensorflow.keras.layers.Input(\n            shape=input_shape, name=""target_image""\n        )\n\n        target_masks = tensorflow.keras.layers.Input(\n            shape=(None,) + mask_shape, name=""target_masks""\n        )\n\n        target_metadata = tensorflow.keras.layers.Input(\n            shape=(3,), name=""target_metadata""\n        )\n\n        options = {""activation"": ""relu"", ""kernel_size"": (3, 3), ""padding"": ""same""}\n\n        inputs = [\n            target_bounding_boxes,\n            target_categories,\n            target_image,\n            target_masks,\n            target_metadata,\n        ]\n\n        backbone = keras_resnet.models.FPN2D50(target_image)\n\n        pyramid_2, pyramid_3, pyramid_4, pyramid_5, pyramid_6 = backbone.outputs\n\n        levels = backbone.outputs\n\n        target_proposal_bounding_boxes_list = []\n        target_proposal_categories_list = []\n        output_proposal_bounding_boxes_list = []\n\n        for index_lvl in range(0, len(levels)):\n            level = levels[len(levels) - index_lvl - 1]\n\n            convolution_3x3 = tensorflow.keras.layers.Conv2D(\n                kernel_size=(3, 3),\n                filters=64,\n                name=""3x3_"" + str(index_lvl),\n                kernel_initializer=tensorflow.keras.initializers.RandomNormal(\n                    mean=0.0, stddev=0.01, seed=None\n                ),\n                bias_initializer=tensorflow.keras.initializers.Constant(value=0.0),\n                padding=""same"",\n            )(level)\n\n            output_deltas = tensorflow.keras.layers.Conv2D(\n                filters=k * 4,\n                kernel_size=(1, 1),\n                activation=""linear"",\n                kernel_initializer=tensorflow.keras.initializers.RandomNormal(\n                    mean=0.0, stddev=0.01, seed=None\n                ),\n                bias_initializer=tensorflow.keras.initializers.Constant(value=0.0),\n                name=""deltas1_"" + str(index_lvl),\n                padding=""same"",\n            )(convolution_3x3)\n\n            output_scores = tensorflow.keras.layers.Conv2D(\n                filters=k * 2,\n                kernel_size=(1, 1),\n                activation=""sigmoid"",\n                kernel_initializer=tensorflow.keras.initializers.RandomNormal(\n                    mean=0.0, stddev=0.01, seed=None\n                ),\n                bias_initializer=tensorflow.keras.initializers.Constant(value=0.0),\n                name=""scores1_"" + str(index_lvl),\n                padding=""valid"",\n            )(convolution_3x3)\n\n            (\n                target_anchors,\n                target_proposal_bounding_boxes,\n                target_proposal_categories,\n            ) = keras_rcnn.layers.Anchor(\n                base_size=minimum_size,\n                padding=anchor_padding,\n                aspect_ratios=anchor_aspect_ratios,\n                scales=[\n                    32\n                    * (2.0 ** (len(levels) - 1 - index_lvl))\n                    / (4 * 2 ** (len(levels) - 1 - index_lvl))\n                ],\n                stride=4 * 2 ** (len(levels) - 1 - index_lvl),\n            )(\n                [target_bounding_boxes, target_metadata, output_scores]\n            )\n\n            output_deltas, output_scores = keras_rcnn.layers.RPN()(\n                [\n                    target_proposal_bounding_boxes,\n                    target_proposal_categories,\n                    output_deltas,\n                    output_scores,\n                ]\n            )\n\n            output_proposal_bounding_boxes = keras_rcnn.layers.ObjectProposal(\n                maximum_proposals=maximum_proposals, minimum_size=minimum_size\n            )([target_anchors, target_metadata, output_deltas, output_scores])\n\n            (\n                target_proposal_bounding_boxes,\n                target_proposal_categories,\n                output_proposal_bounding_boxes,\n            ) = keras_rcnn.layers.ProposalTarget()(\n                [\n                    target_bounding_boxes,\n                    target_categories,\n                    output_proposal_bounding_boxes,\n                ]\n            )\n\n            output_proposal_bounding_boxes_list += [output_proposal_bounding_boxes]\n\n            target_proposal_bounding_boxes_list += [target_proposal_bounding_boxes]\n\n            target_proposal_categories_list += [target_proposal_categories]\n\n        output_proposal_bounding_boxes = tensorflow.keras.layers.concatenate(\n            inputs=output_proposal_bounding_boxes_list, axis=1\n        )\n\n        target_proposal_bounding_boxes = tensorflow.keras.layers.concatenate(\n            inputs=target_proposal_bounding_boxes_list, axis=1\n        )\n\n        target_proposal_categories = tensorflow.keras.layers.concatenate(\n            inputs=target_proposal_categories_list, axis=1\n        )\n\n        output_features = keras_rcnn.layers.RegionOfInterestAlignPyramid(\n            extent=(7, 7), strides=1\n        )(\n            [\n                target_metadata,\n                output_proposal_bounding_boxes,\n                pyramid_2,\n                pyramid_3,\n                pyramid_4,\n                pyramid_5,\n            ]\n        )\n\n        output_features = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=dense_units, activation=""relu"", name=""fc1""\n            )\n        )(output_features)\n\n        output_features = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=dense_units, activation=""relu"", name=""fc2""\n            )\n        )(output_features)\n\n        output_features = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Flatten()\n        )(output_features)\n\n        output_deltas = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=4 * self.n_categories,\n                activation=""linear"",\n                kernel_initializer=""zero"",\n                name=""deltas2"",\n            )\n        )(output_features)\n\n        output_scores = tensorflow.keras.layers.TimeDistributed(\n            tensorflow.keras.layers.Dense(\n                units=1 * self.n_categories,\n                activation=""softmax"",\n                kernel_initializer=""zero"",\n                name=""scores2"",\n            )\n        )(output_features)\n\n        output_deltas, output_scores = keras_rcnn.layers.RCNN()(\n            [\n                target_proposal_bounding_boxes,\n                target_proposal_categories,\n                output_deltas,\n                output_scores,\n            ]\n        )\n\n        output_bounding_boxes, output_categories = keras_rcnn.layers.ObjectDetection()(\n            [\n                target_metadata,\n                output_deltas,\n                output_proposal_bounding_boxes,\n                output_scores,\n            ]\n        )\n\n        outputs = [output_bounding_boxes, output_categories]\n\n        super(RCNN, self).__init__(inputs, outputs)\n\n    def compile(self, optimizer, **kwargs):\n        super(RCNN, self).compile(optimizer, None)\n\n    def predict(self, x, batch_size=None, verbose=0, steps=None, **kwargs):\n        target_bounding_boxes = numpy.zeros((x.shape[0], 1, 4))\n\n        target_categories = numpy.zeros((x.shape[0], 1, self.n_categories))\n\n        target_mask = numpy.zeros((1, 1, *self.mask_shape))\n\n        target_metadata = numpy.array([[x.shape[1], x.shape[2], 1.0]])\n\n        x = [target_bounding_boxes, target_categories, x, target_mask, target_metadata]\n\n        return super(RCNN, self).predict(x, batch_size, verbose, steps)\n'"
keras_rcnn/models/backbone.py,0,"b'# -*- coding: utf-8 -*-\n\nimport keras_resnet.models\nimport tensorflow\n\n\ndef ResNet50():\n    def f(x):\n        y = keras_resnet.models.ResNet50(include_top=False, inputs=x)\n\n        _, _, convolution_4, _ = y.outputs\n\n        return convolution_4\n\n    return f\n\n\ndef VGG16():\n    def f(x):\n        y = tensorflow.keras.applications.VGG16(include_top=False, input_tensor=x)\n\n        return y.layers[-3].output\n\n    return f\n\n\ndef VGG19():\n    def f(x):\n        y = tensorflow.keras.applications.VGG19(include_top=False, input_tensor=x)\n\n        return y.layers[-3].output\n\n    return f\n'"
keras_rcnn/preprocessing/__init__.py,0,b'from ._object_detection import ObjectDetectionGenerator\n'
keras_rcnn/preprocessing/_object_detection.py,0,"b'# -*- coding: utf-8 -*-\n\nimport numpy\nimport skimage.color\nimport skimage.exposure\nimport skimage.io\nimport skimage.transform\nimport tensorflow\n\n\nclass BoundingBoxException(Exception):\n    pass\n\n\nclass MissingImageException(Exception):\n    pass\n\n\nclass DictionaryIterator(tensorflow.keras.preprocessing.image.Iterator):\n    def __init__(\n        self,\n        dictionary,\n        categories,\n        target_size,\n        generator,\n        batch_size=1,\n        color_mode=""rgb"",\n        data_format=None,\n        mask_size=(28, 28),\n        seed=None,\n        shuffle=False,\n    ):\n        if color_mode not in {""grayscale"", ""rgb""}:\n            raise ValueError\n\n        self.batch_size = batch_size\n\n        self.categories = categories\n\n        if color_mode == ""rgb"":\n            self.channels = 3\n        else:\n            self.channels = 1\n\n        self.color_mode = color_mode\n\n        if data_format is None:\n            data_format = tensorflow.keras.backend.image_data_format()\n\n        if data_format not in {""channels_first"", ""channels_last""}:\n            raise ValueError\n\n        self.data_format = data_format\n\n        self.dictionary = dictionary\n\n        self.generator = generator\n\n        if self.color_mode == ""grayscale"":\n            if self.data_format == ""channels_first"":\n                self.image_shape = (*target_size, 1)\n            else:\n                self.image_shape = (1, *target_size)\n        else:\n            if self.data_format == ""channels_last"":\n                self.image_shape = (*target_size, 3)\n            else:\n                self.image_shape = (3, *target_size)\n\n        self.mask_size = mask_size\n\n        self.maximum = numpy.max(target_size)\n\n        self.minimum = numpy.min(target_size)\n\n        self.n_categories = len(self.categories) + 1\n\n        self.n_samples = len(self.dictionary)\n\n        self.target_size = target_size\n\n        super(DictionaryIterator, self).__init__(\n            self.n_samples, batch_size, shuffle, seed\n        )\n\n    def next(self):\n        with self.lock:\n            selection = next(self.index_generator)\n\n        return self._get_batches_of_transformed_samples(selection)\n\n    def find_scale(self, image):\n        r, c, _ = image.shape\n\n        scale = self.minimum / numpy.minimum(r, c)\n\n        if numpy.maximum(r, c) * scale > self.maximum:\n            scale = self.maximum / numpy.maximum(r, c)\n\n        return scale\n\n    def _clear_border(self, bounding_boxes):\n        indices = []\n\n        for index, bounding_box in enumerate(bounding_boxes[0]):\n            minimum_r, minimum_c, maximum_r, maximum_c = bounding_box\n\n            if (\n                minimum_r > 0\n                and minimum_c > 0\n                and maximum_c < self.target_size[0]\n                and maximum_r < self.target_size[1]\n            ):\n                indices += [index]\n\n        return indices\n\n    @staticmethod\n    def _crop_bounding_boxes(bounding_boxes, boundary):\n        cropped_bounding_boxes = numpy.array(boundary)\n\n        bounding_boxes = bounding_boxes.copy()\n\n        bounding_boxes[..., :2] = numpy.maximum(\n            bounding_boxes[..., :2], cropped_bounding_boxes[:2]\n        )\n        bounding_boxes[..., 2:] = numpy.minimum(\n            bounding_boxes[..., 2:], cropped_bounding_boxes[2:]\n        )\n\n        bounding_boxes[..., :2] -= cropped_bounding_boxes[:2]\n        bounding_boxes[..., 2:] -= cropped_bounding_boxes[:2]\n\n        mask = numpy.all(bounding_boxes[..., :2] < bounding_boxes[..., 2:], axis=2)\n\n        bounding_boxes[~mask] = numpy.zeros((4,))\n\n        return bounding_boxes\n\n    def _crop_image(self, image):\n        crop_r = numpy.random.randint(\n            0, image.shape[0] - self.generator.crop_size[0] - 1\n        )\n        crop_c = numpy.random.randint(\n            0, image.shape[1] - self.generator.crop_size[1] - 1\n        )\n\n        crop = image[\n            crop_r : crop_r + self.generator.crop_size[0],\n            crop_c : crop_c + self.generator.crop_size[1],\n            ...,\n        ]\n\n        dimensions = numpy.array(\n            [\n                crop_r,\n                crop_c,\n                crop_r + self.generator.crop_size[0],\n                crop_c + self.generator.crop_size[1],\n            ]\n        )\n\n        return crop, dimensions\n\n    def _get_batches_of_transformed_samples(self, selection):\n        # TODO: permit batch sizes > 1\n        batch_index, image_index = 0, selection[0]\n\n        while True:\n            try:\n                x = self._transform_samples(batch_index, image_index)\n            except BoundingBoxException:\n                # FIXME: This should do something! To ignore the image\n                image_index += 1\n                continue\n            except MissingImageException:\n                image_index = 0\n                continue\n            break\n\n        return x, None\n\n    def _transform_samples(self, batch_index, image_index):\n        x_bounding_boxes = numpy.zeros((self.batch_size, 0, 4))\n\n        x_categories = numpy.zeros((self.batch_size, 0, self.n_categories))\n\n        x_images = numpy.zeros((self.batch_size, *self.target_size, self.channels))\n\n        x_masks = numpy.zeros((self.batch_size, 0, *self.mask_size))\n\n        x_metadata = numpy.zeros((self.batch_size, 3))\n\n        horizontal_flip = False\n\n        if self.generator.horizontal_flip:\n            if numpy.random.random() < 0.5:\n                horizontal_flip = True\n\n        vertical_flip = False\n\n        if self.generator.vertical_flip:\n            if numpy.random.random() < 0.5:\n                vertical_flip = True\n\n        try:\n            pathname = self.dictionary[image_index][""image""][""pathname""]\n\n        except:\n            raise MissingImageException\n\n        target_image = numpy.zeros((*self.target_size, self.channels))\n\n        image = skimage.io.imread(pathname)\n\n        if self.data_format == ""channels_last"":\n            image = image[..., : self.channels]\n        else:\n            image = image[: self.channels, ...]\n\n        dimensions = numpy.array([0, 0, image.shape[0], image.shape[1]])\n\n        if self.generator.crop_size:\n            if (\n                image.shape[0] > self.generator.crop_size[0]\n                and image.shape[1] > self.generator.crop_size[1]\n            ):\n                image, dimensions = self._crop_image(image)\n\n        dimensions = dimensions.astype(numpy.float16)\n\n        scale = self.find_scale(image)\n\n        dimensions *= scale\n\n        image = skimage.transform.rescale(\n            image, scale, anti_aliasing=True, mode=""reflect"", multichannel=True\n        )\n\n        image = self.generator.standardize(image)\n\n        if horizontal_flip:\n            image = numpy.fliplr(image)\n\n        if vertical_flip:\n            image = numpy.flipud(image)\n\n        image_r = image.shape[0]\n        image_c = image.shape[1]\n\n        target_image[:image_r, :image_c] = image\n\n        x_images[batch_index] = target_image\n\n        x_metadata[batch_index] = [*self.target_size, 1.0]\n\n        bounding_boxes = self.dictionary[image_index][""objects""]\n\n        n_objects = len(bounding_boxes)\n\n        if n_objects == 0:\n            return [\n                numpy.zeros((self.batch_size, 0, 4)),\n                numpy.zeros((self.batch_size, 0, self.n_categories)),\n                x_images,\n                numpy.zeros((self.batch_size, 0, self.mask_size[0], self.mask_size[1])),\n                x_metadata,\n            ]\n\n        x_bounding_boxes = numpy.resize(\n            x_bounding_boxes, (self.batch_size, n_objects, 4)\n        )\n\n        x_masks = numpy.resize(x_masks, (self.batch_size, n_objects, *self.mask_size))\n\n        x_categories = numpy.resize(\n            x_categories, (self.batch_size, n_objects, self.n_categories)\n        )\n\n        for bounding_box_index, bounding_box in enumerate(bounding_boxes):\n            if bounding_box[""category""] not in self.categories:\n                continue\n\n            minimum_r = bounding_box[""bounding_box""][""minimum""][""r""]\n            minimum_c = bounding_box[""bounding_box""][""minimum""][""c""]\n\n            maximum_r = bounding_box[""bounding_box""][""maximum""][""r""]\n            maximum_c = bounding_box[""bounding_box""][""maximum""][""c""]\n\n            minimum_r *= scale\n            minimum_c *= scale\n\n            maximum_r *= scale\n            maximum_c *= scale\n\n            minimum_r = int(minimum_r)\n            minimum_c = int(minimum_c)\n\n            maximum_r = int(maximum_r)\n            maximum_c = int(maximum_c)\n\n            target_bounding_box = [minimum_r, minimum_c, maximum_r, maximum_c]\n\n            x_bounding_boxes[batch_index, bounding_box_index] = target_bounding_box\n\n            if ""mask"" in bounding_box:\n                target_mask = skimage.io.imread(bounding_box[""mask""][""pathname""])\n\n                target_mask = skimage.transform.rescale(\n                    target_mask,\n                    scale,\n                    anti_aliasing=True,\n                    mode=""reflect"",\n                    multichannel=False,\n                )\n\n                target_mask = target_mask[\n                    minimum_r : maximum_r + 1, minimum_c : maximum_c + 1\n                ]\n\n                target_mask = skimage.transform.resize(\n                    target_mask,\n                    self.mask_size,\n                    order=0,\n                    mode=""reflect"",\n                    anti_aliasing=True,\n                )\n\n                x_masks[batch_index, bounding_box_index] = target_mask\n\n            target_category = numpy.zeros(self.n_categories)\n\n            target_category[self.categories[bounding_box[""category""]]] = 1\n\n            x_categories[batch_index, bounding_box_index] = target_category\n\n        x = self._shuffle_objects(x_bounding_boxes, x_categories, x_masks)\n\n        x_bounding_boxes, x_categories, x_masks = x\n\n        x_bounding_boxes = self._crop_bounding_boxes(x_bounding_boxes, dimensions)\n\n        cropped = self._cropped_objects(x_bounding_boxes)\n\n        x_bounding_boxes = x_bounding_boxes[:, ~cropped]\n\n        x_categories = x_categories[:, ~cropped]\n\n        x_masks = x_masks[:, ~cropped]\n\n        for bounding_box_index, bounding_box in enumerate(x_bounding_boxes[0]):\n            mask = x_masks[0, bounding_box_index]\n\n            if horizontal_flip:\n                bounding_box = [\n                    bounding_box[0],\n                    image.shape[1] - bounding_box[3],\n                    bounding_box[2],\n                    image.shape[1] - bounding_box[1],\n                ]\n                mask = numpy.fliplr(mask)\n\n            if vertical_flip:\n                bounding_box = [\n                    image.shape[0] - bounding_box[2],\n                    bounding_box[1],\n                    image.shape[0] - bounding_box[0],\n                    bounding_box[3],\n                ]\n                mask = numpy.flipud(mask)\n\n            x_bounding_boxes[batch_index, bounding_box_index] = bounding_box\n\n            x_masks[batch_index, bounding_box_index] = mask\n\n        if self.generator.clear_border:\n            indices = self._clear_border(x_bounding_boxes)\n\n            x_bounding_boxes = x_bounding_boxes[:, indices]\n\n            x_categories = x_categories[:, indices]\n\n            x_masks = x_masks[:, indices]\n\n        if x_bounding_boxes.shape == (self.batch_size, 0, 4):\n            raise BoundingBoxException\n\n        x_masks[x_masks > 0.5] = 1.0\n        x_masks[x_masks < 0.5] = 0.0\n\n        return [x_bounding_boxes, x_categories, x_images, x_masks, x_metadata]\n\n    @staticmethod\n    def _cropped_objects(x_bounding_boxes):\n        return numpy.all(x_bounding_boxes[..., :] == 0, axis=2)[0]\n\n    def _shuffle_objects(self, x_bounding_boxes, x_categories, x_masks):\n        n = x_bounding_boxes.shape[1]\n\n        if self.shuffle:\n            indicies = numpy.random.permutation(n)\n        else:\n            indicies = numpy.arange(0, n)\n\n        return [\n            x_bounding_boxes[:, indicies],\n            x_categories[:, indicies],\n            x_masks[:, indicies],\n        ]\n\n\nclass ObjectDetectionGenerator:\n    def __init__(\n        self,\n        clear_border=False,\n        crop_size=None,\n        data_format=None,\n        horizontal_flip=False,\n        preprocessing_function=None,\n        rescale=False,\n        rotation_range=0.0,\n        samplewise_center=False,\n        vertical_flip=False,\n    ):\n        self.clear_border = clear_border\n\n        self.crop_size = crop_size\n\n        self.data_format = data_format\n\n        self.horizontal_flip = horizontal_flip\n\n        self.preprocessing_function = preprocessing_function\n\n        self.rescale = rescale\n\n        self.rotation_range = rotation_range\n\n        self.samplewise_center = samplewise_center\n\n        self.vertical_flip = vertical_flip\n\n    def flow_from_dictionary(\n        self,\n        dictionary,\n        categories,\n        target_size,\n        batch_size=1,\n        color_mode=""rgb"",\n        data_format=None,\n        mask_size=(28, 28),\n        shuffle=True,\n        seed=None,\n    ):\n        return DictionaryIterator(\n            dictionary,\n            categories,\n            target_size,\n            self,\n            batch_size,\n            color_mode,\n            data_format,\n            mask_size,\n            seed,\n            shuffle,\n        )\n\n    def standardize(self, image):\n        image = skimage.exposure.rescale_intensity(image, out_range=(0.0, 1.0))\n\n        if self.preprocessing_function:\n            image = self.preprocessing_function(image)\n\n        if self.rescale:\n            image *= self.rescale\n\n        if self.samplewise_center:\n            image -= numpy.mean(image, keepdims=True)\n\n        return image\n'"
keras_rcnn/utils/__init__.py,0,b'from ._visualization import show_bounding_boxes\n'
keras_rcnn/utils/_visualization.py,0,"b'import matplotlib.cm\nimport matplotlib.patches\nimport matplotlib.pyplot\nimport numpy\n\n\ndef _get_cmap(n, name=""hsv""):\n    return matplotlib.cm.get_cmap(name, n)\n\n\n# TODO: commit upstream to `scikit-image`.\ndef show_bounding_boxes(image, bounding_boxes, categories=None):\n    axis = matplotlib.pyplot.gca()\n\n    n = bounding_boxes.shape[0]\n\n    if categories is not None:\n        assert bounding_boxes.shape[0] == categories.shape[0]\n\n        categories = categories.reshape((-1, 1))\n    else:\n        categories = numpy.zeros((bounding_boxes.shape[0], 1))\n\n    axis.imshow(image)\n\n    colormap = _get_cmap(n)\n\n    for bounding_box, category in zip(bounding_boxes, categories):\n        rectangle = matplotlib.patches.Rectangle(\n            [bounding_box[1], bounding_box[0]],\n            bounding_box[3] - bounding_box[1],\n            bounding_box[2] - bounding_box[0],\n            edgecolor=colormap(category[0]),\n            facecolor=""none"",\n        )\n\n        axis.add_patch(rectangle)\n'"
tests/applications/__init__.py,0,b''
tests/applications/test_hollandi2019.py,0,b''
tests/applications/test_jhung2019.py,0,"b'import keras\nimport numpy\n\nimport keras_rcnn.applications\n\n\nclass TestJHung2019:\n    def setup(self):\n        categories = {\n            ""red blood cell"": 1,\n            ""leukocyte"": 2,\n            ""ring"": 3,\n            ""trophozoite"": 4,\n            ""schizont"": 5,\n            ""gametocyte"": 6,\n        }\n\n        self.instance = keras_rcnn.applications.JHung2019((224, 224, 3), categories)\n\n    def test_compile(self):\n        optimizer = keras.optimizers.SGD()\n\n        self.instance.compile(optimizer,)\n\n    def test_predict(self):\n        image = numpy.random.random((1, 224, 224, 3))\n\n        self.instance.predict(image,)\n'"
tests/backend/__init__.py,0,b''
tests/backend/test_common.py,0,"b'import keras.backend\nimport numpy\nimport numpy.testing\n\nimport keras_rcnn.backend\nimport keras_rcnn.backend.common\nimport keras_rcnn.layers.object_detection._anchor\nimport keras_rcnn.layers.object_detection._object_proposal\n\n\ndef test_anchor():\n    x = numpy.array(\n        [\n            [-8.0, -8.0, 8.0, 8.0],\n            [-16.0, -16.0, 16.0, 16.0],\n            [-24.0, -24.0, 24.0, 24.0],\n            [-11.0, -6.0, 11.0, 6.0],\n            [-23.0, -11.0, 23.0, 11.0],\n            [-34.0, -17.0, 34.0, 17.0],\n            [-14.0, -5.0, 14.0, 5.0],\n            [-28.0, -9.0, 28.0, 9.0],\n            [-42.0, -14.0, 42.0, 14.0],\n        ]\n    )\n\n    y = keras_rcnn.backend.anchor(\n        ratios=keras.backend.cast([1, 2, 3], keras.backend.floatx()),\n        scales=keras.backend.cast([1, 2, 3], keras.backend.floatx()),\n    )\n    y = keras.backend.eval(y)\n\n    numpy.testing.assert_array_almost_equal(x, y, 0)\n\n\ndef test_clip():\n    boxes = numpy.array(\n        [[0, 0, 0, 0], [1, 2, 3, 4], [-4, 2, 1000, 6000], [3, -10, 223, 224]]\n    )\n    shape = [224, 224]\n    boxes = keras.backend.variable(boxes)\n    results = keras_rcnn.backend.clip(boxes, shape)\n    results = keras.backend.eval(results)\n    expected = numpy.array(\n        [[0, 0, 0, 0], [1, 2, 3, 4], [0, 2, 223, 223], [3, 0, 223, 223]]\n    )\n    numpy.testing.assert_array_almost_equal(results, expected)\n\n    boxes = numpy.reshape(numpy.arange(200, 200 + 12 * 5), (-1, 12))\n    shape = [224, 224]\n    boxes = keras.backend.variable(boxes)\n    results = keras_rcnn.backend.clip(boxes, shape)\n    results = keras.backend.eval(results)\n    expected = numpy.array(\n        [\n            [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211],\n            [212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223],\n            [223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223],\n            [223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223],\n            [223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223],\n        ]\n    )\n    numpy.testing.assert_array_almost_equal(results, expected, 0)\n\n\ndef test_bbox_transform():\n    gt_rois = numpy.array(\n        [\n            [-84.0, -40.0, 99.0, 55.0],\n            [-176.0, -88.0, 191.0, 103.0],\n            [-360.0, -184.0, 375.0, 199.0],\n            [-56.0, -56.0, 71.0, 71.0],\n            [-120.0, -120.0, 135.0, 135.0],\n            [-248.0, -248.0, 263.0, 263.0],\n            [-36.0, -80.0, 51.0, 95.0],\n            [-80.0, -168.0, 95.0, 183.0],\n            [-168.0, -344.0, 183.0, 359.0],\n        ]\n    )\n    ex_rois = 2 * gt_rois\n    results = keras_rcnn.backend.bbox_transform(ex_rois, gt_rois)\n    results = keras.backend.eval(results)\n    expected = numpy.array(\n        [\n            [\n                -0.020491803278688523,\n                -0.039473684210526314,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.010217983651226158,\n                -0.01963350785340314,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.00510204081632653,\n                -0.0097911227154047,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.02952755905511811,\n                -0.02952755905511811,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.014705882352941176,\n                -0.014705882352941176,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.007338551859099804,\n                -0.007338551859099804,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.04310344827586207,\n                -0.02142857142857143,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.02142857142857143,\n                -0.010683760683760684,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n            [\n                -0.010683760683760684,\n                -0.005334281650071123,\n                -0.6931471805599453,\n                -0.6931471805599453,\n            ],\n        ]\n    )\n    numpy.testing.assert_array_almost_equal(results, expected)\n\n\ndef test_mkanchors():\n    ws = numpy.array([1, 2, 3])\n    hs = numpy.array([4, 5, 6])\n    x_ctr = keras.backend.variable([0], ""float32"")\n    y_ctr = keras.backend.variable([2], ""float32"")\n    ws = keras.backend.variable(ws, ""float32"")\n    hs = keras.backend.variable(hs, ""float32"")\n    results = keras_rcnn.backend.common._mkanchors(ws, hs, x_ctr, y_ctr)\n    results = keras.backend.eval(results)\n    expected = numpy.array(\n        [[-0.5, 0.0, 0.5, 4.0], [-1.0, -0.5, 1.0, 4.5], [-1.5, -1.0, 1.5, 5.0]]\n    )\n    numpy.testing.assert_array_equal(results, expected)\n\n\ndef test_overlap():\n    x = numpy.asarray(\n        [\n            [0, 10, 0, 10],\n            [0, 20, 0, 20],\n            [0, 30, 0, 30],\n            [0, 40, 0, 40],\n            [0, 50, 0, 50],\n            [0, 60, 0, 60],\n            [0, 70, 0, 70],\n            [0, 80, 0, 80],\n            [0, 90, 0, 90],\n        ]\n    )\n    x = keras.backend.variable(x)\n\n    y = numpy.asarray([[0, 20, 0, 20], [0, 40, 0, 40], [0, 60, 0, 60], [0, 80, 0, 80]])\n    y = keras.backend.variable(y)\n\n    overlapping = keras_rcnn.backend.common.intersection_over_union(x, y)\n\n    overlapping = keras.backend.eval(overlapping)\n\n    expected = numpy.array(\n        [\n            [0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 1.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 1.0],\n            [0.0, 0.0, 0.0, 0.0],\n        ]\n    )\n\n    numpy.testing.assert_array_equal(overlapping, expected)\n\n\ndef test_ratio_enum():\n    anchor = numpy.expand_dims(numpy.array([0, 0, 16, 16]), 0)\n    ratios = numpy.array([0.5, 2])\n    anchor = keras.backend.variable(anchor)\n    ratios = keras.backend.variable(ratios)\n    results = keras_rcnn.backend.common._ratio_enum(anchor, ratios)\n    results = keras.backend.round(results)\n    results = keras.backend.eval(results)\n    expected = numpy.array([[2.0, -3.0, 14.0, 19.0], [-3.0, 2.0, 19.0, 14.0]])\n    numpy.testing.assert_array_equal(results, expected)\n\n\ndef test_scale_enum():\n    anchor = numpy.expand_dims(numpy.array([0, 0, 16, 16]), 0)\n    scales = numpy.array([0.5])\n    anchor = keras.backend.variable(anchor)\n    scales = keras.backend.variable(scales)\n    results = keras_rcnn.backend.common._scale_enum(anchor, scales)\n    results = keras.backend.eval(results)\n    expected = numpy.array([[4, 4, 12, 12]])\n    numpy.testing.assert_array_equal(results, expected)\n\n\ndef test_whctrs():\n    anchor = keras.backend.cast(keras.backend.expand_dims([0, 0, 16, 16], 0), ""float32"")\n    results0, results1, results2, results3 = keras_rcnn.backend.common._whctrs(anchor)\n    results = numpy.array(\n        [\n            keras.backend.eval(results0),\n            keras.backend.eval(results1),\n            keras.backend.eval(results2),\n            keras.backend.eval(results3),\n        ]\n    )\n    expected = numpy.expand_dims([16, 16, 8, 8], 1)\n    numpy.testing.assert_array_equal(results, expected)\n\n\ndef test_shift():\n    y = keras_rcnn.backend.shift((14, 14), 16)\n\n    assert keras.backend.eval(y).shape == (2940, 4), keras.backend.eval(y).shape\n\n    assert y.dtype == keras.backend.floatx()\n\n\ndef test_bbox_transform_inv():\n    anchors = 15\n    features = (14, 14)\n    shifted = keras_rcnn.backend.shift(features, 16)\n    deltas = numpy.zeros((features[0] * features[1] * anchors, 4), numpy.float32)\n    pred_boxes = keras_rcnn.backend.bbox_transform_inv(shifted, deltas)\n    assert keras.backend.eval(pred_boxes).shape == (2940, 4)\n\n    rois = numpy.array(\n        [\n            [55.456726, 67.949135, 86.19536, 98.13131],\n            [101.945526, 0.0, 125.88675, 37.465652],\n            [50.71209, 70.44628, 90.61703, 99.66291],\n            [108.26308, 18.048904, 127.0, 51.715424],\n            [83.864334, 0.0, 126.68436, 17.052166],\n            [92.37217, 5.155298, 120.51592, 36.1372],\n            [56.470436, 67.04311, 88.26943, 107.08549],\n            [49.678932, 85.499756, 94.36956, 122.12128],\n        ],\n        numpy.float32,\n    )\n    deltas = numpy.array(\n        [\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.02123459,\n                0.1590581,\n                0.28013495,\n                -0.39532417,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.08355092,\n                0.17592771,\n                -0.0810278,\n                -0.01217953,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.02042394,\n                0.09747629,\n                0.02641878,\n                -0.36387,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                -0.08266437,\n                -0.2706405,\n                0.15300322,\n                0.09181178,\n            ],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        ]\n    )\n    pred_boxes = keras_rcnn.backend.bbox_transform_inv(rois, deltas)\n    expected = numpy.array(\n        [\n            [\n                55.456726,\n                67.949135,\n                86.19536,\n                98.13131,\n                51.0,\n                78.0,\n                93.0,\n                99.0,\n                55.456726,\n                67.949135,\n                86.19536,\n                98.13131,\n            ],\n            [\n                101.945526,\n                0.0,\n                125.88675,\n                37.465652,\n                105.0,\n                7.0,\n                127.0,\n                44.0,\n                101.945526,\n                0.0,\n                125.88675,\n                37.465652,\n            ],\n            [\n                50.71209,\n                70.44628,\n                90.61703,\n                99.66291,\n                51.0,\n                78.0,\n                93.0,\n                99.0,\n                50.71209,\n                70.44628,\n                90.61703,\n                99.66291,\n            ],\n            [\n                108.26308,\n                18.048904,\n                127.0,\n                51.715424,\n                108.26308,\n                18.048904,\n                127.0,\n                51.715424,\n                105.0,\n                7.0,\n                127.0,\n                44.0,\n            ],\n            [\n                83.864334,\n                0.0,\n                126.68436,\n                17.052166,\n                83.864334,\n                0.0,\n                126.68436,\n                17.052166,\n                83.864334,\n                0.0,\n                126.68436,\n                17.052166,\n            ],\n            [\n                92.37217,\n                5.155298,\n                120.51592,\n                36.1372,\n                92.37217,\n                5.155298,\n                120.51592,\n                36.1372,\n                92.37217,\n                5.155298,\n                120.51592,\n                36.1372,\n            ],\n            [\n                56.470436,\n                67.04311,\n                88.26943,\n                107.08549,\n                56.470436,\n                67.04311,\n                88.26943,\n                107.08549,\n                56.470436,\n                67.04311,\n                88.26943,\n                107.08549,\n            ],\n            [\n                49.678932,\n                85.499756,\n                94.36956,\n                122.12128,\n                49.678932,\n                85.499756,\n                94.36956,\n                122.12128,\n                49.678932,\n                85.499756,\n                94.36956,\n                122.12128,\n            ],\n        ],\n        numpy.float32,\n    )\n    numpy.testing.assert_array_almost_equal(\n        keras.backend.eval(pred_boxes), expected, 0, verbose=True\n    )\n\n\ndef test_smooth_l1():\n    output = keras.backend.variable(\n        [\n            [[2.5, 0.0, 0.4, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 2.5, 0.0, 0.4]],\n            [[3.5, 0.0, 0.0, 0.0], [0.0, 0.4, 0.0, 0.9], [0.0, 0.0, 1.5, 0.0]],\n        ]\n    )\n\n    target = keras.backend.zeros_like(output)\n\n    x = keras_rcnn.backend.smooth_l1(output, target)\n\n    numpy.testing.assert_approx_equal(keras.backend.eval(x), 8.645)\n\n    weights = keras.backend.variable([[2, 1, 1], [0, 3, 0]])\n\n    x = keras_rcnn.backend.smooth_l1(output, target, weights=weights)\n\n    numpy.testing.assert_approx_equal(keras.backend.eval(x), 7.695)\n\n\ndef test_softmax_classification():\n    output = [\n        [[-100, 100, -100], [100, -100, -100], [0, 0, -100], [-100, -100, 100]],\n        [[-100, 0, 0], [-100, 100, -100], [-100, 100, -100], [100, -100, -100]],\n    ]\n\n    target = [\n        [[0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1]],\n        [[0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0]],\n    ]\n\n    weights = [[1.0, 1.0, 0.5, 1.0], [1.0, 1.0, 1.0, 0.0]]\n\n    x = keras_rcnn.backend.softmax_classification(\n        keras.backend.variable(target),\n        keras.backend.variable(output),\n        weights=keras.backend.variable(weights),\n    )\n\n    output_y = numpy.reshape(output, [-1, 3])\n    target_y = numpy.reshape(target, [-1, 3])\n    output_y = output_y / numpy.sum(output_y, axis=1, keepdims=True)\n\n    # epsilon = keras.backend.epsilon()\n    # stop python for complaining weakref\n    epsilon = 1e-07\n\n    output_y = numpy.clip(output_y, epsilon, 1.0 - epsilon)\n    _y = -numpy.sum(target_y * numpy.log(output_y), axis=1)\n    y = _y * numpy.reshape(weights, -1)\n\n    numpy.testing.assert_array_almost_equal(keras.backend.eval(x), y)\n\n    x = keras_rcnn.backend.softmax_classification(\n        keras.backend.variable(target),\n        keras.backend.variable(output),\n        anchored=True,\n        weights=keras.backend.variable(weights),\n    )\n\n    y = weights * numpy.reshape(_y, numpy.asarray(weights).shape)\n    numpy.testing.assert_array_almost_equal(keras.backend.eval(x), y)\n'"
tests/backend/test_tensorflow_backend.py,0,"b'import keras.backend\nimport numpy\nimport tensorflow\n\nimport keras_rcnn.backend.tensorflow_backend\nimport keras_rcnn.backend.common\n\n\ndef test_transpose():\n    x = numpy.arange(4).reshape((2, 2))\n\n    target = numpy.transpose(x)\n\n    output = keras_rcnn.backend.transpose(x)\n\n    output = keras.backend.eval(output)\n\n    numpy.testing.assert_array_equal(target, output)\n\n    x = numpy.ones((1, 2, 3))\n\n    target = numpy.transpose(x, (1, 0, 2))\n\n    output = keras_rcnn.backend.transpose(x, [1, 0, 2])\n\n    output = keras.backend.eval(output)\n\n    numpy.testing.assert_array_equal(target, output)\n\n\ndef test_shuffle():\n    x = keras.backend.variable(numpy.random.random((10,)))\n\n    keras_rcnn.backend.shuffle(x)\n\n\ndef test_matmul():\n    pass\n\n\ndef test_gather_nd():\n    pass\n\n\ndef test_argsort():\n    pass\n\n\ndef test_scatter_add_tensor():\n    ref = keras.backend.ones((4, 5))\n    ii = keras.backend.reshape(\n        keras.backend.cast(keras.backend.zeros((4,)), ""int32""), (-1, 1)\n    )\n    jj = keras.backend.reshape(keras.backend.arange(0, 4), (-1, 1))\n    indices = keras.backend.concatenate([ii, jj], 1)\n    updates = keras.backend.arange(4, dtype=keras.backend.floatx()) * 2\n    result = keras_rcnn.backend.scatter_add_tensor(ref, indices, updates)\n    result = keras.backend.eval(result)\n    expected = numpy.ones((4, 5))\n    expected[0, :4] += numpy.arange(4) * 2\n    numpy.testing.assert_array_almost_equal(result, expected)\n\n\ndef test_meshgrid():\n    pass\n\n\ndef test_unique():\n    pass\n\n\ndef test_smooth_l1():\n    pass\n\n\ndef test_where():\n    pass\n\n\ndef test_non_max_suppression():\n    boxes = numpy.zeros((1764, 4))\n    scores = numpy.random.rand(14 * 14, 9).flatten()\n    threshold = 0.5\n    maximum = 100\n    nms = tensorflow.image.non_max_suppression(\n        boxes=boxes, iou_threshold=threshold, max_output_size=maximum, scores=scores\n    )\n    assert keras.backend.eval(nms).shape == (maximum,)\n\n\ndef test_crop_and_resize():\n    image = keras.backend.variable(numpy.ones((1, 28, 28, 3)))\n\n    boxes = keras.backend.variable(\n        numpy.array([[0.1, 0.1, 0.2, 0.2], [0.5, 0.5, 0.8, 0.8]])\n    )\n\n    size = [7, 7]\n\n    slices = keras_rcnn.backend.crop_and_resize(image, boxes, size)\n\n    assert keras.backend.eval(slices).shape == (2, 7, 7, 3)\n\n\ndef test_squeeze():\n    x = [[[0], [1], [2]]]\n\n    x = keras.backend.variable(x)\n\n    assert keras.backend.int_shape(x) == (1, 3, 1)\n\n    y = keras_rcnn.backend.tensorflow_backend.squeeze(x)\n\n    assert keras.backend.int_shape(y) == (3,)\n\n    y = keras_rcnn.backend.tensorflow_backend.squeeze(x, 0)\n\n    assert keras.backend.int_shape(y) == (3, 1)\n\n    y = keras_rcnn.backend.tensorflow_backend.squeeze(x, 2)\n\n    assert keras.backend.int_shape(y) == (1, 3)\n'"
tests/datasets/__init__.py,0,b''
tests/datasets/test_shape.py,0,"b'def test_load_data(training_dictionary, test_dictionary):\n    assert len(training_dictionary) == 256\n\n    assert len(test_dictionary) == 256\n'"
tests/layers/test_detection.py,0,"b'import keras.backend\nimport keras.utils\nimport numpy\n\nimport keras_rcnn.layers\n\n\n# class TestDetection:\n#     def test_call(self):\n#         num_classes = 2\n#         detections = keras_rcnn.layers.ObjectDetection()\n#\n#         proposals = numpy.array([[5, 10, 30, 50], [6.4, 1, 24.2, 33.2]])\n#         proposals = keras.backend.variable(proposals)\n#         pred_boxes = numpy.array([[.05, -.1, .03, .2, -.02, -.07, 0.1, -.04], [0.5, 0.3, -0.2, -0.4, .5, -.3, -.2, .4]])\n#         pred_boxes = keras.backend.variable(pred_boxes)\n#\n#         pred_scores = numpy.array([[[0.4, 0.6], [0.9, .1]]])\n#         pred_scores = keras.backend.variable(pred_scores)\n#\n#         metadata = keras.backend.variable([[35, 35, 1]])\n#\n#         boxes, classes = detections.call(\n#             [proposals, pred_boxes, pred_scores, metadata], training=False)\n#\n#         # expected = numpy.array([[  5.90409106, 1.36124346, 32.69590894,  34.,\n#         #                            3.11277807, 7.9338165, 31.84722193, 34.],\n#         #                         [ 17.50393092,  16.43268724,  32.89606908,  34.,\n#         #                           17.50393092,   0., 32.89606908,  32.40428998]])\n#         expected = numpy.array([[ 3.11277807, 7.9338165, 31.84722193, 34.],\n#                         [ 17.50393092,  16.43268724,  32.89606908,  34.]])\n#         expected = numpy.expand_dims(expected, 0)\n#\n#         boxes = keras.backend.eval(boxes)\n#         classes = keras.backend.eval(classes)\n#\n#         numpy.testing.assert_approx_equal(expected.sum(), boxes.sum(), 3)\n#\n#         assert boxes.shape == expected.shape\n#         assert classes.shape[:2] == boxes.shape[:2]\n#\n#         assert classes.shape[-1] == num_classes\n'"
tests/layers/test_pooling.py,0,"b'import numpy\nimport keras.backend\nimport keras_rcnn.layers\nimport mock\n\n\ndef test_roi():\n    image = keras.backend.variable(numpy.random.random((1, 28, 14, 3)))\n    boxes = keras.backend.variable(numpy.array([[[1, 2, 3, 4], [4, 3, 2, 1]]]))\n\n    metadata = keras.backend.variable([[28, 14, 3]])\n\n    roi_align = keras_rcnn.layers.RegionOfInterest(extent=[7, 7], strides=1)\n\n    slices = roi_align([metadata, image, boxes])\n\n    assert keras.backend.eval(slices).shape == (1, 2, 7, 7, 3)\n\n    with mock.patch(""keras_rcnn.backend.crop_and_resize"", lambda x, y, z: y):\n        boxes = roi_align([metadata, image, boxes])\n        numpy.testing.assert_array_almost_equal(\n            keras.backend.eval(boxes),\n            [\n                [\n                    [2.0 / 28, 1.0 / 14, 4.0 / 28, 3.0 / 14],\n                    [3.0 / 28, 4.0 / 14, 1.0 / 28, 2.0 / 14],\n                ]\n            ],\n        )\n\n    a = keras.backend.placeholder(shape=(None, 224, 224, 3))\n    b = keras.backend.placeholder(shape=(1, None, 4))\n    y = keras_rcnn.layers.RegionOfInterest([7, 7])([metadata, a, b])\n    # Should be (None, None, 7, 7, 3)\n    assert keras.backend.int_shape(y) == (1, None, 7, 7, 3)\n'"
tests/layers/test_upsample.py,0,"b'# -*- coding: utf-8 -*-\n\nimport keras\nimport numpy\n\nimport keras_rcnn.layers\n\n\nclass TestUpsample(object):\n    def test_call(self):\n        upsample = keras_rcnn.layers.Upsample()\n\n        a = numpy.zeros((1, 2, 2, 1))\n        b = numpy.zeros((1, 5, 5, 1))\n\n        output = upsample.call([a, b])\n        target = b\n\n        numpy.testing.assert_array_equal(keras.backend.eval(output), target)\n'"
tests/preprocessing/test_object_detection.py,0,"b'class TestDictionaryIterator:\n    def find_scale(self):\n        pass\n\n    def test_next(self):\n        pass\n\n    def test__get_batches_of_transformed_samples(self):\n        pass\n\n\nclass TestObjectDetectionGenerator:\n    def test_flow_from_dictionary(self, generator):\n        x, _ = generator.next()\n\n        bounding_boxes, categories, images, masks, metadata = x\n\n        assert bounding_boxes.shape == (1, 15, 4)\n\n        assert categories.shape == (1, 15, 4)\n\n        assert images.shape == (1, 224, 224, 3)\n\n        assert masks.shape == (1, 15, 28, 28)\n\n        assert metadata.shape == (1, 3)\n\n    def test_standardize(self):\n        pass\n'"
keras_rcnn/data/shape/shape.py,0,"b'import hashlib\nimport json\nimport os.path\nimport shutil\nimport uuid\n\nimport skimage.draw\nimport skimage.io\n\n\ndef md5sum(pathname, blocksize=65536):\n    checksum = hashlib.md5()\n\n    with open(pathname, ""rb"") as stream:\n        for block in iter(lambda: stream.read(blocksize), b""""):\n            checksum.update(block)\n\n    return checksum.hexdigest()\n\n\ndef __main__():\n    pathname = ""images""\n\n    if os.path.exists(pathname):\n        shutil.rmtree(pathname)\n\n    os.mkdir(pathname)\n\n    groups = (""training"", ""test"")\n\n    r, c = 224, 224\n\n    for group in groups:\n        dictionaries = []\n\n        for _ in range(256):\n            identifier = uuid.uuid4()\n\n            image, objects = skimage.draw.random_shapes((r, c), 32, 2, 32)\n\n            filename = ""{}.png"".format(identifier)\n\n            pathname = os.path.join(""images"", filename)\n\n            skimage.io.imsave(pathname, image)\n\n            if os.path.exists(pathname):\n                dictionary = {\n                    ""image"": {\n                        ""checksum"": md5sum(pathname),\n                        ""pathname"": pathname,\n                        ""shape"": {""r"": r, ""c"": c, ""channels"": 3},\n                    },\n                    ""objects"": [],\n                }\n\n                for category, (bounding_box_r, bounding_box_c) in objects:\n                    minimum_r, maximum_r = bounding_box_r\n                    minimum_c, maximum_c = bounding_box_c\n\n                    object_dictionary = {\n                        ""bounding_box"": {\n                            ""minimum"": {""r"": minimum_r - 1, ""c"": minimum_c - 1},\n                            ""maximum"": {""r"": maximum_r - 1, ""c"": maximum_c - 1},\n                        },\n                        ""category"": category,\n                    }\n\n                    dictionary[""objects""].append(object_dictionary)\n\n                dictionaries.append(dictionary)\n\n        filename = ""{}.json"".format(group)\n\n        with open(filename, ""w"") as stream:\n            json.dump(dictionaries, stream)\n\n\nif __name__ == ""__main__"":\n    __main__()\n'"
keras_rcnn/layers/losses/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\nfrom ._mask_rcnn import RCNNMaskLoss\n\n\nclass RCNN(tensorflow.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(RCNN, self).__init__(**kwargs)\n\n    def classification_loss(self):\n        loss = keras_rcnn.backend.softmax_classification(\n            self.target_scores, self.output_scores, anchored=True\n        )\n\n        return tensorflow.keras.backend.mean(loss)\n\n    def regression_loss(self):\n        output_deltas = self.output_deltas[:, :, 4:]\n        target_deltas = self.target_deltas[:, :, 4:]\n\n        # mask out output values where class is different from targetrcnn loss\n        # function\n        mask = self.target_scores\n\n        labels = tensorflow.keras.backend.repeat_elements(mask, 4, -1)\n        labels = labels[:, :, 4:]\n\n        loss = keras_rcnn.backend.smooth_l1(\n            output_deltas * labels, target_deltas * labels, anchored=True\n        )\n\n        target_scores = self.target_scores[:, :, 1:]\n\n        return tensorflow.keras.backend.sum(loss) / tensorflow.keras.backend.maximum(\n            tensorflow.keras.backend.epsilon(),\n            tensorflow.keras.backend.sum(target_scores),\n        )\n\n    def call(self, inputs, **kwargs):\n        target_deltas, target_scores, output_deltas, output_scores = inputs\n\n        self.target_deltas = target_deltas\n        self.target_scores = target_scores\n\n        self.output_deltas = output_deltas\n        self.output_scores = output_scores\n\n        loss = self.classification_loss() + self.regression_loss()\n\n        weight = 1.0\n\n        loss = weight * loss\n\n        self.add_loss(loss)\n\n        return [output_deltas, output_scores]\n\n\nclass RPN(tensorflow.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(RPN, self).__init__(**kwargs)\n\n    def call(self, inputs, **kwargs):\n        target_deltas, target_scores, output_deltas, output_scores = inputs\n\n        a = self.classification_loss(target_scores, output_scores)\n\n        b = self.regression_loss(target_deltas, target_scores, output_deltas)\n\n        weight = 1.0\n\n        loss = weight * (a + b)\n\n        self.add_loss(loss)\n\n        return [output_deltas, output_scores]\n\n    @staticmethod\n    def classification_loss(target_scores, output_scores):\n        output_scores = tensorflow.keras.backend.reshape(output_scores, (1, -1))\n\n        condition = tensorflow.keras.backend.not_equal(target_scores, -1)\n\n        indices = keras_rcnn.backend.where(condition)\n\n        indices = tensorflow.keras.backend.expand_dims(indices, 0)\n\n        target = keras_rcnn.backend.gather_nd(target_scores, indices)\n        output = keras_rcnn.backend.gather_nd(output_scores, indices)\n\n        loss = tensorflow.keras.backend.binary_crossentropy(target, output)\n\n        return tensorflow.keras.backend.mean(loss)\n\n    @staticmethod\n    def regression_loss(target_deltas, target_scores, output_deltas):\n        output_deltas = tensorflow.keras.backend.reshape(output_deltas, (1, -1, 4))\n\n        condition = tensorflow.keras.backend.not_equal(target_scores, -1)\n\n        indices = keras_rcnn.backend.where(condition)\n\n        output = keras_rcnn.backend.gather_nd(output_deltas, indices)\n        target = keras_rcnn.backend.gather_nd(target_deltas, indices)\n\n        target_scores = keras_rcnn.backend.gather_nd(target_scores, indices)\n\n        condition = tensorflow.keras.backend.greater(target_scores, 0)\n\n        x = tensorflow.keras.backend.zeros_like(target_scores) + 1\n        y = tensorflow.keras.backend.zeros_like(target_scores)\n\n        p_star_i = keras_rcnn.backend.where(condition, x, y)\n\n        p_star_i = tensorflow.keras.backend.expand_dims(p_star_i, 0)\n\n        output = tensorflow.keras.backend.expand_dims(output, 0)\n        target = tensorflow.keras.backend.expand_dims(target, 0)\n\n        a_y = keras_rcnn.backend.smooth_l1(output, target, anchored=True)\n\n        a = p_star_i * a_y\n\n        # Divided by anchor overlaps\n        weight = 1.0\n\n        loss = weight * (\n            tensorflow.keras.backend.sum(a)\n            / tensorflow.keras.backend.maximum(\n                tensorflow.keras.backend.epsilon(),\n                tensorflow.keras.backend.sum(p_star_i),\n            )\n        )\n\n        return loss\n\n    # COMPUTE OUTPUT SHAPE\n'"
keras_rcnn/layers/losses/_mask_rcnn.py,2,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\n\nclass RCNNMaskLoss(tensorflow.keras.layers.Layer):\n    def __init__(self, threshold=0.5, **kwargs):\n        self.threshold = threshold\n\n        super(RCNNMaskLoss, self).__init__(**kwargs)\n\n    def call(self, inputs, training=None, **kwargs):\n        target_boxes, predicred_boxes, target_masks, predicted_masks = inputs\n\n        """"""\n        loss = tensorflow.keras.backend.in_train_phase(\n            lambda: self.mask_loss(target_boundingbox=target_boxes, \n                                   detected_boundingbox=predicred_boxes, \n                                   target_mask=target_masks, \n                                   detected_mask=predicted_masks,\n                                   threshold=self.threshold),\n            tensorflow.keras.backend.variable(0),\n            training=training\n        )\n        """"""\n\n        loss = self.compute_mask_loss(\n            target_bounding_box=target_boxes,\n            output_bounding_box=predicred_boxes,\n            target_mask=target_masks,\n            output_mask=predicted_masks,\n            threshold=self.threshold,\n        )\n\n        self.add_loss(loss, inputs)\n\n        return predicted_masks\n\n    @staticmethod\n    def intersection_over_union(a, b):\n        """"""\n        Args:\n            a: shape (total_bboxes1, 4)\n                with x1, y1, x2, y2 point order.\n\n            b: shape (total_bboxes2, 4)\n                with x1, y1, x2, y2 point order.\n\n            p1 *-----\n               |     |\n               |_____* p2\n\n        Returns:\n            Tensor with shape (total_bboxes1, total_bboxes2)\n            with the IoU (intersection over union) of bboxes1[i] and bboxes2[j]\n            in [i, j].\n        """"""\n\n        a_x1, a_y1, b_x1, b_y1 = (\n            a[:, 0:1],\n            a[:, 1:2],\n            a[:, 2:3],\n            a[:, 3:],\n        )  # tf.split(bboxes1, 4, axis=1)\n        a_x2, a_y2, b_x2, b_y2 = (\n            b[:, 0:1],\n            b[:, 1:2],\n            b[:, 2:3],\n            b[:, 3:],\n        )  # tf.split(bboxes2, 4, axis=1)\n\n        x_intersection_1 = tensorflow.keras.backend.maximum(\n            a_x1, tensorflow.keras.backend.transpose(a_x2)\n        )\n        y_intersection_1 = tensorflow.keras.backend.maximum(\n            a_y1, tensorflow.keras.backend.transpose(a_y2)\n        )\n\n        x_intersection_2 = tensorflow.keras.backend.minimum(\n            b_x1, tensorflow.keras.backend.transpose(b_x2)\n        )\n        y_intersection_2 = tensorflow.keras.backend.minimum(\n            b_y1, tensorflow.keras.backend.transpose(b_y2)\n        )\n\n        a = tensorflow.keras.backend.maximum(x_intersection_2 - x_intersection_1 + 1, 0)\n        b = tensorflow.keras.backend.maximum(y_intersection_2 - y_intersection_1 + 1, 0)\n\n        intersection = a * b\n\n        bounding_boxes_a_area = (b_x1 - a_x1 + 1) * (b_y1 - a_y1 + 1)\n        bounding_boxes_b_area = (b_x2 - a_x2 + 1) * (b_y2 - a_y2 + 1)\n\n        union = (\n            bounding_boxes_a_area\n            + tensorflow.keras.backend.transpose(bounding_boxes_b_area)\n        ) - intersection\n\n        return tensorflow.keras.backend.maximum(intersection / union, 0)\n\n    @staticmethod\n    def binary_crossentropy(_sentinel=None, target=None, output=None):\n        """"""\n        Args:\n            _sentinel: internal use only\n            labels: target image (n_masks2,N)\n            output: network output after softmax or sigmoid of size (n_masks1,N)\n        Returns:\n            Tensor with shape (n_masks1, n_masks2)\n            with the binary cross entropy between probs and labels\n            in [i,j]\n        """"""\n        epsilon = tensorflow.keras.backend.epsilon()\n\n        intermediate = tensorflow.keras.backend.dot(\n            target,\n            tensorflow.keras.backend.transpose(\n                tensorflow.keras.backend.log(output + epsilon)\n            ),\n        ) + tensorflow.keras.backend.dot(\n            (1.0 - target),\n            tensorflow.keras.backend.transpose(\n                tensorflow.keras.backend.log(1.0 - output + epsilon)\n            ),\n        )\n\n        return -intermediate / tensorflow.keras.backend.cast(\n            tensorflow.keras.backend.shape(target)[1],\n            dtype=tensorflow.keras.backend.floatx(),\n        )\n\n    @staticmethod\n    def categorical_crossentropy(_sentinel=None, target=None, output=None):\n        """"""\n        Args:\n            _sentinel: internal use only\n            labels: target image (n_masks1,N)\n            output: network output after softmax or sigmoid of size (n_masks2,N)\n        Returns:\n            Tensor with shape (n_masks1, n_masks2)\n            with the categorical cross entropy between probs and labels\n            in [i,j]\n        """"""\n        epsilon = tensorflow.keras.backend.epsilon()\n\n        # TODO: normalize dot product, size of the logits / number of classes\n        cce = -tensorflow.keras.backend.dot(\n            target,\n            tensorflow.keras.backend.transpose(\n                tensorflow.keras.backend.log(output + epsilon)\n            ),\n        )\n        return cce\n\n    @staticmethod\n    def compute_mask_loss(\n        _sentinel=None,\n        target_bounding_box=None,\n        output_bounding_box=None,\n        target_mask=None,\n        output_mask=None,\n        threshold=0.5,\n    ):\n        """"""\n        Args:\n            _sentinel: internal use only\n            target_bounding_box: ground truth bounding boxes (1,total_boxes1,4)\n            output_bounding_box: predicted bounding boxes (1,total_boxes2,4)\n            target_mask: ground truth masks (1,total_boxes1,N,M)\n            output_mask: predicted masks (1,total_boxes2,N,M)\n            threshold: a scalar iou value after which bounding box is valid for bce\n        Returns:\n            Mean binary cross entropy if IoU between bounding boxes is greater than threshold\n\n        """"""\n\n        target_bounding_box = tensorflow.keras.backend.squeeze(\n            target_bounding_box, axis=0\n        )\n        output_bounding_box = tensorflow.keras.backend.squeeze(\n            output_bounding_box, axis=0\n        )\n        target_mask = tensorflow.keras.backend.squeeze(target_mask, axis=0)\n        output_mask = tensorflow.keras.backend.squeeze(output_mask, axis=0)\n\n        index = tensorflow.keras.backend.prod(\n            tensorflow.keras.backend.shape(target_mask)[1:]\n        )\n\n        target_mask = tensorflow.keras.backend.reshape(target_mask, [-1, index])\n        output_mask = tensorflow.keras.backend.reshape(output_mask, [-1, index])\n\n        iou = RCNNMaskLoss.intersection_over_union(\n            target_bounding_box, output_bounding_box\n        )\n        b = tensorflow.keras.backend.greater(iou, threshold)\n        b = tensorflow.keras.backend.cast(b, dtype=tensorflow.keras.backend.floatx())\n\n        # labels = tensorflow.keras.backend.greater(output_mask, 0.5)\n        # labels = tensorflow.keras.backend.cast(labels, dtype=tensorflow.keras.backend.floatx())\n\n        # labels = tensorflow.keras.backend.equal(labels, target_mask)\n        # labels = tensorflow.keras.backend.cast(labels, dtype=tensorflow.keras.backend.floatx())\n\n        a = RCNNMaskLoss.binary_crossentropy(target=target_mask, output=output_mask)\n\n        # loss = tensorflow.keras.backend.sum(a * b) / tensorflow.keras.backend.sum(b)\n        loss = tensorflow.keras.backend.mean(a * b)\n\n        # TODO: we should try:\n        #   `tensorflow.keras.backend.mean(a * b) + tensorflow.keras.backend.mean(1 - b)`\n\n        return loss\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[3]\n'"
keras_rcnn/layers/object_detection/__init__.py,0,b'# -*- coding: utf-8 -*-\n\nfrom ._anchor import Anchor\n\nfrom ._object_proposal import ObjectProposal\n\nfrom ._proposal_target import ProposalTarget\n'
keras_rcnn/layers/object_detection/_anchor.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\nimport keras_rcnn.layers\n\n\nclass Anchor(tensorflow.keras.layers.Layer):\n    def __init__(\n        self,\n        aspect_ratios=None,\n        base_size=16,\n        clobber_positives=False,\n        negative_overlap=0.3,\n        padding=0,\n        positive_overlap=0.7,\n        scales=None,\n        stride=16,\n        **kwargs\n    ):\n        if aspect_ratios is None:\n            aspect_ratios = [0.5, 1, 2]  # [1:2, 1:1, 2:1]\n\n        if scales is None:\n            scales = [1, 2, 4, 8, 16]  # [32^{2}, 64^{2}, 128^{2}, 256^{2}, 512^{2}]\n\n        self.padding = padding\n\n        self.r = None\n        self.c = None\n\n        self.clobber_positives = clobber_positives\n\n        self.negative_overlap = negative_overlap\n        self.positive_overlap = positive_overlap\n\n        self.stride = stride\n\n        self.base_size = base_size\n\n        self.aspect_ratios = tensorflow.keras.backend.variable(aspect_ratios)\n\n        self.scales = tensorflow.keras.backend.variable(scales)\n\n        self.__shifted_anchors = None\n\n        self.metadata = None\n\n        super(Anchor, self).__init__(**kwargs)\n\n    @property\n    def _shifted_anchors(self):\n        if self.__shifted_anchors:\n            return self.__shifted_anchors\n        else:\n            self.__shifted_anchors = keras_rcnn.backend.shift(\n                (self.r, self.c),\n                self.stride,\n                self.base_size,\n                self.aspect_ratios,\n                self.scales,\n            )\n\n            return self.__shifted_anchors\n\n    def build(self, input_shape):\n        super(Anchor, self).build(input_shape)\n\n    # TODO: should AnchorTarget only be enabled during training\n    def call(self, inputs, **kwargs):\n        target_bounding_boxes, metadata, scores = inputs\n\n        self.metadata = metadata[0, :]\n\n        target_bounding_boxes = target_bounding_boxes[0]\n\n        self.r = tensorflow.keras.backend.shape(scores)[1]\n        self.c = tensorflow.keras.backend.shape(scores)[2]\n\n        self.k = self.r * self.c * tensorflow.keras.backend.shape(scores)[3]\n\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        output_bounding_boxes = self._shifted_anchors\n\n        # only keep anchors inside the image\n        indices_inside, anchors = self._inside_image(output_bounding_boxes)\n\n        anchors = keras_rcnn.backend.clip(anchors, self.metadata[:2])\n\n        # 2. obtain indices of gt boxes with the greatest overlap, balanced\n        # target_categories\n        argmax_overlaps_indices, target_categories = self._label(\n            target_bounding_boxes, anchors, indices_inside\n        )\n\n        target_bounding_boxes = tensorflow.keras.backend.gather(\n            target_bounding_boxes, argmax_overlaps_indices\n        )\n\n        # Convert fixed anchors in (x, y, w, h) to (dx, dy, dw, dh)\n        target_bounding_boxes = keras_rcnn.backend.bbox_transform(\n            anchors, target_bounding_boxes\n        )\n\n        # TODO: Why is target_bounding_box_targets\' shape (5, ?, 4)? Why is target_bounding_boxes\'\n        # shape (None, None, 4) and not (None, 4)?\n        target_bounding_boxes = tensorflow.keras.backend.reshape(\n            target_bounding_boxes, (-1, 4)\n        )\n\n        # map up to original set of anchors\n        target_categories = self._unmap(target_categories, indices_inside, fill=-1)\n\n        target_bounding_boxes = self._unmap(\n            target_bounding_boxes, indices_inside, fill=0\n        )\n\n        target_categories = tensorflow.keras.backend.expand_dims(\n            target_categories, axis=0\n        )\n\n        target_bounding_boxes = tensorflow.keras.backend.expand_dims(\n            target_bounding_boxes, axis=0\n        )\n\n        output_bounding_boxes = tensorflow.keras.backend.expand_dims(\n            output_bounding_boxes, axis=0\n        )\n\n        # TODO: implement inside and outside weights\n        return [output_bounding_boxes, target_bounding_boxes, target_categories]\n\n    def compute_output_shape(self, input_shape):\n        return [(1, None, 4), (1, None, 4), (1, None)]\n\n    def compute_mask(self, inputs, mask=None):\n        # unfortunately this is required\n        return 3 * [None]\n\n    def get_config(self):\n        configuration = {\n            ""aspect_ratios"": self.aspect_ratios,\n            ""base_size"": self.base_size,\n            ""clobber_positives"": self.clobber_positives,\n            ""negative_overlap"": self.negative_overlap,\n            ""padding"": self.padding,\n            ""positive_overlap"": self.positive_overlap,\n            ""scales"": self.scales,\n            ""stride"": self.stride,\n        }\n\n        return {**super(Anchor, self).get_config(), **configuration}\n\n    def _balance(self, labels):\n        """"""\n        balance labels by setting some to -1\n        :param labels: array of labels (1 is positive, 0 is negative, -1 is dont\n        care)\n        :return: array of labels\n        """"""\n\n        # subsample positive labels if we have too many\n        labels = self._subsample_positive_labels(labels)\n\n        # subsample negative labels if we have too many\n        labels = self._subsample_negative_labels(labels)\n\n        return labels\n\n    def _label(self, target, output, inds_inside):\n        """"""\n        Create bbox labels.\n        label: 1 is positive, 0 is negative, -1 is do not care\n\n        :param clobber_positives:\n        :param positive_overlap:\n        :param negative_overlap:\n        :param inds_inside: indices of anchors inside image\n        :param output: anchors\n        :param target: ground truth objects\n\n        :return: indices of gt boxes with the greatest overlap, balanced labels\n        """"""\n        ones = tensorflow.keras.backend.ones_like(\n            inds_inside, dtype=tensorflow.keras.backend.floatx()\n        )\n        labels = ones * -1\n        zeros = tensorflow.keras.backend.zeros_like(\n            inds_inside, dtype=tensorflow.keras.backend.floatx()\n        )\n\n        argmax_overlaps_inds, max_overlaps, gt_argmax_overlaps_inds = self._overlapping(\n            output, target, inds_inside\n        )\n\n        # Assign background labels first so that positive labels can clobber them.\n        if not self.clobber_positives:\n            labels = keras_rcnn.backend.where(\n                tensorflow.keras.backend.less(max_overlaps, self.negative_overlap),\n                zeros,\n                labels,\n            )\n\n        # fg label: for each gt, anchor with highest overlap\n\n        # TODO: generalize unique beyond 1D\n        unique_indices, unique_indices_indices = keras_rcnn.backend.unique(\n            gt_argmax_overlaps_inds, return_index=True\n        )\n\n        inverse_labels = tensorflow.keras.backend.gather(-1 * labels, unique_indices)\n\n        unique_indices = tensorflow.keras.backend.expand_dims(unique_indices, 1)\n\n        updates = tensorflow.keras.backend.ones_like(\n            tensorflow.keras.backend.reshape(unique_indices, (-1,)),\n            dtype=tensorflow.keras.backend.floatx(),\n        )\n\n        labels = keras_rcnn.backend.scatter_add_tensor(\n            labels, unique_indices, inverse_labels + updates\n        )\n\n        # Assign foreground labels based on IoU overlaps that are higher than\n        # RPN_POSITIVE_OVERLAP.\n        labels = keras_rcnn.backend.where(\n            tensorflow.keras.backend.greater_equal(max_overlaps, self.positive_overlap),\n            ones,\n            labels,\n        )\n\n        if self.clobber_positives:\n            # assign bg labels last so that negative labels can clobber positives\n            labels = keras_rcnn.backend.where(\n                tensorflow.keras.backend.less(max_overlaps, self.negative_overlap),\n                zeros,\n                labels,\n            )\n\n        return argmax_overlaps_inds, self._balance(labels)\n\n    @staticmethod\n    def _overlapping(output, target, inds_inside):\n        """"""\n        overlaps between the anchors and the gt boxes\n        :param output: Generated anchors\n        :param target: Ground truth bounding boxes\n        :param inds_inside:\n        :return:\n        """"""\n\n        assert tensorflow.keras.backend.ndim(output) == 2\n        assert tensorflow.keras.backend.ndim(target) == 2\n\n        reference = keras_rcnn.backend.intersection_over_union(output, target)\n\n        gt_argmax_overlaps_inds = tensorflow.keras.backend.argmax(reference, axis=0)\n\n        argmax_overlaps_inds = tensorflow.keras.backend.argmax(reference, axis=1)\n\n        arranged = tensorflow.keras.backend.arange(\n            0, tensorflow.keras.backend.shape(inds_inside)[0]\n        )\n\n        indices = tensorflow.keras.backend.stack(\n            [arranged, tensorflow.keras.backend.cast(argmax_overlaps_inds, ""int32"")],\n            axis=0,\n        )\n\n        indices = tensorflow.keras.backend.transpose(indices)\n\n        max_overlaps = keras_rcnn.backend.gather_nd(reference, indices)\n\n        return argmax_overlaps_inds, max_overlaps, gt_argmax_overlaps_inds\n\n    @staticmethod\n    def _subsample_negative_labels(labels, rpn_batchsize=256):\n        """"""\n        subsample negative labels if we have too many\n        :param labels: array of labels (1 is positive, 0 is negative, -1 is dont\n        care)\n\n        :return:\n        """"""\n        num_bg = (\n            rpn_batchsize\n            - tensorflow.keras.backend.shape(\n                keras_rcnn.backend.where(tensorflow.keras.backend.equal(labels, 1))\n            )[0]\n        )\n\n        bg_inds = keras_rcnn.backend.where(tensorflow.keras.backend.equal(labels, 0))\n\n        num_bg_inds = tensorflow.keras.backend.shape(bg_inds)[0]\n\n        size = num_bg_inds - num_bg\n\n        def more_negative():\n            indices = tensorflow.keras.backend.reshape(bg_inds, (-1,))\n            indices = keras_rcnn.backend.shuffle(indices)[:size]\n\n            updates = tensorflow.ones((size,)) * -1\n\n            inverse_labels = tensorflow.keras.backend.gather(labels, indices) * -1\n\n            indices = tensorflow.keras.backend.reshape(indices, (-1, 1))\n\n            return keras_rcnn.backend.scatter_add_tensor(\n                labels, indices, inverse_labels + updates\n            )\n\n        condition = tensorflow.keras.backend.less_equal(size, 0)\n\n        return tensorflow.keras.backend.switch(\n            condition, labels, lambda: more_negative()\n        )\n\n    @staticmethod\n    def _subsample_positive_labels(labels, rpn_fg_fraction=0.5, rpn_batchsize=256):\n        """"""\n        subsample positive labels if we have too many\n\n        :param labels: array of labels (1 is positive, 0 is negative,\n        -1 is dont care)\n\n        :return:\n        """"""\n\n        num_fg = int(rpn_fg_fraction * rpn_batchsize)\n\n        fg_inds = keras_rcnn.backend.where(tensorflow.keras.backend.equal(labels, 1))\n        num_fg_inds = tensorflow.keras.backend.shape(fg_inds)[0]\n\n        size = num_fg_inds - num_fg\n\n        def more_positive():\n            indices = tensorflow.keras.backend.reshape(fg_inds, (-1,))\n            indices = keras_rcnn.backend.shuffle(indices)[:size]\n\n            updates = tensorflow.ones((size,)) * -1\n\n            inverse_labels = tensorflow.keras.backend.gather(labels, indices) * -1\n\n            indices = tensorflow.keras.backend.reshape(indices, (-1, 1))\n\n            updates = inverse_labels + updates\n\n            return keras_rcnn.backend.scatter_add_tensor(labels, indices, updates)\n\n        condition = tensorflow.keras.backend.less_equal(size, 0)\n\n        return tensorflow.keras.backend.switch(\n            condition, labels, lambda: more_positive()\n        )\n\n    def _unmap(self, data, inds_inside, fill=0):\n        """""" Unmap a subset of item (data) back to the original set of items (of\n        size count) """"""\n\n        if tensorflow.keras.backend.ndim(data) == 1:\n            ret = (\n                tensorflow.ones((self.k,), dtype=tensorflow.keras.backend.floatx())\n                * fill\n            )\n\n            inds_nd = tensorflow.keras.backend.expand_dims(inds_inside)\n        else:\n            ret = (self.k, tensorflow.keras.backend.shape(data)[1])\n            ret = tensorflow.ones(ret, dtype=tensorflow.keras.backend.floatx()) * fill\n\n            data = tensorflow.keras.backend.transpose(data)\n            data = tensorflow.keras.backend.reshape(data, (-1,))\n\n            inds_ii = tensorflow.keras.backend.tile(inds_inside, [4])\n            inds_ii = tensorflow.keras.backend.expand_dims(inds_ii)\n\n            ones = tensorflow.keras.backend.expand_dims(\n                tensorflow.keras.backend.ones_like(inds_inside), 1\n            )\n\n            inds_coords = tensorflow.keras.backend.concatenate(\n                [ones * 0, ones, ones * 2, ones * 3], 0\n            )\n\n            inds_nd = tensorflow.keras.backend.concatenate([inds_ii, inds_coords], 1)\n\n        inverse_ret = keras_rcnn.backend.gather_nd(-1 * ret, inds_nd)\n        inverse_ret = keras_rcnn.backend.squeeze(inverse_ret)\n\n        updates = inverse_ret + data\n\n        ret = keras_rcnn.backend.scatter_add_tensor(ret, inds_nd, updates)\n\n        return ret\n\n    def _inside_image(self, boxes):\n        """"""\n        Calc indices of boxes which are located completely inside of the image\n        whose size is specified by img_info ((height, width, scale)-shaped array).\n\n        :param boxes: (None, 4) tensor containing boxes in original image\n        (x1, y1, x2, y2)\n\n        :param metadata: (height, width, scale)\n\n        :param allowed_border: allow boxes to be outside the image by\n        allowed_border pixels\n\n        :return: (None, 4) indices of boxes completely in original image, (None,\n        4) tensor of boxes completely inside image\n        """"""\n\n        indices = keras_rcnn.backend.where(\n            (boxes[:, 0] >= -self.padding)\n            & (boxes[:, 1] >= -self.padding)\n            & (boxes[:, 2] < self.padding + self.metadata[0])\n            & (boxes[:, 3] < self.padding + self.metadata[1])  # width  # height\n        )\n\n        indices = tensorflow.keras.backend.cast(indices, ""int32"")\n\n        gathered = tensorflow.keras.backend.gather(boxes, indices)\n\n        return indices[:, 0], tensorflow.keras.backend.reshape(gathered, [-1, 4])\n\n    @staticmethod\n    def _inside_and_outside_weights(\n        anchors, subsample, positive_weight, proposed_inside_weights\n    ):\n        """"""\n        Creates the inside_weights and outside_weights bounding-box weights.\n\n        Args:\n            anchors: Generated anchors.\n            subsample:  Labels obtained after subsampling.\n            positive_weight:\n            proposed_inside_weights:\n\n        Returns:\n            inside_weights:  Inside bounding-box weights.\n            outside_weights: Outside bounding-box weights.\n        """"""\n        number_of_anchors = tensorflow.keras.backend.int_shape(anchors)[0]\n\n        proposed_inside_weights = tensorflow.keras.backend.constant(\n            [proposed_inside_weights]\n        )\n        proposed_inside_weights = tensorflow.keras.backend.tile(\n            proposed_inside_weights, (number_of_anchors, 1)\n        )\n\n        positive_condition = tensorflow.keras.backend.equal(subsample, 1)\n        negative_condition = tensorflow.keras.backend.equal(subsample, 0)\n\n        if positive_weight < 0:\n            # Assign equal weights to both positive_weights and negative_weights\n            # labels.\n            examples = tensorflow.keras.backend.cast(\n                negative_condition, tensorflow.keras.backend.floatx()\n            )\n            examples = tensorflow.keras.backend.sum(examples)\n\n            positive_weights = tensorflow.keras.backend.ones_like(anchors) / examples\n            negative_weights = tensorflow.keras.backend.ones_like(anchors) / examples\n        else:\n            # Assign weights that favor either the positive or the\n            # negative_weights labels.\n            assert (positive_weight > 0) & (positive_weight < 1)\n\n            positive_examples = tensorflow.keras.backend.cast(\n                positive_condition, tensorflow.keras.backend.floatx()\n            )\n            positive_examples = tensorflow.keras.backend.sum(positive_examples)\n\n            negative_examples = tensorflow.keras.backend.cast(\n                negative_condition, tensorflow.keras.backend.floatx()\n            )\n            negative_examples = tensorflow.keras.backend.sum(negative_examples)\n\n            positive_weights = (\n                tensorflow.keras.backend.ones_like(anchors)\n                * (0 + positive_weight)\n                / positive_examples\n            )\n            negative_weights = (\n                tensorflow.keras.backend.ones_like(anchors)\n                * (1 - positive_weight)\n                / negative_examples\n            )\n\n        inside_weights = tensorflow.keras.backend.zeros_like(anchors)\n        inside_weights = keras_rcnn.backend.where(\n            positive_condition, proposed_inside_weights, inside_weights\n        )\n\n        outside_weights = tensorflow.keras.backend.zeros_like(anchors)\n        outside_weights = keras_rcnn.backend.where(\n            positive_condition, positive_weights, outside_weights\n        )\n        outside_weights = keras_rcnn.backend.where(\n            negative_condition, negative_weights, outside_weights\n        )\n\n        return inside_weights, outside_weights\n'"
keras_rcnn/layers/object_detection/_object_proposal.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\nimport keras_rcnn.layers\n\n\nclass ObjectProposal(tensorflow.keras.layers.Layer):\n    """"""Propose object-containing regions from anchors\n\n    # Arguments\n        maximum_proposals: maximum number of regions allowed\n        min_size: minimum width/height of proposals\n        stride: stride size\n\n    # Input shape\n        (width of feature map, height of feature map, scale), (None, 4), (None)\n\n    # Output shape\n        (# images, # proposals, 4)\n    """"""\n\n    def __init__(self, maximum_proposals=300, minimum_size=16, stride=16, **kwargs):\n        self.maximum_proposals = maximum_proposals\n\n        # minimum width/height of proposals in original image size\n        self.minimum_size = minimum_size\n\n        self.stride = stride\n\n        super(ObjectProposal, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(ObjectProposal, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        """"""\n        `image_shape_and_scale` has the shape [width, height, scale]\n        """"""\n        anchors, image_shape_and_scale, deltas, scores = inputs\n        anchors = tensorflow.keras.backend.reshape(anchors, (-1, 4))\n\n        # TODO: Fix usage of batch index\n        batch_index = 0\n\n        image_shape = image_shape_and_scale[batch_index, :2]\n        image_scale = image_shape_and_scale[batch_index, -1]\n\n        # 1. generate proposals from bbox deltas and shifted anchors\n\n        deltas = tensorflow.keras.backend.reshape(deltas, (-1, 4))\n        scores = tensorflow.keras.backend.reshape(scores, (-1, 1))\n\n        deltas = keras_rcnn.backend.bbox_transform_inv(anchors, deltas)\n\n        # 2. clip predicted boxes to image\n        proposals = keras_rcnn.backend.clip(deltas, image_shape)\n\n        # 3. remove predicted boxes with either height or width < threshold\n        # (NOTE: convert min_size to input image scale stored in im_info[2])\n        indices = filter_boxes(proposals, self.minimum_size * image_scale)\n        proposals = tensorflow.keras.backend.gather(proposals, indices)\n\n        scores = scores[..., (scores.shape[-1] // 2) :]\n        scores = tensorflow.keras.backend.reshape(scores, (-1, 1))\n        scores = tensorflow.keras.backend.gather(scores, indices)\n        scores = tensorflow.keras.backend.flatten(scores)\n\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        indices = keras_rcnn.backend.argsort(scores)\n\n        # TODO: is this a sensible value? parameterize?\n        rpn_pre_nms_top_n = 12000\n\n        # 5. take top pre_nms_topN (e.g. 6000)\n        if rpn_pre_nms_top_n > 0:\n            indices = indices[:rpn_pre_nms_top_n]\n\n        proposals = tensorflow.keras.backend.gather(proposals, indices)\n        scores = tensorflow.keras.backend.gather(scores, indices)\n\n        # 6. apply nms (e.g. threshold = 0.7)\n        indices = keras_rcnn.backend.non_maximum_suppression(\n            boxes=proposals,\n            scores=scores,\n            maximum=self.maximum_proposals,\n            threshold=0.7,\n        )\n\n        proposals = tensorflow.keras.backend.gather(proposals, indices)\n\n        # 8. return the top proposals (-> RoIs top)\n        return tensorflow.keras.backend.expand_dims(proposals, 0)\n\n    def compute_output_shape(self, input_shape):\n        return None, None, 4\n\n    def get_config(self):\n        configuration = {\n            ""maximum_proposals"": self.maximum_proposals,\n            ""minimum_size"": self.minimum_size,\n            ""stride"": self.stride,\n        }\n\n        return {**super(ObjectProposal, self).get_config(), **configuration}\n\n\ndef filter_boxes(proposals, minimum):\n    """"""\n    Filters proposed RoIs so that all have width and height at least as big as\n    minimum\n    """"""\n    ws = proposals[:, 2] - proposals[:, 0] + 1\n    hs = proposals[:, 3] - proposals[:, 1] + 1\n\n    indices = keras_rcnn.backend.where((ws >= minimum) & (hs >= minimum))\n\n    indices = tensorflow.keras.backend.flatten(indices)\n\n    return tensorflow.keras.backend.cast(indices, ""int32"")\n'"
keras_rcnn/layers/object_detection/_proposal_target.py,0,"b'# -*- coding: utf-8 -*-\n\nimport tensorflow\n\nimport keras_rcnn.backend\n\n\nclass ProposalTarget(tensorflow.keras.layers.Layer):\n    """"""\n    # Arguments\n    fg_fraction: percent foreground objects\n\n    batchsize: number of objects in a batch\n\n    num_images: number of images to consider per batch (set to 1 for the\n    time being)\n\n    num_classes: number of classes (object+background)\n\n    # Input shape\n    (None, None, 4), (None, None, classes), (None, None, 4)\n\n    # Output shape\n    [(None, None, 4), (None, None, classes), (None, None, 4)]\n    """"""\n\n    def __init__(\n        self,\n        foreground=0.5,\n        foreground_threshold=(0.5, 1.0),\n        background_threshold=(0.1, 0.5),\n        maximum_proposals=32,\n        **kwargs\n    ):\n        """"""\n        :param foreground:\n        :param foreground_threshold:\n        :param background_threshold:\n        :param maximum_proposals:\n        """"""\n        self._batch_size = None\n\n        self.foreground = foreground\n\n        self.foreground_threshold = foreground_threshold\n        self.background_threshold = background_threshold\n\n        self.maximum_proposals = maximum_proposals\n\n        self.rois_per_image = self.maximum_proposals / self.batch_size\n        self.fg_rois_per_image = tensorflow.keras.backend.cast(\n            self.foreground * self.rois_per_image, ""int32""\n        )\n\n        self.fg_rois_per_this_image = None\n\n        super(ProposalTarget, self).__init__(**kwargs)\n\n    @property\n    def batch_size(self):\n        if self._batch_size:\n            return self._batch_size\n        else:\n            self._batch_size = 1\n\n            return self._batch_size\n\n    @batch_size.setter\n    def batch_size(self, x):\n        self._batch_size = x\n\n    def build(self, input_shape):\n        super(ProposalTarget, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        # Proposal ROIs (x1, y1, x2, y2) coming from RPN\n        # (i.e., rpn.proposal_layer.ProposalLayer), or any other source\n        # GT boxes (x1, y1, x2, y2)\n        # and other times after box coordinates -- normalize to one format\n\n        # target_categories (class1, class2, ... , num_classes)\n        # Include ground-truth boxes in the set of candidate rois\n        (\n            target_bounding_boxes,\n            target_categories,\n            output_proposal_bounding_boxes,\n        ) = inputs\n\n        output_proposal_bounding_boxes = tensorflow.keras.backend.in_train_phase(\n            x=tensorflow.keras.backend.concatenate(\n                (output_proposal_bounding_boxes, target_bounding_boxes), axis=1\n            ),\n            alt=output_proposal_bounding_boxes,\n            training=training,\n        )\n\n        # Sample rois with classification target_categories and bounding box regression\n        # targets\n\n        # TODO: Fix usage of batch index\n        batch_index = 0\n\n        output_proposal_bounding_boxes = output_proposal_bounding_boxes[\n            batch_index, :, :\n        ]\n        target_bounding_boxes = target_bounding_boxes[batch_index, :, :]\n        target_categories = target_categories[batch_index, :, :]\n\n        # TODO: Fix hack\n        condition = tensorflow.keras.backend.not_equal(\n            tensorflow.keras.backend.sum(target_bounding_boxes), 0\n        )\n\n        def test(proposals, gt_boxes, gt_labels):\n            N = tensorflow.keras.backend.shape(proposals)[0]\n            number_of_classes = tensorflow.keras.backend.shape(gt_labels)[1]\n            number_of_coordinates = 4 * number_of_classes\n            return (\n                proposals,\n                tensorflow.zeros((N, number_of_classes)),\n                tensorflow.zeros((N, number_of_coordinates)),\n            )\n\n        sample_outputs = tensorflow.keras.backend.switch(\n            condition,\n            lambda: self.sample(\n                output_proposal_bounding_boxes, target_bounding_boxes, target_categories\n            ),\n            lambda: test(\n                output_proposal_bounding_boxes, target_bounding_boxes, target_categories\n            ),\n        )\n\n        output_proposal_bounding_boxes = tensorflow.keras.backend.expand_dims(\n            sample_outputs[0], 0\n        )\n        target_categories = tensorflow.keras.backend.expand_dims(sample_outputs[1], 0)\n        bounding_box_targets = tensorflow.keras.backend.expand_dims(\n            sample_outputs[2], 0\n        )\n\n        return [bounding_box_targets, target_categories, output_proposal_bounding_boxes]\n\n    def get_config(self):\n        configuration = {\n            ""background_threshold"": self.background_threshold,\n            ""foreground"": self.foreground,\n            ""foreground_threshold"": self.foreground_threshold,\n            ""maximum_proposals"": self.maximum_proposals,\n        }\n\n        return {**super(ProposalTarget, self).get_config(), **configuration}\n\n    def sample(self, proposals, true_bounding_boxes, true_labels):\n        """"""\n        Generate a random sample of RoIs comprising foreground and background\n        examples.\n\n        all_rois is (N, 4)\n        gt_boxes is (K, 4) with 4 coordinates\n\n        gt_labels is in one hot form\n        """"""\n        number_of_classes = tensorflow.keras.backend.shape(true_labels)[1]\n\n        true_labels = tensorflow.keras.backend.argmax(true_labels, axis=1)\n\n        intersection_over_union = keras_rcnn.backend.intersection_over_union(\n            proposals, true_bounding_boxes\n        )\n\n        gt_assignment = tensorflow.keras.backend.argmax(intersection_over_union, axis=1)\n\n        maximum_intersection_over_union = tensorflow.keras.backend.max(\n            intersection_over_union, axis=1\n        )\n\n        # finds the ground truth labels corresponding to the ground truth boxes with greatest overlap for each predicted regions of interest\n        # TODO: rename `all_labels`\n        all_labels = tensorflow.keras.backend.gather(true_labels, gt_assignment)\n\n        # Select proposals with given parameters for fg/bg objects\n        # TODO: rename `find_foreground_and_background_proposal_indices`\n        # TODO: rename `foreground_and_background_proposal_indices`\n\n        foreground_and_background_proposal_indices = self.find_foreground_and_background_proposal_indices(\n            maximum_intersection_over_union\n        )\n\n        # Select sampled values from various arrays:\n        sampled_labels = tensorflow.keras.backend.gather(\n            all_labels, foreground_and_background_proposal_indices\n        )\n        sampled_proposal_bounding_boxes = tensorflow.keras.backend.gather(\n            proposals, foreground_and_background_proposal_indices\n        )\n\n        sampled_labels = self.set_label_background(sampled_labels)\n\n        true_bounding_boxes = tensorflow.keras.backend.gather(\n            true_bounding_boxes,\n            tensorflow.keras.backend.gather(\n                gt_assignment, foreground_and_background_proposal_indices\n            ),\n        )\n\n        bbox_targets = self.get_bbox_targets(\n            sampled_proposal_bounding_boxes,\n            true_bounding_boxes,\n            sampled_labels,\n            number_of_classes,\n        )\n\n        sampled_labels = tensorflow.keras.backend.one_hot(\n            sampled_labels, number_of_classes\n        )\n\n        return sampled_proposal_bounding_boxes, sampled_labels, bbox_targets\n\n    def set_label_background(self, labels):\n        # Clamp labels for the background RoIs to 0\n        update_indices = tensorflow.keras.backend.arange(\n            self.fg_rois_per_this_image, tensorflow.keras.backend.shape(labels)[0]\n        )\n        update_indices = tensorflow.keras.backend.reshape(update_indices, (-1, 1))\n\n        # By making the label = background\n        inverse_labels = keras_rcnn.backend.gather_nd(labels, update_indices) * -1\n        labels = keras_rcnn.backend.scatter_add_tensor(\n            labels, update_indices, inverse_labels\n        )\n\n        return labels\n\n    def compute_output_shape(self, input_shape):\n        num_classes = input_shape[1][2]\n\n        self.batch_size = input_shape[0][0]\n\n        return [\n            (self.batch_size, None, 4 * num_classes),\n            (self.batch_size, None, num_classes),\n            (self.batch_size, None, 4),\n        ]\n\n    def compute_mask(self, inputs, mask=None):\n        return [None, None, None]\n\n    def get_bbox_targets(self, rois, gt_boxes, labels, num_classes):\n        gt_boxes = tensorflow.keras.backend.cast(\n            gt_boxes, tensorflow.keras.backend.floatx()\n        )\n        targets = keras_rcnn.backend.bbox_transform(rois, gt_boxes)\n        return self.get_bbox_regression_labels(targets, labels, num_classes)\n\n    def find_foreground_and_background_proposal_indices(self, max_overlaps):\n        # Select foreground RoIs as those with >= FG_THRESH overlap\n        fg_inds = keras_rcnn.backend.where(\n            (max_overlaps <= self.foreground_threshold[1])\n            & (max_overlaps >= self.foreground_threshold[0])\n        )\n\n        # Guard against the case when an image has fewer than fg_rois_per_image\n        # foreground RoIs\n        self.fg_rois_per_this_image = tensorflow.keras.backend.minimum(\n            self.fg_rois_per_image, tensorflow.keras.backend.shape(fg_inds)[0]\n        )\n\n        # Sample foreground regions without replacement\n        fg_inds = self.sample_indices(fg_inds, self.fg_rois_per_this_image)\n\n        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n        bg_inds = keras_rcnn.backend.where(\n            (max_overlaps < self.background_threshold[1])\n            & (max_overlaps >= self.background_threshold[0])\n        )\n\n        # Compute number of background RoIs to take from this image (guarding\n        # against there being fewer than desired)\n        bg_rois_per_this_image = (\n            tensorflow.keras.backend.cast(self.rois_per_image, ""int32"")\n            - self.fg_rois_per_this_image\n        )\n        bg_rois_per_this_image = tensorflow.keras.backend.cast(\n            bg_rois_per_this_image, ""int32""\n        )\n        bg_rois_per_this_image = tensorflow.keras.backend.minimum(\n            bg_rois_per_this_image, tensorflow.keras.backend.shape(bg_inds)[0]\n        )\n\n        # Sample background regions without replacement\n        bg_inds = self.sample_indices(bg_inds, bg_rois_per_this_image)\n\n        # The indices that we\'re selecting (both fg and bg)\n        keep_inds = tensorflow.keras.backend.concatenate([fg_inds, bg_inds])\n\n        return keep_inds\n\n    def sample_indices(self, indices, size):\n        return keras_rcnn.backend.shuffle(\n            tensorflow.keras.backend.reshape(indices, (-1,))\n        )[:size]\n\n    def get_bbox_regression_labels(self, bbox_target_data, labels, num_classes):\n        """"""Bounding-box regression targets (bbox_target_data) are stored in a\n        form N x (tx, ty, tw, th), labels N\n        This function expands those targets into the 4-of-4*K representation used\n        by the network (i.e. only one class has non-zero targets).\n        Returns:\n            bbox_target: N x 4K blob of regression targets\n        """"""\n\n        n = tensorflow.keras.backend.shape(bbox_target_data)[0]\n\n        bbox_targets = tensorflow.zeros(\n            (n, 4 * num_classes), dtype=tensorflow.keras.backend.floatx()\n        )\n\n        inds = tensorflow.keras.backend.reshape(\n            keras_rcnn.backend.where(labels > 0), (-1,)\n        )\n\n        labels = tensorflow.keras.backend.gather(labels, inds)\n\n        start = 4 * labels\n\n        ii = tensorflow.keras.backend.expand_dims(inds)\n        ii = tensorflow.keras.backend.tile(ii, [4, 1])\n\n        aa = tensorflow.keras.backend.expand_dims(\n            tensorflow.keras.backend.concatenate(\n                [start, start + 1, start + 2, start + 3], 0\n            )\n        )\n        aa = tensorflow.keras.backend.cast(aa, dtype=""int64"")\n\n        indices = tensorflow.keras.backend.concatenate([ii, aa], 1)\n\n        updates = tensorflow.keras.backend.gather(bbox_target_data, inds)\n        updates = tensorflow.keras.backend.transpose(updates)\n        updates = tensorflow.keras.backend.reshape(updates, (-1,))\n\n        updates = tensorflow.keras.backend.cast(\n            updates, tensorflow.keras.backend.floatx()\n        )\n        bbox_targets = keras_rcnn.backend.scatter_add_tensor(\n            bbox_targets, indices, updates\n        )\n\n        return bbox_targets\n'"
tests/layers/losses/__init__.py,0,b''
tests/layers/losses/test_mask_rcnn.py,66,"b'import keras_rcnn.layers.losses\nimport keras.backend\nimport numpy\n\nimport pytest\n\n\ndef bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the intersection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n\n    # return the intersection over union value\n    return iou\n\n\ndef cross_entropy(predictions, targets, epsilon=1e-12):\n    """"""\n    Computes cross entropy between targets (encoded as one-hot vectors)\n    and predictions.\n    Input: predictions (N, k) ndarray\n           targets (N, k) ndarray\n    Returns: scalar\n    """"""\n    predictions = numpy.clip(predictions, epsilon, 1.0 - epsilon)\n    N = predictions.shape[0]\n    ce = -numpy.sum(targets * numpy.log(predictions))\n    return ce\n\n\nclass TestMaskRCNN:\n    def test_categorical_crossentropy(self):\n        layer = keras_rcnn.layers.losses.RCNNMaskLoss()\n\n        n1 = 30\n        n2 = 50\n        N = 10\n\n        output = numpy.random.random((n1, N))\n        target = numpy.random.random((n2, N))\n        output_tensor = keras.backend.tf.convert_to_tensor(\n            output, dtype=keras.backend.tf.float32\n        )\n        target_tensor = keras.backend.tf.convert_to_tensor(\n            target, dtype=keras.backend.tf.float32\n        )\n\n        expected_losses = numpy.zeros((n2, n1))\n        for i in range(0, n2):\n            for j in range(0, n1):\n                expected_losses[i, j] = cross_entropy(\n                    output[j, :], target[i, :], epsilon=keras.backend.epsilon()\n                )\n\n        losses = numpy.array(\n            keras.backend.eval(\n                layer.categorical_crossentropy(\n                    target=target_tensor, output=output_tensor\n                )\n            )\n        )\n\n        precision = 0.001\n        expected_loss = round(numpy.mean(expected_losses) / precision) * precision\n        loss = round(numpy.mean(losses) / precision) * precision\n\n        # print(\'\')\n        # print(loss)\n        # print(expected_loss)\n\n        assert expected_loss == loss\n\n    def test_binary_crossentropy(self):\n        layer = keras_rcnn.layers.losses.RCNNMaskLoss()\n\n        n1 = 30\n        n2 = 50\n\n        output = numpy.random.random((n1, 1))\n        output = numpy.concatenate((output, 1 - output), axis=1)\n        target = numpy.random.random((n2, 1))\n        target = numpy.concatenate((target, 1 - target), axis=1)\n        output_tensor = keras.backend.tf.convert_to_tensor(\n            output, dtype=keras.backend.tf.float32\n        )\n        target_tensor = keras.backend.tf.convert_to_tensor(\n            target, dtype=keras.backend.tf.float32\n        )\n\n        lossesBCE = numpy.array(\n            keras.backend.eval(\n                layer.binary_crossentropy(target=target_tensor, output=output_tensor)\n            )\n        )\n\n        lossesCCE = numpy.array(\n            keras.backend.eval(\n                layer.categorical_crossentropy(\n                    target=target_tensor, output=output_tensor\n                )\n            )\n        )\n\n        precision = 1000\n        lossBCE = round(numpy.mean(lossesBCE) * precision) / precision\n        lossCCE = round(numpy.mean(lossesCCE) * precision) / precision\n\n        # print(\'\')\n        # print(lossBCE)\n        # print(lossCCE)\n\n        assert lossBCE == lossCCE\n        # assert True\n\n    def test_compute_mask_loss(self):\n        layer = keras_rcnn.layers.losses.RCNNMaskLoss()\n\n        threshold = 0.15\n\n        nbTar = 50\n        nbOut = 30\n        mask_size = 28\n        target_bounding_boxes1 = numpy.random.randint(0, 256, (1, nbTar, 1))\n        target_bounding_boxes2 = numpy.random.randint(256, 512, (1, nbTar, 1))\n        target_bounding_boxes3 = numpy.random.randint(0, 256, (1, nbTar, 1))\n        target_bounding_boxes4 = numpy.random.randint(256, 512, (1, nbTar, 1))\n\n        bb_true = numpy.concatenate(\n            (\n                target_bounding_boxes1,\n                target_bounding_boxes2,\n                target_bounding_boxes3,\n                target_bounding_boxes4,\n            ),\n            axis=2,\n        )\n        target_bounding_boxes = keras.backend.tf.convert_to_tensor(\n            bb_true, dtype=keras.backend.tf.float32\n        )\n\n        output_bounding_boxes1 = numpy.random.randint(0, 256, (1, nbOut, 1))\n        output_bounding_boxes2 = numpy.random.randint(256, 512, (1, nbOut, 1))\n        output_bounding_boxes3 = numpy.random.randint(0, 256, (1, nbOut, 1))\n        output_bounding_boxes4 = numpy.random.randint(256, 512, (1, nbOut, 1))\n\n        bb_pred = numpy.concatenate(\n            (\n                output_bounding_boxes1,\n                output_bounding_boxes2,\n                output_bounding_boxes3,\n                output_bounding_boxes4,\n            ),\n            axis=2,\n        )\n        output_bounding_boxes = keras.backend.tf.convert_to_tensor(\n            bb_pred, dtype=keras.backend.tf.float32\n        )\n\n        y_true = numpy.random.randint(0, 2, (1, nbTar, mask_size, mask_size)).astype(\n            float\n        )\n        target_masks = keras.backend.tf.convert_to_tensor(\n            y_true, dtype=keras.backend.tf.float32\n        )\n        y_pred = numpy.random.random((1, nbOut, mask_size, mask_size))\n        output_masks = keras.backend.tf.convert_to_tensor(\n            y_pred, dtype=keras.backend.tf.float32\n        )\n\n        loss = layer.compute_mask_loss(\n            target_bounding_box=target_bounding_boxes,\n            output_bounding_box=output_bounding_boxes,\n            target_mask=target_masks,\n            output_mask=output_masks,\n            threshold=threshold,\n        )\n\n        loss = keras.backend.eval(loss)\n\n        bb_pred = numpy.squeeze(bb_pred, axis=0)\n        bb_true = numpy.squeeze(bb_true, axis=0)\n        count = 0.0\n        for i in range(0, nbTar):\n            for j in range(0, nbOut):\n                iou = bb_intersection_over_union(bb_true[i, :], bb_pred[j, :])\n                if iou > threshold:\n                    tm = keras.backend.eval(target_masks[0, i, :, :])\n                    om = keras.backend.eval(output_masks[0, j, :, :])\n                    tm = keras.backend.tf.convert_to_tensor(numpy.array([tm.flatten()]))\n                    om = keras.backend.tf.convert_to_tensor(numpy.array([om.flatten()]))\n                    cce = keras.backend.eval(\n                        layer.binary_crossentropy(target=tm, output=om)\n                    )[0, 0]\n                    count += cce\n\n        expected_loss = count / nbTar / nbOut\n\n        precision = 1000000.0\n        loss = round(loss * precision)\n        expected_loss = round(expected_loss * precision)\n\n        # print(\'\')\n        # print(loss)\n        # print(expected_loss)\n\n        assert loss == expected_loss\n\n    def test_intersection_over_union(self):\n        s1 = 50\n        s2 = 30\n        x = numpy.random.random((s1, 4))\n        y = numpy.random.random((s2, 4))\n        layer = keras_rcnn.layers.losses.RCNNMaskLoss()\n\n        iou1 = numpy.array(keras.backend.eval(layer.intersection_over_union(x, y)))\n        iou2 = numpy.zeros((s1, s2))\n\n        for i in range(0, s1):\n            for j in range(0, s2):\n                iou2[i, j] = bb_intersection_over_union(x[i, :], y[j, :])\n\n        # print(\'\')\n        # print(numpy.sum(iou1 != iou2))\n        assert numpy.sum(iou1 != iou2) == 0\n\n    def test_layer(self):\n        print("""")\n        layer = keras_rcnn.layers.losses.RCNNMaskLoss()\n\n        # TESTS CROSS-ENTROPY ------------------------------------------------------------------------------------------\n        n1 = 30\n        n2 = 50\n        N = 10\n\n        output = numpy.random.random((n1, N))\n        target = numpy.random.random((n2, N + 1))\n        output_tensor = keras.backend.tf.convert_to_tensor(\n            output, dtype=keras.backend.tf.float32\n        )\n        target_tensor = keras.backend.tf.convert_to_tensor(\n            target, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.categorical_crossentropy(target=target_tensor, output=output_tensor)\n        print(e_info)\n        with pytest.raises(Exception):\n            layer.binary_crossentropy(target=target_tensor, output=output_tensor)\n        print(e_info)\n\n        # TEST SIZE TARGET ---------------------------------------------------------------------------------------------\n        nb1 = 20\n        nb2 = 40\n        N = 15\n        M = 25\n\n        tbb = numpy.random.random((1, nb1, 4))\n        obb = numpy.random.random((1, nb2, 4))\n        tm = numpy.random.random((1, nb1 + 1, N, M))\n        om = numpy.random.random((1, nb2, N, M))\n        target_bounding_box = keras.backend.tf.convert_to_tensor(\n            tbb, dtype=keras.backend.tf.float32\n        )\n        output_bounding_box = keras.backend.tf.convert_to_tensor(\n            obb, dtype=keras.backend.tf.float32\n        )\n        target_mask = keras.backend.tf.convert_to_tensor(\n            tm, dtype=keras.backend.tf.float32\n        )\n        output_mask = keras.backend.tf.convert_to_tensor(\n            om, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        # TEST SIZE OUTPUT ---------------------------------------------------------------------------------------------\n        tm = numpy.random.random((1, nb1, N, M))\n        om = numpy.random.random((1, nb2 + 1, N, M))\n        target_mask = keras.backend.tf.convert_to_tensor(\n            tm, dtype=keras.backend.tf.float32\n        )\n        output_mask = keras.backend.tf.convert_to_tensor(\n            om, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        # TESTS PARAMS BOUNDING BOXES ----------------------------------------------------------------------------------\n        tbb = numpy.random.random((1, nb1, 3))\n        obb = numpy.random.random((1, nb2, 4))\n        tm = numpy.random.random((1, nb1, N, M))\n        om = numpy.random.random((1, nb2, N, M))\n        target_bounding_box = keras.backend.tf.convert_to_tensor(\n            tbb, dtype=keras.backend.tf.float32\n        )\n        output_bounding_box = keras.backend.tf.convert_to_tensor(\n            obb, dtype=keras.backend.tf.float32\n        )\n        target_mask = keras.backend.tf.convert_to_tensor(\n            tm, dtype=keras.backend.tf.float32\n        )\n        output_mask = keras.backend.tf.convert_to_tensor(\n            om, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        tbb = numpy.random.random((1, nb1, 4))\n        obb = numpy.random.random((1, nb2, 3))\n        target_bounding_box = keras.backend.tf.convert_to_tensor(\n            tbb, dtype=keras.backend.tf.float32\n        )\n        output_bounding_box = keras.backend.tf.convert_to_tensor(\n            obb, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        tbb = numpy.random.random((1, nb1, 3))\n        obb = numpy.random.random((1, nb2, 3))\n        target_bounding_box = keras.backend.tf.convert_to_tensor(\n            tbb, dtype=keras.backend.tf.float32\n        )\n        output_bounding_box = keras.backend.tf.convert_to_tensor(\n            obb, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        # TEST BATCH DIMENSION -----------------------------------------------------------------------------------------\n        tbb = numpy.random.random((nb1, 4))\n        obb = numpy.random.random((nb2, 4))\n        tm = numpy.random.random((nb1, N, M))\n        om = numpy.random.random((nb2, N, M))\n        target_bounding_box = keras.backend.tf.convert_to_tensor(\n            tbb, dtype=keras.backend.tf.float32\n        )\n        output_bounding_box = keras.backend.tf.convert_to_tensor(\n            obb, dtype=keras.backend.tf.float32\n        )\n        target_mask = keras.backend.tf.convert_to_tensor(\n            tm, dtype=keras.backend.tf.float32\n        )\n        output_mask = keras.backend.tf.convert_to_tensor(\n            om, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        # TEST MASK FEATURES DIMENSION ---------------------------------------------------------------------------------\n        tbb = numpy.random.random((1, nb1, 4))\n        obb = numpy.random.random((1, nb2, 4))\n        tm = numpy.random.random((1, nb1, N, M))\n        om = numpy.random.random((1, nb2, N, M, 2))\n        target_bounding_box = keras.backend.tf.convert_to_tensor(\n            tbb, dtype=keras.backend.tf.float32\n        )\n        output_bounding_box = keras.backend.tf.convert_to_tensor(\n            obb, dtype=keras.backend.tf.float32\n        )\n        target_mask = keras.backend.tf.convert_to_tensor(\n            tm, dtype=keras.backend.tf.float32\n        )\n        output_mask = keras.backend.tf.convert_to_tensor(\n            om, dtype=keras.backend.tf.float32\n        )\n\n        with pytest.raises(Exception) as e_info:\n            layer.compute_mask_loss(\n                target_bounding_box=target_bounding_box,\n                output_bounding_box=output_bounding_box,\n                target_mask=target_mask,\n                output_mask=output_mask,\n            )\n        print(e_info)\n\n        assert True\n'"
tests/layers/losses/test_rcnn.py,0,"b'import keras.backend\nimport keras_rcnn.layers\nimport numpy\n\n\nclass TestRCNN:\n    def test_call(self):\n        classes = 3\n\n        target_deltas = keras.backend.ones((1, 2, 4 * classes))\n        target_scores = keras.backend.variable([[0, 0, 1], [0, 0, 1]])\n        target_scores = keras.backend.expand_dims(target_scores, 0)\n        target_scores = keras.backend.cast(target_scores, keras.backend.floatx())\n\n        output_deltas = keras.backend.ones((1, 2, 4 * classes))\n        output_scores = keras.backend.variable([[0, 0, 1], [0, 0, 1]])\n        output_scores = keras.backend.expand_dims(output_scores, 0)\n        output_scores = keras.backend.cast(output_scores, keras.backend.floatx())\n\n        layer = keras_rcnn.layers.RCNN()\n\n        layer.get_config()\n\n        layer.call([target_deltas, target_scores, output_deltas, output_scores])\n\n        classification_loss = layer.classification_loss()\n\n        classification_loss = keras.backend.eval(classification_loss)\n\n        numpy.testing.assert_almost_equal(classification_loss, 0.0)\n\n        regression_loss = layer.regression_loss()\n\n        regression_loss = keras.backend.eval(regression_loss)\n\n        numpy.testing.assert_almost_equal(regression_loss, 0.0)\n\n        loss = layer.losses.pop()\n\n        loss = keras.backend.eval(loss)\n\n        numpy.testing.assert_almost_equal(loss, 0.0)\n\n        target_scores = keras.backend.variable([[0, 0, 1], [0, 0, 1]])\n        target_scores = keras.backend.expand_dims(target_scores, 0)\n\n        output_scores = keras.backend.variable([[0, 1, 0], [0, 1, 0]])\n        output_scores = keras.backend.expand_dims(output_scores, 0)\n\n        layer.call([target_deltas, target_scores, output_deltas, output_scores])\n\n        classification_loss = layer.classification_loss()\n\n        classification_loss = keras.backend.eval(classification_loss)\n\n        numpy.testing.assert_almost_equal(\n            classification_loss, numpy.log(1.0 / keras.backend.epsilon()), 5\n        )\n\n        regression_loss = layer.regression_loss()\n\n        regression_loss = keras.backend.eval(regression_loss)\n\n        numpy.testing.assert_almost_equal(regression_loss, 0.0)\n\n        loss = layer.losses.pop()\n\n        loss = keras.backend.eval(loss)\n\n        numpy.testing.assert_almost_equal(\n            loss, numpy.log(1.0 / keras.backend.epsilon()), 5\n        )\n\n        target_scores = keras.backend.variable([[0, 0, 1], [0, 1, 0]])\n        target_scores = keras.backend.expand_dims(target_scores, 0)\n        output_scores = keras.backend.variable([[0, 1, 0], [0, 1, 0]])\n        output_scores = keras.backend.expand_dims(output_scores, 0)\n\n        target_deltas = keras.backend.variable(\n            [\n                [0, 0, 0, 0, 0, 0, 0, 0, -0.1, 0.2, 0.3, 0.4],\n                [-0.1, 0.2, 1, -3, 0, -1, 1.3, -0.1, 2, 0.3, -1.5, 0.6],\n            ]\n        )\n        target_deltas = keras.backend.expand_dims(target_deltas, 0)\n        output_deltas = keras.backend.variable(\n            [\n                [-0.1, 0.2, 1, -3, 0, 0, 0.1, 1, 0, 0.1, -0.1, 0.8],\n                [-0.1, 2, 0, 0, 1, 1, -0.1, -0.5, 0, 1, -0.5, -1],\n            ]\n        )\n        output_deltas = keras.backend.expand_dims(output_deltas, 0)\n\n        layer.call([target_deltas, target_scores, output_deltas, output_scores])\n\n        regression_loss = layer.regression_loss()\n\n        regression_loss = keras.backend.eval(regression_loss)\n\n        numpy.testing.assert_almost_equal(regression_loss, 1.575)\n'"
tests/layers/losses/test_rpn.py,0,"b'import keras.backend\nimport keras_rcnn.layers\nimport numpy\n\n\nclass TestRPN:\n    def test_call(self):\n        anchors = 9\n\n        target_deltas = keras.backend.ones((1, anchors, 4))\n        target_scores = keras.backend.variable([1, 1, 0, -1, 1, 0, -1, -1, 0])\n        target_scores = keras.backend.expand_dims(target_scores, 0)\n        target_scores = keras.backend.cast(target_scores, keras.backend.floatx())\n\n        output_deltas = keras.backend.ones((1, 1, 4 * anchors))\n        output_scores = keras.backend.variable([1, 1, 0, -1, 1, 0, -1, -1, 0])\n        output_scores = keras.backend.expand_dims(output_scores, 0)\n        output_scores = keras.backend.cast(output_scores, keras.backend.floatx())\n\n        layer = keras_rcnn.layers.RPN()\n\n        layer.get_config()\n\n        layer.call([target_deltas, target_scores, output_deltas, output_scores])\n\n        classification_loss = layer.classification_loss(target_scores, output_scores)\n\n        classification_loss = keras.backend.eval(classification_loss)\n\n        numpy.testing.assert_almost_equal(classification_loss, 0.0)\n\n        regression_loss = layer.regression_loss(\n            target_deltas, target_scores, output_deltas\n        )\n\n        regression_loss = keras.backend.eval(regression_loss)\n\n        numpy.testing.assert_almost_equal(regression_loss, 0.0)\n\n        loss = layer.losses.pop()\n\n        loss = keras.backend.eval(loss)\n\n        numpy.testing.assert_almost_equal(loss, 0.0)\n\n        target_scores = keras.backend.variable([1, 1, 0, -1, 1, 0, -1, -1, 0])\n        target_scores = keras.backend.expand_dims(target_scores, 0)\n\n        output_scores = keras.backend.variable([0, 0, 1, 1, 0, 1, 1, 0, 1])\n        output_scores = keras.backend.expand_dims(output_scores, 0)\n\n        layer.call([target_deltas, target_scores, output_deltas, output_scores])\n\n        classification_loss = layer.classification_loss(target_scores, output_scores)\n\n        classification_loss = keras.backend.eval(classification_loss)\n\n        numpy.testing.assert_almost_equal(\n            classification_loss, numpy.log(1.0 / keras.backend.epsilon()), 0\n        )\n\n        regression_loss = layer.regression_loss(\n            target_deltas, target_scores, output_deltas\n        )\n\n        regression_loss = keras.backend.eval(regression_loss)\n\n        numpy.testing.assert_almost_equal(regression_loss, 0.0)\n\n        loss = layer.losses.pop()\n\n        loss = keras.backend.eval(loss)\n\n        numpy.testing.assert_almost_equal(\n            loss, numpy.log(1.0 / keras.backend.epsilon()), 0\n        )\n\n        target_scores = keras.backend.variable([-1, -1, 1, 0, 1, 0, 0, -1])\n        target_scores = keras.backend.expand_dims(target_scores, 0)\n        output_scores = keras.backend.variable([0, 1, 0, 0, 1, 0, 1, 1])\n        output_scores = keras.backend.expand_dims(output_scores, 0)\n\n        target_deltas = keras.backend.variable(\n            [\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [-0.1, 0.2, 0.3, 0.4],\n                [-0.1, 0.2, 1, -3],\n                [0, -1, 1.3, -0.1],\n                [2, 0.3, -1.5, 0.6],\n                [1, 0.1, -1.1, 0.1],\n                [2, 0.2, -1.2, -0.2],\n                [0.3, 3, 1.3, -0.3],\n            ]\n        )\n        target_deltas = keras.backend.expand_dims(target_deltas, 0)\n        output_deltas = keras.backend.variable(\n            [\n                [\n                    -0.1,\n                    0.2,\n                    1,\n                    -3,\n                    0,\n                    0,\n                    0.1,\n                    1,\n                    0,\n                    0.1,\n                    -0.1,\n                    0.8,\n                    -0.1,\n                    2,\n                    0,\n                    0,\n                    1,\n                    1,\n                    -0.1,\n                    -0.5,\n                    0,\n                    1,\n                    -0.5,\n                    -1,\n                    1,\n                    1,\n                    -0.1,\n                    -0.5,\n                    0,\n                    1,\n                    -0.5,\n                    -1,\n                    1,\n                    -2,\n                    3,\n                    -4,\n                ]\n            ]\n        )\n        output_deltas = keras.backend.expand_dims(output_deltas, 0)\n\n        layer.call([target_deltas, target_scores, output_deltas, output_scores])\n\n        regression_loss = layer.regression_loss(\n            target_deltas, target_scores, output_deltas\n        )\n\n        regression_loss = keras.backend.eval(regression_loss)\n\n        weight = 10.0\n\n        numpy.testing.assert_almost_equal(regression_loss, weight * 1.575)\n'"
tests/layers/object_detection/__init__.py,0,b''
tests/layers/object_detection/test_anchor.py,0,"b'import keras.backend\nimport keras.layers\nimport keras.models\nimport numpy\nimport tensorflow\n\nimport keras_rcnn.backend\nimport keras_rcnn.layers\nimport keras_rcnn.layers.object_detection._anchor as anchor_target\n\n\nclass TestAnchor:\n    def test_call(self):\n        target_bounding_boxes = numpy.random.random((1, 10, 4))\n        target_bounding_boxes = keras.backend.variable(target_bounding_boxes)\n\n        target_metadata = keras.backend.variable([[224, 224, 1]])\n\n        output_scores = numpy.random.random((1, 14, 14, 9 * 2))\n        output_scores = keras.backend.variable(output_scores)\n\n        _, _, _ = keras_rcnn.layers.Anchor(padding=1)(\n            [target_bounding_boxes, target_metadata, output_scores]\n        )\n\n    def test_label(self, anchor_layer):\n        stride = 16\n        feat_h, feat_w = (7, 7)\n        img_info = keras.backend.variable([[112, 112, 3]])\n\n        gt_boxes = keras.backend.variable(100 * numpy.random.random((91, 4)))\n        gt_boxes = tensorflow.convert_to_tensor(gt_boxes, dtype=tensorflow.float32)\n\n        all_anchors = keras_rcnn.backend.shift((feat_h, feat_w), stride)\n\n        anchor_layer.padding = 1\n        anchor_layer.metadata = img_info[0]\n\n        inds_inside, all_inside_anchors = anchor_layer._inside_image(all_anchors)\n\n        all_inside_anchors = keras_rcnn.backend.clip(\n            all_inside_anchors, img_info[0][:2]\n        )\n\n        argmax_overlaps_inds, anchor_labels = anchor_layer._label(\n            gt_boxes, all_inside_anchors, inds_inside\n        )\n\n        result1 = keras.backend.eval(argmax_overlaps_inds)\n        result2 = keras.backend.eval(anchor_labels)\n\n        assert result1.shape == (224,), keras.backend.eval(inds_inside).shape\n\n        assert result2.shape == (224,)\n\n        assert numpy.max(result2) <= 1\n\n        assert numpy.min(result2) >= -1\n\n        argmax_overlaps_inds, anchor_labels = anchor_layer._label(\n            gt_boxes, all_inside_anchors, inds_inside\n        )\n\n        result1 = keras.backend.eval(argmax_overlaps_inds)\n        result2 = keras.backend.eval(anchor_labels)\n\n        assert result1.shape == (224,)\n\n        assert result2.shape == (224,)\n\n        assert numpy.max(result2) <= 1\n\n        assert numpy.min(result2) >= -1\n\n        gt_boxes = keras.backend.variable(224 * numpy.random.random((55, 4)))\n        gt_boxes = tensorflow.convert_to_tensor(gt_boxes, dtype=tensorflow.float32)\n\n        argmax_overlaps_inds, anchor_labels = anchor_layer._label(\n            gt_boxes, all_inside_anchors, inds_inside\n        )\n\n        result1 = keras.backend.eval(argmax_overlaps_inds)\n        result2 = keras.backend.eval(anchor_labels)\n\n        assert result1.shape == (224,)\n\n        assert result2.shape == (224,)\n\n        assert numpy.max(result2) <= 1\n\n        assert numpy.min(result2) >= -1\n\n    def test_subsample_positive_labels(self, anchor_layer):\n        x = keras.backend.ones((10,))\n\n        y = anchor_layer._subsample_positive_labels(x)\n\n        numpy.testing.assert_array_equal(keras.backend.eval(x), keras.backend.eval(y))\n\n        x = keras.backend.ones((1000,))\n\n        y = anchor_layer._subsample_positive_labels(x)\n\n        assert keras.backend.eval(keras.backend.sum(y) < keras.backend.sum(x))\n\n    def test_subsample_negative_labels(self, anchor_layer):\n        x = keras.backend.zeros((10,))\n\n        y = anchor_layer._subsample_negative_labels(x)\n\n        numpy.testing.assert_array_equal(keras.backend.eval(x), keras.backend.eval(y))\n\n        x = keras.backend.zeros((1000,))\n\n        y = anchor_layer._subsample_negative_labels(x)\n\n        assert keras.backend.eval(keras.backend.sum(y) < keras.backend.sum(x))\n\n    def test_balance(self, anchor_layer):\n        x = keras.backend.zeros((91,))\n\n        y = anchor_layer._balance(x)\n\n        numpy.testing.assert_array_equal(keras.backend.eval(x), keras.backend.eval(y))\n\n        x = keras.backend.ones((91,))\n\n        y = anchor_layer._balance(x)\n\n        numpy.testing.assert_array_equal(keras.backend.eval(x), keras.backend.eval(y))\n\n        x = keras.backend.ones((1000,))\n\n        y = anchor_layer._balance(x)\n\n        assert keras.backend.eval(keras.backend.sum(y) < keras.backend.sum(x))\n\n    def test_overlapping(self, anchor_layer):\n        stride = 16\n        features = (7, 7)\n        img_info = keras.backend.variable([[112, 112, 3]])\n        gt_boxes = numpy.zeros((91, 4))\n        gt_boxes = keras.backend.variable(gt_boxes)\n\n        all_anchors = keras_rcnn.backend.shift(features, stride)\n\n        anchor_layer.padding = 1\n        anchor_layer.metadata = img_info[0]\n\n        inds_inside, all_inside_anchors = anchor_layer._inside_image(all_anchors)\n\n        all_inside_anchors = keras_rcnn.backend.clip(\n            all_inside_anchors, img_info[0][:2]\n        )\n\n        a, max_overlaps, gt_argmax_overlaps_inds = anchor_layer._overlapping(\n            all_inside_anchors, gt_boxes, inds_inside\n        )\n\n        a = keras.backend.eval(a)\n        max_overlaps = keras.backend.eval(max_overlaps)\n        gt_argmax_overlaps_inds = keras.backend.eval(gt_argmax_overlaps_inds)\n\n        assert a.shape == (224,)\n\n        assert max_overlaps.shape == (224,)\n\n        assert gt_argmax_overlaps_inds.shape == (91,)\n\n    def test_unmap(self, anchor_layer):\n        stride = 16\n        features = (14, 14)\n        anchors = 15\n        total_anchors = features[0] * features[1] * anchors\n        img_info = keras.backend.variable([[224, 224, 3]])\n        gt_boxes = numpy.zeros((91, 4))\n        gt_boxes = keras.backend.variable(gt_boxes)\n\n        all_anchors = keras_rcnn.backend.shift(features, stride)\n\n        anchor_layer.padding = 1\n        anchor_layer.metadata = img_info[0]\n        anchor_layer.k = total_anchors\n\n        inds_inside, all_inside_anchors = anchor_layer._inside_image(all_anchors)\n\n        all_inside_anchors = keras_rcnn.backend.clip(\n            all_inside_anchors, img_info[0][:2]\n        )\n\n        argmax_overlaps_indices, labels = anchor_layer._label(\n            gt_boxes, all_inside_anchors, inds_inside\n        )\n\n        bbox_reg_targets = keras_rcnn.backend.bbox_transform(\n            all_inside_anchors, keras.backend.gather(gt_boxes, argmax_overlaps_indices)\n        )\n\n        labels = anchor_layer._unmap(labels, inds_inside, fill=-1)\n\n        bbox_reg_targets = anchor_layer._unmap(bbox_reg_targets, inds_inside, fill=0)\n\n        assert keras.backend.eval(labels).shape == (total_anchors,)\n        assert keras.backend.eval(bbox_reg_targets).shape == (total_anchors, 4)\n\n    def test_inside_image(self, anchor_layer):\n        stride = 16\n        features = (7, 7)\n\n        all_anchors = keras_rcnn.backend.shift(features, stride)\n\n        img_info = numpy.array([[112, 112, 1]])\n\n        anchor_layer.metadata = img_info[0]\n        anchor_layer.padding = 1\n\n        inds_inside, all_inside_anchors = anchor_layer._inside_image(all_anchors)\n\n        all_inside_anchors = keras_rcnn.backend.clip(\n            all_inside_anchors, img_info[0][:2]\n        )\n\n        inds_inside = keras.backend.eval(inds_inside)\n\n        assert inds_inside.shape == (224,), keras.backend.eval(all_inside_anchors)\n\n        all_inside_anchors = keras.backend.eval(all_inside_anchors)\n\n        assert all_inside_anchors.shape == (224, 4)\n\n    def test_inside_and_outside_weights_1(self, anchor_layer):\n        anchors = numpy.array([[30, 20, 50, 30], [10, 15, 20, 25], [5, 15, 20, 22]])\n\n        anchors = keras.backend.constant(anchors)\n\n        subsample = keras.backend.constant([1, 0, 1])\n\n        positive_weight = -1.0\n\n        proposed_inside_weights = [1.0, 0.5, 0.7, 1.0]\n\n        target_inside_weights = numpy.array(\n            [[1.0, 0.5, 0.7, 1.0], [0.0, 0.0, 0.0, 0.0], [1.0, 0.5, 0.7, 1.0]]\n        )\n\n        target_inside_weights = keras.backend.constant(target_inside_weights)\n\n        target_outside_weights = keras.backend.ones_like(\n            anchors, dtype=keras.backend.floatx()\n        )\n        target_outside_weights /= target_outside_weights\n\n        (\n            output_inside_weights,\n            output_outside_weights,\n        ) = anchor_layer._inside_and_outside_weights(\n            anchors, subsample, positive_weight, proposed_inside_weights\n        )\n\n        target_inside_weights = keras.backend.eval(target_inside_weights)\n        output_inside_weights = keras.backend.eval(output_inside_weights)\n\n        numpy.testing.assert_array_equal(target_inside_weights, output_inside_weights)\n\n        target_outside_weights = keras.backend.eval(target_outside_weights)\n        output_outside_weights = keras.backend.eval(output_outside_weights)\n\n        numpy.testing.assert_array_equal(target_outside_weights, output_outside_weights)\n\n    def test_inside_and_outside_weights_2(self, anchor_layer):\n        anchors = numpy.array([[30, 20, 50, 30], [10, 15, 20, 25], [5, 15, 20, 22]])\n\n        anchors = keras.backend.constant(anchors)\n\n        subsample = keras.backend.constant([1, 0, 1])\n\n        positive_weight = 0.6\n\n        proposed_inside_weights = [1.0, 0.5, 0.7, 1.0]\n\n        target_inside_weights = numpy.array(\n            [[1.0, 0.5, 0.7, 1.0], [0.0, 0.0, 0.0, 0.0], [1.0, 0.5, 0.7, 1.0]]\n        )\n\n        target_inside_weights = keras.backend.constant(target_inside_weights)\n\n        target_outside_weights = numpy.array(\n            [[0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4], [0.3, 0.3, 0.3, 0.3]]\n        )\n\n        target_outside_weights = keras.backend.constant(target_outside_weights)\n\n        (\n            output_inside_weights,\n            output_outside_weights,\n        ) = anchor_layer._inside_and_outside_weights(\n            anchors, subsample, positive_weight, proposed_inside_weights\n        )\n\n        target_inside_weights = keras.backend.eval(target_inside_weights)\n        output_inside_weights = keras.backend.eval(output_inside_weights)\n\n        numpy.testing.assert_array_equal(target_inside_weights, output_inside_weights)\n\n        target_outside_weights = keras.backend.eval(target_outside_weights)\n        output_outside_weights = keras.backend.eval(output_outside_weights)\n\n        numpy.testing.assert_array_equal(target_outside_weights, output_outside_weights)\n'"
tests/layers/object_detection/test_object_proposal.py,0,"b'import keras.backend\nimport numpy\n\nimport keras_rcnn.backend\nimport keras_rcnn.layers\n\n\nclass TestObjectProposal:\n    def test_call(self):\n        metadata = keras.backend.variable([[224, 224, 1.5]])\n\n        deltas = numpy.random.random((1, 14, 14, 9 * 4))\n        scores = numpy.random.random((1, 14, 14, 9 * 2))\n        anchors = numpy.zeros((1, 14 * 14 * 9, 4)).astype(""float32"")\n\n        deltas = keras.backend.variable(deltas)\n        scores = keras.backend.variable(scores)\n\n        object_proposal = keras_rcnn.layers.ObjectProposal()\n\n        object_proposal.call([anchors, metadata, deltas, scores])\n\n\ndef test_filter_boxes():\n    proposals = numpy.array([[0, 2, 3, 10], [-1, -5, 4, 8], [0, 0, 1, 1]])\n\n    minimum = 3\n\n    results = keras_rcnn.layers.object_detection._object_proposal.filter_boxes(\n        proposals, minimum\n    )\n\n    numpy.testing.assert_array_equal(keras.backend.eval(results), numpy.array([0, 1]))\n'"
tests/layers/object_detection/test_proposal_target.py,0,"b'import keras.backend\nimport keras.utils.test_utils\nimport numpy\n\nimport keras_rcnn.backend\nimport keras_rcnn.layers\n\n\ndef test_get_config():\n    layer = keras_rcnn.layers.ProposalTarget()\n\n    expected = {\n        ""background_threshold"": (0.1, 0.5),\n        ""foreground"": 0.5,\n        ""foreground_threshold"": (0.5, 1.0),\n        ""maximum_proposals"": 32,\n        ""name"": ""proposal_target_1"",\n        ""trainable"": True,\n    }\n\n    assert layer.get_config() == expected\n\n\ndef test_proposal_target():\n    keras.backend.set_learning_phase(True)\n\n    a = keras.layers.Input((None, 4))\n    b = keras.layers.Input((None, 2))\n    c = keras.layers.Input((None, 4))\n\n    layer = keras_rcnn.layers.ProposalTarget()\n\n    x, y, z = layer([a, b, c])\n\n    model = keras.models.Model([a, b, c], [x, y, z])\n\n    assert model.output_shape == [(1, None, 8), (1, None, 2), (1, None, 4)]\n\n    a = numpy.random.random((1, 10, 4))\n    b = numpy.random.random((1, 10, 2))\n    c = numpy.random.random((1, 300, 4))\n\n    x, y, z = model.predict([a, b, c])\n\n    assert x.shape == (1, 32, 8), x.shape\n    assert y.shape == (1, 32, 2), y.shape\n    assert z.shape == (1, 32, 4), z.shape\n\n\ndef test_sample():\n    proposal_target = keras_rcnn.layers.ProposalTarget(maximum_proposals=8)\n    all_rois = numpy.array(\n        [\n            [50.71208954, 70.44628143, 90.61702728, 99.66291046],\n            [55.45672607, 67.94913483, 86.19535828, 98.13130951],\n            [107.53626251, 1.19126511, 127.0, 40.5984726],\n            [50.50813293, 79.33403778, 87.0002594, 117.19319916],\n            [101.94552612, 0.0, 125.88674927, 37.46565247],\n            [0.0, 0.0, 127.0, 88.0],\n            [0.0, 24.0, 127.0, 120.0],\n            [98.8294754, 21.01197243, 127.0, 59.66217804],\n            [94.43806458, 3.28402901, 127.0, 44.11544037],\n            [0.0, 56.0, 127.0, 127.0],\n            [0.0, 0.0, 127.0, 56.0],\n            [28.0, 40.0, 127.0, 127.0],\n            [91.25584412, 0.0, 116.2077179, 32.36594391],\n            [0.0, 40.0, 100.0, 127.0],\n            [0.0, 8.0, 100.0, 104.0],\n            [16.0, 0.0, 127.0, 127.0],\n            [28.0, 8.0, 127.0, 104.0],\n            [0.0, 0.0, 100.0, 72.0],\n            [50.74036407, 94.12261963, 86.55892181, 127.0],\n            [0.0, 72.0, 100.0, 127.0],\n            [103.20545959, 86.37854004, 127.0, 127.0],\n            [40.0, 0.0, 127.0, 88.0],\n            [56.0, 0.0, 127.0, 72.0],\n            [28.0, 72.0, 127.0, 127.0],\n            [56.0, 56.0, 127.0, 127.0],\n            [76.0, 32.0, 127.0, 127.0],\n            [0.0, 56.0, 72.0, 127.0],\n            [0.0, 0.0, 72.0, 88.0],\n            [76.0, 0.0, 127.0, 96.0],\n            [0.0, 32.0, 52.0, 127.0],\n            [56.0, 0.0, 127.0, 120.0],\n            [0.0, 0.0, 52.0, 96.0],\n            [0.0, 24.0, 72.0, 127.0],\n            [97.16458893, 98.0249939, 127.0, 127.0],\n            [108.26307678, 18.04890442, 127.0, 51.71542358],\n            [94.32286072, 0.0, 127.0, 22.97975922],\n            [7.79359531, 0.0, 36.87153625, 17.60238075],\n            [0.0, 0.0, 84.0, 127.0],\n            [0.0, 106.34275055, 49.92878723, 127.0],\n            [0.0, 99.0591507, 35.27072525, 127.0],\n            [14.40047836, 102.94164276, 66.9264679, 127.0],\n            [19.59460831, 94.52481079, 58.59706497, 127.0],\n            [0.0, 0.0, 41.47511673, 30.94105721],\n            [0.0, 91.5510025, 23.51871109, 127.0],\n            [98.41709137, 32.52594757, 127.0, 70.71497345],\n            [83.86433411, 0.0, 126.68435669, 17.05216599],\n            [4.25742149, 103.77056885, 40.9630661, 127.0],\n            [92.50645447, 18.36931801, 127.0, 74.69480896],\n            [0.0, 0.0, 23.31191254, 45.55099106],\n            [0.0, 85.20574951, 18.08465958, 127.0],\n            [34.76586914, 91.0597229, 70.21446991, 127.0],\n            [31.57422256, 98.00201416, 82.33876038, 127.0],\n            [0.0, 1.70124626, 18.00316238, 58.02664948],\n            [47.77222824, 100.66429138, 97.60723877, 127.0],\n            [49.00650024, 0.0, 87.90568542, 18.85537338],\n            [2.66373634, 0.0, 31.10864067, 32.25665665],\n            [7.98152542, 98.91853333, 75.25076294, 127.0],\n            [96.10111237, 82.96366882, 127.0, 124.20684814],\n            [100.59230042, 51.73586655, 127.0, 90.42410278],\n            [62.81672668, 105.77951813, 100.79664612, 127.0],\n            [39.32888031, 78.95227814, 70.62407684, 127.0],\n            [83.66664124, 0.0, 123.00033569, 42.14031601],\n            [35.41271973, 96.42881775, 102.99723816, 127.0],\n            [86.72212219, 104.05187988, 121.45080566, 127.0],\n            [34.50315857, 0.0, 72.09360504, 21.45676422],\n            [49.67893219, 85.49975586, 94.36956024, 122.12127686],\n            [16.8948822, 0.0, 57.12744904, 23.87054443],\n            [0.0, 0.0, 49.20021057, 22.89399529],\n            [92.50393677, 31.71694946, 116.82061768, 75.54193115],\n            [101.27336884, 68.32675171, 127.0, 107.76806641],\n            [103.63972473, 64.72322845, 127.0, 121.69217682],\n            [0.0, 2.98220253, 31.56446838, 48.02368927],\n            [98.78588104, 41.22549438, 127.0, 98.6265564],\n            [43.14202118, 83.98725128, 81.71795654, 126.31203461],\n            [77.02322388, 50.71312714, 116.16117859, 85.06006622],\n            [0.0, 81.65694427, 32.60420227, 123.96698761],\n            [103.44710541, 14.21045494, 127.0, 40.28822327],\n            [69.66470337, 0.0, 110.63270569, 21.06321716],\n            [0.0, 3.10115814, 19.54813766, 39.10132217],\n            [0.0, 18.57296562, 24.90319443, 61.45012665],\n            [80.68254089, 100.47480774, 127.0, 127.0],\n            [22.4812355, 0.0, 51.37631607, 19.1945343],\n            [57.52688599, 90.20191956, 91.05926514, 127.0],\n            [95.99333191, 28.11805344, 127.0, 54.7819252],\n            [66.12437439, 109.61106873, 112.16593933, 127.0],\n            [0.0, 34.92645645, 23.40029144, 76.49604797],\n            [0.0, 24.81159973, 30.09602737, 64.4962616],\n            [71.15076447, 0.0, 98.23110199, 24.22634506],\n            [57.86538696, 69.31452942, 100.78405762, 105.31706238],\n            [56.96481705, 0.0, 84.40791321, 16.61228561],\n            [52.54270172, 71.56542969, 84.0104599, 106.01870728],\n            [59.92431641, 51.18797302, 93.5614624, 96.81411743],\n            [46.53039551, 60.00773621, 76.70639038, 105.10090637],\n            [0.0, 51.24388123, 20.01187706, 91.44229126],\n            [0.0, 40.91738892, 29.76662064, 79.57460785],\n            [0.0, 66.71936798, 23.33432007, 107.1989212],\n            [52.53593826, 56.81362534, 87.6943512, 80.978302],\n            [23.93180656, 0.0, 65.24099731, 21.62162399],\n            [52.88409424, 62.21540833, 86.62045288, 94.69174194],\n            [62.93517303, 0.0, 97.4132309, 21.67671776],\n            [80.08380127, 71.31486511, 112.64286804, 106.20582581],\n            [0.0, 75.59233093, 29.57452393, 115.32017517],\n            [0.0, 94.73368835, 29.12003326, 122.7570343],\n            [56.74416351, 62.87185669, 83.51163483, 102.64590454],\n            [56.4704361, 67.04311371, 88.26943207, 107.08548737],\n            [61.38975525, 99.27015686, 116.5189209, 127.0],\n            [74.17579651, 0.0, 111.2371521, 29.42921066],\n            [89.39240265, 47.07178497, 126.46971893, 87.31560516],\n            [69.44123077, 47.38218307, 108.40250397, 79.6802063],\n            [0.0, 57.49836731, 29.66657829, 95.89842224],\n            [78.95790863, 37.56030273, 115.78124237, 78.61038971],\n            [1.05501747, 49.41056061, 23.29784775, 78.91101837],\n            [85.10821533, 96.18637848, 113.51568604, 127.0],\n            [0.0, 4.97987747, 42.77118683, 39.50862122],\n            [20.71508408, 86.12203979, 66.83483887, 127.0],\n            [92.37216949, 5.15529823, 120.51592255, 36.1371994],\n            [18.54267502, 7.85336876, 47.64195633, 40.28590393],\n            [70.94502258, 10.51726341, 96.90571594, 49.78765106],\n            [78.40616608, 34.65525818, 110.38196564, 65.63249969],\n            [62.15895844, 59.15465546, 105.94535065, 89.95876312],\n            [1.27451801, 69.83500671, 25.79981232, 97.2482605],\n            [62.73934174, 83.32158661, 101.73313141, 120.8647995],\n            [45.75923157, 59.99327469, 71.63579559, 98.12197876],\n            [56.19100189, 14.82108879, 84.26691437, 44.36116791],\n            [65.84653473, 42.47505951, 97.59632111, 82.20253754],\n            [69.37020874, 94.50706482, 108.35041809, 127.0],\n            [37.10490417, 9.61324596, 64.72504425, 38.63171387],\n            [85.3844986, 66.10962677, 110.31502533, 107.09311676],\n            [37.36283112, 60.59078217, 61.06542206, 91.03170013],\n            [67.89260864, 71.75512695, 106.5725708, 102.57484436],\n            [77.99923706, 16.48104858, 107.95568848, 48.56900024],\n            [89.20878601, 71.41056824, 125.7568512, 107.29753113],\n            [5.91212463, 32.58755112, 41.92187881, 62.24328232],\n            [52.27661133, 39.26787949, 81.46669006, 69.80540466],\n            [47.65753937, 40.49248505, 76.20034027, 76.36449432],\n            [57.34262848, 7.1027317, 82.6523056, 36.4445343],\n            [18.88934326, 92.23389435, 47.76045227, 124.64731598],\n            [9.30751991, 49.58087158, 42.02339172, 77.00914764],\n            [45.11285782, 61.83740234, 75.83686829, 87.559021],\n            [15.24866962, 45.1537323, 40.4258461, 72.44976044],\n            [26.69694901, 13.21245193, 60.47519302, 42.36695862],\n            [3.4413681, 88.63192749, 40.75950241, 116.03289795],\n            [33.86211777, 72.85886383, 64.36898804, 111.14054108],\n            [27.03687096, 89.1621933, 60.37636566, 116.18755341],\n            [32.48144913, 47.85022736, 59.34322739, 74.05560303],\n            [15.24499512, 57.00760651, 38.60559464, 84.32082367],\n            [9.1018734, 68.70444489, 41.70313263, 95.08370209],\n            [29.1010685, 40.56064987, 54.72600555, 74.27661133],\n            [63.39616394, 20.40607834, 91.86817932, 48.19306564],\n            [59.84752655, 49.76169586, 92.02407074, 74.77848816],\n            [22.08538437, 24.85385132, 47.61610031, 57.60162354],\n            [46.91257477, 17.56220245, 76.49073029, 44.3457756],\n            [31.83808899, 58.19957733, 59.74316406, 81.0170517],\n            [32.10147095, 74.45246124, 59.8211441, 97.25855255],\n            [56.94638062, 24.92946815, 84.93023682, 58.81396484],\n            [17.57852745, 71.32565308, 40.61666107, 100.14833069],\n            [44.46640396, 25.70672989, 69.00811768, 59.31702042],\n            [31.93849182, 42.83921432, 60.13134766, 66.11750793],\n            [47.41349792, 45.91717911, 75.39443207, 69.51273346],\n        ]\n    )\n    all_rois = keras.backend.cast(all_rois, keras.backend.floatx())\n    gt_boxes = numpy.array([[105.0, 7.0, 127.0, 44.0], [51.0, 78.0, 92.0, 98.0]])\n    gt_boxes = keras.backend.cast(gt_boxes, keras.backend.floatx())\n    gt_labels = numpy.array([[0, 1, 0], [0, 0, 1]])\n    gt_labels = keras.backend.cast(gt_labels, keras.backend.floatx())\n\n    rois, labels, bbox_targets = proposal_target.sample(all_rois, gt_boxes, gt_labels)\n\n    out = keras.backend.concatenate([rois, labels, bbox_targets], axis=1)\n    out = keras.backend.eval(out)\n    rois = out[:, :4]\n    labels = out[:, 4 : (4 + 3)]\n    bbox_targets = out[:, (4 + 3) :]\n\n    gt_boxes = keras.backend.eval(gt_boxes)\n\n    pred = keras_rcnn.backend.bbox_transform_inv(rois, bbox_targets)\n    pred = keras.backend.eval(pred)\n\n    classes = numpy.argmax(labels, -1)\n\n    label = classes[0]\n    assert numpy.array_equal(\n        pred[0][4 * label : 4 * label + 4], gt_boxes[0]\n    ) or numpy.array_equal(pred[0][4 * label : 4 * label + 4], gt_boxes[1])\n\n    label = classes[1]\n    assert numpy.array_equal(\n        pred[1][4 * label : 4 * label + 4], gt_boxes[0]\n    ) or numpy.array_equal(pred[1][4 * label : 4 * label + 4], gt_boxes[1])\n\n    assert labels.shape == (8, 3)\n    assert labels[4:, 0].sum() == 4\n    assert labels[:4, 1:].sum() == 4\n    assert bbox_targets[4:].sum() == 0\n    assert bbox_targets[:4, :4].sum() == 0\n\n\ndef test_set_label_background():\n    p = keras_rcnn.layers.ProposalTarget()\n\n    gt_labels = numpy.array(\n        [\n            1,\n            2,\n            3,\n            1,\n            1,\n            2,\n            3,\n            3,\n            3,\n            1,\n            2,\n            2,\n            1,\n            2,\n            2,\n            2,\n            2,\n            1,\n            3,\n            3,\n            1,\n            1,\n            3,\n            1,\n            2,\n            2,\n            3,\n            2,\n            3,\n            3,\n            1,\n            1,\n        ]\n    )\n    gt_labels = keras.backend.one_hot(gt_labels, 4)\n    overlaps = numpy.array(\n        [\n            [0.0, 0.0, 0.0, 0.0],\n            [0.4, 0.0, 0.1, 0.2],\n            [0.0, 0.5, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.2, 0.1, 0.6, 0.7],\n            [0.0, 0.0, 0.9, 0.0],\n            [0.01, 0.1, 0.2, 0.3],\n            [0.0, 0.0, 0.5, 1.0],\n            [0.3, 0.0, 0.0, 1.0],\n        ]\n    )\n    max_overlaps = keras.backend.max(overlaps, axis=1)\n    gt_assignment = keras.backend.argmax(overlaps, axis=1)\n    keep_inds = p.find_foreground_and_background_proposal_indices(max_overlaps)\n    all_labels = keras.backend.gather(gt_labels, gt_assignment)\n    labels = keras.backend.gather(all_labels, keep_inds)\n\n    result = p.set_label_background(labels)\n    result = keras.backend.eval(result)\n    assert result[-1].sum() == 0\n    assert result[0].sum() == 1\n    assert result[1].sum() == 1\n\n\ndef test_get_bbox_regression_labels():\n    p = keras_rcnn.layers.ProposalTarget()\n\n    bbox_target_data = numpy.array(\n        [\n            [0, 0, 0, 0],\n            [0.1, 0.2, -0.1, -0.3],\n            [1, 2, 3, 4],\n            [0.1, 0.2, -0.1, -0.3],\n            [1, 2, 3, 4],\n        ]\n    )\n    num_classes = 3\n    labels = numpy.array([[0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0]])\n    labels = numpy.argmax(labels, axis=1)\n    bbox_targets = p.get_bbox_regression_labels(bbox_target_data, labels, num_classes)\n    bbox_targets = keras.backend.eval(bbox_targets)\n\n    assert bbox_targets.shape == (5, 4 * num_classes)\n    expected = numpy.array(\n        [\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.2, -0.1, -0.3],\n            [0, 0, 0, 0, 1, 2, 3, 4, 0, 0, 0, 0],\n        ]\n    )\n    numpy.testing.assert_array_almost_equal(bbox_targets, expected)\n\n\ndef test_get_bbox_targets():\n    p = keras_rcnn.layers.ProposalTarget()\n\n    rois = numpy.array(\n        [\n            [0, 14.4, 30.1, 99.0],\n            [40.0, 3.2, 55.5, 33.7],\n            [30.7, 1.2, 66.5, 23.8],\n            [50.7, 50.2, 86.5, 63.8],\n        ]\n    )\n    gt_boxes = numpy.array(\n        [\n            [3.0, 20.0, 22.0, 100.0],\n            [40.0, 3.0, 55.0, 33.0],\n            [30.0, 1.0, 66.0, 24.0],\n            [15.0, 18.0, 59.0, 59.0],\n        ]\n    )\n    num_classes = 4\n    labels = numpy.array([1, 1, 2, 1])\n\n    bbox_targets = p.get_bbox_targets(rois, gt_boxes, labels, num_classes)\n    bbox_targets = keras.backend.eval(bbox_targets)\n    expected = numpy.array(\n        [\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                -0.08471760797342194,\n                0.039007092198581526,\n                -0.46008619258838973,\n                -0.05590763193829595,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                -0.016129032258064516,\n                -0.014754098360655828,\n                -0.03278982282299084,\n                -0.016529301951210697,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                -0.01675977653631289,\n                0.0,\n                0.0055710450494554295,\n                0.017544309650909525,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n            [\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                -0.88268156424581,\n                -1.3602941176470593,\n                0.20624174051160657,\n                1.103502273962302,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n                0.0,\n            ],\n        ]\n    )\n    numpy.testing.assert_array_almost_equal(bbox_targets, expected)\n\n\ndef test_find_foreground_and_background_proposal_indices():\n    # 1\n    overlaps = numpy.array(\n        [\n            [0.0, 0.0, 0.0, 0.0],\n            [0.4, 0.0, 0.1, 0.2],\n            [0.0, 0.5, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.2, 0.1, 0.6, 0.7],\n            [0.0, 0.0, 0.9, 0.0],\n            [0.01, 0.1, 0.2, 0.3],\n            [0.0, 0.0, 0.5, 1.0],\n            [0.3, 0.0, 0.0, 1.0],\n        ]\n    )\n    max_overlaps = keras.backend.max(overlaps, axis=1)\n    fg_fraction = 0.5\n    batchsize = 3\n    p = keras_rcnn.layers.ProposalTarget(maximum_proposals=batchsize)\n    keep_inds = p.find_foreground_and_background_proposal_indices(max_overlaps)\n    keep_inds = keras.backend.eval(keep_inds)\n    expected_fg = numpy.array([2, 3, 4, 5, 7, 8])\n    expected_bg = numpy.array([1, 6])\n\n    assert keep_inds[0] in expected_fg, keep_inds\n    assert keep_inds[1] in expected_bg, keep_inds\n    assert keep_inds[2] in expected_bg, keep_inds\n    assert len(keep_inds) == 3\n\n    # 2\n    batchsize = 8\n    p = keras_rcnn.layers.ProposalTarget(maximum_proposals=batchsize)\n    keep_inds = p.find_foreground_and_background_proposal_indices(max_overlaps)\n    keep_inds = keras.backend.eval(keep_inds)\n    expected_fg = numpy.array([2, 3, 4, 5, 7, 8])\n    expected_bg = numpy.array([1, 6])\n\n    fg_rois = numpy.minimum(int(numpy.round(fg_fraction * batchsize)), len(expected_fg))\n    bg_rois = numpy.minimum(batchsize - fg_rois, len(expected_bg))\n    assert keep_inds[0] in expected_fg\n    assert keep_inds[1] in expected_fg\n    assert keep_inds[2] in expected_fg\n    assert keep_inds[3] in expected_fg\n    assert keep_inds[4] in expected_bg\n    assert keep_inds[5] in expected_bg\n    assert len(keep_inds) == 6\n\n    # 3\n    batchsize = 16\n    p = keras_rcnn.layers.ProposalTarget(maximum_proposals=batchsize)\n    keep_inds = p.find_foreground_and_background_proposal_indices(max_overlaps)\n    keep_inds = keras.backend.eval(keep_inds)\n    expected_fg = numpy.array([2, 3, 4, 5, 7, 8])\n    expected_bg = numpy.array([1, 6])\n\n    fg_rois = numpy.minimum(int(numpy.round(fg_fraction * batchsize)), len(expected_fg))\n    bg_rois = numpy.minimum(batchsize - fg_rois, len(expected_bg))\n    assert keep_inds[0] in expected_fg\n    assert keep_inds[1] in expected_fg\n    assert keep_inds[2] in expected_fg\n    assert keep_inds[3] in expected_fg\n    assert keep_inds[4] in expected_fg\n    assert keep_inds[5] in expected_fg\n    assert keep_inds[6] in expected_bg\n    assert keep_inds[7] in expected_bg\n    assert len(keep_inds) == 8\n\n    # 4\n    overlaps = numpy.array(\n        [\n            [0.2, 0.0, 0.0, 0.0],\n            [0.4, 0.0, 0.1, 0.2],\n            [0.0, 0.5, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.2, 0.1, 0.6, 0.7],\n            [0.0, 0.0, 0.9, 0.0],\n            [0.01, 0.1, 0.2, 0.3],\n            [0.0, 0.0, 0.5, 1.0],\n            [0.3, 0.0, 0.0, 1.0],\n        ]\n    )\n    max_overlaps = keras.backend.max(overlaps, axis=1)\n    batchsize = 6\n    p = keras_rcnn.layers.ProposalTarget(maximum_proposals=batchsize)\n    keep_inds = p.find_foreground_and_background_proposal_indices(max_overlaps)\n    keep_inds = keras.backend.eval(keep_inds)\n    expected_fg = numpy.array([2, 3, 4, 5, 7, 8])\n    expected_bg = numpy.array([0, 1, 6])\n\n    fg_rois = numpy.minimum(int(numpy.round(fg_fraction * batchsize)), len(expected_fg))\n    bg_rois = numpy.minimum(batchsize - fg_rois, len(expected_bg))\n    assert keep_inds[0] in expected_fg\n    assert keep_inds[1] in expected_fg\n    assert keep_inds[2] in expected_fg\n    assert keep_inds[3] in expected_bg\n    assert keep_inds[4] in expected_bg\n    assert keep_inds[5] in expected_bg\n    assert len(keep_inds) == 6\n\n\ndef test_sample_indices():\n    p = keras_rcnn.layers.ProposalTarget()\n    indices = numpy.array([0, 3, 4, 10, 22])\n    sampled = p.sample_indices(indices, 3)\n    sampled = keras.backend.eval(sampled)\n    assert sampled.shape == (3,)\n    for i in sampled:\n        assert i in indices\n\n    sampled = p.sample_indices(indices, 4)\n    sampled = keras.backend.eval(sampled)\n    assert sampled.shape == (4,)\n    for i in sampled:\n        assert i in indices\n'"
