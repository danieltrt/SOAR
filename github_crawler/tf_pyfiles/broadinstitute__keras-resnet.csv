file_path,api_count,code
setup.py,0,"b'import setuptools\n\nsetuptools.setup(\n    author=""Allen Goodman"",\n    author_email=""allen.goodman@icloud.com"",\n    extras_require={\n        ""benchmark"": [\n            ""click"",\n            ""sklearn""\n        ],\n        ""test"": [\n            ""pytest""\n        ]\n    },\n    install_requires=[\n        ""keras>=2.2.4""\n    ],\n    license=""MIT"",\n    name=""keras-resnet"",\n    package_data={\n        ""keras-resnet"": [\n            ""data/checkpoints/*/*.hdf5"",\n            ""data/logs/*/*.csv""\n        ]\n    },\n    packages=setuptools.find_packages(\n        exclude=[\n            ""tests""\n        ]\n    ),\n    url=""https://github.com/broadinstitute/keras-resnet"",\n    version=""0.2.0""\n)\n'"
keras_resnet/__init__.py,0,"b""from . import layers\n\ncustom_objects = {\n    'BatchNormalization': layers.BatchNormalization,\n}\n"""
keras_resnet/metrics.py,0,"b'import keras.metrics\n\n\ndef top_1_categorical_error(y_true, y_pred):\n    return 1.0 - keras.metrics.top_k_categorical_accuracy(y_true, y_pred, 1)\n\n\ndef top_5_categorical_error(y_true, y_pred):\n    return 1.0 - keras.metrics.top_k_categorical_accuracy(y_true, y_pred, 5)\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'import keras.layers\nimport pytest\n\n\n@pytest.fixture(scope=""module"")\ndef x():\n    shape = (224, 224, 3)\n\n    return keras.layers.Input(shape)\n'"
tests/test_block.py,0,b''
tests/test_models.py,0,"b'import keras_resnet.models\n\n\nclass TestResNet18:\n    def test_constructor(self, x):\n        model = keras_resnet.models.ResNet2D18(x)\n\n        assert len(model.layers) == 87\n\n\nclass TestResNet34:\n    def test_constructor(self, x):\n        model = keras_resnet.models.ResNet2D34(x)\n\n        assert len(model.layers) == 159\n\n\nclass TestResNet50:\n    def test_constructor(self, x):\n        model = keras_resnet.models.ResNet2D50(x)\n\n        assert len(model.layers) == 191\n\n\nclass TestResNet101:\n    def test_constructor(self, x):\n        model = keras_resnet.models.ResNet2D101(x)\n\n        assert len(model.layers) == 378\n\n\nclass TestResNet152:\n    def test_constructor(self, x):\n        model = keras_resnet.models.ResNet2D152(x)\n\n        assert len(model.layers) == 565\n\n\nclass TestResNet200:\n    def test_constructor(self, x):\n        model = keras_resnet.models.ResNet2D200(x)\n\n        assert len(model.layers) == 741\n'"
tools/export-caffe-weights.py,0,"b'#!/usr/bin/env python\n\nimport caffe\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Export caffe weights to h5 format."")\n    parser.add_argument(""prototxt"", help=""Path to prototxt file."")\n    parser.add_argument(""caffemodel"", help=""Path to weights file."")\n    parser.add_argument(""output"", help=""Path to output weights."")\n\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n\n    net = caffe.Net(args.prototxt, caffe.TEST, weights=args.caffemodel)\n    net.save_hdf5(args.output)\n\n    print(""done."")\n'"
tools/import-caffe-weights.py,0,"b'#!/usr/bin/env python\n\nimport keras_resnet.models\nimport keras\n\nimport h5py\nimport argparse\nimport numpy as np\n\n\ndef convert_conv_weights(weights):\n    return np.array(weights).transpose((2, 3, 1, 0))\n\n\ndef convert_dense_weights(weights, biases):\n    return [np.array(weights).T, np.array(biases)]\n\n\ndef create_model(resnet):\n    valid = [""resnet50"", ""resnet101"", ""resnet152""]\n    if resnet not in valid:\n        raise ValueError(""Invalid resnet argument (valid: {}) : \'{}\'"".format(valid, resnet))\n\n    image = keras.layers.Input((None, None, 3))\n    if resnet == ""resnet50"":\n        return keras_resnet.models.ResNet50(image)\n    elif resnet == ""resnet101"":\n        return keras_resnet.models.ResNet101(image)\n    elif resnet == ""resnet152"":\n        return keras_resnet.models.ResNet152(image)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Import caffe weights from h5 format."")\n    parser.add_argument(""weights"", help=""Path to weights (.h5) file."")\n    parser.add_argument(""output"", help=""Path to output Keras model to."")\n    parser.add_argument(""resnet"", help=""ResNet type (one of \'resnet50\', \'resnet101\', \'resnet152\')."")\n\n    return parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n\n    # first create the model\n    model = create_model(args.resnet)\n\n    # load the caffe weights\n    weights = h5py.File(args.weights).get(""data"")\n\n    # port each layer\n    for index, l in enumerate(model.layers):\n        if isinstance(l, keras.layers.Conv2D):\n            l.set_weights([convert_conv_weights(weights.get(l.name).get(""0""))])\n        elif isinstance(l, keras.layers.Dense):\n            l.set_weights(convert_dense_weights(weights.get(l.name).get(""0""), weights.get(l.name).get(""1"")))\n        elif isinstance(l, keras.layers.BatchNormalization):\n            scale_name = l.name.replace(""bn"", ""scale"")\n            bn_weights = weights.get(l.name)\n            scale_weights = weights.get(scale_name)\n\n            l.set_weights([\n                np.array(scale_weights.get(""0"")),  # gamma\n                np.array(scale_weights.get(""1"")),  # beta\n                np.array(bn_weights.get(""0"")),     # mean\n                np.array(bn_weights.get(""1"")),     # variance\n            ])\n\n        print(""imported layer: {}/{}"".format(index, len(model.layers)), end=""\\r"")\n\n    print(""saving..."")\n    model.save(args.output)\n    print(""done."")\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# keras-resnet documentation build configuration file, created by\n# sphinx-quickstart on Mon Apr 24 14:22:30 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\nimport pkg_resources\nimport sphinx_rtd_theme\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n              \'sphinx.ext.doctest\',\n              \'sphinx.ext.coverage\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.viewcode\',\n              \'sphinx.ext.githubpages\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'keras-resnet\'\ncopyright = \'2017, Allen Goodman\'\nauthor = \'Allen Goodman\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = pkg_resources.get_distribution(""keras-resnet"").version\n# The full version, including alpha/beta/rc tags.\nrelease = pkg_resources.get_distribution(""keras-resnet"").version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n\nhtml_theme_options = {\n    \'navigation_depth\': 4\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'keras-resnetdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'keras-resnet.tex\', \'keras-resnet Documentation\',\n     \'Allen Goodman\', \'manual\'),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'keras-resnet\', \'keras-resnet Documentation\',\n     [author], 1)\n]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'keras-resnet\', \'keras-resnet Documentation\',\n     author, \'keras-resnet\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n'"
keras_resnet/backend/__init__.py,0,"b'import os\n\nfrom .common import *\n\n_BACKEND = ""tensorflow""\n\nif ""KERAS_BACKEND"" in os.environ:\n    _backend = os.environ[""KERAS_BACKEND""]\n\n    backends = {\n        ""cntk"",\n        ""tensorflow"",\n    }\n\n    assert _backend in backends\n\n    _BACKEND = _backend\n\nif _BACKEND == ""cntk"":\n    from .cntk_backend import *\nelif _BACKEND == ""tensorflow"":\n    from .tensorflow_backend import *\nelse:\n    raise ValueError(""Unknown backend: "" + str(_BACKEND))\n'"
keras_resnet/backend/cntk_backend.py,0,"b'def resize(image, output_shape):\n    raise NotImplementedError\n'"
keras_resnet/backend/common.py,0,b''
keras_resnet/backend/tensorflow_backend.py,0,"b'import tensorflow\n\n\ndef resize(image, output_shape):\n    """"""\n    Resize an image or images to match a certain size.\n\n    :param image: Input image or images with the shape:\n\n        (rows, columns, channels)\n\n    or:\n\n        (batch, rows, columns, channels).\n\n    :param output_shape: Shape of the output image:\n\n        (rows, columns).\n\n    :return: If an image is provided a resized image with the shape:\n\n        (resized rows, resized columns, channels)\n\n    is returned.\n\n    If more than one image is provided then a batch of resized images with\n    the shape:\n\n        (batch size, resized rows, resized columns, channels)\n\n    are returned.\n    """"""\n    return tensorflow.image.resize_images(image, output_shape)\n'"
keras_resnet/backend/theano_backend.py,0,"b'def resize(image, output_shape):\n    raise NotImplementedError\n'"
keras_resnet/benchmarks/__init__.py,0,"b'import os.path\n\nimport click\nimport keras\nimport keras.preprocessing.image\nimport numpy\nimport pkg_resources\nimport sklearn.model_selection\nimport tensorflow\n\nimport keras_resnet.metrics\nimport keras_resnet.models\n\n_benchmarks = {\n    ""CIFAR-10"": keras.datasets.cifar10,\n    ""CIFAR-100"": keras.datasets.cifar100,\n    ""MNIST"": keras.datasets.mnist\n}\n\n\n_names = {\n    ""ResNet-18"": keras_resnet.models.ResNet2D18,\n    ""ResNet-34"": keras_resnet.models.ResNet2D34,\n    ""ResNet-50"": keras_resnet.models.ResNet2D50,\n    ""ResNet-101"": keras_resnet.models.ResNet2D101,\n    ""ResNet-152"": keras_resnet.models.ResNet2D152,\n    ""ResNet-200"": keras_resnet.models.ResNet2D200\n}\n\n\n@click.command()\n@click.option(\n    ""--benchmark"",\n    default=""CIFAR-10"",\n    type=click.Choice(\n        [\n            ""CIFAR-10"",\n            ""CIFAR-100"",\n            ""ImageNet"",\n            ""MNIST""\n        ]\n    )\n)\n@click.option(""--device"", default=0)\n@click.option(\n    ""--name"",\n    default=""ResNet-50"",\n    type=click.Choice(\n        [\n            ""ResNet-18"",\n            ""ResNet-34"",\n            ""ResNet-50"",\n            ""ResNet-101"",\n            ""ResNet-152"",\n            ""ResNet-200""\n        ]\n    )\n)\ndef __main__(benchmark, device, name):\n    configuration = tensorflow.ConfigProto()\n\n    configuration.gpu_options.allow_growth = True\n\n    configuration.gpu_options.visible_device_list = str(device)\n\n    session = tensorflow.Session(config=configuration)\n\n    keras.backend.set_session(session)\n\n    (training_x, training_y), _ = _benchmarks[benchmark].load_data()\n\n    training_x = training_x.astype(numpy.float16)\n\n    if benchmark is ""MNIST"":\n        training_x = numpy.expand_dims(training_x, -1)\n\n    training_y = keras.utils.np_utils.to_categorical(training_y)\n\n    training_x, validation_x, training_y, validation_y = sklearn.model_selection.train_test_split(\n        training_x,\n        training_y\n    )\n\n    generator = keras.preprocessing.image.ImageDataGenerator(\n        horizontal_flip=True\n    )\n\n    generator.fit(training_x)\n\n    generator = generator.flow(\n        x=training_x,\n        y=training_y,\n        batch_size=256\n    )\n\n    validation_data = keras.preprocessing.image.ImageDataGenerator()\n\n    validation_data.fit(validation_x)\n\n    validation_data = validation_data.flow(\n        x=validation_x,\n        y=validation_y,\n        batch_size=256\n    )\n\n    shape, classes = training_x.shape[1:], training_y.shape[-1]\n\n    x = keras.layers.Input(shape)\n\n    model = _names[name](inputs=x, classes=classes)\n\n    metrics = [\n        keras_resnet.metrics.top_1_categorical_error,\n        keras_resnet.metrics.top_5_categorical_error\n    ]\n\n    model.compile(""adam"", ""categorical_crossentropy"", metrics)\n\n    pathname = os.path.join(""data"", ""checkpoints"", benchmark, ""{}.hdf5"".format(name))\n\n    pathname = pkg_resources.resource_filename(""keras_resnet"", pathname)\n\n    model_checkpoint = keras.callbacks.ModelCheckpoint(pathname)\n\n    pathname = os.path.join(""data"", ""logs"", benchmark, ""{}.csv"".format(name))\n\n    pathname = pkg_resources.resource_filename(""keras_resnet"", pathname)\n\n    csv_logger = keras.callbacks.CSVLogger(pathname)\n\n    callbacks = [\n        csv_logger,\n        model_checkpoint\n    ]\n\n    model.fit_generator(\n        callbacks=callbacks,\n        epochs=100,\n        generator=generator,\n        validation_data=validation_data\n    )\n\n\nif __name__ == ""__main__"":\n    __main__()\n'"
keras_resnet/blocks/_1d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.blocks._1d\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements a number of popular one-dimensional residual blocks.\n""""""\n\nimport keras.layers\nimport keras.regularizers\n\nimport keras_resnet.layers\n\nparameters = {\n    ""kernel_initializer"": ""he_normal""\n}\n\n\ndef basic_1d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n    A one-dimensional basic block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.basic_1d(64)\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = -1\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.ZeroPadding1D(\n            padding=1, \n            name=""padding{}{}_branch2a"".format(stage_char, block_char)\n        )(x)\n        \n        y = keras.layers.Conv1D(\n            filters,\n            kernel_size,\n            strides=stride,\n            use_bias=False,\n            name=""res{}{}_branch2a"".format(stage_char, block_char),\n            **parameters\n        )(y)\n        \n        y = keras_resnet.layers.BatchNormalization(\n            axis=axis,\n            epsilon=1e-5,\n            freeze=freeze_bn,\n            name=""bn{}{}_branch2a"".format(stage_char, block_char)\n        )(y)\n        \n        y = keras.layers.Activation(\n            ""relu"",\n            name=""res{}{}_branch2a_relu"".format(stage_char, block_char)\n        )(y)\n\n        y = keras.layers.ZeroPadding1D(\n            padding=1,\n            name=""padding{}{}_branch2b"".format(stage_char, block_char)\n        )(y)\n        \n        y = keras.layers.Conv1D(\n            filters,\n            kernel_size,\n            use_bias=False,\n            name=""res{}{}_branch2b"".format(stage_char, block_char),\n            **parameters\n        )(y)\n        \n        y = keras_resnet.layers.BatchNormalization(\n            axis=axis,\n            epsilon=1e-5,\n            freeze=freeze_bn,\n            name=""bn{}{}_branch2b"".format(stage_char, block_char)\n        )(y)\n\n        if block == 0:\n            shortcut = keras.layers.Conv1D(\n                filters,\n                1,\n                strides=stride,\n                use_bias=False,\n                name=""res{}{}_branch1"".format(stage_char, block_char),\n                **parameters\n            )(x)\n\n            shortcut = keras_resnet.layers.BatchNormalization(\n                axis=axis,\n                epsilon=1e-5,\n                freeze=freeze_bn,\n                name=""bn{}{}_branch1"".format(stage_char, block_char)\n            )(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(\n            name=""res{}{}"".format(stage_char, block_char)\n        )([y, shortcut])\n        \n        y = keras.layers.Activation(\n            ""relu"",\n            name=""res{}{}_relu"".format(stage_char, block_char)\n        )(y)\n\n        return y\n\n    return f\n\n\ndef bottleneck_1d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n    A one-dimensional bottleneck block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.bottleneck_1d(64)\n    """"""\n    if stride is None:\n        stride = 1 if block != 0 or stage == 0 else 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = -1\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.Conv1D(\n            filters,\n            1,\n            strides=stride,\n            use_bias=False,\n            name=""res{}{}_branch2a"".format(stage_char, block_char),\n            **parameters\n        )(x)\n\n        y = keras_resnet.layers.BatchNormalization(\n            axis=axis,\n            epsilon=1e-5,\n            freeze=freeze_bn,\n            name=""bn{}{}_branch2a"".format(stage_char, block_char)\n        )(y)\n\n        y = keras.layers.Activation(\n            ""relu"",\n            name=""res{}{}_branch2a_relu"".format(stage_char, block_char)\n        )(y)\n\n        y = keras.layers.ZeroPadding1D(\n            padding=1,\n            name=""padding{}{}_branch2b"".format(stage_char, block_char)\n        )(y)\n\n        y = keras.layers.Conv1D(\n            filters,\n            kernel_size,\n            use_bias=False,\n            name=""res{}{}_branch2b"".format(stage_char, block_char),\n            **parameters\n        )(y)\n\n        y = keras_resnet.layers.BatchNormalization(\n            axis=axis,\n            epsilon=1e-5,\n            freeze=freeze_bn,\n            name=""bn{}{}_branch2b"".format(stage_char, block_char)\n        )(y)\n\n        y = keras.layers.Activation(\n            ""relu"",\n            name=""res{}{}_branch2b_relu"".format(stage_char, block_char)\n        )(y)\n\n        y = keras.layers.Conv1D(\n            filters * 4,\n            1,\n            use_bias=False,\n            name=""res{}{}_branch2c"".format(stage_char, block_char),\n            **parameters\n        )(y)\n\n        y = keras_resnet.layers.BatchNormalization(\n            axis=axis,\n            epsilon=1e-5,\n            freeze=freeze_bn,\n            name=""bn{}{}_branch2c"".format(stage_char, block_char)\n        )(y)\n\n        if block == 0:\n            shortcut = keras.layers.Conv1D(\n                filters * 4,\n                1,\n                strides=stride,\n                use_bias=False,\n                name=""res{}{}_branch1"".format(stage_char, block_char),\n                **parameters\n            )(x)\n\n            shortcut = keras_resnet.layers.BatchNormalization(\n                axis=axis,\n                epsilon=1e-5,\n                freeze=freeze_bn,\n                name=""bn{}{}_branch1"".format(stage_char, block_char)\n            )(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(\n            name=""res{}{}"".format(stage_char, block_char)\n        )([y, shortcut])\n\n        y = keras.layers.Activation(\n            ""relu"",\n            name=""res{}{}_relu"".format(stage_char, block_char)\n        )(y)\n\n        return y\n\n    return f\n'"
keras_resnet/blocks/_2d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.blocks._2d\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements a number of popular two-dimensional residual blocks.\n""""""\n\nimport keras.layers\nimport keras.regularizers\n\nimport keras_resnet.layers\n\nparameters = {\n    ""kernel_initializer"": ""he_normal""\n}\n\n\ndef basic_2d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n    A two-dimensional basic block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.basic_2d(64)\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.ZeroPadding2D(padding=1, name=""padding{}{}_branch2a"".format(stage_char, block_char))(x)\n\n        y = keras.layers.Conv2D(filters, kernel_size, strides=stride, use_bias=False, name=""res{}{}_branch2a"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_branch2a_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.ZeroPadding2D(padding=1, name=""padding{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Conv2D(filters, kernel_size, use_bias=False, name=""res{}{}_branch2b"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        if block == 0:\n            shortcut = keras.layers.Conv2D(filters, (1, 1), strides=stride, use_bias=False, name=""res{}{}_branch1"".format(stage_char, block_char), **parameters)(x)\n\n            shortcut = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch1"".format(stage_char, block_char))(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(name=""res{}{}"".format(stage_char, block_char))([y, shortcut])\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_relu"".format(stage_char, block_char))(y)\n\n        return y\n\n    return f\n\n\ndef bottleneck_2d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n    A two-dimensional bottleneck block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.bottleneck_2d(64)\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.Conv2D(filters, (1, 1), strides=stride, use_bias=False, name=""res{}{}_branch2a"".format(stage_char, block_char), **parameters)(x)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_branch2a_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.ZeroPadding2D(padding=1, name=""padding{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Conv2D(filters, kernel_size, use_bias=False, name=""res{}{}_branch2b"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_branch2b_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Conv2D(filters * 4, (1, 1), use_bias=False, name=""res{}{}_branch2c"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2c"".format(stage_char, block_char))(y)\n\n        if block == 0:\n            shortcut = keras.layers.Conv2D(filters * 4, (1, 1), strides=stride, use_bias=False, name=""res{}{}_branch1"".format(stage_char, block_char), **parameters)(x)\n\n            shortcut = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch1"".format(stage_char, block_char))(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(name=""res{}{}"".format(stage_char, block_char))([y, shortcut])\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_relu"".format(stage_char, block_char))(y)\n\n        return y\n\n    return f\n'"
keras_resnet/blocks/_3d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.blocks._3d\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements a number of popular three-dimensional residual blocks.\n""""""\n\nimport keras.layers\nimport keras.regularizers\n\nimport keras_resnet.layers\n\nparameters = {\n    ""kernel_initializer"": ""he_normal""\n}\n\n\ndef basic_3d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n    A three-dimensional basic block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.basic_3d(64)\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.ZeroPadding3D(padding=1, name=""padding{}{}_branch2a"".format(stage_char, block_char))(x)\n\n        y = keras.layers.Conv3D(filters, kernel_size, strides=stride, use_bias=False, name=""res{}{}_branch2a"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_branch2a_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.ZeroPadding3D(padding=1, name=""padding{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Conv3D(filters, kernel_size, use_bias=False, name=""res{}{}_branch2b"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        if block == 0:\n            shortcut = keras.layers.Conv3D(filters, (1, 1), strides=stride, use_bias=False, name=""res{}{}_branch1"".format(stage_char, block_char), **parameters)(x)\n\n            shortcut = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch1"".format(stage_char, block_char))(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(name=""res{}{}"".format(stage_char, block_char))([y, shortcut])\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_relu"".format(stage_char, block_char))(y)\n\n        return y\n\n    return f\n\n\ndef bottleneck_3d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n    A three-dimensional bottleneck block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.bottleneck_3d(64)\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.Conv3D(filters, (1, 1), strides=stride, use_bias=False, name=""res{}{}_branch2a"".format(stage_char, block_char), **parameters)(x)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_branch2a_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.ZeroPadding3D(padding=1, name=""padding{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Conv3D(filters, kernel_size, use_bias=False, name=""res{}{}_branch2b"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_branch2b_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.Conv3D(filters * 4, (1, 1), use_bias=False, name=""res{}{}_branch2c"".format(stage_char, block_char), **parameters)(y)\n\n        y = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch2c"".format(stage_char, block_char))(y)\n\n        if block == 0:\n            shortcut = keras.layers.Conv3D(filters * 4, (1, 1), strides=stride, use_bias=False, name=""res{}{}_branch1"".format(stage_char, block_char), **parameters)(x)\n\n            shortcut = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn{}{}_branch1"".format(stage_char, block_char))(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(name=""res{}{}"".format(stage_char, block_char))([y, shortcut])\n\n        y = keras.layers.Activation(""relu"", name=""res{}{}_relu"".format(stage_char, block_char))(y)\n\n        return y\n\n    return f\n'"
keras_resnet/blocks/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.blocks\n~~~~~~~~~~~~~~~~~~~\n\nThis module implements a number of popular residual blocks.\n""""""\n\nfrom ._1d import (\n    basic_1d,\n    bottleneck_1d\n)\n\nfrom ._2d import (\n    basic_2d,\n    bottleneck_2d\n)\n\nfrom ._3d import (\n    basic_3d,\n    bottleneck_3d\n)\n\nfrom ._time_distributed_2d import (\n    time_distributed_basic_2d,\n    time_distributed_bottleneck_2d\n)\n'"
keras_resnet/blocks/_time_distributed_2d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.blocks._time_distributed_2d\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements a number of popular time distributed two-dimensional residual blocks.\n""""""\n\nimport keras.layers\nimport keras.regularizers\n\nimport keras_resnet.layers\n\nparameters = {\n    ""kernel_initializer"": ""he_normal""\n}\n\n\ndef time_distributed_basic_2d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n\n    A time distributed two-dimensional basic block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.time_distributed_basic_2d(64)\n\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.TimeDistributed(keras.layers.ZeroPadding2D(padding=1), name=""padding{}{}_branch2a"".format(stage_char, block_char))(x)\n\n        y = keras.layers.TimeDistributed(keras.layers.Conv2D(filters, kernel_size, strides=stride, use_bias=False, **parameters), name=""res{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.Activation(""relu""), name=""res{}{}_branch2a_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.ZeroPadding2D(padding=1), name=""padding{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.Conv2D(filters, kernel_size, use_bias=False, **parameters), name=""res{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        if block == 0:\n            shortcut = keras.layers.TimeDistributed(keras.layers.Conv2D(filters, (1, 1), strides=stride, use_bias=False, **parameters), name=""res{}{}_branch1"".format(stage_char, block_char))(x)\n\n            shortcut = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch1"".format(stage_char, block_char))(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(name=""res{}{}"".format(stage_char, block_char))([y, shortcut])\n\n        y = keras.layers.TimeDistributed(keras.layers.Activation(""relu""), name=""res{}{}_relu"".format(stage_char, block_char))(y)\n\n        return y\n\n    return f\n\n\ndef time_distributed_bottleneck_2d(\n    filters,\n    stage=0,\n    block=0,\n    kernel_size=3,\n    numerical_name=False,\n    stride=None,\n    freeze_bn=False\n):\n    """"""\n\n    A time distributed two-dimensional bottleneck block.\n\n    :param filters: the output\xe2\x80\x99s feature space\n\n    :param stage: int representing the stage of this block (starting from 0)\n\n    :param block: int representing this block (starting from 0)\n\n    :param kernel_size: size of the kernel\n\n    :param numerical_name: if true, uses numbers to represent blocks instead of chars (ResNet{101, 152, 200})\n\n    :param stride: int representing the stride used in the shortcut and the first conv layer, default derives stride from block id\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n\n        >>> keras_resnet.blocks.time_distributed_bottleneck_2d(64)\n\n    """"""\n    if stride is None:\n        if block != 0 or stage == 0:\n            stride = 1\n        else:\n            stride = 2\n\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    if block > 0 and numerical_name:\n        block_char = ""b{}"".format(block)\n    else:\n        block_char = chr(ord(\'a\') + block)\n\n    stage_char = str(stage + 2)\n\n    def f(x):\n        y = keras.layers.TimeDistributed(keras.layers.Conv2D(filters, (1, 1), strides=stride, use_bias=False, **parameters), name=""res{}{}_branch2a"".format(stage_char, block_char))(x)\n\n        y = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch2a"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.Activation(""relu""), name=""res{}{}_branch2a_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.ZeroPadding2D(padding=1), name=""padding{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.Conv2D(filters, kernel_size, use_bias=False, **parameters), name=""res{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch2b"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.Activation(""relu""), name=""res{}{}_branch2b_relu"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras.layers.Conv2D(filters * 4, (1, 1), use_bias=False, **parameters), name=""res{}{}_branch2c"".format(stage_char, block_char))(y)\n\n        y = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch2c"".format(stage_char, block_char))(y)\n\n        if block == 0:\n            shortcut = keras.layers.TimeDistributed(keras.layers.Conv2D(filters * 4, (1, 1), strides=stride, use_bias=False, **parameters), name=""res{}{}_branch1"".format(stage_char, block_char))(x)\n\n            shortcut = keras.layers.TimeDistributed(keras.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn{}{}_branch1"".format(stage_char, block_char))(shortcut)\n        else:\n            shortcut = x\n\n        y = keras.layers.Add(name=""res{}{}"".format(stage_char, block_char))([y, shortcut])\n\n        y = keras.layers.TimeDistributed(keras.layers.Activation(""relu""), name=""res{}{}_relu"".format(stage_char, block_char))(y)\n\n        return y\n\n    return f\n'"
keras_resnet/classifiers/_2d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.classifiers\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular residual two-dimensional classifiers.\n""""""\n\nimport keras.backend\nimport keras.layers\nimport keras.models\nimport keras.regularizers\n\nimport keras_resnet.models\n\n\nclass ResNet18(keras.models.Model):\n    """"""\n    A :class:`ResNet18 <ResNet18>` object.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    Usage:\n\n        >>> import keras_resnet.classifiers\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.classifiers.ResNet18(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, classes):\n        outputs = keras_resnet.models.ResNet18(inputs)\n\n        outputs = keras.layers.Flatten()(outputs.output)\n\n        outputs = keras.layers.Dense(classes, activation=""softmax"")(outputs)\n\n        super(ResNet18, self).__init__(inputs, outputs)\n\n\nclass ResNet34(keras.models.Model):\n    """"""\n    A :class:`ResNet34 <ResNet34>` object.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    Usage:\n\n        >>> import keras_resnet.classifiers\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.classifiers.ResNet34(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, classes):\n        outputs = keras_resnet.models.ResNet34(inputs)\n\n        outputs = keras.layers.Flatten()(outputs.output)\n\n        outputs = keras.layers.Dense(classes, activation=""softmax"")(outputs)\n\n        super(ResNet34, self).__init__(inputs, outputs)\n\n\nclass ResNet50(keras.models.Model):\n    """"""\n    A :class:`ResNet50 <ResNet50>` object.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    Usage:\n\n        >>> import keras_resnet.classifiers\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.classifiers.ResNet50(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, classes):\n        outputs = keras_resnet.models.ResNet50(inputs)\n\n        outputs = keras.layers.Flatten()(outputs.output)\n\n        outputs = keras.layers.Dense(classes, activation=""softmax"")(outputs)\n\n        super(ResNet50, self).__init__(inputs, outputs)\n\n\nclass ResNet101(keras.models.Model):\n    """"""\n    A :class:`ResNet101 <ResNet101>` object.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    Usage:\n\n        >>> import keras_resnet.classifiers\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.classifiers.ResNet101(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, classes):\n        outputs = keras_resnet.models.ResNet101(inputs)\n\n        outputs = keras.layers.Flatten()(outputs.output)\n\n        outputs = keras.layers.Dense(classes, activation=""softmax"")(outputs)\n\n        super(ResNet101, self).__init__(inputs, outputs)\n\n\nclass ResNet152(keras.models.Model):\n    """"""\n    A :class:`ResNet152 <ResNet152>` object.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    Usage:\n\n        >>> import keras_resnet.classifiers\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.classifiers.ResNet152(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n\n    """"""\n    def __init__(self, inputs, classes):\n        outputs = keras_resnet.models.ResNet152(inputs)\n\n        outputs = keras.layers.Flatten()(outputs.output)\n\n        outputs = keras.layers.Dense(classes, activation=""softmax"")(outputs)\n\n        super(ResNet152, self).__init__(inputs, outputs)\n\n\nclass ResNet200(keras.models.Model):\n    """"""\n    A :class:`ResNet200 <ResNet200>` object.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    Usage:\n\n        >>> import keras_resnet.classifiers\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.classifiers.ResNet200(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, classes):\n        outputs = keras_resnet.models.ResNet200(inputs)\n\n        outputs = keras.layers.Flatten()(outputs.output)\n\n        outputs = keras.layers.Dense(classes, activation=""softmax"")(outputs)\n\n        super(ResNet200, self).__init__(inputs, outputs)\n'"
keras_resnet/classifiers/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.classifiers\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular residual classifiers.\n""""""\n\nfrom ._2d import (\n    ResNet18,\n    ResNet34,\n    ResNet50,\n    ResNet101,\n    ResNet152,\n    ResNet200\n)\n'"
keras_resnet/layers/__init__.py,0,b'from ._batch_normalization import BatchNormalization\n'
keras_resnet/layers/_batch_normalization.py,0,"b'import keras\n\n\nclass BatchNormalization(keras.layers.BatchNormalization):\n    """"""\n    Identical to keras.layers.BatchNormalization, but adds the option to freeze parameters.\n    """"""\n    def __init__(self, freeze, *args, **kwargs):\n        self.freeze = freeze\n        super(BatchNormalization, self).__init__(*args, **kwargs)\n\n        # set to non-trainable if freeze is true\n        self.trainable = not self.freeze\n\n    def call(self, *args, **kwargs):\n        # Force test mode if frozen, otherwise use default behaviour (i.e., training=None).\n        if self.freeze:\n            kwargs[\'training\'] = False\n        return super(BatchNormalization, self).call(*args, **kwargs)\n\n    def get_config(self):\n        config = super(BatchNormalization, self).get_config()\n        config.update({\'freeze\': self.freeze})\n        return config\n'"
keras_resnet/models/_1d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.models._1d\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular one-dimensional residual models.\n""""""\n\nimport keras.backend\nimport keras.layers\nimport keras.models\nimport keras.regularizers\n\nimport keras_resnet.blocks\nimport keras_resnet.layers\n\n\nclass ResNet1D(keras.Model):\n    """"""\n    Constructs a `keras.models.Model` object using the given block count.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param block: a residual block (e.g. an instance of `keras_resnet.blocks.basic_1d`)\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :param numerical_names: list of bool, same size as blocks, used to indicate whether names of layers should include numbers or letters\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> blocks = [2, 2, 2, 2]\n\n        >>> block = keras_resnet.blocks.basic_1d\n\n        >>> model = keras_resnet.models.ResNet(x, classes, blocks, block, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(\n        self,\n        inputs,\n        blocks,\n        block,\n        include_top=True,\n        classes=1000,\n        freeze_bn=True,\n        numerical_names=None,\n        *args,\n        **kwargs\n    ):\n        if keras.backend.image_data_format() == ""channels_last"":\n            axis = 3\n        else:\n            axis = 1\n\n        if numerical_names is None:\n            numerical_names = [True] * len(blocks)\n\n        x = keras.layers.ZeroPadding1D(padding=3, name=""padding_conv1"")(inputs)\n        x = keras.layers.Conv1D(64, (7, 7), strides=(2, 2), use_bias=False, name=""conv1"")(x)\n        x = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn_conv1"")(x)\n        x = keras.layers.Activation(""relu"", name=""conv1_relu"")(x)\n        x = keras.layers.MaxPooling1D((3, 3), strides=(2, 2), padding=""same"", name=""pool1"")(x)\n\n        features = 64\n\n        outputs = []\n\n        for stage_id, iterations in enumerate(blocks):\n            for block_id in range(iterations):\n                x = block(\n                    features,\n                    stage_id,\n                    block_id,\n                    numerical_name=(block_id > 0 and numerical_names[stage_id]),\n                    freeze_bn=freeze_bn\n                )(x)\n\n            features *= 2\n\n            outputs.append(x)\n\n        if include_top:\n            assert classes > 0\n\n            x = keras.layers.GlobalAveragePooling1D(name=""pool5"")(x)\n            x = keras.layers.Dense(classes, activation=""softmax"", name=""fc1000"")(x)\n\n            super(ResNet1D, self).__init__(inputs=inputs, outputs=x, *args, **kwargs)\n        else:\n            # Else output each stages features\n            super(ResNet1D, self).__init__(inputs=inputs, outputs=outputs, *args, **kwargs)\n\n\nclass ResNet1D18(ResNet1D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet18 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet18(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [2, 2, 2, 2]\n\n        super(ResNet1D18, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_1d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet1D34(ResNet1D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet34 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet34(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        super(ResNet1D34, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_1d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet1D50(ResNet1D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet50 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet50(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        numerical_names = [False, False, False, False]\n\n        super(ResNet1D50, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_1d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet1D101(ResNet1D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet101 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet101(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 23, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet1D101, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_1d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet1D152(ResNet1D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet152 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet152(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 8, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet1D152, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_1d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet1D200(ResNet1D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet200 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet200(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 24, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet1D200, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_1d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n'"
keras_resnet/models/_2d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.models._2d\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular two-dimensional residual models.\n""""""\n\nimport keras.backend\nimport keras.layers\nimport keras.models\nimport keras.regularizers\n\nimport keras_resnet.blocks\nimport keras_resnet.layers\n\n\nclass ResNet2D(keras.Model):\n    """"""\n    Constructs a `keras.models.Model` object using the given block count.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param block: a residual block (e.g. an instance of `keras_resnet.blocks.basic_2d`)\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :param numerical_names: list of bool, same size as blocks, used to indicate whether names of layers should include numbers or letters\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> blocks = [2, 2, 2, 2]\n\n        >>> block = keras_resnet.blocks.basic_2d\n\n        >>> model = keras_resnet.models.ResNet(x, classes, blocks, block, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(\n        self,\n        inputs,\n        blocks,\n        block,\n        include_top=True,\n        classes=1000,\n        freeze_bn=True,\n        numerical_names=None,\n        *args,\n        **kwargs\n    ):\n        if keras.backend.image_data_format() == ""channels_last"":\n            axis = 3\n        else:\n            axis = 1\n\n        if numerical_names is None:\n            numerical_names = [True] * len(blocks)\n\n        x = keras.layers.Conv2D(64, (7, 7), strides=(2, 2), use_bias=False, name=""conv1"", padding=""same"")(inputs)\n        x = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn_conv1"")(x)\n        x = keras.layers.Activation(""relu"", name=""conv1_relu"")(x)\n        x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=""same"", name=""pool1"")(x)\n\n        features = 64\n\n        outputs = []\n\n        for stage_id, iterations in enumerate(blocks):\n            for block_id in range(iterations):\n                x = block(\n                    features,\n                    stage_id,\n                    block_id,\n                    numerical_name=(block_id > 0 and numerical_names[stage_id]),\n                    freeze_bn=freeze_bn\n                )(x)\n\n            features *= 2\n\n            outputs.append(x)\n\n        if include_top:\n            assert classes > 0\n\n            x = keras.layers.GlobalAveragePooling2D(name=""pool5"")(x)\n            x = keras.layers.Dense(classes, activation=""softmax"", name=""fc1000"")(x)\n\n            super(ResNet2D, self).__init__(inputs=inputs, outputs=x, *args, **kwargs)\n        else:\n            # Else output each stages features\n            super(ResNet2D, self).__init__(inputs=inputs, outputs=outputs, *args, **kwargs)\n\n\nclass ResNet2D18(ResNet2D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet18 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet18(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [2, 2, 2, 2]\n\n        super(ResNet2D18, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_2d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet2D34(ResNet2D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet34 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet34(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        super(ResNet2D34, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_2d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet2D50(ResNet2D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet50 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet50(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        numerical_names = [False, False, False, False]\n\n        super(ResNet2D50, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet2D101(ResNet2D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet101 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet101(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 23, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet2D101, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet2D152(ResNet2D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet152 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet152(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 8, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet2D152, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet2D200(ResNet2D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet200 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet200(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 24, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet2D200, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n'"
keras_resnet/models/_3d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.models._3d\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular three-dimensional residual models.\n""""""\n\nimport keras.backend\nimport keras.layers\nimport keras.models\nimport keras.regularizers\n\nimport keras_resnet.blocks\nimport keras_resnet.layers\n\n\nclass ResNet3D(keras.Model):\n    """"""\n    Constructs a `keras.models.Model` object using the given block count.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param block: a residual block (e.g. an instance of `keras_resnet.blocks.basic_3d`)\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :param numerical_names: list of bool, same size as blocks, used to indicate whether names of layers should include numbers or letters\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> blocks = [2, 2, 2, 2]\n\n        >>> block = keras_resnet.blocks.basic_3d\n\n        >>> model = keras_resnet.models.ResNet(x, classes, blocks, block, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(\n        self,\n        inputs,\n        blocks,\n        block,\n        include_top=True,\n        classes=1000,\n        freeze_bn=True,\n        numerical_names=None,\n        *args,\n        **kwargs\n    ):\n        if keras.backend.image_data_format() == ""channels_last"":\n            axis = 3\n        else:\n            axis = 1\n\n        if numerical_names is None:\n            numerical_names = [True] * len(blocks)\n\n        x = keras.layers.ZeroPadding3D(padding=3, name=""padding_conv1"")(inputs)\n        x = keras.layers.Conv3D(64, (7, 7), strides=(2, 2), use_bias=False, name=""conv1"")(x)\n        x = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn_conv1"")(x)\n        x = keras.layers.Activation(""relu"", name=""conv1_relu"")(x)\n        x = keras.layers.MaxPooling3D((3, 3), strides=(2, 2), padding=""same"", name=""pool1"")(x)\n\n        features = 64\n\n        outputs = []\n\n        for stage_id, iterations in enumerate(blocks):\n            for block_id in range(iterations):\n                x = block(\n                    features,\n                    stage_id,\n                    block_id,\n                    numerical_name=(block_id > 0 and numerical_names[stage_id]),\n                    freeze_bn=freeze_bn\n                )(x)\n\n            features *= 2\n\n            outputs.append(x)\n\n        if include_top:\n            assert classes > 0\n\n            x = keras.layers.GlobalAveragePooling3D(name=""pool5"")(x)\n            x = keras.layers.Dense(classes, activation=""softmax"", name=""fc1000"")(x)\n\n            super(ResNet3D, self).__init__(inputs=inputs, outputs=x, *args, **kwargs)\n        else:\n            # Else output each stages features\n            super(ResNet3D, self).__init__(inputs=inputs, outputs=outputs, *args, **kwargs)\n\n\nclass ResNet3D18(ResNet3D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet18 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet18(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [2, 2, 2, 2]\n\n        super(ResNet3D18, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_3d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet3D34(ResNet3D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet34 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet34(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        super(ResNet3D34, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_3d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet3D50(ResNet3D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet50 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet50(x)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        numerical_names = [False, False, False, False]\n\n        super(ResNet3D50, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_3d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet3D101(ResNet3D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet101 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet101(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 23, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet3D101, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_3d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet3D152(ResNet3D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet152 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet152(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 8, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet3D152, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_3d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n\n\nclass ResNet3D200(ResNet3D):\n    """"""\n    Constructs a `keras.models.Model` according to the ResNet200 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> model = keras_resnet.models.ResNet200(x, classes=classes)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    def __init__(self, inputs, blocks=None, include_top=True, classes=1000, freeze_bn=False, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 24, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(ResNet3D200, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_3d,\n            include_top=include_top,\n            classes=classes,\n            freeze_bn=freeze_bn,\n            *args,\n            **kwargs\n        )\n'"
keras_resnet/models/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.models\n~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular residual models.\n""""""\n\nfrom ._1d import (\n    ResNet1D,\n    ResNet1D18,\n    ResNet1D34,\n    ResNet1D50,\n    ResNet1D101,\n    ResNet1D152,\n    ResNet1D200\n)\n\nfrom ._2d import (\n    ResNet2D,\n    ResNet2D18,\n    ResNet2D34,\n    ResNet2D50,\n    ResNet2D101,\n    ResNet2D152,\n    ResNet2D200\n)\n\nfrom ._3d import (\n    ResNet3D,\n    ResNet3D18,\n    ResNet3D34,\n    ResNet3D50,\n    ResNet3D101,\n    ResNet3D152,\n    ResNet3D200\n)\n\nfrom ._feature_pyramid_2d import (\n    FPN2D,\n    FPN2D18,\n    FPN2D34,\n    FPN2D50,\n    FPN2D101,\n    FPN2D152,\n    FPN2D200\n)\n\nfrom ._time_distributed_2d import (\n    TimeDistributedResNet,\n    TimeDistributedResNet18,\n    TimeDistributedResNet34,\n    TimeDistributedResNet50,\n    TimeDistributedResNet101,\n    TimeDistributedResNet152,\n    TimeDistributedResNet200\n)\n\n# for backwards compatibility reasons\nResNet = ResNet2D\nResNet18 = ResNet2D18\nResNet34 = ResNet2D34\nResNet50 = ResNet2D50\nResNet101 = ResNet2D101\nResNet152 = ResNet2D152\nResNet200 = ResNet2D200\n'"
keras_resnet/models/_feature_pyramid_2d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.models._feature_pyramid_2d\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular two-dimensional feature pyramid networks (FPNs).\n""""""\n\nimport keras.backend\nimport keras.layers\nimport keras.models\nimport keras.regularizers\n\nimport keras_resnet.blocks\nimport keras_resnet.layers\n\n\nclass FPN2D(keras.Model):\n    def __init__(\n            self,\n            inputs,\n            blocks,\n            block,\n            freeze_bn=True,\n            numerical_names=None,\n            *args,\n            **kwargs\n    ):\n        if keras.backend.image_data_format() == ""channels_last"":\n            axis = 3\n        else:\n            axis = 1\n\n        if numerical_names is None:\n            numerical_names = [True] * len(blocks)\n\n        x = keras.layers.Conv2D(64, (7, 7), strides=(2, 2), use_bias=False, name=""conv1"", padding=""same"")(inputs)\n        x = keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn, name=""bn_conv1"")(x)\n        x = keras.layers.Activation(""relu"", name=""conv1_relu"")(x)\n        x = keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=""same"", name=""pool1"")(x)\n\n        features = 64\n\n        outputs = []\n\n        for stage_id, iterations in enumerate(blocks):\n            for block_id in range(iterations):\n                x = block(\n                    features,\n                    stage_id,\n                    block_id,\n                    numerical_name=(block_id > 0 and numerical_names[stage_id]),\n                    freeze_bn=freeze_bn\n                )(x)\n\n            features *= 2\n\n            outputs.append(x)\n\n        c2, c3, c4, c5 = outputs\n\n        pyramid_5 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=1,\n            strides=1,\n            padding=""same"",\n            name=""c5_reduced""\n        )(c5)\n\n        upsampled_p5 = keras.layers.UpSampling2D(\n            interpolation=""bilinear"",\n            name=""p5_upsampled"",\n            size=(2, 2)\n        )(pyramid_5)\n\n        pyramid_4 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=1,\n            strides=1,\n            padding=""same"",\n            name=""c4_reduced""\n        )(c4)\n\n        pyramid_4 = keras.layers.Add(\n            name=""p4_merged""\n        )([upsampled_p5, pyramid_4])\n\n        upsampled_p4 = keras.layers.UpSampling2D(\n            interpolation=""bilinear"",\n            name=""p4_upsampled"",\n            size=(2, 2)\n        )(pyramid_4)\n\n        pyramid_4 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=3,\n            strides=1,\n            padding=""same"",\n            name=""p4""\n        )(pyramid_4)\n\n        pyramid_3 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=1,\n            strides=1,\n            padding=""same"",\n            name=""c3_reduced""\n        )(c3)\n\n        pyramid_3 = keras.layers.Add(\n            name=""p3_merged""\n        )([upsampled_p4, pyramid_3])\n\n        upsampled_p3 = keras.layers.UpSampling2D(\n            interpolation=""bilinear"",\n            name=""p3_upsampled"",\n            size=(2, 2)\n        )(pyramid_3)\n\n        pyramid_3 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=3,\n            strides=1,\n            padding=""same"",\n            name=""p3""\n        )(pyramid_3)\n\n        pyramid_2 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=1,\n            strides=1,\n            padding=""same"",\n            name=""c2_reduced""\n        )(c2)\n\n        pyramid_2 = keras.layers.Add(\n            name=""p2_merged""\n        )([upsampled_p3, pyramid_2])\n\n        pyramid_2 = keras.layers.Conv2D(\n            filters=256,\n            kernel_size=3,\n            strides=1,\n            padding=""same"",\n            name=""p2""\n        )(pyramid_2)\n\n        pyramid_6 = keras.layers.MaxPooling2D(strides=2, name=""p6"")(pyramid_5)\n\n        outputs = [\n            pyramid_2,\n            pyramid_3,\n            pyramid_4,\n            pyramid_5,\n            pyramid_6\n        ]\n\n        super(FPN2D, self).__init__(\n            inputs=inputs,\n            outputs=outputs,\n            *args,\n            **kwargs\n        )\n\n\nclass FPN2D50(FPN2D):\n    def __init__(self, inputs, blocks=None, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        numerical_names = [False, False, False, False]\n\n        super(FPN2D50, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            *args,\n            **kwargs\n        )\n\n\nclass FPN2D18(FPN2D):\n    def __init__(self, inputs, blocks=None, *args, **kwargs):\n        if blocks is None:\n            blocks = [2, 2, 2, 2]\n\n        super(FPN2D18, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_2d,\n            *args,\n            **kwargs\n        )\n\n\nclass FPN2D34(FPN2D):\n    def __init__(self, inputs, blocks=None, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 6, 3]\n\n        super(FPN2D34, self).__init__(\n            inputs,\n            blocks,\n            block=keras_resnet.blocks.basic_2d,\n            *args,\n            **kwargs\n        )\n\n\nclass FPN2D101(FPN2D):\n    def __init__(self, inputs, blocks=None, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 4, 23, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(FPN2D101, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            *args,\n            **kwargs\n        )\n\n\nclass FPN2D152(FPN2D):\n    def __init__(self, inputs, blocks=None, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 8, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(FPN2D152, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            *args,\n            **kwargs\n        )\n\n\nclass FPN2D200(FPN2D):\n    def __init__(self, inputs, blocks=None, *args, **kwargs):\n        if blocks is None:\n            blocks = [3, 24, 36, 3]\n\n        numerical_names = [False, True, True, False]\n\n        super(FPN2D200, self).__init__(\n            inputs,\n            blocks,\n            numerical_names=numerical_names,\n            block=keras_resnet.blocks.bottleneck_2d,\n            *args,\n            **kwargs\n        )\n'"
keras_resnet/models/_time_distributed_2d.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\nkeras_resnet.models._time_distributed_2d\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements popular time distributed two-dimensional residual networks.\n""""""\n\nimport keras.backend\nimport keras.layers\nimport keras.models\nimport keras.regularizers\n\nimport keras_resnet.blocks\nimport keras_resnet.layers\n\n\ndef TimeDistributedResNet(inputs, blocks, block, include_top=True, classes=1000, freeze_bn=True, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` object using the given block count.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param block: a time distributed residual block (e.g. an instance of `keras_resnet.blocks.time_distributed_basic_2d`)\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :param freeze_bn: if true, freezes BatchNormalization layers (ie. no updates are done in these layers)\n\n    :return model: Time distributed ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.blocks\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> blocks = [2, 2, 2, 2]\n\n        >>> blocks = keras_resnet.blocks.time_distributed_basic_2d\n\n        >>> y = keras_resnet.models.TimeDistributedResNet(x, classes, blocks, blocks)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if keras.backend.image_data_format() == ""channels_last"":\n        axis = 3\n    else:\n        axis = 1\n\n    x = keras.layers.TimeDistributed(keras.layers.ZeroPadding2D(padding=3), name=""padding_conv1"")(inputs)\n    x = keras.layers.TimeDistributed(keras.layers.Conv2D(64, (7, 7), strides=(2, 2), use_bias=False), name=""conv1"")(x)\n    x = keras.layers.TimeDistributed(keras_resnet.layers.BatchNormalization(axis=axis, epsilon=1e-5, freeze=freeze_bn), name=""bn_conv1"")(x)\n    x = keras.layers.TimeDistributed(keras.layers.Activation(""relu""), name=""conv1_relu"")(x)\n    x = keras.layers.TimeDistributed(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding=""same""), name=""pool1"")(x)\n\n    features = 64\n\n    outputs = []\n\n    for stage_id, iterations in enumerate(blocks):\n        for block_id in range(iterations):\n            x = block(features, stage_id, block_id, numerical_name=(blocks[stage_id] > 6), freeze_bn=freeze_bn)(x)\n\n        features *= 2\n        outputs.append(x)\n\n    if include_top:\n        assert classes > 0\n\n        x = keras.layers.TimeDistributed(keras.layers.GlobalAveragePooling2D(), name=""pool5"")(x)\n        x = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""), name=""fc1000"")(x)\n\n        return keras.models.Model(inputs=inputs, outputs=x, *args, **kwargs)\n    else:\n        # Else output each stages features\n        return keras.models.Model(inputs=inputs, outputs=outputs, *args, **kwargs)\n\n\ndef TimeDistributedResNet18(inputs, blocks=None, include_top=True, classes=1000, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` according to the ResNet18 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :return model: Time distributed ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> y = keras_resnet.models.TimeDistributedResNet18(x)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if blocks is None:\n        blocks = [2, 2, 2, 2]\n\n    return TimeDistributedResNet(inputs, blocks, block=keras_resnet.blocks.time_distributed_basic_2d, include_top=include_top, classes=classes, *args, **kwargs)\n\n\ndef TimeDistributedResNet34(inputs, blocks=None, include_top=True, classes=1000, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` according to the ResNet34 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :return model: Time distributed ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> y = keras_resnet.models.TimeDistributedResNet34(x)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if blocks is None:\n        blocks = [3, 4, 6, 3]\n\n    return TimeDistributedResNet(inputs, blocks, block=keras_resnet.blocks.time_distributed_basic_2d, include_top=include_top, classes=classes, *args, **kwargs)\n\n\ndef TimeDistributedResNet50(inputs, blocks=None, include_top=True, classes=1000, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` according to the ResNet50 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> y = keras_resnet.models.TimeDistributedResNet50(x)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if blocks is None:\n        blocks = [3, 4, 6, 3]\n\n    return TimeDistributedResNet(inputs, blocks, block=keras_resnet.blocks.time_distributed_bottleneck_2d, include_top=include_top, classes=classes, *args, **kwargs)\n\n\ndef TimeDistributedResNet101(inputs, blocks=None, include_top=True, classes=1000, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` according to the ResNet101 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :return model: Time distributed ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> y = keras_resnet.models.TimeDistributedResNet101(x)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if blocks is None:\n        blocks = [3, 4, 23, 3]\n\n    return TimeDistributedResNet(inputs, blocks, block=keras_resnet.blocks.time_distributed_bottleneck_2d, include_top=include_top, classes=classes, *args, **kwargs)\n\n\ndef TimeDistributedResNet152(inputs, blocks=None, include_top=True, classes=1000, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` according to the ResNet152 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :return model: Time distributed ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> y = keras_resnet.models.TimeDistributedResNet152(x)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if blocks is None:\n        blocks = [3, 8, 36, 3]\n\n    return TimeDistributedResNet(inputs, blocks, block=keras_resnet.blocks.time_distributed_bottleneck_2d, include_top=include_top, classes=classes, *args, **kwargs)\n\n\ndef TimeDistributedResNet200(inputs, blocks=None, include_top=True, classes=1000, *args, **kwargs):\n    """"""\n    Constructs a time distributed `keras.models.Model` according to the ResNet200 specifications.\n\n    :param inputs: input tensor (e.g. an instance of `keras.layers.Input`)\n\n    :param blocks: the network\xe2\x80\x99s residual architecture\n\n    :param include_top: if true, includes classification layers\n\n    :param classes: number of classes to classify (include_top must be true)\n\n    :return model: Time distributed ResNet model with encoding output (if `include_top=False`) or classification output (if `include_top=True`)\n\n    Usage:\n\n        >>> import keras_resnet.models\n\n        >>> shape, classes = (224, 224, 3), 1000\n\n        >>> x = keras.layers.Input(shape)\n\n        >>> y = keras_resnet.models.TimeDistributedResNet200(x)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Flatten())(y.output)\n\n        >>> y = keras.layers.TimeDistributed(keras.layers.Dense(classes, activation=""softmax""))(y)\n\n        >>> model = keras.models.Model(x, y)\n\n        >>> model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])\n    """"""\n    if blocks is None:\n        blocks = [3, 24, 36, 3]\n\n    return TimeDistributedResNet(inputs, blocks, block=keras_resnet.blocks.time_distributed_bottleneck_2d, include_top=include_top, classes=classes, *args, **kwargs)\n'"
