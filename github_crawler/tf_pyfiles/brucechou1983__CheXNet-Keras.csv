file_path,api_count,code
augmenter.py,0,"b'from imgaug import augmenters as iaa\n\naugmenter = iaa.Sequential(\n    [\n        iaa.Fliplr(0.5),\n    ],\n    random_order=True,\n)\n'"
callback.py,0,"b'import json\nimport keras.backend as kb\nimport numpy as np\nimport os\nimport shutil\nimport warnings\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\n\nclass MultipleClassAUROC(Callback):\n    """"""\n    Monitor mean AUROC and update model\n    """"""\n    def __init__(self, sequence, class_names, weights_path, stats=None, workers=1):\n        super(Callback, self).__init__()\n        self.sequence = sequence\n        self.workers = workers\n        self.class_names = class_names\n        self.weights_path = weights_path\n        self.best_weights_path = os.path.join(\n            os.path.split(weights_path)[0],\n            f""best_{os.path.split(weights_path)[1]}"",\n        )\n        self.best_auroc_log_path = os.path.join(\n            os.path.split(weights_path)[0],\n            ""best_auroc.log"",\n        )\n        self.stats_output_path = os.path.join(\n            os.path.split(weights_path)[0],\n            "".training_stats.json""\n        )\n        # for resuming previous training\n        if stats:\n            self.stats = stats\n        else:\n            self.stats = {""best_mean_auroc"": 0}\n\n        # aurocs log\n        self.aurocs = {}\n        for c in self.class_names:\n            self.aurocs[c] = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        """"""\n        Calculate the average AUROC and save the best model weights according\n        to this metric.\n\n        """"""\n        print(""\\n*********************************"")\n        self.stats[""lr""] = float(kb.eval(self.model.optimizer.lr))\n        print(f""current learning rate: {self.stats[\'lr\']}"")\n\n        """"""\n        y_hat shape: (#samples, len(class_names))\n        y: [(#samples, 1), (#samples, 1) ... (#samples, 1)]\n        """"""\n        y_hat = self.model.predict_generator(self.sequence, workers=self.workers)\n        y = self.sequence.get_y_true()\n\n        print(f""*** epoch#{epoch + 1} dev auroc ***"")\n        current_auroc = []\n        for i in range(len(self.class_names)):\n            try:\n                score = roc_auc_score(y[:, i], y_hat[:, i])\n            except ValueError:\n                score = 0\n            self.aurocs[self.class_names[i]].append(score)\n            current_auroc.append(score)\n            print(f""{i+1}. {self.class_names[i]}: {score}"")\n        print(""*********************************"")\n\n        # customize your multiple class metrics here\n        mean_auroc = np.mean(current_auroc)\n        print(f""mean auroc: {mean_auroc}"")\n        if mean_auroc > self.stats[""best_mean_auroc""]:\n            print(f""update best auroc from {self.stats[\'best_mean_auroc\']} to {mean_auroc}"")\n\n            # 1. copy best model\n            shutil.copy(self.weights_path, self.best_weights_path)\n\n            # 2. update log file\n            print(f""update log file: {self.best_auroc_log_path}"")\n            with open(self.best_auroc_log_path, ""a"") as f:\n                f.write(f""(epoch#{epoch + 1}) auroc: {mean_auroc}, lr: {self.stats[\'lr\']}\\n"")\n\n            # 3. write stats output, this is used for resuming the training\n            with open(self.stats_output_path, \'w\') as f:\n                json.dump(self.stats, f)\n\n            print(f""update model file: {self.weights_path} -> {self.best_weights_path}"")\n            self.stats[""best_mean_auroc""] = mean_auroc\n            print(""*********************************"")\n        return\n\n\nclass MultiGPUModelCheckpoint(Callback):\n    """"""\n    Checkpointing callback for multi_gpu_model\n    copy from https://github.com/keras-team/keras/issues/8463\n    """"""\n    def __init__(self, filepath, base_model, monitor=\'val_loss\', verbose=0,\n                 save_best_only=False, save_weights_only=False,\n                 mode=\'auto\', period=1):\n        super(Callback, self).__init__()\n        self.base_model = base_model\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.period = period\n        self.epochs_since_last_save = 0\n\n        if mode not in [\'auto\', \'min\', \'max\']:\n            warnings.warn(\'ModelCheckpoint mode %s is unknown, \'\n                          \'fallback to auto mode.\' % (mode),\n                          RuntimeWarning)\n            mode = \'auto\'\n\n        if mode == \'min\':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == \'max\':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if \'acc\' in self.monitor or self.monitor.startswith(\'fmeasure\'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n                self.best = np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n            if self.save_best_only:\n                current = logs.get(self.monitor)\n                if current is None:\n                    warnings.warn(\'Can save best model only with %s available, \'\n                                  \'skipping.\' % (self.monitor), RuntimeWarning)\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose > 0:\n                            print(\'Epoch %05d: %s improved from %0.5f to %0.5f,\'\n                                  \' saving model to %s\'\n                                  % (epoch + 1, self.monitor, self.best,\n                                     current, filepath))\n                        self.best = current\n                        if self.save_weights_only:\n                            self.base_model.save_weights(filepath, overwrite=True)\n                        else:\n                            self.base_model.save(filepath, overwrite=True)\n                    else:\n                        if self.verbose > 0:\n                            print(\'Epoch %05d: %s did not improve\' %\n                                  (epoch + 1, self.monitor))\n            else:\n                if self.verbose > 0:\n                    print(\'Epoch %05d: saving model to %s\' % (epoch + 1, filepath))\n                if self.save_weights_only:\n                    self.base_model.save_weights(filepath, overwrite=True)\n                else:\n                    self.base_model.save(filepath, overwrite=True)\n'"
cam.py,0,"b'import cv2\nimport numpy as np\nimport os\nimport pandas as pd\nfrom configparser import ConfigParser\nfrom generator import AugmentedImageSequence\nfrom models.keras import ModelFactory\nfrom keras import backend as kb\n\n\ndef get_output_layer(model, layer_name):\n    # get the symbolic outputs of each ""key"" layer (we gave them unique names).\n    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n    layer = layer_dict[layer_name]\n    return layer\n\n\ndef create_cam(df_g, output_dir, image_source_dir, model, generator, class_names):\n    """"""\n    Create a CAM overlay image for the input image\n\n    :param df_g: pandas.DataFrame, bboxes on the same image\n    :param output_dir: str\n    :param image_source_dir: str\n    :param model: keras model\n    :param generator: generator.AugmentedImageSequence\n    :param class_names: list of str\n    """"""\n    file_name = df_g[""file_name""]\n    print(f""process image: {file_name}"")\n\n    # draw bbox with labels\n    img_ori = cv2.imread(filename=os.path.join(image_source_dir, file_name))\n\n    label = df_g[""label""]\n    if label == ""Infiltrate"":\n        label = ""Infiltration""\n    index = class_names.index(label)\n\n    output_path = os.path.join(output_dir, f""{label}.{file_name}"")\n\n    img_transformed = generator.load_image(file_name)\n\n    # CAM overlay\n    # Get the 512 input weights to the softmax.\n    class_weights = model.layers[-1].get_weights()[0]\n    final_conv_layer = get_output_layer(model, ""bn"")\n    get_output = kb.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n    [conv_outputs, predictions] = get_output([np.array([img_transformed])])\n    conv_outputs = conv_outputs[0, :, :, :]\n\n    # Create the class activation map.\n    cam = np.zeros(dtype=np.float32, shape=(conv_outputs.shape[:2]))\n    for i, w in enumerate(class_weights[index]):\n        cam += w * conv_outputs[:, :, i]\n    # print(f""predictions: {predictions}"")\n    cam /= np.max(cam)\n    cam = cv2.resize(cam, img_ori.shape[:2])\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n    heatmap[np.where(cam < 0.2)] = 0\n    img = heatmap * 0.5 + img_ori\n\n    # add label & rectangle\n    # ratio = output dimension / 1024\n    ratio = 1\n    x1 = int(df_g[""x""] * ratio)\n    y1 = int(df_g[""y""] * ratio)\n    x2 = int((df_g[""x""] + df_g[""w""]) * ratio)\n    y2 = int((df_g[""y""] + df_g[""h""]) * ratio)\n    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n    cv2.putText(img, text=label, org=(5, 20), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                fontScale=0.8, color=(0, 0, 255), thickness=1)\n    cv2.imwrite(output_path, img)\n\n\ndef main():\n    # parser config\n    config_file = ""./config.ini""\n    cp = ConfigParser()\n    cp.read(config_file)\n\n    # default config\n    output_dir = cp[""DEFAULT""].get(""output_dir"")\n    base_model_name = cp[""DEFAULT""].get(""base_model_name"")\n    class_names = cp[""DEFAULT""].get(""class_names"").split("","")\n    image_source_dir = cp[""DEFAULT""].get(""image_source_dir"")\n    image_dimension = cp[""TRAIN""].getint(""image_dimension"")\n\n    # parse weights file path\n    output_weights_name = cp[""TRAIN""].get(""output_weights_name"")\n    weights_path = os.path.join(output_dir, output_weights_name)\n    best_weights_path = os.path.join(output_dir, f""best_{output_weights_name}"")\n\n    # CAM config\n    bbox_list_file = cp[""CAM""].get(""bbox_list_file"")\n    use_best_weights = cp[""CAM""].getboolean(""use_best_weights"")\n\n    print(""** load model **"")\n    if use_best_weights:\n        print(""** use best weights **"")\n        model_weights_path = best_weights_path\n    else:\n        print(""** use last weights **"")\n        model_weights_path = weights_path\n    model_factory = ModelFactory()\n    model = model_factory.get_model(\n        class_names,\n        model_name=base_model_name,\n        use_base_weights=False,\n        weights_path=model_weights_path)\n\n    print(""read bbox list file"")\n    df_images = pd.read_csv(bbox_list_file, header=None, skiprows=1)\n    df_images.columns = [""file_name"", ""label"", ""x"", ""y"", ""w"", ""h""]\n\n    print(""create a generator for loading transformed images"")\n    cam_sequence = AugmentedImageSequence(\n        dataset_csv_file=os.path.join(output_dir, ""test.csv""),\n        class_names=class_names,\n        source_image_dir=image_source_dir,\n        batch_size=1,\n        target_size=(image_dimension, image_dimension),\n        augmenter=None,\n        steps=1,\n        shuffle_on_epoch_end=False,\n    )\n\n    image_output_dir = os.path.join(output_dir, ""cam"")\n    if not os.path.isdir(image_output_dir):\n        os.makedirs(image_output_dir)\n\n    print(""create CAM"")\n    df_images.apply(\n        lambda g: create_cam(\n            df_g=g,\n            output_dir=image_output_dir,\n            image_source_dir=image_source_dir,\n            model=model,\n            generator=cam_sequence,\n            class_names=class_names,\n        ),\n        axis=1,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
generator.py,0,"b'import numpy as np\nimport os\nimport pandas as pd\nfrom keras.utils import Sequence\nfrom PIL import Image\nfrom skimage.transform import resize\n\n\nclass AugmentedImageSequence(Sequence):\n    """"""\n    Thread-safe image generator with imgaug support\n\n    For more information of imgaug see: https://github.com/aleju/imgaug\n    """"""\n\n    def __init__(self, dataset_csv_file, class_names, source_image_dir, batch_size=16,\n                 target_size=(224, 224), augmenter=None, verbose=0, steps=None,\n                 shuffle_on_epoch_end=True, random_state=1):\n        """"""\n        :param dataset_csv_file: str, path of dataset csv file\n        :param class_names: list of str\n        :param batch_size: int\n        :param target_size: tuple(int, int)\n        :param augmenter: imgaug object. Do not specify resize in augmenter.\n                          It will be done automatically according to input_shape of the model.\n        :param verbose: int\n        """"""\n        self.dataset_df = pd.read_csv(dataset_csv_file)\n        self.source_image_dir = source_image_dir\n        self.batch_size = batch_size\n        self.target_size = target_size\n        self.augmenter = augmenter\n        self.verbose = verbose\n        self.shuffle = shuffle_on_epoch_end\n        self.random_state = random_state\n        self.class_names = class_names\n        self.prepare_dataset()\n        if steps is None:\n            self.steps = int(np.ceil(len(self.x_path) / float(self.batch_size)))\n        else:\n            self.steps = int(steps)\n\n    def __bool__(self):\n        return True\n\n    def __len__(self):\n        return self.steps\n\n    def __getitem__(self, idx):\n        batch_x_path = self.x_path[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = np.asarray([self.load_image(x_path) for x_path in batch_x_path])\n        batch_x = self.transform_batch_images(batch_x)\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        return batch_x, batch_y\n\n    def load_image(self, image_file):\n        image_path = os.path.join(self.source_image_dir, image_file)\n        image = Image.open(image_path)\n        image_array = np.asarray(image.convert(""RGB""))\n        image_array = image_array / 255.\n        image_array = resize(image_array, self.target_size)\n        return image_array\n\n    def transform_batch_images(self, batch_x):\n        if self.augmenter is not None:\n            batch_x = self.augmenter.augment_images(batch_x)\n        imagenet_mean = np.array([0.485, 0.456, 0.406])\n        imagenet_std = np.array([0.229, 0.224, 0.225])\n        batch_x = (batch_x - imagenet_mean) / imagenet_std\n        return batch_x\n\n    def get_y_true(self):\n        """"""\n        Use this function to get y_true for predict_generator\n        In order to get correct y, you have to set shuffle_on_epoch_end=False.\n\n        """"""\n        if self.shuffle:\n            raise ValueError(""""""\n            You\'re trying run get_y_true() when generator option \'shuffle_on_epoch_end\' is True.\n            """""")\n        return self.y[:self.steps*self.batch_size, :]\n\n    def prepare_dataset(self):\n        df = self.dataset_df.sample(frac=1., random_state=self.random_state)\n        self.x_path, self.y = df[""Image Index""].as_matrix(), df[self.class_names].as_matrix()\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            self.random_state += 1\n            self.prepare_dataset()\n'"
test.py,0,"b'import numpy as np\nimport os\nfrom configparser import ConfigParser\nfrom generator import AugmentedImageSequence\nfrom models.keras import ModelFactory\nfrom sklearn.metrics import roc_auc_score\nfrom utility import get_sample_counts\n\n\ndef main():\n    # parser config\n    config_file = ""./config.ini""\n    cp = ConfigParser()\n    cp.read(config_file)\n\n    # default config\n    output_dir = cp[""DEFAULT""].get(""output_dir"")\n    base_model_name = cp[""DEFAULT""].get(""base_model_name"")\n    class_names = cp[""DEFAULT""].get(""class_names"").split("","")\n    image_source_dir = cp[""DEFAULT""].get(""image_source_dir"")\n\n    # train config\n    image_dimension = cp[""TRAIN""].getint(""image_dimension"")\n\n    # test config\n    batch_size = cp[""TEST""].getint(""batch_size"")\n    test_steps = cp[""TEST""].get(""test_steps"")\n    use_best_weights = cp[""TEST""].getboolean(""use_best_weights"")\n\n    # parse weights file path\n    output_weights_name = cp[""TRAIN""].get(""output_weights_name"")\n    weights_path = os.path.join(output_dir, output_weights_name)\n    best_weights_path = os.path.join(output_dir, f""best_{output_weights_name}"")\n\n    # get test sample count\n    test_counts, _ = get_sample_counts(output_dir, ""test"", class_names)\n\n    # compute steps\n    if test_steps == ""auto"":\n        test_steps = int(test_counts / batch_size)\n    else:\n        try:\n            test_steps = int(test_steps)\n        except ValueError:\n            raise ValueError(f""""""\n                test_steps: {test_steps} is invalid,\n                please use \'auto\' or integer.\n                """""")\n    print(f""** test_steps: {test_steps} **"")\n\n    print(""** load model **"")\n    if use_best_weights:\n        print(""** use best weights **"")\n        model_weights_path = best_weights_path\n    else:\n        print(""** use last weights **"")\n        model_weights_path = weights_path\n    model_factory = ModelFactory()\n    model = model_factory.get_model(\n        class_names,\n        model_name=base_model_name,\n        use_base_weights=False,\n        weights_path=model_weights_path)\n\n    print(""** load test generator **"")\n    test_sequence = AugmentedImageSequence(\n        dataset_csv_file=os.path.join(output_dir, ""dev.csv""),\n        class_names=class_names,\n        source_image_dir=image_source_dir,\n        batch_size=batch_size,\n        target_size=(image_dimension, image_dimension),\n        augmenter=None,\n        steps=test_steps,\n        shuffle_on_epoch_end=False,\n    )\n\n    print(""** make prediction **"")\n    y_hat = model.predict_generator(test_sequence, verbose=1)\n    y = test_sequence.get_y_true()\n\n    test_log_path = os.path.join(output_dir, ""test.log"")\n    print(f""** write log to {test_log_path} **"")\n    aurocs = []\n    with open(test_log_path, ""w"") as f:\n        for i in range(len(class_names)):\n            try:\n                score = roc_auc_score(y[:, i], y_hat[:, i])\n                aurocs.append(score)\n            except ValueError:\n                score = 0\n            f.write(f""{class_names[i]}: {score}\\n"")\n        mean_auroc = np.mean(aurocs)\n        f.write(""-------------------------\\n"")\n        f.write(f""mean auroc: {mean_auroc}\\n"")\n        print(f""mean auroc: {mean_auroc}"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
train.py,0,"b'import json\nimport shutil\nimport os\nimport pickle\nfrom callback import MultipleClassAUROC, MultiGPUModelCheckpoint\nfrom configparser import ConfigParser\nfrom generator import AugmentedImageSequence\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras.utils import multi_gpu_model\nfrom models.keras import ModelFactory\nfrom utility import get_sample_counts\nfrom weights import get_class_weights\nfrom augmenter import augmenter\n\n\ndef main():\n    # parser config\n    config_file = ""./config.ini""\n    cp = ConfigParser()\n    cp.read(config_file)\n\n    # default config\n    output_dir = cp[""DEFAULT""].get(""output_dir"")\n    image_source_dir = cp[""DEFAULT""].get(""image_source_dir"")\n    base_model_name = cp[""DEFAULT""].get(""base_model_name"")\n    class_names = cp[""DEFAULT""].get(""class_names"").split("","")\n\n    # train config\n    use_base_model_weights = cp[""TRAIN""].getboolean(""use_base_model_weights"")\n    use_trained_model_weights = cp[""TRAIN""].getboolean(""use_trained_model_weights"")\n    use_best_weights = cp[""TRAIN""].getboolean(""use_best_weights"")\n    output_weights_name = cp[""TRAIN""].get(""output_weights_name"")\n    epochs = cp[""TRAIN""].getint(""epochs"")\n    batch_size = cp[""TRAIN""].getint(""batch_size"")\n    initial_learning_rate = cp[""TRAIN""].getfloat(""initial_learning_rate"")\n    generator_workers = cp[""TRAIN""].getint(""generator_workers"")\n    image_dimension = cp[""TRAIN""].getint(""image_dimension"")\n    train_steps = cp[""TRAIN""].get(""train_steps"")\n    patience_reduce_lr = cp[""TRAIN""].getint(""patience_reduce_lr"")\n    min_lr = cp[""TRAIN""].getfloat(""min_lr"")\n    validation_steps = cp[""TRAIN""].get(""validation_steps"")\n    positive_weights_multiply = cp[""TRAIN""].getfloat(""positive_weights_multiply"")\n    dataset_csv_dir = cp[""TRAIN""].get(""dataset_csv_dir"")\n    # if previously trained weights is used, never re-split\n    if use_trained_model_weights:\n        # resuming mode\n        print(""** use trained model weights **"")\n        # load training status for resuming\n        training_stats_file = os.path.join(output_dir, "".training_stats.json"")\n        if os.path.isfile(training_stats_file):\n            # TODO: add loading previous learning rate?\n            training_stats = json.load(open(training_stats_file))\n        else:\n            training_stats = {}\n    else:\n        # start over\n        training_stats = {}\n\n    show_model_summary = cp[""TRAIN""].getboolean(""show_model_summary"")\n    # end parser config\n\n    # check output_dir, create it if not exists\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir)\n\n    running_flag_file = os.path.join(output_dir, "".training.lock"")\n    if os.path.isfile(running_flag_file):\n        raise RuntimeError(""A process is running in this directory!!!"")\n    else:\n        open(running_flag_file, ""a"").close()\n\n    try:\n        print(f""backup config file to {output_dir}"")\n        shutil.copy(config_file, os.path.join(output_dir, os.path.split(config_file)[1]))\n\n        datasets = [""train"", ""dev"", ""test""]\n        for dataset in datasets:\n            shutil.copy(os.path.join(dataset_csv_dir, f""{dataset}.csv""), output_dir)\n\n        # get train/dev sample counts\n        train_counts, train_pos_counts = get_sample_counts(output_dir, ""train"", class_names)\n        dev_counts, _ = get_sample_counts(output_dir, ""dev"", class_names)\n\n        # compute steps\n        if train_steps == ""auto"":\n            train_steps = int(train_counts / batch_size)\n        else:\n            try:\n                train_steps = int(train_steps)\n            except ValueError:\n                raise ValueError(f""""""\n                train_steps: {train_steps} is invalid,\n                please use \'auto\' or integer.\n                """""")\n        print(f""** train_steps: {train_steps} **"")\n\n        if validation_steps == ""auto"":\n            validation_steps = int(dev_counts / batch_size)\n        else:\n            try:\n                validation_steps = int(validation_steps)\n            except ValueError:\n                raise ValueError(f""""""\n                validation_steps: {validation_steps} is invalid,\n                please use \'auto\' or integer.\n                """""")\n        print(f""** validation_steps: {validation_steps} **"")\n\n        # compute class weights\n        print(""** compute class weights from training data **"")\n        class_weights = get_class_weights(\n            train_counts,\n            train_pos_counts,\n            multiply=positive_weights_multiply,\n        )\n        print(""** class_weights **"")\n        print(class_weights)\n\n        print(""** load model **"")\n        if use_trained_model_weights:\n            if use_best_weights:\n                model_weights_file = os.path.join(output_dir, f""best_{output_weights_name}"")\n            else:\n                model_weights_file = os.path.join(output_dir, output_weights_name)\n        else:\n            model_weights_file = None\n\n        model_factory = ModelFactory()\n        model = model_factory.get_model(\n            class_names,\n            model_name=base_model_name,\n            use_base_weights=use_base_model_weights,\n            weights_path=model_weights_file,\n            input_shape=(image_dimension, image_dimension, 3))\n\n        if show_model_summary:\n            print(model.summary())\n\n        print(""** create image generators **"")\n        train_sequence = AugmentedImageSequence(\n            dataset_csv_file=os.path.join(output_dir, ""train.csv""),\n            class_names=class_names,\n            source_image_dir=image_source_dir,\n            batch_size=batch_size,\n            target_size=(image_dimension, image_dimension),\n            augmenter=augmenter,\n            steps=train_steps,\n        )\n        validation_sequence = AugmentedImageSequence(\n            dataset_csv_file=os.path.join(output_dir, ""dev.csv""),\n            class_names=class_names,\n            source_image_dir=image_source_dir,\n            batch_size=batch_size,\n            target_size=(image_dimension, image_dimension),\n            augmenter=augmenter,\n            steps=validation_steps,\n            shuffle_on_epoch_end=False,\n        )\n\n        output_weights_path = os.path.join(output_dir, output_weights_name)\n        print(f""** set output weights path to: {output_weights_path} **"")\n\n        print(""** check multiple gpu availability **"")\n        gpus = len(os.getenv(""CUDA_VISIBLE_DEVICES"", ""1"").split("",""))\n        if gpus > 1:\n            print(f""** multi_gpu_model is used! gpus={gpus} **"")\n            model_train = multi_gpu_model(model, gpus)\n            # FIXME: currently (Keras 2.1.2) checkpoint doesn\'t work with multi_gpu_model\n            checkpoint = MultiGPUModelCheckpoint(\n                filepath=output_weights_path,\n                base_model=model,\n            )\n        else:\n            model_train = model\n            checkpoint = ModelCheckpoint(\n                 output_weights_path,\n                 save_weights_only=True,\n                 save_best_only=True,\n                 verbose=1,\n            )\n\n        print(""** compile model with class weights **"")\n        optimizer = Adam(lr=initial_learning_rate)\n        model_train.compile(optimizer=optimizer, loss=""binary_crossentropy"")\n        auroc = MultipleClassAUROC(\n            sequence=validation_sequence,\n            class_names=class_names,\n            weights_path=output_weights_path,\n            stats=training_stats,\n            workers=generator_workers,\n        )\n        callbacks = [\n            checkpoint,\n            TensorBoard(log_dir=os.path.join(output_dir, ""logs""), batch_size=batch_size),\n            ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=patience_reduce_lr,\n                              verbose=1, mode=""min"", min_lr=min_lr),\n            auroc,\n        ]\n\n        print(""** start training **"")\n        history = model_train.fit_generator(\n            generator=train_sequence,\n            steps_per_epoch=train_steps,\n            epochs=epochs,\n            validation_data=validation_sequence,\n            validation_steps=validation_steps,\n            callbacks=callbacks,\n            class_weight=class_weights,\n            workers=generator_workers,\n            shuffle=False,\n        )\n\n        # dump history\n        print(""** dump history **"")\n        with open(os.path.join(output_dir, ""history.pkl""), ""wb"") as f:\n            pickle.dump({\n                ""history"": history.history,\n                ""auroc"": auroc.aurocs,\n            }, f)\n        print(""** done! **"")\n\n    finally:\n        os.remove(running_flag_file)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utility.py,0,"b'import numpy as np\nimport os\nimport pandas as pd\n\n\ndef get_sample_counts(output_dir, dataset, class_names):\n    """"""\n    Get total and class-wise positive sample count of a dataset\n\n    Arguments:\n    output_dir - str, folder of dataset.csv\n    dataset - str, train|dev|test\n    class_names - list of str, target classes\n\n    Returns:\n    total_count - int\n    class_positive_counts - dict of int, ex: {""Effusion"": 300, ""Infiltration"": 500 ...}\n    """"""\n    df = pd.read_csv(os.path.join(output_dir, f""{dataset}.csv""))\n    total_count = df.shape[0]\n    labels = df[class_names].as_matrix()\n    positive_counts = np.sum(labels, axis=0)\n    class_positive_counts = dict(zip(class_names, positive_counts))\n    return total_count, class_positive_counts\n'"
weights.py,0,"b'import numpy as np\n\n\ndef get_class_weights(total_counts, class_positive_counts, multiply):\n    """"""\n    Calculate class_weight used in training\n\n    Arguments:\n    total_counts - int\n    class_positive_counts - dict of int, ex: {""Effusion"": 300, ""Infiltration"": 500 ...}\n    multiply - int, positve weighting multiply\n    use_class_balancing - boolean \n\n    Returns:\n    class_weight - dict of dict, ex: {""Effusion"": { 0: 0.01, 1: 0.99 }, ... }\n    """"""\n    def get_single_class_weight(pos_counts, total_counts):\n        denominator = (total_counts - pos_counts) * multiply + pos_counts\n        return {\n            0: pos_counts / denominator,\n            1: (denominator - pos_counts) / denominator,\n        }\n\n    class_names = list(class_positive_counts.keys())\n    label_counts = np.array(list(class_positive_counts.values()))\n    class_weights = []\n    for i, class_name in enumerate(class_names):\n        class_weights.append(get_single_class_weight(label_counts[i], total_counts))\n\n    return class_weights\n'"
models/__init__.py,0,b''
models/keras.py,0,"b'import importlib\nfrom keras.layers import Input\nfrom keras.layers.core import Dense\nfrom keras.models import Model\n\n\nclass ModelFactory:\n    """"""\n    Model facotry for Keras default models\n    """"""\n\n    def __init__(self):\n        self.models_ = dict(\n            VGG16=dict(\n                input_shape=(224, 224, 3),\n                module_name=""vgg16"",\n                last_conv_layer=""block5_conv3"",\n            ),\n            VGG19=dict(\n                input_shape=(224, 224, 3),\n                module_name=""vgg19"",\n                last_conv_layer=""block5_conv4"",\n            ),\n            DenseNet121=dict(\n                input_shape=(224, 224, 3),\n                module_name=""densenet"",\n                last_conv_layer=""bn"",\n            ),\n            ResNet50=dict(\n                input_shape=(224, 224, 3),\n                module_name=""resnet50"",\n                last_conv_layer=""activation_49"",\n            ),\n            InceptionV3=dict(\n                input_shape=(299, 299, 3),\n                module_name=""inception_v3"",\n                last_conv_layer=""mixed10"",\n            ),\n            InceptionResNetV2=dict(\n                input_shape=(299, 299, 3),\n                module_name=""inception_resnet_v2"",\n                last_conv_layer=""conv_7b_ac"",\n            ),\n            NASNetMobile=dict(\n                input_shape=(224, 224, 3),\n                module_name=""nasnet"",\n                last_conv_layer=""activation_188"",\n            ),\n            NASNetLarge=dict(\n                input_shape=(331, 331, 3),\n                module_name=""nasnet"",\n                last_conv_layer=""activation_260"",\n            ),\n        )\n\n    def get_last_conv_layer(self, model_name):\n        return self.models_[model_name][""last_conv_layer""]\n\n    def get_input_size(self, model_name):\n        return self.models_[model_name][""input_shape""][:2]\n\n    def get_model(self, class_names, model_name=""DenseNet121"", use_base_weights=True,\n                  weights_path=None, input_shape=None):\n\n        if use_base_weights is True:\n            base_weights = ""imagenet""\n        else:\n            base_weights = None\n\n        base_model_class = getattr(\n            importlib.import_module(\n                f""keras.applications.{self.models_[model_name][\'module_name\']}""\n            ),\n            model_name)\n\n        if input_shape is None:\n            input_shape = self.models_[model_name][""input_shape""]\n\n        img_input = Input(shape=input_shape)\n\n        base_model = base_model_class(\n            include_top=False,\n            input_tensor=img_input,\n            input_shape=input_shape,\n            weights=base_weights,\n            pooling=""avg"")\n        x = base_model.output\n        predictions = Dense(len(class_names), activation=""sigmoid"", name=""predictions"")(x)\n        model = Model(inputs=img_input, outputs=predictions)\n\n        if weights_path == """":\n            weights_path = None\n\n        if weights_path is not None:\n            print(f""load model weights_path: {weights_path}"")\n            model.load_weights(weights_path)\n        return model\n'"
