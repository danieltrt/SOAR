file_path,api_count,code
settings.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\n\nPROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))'"
chatbot/botpredictor.py,3,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport nltk\nimport os\nimport string\nimport tensorflow as tf\n\nfrom chatbot.tokenizeddata import TokenizedData\nfrom chatbot.modelcreator import ModelCreator\nfrom chatbot.knowledgebase import KnowledgeBase\nfrom chatbot.sessiondata import SessionData\nfrom chatbot.patternutils import check_patterns_and_replace\nfrom chatbot.functiondata import call_function\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass BotPredictor(object):\n    def __init__(self, session, corpus_dir, knbase_dir, result_dir, result_file):\n        """"""\n        Args:\n            session: The TensorFlow session.\n            corpus_dir: Name of the folder storing corpus files and vocab information.\n            knbase_dir: Name of the folder storing data files for the knowledge base.\n            result_dir: The folder containing the trained result files.\n            result_file: The file name of the trained model.\n        """"""\n        self.session = session\n\n        # Prepare data and hyper parameters\n        print(""# Prepare dataset placeholder and hyper parameters ..."")\n        tokenized_data = TokenizedData(corpus_dir=corpus_dir, training=False)\n\n        self.knowledge_base = KnowledgeBase()\n        self.knowledge_base.load_knbase(knbase_dir)\n\n        self.session_data = SessionData()\n\n        self.hparams = tokenized_data.hparams\n        self.src_placeholder = tf.placeholder(shape=[None], dtype=tf.string)\n        src_dataset = tf.data.Dataset.from_tensor_slices(self.src_placeholder)\n        self.infer_batch = tokenized_data.get_inference_batch(src_dataset)\n\n        # Create model\n        print(""# Creating inference model ..."")\n        self.model = ModelCreator(training=False, tokenized_data=tokenized_data,\n                                  batch_input=self.infer_batch)\n        # Restore model weights\n        print(""# Restoring model weights ..."")\n        self.model.saver.restore(session, os.path.join(result_dir, result_file))\n\n        self.session.run(tf.tables_initializer())\n\n    def predict(self, session_id, question):\n        chat_session = self.session_data.get_session(session_id)\n        chat_session.before_prediction()  # Reset before each prediction\n\n        if question.strip() == \'\':\n            answer = ""Don\'t you want to say something to me?""\n            chat_session.after_prediction(question, answer)\n            return answer\n\n        pat_matched, new_sentence, para_list = check_patterns_and_replace(question)\n\n        for pre_time in range(2):\n            tokens = nltk.word_tokenize(new_sentence.lower())\n            tmp_sentence = [\' \'.join(tokens[:]).strip()]\n\n            self.session.run(self.infer_batch.initializer,\n                             feed_dict={self.src_placeholder: tmp_sentence})\n\n            outputs, _ = self.model.infer(self.session)\n\n            if self.hparams.beam_width > 0:\n                outputs = outputs[0]\n\n            eos_token = self.hparams.eos_token.encode(""utf-8"")\n            outputs = outputs.tolist()[0]\n\n            if eos_token in outputs:\n                outputs = outputs[:outputs.index(eos_token)]\n\n            if pat_matched and pre_time == 0:\n                out_sentence, if_func_val = self._get_final_output(outputs, chat_session,\n                                                                   para_list=para_list)\n                if if_func_val:\n                    chat_session.after_prediction(question, out_sentence)\n                    return out_sentence\n                else:\n                    new_sentence = question\n            else:\n                out_sentence, _ = self._get_final_output(outputs, chat_session)\n                chat_session.after_prediction(question, out_sentence)\n                return out_sentence\n\n    def _get_final_output(self, sentence, chat_session, para_list=None):\n        sentence = b\' \'.join(sentence).decode(\'utf-8\')\n        if sentence == \'\':\n            return ""I don\'t know what to say."", False\n\n        if_func_val = False\n        last_word = None\n        word_list = []\n        for word in sentence.split(\' \'):\n            word = word.strip()\n            if not word:\n                continue\n\n            if word.startswith(\'_func_val_\'):\n                if_func_val = True\n                word = call_function(word[10:], knowledge_base=self.knowledge_base,\n                                     chat_session=chat_session, para_list=para_list)\n                if word is None or word == \'\':\n                    continue\n            else:\n                if word in self.knowledge_base.upper_words:\n                    word = self.knowledge_base.upper_words[word]\n\n                if (last_word is None or last_word in [\'.\', \'!\', \'?\']) and not word[0].isupper():\n                    word = word.capitalize()\n\n            if not word.startswith(\'\\\'\') and word != \'n\\\'t\' \\\n                and (word[0] not in string.punctuation or word in [\'(\', \'[\', \'{\', \'``\', \'$\']) \\\n                and last_word not in [\'(\', \'[\', \'{\', \'``\', \'$\']:\n                word = \' \' + word\n\n            word_list.append(word)\n            last_word = word\n\n        return \'\'.join(word_list).strip(), if_func_val\n'"
chatbot/bottrainer.py,8,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport math\nimport os\nimport time\n\nimport tensorflow as tf\nfrom chatbot.tokenizeddata import TokenizedData\nfrom chatbot.modelcreator import ModelCreator\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass BotTrainer(object):\n    def __init__(self, corpus_dir):\n        """"""\n        Constructor of the BotTrainer.\n        Args:\n            corpus_dir: The folder to save all the training related data.\n        """"""\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            tokenized_data = TokenizedData(corpus_dir=corpus_dir)\n\n            self.hparams = tokenized_data.hparams\n            self.train_batch = tokenized_data.get_training_batch()\n            self.model = ModelCreator(training=True, tokenized_data=tokenized_data,\n                                      batch_input=self.train_batch)\n\n    def train(self, result_dir, target="""", last_end_file=None, last_end_epoch=0, last_end_lr=8e-4):\n        """"""Train a seq2seq model.""""""\n        # Summary writer\n        summary_name = ""train_log""\n        summary_writer = tf.summary.FileWriter(os.path.join(result_dir, summary_name), self.graph)\n\n        log_device_placement = self.hparams.log_device_placement\n        num_epochs = self.hparams.num_epochs\n\n        config_proto = tf.ConfigProto(log_device_placement=log_device_placement,\n                                      allow_soft_placement=True)\n        config_proto.gpu_options.allow_growth = True\n\n        with tf.Session(target=target, config=config_proto, graph=self.graph) as sess:\n            # This initialization is useful even when the model is restored from the last time\n            # because not all variables used in the model training may be saved.\n            sess.run(tf.global_variables_initializer())\n            if last_end_file:  # Continue training from last time\n                print(""# Restoring model weights from last time ..."")\n                self.model.saver.restore(sess, os.path.join(result_dir, last_end_file))\n\n            sess.run(tf.tables_initializer())\n            global_step = self.model.global_step.eval(session=sess)\n\n            # Initialize all of the iterators\n            sess.run(self.train_batch.initializer)\n\n            # Initialize the statistic variables\n            ckpt_loss, ckpt_predict_count = 0.0, 0.0\n            train_perp, last_record_perp = 2000.0, 200.0\n            train_epoch = last_end_epoch\n            learning_rate = pre_lr = last_end_lr\n\n            print(""# Training loop started @ {}"".format(time.strftime(""%Y-%m-%d %H:%M:%S"")))\n            epoch_start_time = time.time()\n            while train_epoch < num_epochs:\n                # Each run of this while loop is a training step, multiple time/steps will trigger\n                # the train_epoch to be increased.\n                try:\n                    step_result = self.model.train_step(sess, learning_rate=learning_rate)\n                    (_, step_loss, step_predict_count, step_summary, global_step,\n                     step_word_count, batch_size) = step_result\n\n                    # Write step summary.\n                    summary_writer.add_summary(step_summary, global_step)\n\n                    # update statistics\n                    ckpt_loss += (step_loss * batch_size)\n                    ckpt_predict_count += step_predict_count\n                except tf.errors.OutOfRangeError:\n                    # Finished going through the training dataset. Go to next epoch.\n                    train_epoch += 1\n\n                    mean_loss = ckpt_loss / ckpt_predict_count\n                    train_perp = math.exp(float(mean_loss)) if mean_loss < 300 else math.inf\n\n                    epoch_dur = time.time() - epoch_start_time\n                    print(""# Finished epoch {:2d} @ step {:5d} @ {}. In the epoch, learning rate = {:.6f}, ""\n                          ""mean loss = {:.4f}, perplexity = {:8.4f}, and {:.2f} seconds elapsed.""\n                          .format(train_epoch, global_step, time.strftime(""%Y-%m-%d %H:%M:%S""),\n                                  learning_rate, mean_loss, train_perp, round(epoch_dur, 2)))\n                    epoch_start_time = time.time()  # The start time of the next epoch\n\n                    summary = tf.Summary(value=[tf.Summary.Value(tag=""train_perp"", simple_value=train_perp)])\n                    summary_writer.add_summary(summary, global_step)\n\n                    # Save checkpoint\n                    if train_perp < last_record_perp:\n                        self.model.saver.save(sess, os.path.join(result_dir, ""basic""), global_step=train_epoch)\n                        last_record_perp = train_perp\n\n                    ckpt_loss, ckpt_predict_count = 0.0, 0.0\n\n                    learning_rate = self._get_learning_rate(train_perp, pre_lr, train_epoch)\n                    pre_lr = learning_rate\n\n                    sess.run(self.model.batch_input.initializer)\n                    continue\n\n            # Done training\n            self.model.saver.save(sess, os.path.join(result_dir, ""basic""), global_step=train_epoch)\n            summary_writer.close()\n\n    @staticmethod\n    def _get_learning_rate(perplexity, pre_lr, train_epoch):\n        # Check the number of epochs used to reach the perplexity of 32 and then 16. If it takes\n        # too many or too less, it may suggest that the model is too small or too big compared\n        # with your training data (and vocab size, sequence length, etc.)\n        new_lr = round(pre_lr * 0.96, 6)\n        if train_epoch >= 55:\n            return 9.6e-5\n        elif train_epoch >= 50:\n            return 1e-4\n        elif perplexity <= 16.0:\n            return max(min(new_lr, 4e-4), 1e-4)\n        elif perplexity <= 32.0:\n            return max(min(new_lr, 6e-4), 4e-4)\n        else:\n            return max(min(new_lr, 8e-4), 6e-4)\n\n\nif __name__ == ""__main__"":\n    from settings import PROJECT_ROOT\n\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    res_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Result\')\n    bt = BotTrainer(corpus_dir=corp_dir)\n    # bt.train(res_dir, last_end_file=\'basic-50\', last_end_epoch=50, last_end_lr=1e-4)\n    bt.train(res_dir)\n'"
chatbot/botui.py,1,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport re\nimport sys\nimport tensorflow as tf\n\nfrom settings import PROJECT_ROOT\nfrom chatbot.botpredictor import BotPredictor\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef bot_ui():\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    knbs_dir = os.path.join(PROJECT_ROOT, \'Data\', \'KnowledgeBase\')\n    res_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Result\')\n\n    with tf.Session() as sess:\n        predictor = BotPredictor(sess, corpus_dir=corp_dir, knbase_dir=knbs_dir,\n                                 result_dir=res_dir, result_file=\'basic\')\n        # This command UI has a single chat session only\n        session_id = predictor.session_data.add_session()\n\n        print(""Welcome to Chat with ChatLearner!"")\n        print(""Type exit and press enter to end the conversation."")\n        # Waiting from standard input.\n        sys.stdout.write(""> "")\n        sys.stdout.flush()\n        question = sys.stdin.readline()\n        while question:\n            if question.strip() == \'exit\':\n                print(""Thank you for using ChatLearner. Goodbye."")\n                break\n\n            print(re.sub(r\'_nl_|_np_\', \'\\n\', predictor.predict(session_id, question)).strip())\n            print(""> "", end="""")\n            sys.stdout.flush()\n            question = sys.stdin.readline()\n\nif __name__ == ""__main__"":\n    bot_ui()\n'"
chatbot/functiondata.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This class is only used at inference time.""""""\nimport calendar as cal\nimport datetime as dt\nimport random\nimport re\nimport time\n\n\nclass FunctionData:\n    easy_list = [\n        """", """",\n        ""Here you are: "",\n        ""Here is the result: "",\n        ""That\'s easy: "",\n        ""That was an easy one: "",\n        ""It was a piece of cake: "",\n        ""That\'s simple, and I know how to solve it: "",\n        ""That wasn\'t hard. Here is the result: "",\n        ""Oh, I know how to deal with this: ""\n    ]\n    hard_list = [\n        """", """",\n        ""Here you are: "",\n        ""Here is the result: "",\n        ""That\'s a little hard: "",\n        ""That was an tough one, and I had to use a calculator: "",\n        ""That\'s a little difficult, but I know how to solve it: "",\n        ""It was hard and took me a little while to figure it out. Here is the result: "",\n        ""It took me a little while, and finally I got the result: "",\n        ""I had to use my cell phone for this calculation. Here is the outcome: ""\n    ]\n    ask_howru_list = [\n        ""And you?"",\n        ""How are you?"",\n        ""How about yourself?""\n    ]\n    ask_name_list = [\n        ""May I also have your name, please?"",\n        ""Would you also like to tell me your name, please?"",\n        ""And, how should I call you, please?"",\n        ""And, what do you want me to call you, dear sir or madam?""\n    ]\n\n    def __init__(self, knowledge_base, chat_session):\n        """"""\n        Args:\n            knowledge_base: The knowledge base data needed for prediction.\n            chat_session: The chat session object that can be read and written.\n        """"""\n        self.knowledge_base = knowledge_base\n        self.chat_session = chat_session\n\n    """"""\n    # Rule 2: Date and Time\n    """"""\n    @staticmethod\n    def get_date_time():\n        return time.strftime(""%Y-%m-%d %H:%M"")\n\n    @staticmethod\n    def get_time():\n        return time.strftime(""%I:%M %p"")\n\n    @staticmethod\n    def get_today():\n        return ""{:%B %d, %Y}"".format(dt.date.today())\n\n    @staticmethod\n    def get_weekday(day_delta):\n        now = dt.datetime.now()\n        if day_delta == \'d_2\':\n            day_time = now - dt.timedelta(days=2)\n        elif day_delta == \'d_1\':\n            day_time = now - dt.timedelta(days=1)\n        elif day_delta == \'d1\':\n            day_time = now + dt.timedelta(days=1)\n        elif day_delta == \'d2\':\n            day_time = now + dt.timedelta(days=2)\n        else:\n            day_time = now\n\n        weekday = cal.day_name[day_time.weekday()]\n        return ""{}, {:%B %d, %Y}"".format(weekday, day_time)\n\n    """"""\n    # Rule 3: Stories and Jokes, and last topic\n    """"""\n    def get_story_any(self):\n        self.chat_session.last_topic = ""STORY""\n        self.chat_session.keep_topic = True\n\n        stories = self.knowledge_base.stories\n        _, content = random.choice(list(stories.items()))\n        return content\n\n    def get_story_name(self, story_name):\n        self.chat_session.last_topic = ""STORY""\n        self.chat_session.keep_topic = True\n\n        stories = self.knowledge_base.stories\n        content = stories[story_name]\n        return content\n\n    def get_joke_any(self):\n        self.chat_session.last_topic = ""JOKE""\n        self.chat_session.keep_topic = True\n\n        jokes = self.knowledge_base.jokes\n        content = random.choice(jokes)\n        return content\n\n    def continue_last_topic(self):\n        if self.chat_session.last_topic == ""STORY"":\n            self.chat_session.keep_topic = True\n            return self.get_story_any()\n        elif self.chat_session.last_topic == ""JOKE"":\n            self.chat_session.keep_topic = True\n            return self.get_joke_any()\n        else:\n            return ""Sorry, but what topic do you prefer?""\n\n    """"""\n    # Rule 4: Arithmetic ops\n    """"""\n    @staticmethod\n    def get_number_plus(num1, num2):\n        res = num1 + num2\n        desc = random.choice(FunctionData.easy_list)\n        return ""{}{} + {} = {}"".format(desc, num1, num2, res)\n\n    @staticmethod\n    def get_number_minus(num1, num2):\n        res = num1 - num2\n        desc = random.choice(FunctionData.easy_list)\n        return ""{}{} - {} = {}"".format(desc, num1, num2, res)\n\n    @staticmethod\n    def get_number_multiply(num1, num2):\n        res = num1 * num2\n        if num1 > 100 and num2 > 100 and num1 % 2 == 1 and num2 % 2 == 1:\n            desc = random.choice(FunctionData.hard_list)\n        else:\n            desc = random.choice(FunctionData.easy_list)\n        return ""{}{} * {} = {}"".format(desc, num1, num2, res)\n\n    @staticmethod\n    def get_number_divide(num1, num2):\n        if num2 == 0:\n            return ""Sorry, but that does not make sense as the divisor cannot be zero.""\n        else:\n            res = num1 / num2\n            if isinstance(res, int):\n                if 50 < num1 != num2 > 50:\n                    desc = random.choice(FunctionData.hard_list)\n                else:\n                    desc = random.choice(FunctionData.easy_list)\n                return ""{}{} / {} = {}"".format(desc, num1, num2, res)\n            else:\n                if num1 > 20 and num2 > 20:\n                    desc = random.choice(FunctionData.hard_list)\n                else:\n                    desc = random.choice(FunctionData.easy_list)\n                return ""{}{} / {} = {:.2f}"".format(desc, num1, num2, res)\n\n    """"""\n    # Rule 5: User name, call me information, and last question and answer\n    """"""\n    def ask_howru_if_not_yet(self):\n        howru_asked = self.chat_session.howru_asked\n        if howru_asked:\n            return """"\n        else:\n            self.chat_session.howru_asked = True\n            return random.choice(FunctionData.ask_howru_list)\n\n    def ask_name_if_not_yet(self):\n        user_name = self.chat_session.user_name\n        call_me = self.chat_session.call_me\n        if user_name or call_me:\n            return """"\n        else:\n            return random.choice(FunctionData.ask_name_list)\n\n    def get_user_name_and_reply(self):\n        user_name = self.chat_session.user_name\n        if user_name and user_name.strip() != \'\':\n            return user_name\n        else:\n            return ""Did you tell me your name? Sorry, I missed that.""\n\n    def get_callme(self, punc_type):\n        call_me = self.chat_session.call_me\n        user_name = self.chat_session.user_name\n\n        if call_me and call_me.strip() != \'\':\n            if punc_type == \'comma0\':\n                return "", {}"".format(call_me)\n            else:\n                return call_me\n        elif user_name and user_name.strip() != \'\':\n            if punc_type == \'comma0\':\n                return "", {}"".format(user_name)\n            else:\n                return user_name\n        else:\n            return """"\n\n    def get_last_question(self):\n        # Do not record this pair as the last question and answer\n        self.chat_session.update_pair = False\n\n        last_question = self.chat_session.last_question\n        if last_question is None or last_question.strip() == \'\':\n            return ""You did not say anything.""\n        else:\n            return ""You have just said: {}"".format(last_question)\n\n    def get_last_answer(self):\n        # Do not record this pair as the last question and answer\n        self.chat_session.update_pair = False\n\n        last_answer = self.chat_session.last_answer\n        if last_answer is None or last_answer.strip() == \'\':\n            return ""I did not say anything.""\n        else:\n            return ""I have just said: {}"".format(last_answer)\n\n    def update_user_name(self, new_name):\n        return self.update_user_name_and_call_me(new_name=new_name)\n\n    def update_call_me(self, new_call):\n        return self.update_user_name_and_call_me(new_call=new_call)\n\n    def update_user_name_and_call_me(self, new_name=None, new_call=None):\n        user_name = self.chat_session.user_name\n        call_me = self.chat_session.call_me\n        # print(""{}; {}; {}; {}"".format(user_name, call_me, new_name, new_call))\n\n        if user_name and new_name and new_name.strip() != \'\':\n            if new_name.lower() != user_name.lower():\n                self.chat_session.update_pending_action(\'update_user_name_confirmed\', None, new_name)\n                return ""I am confused. I have your name as {}. Did I get it correctly?"".format(user_name)\n            else:\n                return ""You told me your name already. Thank you, {}, for assuring me."".format(user_name)\n\n        if call_me and new_call and new_call.strip() != \'\':\n            if new_call.lower() != call_me.lower():\n                self.chat_session.update_pending_action(\'update_call_me_confirmed\', new_call, None)\n                return ""You wanted me to call you {}. Would you like me to call you {} now?""\\\n                    .format(call_me, new_call)\n            else:\n                return ""Thank you for letting me again, {}."".format(call_me)\n\n        if new_call and new_call.strip() != \'\':\n            if new_name and new_name.strip() != \'\':\n                self.chat_session.user_name = new_name\n\n            self.chat_session.call_me = new_call\n            return ""Thank you, {}."".format(new_call)\n        elif new_name and new_name.strip() != \'\':\n            self.chat_session.user_name = new_name\n            return ""Thank you, {}."".format(new_name)\n\n        return ""Sorry, I am confused. I could not figure out what you meant.""\n\n    def update_user_name_enforced(self, new_name):\n        if new_name and new_name.strip() != \'\':\n            self.chat_session.user_name = new_name\n            return ""OK, thank you, {}."".format(new_name)\n        else:\n            self.chat_session.user_name = None  # Clear the existing user_name, if any.\n            return ""Sorry, I am lost.""\n\n    def update_call_me_enforced(self, new_call):\n        if new_call and new_call.strip() != \'\':\n            self.chat_session.call_me = new_call\n            return ""OK, got it. Thank you, {}."".format(new_call)\n        else:\n            self.chat_session.call_me = None  # Clear the existing call_me, if any.\n            return ""Sorry, I am totally lost.""\n\n    def update_user_name_and_reply_papaya(self, new_name):\n        user_name = self.chat_session.user_name\n\n        if new_name and new_name.strip() != \'\':\n            if user_name:\n                if new_name.lower() != user_name.lower():\n                    self.chat_session.update_pending_action(\'update_user_name_confirmed\', None, new_name)\n                    return ""I am confused. I have your name as {}. Did I get it correctly?"".format(user_name)\n                else:\n                    return ""Thank you, {}, for assuring me your name. My name is Papaya."".format(user_name)\n            else:\n                self.chat_session.user_name = new_name\n                return ""Thank you, {}. BTW, my name is Papaya."".format(new_name)\n        else:\n            return ""My name is Papaya. Thanks.""\n\n    def correct_user_name(self, new_name):\n        if new_name and new_name.strip() != \'\':\n            self.chat_session.user_name = new_name\n            return ""Thank you, {}."".format(new_name)\n        else:\n            # Clear the existing user_name and call_me information\n            self.chat_session.user_name = None\n            self.chat_session.call_me = None\n            return ""I am totally lost.""\n\n    def clear_user_name_and_call_me(self):\n        self.chat_session.user_name = None\n        self.chat_session.call_me = None\n\n    def execute_pending_action_and_reply(self, answer):\n        func = self.chat_session.pending_action[\'func\']\n        if func == \'update_user_name_confirmed\':\n            if answer.lower() == \'yes\':\n                reply = ""Thank you, {}, for confirming this."".format(self.chat_session.user_name)\n            else:\n                new_name = self.chat_session.pending_action[\'No\']\n                self.chat_session.user_name = new_name\n                reply = ""Thank you, {}, for correcting me."".format(new_name)\n        elif func == \'update_call_me_confirmed\':\n            if answer.lower() == \'yes\':\n                new_call = self.chat_session.pending_action[\'Yes\']\n                self.chat_session.call_me = new_call\n                reply = ""Thank you, {}, for correcting me."".format(new_call)\n            else:\n                reply = ""Thank you. I will continue to call you {}."".format(self.chat_session.call_me)\n        else:\n            reply = ""OK, thanks.""  # Just presents a reply that is good for most situations\n\n        # Clear the pending action anyway\n        self.chat_session.clear_pending_action()\n        return reply\n\n    """"""\n    # Other Rules: Client Code\n    """"""\n    def client_code_show_picture_randomly(self, picture_name):\n        return \' _cc_start_show_picture_randomly_para1_\' + picture_name + \'_cc_end_\'\n\n\ndef call_function(func_info, knowledge_base=None, chat_session=None, para_list=None):\n    func_data = FunctionData(knowledge_base, chat_session)\n\n    func_dict = {\n        \'get_date_time\': FunctionData.get_date_time,\n        \'get_time\': FunctionData.get_time,\n        \'get_today\': FunctionData.get_today,\n        \'get_weekday\': FunctionData.get_weekday,\n\n        \'get_story_any\': func_data.get_story_any,\n        \'get_story_name\': func_data.get_story_name,\n        \'get_joke_any\': func_data.get_joke_any,\n        \'continue_last_topic\': func_data.continue_last_topic,\n\n        \'get_number_plus\': FunctionData.get_number_plus,\n        \'get_number_minus\': FunctionData.get_number_minus,\n        \'get_number_multiply\': FunctionData.get_number_multiply,\n        \'get_number_divide\': FunctionData.get_number_divide,\n\n        \'ask_howru_if_not_yet\': func_data.ask_howru_if_not_yet,\n        \'ask_name_if_not_yet\': func_data.ask_name_if_not_yet,\n        \'get_user_name_and_reply\': func_data.get_user_name_and_reply,\n        \'get_callme\': func_data.get_callme,\n        \'get_last_question\': func_data.get_last_question,\n        \'get_last_answer\': func_data.get_last_answer,\n\n        \'update_user_name\': func_data.update_user_name,\n        \'update_call_me\': func_data.update_call_me,\n        \'update_user_name_and_call_me\': func_data.update_user_name_and_call_me,\n        \'update_user_name_enforced\': func_data.update_user_name_enforced,\n        \'update_call_me_enforced\': func_data.update_call_me_enforced,\n        \'update_user_name_and_reply_papaya\': func_data.update_user_name_and_reply_papaya,\n\n        \'correct_user_name\': func_data.correct_user_name,\n        \'clear_user_name_and_call_me\': func_data.clear_user_name_and_call_me,\n\n        \'execute_pending_action_and_reply\': func_data.execute_pending_action_and_reply,\n\n        \'client_code_show_picture_randomly\': func_data.client_code_show_picture_randomly\n    }\n\n    para1_index = func_info.find(\'_para1_\')\n    para2_index = func_info.find(\'_para2_\')\n    if para1_index == -1:  # No parameter at all\n        func_name = func_info\n        if func_name in func_dict:\n            return func_dict[func_name]()\n    else:\n        func_name = func_info[:para1_index]\n        if para2_index == -1:  # Only one parameter\n            func_para = func_info[para1_index+7:]\n            if func_para == \'_name_\' and para_list is not None and len(para_list) >= 1:\n                return func_dict[func_name](para_list[0])\n            elif func_para == \'_callme_\' and para_list is not None and len(para_list) >= 2:\n                return func_dict[func_name](para_list[1])\n            else:  # The parameter value was embedded in the text (part of the string) of the training example.\n                return func_dict[func_name](func_para)\n        else:\n            func_para1 = func_info[para1_index+7:para2_index]\n            func_para2 = func_info[para2_index+7:]\n            if para_list is not None and len(para_list) >= 2:\n                para1_val = para_list[0]\n                para2_val = para_list[1]\n\n                if func_para1 == \'_num1_\' and func_para2 == \'_num2_\':\n                    return func_dict[func_name](para1_val, para2_val)\n                elif func_para1 == \'_num2_\' and func_para2 == \'_num1_\':\n                    return func_dict[func_name](para2_val, para1_val)\n                elif func_para1 == \'_name_\' and func_para2 == \'_callme_\':\n                    return func_dict[func_name](para1_val, para2_val)\n\n    return ""You beat me to it, and I cannot tell which is which for this question.""\n\n\n# if __name__ == ""__main__"":\n#     import os\n#     from settings import PROJECT_ROOT\n#     from chatbot.knowledgebase import KnowledgeBase\n#     from chatbot.sessiondata import ChatSession\n#\n#     knbs = KnowledgeBase()\n#     knbs.load_knbase(os.path.join(PROJECT_ROOT, \'Data\', \'KnowledgeBase\'))\n#\n#     cs = ChatSession(1)\n#\n#     print(call_function(\'get_story_any\', knbs, cs))\n#     print(call_function(\'get_story_any\', knbs, cs))\n#     print(call_function(\'get_joke_any\', knbs, cs))\n#     print(call_function(\'get_joke_any\', knbs, cs))\n#     print(call_function(\'get_weekday_para1_d_2\'))\n#     print(call_function(\'get_weekday_para1_d_1\'))\n#     print(call_function(\'get_weekday_para1_d0\'))\n#     print(call_function(\'get_weekday_para1_d1\'))\n#     print(call_function(\'get_weekday_para1_d2\'))\n'"
chatbot/hparams.py,3,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport codecs\nimport json\nimport os\nimport tensorflow as tf\n\n\nclass HParams:\n    def __init__(self, model_dir):\n        """"""\n        Args:\n            model_dir: Name of the folder storing the hparams.json file.\n        """"""\n        self.hparams = self.load_hparams(model_dir)\n\n    @staticmethod\n    def load_hparams(model_dir):\n        """"""Load hparams from an existing directory.""""""\n        hparams_file = os.path.join(model_dir, ""hparams.json"")\n        if tf.gfile.Exists(hparams_file):\n            print(""# Loading hparams from {} ..."".format(hparams_file))\n            with codecs.getreader(""utf-8"")(tf.gfile.GFile(hparams_file, ""rb"")) as f:\n                try:\n                    hparams_values = json.load(f)\n                    hparams = tf.contrib.training.HParams(**hparams_values)\n                except ValueError:\n                    print(""Error loading hparams file."")\n                    return None\n            return hparams\n        else:\n            return None\n'"
chatbot/knowledgebase.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This class is only used at inference time.""""""\nimport os\n\nUPPER_FILE = ""upper_words.txt""\nSTORIES_FILE = ""stories.txt""\nJOKES_FILE = ""jokes.txt""\n\n\nclass KnowledgeBase:\n    def __init__(self):\n        self.upper_words = {}\n        self.stories = {}\n        self.jokes = []\n\n    def load_knbase(self, knbase_dir):\n        """"""\n        Args:\n             knbase_dir: Name of the KnowledgeBase folder. The file names inside are fixed.\n        """"""\n        upper_file_name = os.path.join(knbase_dir, UPPER_FILE)\n        stories_file_name = os.path.join(knbase_dir, STORIES_FILE)\n        jokes_file_name = os.path.join(knbase_dir, JOKES_FILE)\n\n        with open(upper_file_name, \'r\') as upper_f:\n            for line in upper_f:\n                ln = line.strip()\n                if not ln or ln.startswith(\'#\'):\n                    continue\n                cap_words = ln.split(\',\')\n                for cpw in cap_words:\n                    tmp = cpw.strip()\n                    self.upper_words[tmp.lower()] = tmp\n\n        with open(stories_file_name, \'r\') as stories_f:\n            s_name, s_content = \'\', \'\'\n            for line in stories_f:\n                ln = line.strip()\n                if not ln or ln.startswith(\'#\'):\n                    continue\n                if ln.startswith(\'_NAME:\'):\n                    if s_name != \'\' and s_content != \'\':\n                        self.stories[s_name] = s_content\n                        s_name, s_content = \'\', \'\'\n                    s_name = ln[6:].strip().lower()\n                elif ln.startswith(\'_CONTENT:\'):\n                    s_content = ln[9:].strip()\n                else:\n                    s_content += \' \' + ln.strip()\n\n            if s_name != \'\' and s_content != \'\':  # The last one\n                self.stories[s_name] = s_content\n\n        with open(jokes_file_name, \'r\') as jokes_f:\n            for line in jokes_f:\n                ln = line.strip()\n                if not ln or ln.startswith(\'#\'):\n                    continue\n                self.jokes.append(ln)'"
chatbot/modelcreator.py,54,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport tensorflow as tf\nimport chatbot.modelhelper as model_helper\n\nfrom tensorflow.python.layers import core as layers_core\n\n\nclass ModelCreator(object):\n    """"""Sequence-to-sequence model creator to create models for training or inference""""""\n    def __init__(self, training, tokenized_data, batch_input, scope=None):\n        """"""\n        Create the model.\n\n        Args:\n            training: A boolean value to indicate whether this model will be used for training.\n            tokenized_data: The data object containing all information required for the model.\n            scope: scope of the model.\n        """"""\n        self.training = training\n        self.batch_input = batch_input\n        self.vocab_table = tokenized_data.vocab_table\n        self.vocab_size = tokenized_data.vocab_size\n        self.reverse_vocab_table = tokenized_data.reverse_vocab_table\n\n        hparams = tokenized_data.hparams\n        self.hparams = hparams\n\n        self.num_layers = hparams.num_layers\n        self.time_major = hparams.time_major\n\n        # Initializer\n        initializer = model_helper.get_initializer(\n            hparams.init_op, hparams.random_seed, hparams.init_weight)\n        tf.get_variable_scope().set_initializer(initializer)\n\n        # Embeddings\n        self.embedding = (model_helper.create_embbeding(vocab_size=self.vocab_size,\n                                                        embed_size=hparams.num_units,\n                                                        scope=scope))\n        # This batch_size might vary among each batch instance due to the bucketing and/or reach\n        # the end of the training set. Treat it as size_of_the_batch.\n        self.batch_size = tf.size(self.batch_input.source_sequence_length)\n\n        # Projection\n        with tf.variable_scope(scope or ""build_network""):\n            with tf.variable_scope(""decoder/output_projection""):\n                self.output_layer = layers_core.Dense(\n                    self.vocab_size, use_bias=False, name=""output_projection"")\n\n        # Training or inference graph\n        print(""# Building graph for the model ..."")\n        res = self.build_graph(hparams, scope=scope)\n\n        if training:\n            self.train_loss = res[1]\n            self.word_count = tf.reduce_sum(self.batch_input.source_sequence_length) + \\\n                              tf.reduce_sum(self.batch_input.target_sequence_length)\n            # Count the number of predicted words for compute perplexity.\n            self.predict_count = tf.reduce_sum(self.batch_input.target_sequence_length)\n        else:\n            self.infer_logits, _, self.final_context_state, self.sample_id = res\n            self.sample_words = self.reverse_vocab_table.lookup(tf.to_int64(self.sample_id))\n\n        self.global_step = tf.Variable(0, trainable=False)\n\n        params = tf.trainable_variables()\n\n        # Gradients update operation for training the model.\n        if training:\n            self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\'learning_rate\')\n            opt = tf.train.AdamOptimizer(self.learning_rate)\n\n            gradients = tf.gradients(self.train_loss, params)\n\n            clipped_gradients, gradient_norm_summary = model_helper.gradient_clip(\n                gradients, max_gradient_norm=hparams.max_gradient_norm)\n\n            self.update = opt.apply_gradients(\n                zip(clipped_gradients, params), global_step=self.global_step)\n\n            # Summary\n            self.train_summary = tf.summary.merge([\n                tf.summary.scalar(""learning_rate"", self.learning_rate),\n                tf.summary.scalar(""train_loss"", self.train_loss),\n            ] + gradient_norm_summary)\n        else:\n            self.infer_summary = tf.no_op()\n\n        # Saver\n        self.saver = tf.train.Saver(tf.global_variables())\n\n        # Print trainable variables\n        if training:\n            print(""# Trainable variables:"")\n            for param in params:\n                print(""  {}, {}, {}"".format(param.name, str(param.get_shape()), param.op.device))\n\n    def train_step(self, sess, learning_rate):\n        """"""Run one step of training.""""""\n        assert self.training\n\n        return sess.run([self.update,\n                         self.train_loss,\n                         self.predict_count,\n                         self.train_summary,\n                         self.global_step,\n                         self.word_count,\n                         self.batch_size],\n                        feed_dict={self.learning_rate: learning_rate})\n\n    def build_graph(self, hparams, scope=None):\n        """"""Creates a sequence-to-sequence model with dynamic RNN decoder API.""""""\n        dtype = tf.float32\n\n        with tf.variable_scope(scope or ""dynamic_seq2seq"", dtype=dtype):\n            # Encoder\n            encoder_outputs, encoder_state = self._build_encoder(hparams)\n\n            # Decoder\n            logits, sample_id, final_context_state = self._build_decoder(\n                encoder_outputs, encoder_state, hparams)\n\n            # Loss\n            if self.training:\n                loss = self._compute_loss(logits)\n            else:\n                loss = None\n\n            return logits, loss, final_context_state, sample_id\n\n    def _build_encoder(self, hparams):\n        """"""Build an encoder.""""""\n        source = self.batch_input.source\n        if self.time_major:\n            source = tf.transpose(source)\n\n        with tf.variable_scope(""encoder"") as scope:\n            dtype = scope.dtype\n            # Look up embedding, emp_inp: [max_time, batch_size, num_units]\n            encoder_emb_inp = tf.nn.embedding_lookup(self.embedding, source)\n\n            # Encoder_outpus: [max_time, batch_size, num_units]\n            cell = self._build_encoder_cell(hparams)\n\n            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n                cell,\n                encoder_emb_inp,\n                dtype=dtype,\n                sequence_length=self.batch_input.source_sequence_length,\n                time_major=self.time_major)\n\n        return encoder_outputs, encoder_state\n\n    def _build_encoder_cell(self, hparams):\n        """"""Build a multi-layer RNN cell that can be used by encoder.""""""\n        return model_helper.create_rnn_cell(\n            num_units=hparams.num_units,\n            num_layers=hparams.num_layers,\n            keep_prob=hparams.keep_prob)\n\n    def _build_decoder(self, encoder_outputs, encoder_state, hparams):\n        """"""Build and run a RNN decoder with a final projection layer.""""""\n        bos_id = tf.cast(self.vocab_table.lookup(tf.constant(hparams.bos_token)), tf.int32)\n        eos_id = tf.cast(self.vocab_table.lookup(tf.constant(hparams.eos_token)), tf.int32)\n\n        # maximum_iteration: The maximum decoding steps.\n        if hparams.tgt_max_len_infer:\n            maximum_iterations = hparams.tgt_max_len_infer\n        else:\n            decoding_length_factor = 2.0\n            max_encoder_length = tf.reduce_max(self.batch_input.source_sequence_length)\n            maximum_iterations = tf.to_int32(tf.round(\n                tf.to_float(max_encoder_length) * decoding_length_factor))\n\n        # Decoder.\n        with tf.variable_scope(""decoder"") as decoder_scope:\n            cell, decoder_initial_state = self._build_decoder_cell(\n                hparams, encoder_outputs, encoder_state,\n                self.batch_input.source_sequence_length)\n\n            # Training\n            if self.training:\n                # decoder_emp_inp: [max_time, batch_size, num_units]\n                target_input = self.batch_input.target_input\n                if self.time_major:\n                    target_input = tf.transpose(target_input)\n                decoder_emb_inp = tf.nn.embedding_lookup(self.embedding, target_input)\n\n                # Helper\n                helper = tf.contrib.seq2seq.TrainingHelper(\n                    decoder_emb_inp, self.batch_input.target_sequence_length,\n                    time_major=self.time_major)\n\n                # Decoder\n                my_decoder = tf.contrib.seq2seq.BasicDecoder(\n                    cell,\n                    helper,\n                    decoder_initial_state,)\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    output_time_major=self.time_major,\n                    swap_memory=True,\n                    scope=decoder_scope)\n\n                sample_id = outputs.sample_id\n                logits = self.output_layer(outputs.rnn_output)\n            # Inference\n            else:\n                beam_width = hparams.beam_width\n                length_penalty_weight = hparams.length_penalty_weight\n                start_tokens = tf.fill([self.batch_size], bos_id)\n                end_token = eos_id\n\n                if beam_width > 0:\n                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                        cell=cell,\n                        embedding=self.embedding,\n                        start_tokens=start_tokens,\n                        end_token=end_token,\n                        initial_state=decoder_initial_state,\n                        beam_width=beam_width,\n                        output_layer=self.output_layer,\n                        length_penalty_weight=length_penalty_weight)\n                else:\n                    # Helper\n                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                        self.embedding, start_tokens, end_token)\n\n                    # Decoder\n                    my_decoder = tf.contrib.seq2seq.BasicDecoder(\n                        cell,\n                        helper,\n                        decoder_initial_state,\n                        output_layer=self.output_layer  # applied per timestep\n                    )\n\n                # Dynamic decoding\n                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                    my_decoder,\n                    maximum_iterations=maximum_iterations,\n                    output_time_major=self.time_major,\n                    swap_memory=True,\n                    scope=decoder_scope)\n\n                if beam_width > 0:\n                    logits = tf.no_op()\n                    sample_id = outputs.predicted_ids\n                else:\n                    logits = outputs.rnn_output\n                    sample_id = outputs.sample_id\n\n        return logits, sample_id, final_context_state\n\n    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,\n                            source_sequence_length):\n        """"""Build a RNN cell with attention mechanism that can be used by decoder.""""""\n        num_units = hparams.num_units\n        num_layers = hparams.num_layers\n        beam_width = hparams.beam_width\n\n        dtype = tf.float32\n\n        if self.time_major:\n            memory = tf.transpose(encoder_outputs, [1, 0, 2])\n        else:\n            memory = encoder_outputs\n\n        if not self.training and beam_width > 0:\n            memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n            source_sequence_length = tf.contrib.seq2seq.tile_batch(source_sequence_length,\n                                                                   multiplier=beam_width)\n            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n                                                          multiplier=beam_width)\n            batch_size = self.batch_size * beam_width\n        else:\n            batch_size = self.batch_size\n\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n            num_units, memory, memory_sequence_length=source_sequence_length)\n\n        cell = model_helper.create_rnn_cell(\n            num_units=num_units,\n            num_layers=num_layers,\n            keep_prob=hparams.keep_prob)\n\n        # Only generate alignment in greedy INFER mode.\n        alignment_history = (not self.training and beam_width == 0)\n        cell = tf.contrib.seq2seq.AttentionWrapper(\n            cell,\n            attention_mechanism,\n            attention_layer_size=num_units,\n            alignment_history=alignment_history,\n            name=""attention"")\n\n        if hparams.pass_hidden_state:\n            decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state)\n        else:\n            decoder_initial_state = cell.zero_state(batch_size, dtype)\n\n        return cell, decoder_initial_state\n\n    def _compute_loss(self, logits):\n        """"""Compute optimization loss.""""""\n        target_output = self.batch_input.target_output\n        if self.time_major:\n            target_output = tf.transpose(target_output)\n        max_time = self.get_max_time(target_output)\n        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=target_output, logits=logits)\n        target_weights = tf.sequence_mask(\n            self.batch_input.target_sequence_length, max_time, dtype=logits.dtype)\n        if self.time_major:\n            target_weights = tf.transpose(target_weights)\n\n        loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(self.batch_size)\n        return loss\n\n    def get_max_time(self, tensor):\n        time_axis = 0 if self.time_major else 1\n        return tensor.shape[time_axis].value or tf.shape(tensor)[time_axis]\n\n    def infer(self, sess):\n        assert not self.training\n        _, infer_summary, _, sample_words = sess.run([\n            self.infer_logits, self.infer_summary, self.sample_id, self.sample_words\n        ])\n\n        # make sure outputs is of shape [batch_size, time]\n        if self.time_major:\n            sample_words = sample_words.transpose()\n\n        return sample_words, infer_summary\n'"
chatbot/modelhelper.py,13,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport tensorflow as tf\n\n\ndef get_initializer(init_op, seed=None, init_weight=None):\n    """"""Create an initializer. init_weight is only for uniform.""""""\n    if init_op == ""uniform"":\n        assert init_weight\n        return tf.random_uniform_initializer(-init_weight, init_weight, seed=seed)\n    elif init_op == ""glorot_normal"":\n        return tf.contrib.keras.initializers.glorot_normal(seed=seed)\n    elif init_op == ""glorot_uniform"":\n        return tf.contrib.keras.initializers.glorot_uniform(seed=seed)\n    else:\n        raise ValueError(""Unknown init_op %s"" % init_op)\n\n\n# def get_device_str(device_id, num_gpus):\n#     """"""Return a device string for multi-GPU setup.""""""\n#     if num_gpus == 0:\n#         return ""/cpu:0""\n#     device_str_output = ""/gpu:%d"" % (device_id % num_gpus)\n#     return device_str_output\n\n\ndef create_embbeding(vocab_size, embed_size, dtype=tf.float32, scope=None):\n    """"""Create embedding matrix for both encoder and decoder.""""""\n    with tf.variable_scope(scope or ""embeddings"", dtype=dtype):\n        embedding = tf.get_variable(""embedding"", [vocab_size, embed_size], dtype)\n\n    return embedding\n\n\ndef _single_cell(num_units, keep_prob, device_str=None):\n    """"""Create an instance of a single RNN cell.""""""\n    single_cell = tf.contrib.rnn.GRUCell(num_units)\n\n    if keep_prob < 1.0:\n        single_cell = tf.contrib.rnn.DropoutWrapper(cell=single_cell, input_keep_prob=keep_prob)\n\n    # Device Wrapper\n    if device_str:\n        single_cell = tf.contrib.rnn.DeviceWrapper(single_cell, device_str)\n\n    return single_cell\n\n\ndef create_rnn_cell(num_units, num_layers, keep_prob):\n    """"""Create multi-layer RNN cell.""""""\n    cell_list = []\n    for i in range(num_layers):\n        single_cell = _single_cell(num_units=num_units, keep_prob=keep_prob)\n        cell_list.append(single_cell)\n\n    if len(cell_list) == 1:  # Single layer.\n        return cell_list[0]\n    else:  # Multi layers\n        return tf.contrib.rnn.MultiRNNCell(cell_list)\n\n\ndef gradient_clip(gradients, max_gradient_norm):\n    """"""Clipping gradients of a model.""""""\n    clipped_gradients, gradient_norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n    gradient_norm_summary = [tf.summary.scalar(""grad_norm"", gradient_norm)]\n    gradient_norm_summary.append(\n        tf.summary.scalar(""clipped_gradient"", tf.global_norm(clipped_gradients)))\n\n    return clipped_gradients, gradient_norm_summary\n'"
chatbot/patternutils.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This file is only used at inference time.""""""\nimport re\n\n\ndef check_patterns_and_replace(question):\n    pat_matched, new_sentence, para_list = _check_arithmetic_pattern_and_replace(question)\n\n    if not pat_matched:\n        pat_matched, new_sentence, para_list = _check_not_username_pattern_and_replace(new_sentence)\n\n    if not pat_matched:\n        pat_matched, new_sentence, para_list = _check_username_callme_pattern_and_replace(new_sentence)\n\n    return pat_matched, new_sentence, para_list\n\n\ndef _check_arithmetic_pattern_and_replace(sentence):\n    pat_matched, ind_list, num_list = _contains_arithmetic_pattern(sentence)\n    if pat_matched:\n        s1, e1 = ind_list[0]\n        s2, e2 = ind_list[1]\n        # Leave spaces around the special tokens so that NLTK knows they are separate tokens\n        new_sentence = sentence[:s1] + \' _num1_ \' + sentence[e1:s2] + \' _num2_ \' + sentence[e2:]\n        return True, new_sentence, num_list\n    else:\n        return False, sentence, num_list\n\n\ndef _contains_arithmetic_pattern(sentence):\n    numbers = [\n        ""zero"", ""one"", ""two"", ""three"", ""four"", ""five"", ""six"", ""seven"",\n        ""eight"", ""nine"", ""ten"", ""eleven"", ""twelve"", ""thirteen"", ""fourteen"",\n        ""fifteen"", ""sixteen"", ""seventeen"", ""eighteen"", ""nineteen"",\n        ""twenty"", ""thirty"", ""forty"", ""fifty"", ""sixty"", ""seventy"", ""eighty"", ""ninety"",\n        ""hundred"", ""thousand"", ""million"", ""billion"", ""trillion""]\n\n    pat_op1 = re.compile(\n        r\'\\s(plus|add|added|\\+|minus|subtract|subtracted|-|times|multiply|multiplied|\\*|divide|(divided\\s+by)|/)\\s\',\n        re.IGNORECASE)\n    pat_op2 = re.compile(r\'\\s((sum\\s+of)|(product\\s+of))\\s\', re.IGNORECASE)\n    pat_as = re.compile(r\'((\\bis\\b)|=|(\\bequals\\b)|(\\bget\\b))\', re.IGNORECASE)\n\n    mat_op1 = re.search(pat_op1, sentence)\n    mat_op2 = re.search(pat_op2, sentence)\n    mat_as = re.search(pat_as, sentence)\n    if (mat_op1 or mat_op2) and mat_as:  # contains an arithmetic operator and an assign operator\n        # Replace all occurrences of word ""and"" with 3 whitespaces before feeding to\n        # the pattern matcher.\n        pat_and = re.compile(r\'\\band\\b\', re.IGNORECASE)\n        if mat_op1:\n            tmp_sentence = pat_and.sub(\'   \', sentence)\n        else:  # Do not support word \'and\' in the English numbers any more as that can be ambiguous.\n            tmp_sentence = pat_and.sub(\'_T_\', sentence)\n\n        number_rx = r\'(?:{})\'.format(\'|\'.join(numbers))\n        pat_num = re.compile(r\'\\b{0}(?:(?:\\s+(?:and\\s+)?|-){0})*\\b|\\d+\'.format(number_rx),\n                             re.IGNORECASE)\n        ind_list = [(m.start(0), m.end(0)) for m in re.finditer(pat_num, tmp_sentence)]\n        num_list = []\n        if len(ind_list) == 2:  # contains exactly two numbers\n            for start, end in ind_list:\n                text = sentence[start:end]\n                text_int = _text2int(text)\n                if text_int == -1:\n                    return False, [], []\n                num_list.append(text_int)\n\n            return True, ind_list, num_list\n\n    return False, [], []\n\n\ndef _text2int(text):\n    if text.isdigit():\n        return int(text)\n\n    num_words = {}\n    units = [\n        ""zero"", ""one"", ""two"", ""three"", ""four"", ""five"", ""six"", ""seven"", ""eight"",\n        ""nine"", ""ten"", ""eleven"", ""twelve"", ""thirteen"", ""fourteen"", ""fifteen"",\n        ""sixteen"", ""seventeen"", ""eighteen"", ""nineteen"",\n    ]\n    tens = ["""", """", ""twenty"", ""thirty"", ""forty"", ""fifty"", ""sixty"", ""seventy"", ""eighty"", ""ninety""]\n    scales = [""hundred"", ""thousand"", ""million"", ""billion"", ""trillion""]\n\n    num_words[""and""] = (1, 0)\n    for idx, word in enumerate(units):\n        num_words[word] = (1, idx)\n    for idx, word in enumerate(tens):\n        num_words[word] = (1, idx * 10)\n    for idx, word in enumerate(scales):\n        num_words[word] = (10 ** (idx * 3 or 2), 0)\n\n    current = result = 0\n    for word in text.replace(""-"", "" "").lower().split():\n        if word not in num_words:\n            return -1\n\n        scale, increment = num_words[word]\n        current = current * scale + increment\n        if scale > 100:\n            result += current\n            current = 0\n\n    return result + current\n\n\ndef _check_not_username_pattern_and_replace(sentence):\n    import nltk\n\n    tokens = nltk.word_tokenize(sentence)\n    tmp_sentence = \' \'.join(tokens[:]).strip()\n\n    pat_not_but = re.compile(r\'(\\s|^)my\\s+name\\s+is\\s+(not|n\\\'t)\\s+(.+?)(\\s\\.|\\s,|\\s!)\\s*but\\s+(.+?)(\\s\\.|\\s,|\\s!|$)\',\n                             re.IGNORECASE)\n    mat_not_but = re.search(pat_not_but, tmp_sentence)\n\n    pat_not = re.compile(r\'(\\s|^)my\\s+name\\s+is\\s+(not|n\\\'t)\\s+(.+?)(\\s\\.|\\s,|\\s!|$)\', re.IGNORECASE)\n    mat_not = re.search(pat_not, tmp_sentence)\n\n    para_list = []\n    found = 0\n    if mat_not_but:\n        wrong_name = mat_not_but.group(3).strip()\n        correct_name = mat_not_but.group(5).strip()\n        para_list.append(correct_name)\n        new_sentence = sentence.replace(wrong_name, \' _ignored_ \', 1).replace(correct_name, \' _name_ \', 1)\n        # print(""User name is not: {}, but {}."".format(wrong_name, correct_name))\n        found += 1\n    elif mat_not:\n        wrong_name = mat_not.group(3).strip()\n        new_sentence = sentence.replace(wrong_name, \' _ignored_ \', 1)\n        # print(""User name is not: {}."".format(wrong_name))\n        found += 1\n    else:\n        new_sentence = sentence\n        # print(""Wrong name not found."")\n\n    if found >= 1:\n        return True, new_sentence, para_list\n    else:\n        return False, sentence, para_list\n\n\ndef _check_username_callme_pattern_and_replace(sentence):\n    import nltk\n\n    tokens = nltk.word_tokenize(sentence)\n    tmp_sentence = \' \'.join(tokens[:]).strip()\n\n    pat_name = re.compile(r\'(\\s|^)my\\s+name\\s+is\\s+(.+?)(\\s\\.|\\s,|\\s!|$)\', re.IGNORECASE)\n    pat_call = re.compile(r\'(\\s|^)call\\s+me\\s+(.+?)(\\s(please|pls))?(\\s\\.|\\s,|\\s!|$)\', re.IGNORECASE)\n\n    mat_name = re.search(pat_name, tmp_sentence)\n    mat_call = re.search(pat_call, tmp_sentence)\n\n    para_list = []\n    found = 0\n    if mat_name:\n        user_name = mat_name.group(2).strip()\n        para_list.append(user_name)\n        new_sentence = sentence.replace(user_name, \' _name_ \', 1)\n        # print(""User name is: {}."".format(user_name))\n        found += 1\n    else:\n        para_list.append(\'\')  # reserve the slot\n        new_sentence = sentence\n        # print(""User name not found."")\n\n    if mat_call:\n        call_me = mat_call.group(2).strip()\n        para_list.append(call_me)\n        new_sentence = new_sentence.replace(call_me, \' _callme_ \')\n        # print(""Call me {}."".format(call_me))\n        found += 1\n    else:\n        para_list.append(\'\')\n        # print(""call me not found."")\n\n    if found >= 1:\n        return True, new_sentence, para_list\n    else:\n        return False, sentence, para_list\n\n\nif __name__ == ""__main__"":\n    sentence = ""My name is jack brown. Please call me Mr. Brown.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n    print(ns)\n\n    sentence = ""My name is Bo Shao.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n    print(ns)\n\n    sentence = ""You can call me Dr. Shao.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n    print(ns)\n\n    sentence = ""Call me Ms. Tailor please.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n    print(ns)\n\n    sentence = ""My name is Mark. Please call me Mark D.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n    print(ns)\n\n    sentence = ""My name is not just Shao, but Bo Shao.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_not_username_pattern_and_replace(sentence)\n    print(ns)\n\n    sentence = ""My name is not just Shao.""\n    print(""# {}"".format(sentence))\n    _, ns, _ = _check_not_username_pattern_and_replace(sentence)\n    print(ns)'"
chatbot/sessiondata.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nThis class is only used at inference time.\nIn the case of a production system, the SessionData has to be maintained so that ChatSession objects\ncan expire and then be cleaned from the memory.\n""""""\n\n\nclass SessionData:\n    def __init__(self):\n        self.session_dict = {}\n\n    def add_session(self):\n        items = self.session_dict.items()\n        if items:\n            last_id = max(k for k, v in items)\n        else:\n            last_id = 0\n        new_id = last_id + 1\n\n        self.session_dict[new_id] = ChatSession(new_id)\n        return new_id\n\n    def get_session(self, session_id):\n        return self.session_dict[session_id]\n\n\nclass ChatSession:\n    def __init__(self, session_id):\n        """"""\n        Args:\n            session_id: The integer ID of the chat session.\n        """"""\n        self.session_id = session_id\n\n        self.howru_asked = False\n\n        self.user_name = None\n        self.call_me = None\n\n        self.last_question = None\n        self.last_answer = None\n        self.update_pair = True\n\n        self.last_topic = None\n        self.keep_topic = False\n\n        # Will be storing the information of the pending action:\n        # The action function name, the parameter for answer yes, and the parameter for answer no.\n        self.pending_action = {\'func\': None, \'Yes\': None, \'No\': None}\n\n    def before_prediction(self):\n        self.update_pair = True\n        self.keep_topic = False\n\n    def after_prediction(self, new_question, new_answer):\n        self._update_last_pair(new_question, new_answer)\n        self._clear_last_topic()\n\n    def _update_last_pair(self, new_question, new_answer):\n        """"""\n        Last pair is updated after each prediction except in a few cases.\n        """"""\n        if self.update_pair:\n            self.last_question = new_question\n            self.last_answer = new_answer\n\n    def _clear_last_topic(self):\n        """"""\n        Last topic is cleared after each prediction except in a few cases.\n        """"""\n        if not self.keep_topic:\n            self.last_topic = None\n\n    def update_pending_action(self, func_name, yes_para, no_para):\n        self.pending_action[\'func\'] = func_name\n        self.pending_action[\'Yes\'] = yes_para\n        self.pending_action[\'No\'] = no_para\n\n    def clear_pending_action(self):\n        """"""\n        Pending action is, and only is, cleared at the end of function: execute_pending_action_and_reply.\n        """"""\n        self.pending_action[\'func\'] = None\n        self.pending_action[\'Yes\'] = None\n        self.pending_action[\'No\'] = None\n'"
chatbot/tokenizeddata.py,46,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport codecs\nimport os\nimport tensorflow as tf\n\nfrom collections import namedtuple\nfrom tensorflow.python.ops import lookup_ops\n\nfrom chatbot.hparams import HParams\n\nCOMMENT_LINE_STT = ""#==""\nCONVERSATION_SEP = ""===""\n\nAUG0_FOLDER = ""Augment0""\nAUG1_FOLDER = ""Augment1""\nAUG2_FOLDER = ""Augment2""\n\nMAX_LEN = 1000  # Assume no line in the training data is having more than this number of characters\nVOCAB_FILE = ""vocab.txt""\n\n\nclass TokenizedData:\n    def __init__(self, corpus_dir, hparams=None, training=True, buffer_size=8192):\n        """"""\n        Args:\n            corpus_dir: Name of the folder storing corpus files for training.\n            hparams: The object containing the loaded hyper parameters. If None, it will be \n                    initialized here.\n            training: Whether to use this object for training.\n            buffer_size: The buffer size used for mapping process during data processing.\n        """"""\n        if hparams is None:\n            self.hparams = HParams(corpus_dir).hparams\n        else:\n            self.hparams = hparams\n\n        self.src_max_len = self.hparams.src_max_len\n        self.tgt_max_len = self.hparams.tgt_max_len\n\n        self.training = training\n        self.text_set = None\n        self.id_set = None\n\n        vocab_file = os.path.join(corpus_dir, VOCAB_FILE)\n        self.vocab_size, _ = check_vocab(vocab_file)\n        self.vocab_table = lookup_ops.index_table_from_file(vocab_file,\n                                                            default_value=self.hparams.unk_id)\n        # print(""vocab_size = {}"".format(self.vocab_size))\n\n        if training:\n            self.case_table = prepare_case_table()\n            self.reverse_vocab_table = None\n            self._load_corpus(corpus_dir)\n            self._convert_to_tokens(buffer_size)\n        else:\n            self.case_table = None\n            self.reverse_vocab_table = \\\n                lookup_ops.index_to_string_table_from_file(vocab_file,\n                                                           default_value=self.hparams.unk_token)\n\n    def get_training_batch(self, num_threads=4):\n        assert self.training\n\n        buffer_size = self.hparams.batch_size * 400\n\n        # Comment this line for debugging.\n        train_set = self.id_set.shuffle(buffer_size=buffer_size)\n\n        # Create a target input prefixed with BOS and a target output suffixed with EOS.\n        # After this mapping, each element in the train_set contains 3 columns/items.\n        train_set = train_set.map(lambda src, tgt:\n                                  (src, tf.concat(([self.hparams.bos_id], tgt), 0),\n                                   tf.concat((tgt, [self.hparams.eos_id]), 0)),\n                                  num_parallel_calls=num_threads).prefetch(buffer_size)\n\n        # Add in sequence lengths.\n        train_set = train_set.map(lambda src, tgt_in, tgt_out:\n                                  (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)),\n                                  num_parallel_calls=num_threads).prefetch(buffer_size)\n\n        def batching_func(x):\n            return x.padded_batch(\n                self.hparams.batch_size,\n                # The first three entries are the source and target line rows, these have unknown-length\n                # vectors. The last two entries are the source and target row sizes, which are scalars.\n                padded_shapes=(tf.TensorShape([None]),  # src\n                               tf.TensorShape([None]),  # tgt_input\n                               tf.TensorShape([None]),  # tgt_output\n                               tf.TensorShape([]),      # src_len\n                               tf.TensorShape([])),     # tgt_len\n                # Pad the source and target sequences with eos tokens. Though we don\'t generally need to\n                # do this since later on we will be masking out calculations past the true sequence.\n                padding_values=(self.hparams.eos_id,  # src\n                                self.hparams.eos_id,  # tgt_input\n                                self.hparams.eos_id,  # tgt_output\n                                0,       # src_len -- unused\n                                0))      # tgt_len -- unused\n\n        if self.hparams.num_buckets > 1:\n            bucket_width = (self.src_max_len + self.hparams.num_buckets - 1) // self.hparams.num_buckets\n\n            # Parameters match the columns in each element of the dataset.\n            def key_func(unused_1, unused_2, unused_3, src_len, tgt_len):\n                # Calculate bucket_width by maximum source sequence length. Pairs with length [0, bucket_width)\n                # go to bucket 0, length [bucket_width, 2 * bucket_width) go to bucket 1, etc. Pairs with\n                # length over ((num_bucket-1) * bucket_width) words all go into the last bucket.\n                # Bucket sentence pairs by the length of their source sentence and target sentence.\n                bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width)\n                return tf.to_int64(tf.minimum(self.hparams.num_buckets, bucket_id))\n\n            # No key to filter the dataset. Therefore the key is unused.\n            def reduce_func(unused_key, windowed_data):\n                return batching_func(windowed_data)\n\n            batched_dataset = train_set.apply(\n                tf.contrib.data.group_by_window(key_func=key_func,\n                                                reduce_func=reduce_func,\n                                                window_size=self.hparams.batch_size))\n        else:\n            batched_dataset = batching_func(train_set)\n\n        batched_iter = batched_dataset.make_initializable_iterator()\n        (src_ids, tgt_input_ids, tgt_output_ids, src_seq_len, tgt_seq_len) = (batched_iter.get_next())\n\n        return BatchedInput(initializer=batched_iter.initializer,\n                            source=src_ids,\n                            target_input=tgt_input_ids,\n                            target_output=tgt_output_ids,\n                            source_sequence_length=src_seq_len,\n                            target_sequence_length=tgt_seq_len)\n\n    def get_inference_batch(self, src_dataset):\n        text_dataset = src_dataset.map(lambda src: tf.string_split([src]).values)\n\n        if self.hparams.src_max_len_infer:\n            text_dataset = text_dataset.map(lambda src: src[:self.hparams.src_max_len_infer])\n        # Convert the word strings to ids\n        id_dataset = text_dataset.map(lambda src: tf.cast(self.vocab_table.lookup(src),\n                                                          tf.int32))\n        if self.hparams.source_reverse:\n            id_dataset = id_dataset.map(lambda src: tf.reverse(src, axis=[0]))\n        # Add in the word counts.\n        id_dataset = id_dataset.map(lambda src: (src, tf.size(src)))\n\n        def batching_func(x):\n            return x.padded_batch(\n                self.hparams.batch_size_infer,\n                # The entry is the source line rows; this has unknown-length vectors.\n                # The last entry is the source row size; this is a scalar.\n                padded_shapes=(tf.TensorShape([None]),  # src\n                               tf.TensorShape([])),     # src_len\n                # Pad the source sequences with eos tokens. Though notice we don\'t generally need to\n                # do this since later on we will be masking out calculations past the true sequence.\n                padding_values=(self.hparams.eos_id,  # src\n                                0))                   # src_len -- unused\n\n        id_dataset = batching_func(id_dataset)\n\n        infer_iter = id_dataset.make_initializable_iterator()\n        (src_ids, src_seq_len) = infer_iter.get_next()\n\n        return BatchedInput(initializer=infer_iter.initializer,\n                            source=src_ids,\n                            target_input=None,\n                            target_output=None,\n                            source_sequence_length=src_seq_len,\n                            target_sequence_length=None)\n\n    def _load_corpus(self, corpus_dir):\n        for fd in range(2, -1, -1):\n            file_list = []\n            if fd == 0:\n                file_dir = os.path.join(corpus_dir, AUG0_FOLDER)\n            elif fd == 1:\n                file_dir = os.path.join(corpus_dir, AUG1_FOLDER)\n            else:\n                file_dir = os.path.join(corpus_dir, AUG2_FOLDER)\n\n            for data_file in sorted(os.listdir(file_dir)):\n                full_path_name = os.path.join(file_dir, data_file)\n                if os.path.isfile(full_path_name) and data_file.lower().endswith(\'.txt\'):\n                    file_list.append(full_path_name)\n\n            assert len(file_list) > 0\n            dataset = tf.data.TextLineDataset(file_list)\n\n            src_dataset = dataset.filter(lambda line:\n                                         tf.logical_and(tf.size(line) > 0,\n                                                        tf.equal(tf.substr(line, 0, 2), tf.constant(\'Q:\'))))\n            src_dataset = src_dataset.map(lambda line:\n                                          tf.substr(line, 2, MAX_LEN)).prefetch(4096)\n            tgt_dataset = dataset.filter(lambda line:\n                                         tf.logical_and(tf.size(line) > 0,\n                                                        tf.equal(tf.substr(line, 0, 2), tf.constant(\'A:\'))))\n            tgt_dataset = tgt_dataset.map(lambda line:\n                                          tf.substr(line, 2, MAX_LEN)).prefetch(4096)\n\n            src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))\n            if fd == 1:\n                src_tgt_dataset = src_tgt_dataset.repeat(self.hparams.aug1_repeat_times)\n            elif fd == 2:\n                src_tgt_dataset = src_tgt_dataset.repeat(self.hparams.aug2_repeat_times)\n\n            if self.text_set is None:\n                self.text_set = src_tgt_dataset\n            else:\n                self.text_set = self.text_set.concatenate(src_tgt_dataset)\n\n    def _convert_to_tokens(self, buffer_size):\n        # The following 3 steps act as a python String lower() function\n        # Split to characters\n        self.text_set = self.text_set.map(lambda src, tgt:\n                                          (tf.string_split([src], delimiter=\'\').values,\n                                           tf.string_split([tgt], delimiter=\'\').values)\n                                          ).prefetch(buffer_size)\n        # Convert all upper case characters to lower case characters\n        self.text_set = self.text_set.map(lambda src, tgt:\n                                          (self.case_table.lookup(src), self.case_table.lookup(tgt))\n                                          ).prefetch(buffer_size)\n        # Join characters back to strings\n        self.text_set = self.text_set.map(lambda src, tgt:\n                                          (tf.reduce_join([src]), tf.reduce_join([tgt]))\n                                          ).prefetch(buffer_size)\n\n        # Split to word tokens\n        self.text_set = self.text_set.map(lambda src, tgt:\n                                          (tf.string_split([src]).values, tf.string_split([tgt]).values)\n                                          ).prefetch(buffer_size)\n        # Remove sentences longer than the model allows\n        self.text_set = self.text_set.map(lambda src, tgt:\n                                          (src[:self.src_max_len], tgt[:self.tgt_max_len])\n                                          ).prefetch(buffer_size)\n\n        # Reverse the source sentence if applicable\n        if self.hparams.source_reverse:\n            self.text_set = self.text_set.map(lambda src, tgt:\n                                              (tf.reverse(src, axis=[0]), tgt)\n                                              ).prefetch(buffer_size)\n\n        # Convert the word strings to ids.  Word strings that are not in the vocab get\n        # the lookup table\'s default_value integer.\n        self.id_set = self.text_set.map(lambda src, tgt:\n                                        (tf.cast(self.vocab_table.lookup(src), tf.int32),\n                                         tf.cast(self.vocab_table.lookup(tgt), tf.int32))\n                                        ).prefetch(buffer_size)\n\n\ndef check_vocab(vocab_file):\n    """"""Check to make sure vocab_file exists""""""\n    if tf.gfile.Exists(vocab_file):\n        vocab_list = []\n        with codecs.getreader(""utf-8"")(tf.gfile.GFile(vocab_file, ""rb"")) as f:\n            for word in f:\n                vocab_list.append(word.strip())\n    else:\n        raise ValueError(""The vocab_file does not exist. Please run the script to create it."")\n\n    return len(vocab_list), vocab_list\n\n\ndef prepare_case_table():\n    keys = tf.constant([chr(i) for i in range(32, 127)])\n\n    l1 = [chr(i) for i in range(32, 65)]\n    l2 = [chr(i) for i in range(97, 123)]\n    l3 = [chr(i) for i in range(91, 127)]\n    values = tf.constant(l1 + l2 + l3)\n\n    return tf.contrib.lookup.HashTable(\n        tf.contrib.lookup.KeyValueTensorInitializer(keys, values), \' \')\n\n\nclass BatchedInput(namedtuple(""BatchedInput"",\n                              [""initializer"",\n                               ""source"",\n                               ""target_input"",\n                               ""target_output"",\n                               ""source_sequence_length"",\n                               ""target_sequence_length""])):\n    pass\n\n# The code below is kept for debugging purpose only. Uncomment and run it to understand\n# the pipe line used in the new NMT model.\n# if __name__ == ""__main__"":\n#     import nltk\n#     from settings import PROJECT_ROOT\n#\n#     corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n#     training = True\n#     if training:\n#         td = TokenizedData(corp_dir)\n#         train_batch = td.get_training_batch()\n#\n#         with tf.Session() as sess:\n#             sess.run(tf.tables_initializer())\n#             sess.run(train_batch.initializer)\n#             print(""Initialized ... ..."")\n#\n#             for i in range(5):\n#                 try:\n#                     # Note that running training_batch directly won\'t trigger get_next() call.\n#                     # Run any of the 5 components will do the trick.\n#                     element = sess.run([train_batch.source, train_batch.target_input,\n#                                         train_batch.source_sequence_length, train_batch.target_sequence_length])\n#                     print(i, element)\n#                 except tf.errors.OutOfRangeError:\n#                     print(""end of data @ {}"".format(i))\n#                     break\n#     else:\n#         questions = [""How are you?"", ""What\'s your name?"", ""What time is it now?"",\n#                      ""When was the last time I met you? Do you remember?""]\n#         new_q_list = []\n#         for q in questions:\n#             tokens = nltk.word_tokenize(q.lower())\n#             new_q = \' \'.join(tokens[:]).strip()\n#             new_q_list.append(new_q)\n#\n#         td = TokenizedData(corpus_dir=corp_dir, training=False)\n#         src_dataset = tf.data.Dataset.from_tensor_slices(tf.constant(new_q_list))\n#         infer_batch = td.get_inference_batch(src_dataset)\n#\n#         with tf.Session() as sess:\n#             sess.run(tf.tables_initializer())\n#             sess.run(infer_batch.initializer)\n#             print(""Initialized ... ..."")\n#\n#             for i in range(10):\n#                 try:\n#                     element = sess.run([infer_batch.source, infer_batch.source_sequence_length])\n#                     print(i, element)\n#                 except tf.errors.OutOfRangeError:\n#                     print(""end of data @ {}"".format(i))\n#                     break\n'"
Data/Corpus/cornelldatacleaner.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport ast\nimport nltk\nimport re\n\nMOVIE_LINES_FIELDS = [""lineID"", ""characterID"", ""movieID"", ""character"", ""text""]\nMOVIE_CONVERSATIONS_FIELDS = [""character1ID"", ""character2ID"", ""movieID"", ""utteranceIDs""]\n\n\nclass CornellDataCleaner:\n    """"""\n    Load the cornell movie dialog corpus and clean the data briefly.\n\n    Available from here:\n    http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n    \n    Please download the zip file, extract the movie_lines.txt and movie_conversations.txt,\n    and copy them to the corpus_dir below before you run this script.\n    \n    Other cleaning work could have been automated, but were later found during the manual \n    cleaning process.\n     1. Remove all \'*\' characters.\n     2. Remove markers like [3], [4], [5], and [beat]. And remove all these brackets.\n    """"""\n    def __init__(self, corpus_dir):\n        """"""\n        Args:\n            corpus_dir: File directory where to load the corpus.\n        """"""\n        line_file = os.path.join(corpus_dir, ""movie_lines.txt"")\n        conv_file = os.path.join(corpus_dir, ""movie_conversations.txt"")\n\n        self.lines = self.load_lines(line_file, MOVIE_LINES_FIELDS)\n        self.conversations = self.load_conversations(conv_file, MOVIE_CONVERSATIONS_FIELDS)\n\n    def load_conversations(self, filename, fields):\n        """"""\n        Args:\n            filename: File to load.\n            fields: Fields to extract.\n        Return:\n            dict<dict<str>>: the extracted fields for each line\n        """"""\n        conv_list = []\n\n        with open(filename, \'r\', encoding=\'iso-8859-1\') as f:\n            for line in f:\n                values = line.split("" +++$+++ "")\n\n                # Extract fields\n                conv_obj = {}\n                for i, field in enumerate(fields):\n                    conv_obj[field] = values[i]\n\n                # Convert string to list (conv_obj[""utteranceIDs""] == ""[\'L598485\', \'L598486\', ...]"")\n                line_ids = ast.literal_eval(conv_obj[""utteranceIDs""])\n\n                # Reassemble lines\n                conv_obj[""lines""] = []\n                for line_id in line_ids:\n                    conv_obj[""lines""].append(self.lines[line_id])\n\n                conv_list.append(conv_obj)\n\n        return conv_list\n\n    def write_cleaned_conversations(self, out_file):\n        """"""\n        Args:\n            out_file: File to save the cleaned data. \n        """"""\n        pat_curse = re.compile(\n            r\'\\b(ass|asshole|bastard|bitch|child-fucker|damn|fuck|fucking|motherfucker|motherfucking|\'\n            r\'nigger|shit|shitass)\\b\',\n            re.IGNORECASE)\n\n        with open(out_file, \'a\') as f_out:\n            for c in self.conversations:\n                written = False\n                for i in range(0, len(c[\'lines\']) - 1, 2):\n                    input_line = c[\'lines\'][i][\'text\'].strip()\n                    target_line = c[\'lines\'][i + 1][\'text\'].strip()\n\n                    if all(ord(char) < 128 for char in input_line) and \\\n                            all(ord(char) < 128 for char in target_line):\n                        input_line = self.get_formatted_line(input_line)\n                        target_line = self.get_formatted_line(target_line)\n\n                        # Only discard those conversation pairs in which the answer line contains\n                        # any common cursing words\n                        if re.search(pat_curse, target_line):\n                            continue\n\n                        # Discard sentences starting with an ellipsis\n                        if input_line.startswith(""..."") or target_line.startswith(""...""):\n                            continue\n\n                        # Discard sentences starting with a dash\n                        if input_line.startswith(""-"") or target_line.startswith(""-""):\n                            continue\n\n                        # This is to speed up the parsing below\n                        if len(input_line) > 160 or len(target_line) > 160:\n                            continue\n\n                        in_tokens = nltk.word_tokenize(input_line)\n                        tg_tokens = nltk.word_tokenize(target_line)\n                        if 8 <= len(in_tokens) < 36 and 6 <= len(tg_tokens) <= 40:\n                            f_out.write(""{}\\n"".format(input_line))\n                            f_out.write(""{}\\n"".format(target_line))\n                            written = True\n\n                if written:\n                    f_out.write(""===\\n"")\n\n    @staticmethod\n    def get_formatted_line(line):\n        pat_dot = re.compile(r\'\\.\\s+\\.\')\n        pat_dash = re.compile(r\'-\\s+-\')\n        pat_html = re.compile(r\'<.*?>\')\n\n        # Use formal ellipsis and dashes\n        while re.search(pat_dot, line):\n            line = re.sub(pat_dot, \'..\', line)\n\n        while re.search(pat_dash, line):\n            line = re.sub(pat_dash, \'--\', line)\n\n        line = re.sub(\'\\.{3,}\', \'... \', line)\n        line = re.sub(\'-{2,}\', \' -- \', line)\n\n        # Use formal apostrophe\n        line = line.replace(\' \\\' \', \'\\\'\')\n\n        # Remove extra spaces\n        line = re.sub(\'\\s+\', \' \', line).strip()\n        line = line.replace(\' .\', \'.\').replace(\' ?\', \'?\').replace(\' !\', \'!\')\n\n        # Remove HTML tags\n        line = re.sub(pat_html, \'\', line)\n\n        # Remove extra punctuations and m\'s\n        line = re.sub(\'\\?{2,}\', \'?\', line)\n        line = re.sub(\'!{2,}\', \'!\', line)\n        line = re.sub(\'m{3,}\', \'mm\', line)\n\n        return line\n\n    @staticmethod\n    def load_lines(filename, fields):\n        """"""\n        Args:\n            filename: File to load.\n            fields: Fields to extract.\n        Return:\n            dict<dict<str>>: the extracted fields for each line\n        """"""\n        lines = {}\n\n        with open(filename, \'r\', encoding=\'iso-8859-1\') as f:\n            for line in f:\n                values = line.split("" +++$+++ "")\n\n                # Extract fields\n                line_obj = {}\n                for i, field in enumerate(fields):\n                    line_obj[field] = values[i]\n\n                lines[line_obj[\'lineID\']] = line_obj\n\n        return lines\n\nif __name__ == ""__main__"":\n    from settings import PROJECT_ROOT\n\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    cd = CornellDataCleaner(corp_dir)\n    print(""{} conversations loaded."".format(len(cd.conversations)))\n\n    out_file = os.path.join(corp_dir, \'cornell_cleaned.txt\')\n    cd.write_cleaned_conversations(out_file)'"
Data/Corpus/preprocesser.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport nltk\nimport os\n\nCOMMENT_LINE_STT = ""#==""\nCONVERSATION_SEP = ""===""\n\n\ndef corpus_pre_process(file_dir):\n    """"""\n    Pre-process the training data so that it is ready to be handled by TensorFlow TextLineDataSet\n    """"""\n    for data_file in sorted(os.listdir(file_dir)):\n        full_path_name = os.path.join(file_dir, data_file)\n        if os.path.isfile(full_path_name) and data_file.lower().endswith(\'.txt\'):\n            new_name = data_file.lower().replace(\'.txt\', \'_new.txt\')\n            full_new_name = os.path.join(file_dir, new_name)\n\n            conversations = []\n            with open(full_path_name, \'r\') as f:\n                samples = []\n                for line in f:\n                    l = line.strip()\n                    if not l or l.startswith(COMMENT_LINE_STT):\n                        continue\n                    if l == CONVERSATION_SEP:\n                        if len(samples):\n                            conversations.append(samples)\n                        samples = []\n                    else:\n                        samples.append({""text"": l})\n\n                if len(samples):  # Add the last one\n                    conversations.append(samples)\n\n            with open(full_new_name, \'a\') as f_out:\n                i = 0\n                for conversation in conversations:\n                    i += 1\n                    step = 2\n                    # Iterate over all the samples of the conversation to get chat pairs\n                    for i in range(0, len(conversation) - 1, step):\n                        source_tokens = nltk.word_tokenize(conversation[i][\'text\'])\n                        target_tokens = nltk.word_tokenize(conversation[i + 1][\'text\'])\n\n                        source_line = ""Q: "" + \' \'.join(source_tokens[:]).strip()\n                        target_line = ""A: "" + \' \'.join(target_tokens[:]).strip()\n\n                        f_out.write(""{}\\n"".format(source_line))\n                        f_out.write(""{}\\n"".format(target_line))\n\n                    f_out.write(""===\\n"")\n\nif __name__ == ""__main__"":\n    from settings import PROJECT_ROOT\n\n    file_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\', \'temp\')\n    corpus_pre_process(file_dir)'"
Data/Corpus/vocabgenerator.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\n\nAUG0_FOLDER = ""Augment0""\nAUG1_FOLDER = ""Augment1""\nAUG2_FOLDER = ""Augment2""\n\nVOCAB_FILE = ""vocab.txt""\nCORNELL_DATA_FILE = ""cornell_cleaned_new.txt""\nREDDIT_DATA_FILE = ""reddit_cleaned_part.txt""\nEXCLUDED_FILE = ""excluded.txt""\n\n\ndef generate_vocab_file(corpus_dir):\n    """"""\n    Generate the vocab.txt file for the training and prediction/inference. \n    Manually remove the empty bottom line in the generated file.\n    """"""\n    vocab_list = []\n\n    # Special tokens, with IDs: 0, 1, 2\n    for t in [\'_unk_\', \'_bos_\', \'_eos_\']:\n        vocab_list.append(t)\n\n    # The word following this punctuation should be capitalized in the prediction output.\n    for t in [\'.\', \'!\', \'?\']:\n        vocab_list.append(t)\n\n    # The word following this punctuation should not precede with a space in the prediction output.\n    for t in [\'(\', \'[\', \'{\', \'``\', \'$\']:\n        vocab_list.append(t)\n\n    for fd in range(2, -1, -1):\n        if fd == 0:\n            file_dir = os.path.join(corpus_dir, AUG0_FOLDER)\n        elif fd == 1:\n            file_dir = os.path.join(corpus_dir, AUG1_FOLDER)\n        else:\n            file_dir = os.path.join(corpus_dir, AUG2_FOLDER)\n\n        for data_file in sorted(os.listdir(file_dir)):\n            full_path_name = os.path.join(file_dir, data_file)\n            if os.path.isfile(full_path_name) and data_file.lower().endswith(\'.txt\'):\n                if fd == 0 and (data_file == CORNELL_DATA_FILE or data_file == REDDIT_DATA_FILE):\n                    continue  # Will be processed below\n                with open(full_path_name, \'r\') as f:\n                    for line in f:\n                        l = line.strip()\n                        if not l:\n                            continue\n                        if l.startswith(""Q:"") or l.startswith(""A:""):\n                            tokens = l[2:].strip().split(\' \')\n                            for token in tokens:\n                                if len(token) and token != \' \':\n                                    t = token.lower()\n                                    if t not in vocab_list:\n                                        vocab_list.append(t)\n\n    print(""Vocab size after all base data files scanned: {}"".format(len(vocab_list)))\n\n    temp_dict = {}  # A temp dict\n    cornell_file = os.path.join(corpus_dir, AUG0_FOLDER, CORNELL_DATA_FILE)\n    if os.path.exists(cornell_file):\n        with open(cornell_file, \'r\') as f1:\n            for line in f1:\n                ln = line.strip()\n                if not ln:\n                    continue\n                if ln.startswith(""Q:"") or ln.startswith(""A:""):\n                    tokens = ln[2:].strip().split(\' \')\n                    for token in tokens:\n                        if len(token) and token != \' \':\n                            t = token.lower()\n                            if t not in vocab_list:\n                                if ln.startswith(""A:""):  # Keep all for responses\n                                    vocab_list.append(t)\n                                else:\n                                    if t not in temp_dict:\n                                        temp_dict[t] = 1\n                                    else:\n                                        temp_dict[t] += 1\n                                        if temp_dict[t] >= 2:\n                                            vocab_list.append(t)\n\n    print(""Vocab size after cornell data file scanned: {}"".format(len(vocab_list)))\n\n    reddit_file = os.path.join(corpus_dir, AUG0_FOLDER, REDDIT_DATA_FILE)\n    if os.path.exists(reddit_file):\n        with open(reddit_file, \'r\') as f2:\n            line_cnt = 0\n            for line in f2:\n                line_cnt += 1\n                if line_cnt % 200000 == 0:\n                    print(""{:,} lines of reddit data file scanned."".format(line_cnt))\n                ln = line.strip()\n                if not ln:\n                    continue\n                if ln.startswith(""Q:"") or ln.startswith(""A:""):\n                    tokens = ln[2:].strip().split(\' \')\n                    for token in tokens:\n                        if len(token) and token != \' \':\n                            t = token.lower()\n                            if t not in vocab_list:\n                                if ln.startswith(""A:""):  # Keep all for responses\n                                    vocab_list.append(t)\n                                else:\n                                    if t not in temp_dict:\n                                        temp_dict[t] = 1\n                                    else:\n                                        temp_dict[t] += 1\n                                        if temp_dict[t] >= 2:\n                                            if t.startswith(\'.\') or t.startswith(\'-\') \\\n                                                    or t.endswith(\'..\') or t.endswith(\'-\'):\n                                                continue\n\n                                            vocab_list.append(t)\n\n    with open(VOCAB_FILE, \'a\') as f_voc:\n        for v in vocab_list:\n            f_voc.write(""{}\\n"".format(v))\n\n    print(""The final vocab file generated. Vocab size: {}"".format(len(vocab_list)))\n\n    with open(EXCLUDED_FILE, \'a\') as f_excluded:\n        for k, _ in temp_dict.items():\n            if k not in vocab_list:\n                f_excluded.write(""{}\\n"".format(k))\n\nif __name__ == ""__main__"":\n    from settings import PROJECT_ROOT\n\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    generate_vocab_file(corp_dir)\n'"
Data/Test/testdemo.py,1,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport re\nimport tensorflow as tf\nimport time\n\nfrom settings import PROJECT_ROOT\nfrom chatbot.botpredictor import BotPredictor\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef test_demo():\n    print(""# Creating TF session ..."")\n\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    knbs_dir = os.path.join(PROJECT_ROOT, \'Data\', \'KnowledgeBase\')\n    res_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Result\')\n\n    test_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Test\')\n    in_file = os.path.join(test_dir, \'samples.txt\')\n    out_file = os.path.join(test_dir, \'responses.txt\')\n\n    with tf.Session() as sess:\n        predictor = BotPredictor(sess, corpus_dir=corp_dir, knbase_dir=knbs_dir,\n                                 result_dir=res_dir, result_file=\'basic\')\n        session_id = predictor.session_data.add_session()\n\n        print(""# Prediction started ..."")\n        t0 = time.time()\n        with open(in_file, \'r\') as f_in:\n            with open(out_file, \'a\') as f_out:\n                f_out.write(get_header())\n                for line in f_in:\n                    sentence = line.strip()\n                    if not sentence or sentence.startswith(""#==""):\n                        continue\n                    f_out.write(""> {}\\n"".format(sentence))\n                    output = re.sub(r\'_nl_|_np_\', \'\\n\', predictor.predict(session_id, sentence)).strip()\n                    f_out.write(""{}\\n\\n"".format(output))\n\n        t1 = time.time()\n        print(""# Prediction completed. Time spent on prediction: {:4.2f} seconds"".format(t1-t0))\n\n\ndef get_header():\n    desc = ""# This file was generated by testdemo.py for testing purpose. It reads samples.txt "" \\\n           ""lines by line, and feeds each line to the predictor.""\n    return ""{}\\n# Command: python testdemo.py\\n# Date and Time Generated: {}\\n\\n"".\\\n        format(desc, time.strftime(""%Y-%m-%d %H:%M""))\n\n\nif __name__ == ""__main__"":\n    test_demo()\n'"
webui/server/chatservice.py,1,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport tensorflow as tf\n\nimport tornado.httpserver\nimport tornado.ioloop\n\nfrom webui.server.tornadows import soaphandler\nfrom webui.server.tornadows import webservices\nfrom webui.server.tornadows import complextypes\nfrom webui.server.tornadows import xmltypes\nfrom webui.server.tornadows.soaphandler import webservice\n\nfrom settings import PROJECT_ROOT\nfrom chatbot.botpredictor import BotPredictor\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\nclass SessionSentence(complextypes.ComplexType):\n    sessionId = int\n    sentence = str\n\n\nclass ChatService(soaphandler.SoapHandler):\n    def initialize(self, **kwargs):\n        self.predictor = kwargs.pop(\'predictor\')\n\n    @webservice(_params=[xmltypes.Integer, xmltypes.String], _returns=SessionSentence)\n    def reply(self, sessionId, question):\n        """"""\n        Args:\n            sessionId: ID of the chat session. \n            question: The sentence given by the end user who chats with the ChatLearner. \n        Returns:\n            outputSentence: The sessionId is the same as in the input for validation purpose. \n            The answer is the response from the ChatLearner.\n        """"""\n        if sessionId not in predictor.session_data.session_dict:  # Including the case of 0\n            sessionId = self.predictor.session_data.add_session()\n\n        answer = self.predictor.predict(sessionId, question)\n\n        outputSentence = SessionSentence()\n        outputSentence.sessionId = sessionId\n        outputSentence.sentence = answer\n        return outputSentence\n\nif __name__ == ""__main__"":\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    knbs_dir = os.path.join(PROJECT_ROOT, \'Data\', \'KnowledgeBase\')\n    res_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Result\')\n\n    with tf.Session() as sess:\n        predictor = BotPredictor(sess, corpus_dir=corp_dir, knbase_dir=knbs_dir,\n                                 result_dir=res_dir, result_file=\'basic\')\n\n        service = [(\'ChatService\', ChatService, {\'predictor\': predictor})]\n        app = webservices.WebService(service)\n        ws = tornado.httpserver.HTTPServer(app)\n        ws.listen(8080)\n        print(""Web service started."")\n        tornado.ioloop.IOLoop.instance().start()\n'"
webui_alternative/server/chatservice.py,1,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport tensorflow as tf\n\nfrom flask import Flask, request, jsonify\nfrom settings import PROJECT_ROOT\nfrom chatbot.botpredictor import BotPredictor\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\napp = Flask(__name__)\n\n\n@app.route(\'/reply\', methods=[\'POST\', \'GET\'])\ndef reply():\n    session_id = int(request.args.get(\'sessionId\'))\n    question = request.args.get(\'question\')\n\n    if session_id not in predictor.session_data.session_dict:  # Including the case of 0\n        session_id = predictor.session_data.add_session()\n\n    answer = predictor.predict(session_id, question)\n    return jsonify({\'sessionId\': session_id, \'sentence\': answer})\n\n\nif __name__ == ""__main__"":\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\')\n    knbs_dir = os.path.join(PROJECT_ROOT, \'Data\', \'KnowledgeBase\')\n    res_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Result\')\n\n    with tf.Session() as sess:\n        predictor = BotPredictor(sess, corpus_dir=corp_dir, knbase_dir=knbs_dir,\n                                 result_dir=res_dir, result_file=\'basic\')\n\n        app.run(port=5000)\n        print(""Web service started."")\n'"
Data/Corpus/RedditData/redditdatacleaner.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport nltk\nimport re\n\nCONVERSATION_SEP = ""===""\n\n\nclass RedditDataCleaner:\n    """"""\n    Load the reddit comment corpus dnd clean the data briefly.\n    \n    Download the reddit torrent and then the .bz2 file. Run redditparser.py to generate files \n    as the input of this script.\n    """"""\n    def __init__(self, corpus_dir):\n        """"""\n        Args:\n            corpus_dir: File directory where to load the corpus.\n        """"""\n        self.conversations = []\n\n        for data_file in sorted(os.listdir(corpus_dir)):\n            full_path_name = os.path.join(corpus_dir, data_file)\n            if os.path.isfile(full_path_name) and data_file.lower().endswith(\'.txt\'):\n                with open(full_path_name, \'r\', encoding=\'iso-8859-1\') as f:\n                    samples = []\n                    for line in f:\n                        l = line.strip()\n                        if not l:\n                            continue\n                        if l == CONVERSATION_SEP:\n                            if len(samples):\n                                self.conversations.append(samples)\n                            samples = []\n                        else:\n                            l = l[2:].strip()  # Remove Q: or A:\n                            samples.append({""text"": l})\n\n                    if len(samples):  # Add the last one\n                        self.conversations.append(samples)\n\n    def write_cleaned_conversations(self, out_file):\n        """"""\n        Args:\n            out_file: File to save the cleaned data. \n        """"""\n        pat_curse = re.compile(\n            r\'\\b(ass|asshole|bastard|bitch|child-fucker|damn|fuck|fucking|motherfucker|motherfucking|\'\n            r\'nigger|shit|shitass)\\b\',\n            re.IGNORECASE)\n\n        # Based on the assumption that we have enough data, we want to get the best quality part only.\n        # Get rid of pairs containing "", #, $, %, &, (, ), *, +, /, 0-9, <, = , >, @, [, \\, ], ^, _, ` and\n        # ord >= 123.\n        # Note that keeping number characters will introduce further complexities when generating\n        # vocabulary set. However, if you use only this data file for training, you may need to consider\n        # keeping numbers.\n        special_chars = [34, 35, 36, 37, 38, 40, 41, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n                         60, 61, 62, 64, 91, 92, 93, 94, 95, 96]\n        with open(out_file, \'a\') as f_out:\n            for conversation in self.conversations:\n                written = False\n                # Iterate over all the samples of the conversation to get chat pairs\n                for i in range(0, len(conversation) - 1, 2):\n                    input_line = conversation[i][\'text\'].strip()\n                    target_line = conversation[i + 1][\'text\'].strip()\n\n                    if all(ord(char) < 123 and ord(char) not in special_chars for char in input_line) and \\\n                            all(ord(char) < 123 and ord(char) not in special_chars for char in target_line):\n                        input_line = self.get_formatted_line(input_line)\n                        target_line = self.get_formatted_line(target_line)\n\n                        # Only discard those conversation pairs in which the answer line contains\n                        # any common cursing words\n                        if re.search(pat_curse, target_line):\n                            continue\n\n                        # Discard sentences starting with a dot\n                        if input_line.startswith(""."") or target_line.startswith("".""):\n                            continue\n\n                        # Discard sentences starting with a dash\n                        if input_line.startswith(""-"") or target_line.startswith(""-""):\n                            continue\n\n                        # This is to speed up the parsing below\n                        if len(input_line) > 180 or len(target_line) > 180:\n                            continue\n\n                        in_tokens = nltk.word_tokenize(input_line)\n                        tg_tokens = nltk.word_tokenize(target_line)\n                        if 8 <= len(in_tokens) <= 32 and 8 <= len(tg_tokens) <= 32:\n                            f_out.write(""{}\\n"".format(input_line))\n                            f_out.write(""{}\\n"".format(target_line))\n                            written = True\n\n                if written:\n                    f_out.write(""===\\n"")\n\n    @staticmethod\n    def get_formatted_line(line):\n        pat_dot = re.compile(r\'\\.\\s+\\.\')\n        pat_dash = re.compile(r\'-\\s+-\')\n        # pat_html = re.compile(r\'<.*?>\')\n\n        # Use formal ellipsis and dashes\n        while re.search(pat_dot, line):\n            line = re.sub(pat_dot, \'..\', line)\n\n        while re.search(pat_dash, line):\n            line = re.sub(pat_dash, \'--\', line)\n\n        line = re.sub(\'\\.{3,}\', \'... \', line)\n        line = re.sub(\'-{2,}\', \' -- \', line)\n\n        # Use formal apostrophe\n        line = line.replace(\' \\\' \', \'\\\'\')\n\n        # Remove extra spaces\n        line = re.sub(\'\\s+\', \' \', line).strip()\n        line = line.replace(\' .\', \'.\').replace(\' ?\', \'?\').replace(\' !\', \'!\')\n\n        # Remove HTML tags\n        # line = re.sub(pat_html, \'\', line)\n\n        # Remove extra punctuations and m\'s\n        line = re.sub(\'\\?{2,}\', \'?\', line)\n        line = re.sub(\'!{2,}\', \'!\', line)\n        line = re.sub(\'m{3,}\', \'mm\', line)\n\n        return line\n\nif __name__ == ""__main__"":\n    from settings import PROJECT_ROOT\n\n    corp_dir = os.path.join(PROJECT_ROOT, \'Data\', \'Corpus\', \'RedditData\', \'standard\')\n    cd = RedditDataCleaner(corp_dir)\n    print(""{} conversations loaded."".format(len(cd.conversations)))\n\n    out_file = os.path.join(corp_dir, \'reddit_cleaned.txt\')\n    cd.write_cleaned_conversations(out_file)'"
Data/Corpus/RedditData/redditparser.py,0,"b'""""""\nThis file was modified from the reddit parser on https://github.com/pender/chatbot-rnn.\nWhen unzipped, the generated files contains conversations in the same format as those \nin other data files in this Papaya Data Set corpus. They will be further cleaned and \nprocessed using other scripts in this folder.\n""""""\nfrom bz2 import BZ2File\nimport os\nimport json\nimport re\nimport sys\n\nCONFIG_FILE = ""redditparser_config.json""\nFILE_SUFFIX = "".bz2""\n\n\nclass RedditParser(object):\n    def __init__(self):\n        with open(CONFIG_FILE, \'r\') as cf:\n            config = json.load(cf)\n\n        self.input_file = config[\'input_file\']\n        self.output_dir = config[\'output_dir\']\n        self.output_file = config[\'output_file\']\n        self.report_file = config[\'report_file\']\n        self.comment_cache_size = config[\'comment_cache_size\']\n        self.output_file_size = config[\'output_file_size\']\n        self.print_every = config[\'print_every\']\n\n        self.subreddit_blacklist = set(config[\'subreddit_blacklist\'])\n        self.subreddit_whitelist = set(config[\'subreddit_whitelist\'])\n        self.substring_blacklist = set(config[\'substring_blacklist\'])\n\n    def parse(self):\n        if not os.path.exists(self.input_file):\n            print(""File not found: {}"".format(self.input_file))\n            return\n        if os.path.isfile(self.output_dir):\n            print(""A File with the same name already exists at output directory location: {}"".\n                  format(self.output_dir))\n            return\n\n        if not os.path.exists(self.output_dir):\n            os.mkdir(self.output_dir)\n\n        subreddit_dict = {}\n        comment_dict = {}\n        cache_count = 0\n\n        raw_data = self.get_raw_data_enumerator()\n        output_handler = OutputHandler(os.path.join(self.output_dir, self.output_file),\n                                       self.output_file_size)\n        for cnt, line in enumerate(raw_data):\n            line = line.decode(""utf-8"")\n            if len(line) > 1 and (line[-1] == \'}\' or line[-2] == \'}\'):\n                comment = json.loads(line)\n                if self.post_qualifies(comment):\n                    sub = comment[\'subreddit\']\n                    if sub in subreddit_dict:\n                        subreddit_dict[sub] += 1\n                    else:\n                        subreddit_dict[sub] = 1\n                    comment_dict[comment[\'name\']] = RedditComment(comment)\n                    cache_count += 1\n                    if cache_count % self.print_every == 0:\n                        print(""\\rCached {} comments. "".format(cache_count), end=\'\')\n                        sys.stdout.flush()\n                    if cache_count > self.comment_cache_size:\n                        print()\n                        self.process_comment_cached(comment_dict)\n                        self.write_comment_cached(comment_dict, output_handler)\n                        self.write_report(subreddit_dict)\n                        comment_dict.clear()\n                        cache_count = 0\n\n        print(""\\nRead all {} lines from {}."".format(cnt, self.input_file))\n        self.process_comment_cached(comment_dict)\n        self.write_comment_cached(comment_dict, output_handler)\n        self.write_report(subreddit_dict)\n\n    def get_raw_data_enumerator(self):\n        print(""Reading from {}"".format(self.input_file))\n        with BZ2File(self.input_file, ""r"") as raw_data:\n            for line in raw_data:\n                yield line\n\n    def post_qualifies(self, json_object):\n        body = json_object[\'body\'].encode(\'ascii\', \'ignore\').strip()\n        body = body.decode(""utf-8"")\n\n        post_length = len(body)\n        if post_length < 8 or post_length > 240:\n            return False\n\n        subreddit = json_object[\'subreddit\']\n\n        # Filter posts based on the configured whitelist and blacklist\n        if len(self.subreddit_whitelist) > 0 and subreddit not in self.subreddit_whitelist:\n            return False\n        if len(self.subreddit_blacklist) > 0 and subreddit in self.subreddit_blacklist:\n            return False\n        if len(self.substring_blacklist) > 0:\n            for substring in self.substring_blacklist:\n                if body.find(substring) >= 0:\n                    return False\n\n        # Preprocess the comment text\n        body = re.sub(\'[ \\t\\n]+\', \' \', body) # Replace runs of whitespace with a single space.\n        body = re.sub(\'\\^\', \'\', body) # Strip out carets.\n        body = re.sub(\'\\\\\\\\\', \'\', body) # Strip out backslashes.\n        body = re.sub(\'&lt;\', \'<\', body) # Replace \'&lt;\' with \'<\'\n        body = re.sub(\'&gt;\', \'>\', body) # Replace \'&gt;\' with \'>\'\n        body = re.sub(\'&amp;\', \'&\', body) # Replace \'&amp;\' with \'&\'\n\n        post_length = len(body)\n        if post_length < 8 or post_length > 240:\n            return False\n\n        json_object[\'body\'] = body # Save our changes\n\n        return True\n\n    def process_comment_cached(self, comment_dict):\n        i = 0\n        for my_id, my_comment in comment_dict.items():\n            i += 1\n            if i % self.print_every == 0:\n                print(""\\rProcessed {} comments"".format(i), end=\'\')\n                sys.stdout.flush()\n\n            if my_comment.parent_id is not None:  # If we\'re not a top-level post...\n                if my_comment.parent_id in comment_dict:  # ...and the parent is in our data set...\n                    parent = comment_dict[my_comment.parent_id]\n                    if parent.child_id is None:  # If my parent doesn\'t already have a child, adopt me!\n                        parent.child_id = my_id\n                    else:  # My parent already has a child.\n                        parent_previous_child = comment_dict[parent.child_id]\n                        if parent.parent_id in comment_dict:  # If my grandparent is in our data set...\n                            grandparent = comment_dict[parent.parent_id]\n                            if my_comment.author == grandparent.author:\n                                # If I share an author with grandparent, adopt me!\n                                parent.child_id = my_id\n                            elif (parent_previous_child.author != grandparent.author\n                                and my_comment.score > parent_previous_child.score):\n                                # If the existing child doesn\'t share an author with grandparent,\n                                # higher score prevails.\n                                parent.child_id = my_id\n                        elif my_comment.score > parent_previous_child.score:\n                            # If there\'s no grandparent, the higher-score child prevails.\n                            parent.child_id = my_id\n                else:\n                    # Parent IDs that aren\'t in the data set get de-referenced.\n                    my_comment.parent_id = None\n        print()\n\n    def write_comment_cached(self, comment_dict, output_handler):\n        i = 0\n        prev_print_count = 0\n        for k, v in comment_dict.items():\n            if v.parent_id is None and v.child_id is not None:\n                comment = v\n                depth = 0\n                output_string = """"\n                while comment is not None:\n                    depth += 1\n                    if depth % 2 == 1:\n                        output_string += \'Q: \'\n                    else:\n                        output_string += \'A: \'\n                    output_string += comment.body + \'\\n\'\n                    if comment.child_id in comment_dict:\n                        comment = comment_dict[comment.child_id]\n                    else:\n                        comment = None\n                        if depth % 2 == 0:\n                            output_handler.write(output_string + \'===\\n\')\n                            i += depth\n                            if i > prev_print_count + self.print_every:\n                                prev_print_count = i\n                                print(""\\rWrote {} comments"".format(i), end=\'\')\n                                sys.stdout.flush()\n        print()\n\n    def write_report(self, subreddit_dict):\n        out_report_file = os.path.join(self.output_dir, self.report_file)\n        print(""Updating subreddit report file"")\n        subreddit_list = sorted(subreddit_dict.items(), key=lambda x: -x[1])\n        with open(out_report_file, ""w"") as f:\n            for item in subreddit_list:\n                f.write(""{}: {}\\n"".format(*item))\n\n\nclass RedditComment(object):\n    def __init__(self, json_object):\n        self.body = json_object[\'body\']\n        self.score = json_object[\'ups\'] - json_object[\'downs\']\n        self.author = json_object[\'author\']\n        self.parent_id = json_object[\'parent_id\']\n        self.child_id = None\n\n\nclass OutputHandler(object):\n    def __init__(self, path, output_file_size):\n        if path.endswith(FILE_SUFFIX):\n            path = path[:-len(FILE_SUFFIX)]\n        self.base_path = path\n        self.output_file_size = output_file_size\n        self.file_reference = None\n\n    def write(self, data):\n        if self.file_reference is None:\n            self._get_current_path()\n        self.file_reference.write(data.encode(\'ascii\', \'ignore\'))\n        self.current_file_size += len(data)\n        if self.current_file_size >= self.output_file_size:\n            self.file_reference.close()\n            self.file_reference = None\n\n    def _get_current_path(self):\n        i = 1\n        while True:\n            path = ""{} {}{}"".format(self.base_path, i, FILE_SUFFIX)\n            if not os.path.exists(path): break\n            i += 1\n        self.current_path = path\n        self.current_file_size = 0\n        self.file_reference = BZ2File(self.current_path, ""w"")\n\n\nif __name__ == \'__main__\':\n    RedditParser().parse()\n'"
Data/Corpus/RedditData/secondcleaner.py,0,"b'# Copyright 2017 Bo Shao. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nThis optional cleaning step is to further remove those pairs that including the words in the \nexcluded.txt file (created by vocabgenerator.py), which are infrequent words. This can take \nvery long time (10 hours or more). If you choose to run this, you will have to generate the \nvocab file again based on the newly generated data file.\n""""""\n\nCOMMENT_LINE_STT = ""#==""\nCONVERSATION_SEP = ""===""\n\nREDDIT_INPUT = ""reddit_cleaned_new.txt""\nREDDIT_OUTPUT = ""reddit_cleaned_new2.txt""\nEXCLUDED_FILE = ""excluded.txt""\n\n\ndef clean():\n    exc_list = []\n\n    with open(EXCLUDED_FILE, \'r\') as f_exc:\n        for line in f_exc:\n            l = line.strip()\n            if not l:\n                continue\n            exc_list.append(l)\n\n    conversations = []\n    with open(REDDIT_INPUT, \'r\') as f_inp:\n        samples = []\n        for line in f_inp:\n            l = line.strip()\n            if not l or l.startswith(COMMENT_LINE_STT):\n                continue\n            if l == CONVERSATION_SEP:\n                if len(samples):\n                    conversations.append(samples)\n                samples = []\n            else:\n                samples.append({""text"": l})\n\n        if len(samples):  # Add the last one\n            conversations.append(samples)\n\n    with open(REDDIT_OUTPUT, \'a\') as f_out:\n        cnt = 0\n        for conversation in conversations:\n            written = False\n            # Iterate over all the samples of the conversation to get chat pairs\n            for i in range(0, len(conversation) - 1, 2):\n                src_line = conversation[i][\'text\'].strip()\n                tgt_line = conversation[i + 1][\'text\'].strip()\n\n                assert src_line.startswith(""Q:"") and tgt_line.startswith(""A:"")\n\n                skip = False\n                tokens = (src_line[2:] + \' \' + tgt_line[2:]).split(\' \')\n                for token in tokens:\n                    if len(token) and token != \' \':\n                        t = token.lower()\n                        if t in exc_list:\n                            skip = True\n                            cnt += 1\n                            if cnt % 1000 == 0:\n                                print(""{:,} pairs skipped."".format(cnt))\n                            break\n\n                if not skip:\n                    f_out.write(""{}\\n"".format(src_line))\n                    f_out.write(""{}\\n"".format(tgt_line))\n                    written = True\n\n            if written:\n                f_out.write(""===\\n"")\n\nif __name__ == ""__main__"":\n    clean()\n'"
webui/server/tornadows/__init__.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.'"
webui/server/tornadows/complextypes.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n"""""" Implementation of module with classes and functions for transform python \n    classes in xml schema: \n\n    See the next example:\n\n    \tfrom tornadows.complextypes import ComplexType, StringProperty, IntegerProperty\n\n    \tclass Person(ComplexType):\n\t\tname = StringProperty()\n\t\tage  = IntegerProperty()\n\n\tor you can use some python types\n\n\tclass Person(ComplexType):\n\t\tname = str\n\t\tage  = int\n\n\tis equivalent to:\n\n\t <xsd:complexType name=""Person"">\n\t\t<xsd:sequence>\n\t\t\t<xsd:element name=""name"" type=""xsd:string""/>\n\t\t\t<xsd:element name=""age"" type=""xsd:integer""/> \n\t\t</xsd:sequence>\n\t </xsd:complexType>\n\n""""""\n\nimport webui.server.tornadows.xmltypes\nimport xml.dom.minidom\nimport inspect\nfrom datetime import date, datetime, time\n\t\nclass Property:\n\t"""""" Class base for definition of properties of the attributes of a python class """"""\n\tpass\n\nclass IntegerProperty(Property):\n\t"""""" Class for definitions of Integer Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Integer\n\t\tself.value = None\n\nclass DecimalProperty(Property):\n\t"""""" Class for definitions of Decimal Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Decimal\n\t\tself.value = None\n\nclass DoubleProperty(Property):\n\t"""""" Class for definitions of Double Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Double\n\t\tself.value = None\n\nclass FloatProperty(Property):\n\t"""""" Class for definitions of Float Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Float\n\t\tself.value = None\n\nclass DurationProperty(Property):\n\t"""""" Class for definitions of Duration Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Duration\n\t\tself.value = None\n\nclass DateProperty(Property):\n\t"""""" Class for definitions of Date Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Date\n\t\tself.value = None\n\nclass TimeProperty(Property):\n\t"""""" Class for definitions of Time Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Time\n\t\tself.value = None\n\nclass DateTimeProperty(Property):\n\t"""""" Class for definitions of DateTime Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.DateTime\n\t\tself.value = None\n\nclass StringProperty(Property):\n\t"""""" Class for definitions of String Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.String\n\t\tself.value = None\n\nclass BooleanProperty(Property):\n\t"""""" Class for definitions of Boolean Property """"""\n\tdef __init__(self):\n\t\tself.type = webui.server.tornadows.xmltypes.Boolean\n\t\tself.value = None\n\nclass ArrayProperty(list):\n\t"""""" For create a list of classes """"""\n\tdef __init__(self, object, minOccurs = 1, maxOccurs=None, data=[]):\n\t\tlist.__init__(self,data)\n\t\tself._minOccurs = minOccurs\n\t\tself._maxOccurs = maxOccurs\n\t\tself._object = object\n\t\tself.append(self._object)\n\t\t\n\tdef toXSD(self,namespace=\'xsd\',nameelement=None):\n\t\t"""""" Create xml complex type for ArrayProperty """"""\n\t\txsd = self._object.toXSD()\n\t\tif self._maxOccurs == None:\n\t\t\txsd += \'<%s:element name=""%s"" type=""tns:%s"" minOccurs=""%s""/>\'%(namespace,nameelement,self._object.getName(),self._minOccurs)\n\t\telif self._maxOccurs != None:\n\t\t\txsd += \'<%s:element name=""%s"" type=""tns:%s"" minOccurs=""%s"" maxOccurs=""%s""/>\'%(namespace,nameelement,self._object.getName(),str(self._minOccurs),str(self._maxOccurs))\n\t\treturn xsd\n\nclass ComplexType(object):\n\t"""""" Base class for definitions of python class like xml document and schema:\n\n\t    from webui.server.tornadows.complextypes import ComplexType,StringProperty, IntegerProperty\n\n\t    class Person(ComplexType):\n\t\tname = StringProperty\n\t\tage  = IntegerProperty\n\t\n\t    if __name__ == \'__main__\':\n\t\tprint \'XML Schema : \'\n\t\tprint(Person.toXSD())\n\t\t\n\t\tp = Person()\n\t\tp.name.value = \'Steve J\'\n\t\tp.age.value  = 38\n\n\t\tprint(\'XML Document : \')\n\t\tprint(p.toXML())\n\n\t    or you if you want to use some python types (int, str, float, bool)\n\n\t    from webui.server.tornadows.complextypes import ComplexType\n\n\t    class Person(ComplexType):\n\t\tname = str \n\t\tage  = int\n\t\n\t    if __name__ == \'__main__\':\n\t\tprint(\'XML Schema : \')\n\t\tprint(Person.toXSD())\n\t\t\n\t\tp = Person()\n\t\tp.name.value = \'Steve J\'\n\t\tp.age.value  = 38\n\n\t\tprint(\'XML Document : \')\n\t\tprint(p.toXML())\n\n\t""""""\n\tdef __init__(self):\n\t\t"""""" Class constructor for ComplexType """"""\n\t\tdefault_attr = dir(type(\'default\',(object,),{}))\n\t\tfor attr in self.__class__.__dict__.keys():\n\t\t\tif default_attr.count(attr) > 0 or callable(attr):\n\t\t\t\tcontinue\t\n\t\t\telse:\n\t\t\t\telement = self.__class__.__dict__[attr]\n\t\t\t\ttypeobj = self._createAttributeType(element)\n\t\t\t\tsetattr(self,attr,typeobj)\n\n\tdef toXML(self,name=None,method=\'\'):\n\t\t"""""" Method that creates the XML document for the instance of python class.\n\t\t    Return a string with the xml document.\n\t\t """"""\n\t\tnameroot = None\n\n\t\tif name == None:\n\t\t\tnameroot = self.__class__.__name__\n\t\telse:\n\t\t\tnameroot = name\n\t\tnameroot += method\n\n\t\txml = \'<%s>\'%nameroot\n\t\tdefault_attr = dir(type(\'default\',(object,),{}))\n\t\tfor key in dir(self):\n\t\t\tif default_attr.count(key) > 0:\n\t\t\t\tcontinue\n\t\t\telement = findElementFromDict(self.__dict__,key)\n\t\t\tif element == None:\n\t\t\t\tcontinue\n\t\t\tif isinstance(element,list):\n\t\t\t\tfor e in element:\n\t\t\t\t\tif isinstance(e,ComplexType):\n\t\t\t\t\t\txml += e.toXML(name=key)\n\t\t\t\t\telse:\n\t\t\t\t\t\txml += \'<%s>%s</%s>\'%(key,e,key)\n\t\t\telif isinstance(element,Property):\n\t\t\t\txml += \'<%s>%s</%s>\'%(key,element.value,key)\n\t\t\telif isinstance(element,ComplexType):\n\t\t\t\txml += element.toXML(name=key)\n\t\t\telse:\n\t\t\t\txml += \'<%s>%s</%s>\'%(key,convert(type(element).__name__,element),key)\n\t\txml += \'</%s>\'%nameroot\n\t\treturn str(xml)\n\t\t\t\t\t\n\t@classmethod\n\tdef toXSD(cls,xmlns=\'http://www.w3.org/2001/XMLSchema\',namespace=\'xsd\',method=\'\', ltype=[]):\n\t\t"""""" Class method that creates the XSD document for the python class.\n\t\t    Return a string with the xml schema.\n\t\t """"""\n\t\tname = cls.__name__\n\t\txsd  = cls._generateXSD(ltype=ltype)\n\t\treturn xsd\n\t\t\n\t@classmethod\t\n\tdef _generateXSD(cls,xmlns=\'http://www.w3.org/2001/XMLSchema\',namespace=\'xsd\', ltype=[]):\n\t\t"""""" Class method for get the xml schema with the document definition.\n\t\t    Return a string with the xsd document.\n\t\t """"""\n\t\tdefault_attr = dir(type(\'default\',(object,),{}))\n\t\tname = cls.__name__\n\t\txsd  = \'<%s:complexType name=""%s"" xmlns:%s=""%s"">\'%(namespace,name,namespace,xmlns)\n\t\txsd += \'<%s:sequence>\'%namespace\n\t\tcomplextype = []\n\n\t\tfor key in dir(cls):\n\t\t\tif default_attr.count(key) > 0:\n\t\t\t\tcontinue\n\t\t\telement = findElementFromDict(cls.__dict__,key)\n\t\t\tif element == None:\n\t\t\t\tcontinue\n\t\t\tif isinstance(element,Property):\n\t\t\t\txsd += element.type.createElement(str(key))\n\t\t\t\n\t\t\telif isinstance(element,ComplexType): \n\t\t\t\tnameinstance = key\n\n\t\t\t\tif ltype.count(self._elementInput.getName()) == 0:\n\t\t\t\t\tltype.append(self._elementInput.getName())\n\t\t\t\t\tcomplextype.append(element._generateXSD(ltype=[]))\n\t\t\t\t\n\t\t\t\txsd += \'<%s:element name=""%s"" type=""tns:%s""/>\'%(namespace,nameinstance,element.getName())\t\t\t\n\t\t\telif inspect.isclass(element) and issubclass(element,ComplexType): \n\t\t\t\tnameinstance = key\n\t\t\t\t\n\t\t\t\tif ltype.count(element.getName()) == 0:\n\t\t\t\t\tltype.append(element.getName())\n\t\t\t\t\tcomplextype.append(element._generateXSD(ltype=[]))\n\t\t\t\t\n\t\t\t\txsd += \'<%s:element name=""%s"" type=""tns:%s""/>\'%(namespace,nameinstance,element.getName())\t\t\t\n\t\t\telif isinstance(element,ArrayProperty):\n\t\t\t\tif isinstance(element[0],ComplexType) or issubclass(element[0],ComplexType):\n\t\t\t\t\tcomplextype.append(element[0]._generateXSD(ltype=[]))\n\t\t\t\t\txsd += \'<%s:element name=""%s"" type=""tns:%s"" maxOccurs=""unbounded""/>\'%(namespace,key,element[0].__name__)\t\n\t\t\t\telse:\n\t\t\t\t\ttypeelement = createPythonType2XMLType(element[0].__name__)\n\t\t\t\t\txsd += \'<%s:element name=""%s"" type=""%s:%s"" maxOccurs=""unbounded""/>\'%(namespace,key,namespace,typeelement)\t\n\t\t\t\n\t\t\telif isinstance(element,list):\n\t\t\t\tif isinstance(element[0],ComplexType) or issubclass(element[0],ComplexType):\n\n\t\t\t\t\tif ltype.count(element[0].__name__) == 0:\n\t\t\t\t\t\tltype.append(element[0].__name__)\n\t\t\t\t\t\tcomplextype.append(element[0]._generateXSD(ltype=[]))\n\t\t\t\t\t\n\t\t\t\t\txsd += \'<%s:element name=""%s"" type=""tns:%s"" maxOccurs=""unbounded""/>\'%(namespace,key,element[0].__name__)\t\n\t\t\t\telse:\n\t\t\t\t\ttypeelement = createPythonType2XMLType(element[0].__name__)\n\t\t\t\t\txsd += \'<%s:element name=""%s"" type=""%s:%s"" maxOccurs=""unbounded""/>\'%(namespace,key,namespace,typeelement)\t\n\t\t\telif hasattr(element,\'__name__\'):\n\t\t\t\ttypeelement = createPythonType2XMLType(element.__name__)\n\t\t\t\txsd += \'<%s:element name=""%s"" type=""%s:%s""/>\'%(namespace,str(key),namespace,typeelement)\n\n\t\txsd += \'</%s:sequence>\'%namespace\n\t\txsd += \'</%s:complexType>\'%namespace\n\t\t\n\t\tif len(complextype) > 0:\n\t\t\tfor ct in complextype:\n\t\t\t\txsd += ct\n\t\t\t\t\n\t\treturn xsd\n\t\t\n\t@classmethod\n\tdef getName(cls):\n\t\t"""""" Class method return the name of the class """"""\n\t\treturn cls.__name__\n\t\t\n\t@classmethod\t\n\tdef _createAttributeType(self,element):\n\t\t"""""" Class method to create the types of the attributes of a ComplexType """"""\n\t\tif isinstance(element,list):\n\t\t\treturn list()\n\t\telif isinstance(element,IntegerProperty):\n\t\t\treturn IntegerProperty()\n\t\telif isinstance(element,DecimalProperty):\n\t\t\treturn DecimalProperty()\n\t\telif isinstance(element,DoubleProperty):\n\t\t\treturn DoubleProperty()\n\t\telif isinstance(element,FloatProperty):\n\t\t\treturn FloatProperty()\n\t\telif isinstance(element,DurationProperty):\n\t\t\treturn DurationProperty()\n\t\telif isinstance(element,DateProperty):\n\t\t\treturn DateProperty()\n\t\telif isinstance(element,TimeProperty):\n\t\t\treturn TimeProperty()\n\t\telif isinstance(element,DateTimeProperty):\n\t\t\treturn DateTimeProperty()\n\t\telif isinstance(element,StringProperty):\n\t\t\treturn StringProperty()\n\t\telif isinstance(element,BooleanProperty):\n\t\t\treturn BooleanProperty()\n\t\telif issubclass(element,ComplexType):\n\t\t\treturn element()\n\t\telse:\n\t\t\tif   element.__name__ == \'int\':\t\n\t\t\t\treturn int\n\t\t\telif element.__name__ == \'decimal\':\n\t\t\t\treturn float\n\t\t\telif element.__name__ == \'double\':\n\t\t\t\treturn float\n\t\t\telif element.__name__ == \'float\':\n\t\t\t\treturn float\n\t\t\telif element.__name__ == \'duration\':\n\t\t\t\treturn str\n\t\t\telif element.__name__ == \'date\':\n\t\t\t\treturn date\n\t\t\telif element.__name__ == \'time\':\n\t\t\t\treturn time\n\t\t\telif element.__name__ == \'dateTime\':\n\t\t\t\treturn datetime\n\t\t\telif element.__name__ == \'str\':\n\t\t\t\treturn str\n\t\t\telif element.__name__ == \'bool\':\n\t\t\t\treturn bool\n\ndef xml2object(xml,xsd,complex,method=\'\'):\n\t"""""" Function that converts a XML document in a instance of a python class """"""\n\tnamecls = complex.getName()\n\ttypes   = xsd2dict(xsd)\n\tlst     = xml2list(xml,namecls,types,method=method)\n\ttps     = cls2dict(complex)\n\tobj     = generateOBJ(lst,namecls,tps)\n\treturn obj\n\ndef cls2dict(complex):\n\t"""""" Function that creates a dictionary from a ComplexType class with the attributes and types """"""\n\tdefault_attr = dir(type(\'default\',(object,),{}))\n\tdct = {}\n\tfor attr in dir(complex):\n\t\tif default_attr.count(attr) > 0 or callable(attr):\n\t\t\tcontinue\n\t\telse:\n\t\t\telem = findElementFromDict(complex.__dict__,attr)\n\t\t\tif elem != None:\n\t\t\t\tdct[attr] = elem\n\treturn dct\n\ndef xsd2dict(xsd,namespace=\'xsd\'):\n\t"""""" Function that creates a dictionary from a xml schema with the type of element """"""\n\ttypes = [\'xsd:integer\',\'xsd:decimal\',\'xsd:double\',\'xsd:float\',\'xsd:duration\',\'xsd:date\',\'xsd:time\',\'xsd:dateTime\',\'xsd:string\',\'xsd:boolean\']\n\tdct = {}\n\n\telement = \'%s:element\'%namespace\n\telems = xsd.getElementsByTagName(element)\n\tfor e in elems:\n\t\tval = \'complexType\'\n\t\ttyp = str(e.getAttribute(\'type\'))\n\t\tlst = e.hasAttribute(\'maxOccurs\')\n\t\tif types.count(typ) > 0:\n\t\t\tval = \'element\'\n\t\tdct[str(e.getAttribute(\'name\'))] = (val,typ,lst)\n\treturn dct\n\ndef xml2list(xmldoc,name,types,method=\'\'):\n\t"""""" Function that creates a list from xml documento with a tuple element and value """"""\n\tname = name+method\n\t\n\tx = xml.dom.minidom.parseString(xmldoc)\n\tc = None\n\tif x.documentElement.prefix != None:\n\t\tc = x.getElementsByTagName(x.documentElement.prefix+\':\'+name)\n\telse:\n\t\tc = x.getElementsByTagName(name)\n\tattrs = genattr(c)\n\tlst = []\n\tfor a in attrs:\n\t\tt = types[a.nodeName]\n\t\ttyp = t[0]\n\t\ttypxml = t[1]\n\t\tisarray = t[2]\n\t\tif typ == \'complexType\' or typ == \'list\':\n\t\t\tl = xml2list(a.toxml(),str(a.nodeName),types)\n\t\t\tlst.append((str(a.nodeName),l,isarray))\n\t\telse:\n\t\t\tval = None\n\t\t\tif len(a.childNodes) > 0:\n\t\t\t\tval = convert(typxml,str(a.childNodes[0].nodeValue))\n\t\t\t\t# Convert str to bool.\n\t\t\t\tif val == \'true\':\n\t\t\t\t\tval = True\n\t\t\t\telif val == \'false\':\n\t\t\t\t\tval = False\n\t\t\tlst.append((str(a.nodeName),val,isarray))\n\treturn lst\n\ndef generateOBJ(d,namecls,types):\n\t"""""" Function that creates a object from a xml document """"""\n\tdct = {}\n\tlst = []\n\tfor a in d:\n\t\tname  = a[0]\n\t\tvalue = a[1]\n\t\tisarray = a[2]\n\t\tif isinstance(value,list):\n\t\t\to = generateOBJ(value,name,types)\n\t\t\tif isarray:\n\t\t\t\tlst.append(o)\n\t\t\t\tdct[name] = lst\n\t\t\telse:\n\t\t\t\tdct[name] = o\n\t\telse:\n\t\t\ttyp = findElementFromDict(types,name)\n\t\t\tif isinstance(typ,Property):\n\t\t\t\tdct[name] = createProperty(typ,value)\n\t\t\telse:\n\t\t\t\tdct[name] = value\n\treturn type(namecls,(ComplexType,),dct)\n\t\ndef createProperty(typ,value):\n\t"""""" Function that creates a Property class instance, with the value """"""\n\tct = None\n\tif isinstance(typ,IntegerProperty):\n\t\tct = IntegerProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Integer.genType(value)\n\telif isinstance(typ,DecimalProperty):\n\t\tct = DecimalProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Decimal.genType(value)\n\telif isinstance(typ,DoubleProperty):\n\t\tct = DoubleProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Double.genType(value)\n\telif isinstance(typ,FloatProperty):\n\t\tct = FloatProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Float.genType(value)\n\telif isinstance(typ,DurationProperty):\n\t\tct = DurationProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Duration.genType(value)\n\telif isinstance(typ,DateProperty):\n\t\tct = DateProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Date.genType(value)\n\telif isinstance(typ,TimeProperty):\n\t\tct = TimeProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Time.genType(value)\n\telif isinstance(typ,DateTimeProperty):\n\t\tct = DateTimeProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.DateTime.genType(value)\n\telif isinstance(typ,StringProperty):\n\t\tct = StringProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.String.genType(value)\n\telif isinstance(typ,BooleanProperty):\n\t\tct = BooleanProperty()\n\t\tct.value = webui.server.tornadows.xmltypes.Boolean.genType(value)\n\n\treturn ct\n\ndef genattr(elems):\n\t"""""" Function that generates a list with the childnodes of a xml element  """"""\n\td = []\n\tfor e in elems[0].childNodes:\n\t\tif e.nodeType == e.ELEMENT_NODE:\n\t\t\td.append(e)\n\treturn d\n\ndef findElementFromDict(dictionary,key):\n\t"""""" Function to find a element into a dictionary for the key """"""\n\telement = None\n\ttry:\n\t\telement = dictionary[key]\n\t\treturn element\n\texcept KeyError:\n\t\treturn None\n\ndef convert(typeelement,value):\n\t"""""" Function that converts a value depending his type """"""\n\tif typeelement == \'xsd:integer\' or typeelement == \'int\':\t\n\t\treturn int(value)\n\telif typeelement == \'xsd:decimal\':\n\t\treturn float(value)\n\telif typeelement == \'xsd:double\':\n\t\treturn float(value)\n\telif typeelement == \'xsd:float\' or typeelement == \'float\':\n\t\treturn float(value)\n\telif typeelement == \'xsd:duration\':\n\t\treturn str(value)\n\telif typeelement == \'xsd:date\' or typeelement == \'date\':\n\t\tsdate = str(value).split(\'-\')\n\t\treturn date(int(sdate[0]),int(sdate[1]),int(sdate[2]))\n\telif typeelement == \'xsd:time\' or typeelement == \'time\':\n\t\tstime = str(value).split(\':\')\n\t\thour = stime[0]\n\t\tmin  = stime[1]\n\t\tseg  = \'00\'\n\t\tif len(stime) >= 3:\n\t\t\tseg = stime[2].split(\'.\')[0]\n\t\treturn time(int(hour),int(min),int(seg))\n\telif typeelement == \'xsd:dateTime\' or typeelement == \'datetime\':\n\t\tsdatetime = str(value).replace(\'T\',\'-\').replace(\' \',\'-\').replace(\'+\',\'-\').split(\'-\')\n\t\tyear  = sdatetime[0]\n\t\tmon   = sdatetime[1]\n\t\tday   = sdatetime[2]\n\t\tstime = sdatetime[3].split(\':\')\n\t\thour  = stime[0]\n\t\tmin   = stime[1]\n\t\tseg   = \'00\'\n\t\tif len(stime) >= 3:\n\t\t\tseg = stime[2].split(\'.\')[0]\n\t\treturn datetime(int(year),int(mon),int(day),int(hour),int(min),int(seg)).isoformat(\'T\')\n\telif typeelement == \'xsd:string\' or typeelement == \'str\' or typeelement == \'unicode\':\n\t\treturn str(value)\n\telif typeelement == \'xsd:boolean\' or typeelement == \'bool\':\n\t\treturn str(value).lower()\n\ndef createPythonType2XMLType(pyType):\n\t"""""" Function that creates a xml type from a python type """"""\n\txmlType = None\n\tif pyType == \'int\':\n\t\txmlType = \'integer\'\n\telif pyType == \'decimal\':\n\t\txmlType = \'decimal\'\n\telif pyType == \'double\':\n\t\txmlType = \'float\'\t\t\t\t\n\telif pyType == \'float\':\n\t\txmlType = \'float\'\n\telif pyType == \'duration\':\n\t\txmlType = \'duration\'\n\telif pyType == \'date\':\n\t\txmlType = \'date\'\n\telif pyType == \'time\':\n\t\txmlType = \'time\'\n\telif pyType == \'datetime\':\n\t\txmlType = \'dateTime\'\n\telif pyType == \'str\':\n\t\txmlType = \'string\'\n\telif pyType == \'bool\':\n\t\txmlType = \'boolean\'\n\t\t\n\treturn xmlType\n\n'"
webui/server/tornadows/soap.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n"""""" Implementation of a envelope soap 1.1 """"""\n\nimport xml.dom.minidom\n\nclass SoapMessage:\n\t"""""" Implementation of a envelope soap 1.1 with minidom API\n\n\t\timport tornadows.soap\n\t\timport xml.dom.minidom\n\n\t\tsoapenvelope = tornadows.soap.SoapMessage()\n\t\txmlDoc = xml.dom.minidom.parseString(\'<Doc>Hello, world!!!</Doc>\')\n\t\tsoapenvelope.setBody(xmlDoc)\n\t\tfor s in soapenvelope.getBody():\n\t\t\tprint s.toxml()\n\n\t""""""\n\tdef __init__(self):\n\t\tself._soap = xml.dom.minidom.Document()\n\t\tenvurl = \'http://schemas.xmlsoap.org/soap/envelope/\'\n\t\tself._envelope = self._soap.createElementNS(envurl, \'soapenv:Envelope\')\n\t\tself._envelope.setAttribute(\'xmlns:soapenv\', envurl)\n\t\tself._envelope.setAttribute(\'xmlns:xsi\',\n\t\t\t\t""http://www.w3.org/2001/XMLSchema-instance"")\n\t\tself._envelope.setAttribute(\'xsi:schemaLocation\',\n\t\t\t\t\' \'.join((envurl, envurl)))\n\t\tself._soap.appendChild(self._envelope)\n\t\tself._header = self._soap.createElement(\'soapenv:Header\')\n\t\tself._body   = self._soap.createElement(\'soapenv:Body\')\n\t\tself._envelope.appendChild(self._header)\n\t\tself._envelope.appendChild(self._body)\n\n\tdef getSoap(self):\n\t\t"""""" Return the soap envelope as xml.dom.minidom.Document \n\t\t    getSoap() return a xml.dom.minidom.Document object\n\t\t""""""\n\t\treturn self._soap\n\n\tdef getHeader(self):\n\t\t"""""" Return the child elements of Header element \n\t\t    getHeader() return a list with xml.dom.minidom.Element objects\n\t\t""""""\n\t\treturn self._header.childNodes\n\n\tdef getBody(self):\n\t\t"""""" Return the child elements of Body element \n\t\t    getBody() return a list with xml.dom.minidom.Element objects\n\t\t""""""\n\t\treturn self._body.childNodes\n\n\tdef setHeader(self, header):\n\t\t"""""" Set the child content to Header element\n\t\t    setHeader(header), header is a xml.dom.minidom.Document object\n\t\t """"""\n\t\tif isinstance(header,xml.dom.minidom.Document):\n\t\t\tself._header.appendChild(header.documentElement)\n\t\telif isinstance(header,xml.dom.minidom.Element):\n\t\t\tself._header.appendChild(header)\n\n\tdef setBody(self,body):\n\t\t"""""" Set the child content to Body element \n\t\t    setBody(body), body is a xml.dom.minidom.Document object or\n\t\t    a xml.dom.minidom.Element\n\t\t""""""\n\t\tif isinstance(body,xml.dom.minidom.Document):\n\t\t\tself._body.appendChild(body.documentElement)\n\t\telif isinstance(body,xml.dom.minidom.Element):\n\t\t\tself._body.appendChild(body)\n\n\tdef removeHeader(self):\n\t\t"""""" Remove the last child elements from Header element """"""\n\t\tlastElement = self._header.lastChild\n\t\tif lastElement != None:\n\t\t\tself._header.removeChild(lastElement)\n\n\tdef removeBody(self):\n\t\t"""""" Remove last child elements from Body element """"""\n\t\tlastElement = self._body.lastChild\n\t\tif lastElement != None:\n\t\t\tself._body.removeChild(lastElement)\n'"
webui/server/tornadows/soaphandler.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n"""""" Implementation of soaphandler for webservices API 0.9.4.2 (Beta) """"""\n\nimport tornado.httpserver\nimport tornado.web\nimport xml.dom.minidom\nimport inspect\nfrom tornado.options import options\nfrom webui.server.tornadows import soap\nfrom webui.server.tornadows import xmltypes\nfrom webui.server.tornadows import complextypes\nfrom webui.server.tornadows import wsdl\n\n"""""" Global variable. If you want use your own wsdl file """"""\nwsdl_path = None\n\ndef webservice(*params,**kwparams):\n\t"""""" Decorator method for web services operators """"""\n\tdef method(f):\n\t\t_input = None\n\t\t_output = None\n\t\t_inputArray = False\n\t\t_outputArray = False\n\t\t_args = None\n\t\tif len(kwparams):\n\t\t\t_params = kwparams[\'_params\']\n\t\t\tif inspect.isclass(_params) and issubclass(_params,complextypes.ComplexType):\n\t\t\t\t_args = inspect.getargspec(f).args[1:]\n\t\t\t\t_input = _params\n\t\t\telif isinstance(_params,list):\n\t\t\t\t_args = inspect.getargspec(f).args[1:]\n\t\t\t\t_input = {}\n\t\t\t\ti = 0\n\t\t\t\tfor arg in _args:\n\t\t\t\t\t_input[arg] = _params[i]\n\t\t\t\t\ti+=1\n\t\t\telse:\n\t\t\t\t_args = inspect.getargspec(f).args[1:]\n\t\t\t\t_input = {}\n\t\t\t\tfor arg in _args:\n\t\t\t\t\t_input[arg] = _params\n\t\t\t\tif isinstance(_params,xmltypes.Array):\n\t\t\t\t\t_inputArray = True\n\n\t\t\t_returns = kwparams[\'_returns\']\n\t\t\tif isinstance(_returns,xmltypes.Array):\n\t\t\t\t_output = _returns\n\t\t\t\t_outputArray = True\n\t\t\telif isinstance(_returns,list) or issubclass(_returns,xmltypes.PrimitiveType) or issubclass(_returns,complextypes.ComplexType):\n\t\t\t\t_output = _returns\n\t\t\telse:\n\t\t\t\t_output = _returns\n\t\tdef operation(*args,**kwargs):\n\t\t\treturn f(*args,**kwargs)\n\n\t\toperation.__name__ = f.__name__\n\t\toperation._is_operation = True\n\t\toperation._args = _args\n\t\toperation._input = _input\n\t\toperation._output = _output\n\t\toperation._operation = f.__name__\n\t\toperation._inputArray = _inputArray\n\t\toperation._outputArray = _outputArray\n\t\t\n\t\treturn operation\n\treturn method\n\ndef soapfault(faultstring):\n\t"""""" Method for generate a soap fault\n\t    soapfault() return a SoapMessage() object with a message \n\t    for Soap Envelope\n\t """"""\n\tfault = soap.SoapMessage()\n\tfaultmsg  = \'<soapenv:Fault xmlns:soapenv=""http://schemas.xmlsoap.org/soap/envelope"">\\n\'\n\tfaultmsg += \'<faultcode></faultcode>\\n\'\n\tfaultmsg += \'<faultstring>%s</faultstring>\\n\'%faultstring\n\tfaultmsg += \'</soapenv:Fault>\\n\'\n\tfault.setBody(xml.dom.minidom.parseString(faultmsg))\n\treturn fault\n\nclass SoapHandler(tornado.web.RequestHandler):\n\t"""""" This subclass extends tornado.web.RequestHandler class, defining the \n\t    methods get() and post() for handle a soap message (request and response).\n\t""""""\n\tdef get(self):\n\t\t"""""" Method get() returned the WSDL. If wsdl_path is null, the\n\t\t    WSDL is generated dinamically.\n\t\t""""""\n\t\tif hasattr(options,\'wsdl_hostname\') and type(options.wsdl_hostname) is str:\n\t\t\taddress = options.wsdl_hostname\n\t\telse:\n\t\t\taddress = getattr(self, \'targetns_address\',tornado.httpserver.socket.gethostbyname(tornado.httpserver.socket.gethostname()))\n\t\t\n\t\tport = 80 # if you are using the port 80\n\t\tif len(self.request.headers[\'Host\'].split(\':\')) >= 2:\n\t\t\tport = self.request.headers[\'Host\'].split(\':\')[1]\n\t\twsdl_nameservice = self.request.uri.replace(\'/\',\'\').replace(\'?wsdl\',\'\').replace(\'?WSDL\',\'\')\n\t\twsdl_input       = None\n\t\twsdl_output      = None\n\t\twsdl_operation   = None\n\t\twsdl_args        = None\n\t\twsdl_methods     = []\n\n\t\tfor operations in dir(self):\n\t\t\toperation = getattr(self,operations)\n\t\t\tif callable(operation) and hasattr(operation,\'_input\') and hasattr(operation,\'_output\') and hasattr(operation,\'_operation\') \\\n\t\t\t   and hasattr(operation,\'_args\') and hasattr(operation,\'_is_operation\'):\n\t\t\t\twsdl_input     = getattr(operation,\'_input\')\n\t\t\t\twsdl_output    = getattr(operation,\'_output\')\n\t\t\t\twsdl_operation = getattr(operation,\'_operation\')\n\t\t\t\twsdl_args      = getattr(operation,\'_args\')\n\t\t\t\twsdl_data      = {\'args\':wsdl_args,\'input\':(\'params\',wsdl_input),\'output\':(\'returns\',wsdl_output),\'operation\':wsdl_operation}\n\t\t\t\twsdl_methods.append(wsdl_data)\n\n\t\twsdl_targetns = \'http://%s:%s/%s\'%(address,port,wsdl_nameservice)\n\t\twsdl_location = \'http://%s:%s/%s\'%(address,port,wsdl_nameservice)\n\t\tquery = self.request.query\n\t\tself.set_header(\'Content-Type\',\'application/xml; charset=UTF-8\')\n\t\tif query.upper() == \'WSDL\':\n\t\t\tif wsdl_path == None:\n\t\t\t\twsdlfile = wsdl.Wsdl(nameservice=wsdl_nameservice,\n\t\t\t\t\t\t             targetNamespace=wsdl_targetns,\n\t\t\t\t\t\t             methods=wsdl_methods,\n\t\t\t\t\t\t             location=wsdl_location)\n\n\t\t\t\tself.finish(wsdlfile.createWsdl().toxml())\n\t\t\telse:\n\t\t\t\tfd = open(str(wsdl_path),\'r\')\n\t\t\t\txmlWSDL = \'\'\n\t\t\t\tfor line in fd:\n\t\t\t\t\txmlWSDL += line\n\t\t\t\tfd.close()\n\t\t\t\tself.finish(xmlWSDL)\n\n\t@tornado.web.asynchronous\n\tdef post(self):\n\t\t"""""" Method post() to process of requests and responses SOAP messages """"""\n\t\tdef done(response):\n\t\t\tsoapmsg = response.getSoap().toxml()\n\t\t\tself.write(soapmsg)\n\t\t\tself.finish()\n\t\ttry:\n\t\t\tself._request = self._parseSoap(self.request.body)\n\t\t\tsoapaction = self.request.headers[\'SOAPAction\'].replace(\'""\',\'\')\n\t\t\tself.set_header(\'Content-Type\',\'text/xml\')\n\t\t\tfor operations in dir(self):\n\t\t\t\toperation = getattr(self,operations)\n\t\t\t\tmethod = \'\'\n\t\t\t\tif callable(operation) and hasattr(operation,\'_is_operation\'):\n\t\t\t\t\tnum_methods = self._countOperations()\n\t\t\t\t\tif hasattr(operation,\'_operation\') and soapaction.endswith(getattr(operation,\'_operation\')) and num_methods > 1:\n\t\t\t\t\t\tmethod = getattr(operation,\'_operation\') \n\t\t\t\t\t\tself._executeOperation(operation, done, method=method)\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif num_methods == 1:\n\t\t\t\t\t\tself._executeOperation(operation, done, method=\'\')\n\t\t\t\t\t\tbreak\n\t\texcept Exception as detail:\n\t\t\tfault = soapfault(\'Error in web service : %s\'%detail)\n\t\t\tself.write(fault.getSoap().toxml())\n\n\tdef _countOperations(self):\n\t\t"""""" Private method that counts the operations on the web services """"""\n\t\tc = 0\n\t\tfor operations in dir(self):\n\t\t\toperation = getattr(self,operations)\n\t\t\tif callable(operation) and hasattr(operation,\'_is_operation\'):\n\t\t\t\tc += 1\t\n\t\treturn c\n\n\tdef _executeOperation(self, operation, callback, method=\'\'):\n\t\t"""""" Private method that executes operations of web service """"""\n\t\tparams = []\n\t\tresponse = None\n\t\tres = None\n\t\ttypesinput = getattr(operation,\'_input\')\n\t\targs  = getattr(operation,\'_args\')\n\t\tif inspect.isclass(typesinput) and issubclass(typesinput,complextypes.ComplexType):\n\t\t\tobj = self._parseComplexType(typesinput,self._request.getBody()[0],method=method)\n\t\t\tresponse = operation(obj)\n\t\telif hasattr(operation,\'_inputArray\') and getattr(operation,\'_inputArray\'):\n\t\t\tparams = self._parseParams(self._request.getBody()[0],typesinput,args)\n\t\t\tresponse = operation(params)\n\t\telse:\n\t\t\tparams = self._parseParams(self._request.getBody()[0],typesinput,args)\n\t\t\tresponse = operation(*params)\n\t\tis_array = None\n\t\tif hasattr(operation,\'_outputArray\') and getattr(operation,\'_outputArray\'):\n\t\t\tis_array = getattr(operation,\'_outputArray\')\n\t\tdef done(response):\n\t\t\tresponse = response()\n\t\t\ttypesoutput = getattr(operation, \'_output\')\n\t\t\tif inspect.isclass(typesoutput) and issubclass(typesoutput,complextypes.ComplexType):\n\t\t\t\tres = self._createReturnsComplexType(response, method=method)\n\t\t\telse:\n\t\t\t\tres = self._createReturns(response,is_array)\n\t\t\tcallback(res)\n\t\n\t\tif isinstance(response, tornado.concurrent.Future):\n\t\t\tresponse.add_done_callback(lambda p: done(response.result))\n\t\telse:\n\t\t\tdone(lambda: response)\n\t\treturn res\n\n\tdef _parseSoap(self,xmldoc):\n\t\t"""""" Private method parse a message soap from a xmldoc like string\n\t\t    _parseSoap() return a soap.SoapMessage().\n\t\t""""""\n\t\txmldoc = bytes.decode(xmldoc)\n\t\txmldoc = xmldoc.replace(\'\\n\',\' \').replace(\'\\t\',\' \').replace(\'\\r\',\' \')\n\t\tdocument = xml.dom.minidom.parseString(xmldoc)\n\t\tprefix = document.documentElement.prefix\n\t\tnamespace = document.documentElement.namespaceURI\n\t\t\n\t\theader = self._getElementFromMessage(\'Header\',document)\n\t\tbody   = self._getElementFromMessage(\'Body\',document)\n\n\t\theader_elements = self._parseXML(header)\n\t\tbody_elements = self._parseXML(body)\n\t\t\n\t\tsoapMsg = soap.SoapMessage()\n\t\tfor h in header_elements:\n\t\t\tsoapMsg.setHeader(h)\n\t\tfor b in body_elements:\n\t\t\tsoapMsg.setBody(b)\n\t\treturn soapMsg\n\n\tdef _getElementFromMessage(self,name,document):\n\t\t"""""" Private method to search and return elements from XML """"""\n\t\tlist_of_elements = []\n\t\tfor e in document.documentElement.childNodes:\n\t\t\tif e.nodeType == e.ELEMENT_NODE and e.nodeName.count(name) >= 1:\n\t\t\t\tlist_of_elements.append(e)\n\t\treturn list_of_elements\n\n\tdef _parseXML(self,elements):\n\t\t"""""" Private method parse and digest the xml.dom.minidom.Element \n\t\t    finding the childs of Header and Body from soap message. \n\t\t    Return a list object with all of child Elements.\n\t\t""""""\n\t\telem_list = []\n\t\tif len(elements) <= 0:\n\t\t\treturn elem_list\n\t\tif elements[0].childNodes.length <= 0:\n\t\t\treturn elem_list\n\t\tfor element in elements[0].childNodes:\n\t\t\tif element.nodeType == element.ELEMENT_NODE:\n\t\t\t\tprefix = element.prefix\n\t\t\t\tnamespace = element.namespaceURI\n\t\t\t\tif prefix != None and namespace != None:\n\t\t\t\t\telement.setAttribute(\'xmlns:\'+prefix,namespace)\n\t\t\t\telse:\n\t\t\t\t\telement.setAttribute(\'xmlns:xsd\',""http://www.w3.org/2001/XMLSchema"")\n\t\t\t\t\telement.setAttribute(\'xmlns:xsi\',""http://www.w3.org/2001/XMLSchema-instance"")\n\t\t\t\telem_list.append(xml.dom.minidom.parseString(element.toxml()))\n\t\treturn elem_list\n\n\tdef _parseComplexType(self,complex,xmld,method=\'\'):\n\t\t"""""" Private method for generate an instance of class nameclass. """"""\n\t\txsdd  = \'<xsd:schema xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"">\'\n\t\txsdd += complex.toXSD(method=method,ltype=[])\n\t\txsdd += \'</xsd:schema>\'\n\t\txsd = xml.dom.minidom.parseString(xsdd)\n\t\tobj = complextypes.xml2object(xmld.toxml(),xsd,complex,method=method)\n\n\t\treturn obj\n\t\n\tdef _parseParams(self,elements,types=None,args=None):\n\t\t"""""" Private method to parse a Body element of SOAP Envelope and extract\n\t\t    the values of the request document like parameters for the soapmethod,\n\t\t    this method return a list values of parameters.\n\t\t """"""\n\t\tvalues   = []\n\t\tfor tagname in args:\n\t\t\ttype = types[tagname]\n\t\t\tvalues += self._findValues(tagname,type,elements)\n\t\treturn values\t\t\n\n\tdef _findValues(self,name,type,xml):\n\t\t"""""" Private method to find the values of elements in the XML of input """"""\n\t\telems = xml.getElementsByTagName(name)\n\t\tvalues = []\n\t\tfor e in elems:\n\t\t\tif e.hasChildNodes and len(e.childNodes) > 0:\n\t\t\t\tv = None\n\t\t\t\tif inspect.isclass(type) and (issubclass(type,xmltypes.PrimitiveType) or isinstance(type,xmltypes.Array)):\n\t\t\t\t\tv = type.genType(e.childNodes[0].nodeValue)\n\t\t\t\telif hasattr(type,\'__name__\') and (not issubclass(type,xmltypes.PrimitiveType) or not isinstance(type,xmltypes.Array)):\n\t\t\t\t\tv = complextypes.convert(type.__name__,e.childNodes[0].nodeValue)\n\t\t\t\tvalues.append(v)\n\t\t\telse:\n\t\t\t\tvalues.append(None)\n\t\treturn values\n\n\tdef _createReturnsComplexType(self,result,method=\'\'):\n\t\t"""""" Private method to generate the xml document with the response. \n\t\t    Return an SoapMessage() with XML document.\n\t\t""""""\n\t\tresponse = xml.dom.minidom.parseString(result.toXML(method=method))\n\t\t\n\t\tsoapResponse = soap.SoapMessage()\n\t\tsoapResponse.setBody(response)\n\t\treturn soapResponse\n\t\t\t\n\tdef _createReturns(self,result,is_array):\n\t\t"""""" Private method to generate the xml document with the response. \n\t\t    Return an SoapMessage().\n\t\t""""""\n\t\txmlresponse = \'\'\n\t\tif isinstance(result,list):\n\t\t\txmlresponse = \'<returns>\\n\'\n\t\t\ti = 1\n\t\t\tfor r in result:\n\t\t\t\tif is_array == True:\n\t\t\t\t\txmlresponse += \'<value>%s</value>\\n\'%str(r)\n\t\t\t\telse:\n\t\t\t\t\txmlresponse += \'<value%d>%s</value%d>\\n\'%(i,str(r),i)\n\t\t\t\ti+=1\n\t\t\txmlresponse += \'</returns>\\n\'\n\t\telse:\n\t\t\txmlresponse = \'<returns>%s</returns>\\n\'%str(result)\n\t\n\t\tresponse = xml.dom.minidom.parseString(xmlresponse)\n\n\t\tsoapResponse = soap.SoapMessage()\n\t\tsoapResponse.setBody(response)\n\t\treturn soapResponse\n'"
webui/server/tornadows/webservices.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n"""""" Implementation of webservices API 0.9 """"""\n\nimport tornado.httpserver\nimport tornado.ioloop\nimport tornado.web\nimport tornado.wsgi\n\nclass WebService(tornado.web.Application):\n\t"""""" A implementation of web services for tornado web server.\n\n\t\timport tornado.httpserver\n\t\timport tornado.ioloop\n\t\tfrom tornadows import webservices\n\t\tfrom tornadows import xmltypes\n\t   \tfrom tornadows import soaphandler\n\t\tfrom tornadows.soaphandler import webservice\n \n\t\tclass MyService(soaphandler.SoapHandler):\n\t\t\t@webservice(_params=[xmltypes.Integer, xmltypes.Integer],_returns=xmltypes.Integer)\n\t\t\tdef sum(self, value1, value2):\n\t\t\t\tresult = value1 + value2\n\t\n\t\t\t\treturn result  \n\n\t\tif __name__ == ""__main__"":\n\t\t\tapp = webservices.WebService(""MyService"",MyService)\n\t\t\tws_server = tornado.httpserver.HTTPServer(app)\n\t\t\tws_server.listen(8080)\n\t\t\ttornado.ioloop.IOLoop.instance().start()\n\t\t\n\t""""""\n\tdef __init__(self,services,object=None,wsdl=None):\n\t\t"""""" Initializes the application for web services\n\n\t\t    Instances of this class are callable and can be passed to\n\t\t    HTTPServer of tornado to serve the web services.\n\n\t\t    The constructor for this class takes the name for the web \n\t\t    service (service), the class with the web service (object) \n\t\t    and wsdl with the wsdl file path (if this exist).\n\t\t """"""\n\t\tif isinstance(services,list) and object == None:\n\t\t\tsrvs = []\n\t\t\tfor s in services:\n\t\t\t\tsrv = s[0]\n\t\t\t\tobj = s[1]\n\t\t\t\tdic = s[2]\n\t\t\t\tsrvs.append((r""/"" + str(srv), obj, dic))\n\t\t\t\tsrvs.append((r""/"" + str(srv) + ""/"", obj, dic))\n\t\t\ttornado.web.Application.__init__(self, srvs)\n\t\telse:\n\t\t\tself._service = services\n\t\t\tself._object = object\n\t\t\tself._services = [(r""/""+str(self._service),self._object),\n\t\t\t\t\t  (r""/""+str(self._service)+""/"",self._object),]\n\t\t\ttornado.web.Application.__init__(self,self._services)\n\nclass WSGIWebService(tornado.wsgi.WSGIApplication):\n\t"""""" A implementation of web services for tornado web server.\n\n\t\timport tornado.httpserver\n\t\timport tornado.ioloop\n\t\tfrom tornadows import webservices\n\t\tfrom tornadows import xmltypes\n\t   \tfrom tornadows import soaphandler\n\t\tfrom tornadows.soaphandler import webservice\n\t\timport wsgiref.simple_server\n \n\t\tclass MyService(soaphandler.SoapHandler):\n\t\t\t@webservice(_params=[xmltypes.Integer, xmltypes.Integer],_returns=xmltypes.Integer)\n\t\t\tdef sum(self, value1, value2):\n\t\t\t\tresult = value1 + value2\n\t\n\t\t\t\treturn result  \n\n\t\tif __name__ == ""__main__"":\n\t\t\tapp = webservices.WSGIWebService(""MyService"",MyService)\n\t\t\tserver = wsgiref.simple_server.make_server(\'\',8888,app)\n\t\t\tserver.serve_forever()\n\t""""""\n\tdef __init__(self,services,object=None,wsdl=None, default_host="""", **settings):\n\t\t"""""" Initializes the application for web services\n\n\t\t    Instances of this class are callable and can be passed to\n\t\t    HTTPServer of tornado to serve the web services.\n\n\t\t    The constructor for this class takes the name for the web \n\t\t    service (service), the class with the web service (object) \n\t\t    and wsdl with the wsdl file path (if this exist).\n\t\t """"""\n\t\tif isinstance(services,list) and object == None:\n\t\t\tsrvs = []\n\t\t\tfor s in services:\n\t\t\t\tsrv = s[0]\n\t\t\t\tobj = s[1]\n\t\t\t\tsrvs.append((r""/""+str(srv),obj))\n\t\t\t\tsrvs.append((r""/""+str(srv)+""/"",obj))\n\t\t\ttornado.wsgi.WSGIApplication.__init__(self,srvs,default_host, **settings)\n\t\telse:\n\t\t\tself._service = services\n\t\t\tself._object = object\n\t\t\tself._services = [(r""/""+str(self._service),self._object),\n\t\t\t\t\t  (r""/""+str(self._service)+""/"",self._object),]\n\t\t\ttornado.wsgi.WSGIApplication.__init__(self,self._services,default_host, **settings)\n'"
webui/server/tornadows/wsdl.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n"""""" Class Wsdl to generate WSDL Document """"""\nimport xml.dom.minidom\nimport inspect\nfrom webui.server.tornadows import xmltypes\nfrom webui.server.tornadows import complextypes\n\nclass Wsdl:\n\t"""""" ToDO:\n\t\t- Incorporate exceptions for parameters inputs.\n\t\t- When elementInput and/or elementOutput are empty trigger a exception.\n\t""""""\n\tdef __init__(self,nameservice=None,targetNamespace=None,methods=None,location=None):\n\t\tself._nameservice = nameservice\n\t\tself._namespace = targetNamespace\n\t\tself._methods   = methods\n\t\tself._location = location\n\n\tdef createWsdl(self):\n\t\t"""""" Method that allows create the wsdl file """"""\n\t\ttypeInput  = None\n\t\ttypeOutput = None\n\t\ttypes  = \'<wsdl:types>\\n\'\n\t\ttypes += \'<xsd:schema targetNamespace=""%s"">\\n\'%self._namespace\n\n\t\tnamespace = \'xsd\'\n\t\ttypes_list = []\n\t\tltype = []\n\t\tfor wsdl_data in self._methods:\n\t\t\tself._arguments = wsdl_data[\'args\']\n\t\t\tself._elementNameInput = wsdl_data[\'input\'][0]\n\t\t\tself._elementInput = wsdl_data[\'input\'][1]\n\t\t\tself._elementNameOutput = wsdl_data[\'output\'][0]\n\t\t\tself._elementOutput = wsdl_data[\'output\'][1]\n\t\t\tself._operation = wsdl_data[\'operation\']\n\n\t\t\tmethod = self._operation\n\n\t\t\tif len(self._methods) == 1:\n\t\t\t\tmethod = \'\'\n\n\t\t\tif inspect.isclass(self._elementInput) and issubclass(self._elementInput,complextypes.ComplexType): \n\t\t\t\ttypeInput = self._elementInput.getName()+method\n\t\t\t\t\n\t\t\t\tif ltype.count(self._elementInput.getName()) == 0:\n\t\t\t\t\tltype.append(self._elementInput.getName())\n\t\t\t\t\ttypes += self._elementInput.toXSD(method=method,ltype=ltype)\n\t\t\t\t\t\n\t\t\t\ttypes += \'<%s:element name=""%s"" type=""tns:%s""/>\'%(namespace,typeInput,self._elementInput.getName())\n\n\t\t\telif isinstance(self._elementInput,dict):\n\t\t\t\ttypeInput = self._elementNameInput+method\n\t\t\t\ttypes += self._createComplexTypes(self._elementNameInput+method, self._arguments, self._elementInput)\n\t\t\telif isinstance(self._elementInput,xmltypes.Array):\n\t\t\t\ttypeInput  = self._elementNameInput+method\n\t\t\t\ttypes += self._elementInput.createArray(typeInput)\t\t\t\n\t\t\telif isinstance(self._elementInput,list) or inspect.isclass(self._elementInput) and issubclass(self._elementInput,xmltypes.PrimitiveType):\n\t\t\t\ttypeInput  = self._elementNameInput+method\n\t\t\t\ttypes += self._createTypes(typeInput,self._elementInput)\t\t\t\n\t\t\telse: # In case if _elementNameInput is a datatype of python (str, int, float, datetime, etc.) or None\n\t\t\t\ttypeInput  = self._elementNameInput+method\n\t\t\t\ttypes += self._createTypes(typeInput,self._elementInput)\n\n\t\t\tif inspect.isclass(self._elementOutput) and issubclass(self._elementOutput,complextypes.ComplexType): \n\t\t\t\ttypeOutput = self._elementOutput.getName()+method\n\n\t\t\t\tif ltype.count(self._elementOutput.getName()) == 0:\n\t\t\t\t\tltype.append(self._elementOutput.getName())\n\t\t\t\t\ttypes += self._elementOutput.toXSD(method=method,ltype=ltype)\n\n\t\t\t\ttypes += \'<%s:element name=""%s"" type=""tns:%s""/>\'%(namespace,typeOutput,self._elementOutput.getName())\n\n\t\t\telif isinstance(self._elementOutput,xmltypes.Array):\n\t\t\t\ttypeOutput = self._elementNameOutput+method\n\t\t\t\ttypes += self._elementOutput.createArray(typeOutput)\n\t\t\telif isinstance(self._elementOutput,list) or inspect.isclass(self._elementOutput) and issubclass(self._elementOutput,xmltypes.PrimitiveType):\n\t\t\t\ttypeOutput = self._elementNameOutput+method\n\t\t\t\ttypes += self._createTypes(typeOutput,self._elementOutput)\n\t\t\telse: # In case if _elementNameOutput is a datatype of python (str, int, float, datetime, etc.) or None\n\t\t\t\ttypeOutput = self._elementNameOutput+method\n\t\t\t\ttypes += self._createTypes(typeOutput,self._elementOutput)\n\n\t\t\ttypes_list.append({\'typeInput\':typeInput,\'typeOutput\':typeOutput,\'method\':method})\n\n\t\ttypes += \'</xsd:schema>\\n\'\n\t\ttypes += \'</wsdl:types>\\n\'\n\t\t\n\t\tmessages = \'\'\n\t\t\n\t\tfor t in types_list:\n\t\t\ttypeInput = t[\'typeInput\']\n\t\t\ttypeOutput = t[\'typeOutput\']\n\t\t\tmethod = t[\'method\']\n\n\t\t\tif len(types_list) == 1:\n\t\t\t\tmethod = \'\'\n\n\t\t\tmessages += \'<wsdl:message name=""%sRequest%s"">\\n\'%(self._nameservice,method)\n\t\t\tmessages += \'<wsdl:part name=""parameters%s"" element=""tns:%s""/>\\n\'%(method,typeInput)\n\t\t\tmessages += \'</wsdl:message>\\n\'\n\n\t\t\tmessages += \'<wsdl:message name=""%sResponse%s"">\\n\'%(self._nameservice,method)\n\t\t\tmessages += \'<wsdl:part name=""returns%s"" element=""tns:%s""/>\\n\'%(method,typeOutput)\n\t\t\tmessages += \'</wsdl:message>\\n\'\n\n\t\tportType  = \'<wsdl:portType name=""%sPortType"">\\n\'%self._nameservice\n\t\t\n\t\tfor wsdl_data in self._methods:\n\t\t\tself._operation = wsdl_data[\'operation\']\n\t\t\t\n\t\t\tmethod = self._operation\n\t\t\tif len(self._methods) == 1:\n\t\t\t\tmethod = \'\'\n\n\t\t\tportType += \'<wsdl:operation name=""%s"">\\n\'%self._operation\n\t\t\tportType += \'<wsdl:input message=""tns:%sRequest%s""/>\\n\'%(self._nameservice,method)\n\t\t\tportType += \'<wsdl:output message=""tns:%sResponse%s""/>\\n\'%(self._nameservice,method)\n\t\t\tportType += \'</wsdl:operation>\\n\'\n\t\t\n\t\tportType += \'</wsdl:portType>\\n\'\n\n\t\tbinding  = \'<wsdl:binding name=""%sBinding"" type=""tns:%sPortType"">\\n\'%(self._nameservice,self._nameservice)\n\t\tbinding += \'<soap:binding style=""document"" transport=""http://schemas.xmlsoap.org/soap/http""/>\\n\'\n\n\t\tfor wsdl_data in self._methods:\n\t\t\tself._operation = wsdl_data[\'operation\']\n\n\t\t\tbinding += \'<wsdl:operation name=""%s"">\\n\'%self._operation\t\t\n\t\t\tbinding += \'<soap:operation soapAction=""%s/%s"" style=""document""/>\\n\'%(self._location,self._operation)\n\t\t\tbinding += \'<wsdl:input><soap:body use=""literal""/></wsdl:input>\\n\'\n\t\t\tbinding += \'<wsdl:output><soap:body use=""literal""/></wsdl:output>\\n\'\n\t\t\tbinding += \'</wsdl:operation>\\n\'\n\n\t\tbinding += \'</wsdl:binding>\\n\'\n\t\t\n\t\tservice  = \'<wsdl:service name=""%s"">\\n\'%self._nameservice\n\t\tservice += \'<wsdl:port name=""%sPort"" binding=""tns:%sBinding"">\\n\'%(self._nameservice,self._nameservice)\n\t\tservice += \'<soap:address location=""%s""/>\\n\'%self._location\n\t\tservice += \'</wsdl:port>\\n\'\n\t\tservice += \'</wsdl:service>\\n\'\n\n\t\tdefinitions  = \'<wsdl:definitions name=""%s""\\n\'%self._nameservice\n\t\tdefinitions  += \'xmlns:xsd=""http://www.w3.org/2001/XMLSchema""\\n\'\n\t\tdefinitions  += \'xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""\\n\'\n\t\tdefinitions  += \'xmlns:tns=""%s""\\n\'%self._namespace\n\t\tdefinitions  += \'xmlns:soap=""http://schemas.xmlsoap.org/wsdl/soap/""\\n\'\n\t\tdefinitions  += \'xmlns:wsdl=""http://schemas.xmlsoap.org/wsdl/""\\n\'\n\t\tdefinitions  += \'targetNamespace=""%s"">\\n\'%self._namespace\n\t\tdefinitions += types\n\t\tdefinitions += messages\n\t\tdefinitions += portType\n\t\tdefinitions += binding\n\t\tdefinitions += service\n\t\tdefinitions += \'</wsdl:definitions>\\n\'\n\t\twsdlXml = xml.dom.minidom.parseString(definitions)\n\n\t\treturn wsdlXml\n\n\tdef _createTypes(self, name, elements):\n\t\t"""""" Private method that creates the types for the elements of wsdl """"""\n\t\telem = \'\'\n\t\tif isinstance(elements,list):\n\t\t\telem = \'<xsd:complexType name=""%sParams"">\\n\'%name\n\t\t\telem += \'<xsd:sequence>\\n\'\n\t\t\telems = \'\'\n\t\t\tidx = 1\n\t\t\tfor e in elements:\n\t\t\t\tif hasattr(e,\'__name__\'):\n\t\t\t\t\telems += \'<xsd:element name=""value%d"" type=""xsd:%s""/>\\n\'%(idx,complextypes.createPythonType2XMLType(e.__name__))\n\t\t\t\telse:\n\t\t\t\t\telems += e.createElement(\'value%s\'%idx)+\'\\n\'\n\t\t\t\tidx += 1\n\t\t\telem += elems+\'</xsd:sequence>\\n\'\n\t\t\telem += \'</xsd:complexType>\\n\'\n\t\t\telem += \'<xsd:element name=""%s"" type=""tns:%sParams""/>\\n\'%(name,name)\n\t\telif inspect.isclass(elements) and issubclass(elements,xmltypes.PrimitiveType):\n\t\t\telem = elements.createElement(name)+\'\\n\'\n\t\telif hasattr(elements,\'__name__\'):\n\t\t\telem += \'<xsd:element name=""%s"" type=""xsd:%s""/>\\n\'%(name,complextypes.createPythonType2XMLType(elements.__name__))\n\n\t\treturn elem\n\n\tdef _createComplexTypes(self, name, arguments, elements):\n\t\t"""""" Private method that creates complex types for wsdl """"""\n\t\telem = \'\'\n\t\tif isinstance(elements,dict):\n\t\t\telem = \'<xsd:complexType name=""%sTypes"">\\n\'%name\n\t\t\telem += \'<xsd:sequence>\\n\'\n\t\t\telems = \'\'\n\t\t\tfor e in arguments:\n\t\t\t\tif  isinstance(elements[e],xmltypes.Array):\n\t\t\t\t\telems += elements[e].createType(e)\n\t\t\t\telif issubclass(elements[e],xmltypes.PrimitiveType):\n\t\t\t\t\telems += elements[e].createElement(e)+\'\\n\'\n\t\t\t\telse:\n\t\t\t\t\telems += \'<xsd:element name=""%s"" type=""xsd:%s""/>\\n\'%(e,complextypes.createPythonType2XMLType(elements[e].__name__))\n\t\t\telem += elems+\'</xsd:sequence>\\n\'\n\t\t\telem += \'</xsd:complexType>\\n\'\n\t\t\telem += \'<xsd:element name=""%s"" type=""tns:%sTypes""/>\\n\'%(name,name)\n\t\telif issubclass(elements,xmltypes.PrimitiveType):\n\t\t\telem = elements.createElement(name)+\'\\n\'\n\n\t\treturn elem\n'"
webui/server/tornadows/xmltypes.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2011 Rodrigo Ancavil del Pino\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n"""""" \n\tAre incorporated the primitive datatypes defined by XML.\n\tArray is defined for the use of array of elements and his respective datatype.\n""""""\n\nimport inspect\nfrom webui.server.tornadows import complextypes\n\ndef createElementXML(name,type,prefix=\'xsd\'):\n\t"""""" Function used for the creation of xml elements. """"""\n\treturn \'<%s:element name=""%s"" type=""%s:%s""/>\'%(prefix,name,prefix,type)\n\ndef createArrayXML(name,type,prefix=\'xsd\',maxoccurs=None):\n\t"""""" Function used for the creation of xml complexElements """"""\n\tcomplexType  = \'<%s:complexType name=""%sParams"">\\n\'%(prefix,name)\n\tcomplexType += \'<%s:sequence>\\n\'%prefix\n\tif maxoccurs == None:\n\t\tcomplexType += \'<%s:element name=""value"" type=""%s:%s"" maxOccurs=""unbounded""/>\\n\'%(prefix,prefix,type)\n\telse:\n\t\tcomplexType += \'<%s:element name=""value"" type=""%s:%s"" maxOccurs=""%d""/>\\n\'%(prefix,prefix,type,maxoccurs)\n\tcomplexType += \'</%s:sequence>\\n\'%prefix\n\tcomplexType += \'</%s:complexType>\\n\'%prefix\n\tcomplexType += \'<%s:element name=""%s"" type=""tns:%sParams""/>\\n\'%(prefix,name,name)\n\treturn complexType\n\nclass Array:\n\t"""""" Create arrays of xml elements.\n\t    \n\t    Here an example:\n\n\t    @webservices(_params=xmltypes.Array(xmltypes.Integer),_returns=xmltypes.Integer)\n\t    def function(sefl, list_of_elements):\n\t\tfor e in list_of_elements:\n\t\t# Do something with the element    \n        \treturn len(list_of_elements)\n\n\t    xmltypes.Array(xmltype.Integer) generate an xml element into schema definition:\n\t\t<xsd:element name=""arrayOfElement"" type=""xsd:integer"" maxOccurs=""unbounded""/>\n\n\t    this make the parameter of the function list_of_elements is a python list.\n\n\t    if you specify xmltypes.Array(xmltypes.Integer,10), is generated:\n\t\t<xsd:element name=""arrayOfElement"" type=""xsd:integer"" maxOccurs=""10""/>\n\t""""""\n\tdef __init__(self,type,maxOccurs=None):\n\t\tself._type = type\n\t\tself._n    = maxOccurs\n\n\tdef createArray(self,name):\n\t\ttype = None\n\t\tif inspect.isclass(self._type) and not issubclass(self._type,PrimitiveType):\n\t\t\ttype = complextypes.createPythonType2XMLType(self._type.__name__)\n\t\telse:\n\t\t\ttype = self._type.getType(self._type)\n\t\treturn createArrayXML(name,type,\'xsd\',self._n)\n\n\tdef createType(self,name):\n\t\tprefix = \'xsd\'\n\t\ttype = None\n\t\tif inspect.isclass(self._type) and not issubclass(self._type,PrimitiveType):\n\t\t\ttype = complextypes.createPythonType2XMLType(self._type.__name__)\n\t\telse:\n\t\t\ttype = self._type.getType(self._type)\n\t\tmaxoccurs = self._n\n\t\tcomplexType = \'\'\n\t\tif self._n == None:\n\t\t\tcomplexType += \'<%s:element name=""%s"" type=""%s:%s"" maxOccurs=""unbounded""/>\\n\'%(prefix,name,prefix,type)\n\t\telse:\n\t\t\tcomplexType += \'<%s:element name=""%s"" type=""%s:%s"" maxOccurs=""%d""/>\\n\'%(prefix,name,prefix,type,maxoccurs)\n\t\treturn complexType\n\n\tdef genType(self,v):\n\t\tvalue = None\n\t\tif inspect.isclass(self._type) and issubclass(self._type,PrimitiveType):\n\t\t\tvalue = self._type.genType(v)\n\t\telif hasattr(self._type,\'__name__\'):\n\t\t\tvalue = complextypes.convert(self._type.__name__,v)\n\t\t\t# Convert str to bool\n\t\t\tif value == \'true\':\n\t\t\t\tvalue = True\n\t\t\telif value == \'false\':\n\t\t\t\tvalue = False\n\t\treturn value\n\nclass PrimitiveType:\n\t"""""" Class father for all derived types. """"""\n\tpass\n\nclass Integer(PrimitiveType):\n\t"""""" 1. XML primitive type : integer """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'integer\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'integer\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn int(v)\n\nclass Decimal(PrimitiveType):\n\t"""""" 2. XML primitive type : decimal """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'decimal\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'decimal\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn float(v)\n\nclass Double(PrimitiveType):\n\t"""""" 3. XML primitive type : double """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'double\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'double\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn float(v)\n\nclass Float(PrimitiveType):\n\t"""""" 4. XML primitive type : float """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'float\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'float\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn float(v)\n\nclass Duration(PrimitiveType):\n\t"""""" 5. XML primitive type : duration """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'duration\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'duration\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn str(v)\n\nclass Date(PrimitiveType):\n\t"""""" 6. XML primitive type : date """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'date\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'date\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn str(v)\n\nclass Time(PrimitiveType):\n\t"""""" 7. XML primitive type : time """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'time\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'time\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn str(v)\n\nclass DateTime(PrimitiveType):\n\t"""""" 8. XML primitive type : dateTime """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'dateTime\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'dateTime\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn str(v)\n\nclass String(PrimitiveType):\n\t"""""" 9. XML primitive type : string """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'string\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'string\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn str(v)\n\nclass Boolean(PrimitiveType):\n\t"""""" 10. XML primitive type : boolean """"""\n\t@staticmethod\n\tdef createElement(name,prefix=\'xsd\'):\n\t\treturn createElementXML(name,\'boolean\')\n\t@staticmethod\n\tdef getType(self):\n\t\treturn \'boolean\'\n\t@classmethod\n\tdef genType(self,v):\n\t\treturn str(v).lower()\n'"
